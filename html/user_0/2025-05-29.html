<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-29</h1>
<h3>Title: The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows</h3>
<ul>
<li><strong>Authors: </strong>Harry Li, Gabriel Appleby, Kenneth Alperin, Steven R Gomez, Ashley Suh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21512">https://arxiv.org/abs/2505.21512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21512">https://arxiv.org/pdf/2505.21512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21512]] The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows(https://arxiv.org/abs/2505.21512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) are powerful data structures, but exploring them effectively remains difficult for even expert users. Large language models (LLMs) are increasingly used to address this gap, yet little is known empirically about how their usage with KGs shapes user trust, exploration strategies, or downstream decision-making - raising key design challenges for LLM-based KG visual analysis systems. To study these effects, we developed LinkQ, a KG exploration system that converts natural language questions into structured queries with an LLM. We collaborated with KG experts to design five visual mechanisms that help users assess the accuracy of both KG queries and LLM responses: an LLM-KG state diagram that illustrates which stage of the exploration pipeline LinkQ is in, a query editor displaying the generated query paired with an LLM explanation, an entity-relation ID table showing extracted KG entities and relations with semantic descriptions, a query structure graph that depicts the path traversed in the KG, and an interactive graph visualization of query results. From a qualitative evaluation with 14 practitioners, we found that users - even KG experts - tended to overtrust LinkQ's outputs due to its "helpful" visualizations, even when the LLM was incorrect. Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all - despite often being designed as if they are. Our findings highlight the risks of false trust in LLM-assisted data analysis tools and the need for further investigation into the role of visualization as a mitigation technique.</li>
</ul>

<h3>Title: Enhancing Vision Transformer Explainability Using Artificial Astrocytes</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Echevarrieta-Catalan, Ana Ribas-Rodriguez, Francisco Cedron, Odelia Schwartz, Vanessa Aguiar-Pulido</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21513">https://arxiv.org/abs/2505.21513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21513">https://arxiv.org/pdf/2505.21513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21513]] Enhancing Vision Transformer Explainability Using Artificial Astrocytes(https://arxiv.org/abs/2505.21513)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.</li>
</ul>

<h3>Title: SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Mingchao Jiang, Abhinav Jain, Sophia Zorek, Chris Jermaine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21514">https://arxiv.org/abs/2505.21514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21514">https://arxiv.org/pdf/2505.21514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21514]] SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation(https://arxiv.org/abs/2505.21514)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce SIMCOPILOT, a benchmark that simulates the role of large language models (LLMs) as interactive, "copilot"-style coding assistants. Targeting both completion (finishing incomplete methods or code blocks) and infill tasks (filling missing segments within existing code), SIMCOPILOT provides a comprehensive framework for evaluating LLM coding capabilities. The benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python (SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our key contributions include: (a) establishing a realistic, detailed evaluation environment to assess LLM utility in practical coding scenarios, and (b) providing fine-grained analyses that address critical factors frequently overlooked by existing benchmarks, such as task-specific performance nuances, contextual understanding across code segments, and sensitivity to variable scope. Evaluations conducted across domains-including algorithms, databases, computer vision, and neural networks-offer insights into model strengths and highlight persistent challenges in maintaining logical consistency within complex dependency structures. Beyond benchmarking, our study sheds light on the current limitations of LLM-driven code generation and underscores the ongoing transition of LLMs from merely syntax-aware generators toward reliable, intelligent software development partners.</li>
</ul>

<h3>Title: Do DeepFake Attribution Models Generalize?</h3>
<ul>
<li><strong>Authors: </strong>Spiros Baxavanakis, Manos Schinas, Symeon Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21520">https://arxiv.org/abs/2505.21520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21520">https://arxiv.org/pdf/2505.21520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21520]] Do DeepFake Attribution Models Generalize?(https://arxiv.org/abs/2505.21520)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Recent advancements in DeepFake generation, along with the proliferation of open-source tools, have significantly lowered the barrier for creating synthetic media. This trend poses a serious threat to the integrity and authenticity of online information, undermining public trust in institutions and media. State-of-the-art research on DeepFake detection has primarily focused on binary detection models. A key limitation of these models is that they treat all manipulation techniques as equivalent, despite the fact that different methods introduce distinct artifacts and visual cues. Only a limited number of studies explore DeepFake attribution models, although such models are crucial in practical settings. By providing the specific manipulation method employed, these models could enhance both the perceived trustworthiness and explainability for end users. In this work, we leverage five state-of-the-art backbone models and conduct extensive experiments across six DeepFake datasets. First, we compare binary and multi-class models in terms of cross-dataset generalization. Second, we examine the accuracy of attribution models in detecting seen manipulation methods in unknown datasets, hence uncovering data distribution shifts on the same DeepFake manipulations. Last, we assess the effectiveness of contrastive methods in improving cross-dataset generalization performance. Our findings indicate that while binary models demonstrate better generalization abilities, larger models, contrastive methods, and higher data quality can lead to performance improvements in attribution models. The code of this work is available on GitHub.</li>
</ul>

<h3>Title: CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures</h3>
<ul>
<li><strong>Authors: </strong>Shan Gao, Zhiqiang Wu, Yawen Niu, Xiaotao Li, Qingqing Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21522">https://arxiv.org/abs/2505.21522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21522">https://arxiv.org/pdf/2505.21522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21522]] CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures(https://arxiv.org/abs/2505.21522)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB</li>
</ul>

<h3>Title: More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21523">https://arxiv.org/abs/2505.21523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21523">https://arxiv.org/pdf/2505.21523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21523]] More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models(https://arxiv.org/abs/2505.21523)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.</li>
</ul>

<h3>Title: Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Peiliang Gong, Yucheng Wang, Min Wu, Zhenghua Chen, Xiaoli Li, Daoqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21525">https://arxiv.org/abs/2505.21525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21525">https://arxiv.org/pdf/2505.21525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21525]] Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation(https://arxiv.org/abs/2505.21525)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from an annotated source domain to an unlabelled target domain without accessing the source data, thereby preserving data privacy. While existing SFDA methods have proven effective in reducing reliance on source data, they struggle to perform well on multivariate time series (MTS) due to their failure to consider the intrinsic spatial correlations inherent in MTS data. These spatial correlations are crucial for accurately representing MTS data and preserving invariant information across domains. To address this challenge, we propose Temporal Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method tailored for MTS data. Specifically, TERSE comprises a customized spatial-temporal feature encoder designed to capture the underlying spatial-temporal characteristics, coupled with both temporal restoration and spatial rewiring tasks to reinstate latent representations of the temporally masked time series and the spatially masked correlated structures. During the target adaptation phase, the target encoder is guided to produce spatially and temporally consistent features with the source domain by leveraging the source pre-trained temporal restoration and spatial rewiring networks. Therefore, TERSE can effectively model and transfer spatial-temporal dependencies across domains, facilitating implicit feature alignment. In addition, as the first approach to simultaneously consider spatial-temporal consistency in MTS-SFDA, TERSE can also be integrated as a versatile plug-and-play module into established SFDA methods. Extensive experiments on three real-world time series datasets demonstrate the effectiveness and versatility of our approach.</li>
</ul>

<h3>Title: UniDB++: Fast Sampling of Unified Diffusion Bridge</h3>
<ul>
<li><strong>Authors: </strong>Mokai Pan, Kaizhen Zhu, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21528">https://arxiv.org/abs/2505.21528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21528">https://arxiv.org/pdf/2505.21528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21528]] UniDB++: Fast Sampling of Unified Diffusion Bridge(https://arxiv.org/abs/2505.21528)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Bridges enable transitions between arbitrary distributions, with the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's reliance on iterative Euler sampling methods results in slow, computationally expensive inference, while existing acceleration techniques for diffusion or diffusion bridge models fail to address its unique challenges: missing terminal mean constraints and SOC-specific penalty coefficients in its SDEs. We present UniDB++, a training-free sampling algorithm that significantly improves upon these limitations. The method's key advancement comes from deriving exact closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the error accumulation inherent in Euler approximations and enabling high-quality generation with up to 20$\times$ fewer sampling steps. This method is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes (5-10 steps). Additionally, we demonstrate that UniDB++ aligns with existing diffusion bridge acceleration methods by evaluating their update rules, and UniDB++ can recover DBIMs as special cases under some theoretical conditions. Experiments demonstrate UniDB++'s state-of-the-art performance in image restoration tasks, outperforming Euler-based methods in fidelity and speed while reducing inference time significantly. This work bridges the gap between theoretical generality and practical efficiency in SOC-driven diffusion bridge models. Our code is available at this https URL.</li>
</ul>

<h3>Title: How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</h3>
<ul>
<li><strong>Authors: </strong>Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21531">https://arxiv.org/abs/2505.21531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21531">https://arxiv.org/pdf/2505.21531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21531]] How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control(https://arxiv.org/abs/2505.21531)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explore Large Language Models (LLMs)' human motion knowledge through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations as a clear verification lens for human evaluators. Through carefully designed 20 representative motion instructions with full coverage of basic movement primitives and balanced body part usage, we conduct comprehensive evaluations including human assessment of both generated animations and high-level movement plans, as well as automatic comparison with oracle positions in low-level planning. We find that LLMs are strong at interpreting the high-level body movements but struggle with precise body part positioning. While breaking down motion queries into atomic components improves planning performance, LLMs have difficulty with multi-step movements involving high-degree-of-freedom body parts. Furthermore, LLMs provide reasonable approximation for general spatial descriptions, but fail to handle precise spatial specifications in text, and the precise spatial-temporal parameters needed for avatar control. Notably, LLMs show promise in conceptualizing creative motions and distinguishing culturally-specific motion patterns.</li>
</ul>

<h3>Title: EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media</h3>
<ul>
<li><strong>Authors: </strong>Ismail Erbas, Ferhat Demirkiran, Karthik Swaminathan, Naigang Wang, Navid Ibtehaj Nizam, Stefan T. Radev, Kaoutar El Maghraoui, Xavier Intes, Vikas Pandey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21532">https://arxiv.org/abs/2505.21532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21532">https://arxiv.org/pdf/2505.21532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21532]] EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media(https://arxiv.org/abs/2505.21532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology employed for distance and depth estimation across medical, automotive, and other fields, encounters significant computational challenges in scattering media. The complex nature of the acquired FLiDAR signal, particularly in such environments, makes isolating photon time-of-flight (related to target depth) and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the effectiveness of current analytical and computational methodologies. To overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE) framework tailored for specialized modeling of diverse temporal components. In contrast to the conventional MoE approaches our expert models are informed by underlying physics, such as the radiative transport equation governing photon propagation in scattering media. Central to our approach is EvidenceMoE, which integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess the reliability of each expert's output by providing per-expert quality scores and corrective feedback. A Decider Network then leverages this information to fuse expert predictions into a robust final estimate adaptively. We validate our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for non-invasive cancer cell depth detection generated from photon transport models in tissue. Our framework demonstrates strong performance, achieving a normalized root mean squared error (NRMSE) of 0.030 for depth estimation and 0.074 for fluorescence lifetime.</li>
</ul>

<h3>Title: Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21535">https://arxiv.org/abs/2505.21535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21535">https://arxiv.org/pdf/2505.21535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21535]] Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement(https://arxiv.org/abs/2505.21535)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module.</li>
</ul>

<h3>Title: Equivariant Flow Matching for Point Cloud Assembly</h3>
<ul>
<li><strong>Authors: </strong>Ziming Wang, Nan Xue, Rebecka Jörnsten</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21539">https://arxiv.org/abs/2505.21539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21539">https://arxiv.org/pdf/2505.21539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21539]] Equivariant Flow Matching for Point Cloud Assembly(https://arxiv.org/abs/2505.21539)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The goal of point cloud assembly is to reconstruct a complete 3D shape by aligning multiple point cloud pieces. This work presents a novel equivariant solver for assembly tasks based on flow matching models. We first theoretically show that the key to learning equivariant distributions via flow matching is to learn related vector fields. Based on this result, we propose an assembly model, called equivariant diffusion assembly (Eda), which learns related vector fields conditioned on the input pieces. We further construct an equivariant path for Eda, which guarantees high data efficiency of the training process. Our numerical results show that Eda is highly competitive on practical datasets, and it can even handle the challenging situation where the input pieces are non-overlapped.</li>
</ul>

<h3>Title: DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21541">https://arxiv.org/abs/2505.21541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21541">https://arxiv.org/pdf/2505.21541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21541]] DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers(https://arxiv.org/abs/2505.21541)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: this https URL.</li>
</ul>

<h3>Title: Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance</h3>
<ul>
<li><strong>Authors: </strong>Semanto Mondal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21544">https://arxiv.org/abs/2505.21544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21544">https://arxiv.org/pdf/2505.21544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21544]] Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance(https://arxiv.org/abs/2505.21544)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As a social being, we have an intimate bond with the environment. A plethora of things in human life, such as lifestyle, health, and food are dependent on the environment and agriculture. It comes under our responsibility to support the environment as well as agriculture. However, traditional farming practices often result in inefficient resource use and environmental challenges. To address these issues, precision agriculture has emerged as a promising approach that leverages advanced technologies to optimise agricultural processes. In this work, a hybrid approach is proposed that combines the three different potential fields of model AI: object detection, large language model (LLM), and Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to combine the vision and language models to work together to identify potential diseases in the tree leaf. This study introduces a novel AI-based precision agriculture system that uses Retrieval Augmented Generation (RAG) to provide context-aware diagnoses and natural language processing (NLP) and YOLOv8 for crop disease detection. The system aims to tackle major issues with large language models (LLMs), especially hallucinations and allows for adaptive treatment plans and real-time disease detection. The system provides an easy-to-use interface to the farmers, which they can use to detect the different diseases related to coffee leaves by just submitting the image of the affected leaf the model will detect the diseases as well as suggest potential remediation methodologies which aim to lower the use of pesticides, preserving livelihoods, and encouraging environmentally friendly methods. With an emphasis on scalability, dependability, and user-friendliness, the project intends to improve RAG-integrated object detection systems for wider agricultural applications in the future.</li>
</ul>

<h3>Title: Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chika Maduabuchi, Hao Chen, Yujin Han, Jindong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21545">https://arxiv.org/abs/2505.21545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21545">https://arxiv.org/pdf/2505.21545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21545]] Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation(https://arxiv.org/abs/2505.21545)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are sensitive to imperfect conditioning, which causes semantic drift and temporal incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the first corruption-aware training framework for LVDMs that improves robustness through structured, data-aligned noise injection. Our method includes Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal consistency. BCNI is especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral directions to improve low-frequency smoothness, showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101. Ablation studies confirm the benefit of low-rank, data-aligned noise. Our theoretical analysis further explains how such perturbations tighten entropy, Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM establishes a principled, scalable training approach for robust video diffusion under multimodal noise. Code and models: this https URL</li>
</ul>

<h3>Title: Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing</h3>
<ul>
<li><strong>Authors: </strong>Weixing Wang, Zifeng Ding, Jindong Gu, Rui Cao, Christoph Meinel, Gerard de Melo, Haojin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21547">https://arxiv.org/abs/2505.21547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21547">https://arxiv.org/pdf/2505.21547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21547]] Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing(https://arxiv.org/abs/2505.21547)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) with discrete image tokenizers unify multimodal representations by encoding visual inputs into a finite set of tokens. Despite their effectiveness, we find that these models still hallucinate non-existent objects. We hypothesize that this may be due to visual priors induced during training: When certain image tokens frequently co-occur in the same spatial regions and represent shared objects, they become strongly associated with the verbalizations of those objects. As a result, the model may hallucinate by evoking visually absent tokens that often co-occur with present ones. To test this assumption, we construct a co-occurrence graph of image tokens using a segmentation dataset and employ a Graph Neural Network (GNN) with contrastive learning followed by a clustering method to group tokens that frequently co-occur in similar visual contexts. We find that hallucinations predominantly correspond to clusters whose tokens dominate the input, and more specifically, that the visually absent tokens in those clusters show much higher correlation with hallucinated objects compared to tokens present in the image. Based on this observation, we propose a hallucination mitigation method that suppresses the influence of visually absent tokens by modifying latent image embeddings during generation. Experiments show our method reduces hallucinations while preserving expressivity. Code is available at this https URL</li>
</ul>

<h3>Title: Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation</h3>
<ul>
<li><strong>Authors: </strong>Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabeau, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21549">https://arxiv.org/abs/2505.21549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21549">https://arxiv.org/pdf/2505.21549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21549]] Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation(https://arxiv.org/abs/2505.21549)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP's original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at this https URL.</li>
</ul>

<h3>Title: Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment</h3>
<ul>
<li><strong>Authors: </strong>Omid Halimi Milani, Amanda Nikho, Marouane Tliba, Lauren Mills, Ahmet Enis Cetin, Mohammed H Elnagar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21561">https://arxiv.org/abs/2505.21561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21561">https://arxiv.org/pdf/2505.21561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21561]] Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment(https://arxiv.org/abs/2505.21561)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce a novel deep learning framework for the automated staging of spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in both orthodontics and forensic anthropology. Our approach leverages a dual-model architecture wherein a teacher model, trained on manually cropped images, transfers its precise spatial understanding to a student model that operates on full, uncropped images. This knowledge distillation is facilitated by a newly formulated loss function that aligns spatial logits as well as incorporates gradient-based attention spatial mapping, ensuring that the student model internalizes the anatomically relevant features without relying on external cropping or YOLO-based segmentation. By leveraging expert-curated data and feedback at each step, our framework attains robust diagnostic accuracy, culminating in a clinically viable end-to-end pipeline. This streamlined approach obviates the need for additional pre-processing tools and accelerates deployment, thereby enhancing both the efficiency and consistency of skeletal maturation assessment in diverse clinical settings.</li>
</ul>

<h3>Title: Diffusion Model-based Activity Completion for AI Motion Capture from Videos</h3>
<ul>
<li><strong>Authors: </strong>Gao Huayu, Huang Tengjiu, Ye Xiaolong, Tsuyoshi Okita</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21566">https://arxiv.org/abs/2505.21566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21566">https://arxiv.org/pdf/2505.21566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21566]] Diffusion Model-based Activity Completion for AI Motion Capture from Videos(https://arxiv.org/abs/2505.21566)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.</li>
</ul>

<h3>Title: ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</h3>
<ul>
<li><strong>Authors: </strong>Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21569">https://arxiv.org/abs/2505.21569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21569">https://arxiv.org/pdf/2505.21569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21569]] ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools(https://arxiv.org/abs/2505.21569)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.this http URL.</li>
</ul>

<h3>Title: Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Sungwon Kim, Namkyeong Lee, Yunyoung Doh, Seungmin Shin, Guimok Cho, Seung-Won Jeon, Sangkook Kim, Chanyoung Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21572">https://arxiv.org/abs/2505.21572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21572">https://arxiv.org/pdf/2505.21572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21572]] Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks(https://arxiv.org/abs/2505.21572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mesh-based 3D static analysis methods have recently emerged as efficient alternatives to traditional computational numerical solvers, significantly reducing computational costs and runtime for various physics-based analyses. However, these methods primarily focus on surface topology and geometry, often overlooking the inherent thickness of real-world 3D objects, which exhibits high correlations and similar behavior between opposing surfaces. This limitation arises from the disconnected nature of these surfaces and the absence of internal edge connections within the mesh. In this work, we propose a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN), that effectively integrates the thickness of 3D objects while maintaining the computational efficiency of surface meshes. Additionally, we introduce data-driven coordinates that encode spatial information while preserving E(3)-equivariance or invariance properties, ensuring consistent and robust analysis. Evaluations on a real-world industrial dataset demonstrate the superior performance of T-EMNN in accurately predicting node-level 3D deformations, effectively capturing thickness effects while maintaining computational efficiency.</li>
</ul>

<h3>Title: Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes</h3>
<ul>
<li><strong>Authors: </strong>Han Wan, Rui Zhang, Hao Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21573">https://arxiv.org/abs/2505.21573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21573">https://arxiv.org/pdf/2505.21573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21573]] Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes(https://arxiv.org/abs/2505.21573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $\Pi$-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms.</li>
</ul>

<h3>Title: Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dang Nguyen, Jiping Li, Jinghao Zheng, Baharan Mirzasoleiman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21574">https://arxiv.org/abs/2505.21574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21574">https://arxiv.org/pdf/2505.21574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21574]] Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models(https://arxiv.org/abs/2505.21574)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Synthetically augmenting training datasets with diffusion models has been an effective strategy for improving generalization of image classifiers. However, existing techniques struggle to ensure the diversity of generation and increase the size of the data by up to 10-30x to improve the in-distribution performance. In this work, we show that synthetically augmenting part of the data that is not learned early in training outperforms augmenting the entire dataset. By analyzing a two-layer CNN, we prove that this strategy improves generalization by promoting homogeneity in feature learning speed without amplifying noise. Our extensive experiments show that by augmenting only 30%-40% of the data, our method boosts the performance by up to 2.8% in a variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10, CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM. Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and strong augmentation strategies to further boost the performance.</li>
</ul>

<h3>Title: Fairness in Federated Learning: Fairness for Whom?</h3>
<ul>
<li><strong>Authors: </strong>Afaf Taik, Khaoula Chehbouni, Golnoosh Farnadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21584">https://arxiv.org/abs/2505.21584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21584">https://arxiv.org/pdf/2505.21584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21584]] Fairness in Federated Learning: Fairness for Whom?(https://arxiv.org/abs/2505.21584)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, federate, fair</a></li>
<li><strong>Abstract: </strong>Fairness in federated learning has emerged as a rapidly growing area of research, with numerous works proposing formal definitions and algorithmic interventions. Yet, despite this technical progress, fairness in FL is often defined and evaluated in ways that abstract away from the sociotechnical contexts in which these systems are deployed. In this paper, we argue that existing approaches tend to optimize narrow system level metrics, such as performance parity or contribution-based rewards, while overlooking how harms arise throughout the FL lifecycle and how they impact diverse stakeholders. We support this claim through a critical analysis of the literature, based on a systematic annotation of papers for their fairness definitions, design decisions, evaluation practices, and motivating use cases. Our analysis reveals five recurring pitfalls: 1) fairness framed solely through the lens of server client architecture, 2) a mismatch between simulations and motivating use-cases and contexts, 3) definitions that conflate protecting the system with protecting its users, 4) interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects, 5) and a lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once. Building on these insights, we propose a harm centered framework that links fairness definitions to concrete risks and stakeholder vulnerabilities. We conclude with recommendations for more holistic, context-aware, and accountable fairness research in FL.</li>
</ul>

<h3>Title: Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, Tao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21591">https://arxiv.org/abs/2505.21591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21591">https://arxiv.org/pdf/2505.21591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21591]] Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning(https://arxiv.org/abs/2505.21591)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.</li>
</ul>

<h3>Title: Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yang Yang, Siming Zheng, Jinwei Chen, Boxi Wu, Xiaofei He, Deng Cai, Bo Li, Peng-Tao Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21593">https://arxiv.org/abs/2505.21593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21593">https://arxiv.org/pdf/2505.21593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21593]] Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion(https://arxiv.org/abs/2505.21593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion based editing models have enabled realistic camera simulation and image-based bokeh, but video bokeh remains largely unexplored. Existing video editing models cannot explicitly control focus planes or adjust bokeh intensity, limiting their applicability for controllable optical effects. Moreover, naively extending image-based bokeh methods to video often results in temporal flickering and unsatisfactory edge blur transitions due to the lack of temporal modeling and generalization capability. To address these challenges, we propose a novel one-step video bokeh framework that converts arbitrary input videos into temporally coherent, depth-aware bokeh effects. Our method leverages a multi-plane image (MPI) representation constructed through a progressively widening depth sampling function, providing explicit geometric guidance for depth-dependent blur synthesis. By conditioning a single-step video diffusion model on MPI layers and utilizing the strong 3D priors from pre-trained models such as Stable Video Diffusion, our approach achieves realistic and consistent bokeh effects across diverse scenes. Additionally, we introduce a progressive training strategy to enhance temporal consistency, depth robustness, and detail preservation. Extensive experiments demonstrate that our method produces high-quality, controllable bokeh effects and achieves state-of-the-art performance on multiple evaluation benchmarks.</li>
</ul>

<h3>Title: Relevance-driven Input Dropout: an Explanation-guided Regularization Technique</h3>
<ul>
<li><strong>Authors: </strong>Shreyas Gururaj, Lars Grüne, Wojciech Samek, Sebastian Lapuschkin, Leander Weber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21595">https://arxiv.org/abs/2505.21595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21595">https://arxiv.org/pdf/2505.21595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21595]] Relevance-driven Input Dropout: an Explanation-guided Regularization Technique(https://arxiv.org/abs/2505.21595)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Overfitting is a well-known issue extending even to state-of-the-art (SOTA) Machine Learning (ML) models, resulting in reduced generalization, and a significant train-test performance gap. Mitigation measures include a combination of dropout, data augmentation, weight decay, and other regularization techniques. Among the various data augmentation strategies, occlusion is a prominent technique that typically focuses on randomly masking regions of the input during training. Most of the existing literature emphasizes randomness in selecting and modifying the input features instead of regions that strongly influence model decisions. We propose Relevance-driven Input Dropout (RelDrop), a novel data augmentation method which selectively occludes the most relevant regions of the input, nudging the model to use other important features in the prediction process, thus improving model generalization through informed regularization. We further conduct qualitative and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop) affects model decision-making. Through a series of experiments on benchmark datasets, we demonstrate that our approach improves robustness towards occlusion, results in models utilizing more features within the region of interest, and boosts inference time generalization performance. Our code is available at this https URL.</li>
</ul>

<h3>Title: Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Yajiao Liu, Congliang Chen, Junchi Yang, Ruoyu Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21598">https://arxiv.org/abs/2505.21598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21598">https://arxiv.org/pdf/2505.21598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21598]] Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives(https://arxiv.org/abs/2505.21598)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models with data collected from various domains can improve their performance on downstream tasks. However, given a fixed training budget, the sampling proportions of these different domains significantly impact the model's performance. How can we determine the domain weights across different data domains to train the best-performing model within constrained computational resources? In this paper, we provide a comprehensive overview of existing data mixture methods. First, we propose a fine-grained categorization of existing methods, extending beyond the previous offline and online classification. Offline methods are further grouped into heuristic-based, algorithm-based, and function fitting-based methods. For online methods, we categorize them into three groups: online min-max optimization, online mixing law, and other approaches by drawing connections with the optimization frameworks underlying offline methods. Second, we summarize the problem formulations, representative algorithms for each subtype of offline and online methods, and clarify the relationships and distinctions among them. Finally, we discuss the advantages and disadvantages of each method and highlight key challenges in the field of data mixture.</li>
</ul>

<h3>Title: R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21600">https://arxiv.org/abs/2505.21600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21600">https://arxiv.org/pdf/2505.21600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21600]] R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing(https://arxiv.org/abs/2505.21600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at this https URL.</li>
</ul>

<h3>Title: SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Fengqing Jiang, Fengbo Ma, Zhangchen Xu, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bo Li, Xianyan Chen, Zhen Xiang, Radha Poovendran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21605">https://arxiv.org/abs/2505.21605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21605">https://arxiv.org/pdf/2505.21605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21605]] SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge(https://arxiv.org/abs/2505.21605)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios. To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs.</li>
</ul>

<h3>Title: How does Misinformation Affect Large Language Model Behaviors and Preferences?</h3>
<ul>
<li><strong>Authors: </strong>Miao Peng, Nuo Chen, Jianheng Tang, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21608">https://arxiv.org/abs/2505.21608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21608">https://arxiv.org/pdf/2505.21608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21608]] How does Misinformation Affect Large Language Model Behaviors and Preferences?(https://arxiv.org/abs/2505.21608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at this https URL.</li>
</ul>

<h3>Title: Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study</h3>
<ul>
<li><strong>Authors: </strong>Mathew J. Walter, Aaron Barrett, Kimberly Tam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21609">https://arxiv.org/abs/2505.21609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21609">https://arxiv.org/pdf/2505.21609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21609]] Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study(https://arxiv.org/abs/2505.21609)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Adversarial artificial intelligence (AI) attacks pose a significant threat to autonomous transportation, such as maritime vessels, that rely on AI components. Malicious actors can exploit these systems to deceive and manipulate AI-driven operations. This paper addresses three critical research challenges associated with adversarial AI: the limited scope of traditional defences, inadequate security metrics, and the need to build resilience beyond model-level defences. To address these challenges, we propose building defences utilising multiple inputs and data fusion to create defensive components and an AI security metric as a novel approach toward developing more secure AI systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method, and we evaluate it through real-world demonstrations and comprehensive quantitative analyses, comparing a system built with the DFCR method against single-input models and models utilising existing state-of-the-art defences. The findings show that the DFCR approach significantly enhances resilience against adversarial machine learning attacks in maritime autonomous system operations, achieving up to a 35\% reduction in loss for successful multi-pronged perturbation attacks, up to a 100\% reduction in loss for successful adversarial patch attacks and up to 100\% reduction in loss for successful spoofing attacks when using these more resilient systems. We demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI contact confidence and improve decision-making by the system, even when typical adversarial defences have been compromised. Ultimately, this work contributes to the development of more secure and resilient AI-driven systems against adversarial attacks.</li>
</ul>

<h3>Title: VideoMarkBench: Benchmarking Robustness of Video Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21620">https://arxiv.org/abs/2505.21620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21620">https://arxiv.org/pdf/2505.21620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21620]] VideoMarkBench: Benchmarking Robustness of Video Watermarking(https://arxiv.org/abs/2505.21620)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>The rapid development of video generative models has led to a surge in highly realistic synthetic videos, raising ethical concerns related to disinformation and copyright infringement. Recently, video watermarking has been proposed as a mitigation strategy by embedding invisible marks into AI-generated videos to enable subsequent detection. However, the robustness of existing video watermarking methods against both common and adversarial perturbations remains underexplored. In this work, we introduce VideoMarkBench, the first systematic benchmark designed to evaluate the robustness of video watermarks under watermark removal and watermark forgery attacks. Our study encompasses a unified dataset generated by three state-of-the-art video generative models, across three video styles, incorporating four watermarking methods and seven aggregation strategies used during detection. We comprehensively evaluate 12 types of perturbations under white-box, black-box, and no-box threat models. Our findings reveal significant vulnerabilities in current watermarking approaches and highlight the urgent need for more robust solutions. Our code is available at this https URL.</li>
</ul>

<h3>Title: Learning Where to Learn: Training Distribution Selection for Provable OOD Performance</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Guerra, Nicholas H. Nelsen, Yunan Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21626">https://arxiv.org/abs/2505.21626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21626">https://arxiv.org/pdf/2505.21626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21626]] Learning Where to Learn: Training Distribution Selection for Provable OOD Performance(https://arxiv.org/abs/2505.21626)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) generalization remains a fundamental challenge in machine learning. Models trained on one data distribution often experience substantial performance degradation when evaluated on shifted or unseen domains. To address this challenge, the present paper studies the design of training data distributions that maximize average-case OOD performance. First, a theoretical analysis establishes a family of generalization bounds that quantify how the choice of training distribution influences OOD error across a predefined family of target distributions. These insights motivate the introduction of two complementary algorithmic strategies: (i) directly formulating OOD risk minimization as a bilevel optimization problem over the space of probability measures and (ii) minimizing a theoretical upper bound on OOD error. Last, the paper evaluates the two approaches across a range of function approximation and operator learning examples. The proposed methods significantly improve OOD accuracy over standard empirical risk minimization with a fixed distribution. These results highlight the potential of distribution-aware training as a principled and practical framework for robust OOD generalization.</li>
</ul>

<h3>Title: The Feasibility of Topic-Based Watermarking on Academic Peer Reviews</h3>
<ul>
<li><strong>Authors: </strong>Alexander Nemecek, Yuzhou Jiang, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21636">https://arxiv.org/abs/2505.21636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21636">https://arxiv.org/pdf/2505.21636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21636]] The Feasibility of Topic-Based Watermarking on Academic Peer Reviews(https://arxiv.org/abs/2505.21636)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly integrated into academic workflows, with many conferences and journals permitting their use for tasks such as language refinement and literature summarization. However, their use in peer review remains prohibited due to concerns around confidentiality breaches, hallucinated content, and inconsistent evaluations. As LLM-generated text becomes more indistinguishable from human writing, there is a growing need for reliable attribution mechanisms to preserve the integrity of the review process. In this work, we evaluate topic-based watermarking (TBW), a lightweight, semantic-aware technique designed to embed detectable signals into LLM-generated text. We conduct a comprehensive assessment across multiple LLM configurations, including base, few-shot, and fine-tuned variants, using authentic peer review data from academic conferences. Our results show that TBW maintains review quality relative to non-watermarked outputs, while demonstrating strong robustness to paraphrasing-based evasion. These findings highlight the viability of TBW as a minimally intrusive and practical solution for enforcing LLM usage in peer review.</li>
</ul>

<h3>Title: Efficient Diffusion Models for Symmetric Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Oren Mangoubi, Neil He, Nisheeth K. Vishnoi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DS, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21640">https://arxiv.org/abs/2505.21640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21640">https://arxiv.org/pdf/2505.21640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21640]] Efficient Diffusion Models for Symmetric Manifolds(https://arxiv.org/abs/2505.21640)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an "average-case" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.</li>
</ul>

<h3>Title: PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects</h3>
<ul>
<li><strong>Authors: </strong>Maresa Schröder, Justin Hartenstein, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21641">https://arxiv.org/abs/2505.21641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21641">https://arxiv.org/pdf/2505.21641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21641]] PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects(https://arxiv.org/abs/2505.21641)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating a differentially private ATE through output perturbation; (ii) estimating the differentially private variance through a truncated output perturbation mechanism; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.</li>
</ul>

<h3>Title: Reproducible Builds and Insights from an Independent Verifier for Arch Linux</h3>
<ul>
<li><strong>Authors: </strong>Joshua Drexel, Esther Hänggi, Iyán Méndez Veiga</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21642">https://arxiv.org/abs/2505.21642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21642">https://arxiv.org/pdf/2505.21642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21642]] Reproducible Builds and Insights from an Independent Verifier for Arch Linux(https://arxiv.org/abs/2505.21642)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Supply chain attacks have emerged as a prominent cybersecurity threat in recent years. Reproducible and bootstrappable builds have the potential to reduce such attacks significantly. In combination with independent, exhaustive and periodic source code audits, these measures can effectively eradicate compromises in the building process. In this paper we introduce both concepts, we analyze the achievements over the last ten years and explain the remaining challenges. We contribute to the reproducible builds effort by setting up a rebuilder and verifier instance to test the reproducibility of Arch Linux packages. Using the results from this instance, we uncover an unnoticed and security-relevant packaging issue affecting 16 packages related to Certbot, the recommended software to install TLS certificates from Let's Encrypt, making them unreproducible. Additionally, we find the root cause of unreproduciblity in the source code of fwupd, a critical software used to update device firmware on Linux devices, and submit an upstream patch to fix it.</li>
</ul>

<h3>Title: Geometric Feature Prompting of Image Segmentation Models</h3>
<ul>
<li><strong>Authors: </strong>Kenneth Ball, Erin Taylor, Nirav Patel, Andrew Bartels, Gary Koplik, James Polly, Jay Hineman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21644">https://arxiv.org/abs/2505.21644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21644">https://arxiv.org/pdf/2505.21644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21644]] Geometric Feature Prompting of Image Segmentation Models(https://arxiv.org/abs/2505.21644)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Advances in machine learning, especially the introduction of transformer architectures and vision transformers, have led to the development of highly capable computer vision foundation models. The segment anything model (known colloquially as SAM and more recently SAM 2), is a highly capable foundation model for segmentation of natural images and has been further applied to medical and scientific image segmentation tasks. SAM relies on prompts -- points or regions of interest in an image -- to generate associated segmentations. In this manuscript we propose the use of a geometrically motivated prompt generator to produce prompt points that are colocated with particular features of interest. Focused prompting enables the automatic generation of sensitive and specific segmentations in a scientific image analysis task using SAM with relatively few point prompts. The image analysis task examined is the segmentation of plant roots in rhizotron or minirhizotron images, which has historically been a difficult task to automate. Hand annotation of rhizotron images is laborious and often subjective; SAM, initialized with GeomPrompt local ridge prompts has the potential to dramatically improve rhizotron image processing. The authors have concurrently released an open source software suite called geomprompt this https URL that can produce point prompts in a format that enables direct integration with the segment-anything package.</li>
</ul>

<h3>Title: Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ke Zhang, Cihan Xiao, Yiqun Mei, Jiacong Xu, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21653">https://arxiv.org/abs/2505.21653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21653">https://arxiv.org/pdf/2505.21653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21653]] Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation(https://arxiv.org/abs/2505.21653)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent video diffusion models have demonstrated their great capability in generating visually-pleasing results, while synthesizing the correct physical effects in generated videos remains challenging. The complexity of real-world motions, interactions, and dynamics introduce great difficulties when learning physics from data. In this work, we propose DiffPhy, a generic framework that enables physically-correct and photo-realistic video generation by fine-tuning a pre-trained video diffusion model. Our method leverages large language models (LLMs) to explicitly reason a comprehensive physical context from the text prompt and use it to guide the generation. To incorporate physical context into the diffusion model, we leverage a Multimodal large language model (MLLM) as a supervisory signal and introduce a set of novel training objectives that jointly enforce physical correctness and semantic consistency with the input text. We also establish a high-quality physical video dataset containing diverse phyiscal actions and events to facilitate effective finetuning. Extensive experiments on public benchmarks demonstrate that DiffPhy is able to produce state-of-the-art results across diverse physics-related scenarios. Our project page is available at this https URL</li>
</ul>

<h3>Title: Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations</h3>
<ul>
<li><strong>Authors: </strong>Zeinab Dehghani, Koorosh Aslansefat, Adil Khan, Mohammed Naveed Akram</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21657">https://arxiv.org/abs/2505.21657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21657">https://arxiv.org/pdf/2505.21657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21657]] Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations(https://arxiv.org/abs/2505.21657)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.</li>
</ul>

<h3>Title: PreGenie: An Agentic Framework for High-quality Visual Presentation Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaojie Xu, Xinli Xu, Sirui Chen, Haoyu Chen, Fan Zhang, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21660">https://arxiv.org/abs/2505.21660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21660">https://arxiv.org/pdf/2505.21660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21660]] PreGenie: An Agentic Framework for High-quality Visual Presentation Generation(https://arxiv.org/abs/2505.21660)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual presentations are vital for effective communication. Early attempts to automate their creation using deep learning often faced issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding, leading to mismatched visuals and text. These limitations restrict their application in formal contexts like business and scientific research. To address these challenges, we propose PreGenie, an agentic and modular framework powered by multimodal large language models (MLLMs) for generating high-quality visual presentations. PreGenie is built on the Slidev presentation framework, where slides are rendered from Markdown code. It operates in two stages: (1) Analysis and Initial Generation, which summarizes multimodal input and generates initial code, and (2) Review and Re-generation, which iteratively reviews intermediate code and rendered slides to produce final, high-quality presentations. Each stage leverages multiple MLLMs that collaborate and share information. Comprehensive experiments demonstrate that PreGenie excels in multimodal understanding, outperforming existing models in both aesthetics and content consistency, while aligning more closely with human design preferences.</li>
</ul>

<h3>Title: Efficient Controllable Diffusion via Optimal Classifier Guidance</h3>
<ul>
<li><strong>Authors: </strong>Owen Oertell, Shikun Sun, Yiding Chen, Jin Peng Zhou, Zhiyong Wang, Wen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21666">https://arxiv.org/abs/2505.21666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21666">https://arxiv.org/pdf/2505.21666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21666]] Efficient Controllable Diffusion via Optimal Classifier Guidance(https://arxiv.org/abs/2505.21666)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available at this https URL</li>
</ul>

<h3>Title: Rethinking the Outlier Distribution in Large Language Models: An In-depth Study</h3>
<ul>
<li><strong>Authors: </strong>Rahul Raman, Khushi Sharma, Sai Qian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21670">https://arxiv.org/abs/2505.21670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21670">https://arxiv.org/pdf/2505.21670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21670]] Rethinking the Outlier Distribution in Large Language Models: An In-depth Study(https://arxiv.org/abs/2505.21670)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Investigating outliers in large language models (LLMs) is crucial due to their significant impact on various aspects of LLM performance, including quantization and compression. Outliers often cause considerable quantization errors, leading to degraded model performance. Identifying and addressing these outliers can enhance the accuracy and efficiency of the quantization process, enabling smoother deployment on edge devices or specialized hardware. Recent studies have identified two common types of outliers in LLMs: massive activations and channel-wise outliers. While numerous quantization algorithms have been proposed to mitigate their effects and maintain satisfactory accuracy, few have thoroughly explored the root causes of these outliers in depth. In this paper, we conduct a comprehensive investigation into the formation mechanisms of these outliers and propose potential strategies to mitigate their occurrence. Ultimately, we introduce some efficient approaches to eliminate most massive activations and channel-wise outliers with minimal impact on accuracy.</li>
</ul>

<h3>Title: What happens when generative AI models train recursively on each others' generated outputs?</h3>
<ul>
<li><strong>Authors: </strong>Hung Ahn Vu, Galen Reeves, Emily Wenger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21677">https://arxiv.org/abs/2505.21677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21677">https://arxiv.org/pdf/2505.21677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21677]] What happens when generative AI models train recursively on each others' generated outputs?(https://arxiv.org/abs/2505.21677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The internet is full of AI-generated content while also serving as a common source of training data for generative AI (genAI) models. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding downstream effects of such data-mediated model interactions is critical. To this end, we provide empirical evidence for how data-mediated interactions might unfold in practice, develop a theoretical model for this interactive training process, and show experimentally possible long-term results of such interactions. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks.</li>
</ul>

<h3>Title: multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data</h3>
<ul>
<li><strong>Authors: </strong>Andrew J. Loza, Jun Yup Kim, Shangzheng Song, Yihang Liu, Joseph J. Y. Sung, R Andrew Taylor, Dennis L. Shung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21680">https://arxiv.org/abs/2505.21680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21680">https://arxiv.org/pdf/2505.21680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21680]] multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data(https://arxiv.org/abs/2505.21680)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural ordinary differential equations are not well suited for categorical data or informative sampling and require augmentation to handle certain classes of trajectories. Here, we present multivariateGPT, a single architecture for modeling sequences of mixed categorical (including tokenized text) and numeric data. This is accomplished with an autoregressive sequence decomposition, embedding scheme, and loss function that extend the next token prediction task to likelihood estimation of the joint distribution of next token class and value. We demonstrate how this approach can efficiently learn to generalize patterns in simple physical systems and model complex time series including electrocardiograms and multivariate electronic health record data. This work extends the utility of transformer based models to additional classes of data.</li>
</ul>

<h3>Title: LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model</h3>
<ul>
<li><strong>Authors: </strong>Avijit Gayen, Somyajit Chakraborty, Mainak Sen, Soham Paul, Angshuman Jana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21689">https://arxiv.org/abs/2505.21689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21689">https://arxiv.org/pdf/2505.21689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21689]] LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model(https://arxiv.org/abs/2505.21689)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The persistent accumulation of unresolved legal cases, especially within the Indian judiciary, significantly hampers the timely delivery of justice. Manual methods of prioritizing petitions are often prone to inefficiencies and subjective biases further exacerbating delays. To address this issue, we propose LLMPR (Large Language Model-based Petition Ranking), an automated framework that utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency. Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process unstructured legal text and extract features through various embedding techniques, including DistilBERT, LegalBERT, and MiniLM. These textual embeddings are combined with quantitative indicators such as gap days, rank scores, and word counts to train multiple machine learning models, including Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments demonstrate that Random Forest and Decision Tree models yield superior performance, with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Notably, models using only numerical features achieve nearly optimal ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer only marginal gains. These findings suggest that automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization.</li>
</ul>

<h3>Title: MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Raoyuan Zhao, Beiduo Chen, Barbara Plank, Michael A. Hedderich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21693">https://arxiv.org/abs/2505.21693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21693">https://arxiv.org/pdf/2505.21693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21693]] MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs(https://arxiv.org/abs/2505.21693)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are used globally across many languages, but their English-centric pretraining raises concerns about cross-lingual disparities for cultural awareness, often resulting in biased outputs. However, comprehensive multilingual evaluation remains challenging due to limited benchmarks and questionable translation quality. To better assess these disparities, we introduce MAKIEval, an automatic multilingual framework for evaluating cultural awareness in LLMs across languages, regions, and topics. MAKIEval evaluates open-ended text generation, capturing how models express culturally grounded knowledge in natural language. Leveraging Wikidata's multilingual structure as a cross-lingual anchor, it automatically identifies cultural entities in model outputs and links them to structured knowledge, enabling scalable, language-agnostic evaluation without manual annotation or translation. We then introduce four metrics that capture complementary dimensions of cultural awareness: granularity, diversity, cultural specificity, and consensus across languages. We assess 7 LLMs developed from different parts of the world, encompassing both open-source and proprietary systems, across 13 languages, 19 countries and regions, and 6 culturally salient topics (e.g., food, clothing). Notably, we find that models tend to exhibit stronger cultural awareness in English, suggesting that English prompts more effectively activate culturally grounded knowledge. We publicly release our code and data.</li>
</ul>

<h3>Title: AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ganglou Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21695">https://arxiv.org/abs/2505.21695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21695">https://arxiv.org/pdf/2505.21695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21695]] AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling(https://arxiv.org/abs/2505.21695)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning faces critical challenges in balancing communication efficiency and model accuracy. One key issue lies in the approximation of update errors without incurring high computational costs. In this paper, we propose a lightweight yet effective method called Gradient Difference Approximation (GDA), which leverages first-order information to estimate local error trends without computing the full Hessian matrix. The proposed method forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework and provides a unified error modeling strategy for large-scale multi-step adaptive training environments.</li>
</ul>

<h3>Title: Scalable Segmentation for Ultra-High-Resolution Brain MR Images</h3>
<ul>
<li><strong>Authors: </strong>Xiaoling Hu, Peirong Liu, Dina Zemlyanker, Jonathan Williams Ramirez, Oula Puonti, Juan Eugenio Iglesias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21697">https://arxiv.org/abs/2505.21697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21697">https://arxiv.org/pdf/2505.21697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21697]] Scalable Segmentation for Ultra-High-Resolution Brain MR Images(https://arxiv.org/abs/2505.21697)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Although deep learning has shown great success in 3D brain MRI segmentation, achieving accurate and efficient segmentation of ultra-high-resolution brain images remains challenging due to the lack of labeled training data for fine-scale anatomical structures and high computational demands. In this work, we propose a novel framework that leverages easily accessible, low-resolution coarse labels as spatial references and guidance, without incurring additional annotation cost. Instead of directly predicting discrete segmentation maps, our approach regresses per-class signed distance transform maps, enabling smooth, boundary-aware supervision. Furthermore, to enhance scalability, generalizability, and efficiency, we introduce a scalable class-conditional segmentation strategy, where the model learns to segment one class at a time conditioned on a class-specific input. This novel design not only reduces memory consumption during both training and testing, but also allows the model to generalize to unseen anatomical classes. We validate our method through comprehensive experiments on both synthetic and real-world datasets, demonstrating its superior performance and scalability compared to conventional segmentation approaches.</li>
</ul>

<h3>Title: Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing</h3>
<ul>
<li><strong>Authors: </strong>Raoyuan Zhao, Abdullatif Köksal, Ali Modarressi, Michael A. Hedderich, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21701">https://arxiv.org/abs/2505.21701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21701">https://arxiv.org/pdf/2505.21701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21701]] Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing(https://arxiv.org/abs/2505.21701)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The reliability of large language models (LLMs) is greatly compromised by their tendency to hallucinate, underscoring the need for precise identification of knowledge gaps within LLMs. Various methods for probing such gaps exist, ranging from calibration-based to prompting-based methods. To evaluate these probing methods, in this paper, we propose a new process based on using input variations and quantitative metrics. Through this, we expose two dimensions of inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal non-semantic perturbations in prompts lead to considerable variance in detected knowledge gaps within the same probing method; e.g., the simple variation of shuffling answer options can decrease agreement to around 40%. (2) Cross-method inconsistency: Probing methods contradict each other on whether a model knows the answer. Methods are highly inconsistent -- with decision consistency across methods being as low as 7% -- even though the model, dataset, and prompt are all the same. These findings challenge existing probing methods and highlight the urgent need for perturbation-robust probing frameworks.</li>
</ul>

<h3>Title: A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks</h3>
<ul>
<li><strong>Authors: </strong>Julia Boone, Tolunay Seyfi, Fatemeh Afghah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21703">https://arxiv.org/abs/2505.21703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21703">https://arxiv.org/pdf/2505.21703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21703]] A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks(https://arxiv.org/abs/2505.21703)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Internet of Vehicles (IoV) systems, while offering significant advancements in transportation efficiency and safety, introduce substantial security vulnerabilities due to their highly interconnected nature. These dynamic systems produce massive amounts of data between vehicles, infrastructure, and cloud services and present a highly distributed framework with a wide attack surface. In considering network-centered attacks on IoV systems, attacks such as Denial-of-Service (DoS) can prohibit the communication of essential physical traffic safety information between system elements, illustrating that the security concerns for these systems go beyond the traditional confidentiality, integrity, and availability concerns of enterprise systems. Given the complexity and volume of data generated by IoV systems, traditional security mechanisms are often inadequate for accurately detecting sophisticated and evolving cyberattacks. Here, we present an unsupervised autoencoder method trained entirely on benign network data for the purpose of unseen attack detection in IoV networks. We leverage a weighted combination of reconstruction and triplet margin loss to guide the autoencoder training and develop a diverse representation of the benign training set. We conduct extensive experiments on recent network intrusion datasets from two different application domains, industrial IoT and home IoT, that represent the modern IoV task. We show that our method performs robustly for all unseen attack types, with roughly 99% accuracy on benign data and between 97% and 100% performance on anomaly data. We extend these results to show that our model is adaptable through the use of transfer learning, achieving similarly high results while leveraging domain features from one domain to another.</li>
</ul>

<h3>Title: Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mónika Farsang, Ramin Hasani, Radu Grosu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21717">https://arxiv.org/abs/2505.21717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21717">https://arxiv.org/pdf/2505.21717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21717]] Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling(https://arxiv.org/abs/2505.21717)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present LrcSSM, a \textit{nonlinear} recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the state-transition matrix to be diagonal and learned at every step, the full sequence can be solved in parallel with a single prefix-scan, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth $L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its low sequential depth and parameter count $\Theta(D\,L)$, the model follows the compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for Mamba, outperforming quadratic-attention Transformers at equal compute while avoiding the memory overhead of FFT-based long convolutions. We show that on a series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.</li>
</ul>

<h3>Title: OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions</h3>
<ul>
<li><strong>Authors: </strong>Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21724">https://arxiv.org/abs/2505.21724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21724">https://arxiv.org/pdf/2505.21724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21724]] OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions(https://arxiv.org/abs/2505.21724)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality.</li>
</ul>

<h3>Title: Lazarus Group Targets Crypto-Wallets and Financial Data while employing new Tradecrafts</h3>
<ul>
<li><strong>Authors: </strong>Alessio Di Santo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21725">https://arxiv.org/abs/2505.21725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21725">https://arxiv.org/pdf/2505.21725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21725]] Lazarus Group Targets Crypto-Wallets and Financial Data while employing new Tradecrafts(https://arxiv.org/abs/2505.21725)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>This report presents a comprehensive analysis of a malicious software sample, detailing its architecture, behavioral characteristics, and underlying intent. Through static and dynamic examination, the malware core functionalities, including persistence mechanisms, command-and-control communication, and data exfiltration routines, are identified and its supporting infrastructure is mapped. By correlating observed indicators of compromise with known techniques, tactics, and procedures, this analysis situates the sample within the broader context of contemporary threat campaigns and infers the capabilities and motivations of its likely threat actor. Building on these findings, actionable threat intelligence is provided to support proactive defenses. Threat hunting teams receive precise detection hypotheses for uncovering latent adversarial presence, while monitoring systems can refine alert logic to detect anomalous activity in real time. Finally, the report discusses how this structured intelligence enhances predictive risk assessments, informs vulnerability prioritization, and strengthens organizational resilience against advanced persistent threats. By integrating detailed technical insights with strategic threat landscape mapping, this malware analysis report not only reconstructs past adversary actions but also establishes a robust foundation for anticipating and mitigating future attacks.</li>
</ul>

<h3>Title: Deep Reinforcement Learning Agents are not even close to Human Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Quentin Delfosse, Jannis Blüml, Fabian Tatai, Théo Vincent, Bjarne Gregori, Elisabeth Dillies, Jan Peters, Constantin Rothkopf, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21731">https://arxiv.org/abs/2505.21731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21731">https://arxiv.org/pdf/2505.21731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21731]] Deep Reinforcement Learning Agents are not even close to Human Intelligence(https://arxiv.org/abs/2505.21731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence.</li>
</ul>

<h3>Title: Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks</h3>
<ul>
<li><strong>Authors: </strong>Zachary Schlamowitz, Andrew Bennecke, Daniel J. Tward</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21736">https://arxiv.org/abs/2505.21736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21736">https://arxiv.org/pdf/2505.21736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21736]] Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks(https://arxiv.org/abs/2505.21736)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The principle of translation equivariance (if an input image is translated an output image should be translated by the same amount), led to the development of convolutional neural networks that revolutionized machine vision. Other symmetries, like rotations and reflections, play a similarly critical role, especially in biomedical image analysis, but exploiting these symmetries has not seen wide adoption. We hypothesize that this is partially due to the mathematical complexity of methods used to exploit these symmetries, which often rely on representation theory, a bespoke concept in differential geometry and group theory. In this work, we show that the same equivariance can be achieved using a simple form of convolution kernels that we call ``moment kernels,'' and prove that all equivariant kernels must take this form. These are a set of radially symmetric functions of a spatial position $x$, multiplied by powers of the components of $x$ or the identity matrix. We implement equivariant neural networks using standard convolution modules, and provide architectures to execute several biomedical image analysis tasks that depend on equivariance principles: classification (outputs are invariant under orthogonal transforms), 3D image registration (outputs transform like a vector), and cell segmentation (quadratic forms defining ellipses transform like a matrix).</li>
</ul>

<h3>Title: What is Adversarial Training for Diffusion Models?</h3>
<ul>
<li><strong>Authors: </strong>Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, Iacopo Masi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21742">https://arxiv.org/abs/2505.21742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21742">https://arxiv.org/pdf/2505.21742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21742]] What is Adversarial Training for Diffusion Models?(https://arxiv.org/abs/2505.21742)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>We answer the question in the title, showing that adversarial training (AT) for diffusion models (DMs) fundamentally differs from classifiers: while AT in classifiers enforces output invariance, AT in DMs requires equivariance to keep the diffusion process aligned with the data distribution. AT is a way to enforce smoothness in the diffusion flow, improving robustness to outliers and corrupted data. Unlike prior art, our method makes no assumptions about the noise model and integrates seamlessly into diffusion training by adding random noise, similar to randomized smoothing, or adversarial noise, akin to AT. This enables intrinsic capabilities such as handling noisy data, dealing with extreme variability such as outliers, preventing memorization, and improving robustness. We rigorously evaluate our approach with proof-of-concept datasets with known distributions in low- and high-dimensional space, thereby taking a perfect measure of errors; we further evaluate on standard benchmarks such as CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe noise, data corruption, and iterative adversarial attacks.</li>
</ul>

<h3>Title: Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen</h3>
<ul>
<li><strong>Authors: </strong>Zihao Li, Xinyuan Cao, Xiangbo Gao, Kexin Tian, Keshu Wu, Mohammad Anis, Hao Zhang, Keke Long, Jiwan Jiang, Xiaopeng Li, Yunlong Zhang, Tianbao Yang, Dominique Lord, Zhengzhong Tu, Yang Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21743">https://arxiv.org/abs/2505.21743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21743">https://arxiv.org/pdf/2505.21743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21743]] Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen(https://arxiv.org/abs/2505.21743)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero.</li>
</ul>

<h3>Title: Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals</h3>
<ul>
<li><strong>Authors: </strong>Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21750">https://arxiv.org/abs/2505.21750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21750">https://arxiv.org/pdf/2505.21750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21750]] Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals(https://arxiv.org/abs/2505.21750)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hierarchical reinforcement learning (HRL) learns to make decisions on multiple levels of temporal abstraction. A key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals. To address this issue, the high-level policy must capture a complex subgoal distribution while also accounting for uncertainty in its estimates. We propose an approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Building on this framework, we develop a strategy that selects subgoals from both the diffusion policy and GP's predictive mean. Our approach outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.</li>
</ul>

<h3>Title: Visual Loop Closure Detection Through Deep Graph Consensus</h3>
<ul>
<li><strong>Authors: </strong>Martin Büchner, Liza Dahiya, Simon Dorer, Vipul Ramtekkar, Kenji Nishimiya, Daniele Cattaneo, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21754">https://arxiv.org/abs/2505.21754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21754">https://arxiv.org/pdf/2505.21754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21754]] Visual Loop Closure Detection Through Deep Graph Consensus(https://arxiv.org/abs/2505.21754)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual loop closure detection traditionally relies on place recognition methods to retrieve candidate loops that are validated using computationally expensive RANSAC-based geometric verification. As false positive loop closures significantly degrade downstream pose graph estimates, verifying a large number of candidates in online simultaneous localization and mapping scenarios is constrained by limited time and compute resources. While most deep loop closure detection approaches only operate on pairs of keyframes, we relax this constraint by considering neighborhoods of multiple keyframes when detecting loops. In this work, we introduce LoopGNN, a graph neural network architecture that estimates loop closure consensus by leveraging cliques of visually similar keyframes retrieved through place recognition. By propagating deep feature encodings among nodes of the clique, our method yields high-precision estimates while maintaining high recall. Extensive experimental evaluations on the TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms traditional baselines. Additionally, an ablation study across various keypoint extractors demonstrates that our method is robust, regardless of the type of deep feature encodings used, and exhibits higher computational efficiency compared to classical geometric verification baselines. We release our code, supplementary material, and keyframe data at this https URL.</li>
</ul>

<h3>Title: FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21755">https://arxiv.org/abs/2505.21755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21755">https://arxiv.org/pdf/2505.21755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21755]] FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering(https://arxiv.org/abs/2505.21755)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual question answering (VQA) systems face significant challenges when adapting to real-world data shifts, especially in multi-modal contexts. While robust fine-tuning strategies are essential for maintaining performance across in-distribution (ID) and out-of-distribution (OOD) scenarios, current evaluation settings are primarily unimodal or particular to some types of OOD, offering limited insight into the complexities of multi-modal contexts. In this work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA and others, and categorize them into ID, near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. We first conduct a comprehensive comparison of existing robust fine-tuning methods. We then quantify the distribution shifts by calculating the Mahalanobis distance using uni-modal and multi-modal embeddings extracted from various models. Further, we perform an extensive analysis to explore the interactions between uni- and multi-modal shifts as well as modality importance for ID and OOD samples. These analyses offer valuable guidance on developing more robust fine-tuning methods to handle multi-modal distribution shifts. The code is available at this https URL .</li>
</ul>

<h3>Title: BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum</h3>
<ul>
<li><strong>Authors: </strong>Yubin Kim, Zhiyuan Hu, Hyewon Jeong, Eugene Park, Shuyue Stella Li, Chanwoo Park, Shiyun Xiong, MingYu Lu, Hyeonhoon Lee, Xin Liu, Daniel McDuff, Cynthia Breazeal, Samir Tulebaev, Hae Won Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21757">https://arxiv.org/abs/2505.21757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21757">https://arxiv.org/pdf/2505.21757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21757]] BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum(https://arxiv.org/abs/2505.21757)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) as clinical agents require careful behavioral adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs often struggle with proactive engagement, like unprompted identification of critical missing information or risks. We introduce BehaviorBench, a comprehensive dataset to evaluate agent behaviors across a clinical assistance spectrum, ranging from reactive query responses to proactive interventions (e.g., clarifying ambiguities, flagging overlooked critical data). Our BehaviorBench experiments reveal LLMs' inconsistent proactivity. To address this, we propose BehaviorSFT, a novel training strategy using behavioral tokens to explicitly condition LLMs for dynamic behavioral selection along this spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to 96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a superior balance between helpful proactivity (e.g., timely, relevant suggestions) and necessary restraint (e.g., avoiding over-intervention) versus standard fine-tuning or explicit instructed agents.</li>
</ul>

<h3>Title: MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Prasham Yatinkumar Titiya, Jainil Trivedi, Chitta Baral, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21771">https://arxiv.org/abs/2505.21771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21771">https://arxiv.org/pdf/2505.21771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21771]] MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning(https://arxiv.org/abs/2505.21771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal tables those that integrate semi structured data with visual elements such as charts and maps are ubiquitous across real world domains, yet they pose a formidable challenge to current vision language models (VLMs). While Large Language models (LLMs) and VLMs have demonstrated strong capabilities in text and image understanding, their performance on complex, real world multimodal table reasoning remains unexplored. To bridge this gap, we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of 500 real world multimodal tables drawn from diverse real world sources, with a total of 4021 question answer pairs. MMTBENCH questions cover four question types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning types (Mathematical, Extrema Identification, Fact Verification, Vision Based, and Others), and eight table types (Single/Multiple Entity, Maps and Charts with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive evaluation of state of the art models on all types reveals substantial performance gaps, particularly on questions requiring visual-based reasoning and multi-step inference. These findings show the urgent need for improved architectures that more tightly integrate vision and language processing. By providing a challenging, high-quality resource that mirrors the complexity of real-world tasks, MMTBENCH underscores its value as a resource for future research on multimodal tables.</li>
</ul>

<h3>Title: Calibrating LLM Confidence by Probing Perturbed Representation Stability</h3>
<ul>
<li><strong>Authors: </strong>Reza Khanmohammadi, Erfan Miahi, Mehrsa Mardikoraem, Simerjot Kaur, Ivan Brugere, Charese H. Smiley, Kundan Thind, Mohammad M. Ghassemi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21772">https://arxiv.org/abs/2505.21772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21772">https://arxiv.org/pdf/2505.21772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21772]] Calibrating LLM Confidence by Probing Perturbed Representation Stability(https://arxiv.org/abs/2505.21772)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Miscalibration in Large Language Models (LLMs) undermines their reliability, highlighting the need for accurate confidence estimation. We introduce CCPS (Calibrating LLM Confidence by Probing Perturbed Representation Stability), a novel method analyzing internal representational stability in LLMs. CCPS applies targeted adversarial perturbations to final hidden states, extracts features reflecting the model's response to these perturbations, and uses a lightweight classifier to predict answer correctness. CCPS was evaluated on LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and open-ended formats. Our results show that CCPS significantly outperforms current approaches. Across four LLMs and three MMLU variants, CCPS reduces Expected Calibration Error by approximately 55% and Brier score by 21%, while increasing accuracy by 5 percentage points, Area Under the Precision-Recall Curve by 4 percentage points, and Area Under the Receiver Operating Characteristic Curve by 6 percentage points, all relative to the strongest prior method. CCPS delivers an efficient, broadly applicable, and more accurate solution for estimating LLM confidence, thereby improving their trustworthiness.</li>
</ul>

<h3>Title: DualSchool: How Reliable are LLMs for Optimization Education?</h3>
<ul>
<li><strong>Authors: </strong>Michael Klamkin, Arnaud Deza, Sikai Cheng, Haoruo Zhao, Pascal Van Hentenryck</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21775">https://arxiv.org/abs/2505.21775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21775">https://arxiv.org/pdf/2505.21775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21775]] DualSchool: How Reliable are LLMs for Optimization Education?(https://arxiv.org/abs/2505.21775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems.</li>
</ul>

<h3>Title: Memorization to Generalization: Emergence of Diffusion Models from Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21777">https://arxiv.org/abs/2505.21777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21777">https://arxiv.org/pdf/2505.21777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21777]] Memorization to Generalization: Emergence of Diffusion Models from Associative Memory(https://arxiv.org/abs/2505.21777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\,\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.</li>
</ul>

<h3>Title: Compositional Scene Understanding through Inverse Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yanbo Wang, Justin Dauwels, Yilun Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21780">https://arxiv.org/abs/2505.21780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21780">https://arxiv.org/pdf/2505.21780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21780]] Compositional Scene Understanding through Inverse Generative Modeling(https://arxiv.org/abs/2505.21780)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative models have demonstrated remarkable abilities in generating high-fidelity visual content. In this work, we explore how generative models can further be used not only to synthesize visual content but also to understand the properties of a scene given a natural image. We formulate scene understanding as an inverse generative modeling problem, where we seek to find conditional parameters of a visual generative model to best fit a given natural image. To enable this procedure to infer scene structure from images substantially different than those seen during training, we further propose to build this visual generative model compositionally from smaller models over pieces of a scene. We illustrate how this procedure enables us to infer the set of objects in a scene, enabling robust generalization to new test scenes with an increased number of objects of new shapes. We further illustrate how this enables us to infer global scene factors, likewise enabling robust generalization to new scenes. Finally, we illustrate how this approach can be directly applied to existing pretrained text-to-image generative models for zero-shot multi-object perception. Code and visualizations are at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Born a Transformer -- Always a Transformer?</h3>
<ul>
<li><strong>Authors: </strong>Yana Veitsman, Mayank Jobanputra, Yash Sarrof, Aleksandra Bakalova, Vera Demberg, Ellie Pavlick, Michael Hahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21785">https://arxiv.org/abs/2505.21785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21785">https://arxiv.org/pdf/2505.21785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21785]] Born a Transformer -- Always a Transformer?(https://arxiv.org/abs/2505.21785)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining, by studying a family of $\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al. [2024]. We use the recently proposed C-RASP framework for studying length generalization [Huang et al., 2025b] to provide guarantees for each of our settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$ asymmetry, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained Transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain Transformer capabilities, but does not overcome fundamental length-generalization limits.</li>
</ul>

<h3>Title: VeriTrail: Closed-Domain Hallucination Detection with Traceability</h3>
<ul>
<li><strong>Authors: </strong>Dasha Metropolitansky, Jonathan Larson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21786">https://arxiv.org/abs/2505.21786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21786">https://arxiv.org/pdf/2505.21786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21786]] VeriTrail: Closed-Domain Hallucination Detection with Traceability(https://arxiv.org/abs/2505.21786)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Even when instructed to adhere to source material, Language Models often generate unsubstantiated content - a phenomenon known as "closed-domain hallucination." This risk is amplified in processes with multiple generative steps (MGS), compared to processes with a single generative step (SGS). However, due to the greater complexity of MGS processes, we argue that detecting hallucinations in their final outputs is necessary but not sufficient: it is equally important to trace where hallucinated content was likely introduced and how faithful content may have been derived from the source through intermediate outputs. To address this need, we present VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for both MGS and SGS processes. We also introduce the first datasets to include all intermediate outputs as well as human annotations of final outputs' faithfulness for their respective MGS processes. We demonstrate that VeriTrail outperforms baseline methods on both datasets.</li>
</ul>

<h3>Title: Faster Rates for Private Adversarial Bandits</h3>
<ul>
<li><strong>Authors: </strong>Hilal Asi, Vinod Raman, Kunal Talwar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21790">https://arxiv.org/abs/2505.21790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21790">https://arxiv.org/pdf/2505.21790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21790]] Faster Rates for Private Adversarial Bandits(https://arxiv.org/abs/2505.21790)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non-private bandit algorithms gives a regret upper bound of $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all $\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first known separation between central and local differential privacy for this problem. For bandits with expert advice, we give the first differentially private algorithms, with expected regret $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right), O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of actions and experts respectively. These rates allow us to get sublinear regret for different combinations of small and large $K, N$ and $\epsilon.$</li>
</ul>

<h3>Title: Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhe Peng, Jieming Bian, Lei Wang, Yin Huang, Jie Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21792">https://arxiv.org/abs/2505.21792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21792">https://arxiv.org/pdf/2505.21792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21792]] Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms(https://arxiv.org/abs/2505.21792)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Multimodal Federated Learning (MFL) lies at the intersection of two pivotal research areas: leveraging complementary information from multiple modalities to improve downstream inference performance and enabling distributed training to enhance efficiency and preserve privacy. Despite the growing interest in MFL, there is currently no comprehensive taxonomy that organizes MFL through the lens of different Federated Learning (FL) paradigms. This perspective is important because multimodal data introduces distinct challenges across various FL settings. These challenges, including modality heterogeneity, privacy heterogeneity, and communication inefficiency, are fundamentally different from those encountered in traditional unimodal or non-FL scenarios. In this paper, we systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we present the problem formulation, review representative training algorithms, and highlight the most prominent challenge introduced by multimodal data in distributed settings. We also discuss open challenges and provide insights for future research. By establishing this taxonomy, we aim to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms and to offer a new lens through which to understand and advance the development of MFL.</li>
</ul>

<h3>Title: SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21795">https://arxiv.org/abs/2505.21795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21795">https://arxiv.org/pdf/2505.21795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21795]] SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation(https://arxiv.org/abs/2505.21795)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at this https URL.</li>
</ul>

<h3>Title: From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, Clayton Lau, Cole Blondin, Sean O'Brien, Kevin Zhu, Vasu Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21800">https://arxiv.org/abs/2505.21800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21800">https://arxiv.org/pdf/2505.21800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21800]] From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs(https://arxiv.org/abs/2505.21800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.</li>
</ul>

<h3>Title: Towards Operational Automated Greenhouse Gas Plume Detection</h3>
<ul>
<li><strong>Authors: </strong>Brian D. Bue, Jake H. Lee, Andrew K. Thorpe, Philip G. Brodrick, Daniel Cusworth, Alana Ayasse, Vassiliki Mancoridis, Anagha Satish, Shujun Xiong, Riley Duren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21806">https://arxiv.org/abs/2505.21806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21806">https://arxiv.org/pdf/2505.21806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21806]] Towards Operational Automated Greenhouse Gas Plume Detection(https://arxiv.org/abs/2505.21806)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Operational deployment of a fully automated greenhouse gas (GHG) plume detection system remains an elusive goal for imaging spectroscopy missions, despite recent advances in deep learning approaches. With the dramatic increase in data availability, however, automation continues to increase in importance for natural and anthropogenic emissions monitoring. This work reviews and addresses several key obstacles in the field: data and label quality control, prevention of spatiotemporal biases, and correctly aligned modeling objectives. We demonstrate through rigorous experiments using multicampaign data from airborne and spaceborne instruments that convolutional neural networks (CNNs) are able to achieve operational detection performance when these obstacles are alleviated. We demonstrate that a multitask model that learns both instance detection and pixelwise segmentation simultaneously can successfully lead towards an operational pathway. We evaluate the model's plume detectability across emission source types and regions, identifying thresholds for operational deployment. Finally, we provide analysis-ready data, models, and source code for reproducibility, and work to define a set of best practices and validation standards to facilitate future contributions to the field.</li>
</ul>

<h3>Title: TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction</h3>
<ul>
<li><strong>Authors: </strong>Tommy Xu, Zhitian Zhang, Xiangyu Sun, Lauren Kelly Zung, Hossein Hajimirsadeghi, Greg Mori</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21807">https://arxiv.org/abs/2505.21807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21807">https://arxiv.org/pdf/2505.21807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21807]] TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction(https://arxiv.org/abs/2505.21807)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Predictive modeling on tabular data is the cornerstone of many real-world applications. Although gradient boosting machines and some recent deep models achieve strong performance on tabular data, they often lack interpretability. On the other hand, large language models (LLMs) have demonstrated powerful capabilities to generate human-like reasoning and explanations, but remain under-performed for tabular data prediction. In this paper, we propose a new approach that leverages reasoning-based LLMs, trained using reinforcement learning, to perform more accurate and explainable predictions on tabular data. Our method introduces custom reward functions that guide the model not only toward high prediction accuracy but also toward human-understandable reasons for its predictions. Experimental results show that our model achieves promising performance on financial benchmark datasets, outperforming most existing LLMs.</li>
</ul>

<h3>Title: Optimizing Data Augmentation through Bayesian Model Selection</h3>
<ul>
<li><strong>Authors: </strong>Madi Matymov (1), Ba-Hien Tran (2), Michael Kampffmeyer (3 and 4), Markus Heinonen (5), Maurizio Filippone (1) ((1) KAUST, (2) Huawei Paris Research Center, (3) UiT The Arctic University of Norway, (4) Norwegian Computing Center, (5) Aalto University)</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21813">https://arxiv.org/abs/2505.21813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21813">https://arxiv.org/pdf/2505.21813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21813]] Optimizing Data Augmentation through Bayesian Model Selection(https://arxiv.org/abs/2505.21813)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data Augmentation (DA) has become an essential tool to improve robustness and generalization of modern machine learning. However, when deciding on DA strategies it is critical to choose parameters carefully, and this can be a daunting task which is traditionally left to trial-and-error or expensive optimization based on validation performance. In this paper, we counter these limitations by proposing a novel framework for optimizing DA. In particular, we take a probabilistic view of DA, which leads to the interpretation of augmentation parameters as model (hyper)-parameters, and the optimization of the marginal likelihood with respect to these parameters as a Bayesian model selection problem. Due to its intractability, we derive a tractable Evidence Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly with model parameters. We provide extensive theoretical results on variational approximation quality, generalization guarantees, invariance properties, and connections to empirical Bayes. Through experiments on computer vision tasks, we show that our approach improves calibration and yields robust performance over fixed or no augmentation. Our work provides a rigorous foundation for optimizing DA through Bayesian principles with significant potential for robust machine learning.</li>
</ul>

<h3>Title: ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Yang, Lei Lu, Qihui Fan, Changdi Yang, Juyi Lin, Yanzhi Wang, Xuan Zhang, Shangqian Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21817">https://arxiv.org/abs/2505.21817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21817">https://arxiv.org/pdf/2505.21817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21817]] ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation(https://arxiv.org/abs/2505.21817)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional capabilities in generating high-fidelity images. However, their iterative denoising process results in significant computational overhead during inference, limiting their practical deployment in resource-constrained environments. Existing acceleration methods often adopt uniform strategies that fail to capture the temporal variations during diffusion generation, while the commonly adopted sequential pruning-then-fine-tuning strategy suffers from sub-optimality due to the misalignment between pruning decisions made on pretrained weights and the model's final parameters. To address these limitations, we introduce ALTER: All-in-One Layer Pruning and Temporal Expert Routing, a unified framework that transforms diffusion models into a mixture of efficient temporal experts. ALTER achieves a single-stage optimization that unifies layer pruning, expert routing, and model fine-tuning by employing a trainable hypernetwork, which dynamically generates layer pruning decisions and manages timestep routing to specialized, pruned expert sub-networks throughout the ongoing fine-tuning of the UNet. This unified co-optimization strategy enables significant efficiency gains while preserving high generative quality. Specifically, ALTER achieves same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model while utilizing only 25.9% of its total MACs with just 20 inference steps and delivering a 3.64x speedup through 35% sparsity.</li>
</ul>

<h3>Title: Representative Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Charlotte Peale, Vinod Raman, Omer Reingold</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21819">https://arxiv.org/abs/2505.21819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21819">https://arxiv.org/pdf/2505.21819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21819]] Representative Language Generation(https://arxiv.org/abs/2505.21819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce "representative generation," extending the theoretical framework for generation proposed by Kleinberg et al. (2024) and formalized by Li et al. (2024), to additionally address diversity and bias concerns in generative models. Our notion requires outputs of a generative model to proportionally represent groups of interest from the training data. We characterize representative uniform and non-uniform generation, introducing the "group closure dimension" as a key combinatorial quantity. For representative generation in the limit, we analyze both information-theoretic and computational aspects, demonstrating feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but proving a negative result for computability using only membership queries. This contrasts with Kleinberg et al.'s (2024) positive results for standard generation in the limit. Our findings provide a rigorous foundation for developing more diverse and representative generative models.</li>
</ul>

<h3>Title: Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones</h3>
<ul>
<li><strong>Authors: </strong>Parsa Mirtaheri, Ezra Edelman, Samy Jelassi, Eran Malach, Enric Boix-Adsera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21825">https://arxiv.org/abs/2505.21825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21825">https://arxiv.org/pdf/2505.21825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21825]] Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones(https://arxiv.org/abs/2505.21825)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.</li>
</ul>

<h3>Title: In Search of Adam's Secret Sauce</h3>
<ul>
<li><strong>Authors: </strong>Antonio Orvieto, Robert Gower</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21829">https://arxiv.org/abs/2505.21829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21829">https://arxiv.org/pdf/2505.21829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21829]] In Search of Adam's Secret Sauce(https://arxiv.org/abs/2505.21829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study - training over 1,300 language models across different data configurations and scales - comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal. Beyond robust performance, this choice affords new theoretical insights, highlights the "secret sauce" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients-one that arises from a mean-field Gaussian variational inference perspective.</li>
</ul>

<h3>Title: UniMoGen: Universal Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Aliasghar Khani, Arianna Rampini, Evan Atherton, Bruno Roy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21837">https://arxiv.org/abs/2505.21837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21837">https://arxiv.org/pdf/2505.21837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21837]] UniMoGen: Universal Motion Generation(https://arxiv.org/abs/2505.21837)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Motion generation is a cornerstone of computer graphics, animation, gaming, and robotics, enabling the creation of realistic and varied character movements. A significant limitation of existing methods is their reliance on specific skeletal structures, which restricts their versatility across different characters. To overcome this, we introduce UniMoGen, a novel UNet-based diffusion model designed for skeleton-agnostic motion generation. UniMoGen can be trained on motion data from diverse characters, such as humans and animals, without the need for a predefined maximum number of joints. By dynamically processing only the necessary joints for each character, our model achieves both skeleton agnosticism and computational efficiency. Key features of UniMoGen include controllability via style and trajectory inputs, and the ability to continue motions from past frames. We demonstrate UniMoGen's effectiveness on the 100style dataset, where it outperforms state-of-the-art methods in diverse character motion generation. Furthermore, when trained on both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen achieves high performance and improved efficiency across both skeletons. These results highlight UniMoGen's potential to advance motion generation by providing a flexible, efficient, and controllable solution for a wide range of character animations.</li>
</ul>

<h3>Title: An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, Honghao Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21841">https://arxiv.org/abs/2505.21841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21841">https://arxiv.org/pdf/2505.21841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21841]] An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints(https://arxiv.org/abs/2505.21841)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.</li>
</ul>

<h3>Title: Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21844">https://arxiv.org/abs/2505.21844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21844">https://arxiv.org/pdf/2505.21844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21844]] Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2505.21844)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, test-time adaptation has attracted wide interest in the context of vision-language models for image classification. However, to the best of our knowledge, the problem is completely overlooked in dense prediction tasks such as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a novel TTA method tailored to adapting VLMs for segmentation during test time. Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates features from intermediate vision-encoder layers and is performed with different text-prompt templates at both the global CLS token and local pixel-wise levels. Our approach could be used as plug-and-play for any segmentation network, does not require additional training data or labels, and remains effective even with a single test sample. Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which integrates a rigorous evaluation protocol, seven segmentation datasets, and 15 common corruptions, with a total of 82 distinct test scenarios, establishing a standardized and comprehensive testbed for future TTA research in open-vocabulary segmentation. Our experiments on this suite demonstrate that our segmentation-tailored method consistently delivers significant gains over direct adoption of TTA classification baselines.</li>
</ul>

<h3>Title: RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers</h3>
<ul>
<li><strong>Authors: </strong>Xuwei Xu, Yang Li, Yudong Chen, Jiajun Liu, Sen Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21847">https://arxiv.org/abs/2505.21847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21847">https://arxiv.org/pdf/2505.21847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21847]] RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers(https://arxiv.org/abs/2505.21847)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We reveal that feedforward network (FFN) layers, rather than attention layers, are the primary contributors to Vision Transformer (ViT) inference latency, with their impact signifying as model size increases. This finding highlights a critical opportunity for optimizing the efficiency of large-scale ViTs by focusing on FFN layers. In this work, we propose a novel channel idle mechanism that facilitates post-training structural reparameterization for efficient FFN layers during testing. Specifically, a set of feature channels remains idle and bypasses the nonlinear activation function in each FFN layer, thereby forming a linear pathway that enables structural reparameterization during inference. This mechanism results in a family of ReParameterizable Vision Transformers (RePaViTs), which achieve remarkable latency reductions with acceptable sacrifices (sometimes gains) in accuracy across various ViTs. The benefits of our method scale consistently with model sizes, demonstrating greater speed improvements and progressively narrowing accuracy gaps or even higher accuracies on larger models. In particular, RePa-ViT-Large and RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1 accuracies under the same training strategy, respectively. RePaViT is the first to employ structural reparameterization on FFN layers to expedite ViTs to our best knowledge, and we believe that it represents an auspicious direction for efficient ViTs. Source code is available at this https URL.</li>
</ul>

<h3>Title: FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Xu, Chenghao Li, Yuke Zhang, Peter A. Beerel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21848">https://arxiv.org/abs/2505.21848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21848">https://arxiv.org/pdf/2505.21848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21848]] FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings(https://arxiv.org/abs/2505.21848)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable potential in generating high-quality images. However, their tendency to replicate training data raises serious privacy concerns, particularly when the training datasets contain sensitive or private information. Existing mitigation strategies primarily focus on reducing image duplication, modifying the cross-attention mechanism, and altering the denoising backbone architecture of diffusion models. Moreover, recent work has shown that adding a consistent small amount of noise to text embeddings can reduce replication to some degree. In this work, we begin by analyzing the impact of adding varying amounts of noise. Based on our analysis, we propose a fine-grained noise injection technique that probabilistically adds a larger amount of noise to token embeddings. We refer to our method as Fine-grained Probabilistic Addition of Noise (FPAN). Through our extensive experiments, we show that our proposed FPAN can reduce replication by an average of 28.78% compared to the baseline diffusion model without significantly impacting image quality, and outperforms the prior consistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined with other existing mitigation methods, our FPAN approach can further reduce replication by up to 16.82% with similar, if not improved, image quality.</li>
</ul>

<h3>Title: Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task</h3>
<ul>
<li><strong>Authors: </strong>Yanbei Jiang, Yihao Ding, Chao Lei, Jiayang Ao, Jey Han Lau, Krista A. Ehinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21850">https://arxiv.org/abs/2505.21850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21850">https://arxiv.org/pdf/2505.21850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21850]] Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task(https://arxiv.org/abs/2505.21850)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current Multimodal Large Language Models (MLLMs) excel in general visual reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which demands higher-order reasoning to identify abstract rules beyond simple perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing the end result but neglecting the multi-stage nature of reasoning process. Past studies found MLLMs struggle with these benchmarks, but it doesn't explain how they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR benchmark, based on RAVEN, designed to assess reasoning across varying levels of complexity. Additionally, existing metrics like accuracy only focus on the final outcomes while do not account for the correctness of intermediate steps. Therefore, we propose a novel metric, MSEval, which considers the correctness of intermediate steps in addition to the final outcomes. We conduct comprehensive experiments on MultiStAR using 17 representative close-source and open-source MLLMs. The results reveal that while existing MLLMs perform adequately on basic perception tasks, they continue to face challenges in more complex rule detection stages.</li>
</ul>

<h3>Title: Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification</h3>
<ul>
<li><strong>Authors: </strong>Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, Chongshou Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21854">https://arxiv.org/abs/2505.21854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21854">https://arxiv.org/pdf/2505.21854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21854]] Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification(https://arxiv.org/abs/2505.21854)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Gradient-based adversarial attacks have become a dominant approach for evaluating the robustness of point cloud classification models. However, existing methods often rely on uniform update rules that fail to consider the heterogeneous nature of point clouds, resulting in excessive and perceptible perturbations. In this paper, we rethink the design of gradient-based attacks by analyzing the limitations of conventional gradient update mechanisms and propose two new strategies to improve both attack effectiveness and imperceptibility. First, we introduce WAAttack, a novel framework that incorporates weighted gradients and an adaptive step-size strategy to account for the non-uniform contribution of points during optimization. This approach enables more targeted and subtle perturbations by dynamically adjusting updates according to the local structure and sensitivity of each point. Second, we propose SubAttack, a complementary strategy that decomposes the point cloud into subsets and focuses perturbation efforts on structurally critical regions. Together, these methods represent a principled rethinking of gradient-based adversarial attacks for 3D point cloud classification. Extensive experiments demonstrate that our approach outperforms state-of-the-art baselines in generating highly imperceptible adversarial examples. Code will be released upon paper acceptance.</li>
</ul>

<h3>Title: Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries</h3>
<ul>
<li><strong>Authors: </strong>Vishakh Padmakumar, Zichao Wang, David Arbour, Jennifer Healey</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21859">https://arxiv.org/abs/2505.21859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21859">https://arxiv.org/pdf/2505.21859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21859]] Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries(https://arxiv.org/abs/2505.21859)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) are increasingly capable of handling longer contexts, recent work has demonstrated that they exhibit the "lost in the middle" phenomenon (Liu et al., 2024) of unevenly attending to different parts of the provided context. This hinders their ability to cover diverse source material in multi-document summarization, as noted in the DiverseSumm benchmark (Huang et al., 2024). In this work, we contend that principled content selection is a simple way to increase source coverage on this task. As opposed to prompting an LLM to perform the summarization in a single step, we explicitly divide the task into three steps -- (1) reducing document collections to atomic key points, (2) using determinantal point processes (DPP) to perform select key points that prioritize diverse content, and (3) rewriting to the final summary. By combining prompting steps, for extraction and rewriting, with principled techniques, for content selection, we consistently improve source coverage on the DiverseSumm benchmark across various LLMs. Finally, we also show that by incorporating relevance to a provided user intent into the DPP kernel, we can generate personalized summaries that cover relevant source information while retaining coverage.</li>
</ul>

<h3>Title: GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shikhhar Siingh, Abhinav Rawat, Vivek Gupta, Chitta Baral</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21863">https://arxiv.org/abs/2505.21863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21863">https://arxiv.org/pdf/2505.21863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21863]] GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning(https://arxiv.org/abs/2505.21863)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.</li>
</ul>

<h3>Title: Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Guiping Cao, Wenjian Huang, Xiangyuan Lan, Jianguo Zhang, Dongmei Jiang, Yaowei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21868">https://arxiv.org/abs/2505.21868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21868">https://arxiv.org/pdf/2505.21868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21868]] Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection(https://arxiv.org/abs/2505.21868)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Small Object Detection (SOD) poses significant challenges due to limited information and the model's low class prediction score. While Transformer-based detectors have shown promising performance, their potential for SOD remains largely unexplored. In typical DETR-like frameworks, the CNN backbone network, specialized in aggregating local information, struggles to capture the necessary contextual information for SOD. The multiple attention layers in the Transformer Encoder face difficulties in effectively attending to small objects and can also lead to blurring of features. Furthermore, the model's lower class prediction score of small objects compared to large objects further increases the difficulty of SOD. To address these challenges, we introduce a novel approach called Cross-DINO. This approach incorporates the deep MLP network to aggregate initial feature representations with both short and long range information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to integrate these initial representations to the Transformer Encoder feature, enhancing the details of small objects. Additionally, we introduce a new kind of soft label named Category-Size (CS), integrating the Category and Size of objects. By treating CS as new ground truth, we propose a new loss function called Boost Loss to improve the class prediction score of the model. Extensive experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D datasets demonstrate that Cross-DINO efficiently improves the performance of DETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for SOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs. 32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The source codes will be available at this https URL.</li>
</ul>

<h3>Title: Evaluating the Retrieval Robustness of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, Shiyue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21870">https://arxiv.org/abs/2505.21870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21870">https://arxiv.org/pdf/2505.21870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21870]] Evaluating the Retrieval Robustness of Large Language Models(https://arxiv.org/abs/2505.21870)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) generally enhances large language models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also lead to performance degradation due to imperfect retrieval and the model's limited ability to leverage retrieved content. In this work, we evaluate the robustness of LLMs in practical RAG setups (henceforth retrieval robustness). We focus on three research questions: (1) whether RAG is always better than non-RAG; (2) whether more retrieved documents always lead to better performance; (3) and whether document orders impact results. To facilitate this study, we establish a benchmark of 1500 open-domain questions, each with retrieved documents from Wikipedia. We introduce three robustness metrics, each corresponds to one research question. Our comprehensive experiments, involving 11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit surprisingly high retrieval robustness; nonetheless, different degrees of imperfect robustness hinders them from fully utilizing the benefits of RAG.</li>
</ul>

<h3>Title: EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21876">https://arxiv.org/abs/2505.21876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21876">https://arxiv.org/pdf/2505.21876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21876]] EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance(https://arxiv.org/abs/2505.21876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios.</li>
</ul>

<h3>Title: Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongyao Chen, Tianyang Xu, Xiaojun Wu, Josef Kittler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21877">https://arxiv.org/abs/2505.21877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21877">https://arxiv.org/pdf/2505.21877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21877]] Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning(https://arxiv.org/abs/2505.21877)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Batch Normalisation (BN) is widely used in conventional deep neural network training to harmonise the input-output distributions for each batch of data. However, federated learning, a distributed learning paradigm, faces the challenge of dealing with non-independent and identically distributed data among the client nodes. Due to the lack of a coherent methodology for updating BN statistical parameters, standard BN degrades the federated learning performance. To this end, it is urgent to explore an alternative normalisation solution for federated learning. In this work, we resolve the dilemma of the BN layer in federated learning by developing a customised normalisation approach, Hybrid Batch Normalisation (HBN). HBN separates the update of statistical parameters (i.e. , means and variances used for evaluation) from that of learnable parameters (i.e. , parameters that require gradient updates), obtaining unbiased estimates of global statistical parameters in distributed scenarios. In contrast with the existing solutions, we emphasise the supportive power of global statistics for federated learning. The HBN layer introduces a learnable hybrid distribution factor, allowing each computing node to adaptively mix the statistical parameters of the current batch with the global statistics. Our HBN can serve as a powerful plugin to advance federated learning performance. It reflects promising merits across a wide range of federated learning settings, especially for small batch sizes and heterogeneous data.</li>
</ul>

<h3>Title: EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21889">https://arxiv.org/abs/2505.21889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21889">https://arxiv.org/pdf/2505.21889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21889]] EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse(https://arxiv.org/abs/2505.21889)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are often used for infilling tasks, which involve predicting or generating missing information in a given text. These tasks typically require multiple interactions with similar context. To reduce the computation of repeated historical tokens, cross-request key-value (KV) cache reuse, a technique that stores and reuses intermediate computations, has become a crucial method in multi-round interactive services. However, in infilling tasks, the KV cache reuse is often hindered by the structure of the prompt format, which typically consists of a prefix and suffix relative to the insertion point. Specifically, the KV cache of the prefix or suffix part is frequently invalidated as the other part (suffix or prefix) is incrementally generated. To address the issue, we propose EFIM, a transformed prompt format of FIM to unleash the performance potential of KV cache reuse. Although the transformed prompt can solve the inefficiency, it exposes subtoken generation problems in current LLMs, where they have difficulty generating partial words accurately. Therefore, we introduce a fragment tokenization training method which splits text into multiple fragments before tokenization during data processing. Experiments on two representative LLMs show that LLM serving with EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling this http URL's source code is publicly available at this https URL.</li>
</ul>

<h3>Title: Hyperspectral Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Sunil Kumar Narayanan, Lingjun Zhao, Lu Gan, Yongsheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21890">https://arxiv.org/abs/2505.21890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21890">https://arxiv.org/pdf/2505.21890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21890]] Hyperspectral Gaussian Splatting(https://arxiv.org/abs/2505.21890)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise determination of nutritional elements in samples. Recently, 3D reconstruction methods have been used to create implicit neural representations of HSI scenes, which can help localize the target object's nutrient composition spatially and spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit representation that can render hyperspectral channel compositions of each spatial location from any viewing direction. However, it faces limitations in training time and rendering speed. In this paper, we propose Hyperspectral Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of the hyperspectral scenes and novel view synthesis for the entire spectral range. To enhance the model's ability to capture fine-grained reflectance variations across the light spectrum and leverage correlations between adjacent wavelengths for denoising, we introduce a wavelength encoder to generate wavelength-specific spherical harmonics offsets. We also introduce a novel Kullback--Leibler divergence-based loss to mitigate the spectral distribution gap between the rendered image and the ground truth. A diffusion model is further applied for denoising the rendered images and generating photorealistic hyperspectral images. We present extensive evaluations on five diverse hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of our proposed HS-GS framework. The results demonstrate that HS-GS achieves new state-of-the-art performance among all previously published methods. Code will be released upon publication.</li>
</ul>

<h3>Title: SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21893">https://arxiv.org/abs/2505.21893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21893">https://arxiv.org/pdf/2505.21893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21893]] SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training(https://arxiv.org/abs/2505.21893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\&M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.</li>
</ul>

<h3>Title: Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jianchao Jiang, Haofeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21897">https://arxiv.org/abs/2505.21897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21897">https://arxiv.org/pdf/2505.21897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21897]] Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation(https://arxiv.org/abs/2505.21897)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a model that can perform segmentation from only a few annotated images. However, most existing prototype-based FSMIS methods generate multiple prototypes from the support image solely by random sampling or local averaging, which can cause particularly severe boundary blurring due to the tendency for normal features accounting for the majority of features of a specific category. Consequently, we propose to focus more attention to those weaker features that are crucial for clear segmentation boundary. Specifically, we design a Support Self-Prediction (SSP) module to identify such weak features by comparing true support mask with one predicted by global support prototype. Then, a Hard Prototypes Generation (HPG) module is employed to generate multiple hard prototypes based on these weak features. Subsequently, a Multiple Similarity Maps Fusion (MSMF) module is devised to generate final segmenting mask in a dual-path fashion to mitigate the imbalance between foreground and background in medical images. Furthermore, we introduce a boundary loss to further constraint the edge of segmentation. Extensive experiments on three publicly available medical image datasets demonstrate that our method achieves state-of-the-art performance. Code is available at this https URL.</li>
</ul>

<h3>Title: Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development</h3>
<ul>
<li><strong>Authors: </strong>Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng Yang, Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21898">https://arxiv.org/abs/2505.21898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21898">https://arxiv.org/pdf/2505.21898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21898]] Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development(https://arxiv.org/abs/2505.21898)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) and autonomous agents have demonstrated remarkable capabilities across various domains. However, standalone agents frequently encounter limitations when handling complex tasks that demand extensive interactions and substantial computational resources. Although Multi-Agent Systems (MAS) alleviate some of these limitations through collaborative mechanisms like task decomposition, iterative communication, and role specialization, they typically remain resource-unaware, incurring significant inefficiencies due to high token consumption and excessive execution time. To address these limitations, we propose a resource-aware multi-agent system -- Co-Saving (meaning that multiple agents collaboratively engage in resource-saving activities), which leverages experiential knowledge to enhance operational efficiency and solution quality. Our key innovation is the introduction of "shortcuts" -- instructional transitions learned from historically successful trajectories -- which allows to bypass redundant reasoning agents and expedite the collective problem-solving process. Experiments for software development tasks demonstrate significant advantages over existing methods. Specifically, compared to the state-of-the-art MAS ChatDev, our method achieves an average reduction of 50.85% in token usage, and improves the overall code quality by 10.06%.</li>
</ul>

<h3>Title: CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21904">https://arxiv.org/abs/2505.21904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21904">https://arxiv.org/pdf/2505.21904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21904]] CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation(https://arxiv.org/abs/2505.21904)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation demands costly per-pixel annotations and large models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pretrained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, (2) distillation into a compact student via a unified multi-objective loss that couples standard supervision and pseudo-labels with our instance-aware pixel-wise contrastive term, and (3) fine-tuning on labeled data to remove residual pseudo-label bias. Central to CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to mine informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs. 15.2) and outperforms state-of-the-art semi-supervised approaches.</li>
</ul>

<h3>Title: Reference-Guided Identity Preserving Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Mo Zhou, Keren Ye, Viraj Shah, Kangfu Mei, Mauricio Delbracio, Peyman Milanfar, Vishal M. Patel, Hossein Talebi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21905">https://arxiv.org/abs/2505.21905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21905">https://arxiv.org/pdf/2505.21905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21905]] Reference-Guided Identity Preserving Face Restoration(https://arxiv.org/abs/2505.21905)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Preserving face identity is a critical yet persistent challenge in diffusion-based image restoration. While reference faces offer a path forward, existing reference-based methods often fail to fully exploit their potential. This paper introduces a novel approach that maximizes reference face utility for improved face restoration and identity preservation. Our method makes three key contributions: 1) Composite Context, a comprehensive representation that fuses multi-level (high- and low-level) information from the reference face, offering richer guidance than prior singular representations. 2) Hard Example Identity Loss, a novel loss function that leverages the reference face to address the identity learning inefficiencies found in the existing identity loss. 3) A training-free method to adapt the model to multi-reference inputs during inference. The proposed method demonstrably restores high-quality faces and achieves state-of-the-art identity preserving restoration on benchmarks such as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work.</li>
</ul>

<h3>Title: Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding</h3>
<ul>
<li><strong>Authors: </strong>Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian Bartlett, Bryan Hull, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21908">https://arxiv.org/abs/2505.21908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21908">https://arxiv.org/pdf/2505.21908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21908]] Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding(https://arxiv.org/abs/2505.21908)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.</li>
</ul>

<h3>Title: Taming Transformer Without Using Learning Rate Warmup</h3>
<ul>
<li><strong>Authors: </strong>Xianbiao Qi, Yelin He, Jiaquan Ye, Chun-Guang Li, Bojia Zi, Xili Dai, Qin Zou, Rong Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21910">https://arxiv.org/abs/2505.21910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21910">https://arxiv.org/pdf/2505.21910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21910]] Taming Transformer Without Using Learning Rate Warmup(https://arxiv.org/abs/2505.21910)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Scaling Transformer to a large scale without using some technical tricks such as learning rate warump and using an obviously lower learning rate is an extremely challenging task, and is increasingly gaining more attention. In this paper, we provide a theoretical analysis for the process of training Transformer and reveal the rationale behind the model crash phenomenon in the training process, termed \textit{spectral energy concentration} of ${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse, where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the key in Transformer, respectively. To remedy this problem, motivated by \textit{Weyl's Inequality}, we present a novel optimization strategy, \ie, making the weight updating in successive steps smooth -- if the ratio $\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a threshold, we will automatically bound the learning rate to a weighted multiple of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla \bW_t$ is the updating quantity in step $t$. Such an optimization strategy can prevent spectral energy concentration to only a few directions, and thus can avoid malignant entropy collapse which will trigger the model crash. We conduct extensive experiments using ViT, Swin-Transformer and GPT, showing that our optimization strategy can effectively and stably train these Transformers without using learning rate warmup.</li>
</ul>

<h3>Title: AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yiheng Lin, Shifang Zhao, Ting Liu, Xiaochao Qu, Luoqi Liu, Yao Zhao, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21911">https://arxiv.org/abs/2505.21911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21911">https://arxiv.org/pdf/2505.21911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21911]] AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment(https://arxiv.org/abs/2505.21911)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Personalized image generation aims to integrate user-provided concepts into text-to-image models, enabling the generation of customized content based on a given prompt. Recent zero-shot approaches, particularly those leveraging diffusion transformers, incorporate reference image information through multi-modal attention mechanism. This integration allows the generated output to be influenced by both the textual prior from the prompt and the visual prior from the reference image. However, we observe that when the prompt and reference image are misaligned, the generated results exhibit a stronger bias toward the textual prior, leading to a significant loss of reference content. To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment mechanism that enhances personalized image generation by: 1) introducing a learnable token to bridge the gap between the textual and visual priors, 2) incorporating a robust training strategy to ensure proper prior alignment, and 3) employing a selective cross-modal attention mask within the multi-modal attention mechanism to further align the priors. Experimental results demonstrate that AlignGen outperforms existing zero-shot methods and even surpasses popular test-time optimization approaches.</li>
</ul>

<h3>Title: LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments</h3>
<ul>
<li><strong>Authors: </strong>Chenfeng Wei, Qi Wu, Si Zuo, Jiahua Xu, Boyang Zhao, Zeyu Yang, Guotao Xie, Shenhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21914">https://arxiv.org/abs/2505.21914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21914">https://arxiv.org/pdf/2505.21914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21914]] LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments(https://arxiv.org/abs/2505.21914)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Autonomous driving datasets are essential for validating the progress of intelligent vehicle algorithms, which include localization, perception, and prediction. However, existing datasets are predominantly focused on structured urban environments, which limits the exploration of unstructured and specialized scenarios, particularly those characterized by significant dust levels. This paper introduces the LiDARDustX dataset, which is specifically designed for perception tasks under high-dust conditions, such as those encountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR frames captured by six different LiDAR sensors, each accompanied by 3D bounding box annotations and point cloud semantic segmentation. Notably, over 80% of the dataset comprises dust-affected scenes. By utilizing this dataset, we have established a benchmark for evaluating the performance of state-of-the-art 3D detection and segmentation algorithms. Additionally, we have analyzed the impact of dust on perception accuracy and delved into the causes of these effects. The data and further information can be accessed at: this https URL.</li>
</ul>

<h3>Title: BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh</h3>
<ul>
<li><strong>Authors: </strong>Mir Sazzat Hossain, Ovi Paul, Md Akil Raihan Iftee, Rakibul Hasan Rajib, Abu Bakar Siddik Nayem, Anis Sarker, Arshad Momen, Md. Ashraful Amin, Amin Ahsan Ali, AKM Mahbubur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21915">https://arxiv.org/abs/2505.21915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21915">https://arxiv.org/pdf/2505.21915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21915]] BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh(https://arxiv.org/abs/2505.21915)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Land Use Land Cover (LULC) mapping using deep learning significantly enhances the reliability of LULC classification, aiding in understanding geography, socioeconomic conditions, poverty levels, and urban sprawl. However, the scarcity of annotated satellite data, especially in South/East Asian developing countries, poses a major challenge due to limited funding, diverse infrastructures, and dense populations. In this work, we introduce the BD Open LULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes (e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka metropolitan city and its surroundings using high-resolution Bing satellite imagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with ground truth validated through a three-stage process involving GIS experts. We benchmark LULC segmentation using DeepLab V3+ across five major classes and compare performance on Bing and Sentinel-2A imagery. BOLM aims to support reliable deep models and domain adaptation tasks, addressing critical LULC dataset gaps in South/East Asia.</li>
</ul>

<h3>Title: Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing</h3>
<ul>
<li><strong>Authors: </strong>Haruki Kai, Tsuyoshi Okita</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21918">https://arxiv.org/abs/2505.21918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21918">https://arxiv.org/pdf/2505.21918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21918]] Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing(https://arxiv.org/abs/2505.21918)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We developed a deep learning algorithm for human activity recognition using sensor signals as input. In this study, we built a pretrained language model based on the Transformer architecture, which is widely used in natural language processing. By leveraging this pretrained model, we aimed to improve performance on the downstream task of human activity recognition. While this task can be addressed using a vanilla Transformer, we propose an enhanced n-dimensional numerical processing Transformer that incorporates three key features: embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and a linear transformation in the output layer. We evaluated the effectiveness of our proposed model across five different datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15% improvements in accuracy.</li>
</ul>

<h3>Title: InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuanhong Zhang, Muyao Yuan, Weizhan Zhang, Tieliang Gong, Wen Wen, Jiangyong Ying, Weijie Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21920">https://arxiv.org/abs/2505.21920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21920">https://arxiv.org/pdf/2505.21920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21920]] InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective(https://arxiv.org/abs/2505.21920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM), a vision foundation model, exhibits impressive zero-shot capabilities in general tasks but struggles in specialized domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to unleash the potential of SAM in novel scenarios. However, existing PEFT methods for SAM neglect the domain-invariant relations encoded in the pre-trained model. To bridge this gap, we propose InfoSAM, an information-theoretic approach that enhances SAM fine-tuning by distilling and preserving its pre-trained segmentation knowledge. Specifically, we formulate the knowledge transfer process as two novel mutual information-based objectives: (i) to compress the domain-invariant relation extracted from pre-trained SAM, excluding pseudo-invariant information as possible, and (ii) to maximize mutual information between the relational knowledge learned by the teacher (pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM establishes a robust distillation framework for PEFT of SAM. Extensive experiments across diverse benchmarks validate InfoSAM's effectiveness in improving SAM family's performance on real-world tasks, demonstrating its adaptability and superiority in handling specialized scenarios.</li>
</ul>

<h3>Title: RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments</h3>
<ul>
<li><strong>Authors: </strong>Zeyi Liao, Jaylen Jones, Linxi Jiang, Eric Fosler-Lussier, Yu Su, Zhiqiang Lin, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21936">https://arxiv.org/abs/2505.21936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21936">https://arxiv.org/pdf/2505.21936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21936]] RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments(https://arxiv.org/abs/2505.21936)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection. Current evaluations of this threat either lack support realistic but controlled environments or ignore hybrid web-OS attack scenarios involving both interfaces. To address this, we propose RedTeamCUA, an adversarial testing framework featuring a novel hybrid sandbox that integrates a VM-based OS environment with Docker-based web platforms. Our sandbox supports key features tailored for red teaming, such as flexible adversarial scenario configuration, and a setting that decouples adversarial evaluation from navigational limitations of CUAs by initializing tests directly at the point of an adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive benchmark with 864 examples that investigate realistic, hybrid web-OS attack scenarios and fundamental security vulnerabilities. Benchmarking current frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated, still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute adversarial tasks with an Attempt Rate as high as 92.5%, although failing to complete them due to capability limitations. Nevertheless, we observe concerning ASRs of up to 50% in realistic end-to-end settings, with the recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%, demonstrating that indirect prompt injection presents tangible risks for even advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA provides an essential framework for advancing realistic, controlled, and systematic analysis of CUA vulnerabilities, highlighting the urgent need for robust defenses to indirect prompt injection prior to real-world deployment.</li>
</ul>

<h3>Title: Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</h3>
<ul>
<li><strong>Authors: </strong>Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, Jinhang Zuo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21938">https://arxiv.org/abs/2505.21938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21938">https://arxiv.org/pdf/2505.21938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21938]] Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection(https://arxiv.org/abs/2505.21938)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.</li>
</ul>

<h3>Title: RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Bolei He, Xinran He, Mengke Chen, Xianwei Xue, Ying Zhu, Zhenhua Ling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21940">https://arxiv.org/abs/2505.21940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21940">https://arxiv.org/pdf/2505.21940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21940]] RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering(https://arxiv.org/abs/2505.21940)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in many areas but continue to face challenges with complex reasoning tasks, such as Multi-Hop Question Answering (MHQA). MHQA requires integrating evidence from diverse sources while managing intricate logical dependencies, often leads to errors in reasoning. Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces challenges in effectively filtering noisy data and retrieving all necessary evidence, thereby limiting its effectiveness in addressing MHQA challenges. To address these challenges, we propose RISE:Reasoning Enhancement via Iterative Self-Exploration, a novel framework designed to enhance models' reasoning capability through iterative self-exploration. Specifically, RISE involves three key steps in addressing MHQA tasks: question decomposition, retrieve-then-read, and self-critique. By leveraging continuous self-exploration, RISE identifies accurate reasoning paths, iteratively self-improving the model's capability to integrate evidence, maintain logical consistency, and enhance performance in MHQA tasks. Extensive experiments on multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning accuracy and task performance.</li>
</ul>

<h3>Title: Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21956">https://arxiv.org/abs/2505.21956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21956">https://arxiv.org/pdf/2505.21956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21956]] Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation(https://arxiv.org/abs/2505.21956)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency.</li>
</ul>

<h3>Title: Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning</h3>
<ul>
<li><strong>Authors: </strong>Qihuang Zhong, Liang Ding, Fei Liao, Juhua Liu, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21958">https://arxiv.org/abs/2505.21958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21958">https://arxiv.org/pdf/2505.21958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21958]] Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning(https://arxiv.org/abs/2505.21958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Domain-specific instruction-tuning has become the defacto standard for improving the performance of large language models (LLMs) in specialized applications, e.g., medical question answering. Since the instruction-tuning dataset might contain redundant or low-quality data, data selection (DS) is usually required to maximize the data efficiency. Despite the successes in the general domain, current DS methods often struggle to select the desired data for domain-specific instruction-tuning. One of the main reasons is that they neglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs' pretrained knowledge and context knowledge of instruction data, which could damage LLMs' prior abilities and lead to hallucination. To this end, we propose a simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to select the domain-specific instruction-tuning data that meets LLMs' actual needs. The core of KDS is to leverage two knowledge-aware metrics for quantitatively measuring knowledge conflicts from two aspects: context-memory knowledge alignment and intra-memory knowledge consistency. By filtering the data with large knowledge conflicts and sampling the high-quality and diverse data, KDS can effectively stimulate the LLMs' abilities and achieve better domain-specific performance. Taking the medical domain as the testbed, we conduct extensive experiments and empirically prove that KDS surpasses the other baselines and brings significant and consistent performance gains among all LLMs. More encouragingly, KDS effectively improves the model generalization and alleviates the hallucination problem.</li>
</ul>

<h3>Title: EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, Bang An, Bayan Bruss, John Langford, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21959">https://arxiv.org/abs/2505.21959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21959">https://arxiv.org/pdf/2505.21959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21959]] EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles(https://arxiv.org/abs/2505.21959)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With Large Language Models (LLMs) rapidly approaching and potentially surpassing human-level performance, it has become imperative to develop approaches capable of effectively supervising and enhancing these powerful models using smaller, human-level models exposed to only human-level data. We address this critical weak-to-strong (W2S) generalization challenge by proposing a novel method aimed at improving weak experts, by training on the same limited human-level data, enabling them to generalize to complex, super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a token-level ensemble strategy that iteratively combines multiple weak experts, systematically addressing the shortcomings identified in preceding iterations. By continuously refining these weak models, we significantly enhance their collective ability to supervise stronger student models. We extensively evaluate the generalization performance of both the ensemble of weak experts and the subsequent strong student model across in-distribution (ID) and out-of-distribution (OOD) datasets. For OOD, we specifically introduce question difficulty as an additional dimension for defining distributional shifts. Our empirical results demonstrate notable improvements, achieving 4\%, and 3.2\% improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for experts and student models respectively, underscoring the effectiveness of our proposed method in advancing W2S generalization.</li>
</ul>

<h3>Title: One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Senmao Li, Lei Wang, Kai Wang, Tao Liu, Jiehang Xie, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21960">https://arxiv.org/abs/2505.21960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21960">https://arxiv.org/pdf/2505.21960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21960]] One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models(https://arxiv.org/abs/2505.21960)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency.</li>
</ul>

<h3>Title: LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Taro Yano, Yoichi Ishibashi, Masafumi Oyamada</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21963">https://arxiv.org/abs/2505.21963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21963">https://arxiv.org/pdf/2505.21963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21963]] LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents(https://arxiv.org/abs/2505.21963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.</li>
</ul>

<h3>Title: Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Juan Ren, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21967">https://arxiv.org/abs/2505.21967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21967">https://arxiv.org/pdf/2505.21967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21967]] Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack(https://arxiv.org/abs/2505.21967)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have shown remarkable capabilities across a wide range of multimodal tasks. However, their integration of visual inputs introduces expanded attack surfaces, thereby exposing them to novel security vulnerabilities. In this work, we conduct a systematic representational analysis to uncover why conventional adversarial attacks can circumvent the safety mechanisms embedded in LVLMs. We further propose a novel two stage evaluation framework for adversarial attacks on LVLMs. The first stage differentiates among instruction non compliance, outright refusal, and successful adversarial exploitation. The second stage quantifies the degree to which the model's output fulfills the harmful intent of the adversarial prompt, while categorizing refusal behavior into direct refusals, soft refusals, and partial refusals that remain inadvertently helpful. Finally, we introduce a normative schema that defines idealized model behavior when confronted with harmful prompts, offering a principled target for safety alignment in multimodal systems.</li>
</ul>

<h3>Title: Judging LLMs on a Simplex</h3>
<ul>
<li><strong>Authors: </strong>Patrick Vossler, Fan Xia, Yifan Mai, Jean Feng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21972">https://arxiv.org/abs/2505.21972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21972">https://arxiv.org/pdf/2505.21972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21972]] Judging LLMs on a Simplex(https://arxiv.org/abs/2505.21972)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated evaluation of free-form outputs from large language models (LLMs) is challenging because many distinct answers can be equally valid. A common practice is to use LLMs themselves as judges, but the theoretical properties of this approach are not yet well understood. We show that a geometric framework that represents both judges and candidates as points on a probability simplex can provide helpful insight on what is or is not identifiable using LLM judges. Our theoretical analysis uncovers a "phase transition" in ranking identifiability: for binary scoring systems, true rankings are identifiable even with weak judges under mild assumptions, while rankings become non-identifiable for three or more scoring levels even with infinite data, absent additional prior knowledge. This non-identifiability highlights how uncertainty in rankings stems from not only aleatoric uncertainty (i.e., inherent stochasticity in the data) but also epistemic uncertainty regarding which assumptions hold, an aspect that has received limited attention until now. To integrate both types of uncertainty, we use Bayesian inference to encode assumptions as priors and conduct sensitivity analysis of ranking estimates and credible intervals. Empirical evaluations across multiple benchmarks demonstrate that Bayesian inference yields more accurate rankings and substantially improves coverage rates. These results underscore the importance of taking a more holistic approach to uncertainty quantification when using LLMs as judges.</li>
</ul>

<h3>Title: BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL</h3>
<ul>
<li><strong>Authors: </strong>Yu-Heng Hung, Kai-Jie Lin, Yu-Heng Lin, Chien-YiWang, Cheng Sun, Ping-Chun Hsieh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21974">https://arxiv.org/abs/2505.21974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21974">https://arxiv.org/pdf/2505.21974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21974]] BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL(https://arxiv.org/abs/2505.21974)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myopic nature. Despite this, the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the \textit{hypervolume identifiability issue}, which results from the non-Markovian nature of MOBO problems. To tackle this, inspired by the non-Markovian RL literature and the success of Transformers in language modeling, we present a generalized deep Q-learning framework and propose \textit{BOFormer}, which substantiates this framework for MOBO via sequence modeling. Through extensive evaluation, we demonstrate that BOFormer constantly outperforms the benchmark rule-based and learning-based algorithms in various synthetic MOBO and real-world multi-objective hyperparameter optimization problems. We have made the source code publicly available to encourage further research in this direction.</li>
</ul>

<h3>Title: DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Weiguang Zhang, Huangcheng Lu, Maizhen Ning, Xiaowei Huang, Wei Wang, Kaizhu Huang, Qiufeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21975">https://arxiv.org/abs/2505.21975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21975">https://arxiv.org/pdf/2505.21975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21975]] DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model(https://arxiv.org/abs/2505.21975)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Document dewarping aims to rectify deformations in photographic document images, thus improving text readability, which has attracted much attention and made great progress, but it is still challenging to preserve document structures. Given recent advances in diffusion models, it is natural for us to consider their potential applicability to document dewarping. However, it is far from straightforward to adopt diffusion models in document dewarping due to their unfaithful control on highly complex document images (e.g., 2000$\times$3000 resolution). In this paper, we propose DvD, the first generative model to tackle document \textbf{D}ewarping \textbf{v}ia a \textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level denoising instead of typical pixel-level denoising, generating a mapping for deformation rectification. In addition, we further propose a time-variant condition refinement mechanism to enhance the preservation of document structures. In experiments, we find that current document dewarping benchmarks can not evaluate dewarping models comprehensively. To this end, we present AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark comprising 6,300 real image pairs across three distinct domains, enabling fine-grained evaluation of dewarping models. Comprehensive experiments demonstrate that our proposed DvD can achieve state-of-the-art performance with acceptable computational efficiency on multiple metrics across various benchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark and code will be publicly available.</li>
</ul>

<h3>Title: Two-Stage Feature Generation with Transformer and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Wanfu Gao, Zengyao Man, Zebin He, Yuhao Tang, Jun Gao, Kunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21978">https://arxiv.org/abs/2505.21978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21978">https://arxiv.org/pdf/2505.21978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21978]] Two-Stage Feature Generation with Transformer and Reinforcement Learning(https://arxiv.org/abs/2505.21978)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Feature generation is a critical step in machine learning, aiming to enhance model performance by capturing complex relationships within the data and generating meaningful new features. Traditional feature generation methods heavily rely on domain expertise and manual intervention, making the process labor-intensive and challenging to adapt to different scenarios. Although automated feature generation techniques address these issues to some extent, they often face challenges such as feature redundancy, inefficiency in feature space exploration, and limited adaptability to diverse datasets and tasks. To address these problems, we propose a Two-Stage Feature Generation (TSFG) framework, which integrates a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG leverages the Transformer's self-attention mechanism to efficiently represent and transform features, capturing complex dependencies within the data. PPO further enhances TSFG by dynamically adjusting the feature generation strategy based on task-specific feedback, optimizing the process for improved performance and adaptability. TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models. Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability.</li>
</ul>

<h3>Title: Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset</h3>
<ul>
<li><strong>Authors: </strong>Fakhraddin Alwajih, Samar Mohamed Magdy, Abdellah El Mekki, Omer Nacar, Youssef Nafea, Safaa Taher Abdelfadil, Abdulfattah Mohammed Yahya, Hamzah Luqman, Nada Almarwani, Samah Aloufi, Baraah Qawasmeh, Houdaifa Atou, Serry Sibaee, Hamzah A. Alsayadi, Walid Al-Dhabyani, Maged S. Al-shaibani, Aya El aatar, Nour Qandos, Rahaf Alhamouri, Samar Ahmad, Razan Khassib, Lina Hamad, Mohammed Anwar AL-Ghrawi, Fatimah Alshamari, Cheikh Malainine, Doaa Qawasmeh, Aminetou Yacoub, Tfeil moilid, Ruwa AbuHweidi, Ahmed Aboeitta, Vatimetou Mohamed Lemin, Reem Abdel-Salam, Ahlam Bashiti, Adel Ammar, Aisha Alansari, Ahmed Ashraf, Nora Alturayeif, Sara Shatnawi, Alcides Alcoba Inciarte, AbdelRahim A. Elmadany, Mohamedou cheikh tourad, Ismail Berrada, Mustafa Jarrar, Shady Shehata, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21979">https://arxiv.org/abs/2505.21979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21979">https://arxiv.org/pdf/2505.21979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21979]] Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset(https://arxiv.org/abs/2505.21979)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mainstream large vision-language models (LVLMs) inherently encode cultural biases, highlighting the need for diverse multimodal datasets. To address this gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark explicitly designed for cultural understanding. Constructed through advanced agentic workflows and extensive human-in-the-loop annotations by 45 annotators from across the Arab world, Pearl comprises over K multimodal examples spanning ten culturally significant domains covering all Arab countries. We further provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a specialized subset Pearl-X explicitly developed to assess nuanced cultural variations. Comprehensive evaluations on state-of-the-art open and proprietary LVLMs demonstrate that reasoning-centric instruction alignment substantially improves models' cultural grounding compared to conventional scaling methods. Pearl establishes a foundational resource for advancing culturally-informed multimodal modeling research. All datasets and benchmarks are publicly available.</li>
</ul>

<h3>Title: ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Mi, Zhenglun Kong, Geng Yuan, Shaoyi Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21987">https://arxiv.org/abs/2505.21987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21987">https://arxiv.org/pdf/2505.21987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21987]] ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning(https://arxiv.org/abs/2505.21987)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid expansion of large language models (LLMs), the demand for memory and computational resources has grown significantly. Recent advances in LLM pruning aim to reduce the size and computational cost of these models. However, existing methods often suffer from either suboptimal pruning performance or low time efficiency during the pruning process. In this work, we propose an efficient and effective pruning method that simultaneously achieves high pruning performance and fast pruning speed with improved calibration efficiency. Our approach introduces two key innovations: (1) An activation cosine similarity loss-guided pruning metric, which considers the angular deviation of the output activation between the dense and pruned models. (2) An activation variance-guided pruning metric, which helps preserve semantic distinctions in output activations after pruning, enabling effective pruning with shorter input sequences. These two components can be readily combined to enhance LLM pruning in both accuracy and efficiency. Experimental results show that our method achieves up to an 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.</li>
</ul>

<h3>Title: Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data</h3>
<ul>
<li><strong>Authors: </strong>Jihong Zhang, Xinya Liang, Anqi Deng, Nicole Bonge, Lin Tan, Ling Zhang, Nicole Zarrett</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21997">https://arxiv.org/abs/2505.21997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21997">https://arxiv.org/pdf/2505.21997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21997]] Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data(https://arxiv.org/abs/2505.21997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixed methods research integrates quantitative and qualitative data but faces challenges in aligning their distinct structures, particularly in examining measurement characteristics and individual response patterns. Advances in large language models (LLMs) offer promising solutions by generating synthetic survey responses informed by qualitative data. This study investigates whether LLMs, guided by personal interviews, can reliably predict human survey responses, using the Behavioral Regulations in Exercise Questionnaire (BREQ) and interviews from after-school program staff as a case study. Results indicate that LLMs capture overall response patterns but exhibit lower variability than humans. Incorporating interview data improves response diversity for some models (e.g., Claude, GPT), while well-crafted prompts and low-temperature settings enhance alignment between LLM and human responses. Demographic information had less impact than interview content on alignment accuracy. These findings underscore the potential of interview-informed LLMs to bridge qualitative and quantitative methodologies while revealing limitations in response variability, emotional interpretation, and psychometric fidelity. Future research should refine prompt design, explore bias mitigation, and optimize model settings to enhance the validity of LLM-generated survey data in social science research.</li>
</ul>

<h3>Title: Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate</h3>
<ul>
<li><strong>Authors: </strong>Ashim Gupta, Maitrey Mehta, Zhichao Xu, Vivek Srikumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.21999">https://arxiv.org/abs/2505.21999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.21999">https://arxiv.org/pdf/2505.21999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.21999]] Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate(https://arxiv.org/abs/2505.21999)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) provide detailed and impressive responses to queries in English. However, are they really consistent at responding to the same query in other languages? The popular way of evaluating for multilingual performance of LLMs requires expensive-to-collect annotated datasets. Further, evaluating for tasks like open-ended generation, where multiple correct answers may exist, is nontrivial. Instead, we propose to evaluate the predictability of model response across different languages. In this work, we propose a framework to evaluate LLM's cross-lingual consistency based on a simple Translate then Evaluate strategy. We instantiate this evaluation framework along two dimensions of consistency: information and empathy. Our results reveal pronounced inconsistencies in popular LLM responses across thirty languages, with severe performance deficits in certain language families and scripts, underscoring critical weaknesses in their multilingual capabilities. These findings necessitate cross-lingual evaluations that are consistent along multiple dimensions. We invite practitioners to use our framework for future multilingual LLM benchmarking.</li>
</ul>

<h3>Title: D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples</h3>
<ul>
<li><strong>Authors: </strong>Zijing Hu, Fengda Zhang, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22002">https://arxiv.org/abs/2505.22002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22002">https://arxiv.org/pdf/2505.22002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22002]] D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples(https://arxiv.org/abs/2505.22002)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The practical applications of diffusion models have been limited by the misalignment between generated images and corresponding text prompts. Recent studies have introduced direct preference optimization (DPO) to enhance the alignment of these models. However, the effectiveness of DPO is constrained by the issue of visual inconsistency, where the significant visual disparity between well-aligned and poorly-aligned images prevents diffusion models from identifying which factors contribute positively to alignment during fine-tuning. To address this issue, this paper introduces D-Fusion, a method to construct DPO-trainable visually consistent samples. On one hand, by performing mask-guided self-attention fusion, the resulting images are not only well-aligned, but also visually consistent with given poorly-aligned images. On the other hand, D-Fusion can retain the denoising trajectories of the resulting images, which are essential for DPO training. Extensive experiments demonstrate the effectiveness of D-Fusion in improving prompt-image alignment when applied to different reinforcement learning algorithms.</li>
</ul>

<h3>Title: Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance</h3>
<ul>
<li><strong>Authors: </strong>Jatin Gupta, Akhil Sharma, Saransh Singhania, Ali Imam Abidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22003">https://arxiv.org/abs/2505.22003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22003">https://arxiv.org/pdf/2505.22003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22003]] Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance(https://arxiv.org/abs/2505.22003)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the model's applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well.</li>
</ul>

<h3>Title: Event-based Egocentric Human Pose Estimation in Dynamic Environment</h3>
<ul>
<li><strong>Authors: </strong>Wataru Ikeda, Masashi Hatano, Ryosei Hara, Mariko Isogawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22007">https://arxiv.org/abs/2505.22007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22007">https://arxiv.org/pdf/2505.22007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22007]] Event-based Egocentric Human Pose Estimation in Dynamic Environment(https://arxiv.org/abs/2505.22007)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Estimating human pose using a front-facing egocentric camera is essential for applications such as sports motion analysis, VR/AR, and AI for wearable devices. However, many existing methods rely on RGB cameras and do not account for low-light environments or motion blur. Event-based cameras have the potential to address these challenges. In this work, we introduce a novel task of human pose estimation using a front-facing event-based camera mounted on the head and propose D-EventEgo, the first framework for this task. The proposed method first estimates the head poses, and then these are used as conditions to generate body poses. However, when estimating head poses, the presence of dynamic objects mixed with background events may reduce head pose estimation accuracy. Therefore, we introduce the Motion Segmentation Module to remove dynamic objects and extract background information. Extensive experiments on our synthetic event-based dataset derived from EgoBody, demonstrate that our approach outperforms our baseline in four out of five evaluation metrics in dynamic environments.</li>
</ul>

<h3>Title: VulBinLLM: LLM-powered Vulnerability Detection for Stripped Binaries</h3>
<ul>
<li><strong>Authors: </strong>Nasir Hussain, Haohan Chen, Chanh Tran, Philip Huang, Zhuohao Li, Pravir Chugh, William Chen, Ashish Kundu, Yuan Tian</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22010">https://arxiv.org/abs/2505.22010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22010">https://arxiv.org/pdf/2505.22010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22010]] VulBinLLM: LLM-powered Vulnerability Detection for Stripped Binaries(https://arxiv.org/abs/2505.22010)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Recognizing vulnerabilities in stripped binary files presents a significant challenge in software security. Although some progress has been made in generating human-readable information from decompiled binary files with Large Language Models (LLMs), effectively and scalably detecting vulnerabilities within these binary files is still an open problem. This paper explores the novel application of LLMs to detect vulnerabilities within these binary files. We demonstrate the feasibility of identifying vulnerable programs through a combined approach of decompilation optimization to make the vulnerabilities more prominent and long-term memory for a larger context window, achieving state-of-the-art performance in binary vulnerability analysis. Our findings highlight the potential for LLMs to overcome the limitations of traditional analysis methods and advance the field of binary vulnerability detection, paving the way for more secure software systems. In this paper, we present Vul-BinLLM , an LLM-based framework for binary vulnerability detection that mirrors traditional binary analysis workflows with fine-grained optimizations in decompilation and vulnerability reasoning with an extended context. In the decompilation phase, Vul-BinLLM adds vulnerability and weakness comments without altering the code structure or functionality, providing more contextual information for vulnerability reasoning later. Then for vulnerability reasoning, Vul-BinLLM combines in-context learning and chain-of-thought prompting along with a memory management agent to enhance accuracy. Our evaluations encompass the commonly used synthetic dataset Juliet to evaluate the potential feasibility for analysis and vulnerability detection in C/C++ binaries. Our evaluations show that Vul-BinLLM is highly effective in detecting vulnerabilities on the compiled Juliet dataset.</li>
</ul>

<h3>Title: Learning in Compact Spaces with Approximately Normalized Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jörg K.H. Franke, Urs Spiegelhalter, Marianna Nezhurina, Jenia Jitsev, Frank Hutter, Michael Hefenbrock</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22014">https://arxiv.org/abs/2505.22014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22014">https://arxiv.org/pdf/2505.22014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22014]] Learning in Compact Spaces with Approximately Normalized Transformers(https://arxiv.org/abs/2505.22014)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In deep learning, regularization and normalization are common solutions for challenges such as overfitting, numerical instabilities, and the increasing variance in the residual stream. An alternative approach is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic but approximate normalization (anTransformer). Our approach constrains the norm of parameters and normalizes all representations via scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. When applied to GPT training, we observe a 40% faster convergence compared to models with QK normalization, with less than 3% additional runtime. Deriving scaling laws for anGPT, we found our method enables training with larger batch sizes and fewer hyperparameters, while matching the favorable scaling characteristics of classic GPT architectures.</li>
</ul>

<h3>Title: PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Yifei Xia, Shuchen Weng, Siqi Yang, Jingqi Liu, Chengxuan Zhu, Minggui Teng, Zijian Jia, Han Jiang, Boxin Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22016">https://arxiv.org/abs/2505.22016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22016">https://arxiv.org/pdf/2505.22016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22016]] PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms(https://arxiv.org/abs/2505.22016)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Panoramic video generation enables immersive 360° content creation, valuable in applications that demand scene-consistent world exploration. However, existing panoramic video generation models struggle to leverage pre-trained generative priors from conventional text-to-video models for high-quality and diverse panoramic videos generation, due to limited dataset scale and the gap in spatial feature representations. In this paper, we introduce PanoWan to effectively lift pre-trained text-to-video models to the panoramic domain, equipped with minimal modules. PanoWan employs latitude-aware sampling to avoid latitudinal distortion, while its rotated semantic denoising and padded pixel-wise decoding ensure seamless transitions at longitude boundaries. To provide sufficient panoramic videos for learning these lifted representations, we contribute PanoVid, a high-quality panoramic video dataset with captions and diverse scenarios. Consequently, PanoWan achieves state-of-the-art performance in panoramic video generation and demonstrates robustness for zero-shot downstream tasks.</li>
</ul>

<h3>Title: CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Siqi Fan, Peng Han, Shuo Shang, Yequan Wang, Aixin Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22017">https://arxiv.org/abs/2505.22017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22017">https://arxiv.org/pdf/2505.22017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22017]] CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models(https://arxiv.org/abs/2505.22017)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) benefit from increased test-time compute, a phenomenon known as test-time scaling. However, reasoning-optimized models often overthink even simple problems, producing excessively verbose outputs and leading to low token efficiency. By comparing these models with equally sized instruct models, we identify two key causes of this verbosity: (1) reinforcement learning reduces the information density of forward reasoning, and (2) backward chain-of thought training encourages redundant and often unnecessary verification steps. Since LLMs cannot assess the difficulty of a given problem, they tend to apply the same cautious reasoning strategy across all tasks, resulting in inefficient overthinking. To address this, we propose CoThink, an embarrassingly simple pipeline: an instruct model first drafts a high-level solution outline; a reasoning model then works out the solution. We observe that CoThink enables dynamic adjustment of reasoning depth based on input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on average. With reference to the instruct model, we formally define reasoning efficiency and observe a potential reasoning efficiency scaling law in LLMs.</li>
</ul>

<h3>Title: GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Zhihong Tang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22021">https://arxiv.org/abs/2505.22021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22021">https://arxiv.org/pdf/2505.22021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22021]] GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement(https://arxiv.org/abs/2505.22021)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Document Image Enhancement (DIE) serves as a critical component in Document AI systems, where its performance substantially determines the effectiveness of downstream tasks. To address the limitations of existing methods confined to single-degradation restoration or grayscale image processing, we present Global with Local Parametric Generation Enhancement Network (GL-PGENet), a novel architecture designed for multi-degraded color document images, ensuring both efficiency and robustness in real-world scenarios. Our solution incorporates three key innovations: First, a hierarchical enhancement framework that integrates global appearance correction with local refinement, enabling coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network with parametric generation mechanisms that replaces conventional direct prediction, producing enhanced outputs through learned intermediate parametric representations rather than pixel-wise mapping. This approach enhances local consistency while improving model generalization. Finally, a modified NestUNet architecture incorporating dense block to effectively fuse low-level pixel features and high-level semantic features, specifically adapted for document image characteristics. In addition, to enhance generalization performance, we adopt a two-stage training strategy: large-scale pretraining on a synthetic dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive experiments demonstrate the superiority of GL-PGENet, achieving state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The model also exhibits remarkable cross-domain adaptability and maintains computational efficiency for high-resolution images without performance degradation, confirming its practical utility in real-world scenarios.</li>
</ul>

<h3>Title: Jailbreak Distillation: Renewable Safety Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Zhang, Ahmed Elgohary, Xiawei Wang, A S M Iftekhar, Ahmed Magooda, Benjamin Van Durme, Daniel Khashabi, Kyle Jackson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22037">https://arxiv.org/abs/2505.22037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22037">https://arxiv.org/pdf/2505.22037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22037]] Jailbreak Distillation: Renewable Safety Benchmarking(https://arxiv.org/abs/2505.22037)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are rapidly deployed in critical applications, raising urgent needs for robust safety benchmarking. We propose Jailbreak Distillation (JBDistill), a novel benchmark construction framework that "distills" jailbreak attacks into high-quality and easily-updatable safety benchmarks. JBDistill utilizes a small set of development models and existing jailbreak attack algorithms to create a candidate prompt pool, then employs prompt selection algorithms to identify an effective subset of prompts as safety benchmarks. JBDistill addresses challenges in existing safety evaluation: the use of consistent evaluation prompts across models ensures fair comparisons and reproducibility. It requires minimal human effort to rerun the JBDistill pipeline and produce updated benchmarks, alleviating concerns on saturation and contamination. Extensive experiments demonstrate our benchmarks generalize robustly to 13 diverse evaluation models held out from benchmark construction, including proprietary, specialized, and newer-generation LLMs, significantly outperforming existing safety benchmarks in effectiveness while maintaining high separability and diversity. Our framework thus provides an effective, sustainable, and adaptable solution for streamlining safety evaluation.</li>
</ul>

<h3>Title: Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Michael Grohs, Adrian Rebmann, Jana-Rebecca Rehse</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22041">https://arxiv.org/abs/2505.22041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22041">https://arxiv.org/pdf/2505.22041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22041]] Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation(https://arxiv.org/abs/2505.22041)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conformance checking techniques detect undesired process behavior by comparing process executions that are recorded in event logs to desired behavior that is captured in a dedicated process model. If such models are not available, conformance checking techniques are not applicable, but organizations might still be interested in detecting undesired behavior in their processes. To enable this, existing approaches use Large Language Models (LLMs), assuming that they can learn to distinguish desired from undesired behavior through fine-tuning. However, fine-tuning is highly resource-intensive and the fine-tuned LLMs often do not generalize well. To address these limitations, we propose an approach that requires neither a dedicated process model nor resource-intensive fine-tuning to detect undesired process behavior. Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with direct access to a knowledge base that contains both desired and undesired process behavior from other processes, assuming that the LLM can transfer this knowledge to the process at hand. Our evaluation shows that our approach outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that RAG is a viable alternative to resource-intensive fine-tuning, particularly when enriched with relevant context from the event log, such as frequent traces and activities.</li>
</ul>

<h3>Title: Estimating the Effects of Sample Training Orders for Large Language Models without Retraining</h3>
<ul>
<li><strong>Authors: </strong>Hao Yang, Haoxuan Li, Mengyue Yang, Xu Chen, Mingming Gong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22042">https://arxiv.org/abs/2505.22042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22042">https://arxiv.org/pdf/2505.22042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22042]] Estimating the Effects of Sample Training Orders for Large Language Models without Retraining(https://arxiv.org/abs/2505.22042)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The order of training samples plays a crucial role in large language models (LLMs), significantly impacting both their external performance and internal learning dynamics. Traditional methods for investigating this effect generally require retraining the model with various sample orders, which is computationally infeasible for LLMs. In this work, we improve traditional methods by designing a retraining-free framework. By approximating Adam optimizer updates with first- and second-order Taylor expansions and utilizing random projection methods to store intermediate checkpoints, our framework can efficiently estimate model parameters for arbitrary training sample orders. Next, we apply our framework to two downstream research problems: (1) Training curriculum design for LLMs -- we base our retraining-free framework to propose a novel curriculum learning strategy that augments curriculum proposals with estimated model performances, enabling more informed sample scheduling. (2) LLMs' memorization and generalization effect analysis -- we use our retraining-free framework to estimate how the positions of training samples influence LLMs' capacity for memorization and generalization. We conduct extensive experiments to validate the effectiveness of our retraining-free framework in reproducing the true model performances, and further demonstrate its potential in optimizing LLM training curricula and analyzing the memorization and generalization effects of LLMs.</li>
</ul>

<h3>Title: LatentMove: Towards Complex Human Movement Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Farid Boussaid, Aref Miri Rekavandi, Zinuo Li, Qiuhong Ke, Hamid Laga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22046">https://arxiv.org/abs/2505.22046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22046">https://arxiv.org/pdf/2505.22046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22046]] LatentMove: Towards Complex Human Movement Video Generation(https://arxiv.org/abs/2505.22046)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image-to-video (I2V) generation seeks to produce realistic motion sequences from a single reference image. Although recent methods exhibit strong temporal consistency, they often struggle when dealing with complex, non-repetitive human movements, leading to unnatural deformations. To tackle this issue, we present LatentMove, a DiT-based framework specifically tailored for highly dynamic human animation. Our architecture incorporates a conditional control branch and learnable face/body tokens to preserve consistency as well as fine-grained details across frames. We introduce Complex-Human-Videos (CHV), a dataset featuring diverse, challenging human motions designed to benchmark the robustness of I2V systems. We also introduce two metrics to assess the flow and silhouette consistency of generated videos with their ground truth. Experimental results indicate that LatentMove substantially improves human animation quality--particularly when handling rapid, intricate movements--thereby pushing the boundaries of I2V generation. The code, the CHV dataset, and the evaluation metrics will be available at this https URL --.</li>
</ul>

<h3>Title: A Comparative Study of Fuzzers and Static Analysis Tools for Finding Memory Unsafety in C and C++</h3>
<ul>
<li><strong>Authors: </strong>Keno Hassler, Philipp Görz, Stephan Lipp, Thorsten Holz, Marcel Böhme</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22052">https://arxiv.org/abs/2505.22052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22052">https://arxiv.org/pdf/2505.22052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22052]] A Comparative Study of Fuzzers and Static Analysis Tools for Finding Memory Unsafety in C and C++(https://arxiv.org/abs/2505.22052)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Even today, over 70% of security vulnerabilities in critical software systems result from memory safety violations. To address this challenge, fuzzing and static analysis are widely used automated methods to discover such vulnerabilities. Fuzzing generates random program inputs to identify faults, while static analysis examines source code to detect potential vulnerabilities. Although these techniques share a common goal, they take fundamentally different approaches and have evolved largely independently. In this paper, we present an empirical analysis of five static analyzers and 13 fuzzers, applied to over 100 known security vulnerabilities in C/C++ programs. We measure the number of bug reports generated for each vulnerability to evaluate how the approaches differ and complement each other. Moreover, we randomly sample eight bug-containing functions, manually analyze all bug reports therein, and quantify false-positive rates. We also assess limits to bug discovery, ease of use, resource requirements, and integration into the development process. We find that both techniques discover different types of bugs, but there are clear winners for each. Developers should consider these tools depending on their specific workflow and usability requirements. Based on our findings, we propose future directions to foster collaboration between these research domains.</li>
</ul>

<h3>Title: Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?</h3>
<ul>
<li><strong>Authors: </strong>Yujin Choi, Youngjoo Park, Junyoung Byun, Jaewook Lee, Jinseong Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22061">https://arxiv.org/abs/2505.22061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22061">https://arxiv.org/pdf/2505.22061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22061]] Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?(https://arxiv.org/abs/2505.22061)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) mitigates the hallucination problem in large language models (LLMs) and has proven effective for specific, personalized applications. However, passing private retrieved documents directly to LLMs introduces vulnerability to membership inference attacks (MIAs), which try to determine whether the target datum exists in the private external database or not. Based on the insight that MIA queries typically exhibit high similarity to only one target document, we introduce Mirabel, a similarity-based MIA detection framework designed for the RAG system. With the proposed Mirabel, we show that simple detect-and-hide strategies can successfully obfuscate attackers, maintain data utility, and remain system-agnostic. We experimentally prove its detection and defense against various state-of-the-art MIA methods and its adaptability to existing private RAG systems.</li>
</ul>

<h3>Title: From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Xia, Xingjun Ma, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22067">https://arxiv.org/abs/2505.22067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22067">https://arxiv.org/pdf/2505.22067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22067]] From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving(https://arxiv.org/abs/2505.22067)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ensuring robust and generalizable autonomous driving requires not only broad scenario coverage but also efficient repair of failure cases, particularly those related to challenging and safety-critical scenarios. However, existing scenario generation and selection methods often lack adaptivity and semantic relevance, limiting their impact on performance improvement. In this paper, we propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving systems to self-evolve by repairing failure cases through targeted scenario recommendation. By analyzing performance logs, SERA identifies failure patterns and dynamically retrieves semantically aligned scenarios from a structured bank. An LLM-based reflection mechanism further refines these recommendations to maximize relevance and diversity. The selected scenarios are used for few-shot fine-tuning, enabling targeted adaptation with minimal data. Experiments on the benchmark show that SERA consistently improves key metrics across multiple autonomous driving baselines, demonstrating its effectiveness and generalizability under safety-critical conditions.</li>
</ul>

<h3>Title: Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO</h3>
<ul>
<li><strong>Authors: </strong>Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22068">https://arxiv.org/abs/2505.22068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22068">https://arxiv.org/pdf/2505.22068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22068]] Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO(https://arxiv.org/abs/2505.22068)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Previous study suggest that powerful Large Language Models (LLMs) trained with Reinforcement Learning with Verifiable Rewards (RLVR) only refines reasoning path without improving the reasoning capacity in math tasks while supervised-finetuning(SFT) with distillation can. We study this from the view of Scientific information extraction (SciIE) where LLMs and reasoning LLMs underperforms small Bert-based models. SciIE require both the reasoning and memorization. We argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE. We propose two-stage training with 1. MimicSFT, using structured reasoning templates without needing high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and rule-induced rewards. Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction. Our code is available at this https URL.</li>
</ul>

<h3>Title: The Resurrection of the ReLU</h3>
<ul>
<li><strong>Authors: </strong>Coşku Can Horuz, Geoffrey Kasenbacher, Saya Higuchi, Sebastian Kairat, Jendrik Stoltz, Moritz Pesl, Bernhard A. Moser, Christoph Linse, Thomas Martinetz, Sebastian Otte</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22074">https://arxiv.org/abs/2505.22074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22074">https://arxiv.org/pdf/2505.22074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22074]] The Resurrection of the ReLU(https://arxiv.org/abs/2505.22074)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modeling sophisticated activation functions within deep learning architectures has evolved into a distinct research direction. Functions such as GELU, SELU, and SiLU offer smooth gradients and improved convergence properties, making them popular choices in state-of-the-art models. Despite this trend, the classical ReLU remains appealing due to its simplicity, inherent sparsity, and other advantageous topological characteristics. However, ReLU units are prone to becoming irreversibly inactive - a phenomenon known as the dying ReLU problem - which limits their overall effectiveness. In this work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel, plug-and-play regularizer for deep architectures. SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate that avoids zeroing out gradients. We demonstrate that SUGAR, when paired with a well-chosen surrogate function, substantially enhances generalization performance over convolutional network architectures such as VGG-16 and ResNet-18, providing sparser activations while effectively resurrecting dead ReLUs. Moreover, we show that even in modern architectures like Conv2NeXt and Swin Transformer - which typically employ GELU - substituting these with SUGAR yields competitive and even slightly superior performance. These findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, particularly with appropriate gradient handling, can serve as a strong, versatile revived classic across a broad range of deep learning vision models.</li>
</ul>

<h3>Title: ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation</h3>
<ul>
<li><strong>Authors: </strong>Maja Stahl, Timon Ziegenbein, Joonsuk Park, Henning Wachsmuth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22076">https://arxiv.org/abs/2505.22076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22076">https://arxiv.org/pdf/2505.22076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22076]] ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation(https://arxiv.org/abs/2505.22076)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) to follow instructions has significantly enhanced their ability to tackle unseen tasks. However, despite their strong generalization capabilities, instruction-following LLMs encounter difficulties when dealing with tasks that require domain knowledge. This work introduces a specialized instruction fine-tuning for the domain of computational argumentation (CA). The goal is to enable an LLM to effectively tackle any unseen CA tasks while preserving its generalization capabilities. Reviewing existing CA research, we crafted natural language instructions for 105 CA tasks to this end. On this basis, we developed a CA-specific benchmark for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in solving various CA tasks. We synthesized 52k CA-related instructions, adapting the self-instruct process to train a CA-specialized instruction-following LLM. Our experiments suggest that CA-specialized instruction fine-tuning significantly enhances the LLM on both seen and unseen CA tasks. At the same time, performance on the general NLP tasks of the SuperNI benchmark remains stable.</li>
</ul>

<h3>Title: Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?</h3>
<ul>
<li><strong>Authors: </strong>Shun Sato, Issei Sato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22081">https://arxiv.org/abs/2505.22081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22081">https://arxiv.org/pdf/2505.22081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22081]] Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?(https://arxiv.org/abs/2505.22081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Symbolic regression aims to discover mathematical equations that fit given numerical data. It has been applied in various fields of scientific research, such as producing human-readable expressions that explain physical phenomena. Recently, Neural symbolic regression (NSR) methods that involve Transformers pre-trained on large-scale synthetic datasets have gained attention. While these methods offer advantages such as short inference time, they suffer from low performance, particularly when the number of input variables is large. In this study, we hypothesized that this limitation stems from the memorization bias of Transformers in symbolic regression. We conducted a quantitative evaluation of this bias in Transformers using a synthetic dataset and found that Transformers rarely generate expressions not present in the training data. Additional theoretical analysis reveals that this bias arises from the Transformer's inability to construct expressions compositionally while verifying their numerical validity. We finally examined if tailoring test-time strategies can lead to reduced memorization bias and better performance. We empirically demonstrate that providing additional information to the model at test time can significantly mitigate memorization bias. On the other hand, we also find that reducing memorization bias does not necessarily correlate with improved performance. These findings contribute to a deeper understanding of the limitations of NSR approaches and offer a foundation for designing more robust, generalizable symbolic regression methods. Code is available at this https URL .</li>
</ul>

<h3>Title: Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chunyi Peng, Zhipeng Xu, Zhenghao Liu, Yishan Li, Yukun Yan, Shuo Wang, Zhiyuan Liu, Yu Gu, Minghe Yu, Ge Yu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22095">https://arxiv.org/abs/2505.22095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22095">https://arxiv.org/pdf/2505.22095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22095]] Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning(https://arxiv.org/abs/2505.22095)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in mitigating hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge during generation. Existing MRAG methods typically adopt a static retrieval pipeline that fetches relevant information from multiple Knowledge Bases (KBs), followed by a refinement step. However, these approaches overlook the reasoning and planning capabilities of MLLMs to dynamically determine how to interact with different KBs during the reasoning process. To address this limitation, we propose R1-Router, a novel MRAG framework that learns to decide when and where to retrieve knowledge based on the evolving reasoning state. Specifically, R1-Router can generate follow-up queries according to the current reasoning step, routing these intermediate queries to the most suitable KB, and integrating external knowledge into a coherent reasoning trajectory to answer the original query. Furthermore, we introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored reinforcement learning algorithm that assigns step-specific rewards to optimize the reasoning behavior of MLLMs. Experimental results on various open-domain QA benchmarks across multiple modalities demonstrate that R1-Router outperforms baseline models by over 7%. Further analysis shows that R1-Router can adaptively and effectively leverage diverse KBs, reducing unnecessary retrievals and improving both efficiency and accuracy.</li>
</ul>

<h3>Title: Knowledge Base Construction for Knowledge-Augmented Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Jinheon Baek, Horst Samulowitz, Oktie Hassanzadeh, Dharmashankar Subramanian, Sola Shirai, Alfio Gliozzo, Debarun Bhattacharjya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22096">https://arxiv.org/abs/2505.22096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22096">https://arxiv.org/pdf/2505.22096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22096]] Knowledge Base Construction for Knowledge-Augmented Text-to-SQL(https://arxiv.org/abs/2505.22096)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.</li>
</ul>

<h3>Title: UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images</h3>
<ul>
<li><strong>Authors: </strong>Junhuan Liu, San Jiang, Wei Ge, Wei Huang, Bingxuan Guo, Qingquan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22098">https://arxiv.org/abs/2505.22098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22098">https://arxiv.org/pdf/2505.22098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22098]] UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images(https://arxiv.org/abs/2505.22098)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The primary contribution of this paper is a challenging benchmark dataset, UAVPairs, and a training pipeline designed for match pair retrieval of large-scale UAV images. First, the UAVPairs dataset, comprising 21,622 high-resolution images across 30 diverse scenes, is constructed; the 3D points and tracks generated by SfM-based 3D reconstruction are employed to define the geometric similarity of image pairs, ensuring genuinely matchable image pairs are used for training. Second, to solve the problem of expensive mining cost for global hard negative mining, a batched nontrivial sample mining strategy is proposed, leveraging the geometric similarity and multi-scene structure of the UAVPairs to generate training samples as to accelerate training. Third, recognizing the limitation of pair-based losses, the ranked list loss is designed to improve the discrimination of image retrieval models, which optimizes the global similarity structure constructed from the positive set and negative set. Finally, the effectiveness of the UAVPairs dataset and training pipeline is validated through comprehensive experiments on three distinct large-scale UAV datasets. The experiment results demonstrate that models trained with the UAVPairs dataset and the ranked list loss achieve significantly improved retrieval accuracy compared to models trained on existing datasets or with conventional losses. Furthermore, these improvements translate to enhanced view graph connectivity and higher quality of reconstructed 3D models. The models trained by the proposed approach perform more robustly compared with hand-crafted global features, particularly in challenging repetitively textured scenes and weakly textured scenes. For match pair retrieval of large-scale UAV images, the trained image retrieval models offer an effective solution. The dataset would be made publicly available at this https URL.</li>
</ul>

<h3>Title: MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Kehang Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhi-Qin John Xu, Feiyu Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22101">https://arxiv.org/abs/2505.22101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22101">https://arxiv.org/pdf/2505.22101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22101]] MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models(https://arxiv.org/abs/2505.22101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as foundational infrastructure in the pursuit of Artificial General Intelligence (AGI). Despite their remarkable capabilities in language perception and generation, current LLMs fundamentally lack a unified and structured architecture for handling memory. They primarily rely on parametric memory (knowledge encoded in model weights) and ephemeral activation memory (context-limited runtime states). While emerging methods like Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack lifecycle management and multi-modal integration, limiting their capacity for long-term knowledge evolution. To address this, we introduce MemOS, a memory operating system designed for LLMs that, for the first time, elevates memory to a first-class operational resource. It builds unified mechanisms for representation, organization, and governance across three core memory types: parametric, activation, and plaintext. At its core is the MemCube, a standardized memory abstraction that enables tracking, fusion, and migration of heterogeneous memory, while offering structured, traceable access across tasks and contexts. MemOS establishes a memory-centric execution framework with strong controllability, adaptability, and evolvability. It fills a critical gap in current LLM infrastructure and lays the groundwork for continual adaptation, personalized intelligence, and cross-platform coordination in next-generation intelligent systems.</li>
</ul>

<h3>Title: Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hang Chen, Maoyuan Ye, Peng Yang, Haibin He, Juhua Liu, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22105">https://arxiv.org/abs/2505.22105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22105">https://arxiv.org/pdf/2505.22105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22105]] Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation(https://arxiv.org/abs/2505.22105)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Power transmission corridor hazard segmentation (PTCHS) aims to separate transmission equipment and surrounding hazards from complex background, conveying great significance to maintaining electric power transmission safety. Recently, the Segment Anything Model (SAM) has emerged as a foundational vision model and pushed the boundaries of segmentation tasks. However, SAM struggles to deal with the target objects in complex transmission corridor scenario, especially those with fine structure. In this paper, we propose ELE-SAM, adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt Adapter to achieve better prompt tokens via incorporating global-local features and focusing more on key regions. Subsequently, to tackle the hazard objects with fine structure in complex background, we design a High-Fidelity Mask Decoder by leveraging multi-granularity mask features and then scaling them to a higher resolution. Moreover, to train ELE-SAM and advance this field, we construct the ELE-40K benchmark, the first large-scale and real-world dataset for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K demonstrate the superior performance that ELE-SAM outperforms the baseline model with the average 16.8% mIoU and 20.6% mBIoU performance improvement. Moreover, compared with the state-of-the-art method on HQSeg-44K, the average 2.9% mIoU and 3.8% mBIoU absolute improvements further validate the effectiveness of our method on high-quality generic object segmentation. The source code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Curse of High Dimensionality Issue in Transformer for Long-context Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22107">https://arxiv.org/abs/2505.22107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22107">https://arxiv.org/pdf/2505.22107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22107]] Curse of High Dimensionality Issue in Transformer for Long-context Modeling(https://arxiv.org/abs/2505.22107)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive this http URL is available at this https URL.</li>
</ul>

<h3>Title: Inclusive, Differentially Private Federated Learning for Clinical Data</h3>
<ul>
<li><strong>Authors: </strong>Santhosh Parampottupadam, Melih Coşğun, Sarthak Pati, Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav, Ralf Floca, Klaus Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22108">https://arxiv.org/abs/2505.22108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22108">https://arxiv.org/pdf/2505.22108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22108]] Inclusive, Differentially Private Federated Learning for Clinical Data(https://arxiv.org/abs/2505.22108)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.</li>
</ul>

<h3>Title: Autoregression-free video prediction using diffusion model for mitigating error propagation</h3>
<ul>
<li><strong>Authors: </strong>Woonho Ko, Jin Bok Park, Il Yong Chun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22111">https://arxiv.org/abs/2505.22111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22111">https://arxiv.org/pdf/2505.22111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22111]] Autoregression-free video prediction using diffusion model for mitigating error propagation(https://arxiv.org/abs/2505.22111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing long-term video prediction methods often rely on an autoregressive video prediction mechanism. However, this approach suffers from error propagation, particularly in distant future frames. To address this limitation, this paper proposes the first AutoRegression-Free (ARFree) video prediction framework using diffusion models. Different from an autoregressive video prediction mechanism, ARFree directly predicts any future frame tuples from the context frame tuple. The proposed ARFree consists of two key components: 1) a motion prediction module that predicts a future motion using motion feature extracted from the context frame tuple; 2) a training method that improves motion continuity and contextual consistency between adjacent future frame tuples. Our experiments with two benchmark datasets show that the proposed ARFree video prediction framework outperforms several state-of-the-art video prediction methods.</li>
</ul>

<h3>Title: THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Li, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22113">https://arxiv.org/abs/2505.22113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22113">https://arxiv.org/pdf/2505.22113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22113]] THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models(https://arxiv.org/abs/2505.22113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large reasoning models (LRMs) have achieved impressive performance in complex tasks, often outperforming conventional large language models (LLMs). However, the prevalent issue of overthinking severely limits their computational efficiency. Overthinking occurs when models generate excessive and redundant tokens that contribute little to accurate outcomes, especially in simple tasks, resulting in a significant waste of computational resources. To systematically investigate this issue, we introduce Think-Bench, a benchmark designed to evaluate the reasoning efficiency of LRMs. We also propose novel efficiency metrics and conduct a comprehensive evaluation of various LRMs across multiple dimensions, including the reasoning process, outcome quality, and chain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs exhibit overthinking in handling easy questions, generating unnecessarily lengthy reasoning chains. While many LRMs demonstrate high CoT quality, several suffer from low efficiency. We hope that Think-Bench can serve as a robust foundation for advancing research into LRMs.</li>
</ul>

<h3>Title: BiMi Sheets: Infosheets for bias mitigation methods</h3>
<ul>
<li><strong>Authors: </strong>MaryBeth Defrance, Guillaume Bied, Maarten Buyl, Jefrey Lijffijt, Tijl De Bie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22114">https://arxiv.org/abs/2505.22114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22114">https://arxiv.org/pdf/2505.22114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22114]] BiMi Sheets: Infosheets for bias mitigation methods(https://arxiv.org/abs/2505.22114)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Over the past 15 years, hundreds of bias mitigation methods have been proposed in the pursuit of fairness in machine learning (ML). However, algorithmic biases are domain-, task-, and model-specific, leading to a `portability trap': bias mitigation solutions in one context may not be appropriate in another. Thus, a myriad of design choices have to be made when creating a bias mitigation method, such as the formalization of fairness it pursues, and where and how it intervenes in the ML pipeline. This creates challenges in benchmarking and comparing the relative merits of different bias mitigation methods, and limits their uptake by practitioners. We propose BiMi Sheets as a portable, uniform guide to document the design choices of any bias mitigation method. This enables researchers and practitioners to quickly learn its main characteristics and to compare with their desiderata. Furthermore, the sheets' structure allow for the creation of a structured database of bias mitigation methods. In order to foster the sheets' adoption, we provide a platform for finding and creating BiMi Sheets at this http URL.</li>
</ul>

<h3>Title: Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22116">https://arxiv.org/abs/2505.22116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22116">https://arxiv.org/pdf/2505.22116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22116]] Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model(https://arxiv.org/abs/2505.22116)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at this https URL.</li>
</ul>

<h3>Title: LoKI: Low-damage Knowledge Implanting of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runyu Wang, Peng Ping, Zhengyu Guo, Xiaoye Zhang, Quan Shi, Liting Zhou, Tianbo Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22120">https://arxiv.org/abs/2505.22120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22120">https://arxiv.org/pdf/2505.22120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22120]] LoKI: Low-damage Knowledge Implanting of Large Language Models(https://arxiv.org/abs/2505.22120)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning adapts pretrained models for specific tasks but poses the risk of catastrophic forgetting (CF), where critical knowledge from pre-training is overwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs), while efficient, often sacrifice general capabilities. To address the issue of CF in a general-purpose PEFT framework, we propose \textbf{Lo}w-damage \textbf{K}nowledge \textbf{I}mplanting (\textbf{LoKI}), a PEFT technique that is based on a mechanistic understanding of how knowledge is stored in transformer architectures. In two real-world scenarios, LoKI demonstrates task-specific performance that is comparable to or even surpasses that of full fine-tuning and LoRA-based methods across various model types, while significantly better preserving general capabilities. Our work connects mechanistic insights into LLM knowledge storage with practical fine-tuning objectives, achieving state-of-the-art trade-offs between task specialization and the preservation of general capabilities. Our implementation is publicly available as ready-to-use code\footnote{this https URL}.</li>
</ul>

<h3>Title: SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Yifan Chang, Yukang Feng, Jianwen Sun, Jiaxin Ai, Chuanhao Li, S. Kevin Zhou, Kaipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22126">https://arxiv.org/abs/2505.22126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22126">https://arxiv.org/pdf/2505.22126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22126]] SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model(https://arxiv.org/abs/2505.22126)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.</li>
</ul>

<h3>Title: Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach</h3>
<ul>
<li><strong>Authors: </strong>Alejandro D. Mousist</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22128">https://arxiv.org/abs/2505.22128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22128">https://arxiv.org/pdf/2505.22128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22128]] Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach(https://arxiv.org/abs/2505.22128)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This work addresses mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted to space-based edge computing constraints. Leveraging Sentinel-2 data, our method estimates the defocus kernel and trains a restoration model within a GAN framework, effectively operating without reference images. On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and PSNR by 25.00%, confirming the model's ability to recover lost details when the original clean image is known. On IMAGIN-e, where no reference images exist, perceptual quality metrics indicate a substantial enhancement, with NIQE improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard restoration. The approach is currently deployed aboard the IMAGIN-e mission, demonstrating its practical application in an operational space environment. By efficiently handling high-resolution images under edge computing constraints, the method enables applications such as water body segmentation and contour detection while maintaining processing viability despite resource limitations.</li>
</ul>

<h3>Title: What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?</h3>
<ul>
<li><strong>Authors: </strong>Jinhong Ni, Chang-Bin Zhang, Qiang Zhang, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22129">https://arxiv.org/abs/2505.22129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22129">https://arxiv.org/pdf/2505.22129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22129]] What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?(https://arxiv.org/abs/2505.22129)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code will be released.</li>
</ul>

<h3>Title: EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyang Wu, Xinze Li, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Minghe Yu, Cheng Yang, Yu Gu, Ge Yu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22131">https://arxiv.org/abs/2505.22131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22131">https://arxiv.org/pdf/2505.22131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22131]] EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning(https://arxiv.org/abs/2505.22131)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong reasoning capabilities and achieved promising results in mathematical problem-solving tasks. Learning from errors offers the potential to further enhance the performance of LLMs during Supervised Fine-Tuning (SFT). However, the errors in synthesized solutions are typically gathered from sampling trails, making it challenging to generate solution errors for each mathematical problem. This paper introduces the Error-IndUced LEaRning (EULER) model, which aims to develop an error exposure model that generates high-quality solution errors to enhance the mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the error exposure model to increase the generation probability of self-made solution errors while utilizing solutions produced by a superior LLM to regularize the generation quality. Our experiments across various mathematical problem datasets demonstrate the effectiveness of the EULER model, achieving an improvement of over 4% compared to all baseline models. Further analysis reveals that EULER is capable of synthesizing more challenging and educational solution errors, which facilitate both the training and inference processes of LLMs. All codes are available at this https URL.</li>
</ul>

<h3>Title: RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yuichiro Hoshino, Hideyuki Tachibana, Muneyoshi Inahara, Hiroto Takegawa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22135">https://arxiv.org/abs/2505.22135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22135">https://arxiv.org/pdf/2505.22135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22135]] RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding(https://arxiv.org/abs/2505.22135)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hybrid models combining Transformers and State Space Models (SSMs) are promising for balancing performance and efficiency. However, optimizing these hybrid models, particularly by addressing the potential redundancy inherent within the Transformer components, remains a significant challenge. In this paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that uses self-speculative decoding as a diagnostic tool to identify redundant attention layers within the model. These identified layers are then selectively replaced with SSM components, followed by targeted (self-)distillation. Specifically, RAD focuses knowledge transfer on the components identified as redundant, considering architectural changes and specific weight initialization strategies. We experimentally demonstrate that self-distillation using RAD significantly surpasses the performance of the original base model on mathematical and coding tasks. Furthermore, RAD is also effective in standard knowledge distillation settings, achieving up to approximately 2x faster convergence compared to baseline methods. Notably, while a baseline model distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and 22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and 28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers a new pathway for efficient optimization and performance enhancement in the distillation of hybrid models.</li>
</ul>

<h3>Title: Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments</h3>
<ul>
<li><strong>Authors: </strong>Marc Feger, Katarina Boland, Stefan Dietze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22137">https://arxiv.org/abs/2505.22137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22137">https://arxiv.org/pdf/2505.22137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22137]] Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments(https://arxiv.org/abs/2505.22137)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.</li>
</ul>

<h3>Title: FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing</h3>
<ul>
<li><strong>Authors: </strong>Guanwen Feng, Zhiyuan Ma, Yunan Li, Junwei Jing, Jiahao Yang, Qiguang Miao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22141">https://arxiv.org/abs/2505.22141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22141">https://arxiv.org/pdf/2505.22141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22141]] FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing(https://arxiv.org/abs/2505.22141)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in audio-driven talking head generation have achieved impressive results in lip synchronization and emotional expression. However, they largely overlook the crucial task of facial attribute editing. This capability is crucial for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service. In these key domains, the flexible adjustment of visual attributes-such as hairstyle, accessories, and subtle facial features is essential for aligning with user preferences, reflecting diverse brand identities, and adapting to varying contextual demands. In this paper, we present FaceEditTalker, a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos. Our method consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator. This design ensures temporal coherence, visual fidelity, and identity preservation across frames. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art approaches in lip-sync accuracy, video quality, and attribute controllability. Project page: this https URL</li>
</ul>

<h3>Title: Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL</h3>
<ul>
<li><strong>Authors: </strong>Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22151">https://arxiv.org/abs/2505.22151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22151">https://arxiv.org/pdf/2505.22151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22151]] Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL(https://arxiv.org/abs/2505.22151)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication.</li>
</ul>

<h3>Title: Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance</h3>
<ul>
<li><strong>Authors: </strong>Chao Tian, Chao Yang, Guoqing Zhu, Qiang Wang, Zhenyu He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22154">https://arxiv.org/abs/2505.22154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22154">https://arxiv.org/pdf/2505.22154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22154]] Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance(https://arxiv.org/abs/2505.22154)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images to complement RGB data, improving robustness in challenging conditions. Traditional RGB-T detectors assume balanced training data, where both modalities contribute equally. However, in real-world scenarios, modality degradation-due to environmental factors or technical issues-can lead to extreme modality imbalance, causing out-of-distribution (OOD) issues during testing and disrupting model convergence during training. This paper addresses these challenges by proposing a novel base-and-auxiliary detector architecture. We introduce a modality interaction module to adaptively weigh modalities based on their quality and handle imbalanced samples effectively. Additionally, we leverage modality pseudo-degradation to simulate real-world imbalances in training data. The base detector, trained on high-quality pairs, provides a consistency constraint for the auxiliary detector, which receives degraded samples. This framework enhances model robustness, ensuring reliable performance even under severe modality degradation. Experimental results demonstrate the effectiveness of our method in handling extreme modality imbalances~(decreasing the Missing Rate by 55%) and improving performance across various baseline detectors.</li>
</ul>

<h3>Title: InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22156">https://arxiv.org/abs/2505.22156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22156">https://arxiv.org/pdf/2505.22156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22156]] InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing(https://arxiv.org/abs/2505.22156)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs' ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model's context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method.</li>
</ul>

<h3>Title: Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy</h3>
<ul>
<li><strong>Authors: </strong>Paramita Mirza, Lucas Weber, Fabian Küch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22157">https://arxiv.org/abs/2505.22157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22157">https://arxiv.org/pdf/2505.22157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22157]] Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy(https://arxiv.org/abs/2505.22157)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent work shows that post-training datasets for LLMs can be substantially downsampled without noticeably deteriorating performance. However, data selection often incurs high computational costs or is limited to narrow domains. In this paper, we demonstrate that data selection can be both -- efficient and universal -- by using a multi-step pipeline in which we efficiently bin data points into groups, estimate quality using specialized models, and score difficulty with a robust, lightweight method. Task-based categorization allows us to control the composition of our final data -- crucial for finetuning multi-purpose models. To guarantee diversity, we improve upon previous work using embedding models and a clustering algorithm. This integrated strategy enables high-performance fine-tuning with minimal overhead.</li>
</ul>

<h3>Title: The informativeness of the gradient revisited</h3>
<ul>
<li><strong>Authors: </strong>Rustem Takhanov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22158">https://arxiv.org/abs/2505.22158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22158">https://arxiv.org/pdf/2505.22158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22158]] The informativeness of the gradient revisited(https://arxiv.org/abs/2505.22158)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In the past decade gradient-based deep learning has revolutionized several applications. However, this rapid advancement has highlighted the need for a deeper theoretical understanding of its limitations. Research has shown that, in many practical learning tasks, the information contained in the gradient is so minimal that gradient-based methods require an exceedingly large number of iterations to achieve success. The informativeness of the gradient is typically measured by its variance with respect to the random selection of a target function from a hypothesis class. We use this framework and give a general bound on the variance in terms of a parameter related to the pairwise independence of the target function class and the collision entropy of the input distribution. Our bound scales as $ \tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $ \tilde{\mathcal{O}} $ hides factors related to the regularity of the learning model and the loss function, $ \varepsilon $ measures the pairwise independence of the target function class and $\mathcal{E}_c$ is the collision entropy of the input distribution. To demonstrate the practical utility of our bound, we apply it to the class of Learning with Errors (LWE) mappings and high-frequency functions. In addition to the theoretical analysis, we present experiments to understand better the nature of recent deep learning-based attacks on LWE.</li>
</ul>

<h3>Title: Accountable, Scalable and DoS-resilient Secure Vehicular Communication</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Jin, Panos Papadimitratos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22162">https://arxiv.org/abs/2505.22162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22162">https://arxiv.org/pdf/2505.22162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22162]] Accountable, Scalable and DoS-resilient Secure Vehicular Communication(https://arxiv.org/abs/2505.22162)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Paramount to vehicle safety, broadcasted Cooperative Awareness Messages (CAMs) and Decentralized Environmental Notification Messages (DENMs) are pseudonymously authenticated for security and privacy protection, with each node needing to have all incoming messages validated within an expiration deadline. This creates an asymmetry that can be easily exploited by external adversaries to launch a clogging Denial of Service (DoS) attack: each forged VC message forces all neighboring nodes to cryptographically validate it; at increasing rates, easy to generate forged messages gradually exhaust processing resources and severely degrade or deny timely validation of benign CAMs/DENMs. The result can be catastrophic when awareness of neighbor vehicle positions or critical reports are missed. We address this problem making the standardized VC pseudonymous authentication DoS-resilient. We propose efficient cryptographic constructs, which we term message verification facilitators, to prioritize processing resources for verification of potentially valid messages among bogus messages and verify multiple messages based on one signature verification. Any message acceptance is strictly based on public-key based message authentication/verification for accountability, i.e., non-repudiation is not sacrificed, unlike symmetric key based approaches. This further enables drastic misbehavior detection, also exploiting the newly introduced facilitators, based on probabilistic signature verification and cross-checking over multiple facilitators verifying the same message; while maintaining verification latency low even when under attack, trading off modest communication overhead. Our facilitators can also be used for efficient discovery and verification of DENM or any event-driven message, including misbehavior evidence used for our scheme.</li>
</ul>

<h3>Title: Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes</h3>
<ul>
<li><strong>Authors: </strong>Bocheng Li, Zhujin Gao, Linli Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22165">https://arxiv.org/abs/2505.22165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22165">https://arxiv.org/pdf/2505.22165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22165]] Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes(https://arxiv.org/abs/2505.22165)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.</li>
</ul>

<h3>Title: Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Weilun Feng, Chuanguang Yang, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, Michele Magno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22167">https://arxiv.org/abs/2505.22167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22167">https://arxiv.org/pdf/2505.22167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22167]] Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers(https://arxiv.org/abs/2505.22167)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion transformers (DiT) have demonstrated exceptional performance in video generation. However, their large number of parameters and high computational complexity limit their deployment on edge devices. Quantization can reduce storage requirements and accelerate inference by lowering the bit-width of model parameters. Yet, existing quantization methods for image generation models do not generalize well to video generation tasks. We identify two primary challenges: the loss of information during quantization and the misalignment between optimization objectives and the unique requirements of video generation. To address these challenges, we present Q-VDiT, a quantization framework specifically designed for video DiT models. From the quantization perspective, we propose the Token-aware Quantization Estimator (TQE), which compensates for quantization errors in both the token and feature dimensions. From the optimization perspective, we introduce Temporal Maintenance Distillation (TMD), which preserves the spatiotemporal correlations between frames and enables the optimization of each frame with respect to the overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40, setting a new benchmark and outperforming current state-of-the-art quantization methods by 1.9$\times$. Code will be available at this https URL.</li>
</ul>

<h3>Title: ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments</h3>
<ul>
<li><strong>Authors: </strong>Gili Lior, Eliya Habba, Shahar Levy, Avi Caciularu, Gabriel Stanovsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22169">https://arxiv.org/abs/2505.22169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22169">https://arxiv.org/pdf/2505.22169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22169]] ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments(https://arxiv.org/abs/2505.22169)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LLMs are highly sensitive to prompt phrasing, yet standard benchmarks typically report performance using a single prompt, raising concerns about the reliability of such evaluations. In this work, we argue for a stochastic method of moments evaluation over the space of meaning-preserving prompt perturbations. We introduce a formal definition of reliable evaluation that accounts for prompt sensitivity, and suggest ReliableEval - a method for estimating the number of prompt resamplings needed to obtain meaningful results. Using our framework, we stochastically evaluate five frontier LLMs and find that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit substantial prompt sensitivity. Our approach is model-, task-, and metric-agnostic, offering a recipe for meaningful and robust LLM evaluation.</li>
</ul>

<h3>Title: Reverse Preference Optimization for Complex Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Xiang Huang, Ting-En Lin, Feiteng Fang, Yuchuan Wu, Hangyu Li, Yuzhong Qu, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22172">https://arxiv.org/abs/2505.22172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22172">https://arxiv.org/pdf/2505.22172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22172]] Reverse Preference Optimization for Complex Instruction Following(https://arxiv.org/abs/2505.22172)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Instruction following (IF) is a critical capability for large language models (LLMs). However, handling complex instructions with multiple constraints remains challenging. Previous methods typically select preference pairs based on the number of constraints they satisfy, introducing noise where chosen examples may fail to follow some constraints and rejected examples may excel in certain respects over the chosen ones. To address the challenge of aligning with multiple preferences, we propose a simple yet effective method called Reverse Preference Optimization (RPO). It mitigates noise in preference pairs by dynamically reversing the constraints within the instruction to ensure the chosen response is perfect, alleviating the burden of extensive sampling and filtering to collect perfect responses. Besides, reversal also enlarges the gap between chosen and rejected responses, thereby clarifying the optimization direction and making it more robust to noise. We evaluate RPO on two multi-turn IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively. Moreover, RPO scales effectively across model sizes (8B to 70B parameters), with the 70B RPO model surpassing GPT-4o.</li>
</ul>

<h3>Title: TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Vihang Pancholi, Jainit Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22176">https://arxiv.org/abs/2505.22176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22176">https://arxiv.org/pdf/2505.22176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22176]] TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation(https://arxiv.org/abs/2505.22176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating tables qualitatively & quantitatively presents a significant challenge, as traditional metrics often fail to capture nuanced structural and content discrepancies. To address this, we introduce a novel, methodical rubric integrating multi-level structural descriptors with fine-grained contextual quantification, thereby establishing a robust foundation for comprehensive table comparison. Building on this foundation, we propose TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval initially aligns reference tables structurally via TabAlign & subsequently conducts a systematic semantic and syntactic comparison using TabCompare; this approach clarifies the evaluation process and pinpoints subtle discrepancies overlooked by conventional methods. The efficacy of this framework is assessed using TabXBench, a novel, diverse, multi-domain benchmark we developed, featuring realistic table perturbations and human-annotated assessments. Finally, a systematic analysis of existing evaluation methods through sensitivity-specificity trade-offs demonstrates the qualitative and quantitative effectiveness of TabXEval across diverse table-related tasks and domains, paving the way for future innovations in explainable table evaluation.</li>
</ul>

<h3>Title: Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design</h3>
<ul>
<li><strong>Authors: </strong>Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao, Conghui Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22179">https://arxiv.org/abs/2505.22179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22179">https://arxiv.org/pdf/2505.22179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22179]] Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design(https://arxiv.org/abs/2505.22179)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$. Code available at this https URL.</li>
</ul>

<h3>Title: S2AFormer: Strip Self-Attention for Efficient Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Guoan Xu, Wenfeng Huang, Wenjing Jia, Jiamao Li, Guangwei Gao, Guo-Jun Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22195">https://arxiv.org/abs/2505.22195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22195">https://arxiv.org/pdf/2505.22195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22195]] S2AFormer: Strip Self-Attention for Efficient Vision Transformer(https://arxiv.org/abs/2505.22195)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformer (ViT) has made significant advancements in computer vision, thanks to its token mixer's sophisticated ability to capture global dependencies between all tokens. However, the quadratic growth in computational demands as the number of tokens increases limits its practical efficiency. Although recent methods have combined the strengths of convolutions and self-attention to achieve better trade-offs, the expensive pairwise token affinity and complex matrix operations inherent in self-attention remain a bottleneck. To address this challenge, we propose S2AFormer, an efficient Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We design simple yet effective Hybrid Perception Blocks (HPBs) to effectively integrate the local perception capabilities of CNNs with the global context modeling of Transformer's attention mechanisms. A key innovation of SSA lies in its reducing the spatial dimensions of $K$ and $V$ while compressing the channel dimensions of $Q$ and $K$. This design significantly reduces computational overhead while preserving accuracy, striking an optimal balance between efficiency and effectiveness. We evaluate the robustness and efficiency of S2AFormer through extensive experiments on multiple vision benchmarks, including ImageNet-1k for image classification, ADE20k for semantic segmentation, and COCO for object detection and instance segmentation. Results demonstrate that S2AFormer achieves significant accuracy gains with superior efficiency in both GPU and non-GPU environments, making it a strong candidate for efficient vision Transformers.</li>
</ul>

<h3>Title: Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Hu, Zhibin Duan, Bo Chen, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22199">https://arxiv.org/abs/2505.22199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22199">https://arxiv.org/pdf/2505.22199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22199]] Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer(https://arxiv.org/abs/2505.22199)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural networks leads to a multifaceted problem, where various localized explanation techniques reveal that multiple unrelated features influence the decisions, thereby undermining interpretability. To address these challenges, we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates deep neural networks as a conditional Bayesian non-negative factor analysis. By leveraging stochastic latent variables, the BNDL can model complex dependencies and provide robust uncertainty estimation. Moreover, the sparsity and non-negativity of the latent variables encourage the model to learn disentangled representations and decision layers, thereby improving interpretability. We also offer theoretical guarantees that BNDL can achieve effective disentangled learning. In addition, we developed a corresponding variational inference method utilizing a Weibull variational inference network to approximate the posterior distribution of the latent variables. Our experimental results demonstrate that with enhanced disentanglement capabilities, BNDL not only improves the model's accuracy but also provides reliable uncertainty estimation and improved interpretability.</li>
</ul>

<h3>Title: Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuzhen Huang, Weihao Zeng, Xingshan Zeng, Qi Zhu, Junxian He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22203">https://arxiv.org/abs/2505.22203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22203">https://arxiv.org/pdf/2505.22203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22203]] Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning(https://arxiv.org/abs/2505.22203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.</li>
</ul>

<h3>Title: A Survey on Training-free Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Naomi Kombol, Ivan Martinović, Siniša Šegvić</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22209">https://arxiv.org/abs/2505.22209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22209">https://arxiv.org/pdf/2505.22209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22209]] A Survey on Training-free Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2505.22209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is one of the most fundamental tasks in image understanding with a long history of research, and subsequently a myriad of different approaches. Traditional methods strive to train models up from scratch, requiring vast amounts of computational resources and training data. In the advent of moving to open-vocabulary semantic segmentation, which asks models to classify beyond learned categories, large quantities of finely annotated data would be prohibitively expensive. Researchers have instead turned to training-free methods where they leverage existing models made for tasks where data is more easily acquired. Specifically, this survey will cover the history, nuance, idea development and the state-of-the-art in training-free open-vocabulary semantic segmentation that leverages existing multi-modal classification models. We will first give a preliminary on the task definition followed by an overview of popular model archetypes and then spotlight over 30 approaches split into broader research branches: purely CLIP-based, those leveraging auxiliary visual foundation models and ones relying on generative methods. Subsequently, we will discuss the limitations and potential problems of current research, as well as provide some underexplored ideas for future study. We believe this survey will serve as a good onboarding read to new researchers and spark increased interest in the area.</li>
</ul>

<h3>Title: Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunsoo Kim, Jinge Wu, Su-Hwan Kim, Pardeep Vasudev, Jiashu Shen, Honghan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22222">https://arxiv.org/abs/2505.22222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22222">https://arxiv.org/pdf/2505.22222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22222]] Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation(https://arxiv.org/abs/2505.22222)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still suffer from hallucinations and clinically significant errors, limiting their reliability in real-world applications. In this study, we propose Look & Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into the LLM prompting framework. Unlike conventional fine-tuning, L&M leverages in-context learning to achieve substantial performance gains without retraining. When evaluated across multiple domain-specific and general-purpose models, L&M demonstrates significant gains, including a 1.2% improvement in overall metrics (this http URL) for CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for LLaVA-Med. General-purpose models also benefit from L&M combined with in-context learning, with LLaVA-OV achieving an 87.3% clinical average performance (this http URL)-the highest among all models, even surpassing those explicitly trained for CXR report generation. Expert evaluations further confirm that L&M reduces clinically significant errors (by 0.43 average errors per report), such as false predictions and omissions, enhancing both accuracy and reliability. These findings highlight L&M's potential as a scalable and efficient solution for AI-assisted radiology, paving the way for improved diagnostic workflows in low-resource clinical settings.</li>
</ul>

<h3>Title: Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhisong Wang, Yiwen Ye, Ziyang Chen, Yong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22230">https://arxiv.org/abs/2505.22230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22230">https://arxiv.org/pdf/2505.22230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22230]] Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation(https://arxiv.org/abs/2505.22230)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Weakly supervised semantic segmentation (WSSS) in medical imaging struggles with effectively using sparse annotations. One promising direction for WSSS leverages gaze annotations, captured via eye trackers that record regions of interest during diagnostic procedures. However, existing gaze-based methods, such as GazeMedSeg, do not fully exploit the rich information embedded in gaze data. In this paper, we propose GradTrack, a framework that utilizes physicians' gaze track, including fixation points, durations, and temporal order, to enhance WSSS performance. GradTrack comprises two key components: Gaze Track Map Generation and Track Attention, which collaboratively enable progressive feature refinement through multi-level gaze supervision during the decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets demonstrate that GradTrack consistently outperforms existing gaze-based methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively. Moreover, GradTrack significantly narrows the performance gap with fully supervised models such as nnUNet.</li>
</ul>

<h3>Title: Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Ali, Manuel Brack, Max Lübbering, Elias Wendt, Abbas Goher Khan, Richard Rutmann, Alex Jude, Maurice Kraus, Alexander Arno Weber, Felix Stollenwerk, David Kaczér, Florian Mai, Lucie Flek, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Patrick Schramowski, Michael Fromm, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22232">https://arxiv.org/abs/2505.22232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22232">https://arxiv.org/pdf/2505.22232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22232]] Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models(https://arxiv.org/abs/2505.22232)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.</li>
</ul>

<h3>Title: BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain</h3>
<ul>
<li><strong>Authors: </strong>Yunsoo Kim, Yusuf Abdulle, Honghan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22240">https://arxiv.org/abs/2505.22240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22240">https://arxiv.org/pdf/2505.22240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22240]] BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain(https://arxiv.org/abs/2505.22240)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Biomedical reasoning often requires traversing interconnected relationships across entities such as drugs, diseases, and proteins. Despite the increasing prominence of large language models (LLMs), existing benchmarks lack the ability to evaluate multi-hop reasoning in the biomedical domain, particularly for queries involving one-to-many and many-to-many relationships. This gap leaves the critical challenges of biomedical multi-hop reasoning underexplored. To address this, we introduce BioHopR, a novel benchmark designed to evaluate multi-hop, multi-answer reasoning in structured biomedical knowledge graphs. Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop reasoning tasks that reflect real-world biomedical complexities. Evaluations of state-of-the-art models reveal that O3-mini, a proprietary reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on 2-hop tasks, outperforming proprietary models such as GPT4O and open-source biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all models exhibit significant declines in multi-hop performance, underscoring the challenges of resolving implicit reasoning steps in the biomedical domain. By addressing the lack of benchmarks for multi-hop reasoning in biomedical domain, BioHopR sets a new standard for evaluating reasoning capabilities and highlights critical gaps between proprietary and open-source models while paving the way for future advancements in biomedical LLMs.</li>
</ul>

<h3>Title: StateSpaceDiffuser: Bringing Long Context to Diffusion World Models</h3>
<ul>
<li><strong>Authors: </strong>Nedko Savov, Naser Kazemi, Deheng Zhang, Danda Pani Paudel, Xi Wang, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22246">https://arxiv.org/abs/2505.22246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22246">https://arxiv.org/pdf/2505.22246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22246]] StateSpaceDiffuser: Bringing Long Context to Diffusion World Models(https://arxiv.org/abs/2505.22246)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>World models have recently become promising tools for predicting realistic visuals based on actions in complex environments. However, their reliance on a short sequence of observations causes them to quickly lose track of context. As a result, visual consistency breaks down after just a few steps, and generated scenes no longer reflect information seen earlier. This limitation of the state-of-the-art diffusion-based world models comes from their lack of a lasting environment state. To address this problem, we introduce StateSpaceDiffuser, where a diffusion model is enabled to perform on long-context tasks by integrating a sequence representation from a state-space model (Mamba), representing the entire interaction history. This design restores long-term memory without sacrificing the high-fidelity synthesis of diffusion models. To rigorously measure temporal consistency, we develop an evaluation protocol that probes a model's ability to reinstantiate seen content in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser significantly outperforms a strong diffusion-only baseline, maintaining a coherent visual context for an order of magnitude more steps. It delivers consistent views in both a 2D maze navigation and a complex 3D environment. These results establish that bringing state-space representations into diffusion models is highly effective in demonstrating both visual details and long-term memory.</li>
</ul>

<h3>Title: YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mingzhuang Wang, Yvyang Li, Xiyang Zhang, Fei Tan, Qi Shi, Guotao Zhang, Siqi Chen, Yufei Liu, Lei Lei, Ming Zhou, Qiang Lin, Hongqiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22250">https://arxiv.org/abs/2505.22250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22250">https://arxiv.org/pdf/2505.22250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22250]] YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction(https://arxiv.org/abs/2505.22250)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Coral reefs, crucial for sustaining marine biodiversity and ecological processes (e.g., nutrient cycling, habitat provision), face escalating threats, underscoring the need for efficient monitoring. Coral reef ecological monitoring faces dual challenges of low efficiency in manual analysis and insufficient segmentation accuracy in complex underwater scenarios. This study develops the YH-OSI system, establishing an intelligent framework centered on the Multimodal Large Model (MLLM) for "object detection-semantic segmentation-prior input". The system uses the object detection module (mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the segment module to complete pixel-level segmentation in low-light and densely occluded scenarios. The segmentation masks and finetuned classification instructions are fed into the Qwen2-VL-based multimodal model as prior inputs, achieving a genus-level classification accuracy of 88% and simultaneously extracting core ecological metrics. Meanwhile, the system retains the scalability of the multimodal model through standardized interfaces, laying a foundation for future integration into multimodal agent-based underwater robots and supporting the full-process automation of "image acquisition-prior generation-real-time analysis."</li>
</ul>

<h3>Title: Train Sparse Autoencoders Efficiently by Utilizing Features Correlation</h3>
<ul>
<li><strong>Authors: </strong>Vadim Kurochkin, Yaroslav Aksenov, Daniil Laptev, Daniil Gavrilov, Nikita Balagansky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22255">https://arxiv.org/abs/2505.22255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22255">https://arxiv.org/pdf/2505.22255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22255]] Train Sparse Autoencoders Efficiently by Utilizing Features Correlation(https://arxiv.org/abs/2505.22255)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.</li>
</ul>

<h3>Title: Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yongcan Yu, Yanbo Wang, Ran He, Jian Liang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22271">https://arxiv.org/abs/2505.22271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22271">https://arxiv.org/pdf/2505.22271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22271]] Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models(https://arxiv.org/abs/2505.22271)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>While (multimodal) large language models (LLMs) have attracted widespread attention due to their exceptional capabilities, they remain vulnerable to jailbreak attacks. Various defense methods are proposed to defend against jailbreak attacks, however, they are often tailored to specific types of jailbreak attacks, limiting their effectiveness against diverse adversarial strategies. For instance, rephrasing-based defenses are effective against text adversarial jailbreaks but fail to counteract image-based attacks. To overcome these limitations, we propose a universal defense framework, termed Test-time IMmunization (TIM), which can adaptively defend against various jailbreak attacks in a self-evolving way. Specifically, TIM initially trains a gist token for efficient detection, which it subsequently applies to detect jailbreak activities during inference. When jailbreak attempts are identified, TIM implements safety fine-tuning using the detected jailbreak instructions paired with refusal answers. Furthermore, to mitigate potential performance degradation in the detector caused by parameter updates during safety fine-tuning, we decouple the fine-tuning process from the detection module. Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy of TIM.</li>
</ul>

<h3>Title: Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review</h3>
<ul>
<li><strong>Authors: </strong>Zihan Xu, Haotian Ma, Gongbo Zhang, Yihao Ding, Chunhua Weng, Yifan Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22280">https://arxiv.org/abs/2505.22280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22280">https://arxiv.org/pdf/2505.22280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22280]] Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review(https://arxiv.org/abs/2505.22280)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Evidence-based medicine (EBM) is at the forefront of modern healthcare, emphasizing the use of the best available scientific evidence to guide clinical decisions. Due to the sheer volume and rapid growth of medical literature and the high cost of curation, there is a critical need to investigate Natural Language Processing (NLP) methods to identify, appraise, synthesize, summarize, and disseminate evidence in EBM. This survey presents an in-depth review of 129 research studies on leveraging NLP for EBM, illustrating its pivotal role in enhancing clinical decision-making processes. The paper systematically explores how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise, Apply, and Assess. The review not only identifies current limitations within the field but also proposes directions for future research, emphasizing the potential for NLP to revolutionize EBM by refining evidence extraction, evidence synthesis, appraisal, summarization, enhancing data comprehensibility, and facilitating a more efficient clinical workflow.</li>
</ul>

<h3>Title: From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Junyu Fan, Chuanlin Liao, Yi Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22284">https://arxiv.org/abs/2505.22284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22284">https://arxiv.org/pdf/2505.22284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22284]] From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration(https://arxiv.org/abs/2505.22284)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to achieve image restoration caused by multiple degradation patterns via a single model with unified parameters. Although existing AiOIR approaches obtain promising performance in closed and controlled scenarios, they still suffered from considerable performance reduction in real-world scenarios since the gap of data distributions between the training samples (source domain) and real-world test samples (target domain) can lead inferior degradation awareness ability. To address this issue, a Unified Domain-Adaptive Image Restoration (UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the learned knowledge from source domain to target domain. To improve the degradation identification, a codebook is designed to learn a group of discrete embeddings to denote the degradation patterns, and the cross-sample contrastive learning mechanism is further proposed to capture shared features from different samples of certain degradation. To bridge the data gap, a domain adaptation strategy is proposed to build the feature projection between the source and target domains by dynamically aligning their codebook embeddings, and a correlation alignment-based test-time adaptation mechanism is designed to fine-tune the alignment discrepancies by tightening the degradation embeddings to the corresponding cluster center in the source domain. Experimental results on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art performance for the AiOIR task. Most importantly, the feature cluster validate the degradation identification under unknown conditions, and qualitative comparisons showcase robust generalization to real-world scenarios.</li>
</ul>

<h3>Title: Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Neil Sinha, P. Julius Kuehn, Johannes Koppe, Arjan Kuijper, Michael Weinmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22291">https://arxiv.org/abs/2505.22291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22291">https://arxiv.org/pdf/2505.22291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22291]] Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data(https://arxiv.org/abs/2505.22291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The preservation of early visual arts, particularly color photographs, is challenged by deterioration caused by aging and improper storage, leading to issues like blurring, scratches, color bleeding, and fading defects. In this paper, we present the first approach for the automatic removal of greening color defects in digitized autochrome photographs. Our main contributions include a method based on synthetic dataset generation and the use of generative AI with a carefully designed loss function for the restoration of visual arts. To address the lack of suitable training datasets for analyzing greening defects in damaged autochromes, we introduce a novel approach for accurately simulating such defects in synthetic data. We also propose a modified weighted loss function for the ChaIR method to account for color imbalances between defected and non-defected areas. While existing methods struggle with accurately reproducing original colors and may require significant manual effort, our method allows for efficient restoration with reduced time requirements.</li>
</ul>

<h3>Title: Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Samuel Frontull, Thomas Ströhle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22293">https://arxiv.org/abs/2505.22293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22293">https://arxiv.org/pdf/2505.22293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22293]] Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs(https://arxiv.org/abs/2505.22293)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong capabilities in multilingual machine translation, sometimes even outperforming traditional neural systems. However, previous research has highlighted the challenges of using LLMs, particularly with prompt engineering, for low-resource languages. In this work, we introduce Fragment-Shot Prompting, a novel in-context learning method that segments input and retrieves translation examples based on syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables translation without direct parallel data. We evaluate these methods using GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot Prompting is effective for translating into and between the studied low-resource languages, with syntactic coverage positively correlating with translation quality; (2) Models with stronger reasoning abilities make more effective use of retrieved knowledge, generally produce better translations, and enable Pivoted Fragment-Shot to significantly improve translation quality between the Ladin variants; and (3) prompt engineering offers limited, if any, improvements when translating from a low-resource to a high-resource language, where zero-shot prompting already yields satisfactory results. We publicly release our code and the retrieval corpora.</li>
</ul>

<h3>Title: Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Yifan Lu, Jing Li, Yigeng Zhou, Yihui Zhang, Wenya Wang, Xiucheng Li, Meishan Zhang, Fangming Liu, Jun Yu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22298">https://arxiv.org/abs/2505.22298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22298">https://arxiv.org/pdf/2505.22298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22298]] Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing(https://arxiv.org/abs/2505.22298)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit impressive language capabilities but remain vulnerable to malicious prompts and jailbreaking attacks. Existing knowledge editing methods for LLM detoxification face two major challenges. First, they often rely on entity-specific localization, making them ineffective against adversarial inputs without explicit entities. Second, these methods suffer from over-editing, where detoxified models reject legitimate queries, compromising overall performance. In this paper, we propose ToxEdit, a toxicity-aware knowledge editing approach that dynamically detects toxic activation patterns during forward propagation. It then routes computations through adaptive inter-layer pathways to mitigate toxicity effectively. This design ensures precise toxicity mitigation while preserving LLMs' general capabilities. To more accurately assess over-editing, we also enhance the SafeEdit benchmark by incorporating instruction-following evaluation tasks. Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms previous state-of-the-art methods in both detoxification performance and safeguarding general capabilities of LLMs.</li>
</ul>

<h3>Title: CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction</h3>
<ul>
<li><strong>Authors: </strong>Jiali Chen, Xusen Hei, HongFei Liu, Yuancheng Wei, Zikun Deng, Jiayuan Xie, Yi Cai, Li Qing</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22304">https://arxiv.org/abs/2505.22304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22304">https://arxiv.org/pdf/2505.22304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22304]] CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction(https://arxiv.org/abs/2505.22304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computer-aided design (CAD) is crucial in prototyping 3D objects through geometric instructions (i.e., CAD programs). In practical design workflows, designers often engage in time-consuming reviews and refinements of these prototypes by comparing them with reference images. To bridge this gap, we introduce the CAD review task to automatically detect and correct potential errors, ensuring consistency between the constructed 3D objects and reference images. However, recent advanced multimodal large language models (MLLMs) struggle to recognize multiple geometric components and perform spatial geometric operations within the CAD program, leading to inaccurate reviews. In this paper, we propose the CAD program repairer (ReCAD) framework to effectively detect program errors and provide helpful feedback on error correction. Additionally, we create a dataset, CADReview, consisting of over 20K program-image pairs, with diverse errors for the CAD review task. Extensive experiments demonstrate that our ReCAD significantly outperforms existing MLLMs, which shows great potential in design applications.</li>
</ul>

<h3>Title: Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zehua Chen, Yuyang Miao, Liyuan Wang, Luyun Fan, Danilo P. Mandic, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22306">https://arxiv.org/abs/2505.22306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22306">https://arxiv.org/pdf/2505.22306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22306]] Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer(https://arxiv.org/abs/2505.22306)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare.</li>
</ul>

<h3>Title: Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zachary Shinnick, Liangze Jiang, Hemanth Saratchandran, Anton van den Hengel, Damien Teney</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22308">https://arxiv.org/abs/2505.22308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22308">https://arxiv.org/pdf/2505.22308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22308]] Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning(https://arxiv.org/abs/2505.22308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Pretraining on large, semantically rich datasets is key for developing language models. Surprisingly, recent studies have shown that even synthetic data, generated procedurally through simple semantic-free algorithms, can yield some of the same benefits as natural language pretraining. It is unclear what specific capabilities such simple synthetic data instils in a model, where these capabilities reside in the architecture, and how they manifest within its weights. In this short paper, we identify several beneficial forms of procedural data, together with specific algorithmic reasoning skills that improve in small transformers. Our core finding is that different procedural rules instil distinct but complementary inductive structures in the model. With extensive ablations and partial-transfer experiments, we discover that these structures reside in different parts of the model. Attention layers often carry the most transferable information, but some pretraining rules impart useful structure to MLP blocks instead. Most interestingly, the structures induced by multiple rules can be composed to jointly reinforce multiple capabilities. These results suggest an exciting possibility of disentangling the acquisition of knowledge from reasoning in language models, with the goal of improving their robustness and data efficiency.</li>
</ul>

<h3>Title: From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</h3>
<ul>
<li><strong>Authors: </strong>Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina Dziugaite, Michael Curtis Mozer, Eleni Triantafillou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22310">https://arxiv.org/abs/2505.22310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22310">https://arxiv.org/pdf/2505.22310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22310]] From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization(https://arxiv.org/abs/2505.22310)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks.</li>
</ul>

<h3>Title: Skywork Open Reasoner 1 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22312">https://arxiv.org/abs/2505.22312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22312">https://arxiv.org/pdf/2505.22312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22312]] Skywork Open Reasoner 1 Technical Report(https://arxiv.org/abs/2505.22312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.</li>
</ul>

<h3>Title: If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?</h3>
<ul>
<li><strong>Authors: </strong>Ishwar B Balappanawar, Vamshi Krishna Bonagiri, Anish R Joishy, Manas Gaur, Krishnaprasad Thirunarayan, Ponnurangam Kumaraguru</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22318">https://arxiv.org/abs/2505.22318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22318">https://arxiv.org/pdf/2505.22318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22318]] If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?(https://arxiv.org/abs/2505.22318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate impressive reasoning capabilities in familiar contexts, but struggle when the context conflicts with their parametric knowledge. To investigate this phenomenon, we introduce CounterLogic, a dataset containing 1,800 examples across 9 logical schemas, explicitly designed to evaluate logical reasoning through counterfactual (hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11 LLMs across 6 different datasets reveals a consistent performance degradation, with accuracies dropping by 27% on average when reasoning through counterfactual information. We propose Self-Segregate, a prompting method enabling metacognitive awareness (explicitly identifying knowledge conflicts) before reasoning. Our method dramatically narrows the average performance gaps from 27% to just 11%, while significantly increasing the overall accuracy (+7.5%). We discuss the implications of these findings and draw parallels to human cognitive processes, particularly on how humans disambiguate conflicting information during reasoning tasks. Our findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.</li>
</ul>

<h3>Title: A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zhengyu Fang, Zhimeng Jiang, Huiyuan Chen, Xiaoge Zhang, Kaiyu Tang, Xiao Li, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22322">https://arxiv.org/abs/2505.22322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22322">https://arxiv.org/pdf/2505.22322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22322]] A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective(https://arxiv.org/abs/2505.22322)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown strong performance in generating high-quality tabular data, but they carry privacy risks by reproducing exact training samples. While prior work focuses on dataset-level augmentation to reduce memorization, little is known about which individual samples contribute most. We present the first data-centric study of memorization dynamics in tabular diffusion models. We quantify memorization for each real sample based on how many generated samples are flagged as replicas, using a relative distance ratio. Our empirical analysis reveals a heavy-tailed distribution of memorization counts: a small subset of samples contributes disproportionately to leakage, confirmed via sample-removal experiments. To understand this, we divide real samples into top- and non-top-memorized groups and analyze their training-time behaviors. We track when each sample is first memorized and monitor per-epoch memorization intensity (AUC). Memorized samples are memorized slightly earlier and show stronger signals in early training. Based on these insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method: (a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and (c) retrain on the filtered dataset. Across multiple tabular datasets and models, DynamicCut reduces memorization with minimal impact on data diversity and downstream performance. It also complements augmentation-based defenses. Furthermore, DynamicCut enables cross-model transferability: high-ranked samples identified from one model (e.g., a diffusion model) are also effective for reducing memorization when removed from others, such as GANs and VAEs.</li>
</ul>

<h3>Title: Advancing Expert Specialization for Better MoE</h3>
<ul>
<li><strong>Authors: </strong>Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, Xudong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22323">https://arxiv.org/abs/2505.22323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22323">https://arxiv.org/pdf/2505.22323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22323]] Advancing Expert Specialization for Better MoE(https://arxiv.org/abs/2505.22323)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.</li>
</ul>

<h3>Title: NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment</h3>
<ul>
<li><strong>Authors: </strong>Antonia Karamolegkou, Angana Borah, Eunjung Cho, Sagnik Ray Choudhury, Martina Galletti, Rajarshi Ghosh, Pranav Gupta, Oana Ignat, Priyanka Kargupta, Neema Kotonya, Hemank Lamba, Sun-Joo Lee, Arushi Mangla, Ishani Mondal, Deniz Nazarova, Poli Nemkova, Dina Pisarevskaya, Naquee Rizwan, Nazanin Sabri, Dominik Stammbach, Anna Steinberg, David Tomás, Steven R Wilson, Bowen Yi, Jessica H Zhu, Arkaitz Zubiaga, Anders Søgaard, Alexander Fraser, Zhijing Jin, Rada Mihalcea, Joel R. Tetreault, Daryna Dementieva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22327">https://arxiv.org/abs/2505.22327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22327">https://arxiv.org/pdf/2505.22327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22327]] NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment(https://arxiv.org/abs/2505.22327)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have unlocked unprecedented possibilities across a range of applications. However, as a community, we believe that the field of Natural Language Processing (NLP) has a growing need to approach deployment with greater intentionality and responsibility. In alignment with the broader vision of AI for Social Good (Tomašev et al., 2020), this paper examines the role of NLP in addressing pressing societal challenges. Through a cross-disciplinary analysis of social goals and emerging risks, we highlight promising research directions and outline challenges that must be addressed to ensure responsible and equitable progress in NLP4SG research.</li>
</ul>

<h3>Title: Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start</h3>
<ul>
<li><strong>Authors: </strong>Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, Weiran Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22334">https://arxiv.org/abs/2505.22334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22334">https://arxiv.org/pdf/2505.22334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22334]] Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start(https://arxiv.org/abs/2505.22334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at this https URL.</li>
</ul>

<h3>Title: Learning to Infer Parameterized Representations of Plants from 3D Scans</h3>
<ul>
<li><strong>Authors: </strong>Samara Ghrer, Christophe Godin, Stefanie Wuhrer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22337">https://arxiv.org/abs/2505.22337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22337">https://arxiv.org/pdf/2505.22337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22337]] Learning to Infer Parameterized Representations of Plants from 3D Scans(https://arxiv.org/abs/2505.22337)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Reconstructing faithfully the 3D architecture of plants from unstructured observations is a challenging task. Plants frequently contain numerous organs, organized in branching systems in more or less complex spatial networks, leading to specific computational issues due to self-occlusion or spatial proximity between organs. Existing works either consider inverse modeling where the aim is to recover the procedural rules that allow to simulate virtual plants, or focus on specific tasks such as segmentation or skeletonization. We propose a unified approach that, given a 3D scan of a plant, allows to infer a parameterized representation of the plant. This representation describes the plant's branching structure, contains parametric information for each plant organ, and can therefore be used directly in a variety of tasks. In this data-driven approach, we train a recursive neural network with virtual plants generated using an L-systems-based procedural model. After training, the network allows to infer a parametric tree-like representation based on an input 3D point cloud. Our method is applicable to any plant that can be represented as binary axial tree. We evaluate our approach on Chenopodium Album plants, using experiments on synthetic plants to show that our unified framework allows for different tasks including reconstruction, segmentation and skeletonization, while achieving results on-par with state-of-the-art for each task.</li>
</ul>

<h3>Title: Text2Grad: Reinforcement Learning from Natural Language Feedback</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22338">https://arxiv.org/abs/2505.22338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22338">https://arxiv.org/pdf/2505.22338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22338]] Text2Grad: Reinforcement Learning from Natural Language Feedback(https://arxiv.org/abs/2505.22338)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at this https URL</li>
</ul>

<h3>Title: Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training</h3>
<ul>
<li><strong>Authors: </strong>Shriram M S, Xinyue Hao, Shihao Hou, Yang Lu, Laura Sevilla-Lara, Anurag Arnab, Shreyank N Gowda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22342">https://arxiv.org/abs/2505.22342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22342">https://arxiv.org/pdf/2505.22342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22342]] Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training(https://arxiv.org/abs/2505.22342)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The success of the machine learning field has reliably depended on training on large datasets. While effective, this trend comes at an extraordinary cost. This is due to two deeply intertwined factors: the size of models and the size of datasets. While promising research efforts focus on reducing the size of models, the other half of the equation remains fairly mysterious. Indeed, it is surprising that the standard approach to training remains to iterate over and over, uniformly sampling the training dataset. In this paper we explore a series of alternative training paradigms that leverage insights from hard-data-mining and dropout, simple enough to implement and use that can become the new training standard. The proposed Progressive Data Dropout reduces the number of effective epochs to as little as 12.4% of the baseline. This savings actually do not come at any cost for accuracy. Surprisingly, the proposed method improves accuracy by up to 4.82%. Our approach requires no changes to model architecture or optimizer, and can be applied across standard training pipelines, thus posing an excellent opportunity for wide adoption. Code can be found here: this https URL</li>
</ul>

<h3>Title: Task-Driven Implicit Representations for Automated Design of LiDAR Systems</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Behari, Aaron Young, Akshat Dave, Ramesh Raskar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22344">https://arxiv.org/abs/2505.22344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22344">https://arxiv.org/pdf/2505.22344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22344]] Task-Driven Implicit Representations for Automated Design of LiDAR Systems(https://arxiv.org/abs/2505.22344)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Imaging system design is a complex, time-consuming, and largely manual process; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and aerial imaging platforms, adds further complexity through unique spatial and temporal sampling requirements. In this work, we propose a framework for automated, task-driven LiDAR system design under arbitrary constraints. To achieve this, we represent LiDAR configurations in a continuous six-dimensional design space and learn task-specific implicit densities in this space via flow-based generative modeling. We then synthesize new LiDAR systems by modeling sensors as parametric distributions in 6D space and fitting these distributions to our learned implicit density using expectation-maximization, enabling efficient, constraint-aware LiDAR system design. We validate our method on diverse tasks in 3D vision, enabling automated LiDAR system design across real-world-inspired applications in face scanning, robotic tracking, and object detection.</li>
</ul>

<h3>Title: Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Liu, Xingle Xu, Ercong Nie, Zijing Wang, Shi Feng, Daling Wang, Qian Li, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22355">https://arxiv.org/abs/2505.22355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22355">https://arxiv.org/pdf/2505.22355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22355]] Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning(https://arxiv.org/abs/2505.22355)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable to Full Fine-Tuning (FFT) while requiring significantly fewer computing resources, making it the go-to choice for researchers. We find that although PEFT can achieve competitive results on some benchmarks, its performance falls short of FFT in complex tasks, such as reasoning and instruction-based fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in terms of representational capacity and robustness based on optimization theory. We theoretically demonstrate that PEFT is a strict subset of FFT. By providing theoretical upper bounds for PEFT, we show that the limited parameter space constrains the model's representational ability, making it more susceptible to perturbations. Experiments on 15 datasets encompassing classification, generation, reasoning, instruction fine-tuning tasks and 11 adversarial test sets validate our theories. We hope that these results spark further research beyond the realms of well established PEFT. The source code is in the anonymous Github repository\footnote{this https URL}.</li>
</ul>

<h3>Title: Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22358">https://arxiv.org/abs/2505.22358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22358">https://arxiv.org/pdf/2505.22358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22358]] Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs(https://arxiv.org/abs/2505.22358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in a single end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment. To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency, achieving higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark.</li>
</ul>

<h3>Title: Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition</h3>
<ul>
<li><strong>Authors: </strong>Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang, Other Contributors</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22375">https://arxiv.org/abs/2505.22375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22375">https://arxiv.org/pdf/2505.22375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22375]] Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition(https://arxiv.org/abs/2505.22375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode for routine queries and a deeper "slow" mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners.</li>
</ul>

<h3>Title: A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation</h3>
<ul>
<li><strong>Authors: </strong>Lukas Kirchdorfer, Konrad Özdemir, Stjepan Kusenic, Han van der Aa, Heiner Stuckenschmidt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22381">https://arxiv.org/abs/2505.22381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22381">https://arxiv.org/pdf/2505.22381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22381]] A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation(https://arxiv.org/abs/2505.22381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Business Process Simulation (BPS) is a critical tool for analyzing and improving organizational processes by estimating the impact of process changes. A key component of BPS is the case-arrival model, which determines the pattern of new case entries into a process. Although accurate case-arrival modeling is essential for reliable simulations, as it influences waiting and overall cycle times, existing approaches often rely on oversimplified static distributions of inter-arrival times. These approaches fail to capture the dynamic and temporal complexities inherent in organizational environments, leading to less accurate and reliable outcomes. To address this limitation, we propose Auto Time Kernel Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival times of processes by incorporating global dynamics, day-of-week variations, and intraday distributional changes, ensuring both precision and scalability. Experiments conducted across 20 diverse processes demonstrate that AT-KDE is far more accurate and robust than existing approaches while maintaining sensible execution time efficiency.</li>
</ul>

<h3>Title: Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhang, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22391">https://arxiv.org/abs/2505.22391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22391">https://arxiv.org/pdf/2505.22391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22391]] Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation(https://arxiv.org/abs/2505.22391)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.</li>
</ul>

<h3>Title: PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Fan Fei, Jiajun Tang, Fei-Peng Tian, Boxin Shi, Ping Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22394">https://arxiv.org/abs/2505.22394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22394">https://arxiv.org/pdf/2505.22394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22394]] PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models(https://arxiv.org/abs/2505.22394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present PacTure, a novel framework for generating physically-based rendering (PBR) material textures from an untextured 3D mesh, a text description, and an optional image prompt. Early 2D generation-based texturing approaches generate textures sequentially from different views, resulting in long inference times and globally inconsistent textures. More recent approaches adopt multi-view generation with cross-view attention to enhance global consistency, which, however, limits the resolution for each view. In response to these weaknesses, we first introduce view packing, a novel technique that significantly increases the effective resolution for each view during multi-view generation without imposing additional inference cost, by formulating the arrangement of multi-view maps as a 2D rectangle bin packing problem. In contrast to UV mapping, it preserves the spatial proximity essential for image generation and maintains full compatibility with current 2D generative models. To further reduce the inference cost, we enable fine-grained control and multi-domain generation within the next-scale prediction autoregressive framework to create an efficient multi-view multi-domain generative backbone. Extensive experiments show that PacTure outperforms state-of-the-art methods in both quality of generated PBR textures and efficiency in training and inference.</li>
</ul>

<h3>Title: Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Xudong Li, Mengdan Zhang, Peixian Chen, Xiawu Zheng, Yan Zhang, Jingyuan Zheng, Yunhang Shen, Ke Li, Chaoyou Fu, Xing Sun, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22396">https://arxiv.org/abs/2505.22396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22396">https://arxiv.org/pdf/2505.22396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22396]] Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs(https://arxiv.org/abs/2505.22396)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal Large Language Models (MLLMs) excel at single-image tasks but struggle with multi-image understanding due to cross-modal misalignment, leading to hallucinations (context omission, conflation, and misinterpretation). Existing methods using Direct Preference Optimization (DPO) constrain optimization to a solitary image reference within the input sequence, neglecting holistic context modeling. We propose Context-to-Cue Direct Preference Optimization (CcDPO), a multi-level preference optimization framework that enhances per-image perception in multi-image settings by zooming into visual clues -- from sequential context to local details. It features: (i) Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs' multi-image context comprehension and integrates a spectrum of low-cost global sequence preferences for bias mitigation. (ii) Needle-Level Optimization : Directs attention to fine-grained visual details through region-targeted visual prompts and multimodal preference supervision. To support scalable optimization, we also construct MultiScope-42k, an automatically generated dataset with high-quality multi-level preference pairs. Experiments show that CcDPO significantly reduces hallucinations and yields consistent performance gains across general single- and multi-image tasks.</li>
</ul>

<h3>Title: Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiadong Pan, Zhiyuan Ma, Kaiyan Zhang, Ning Ding, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22407">https://arxiv.org/abs/2505.22407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22407">https://arxiv.org/pdf/2505.22407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22407]] Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation(https://arxiv.org/abs/2505.22407)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently demonstrated exceptional performance in image generation task. However, existing image generation methods still significantly suffer from the dilemma of image reasoning, especially in logic-centered image generation tasks. Inspired by the success of Chain of Thought (CoT) and Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL algorithm for diffusion models to achieve reasoning generation of logical images by performing reflection and iteration across generation trajectories. The intermediate samples in the denoising process carry noise, making accurate reward evaluation difficult. To address this challenge, SRRL treats the entire denoising trajectory as a CoT step with multi-round reflective denoising process and introduces condition guided forward process, which allows for reflective iteration between CoT steps. Through SRRL-based iterative diffusion training, we introduce image reasoning through CoT into generation tasks adhering to physical laws and unconventional physical phenomena for the first time. Notably, experimental results of case study exhibit that the superior performance of our SRRL algorithm even compared with GPT-4o. The project page is this https URL.</li>
</ul>

<h3>Title: Frugal Incremental Generative Modeling using Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Victor Enescu, Hichem Sahbi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22408">https://arxiv.org/abs/2505.22408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22408">https://arxiv.org/pdf/2505.22408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22408]] Frugal Incremental Generative Modeling using Variational Autoencoders(https://arxiv.org/abs/2505.22408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual or incremental learning holds tremendous potential in deep learning with different challenges including catastrophic forgetting. The advent of powerful foundation and generative models has propelled this paradigm even further, making it one of the most viable solution to train these models. However, one of the persisting issues lies in the increasing volume of data particularly with replay-based methods. This growth introduces challenges with scalability since continuously expanding data becomes increasingly demanding as the number of tasks grows. In this paper, we attenuate this issue by devising a novel replay-free incremental learning model based on Variational Autoencoders (VAEs). The main contribution of this work includes (i) a novel incremental generative modelling, built upon a well designed multi-modal latent space, and also (ii) an orthogonality criterion that mitigates catastrophic forgetting of the learned VAEs. The proposed method considers two variants of these VAEs: static and dynamic with no (or at most a controlled) growth in the number of parameters. Extensive experiments show that our method is (at least) an order of magnitude more ``memory-frugal'' compared to the closely related works while achieving SOTA accuracy scores.</li>
</ul>

<h3>Title: Mitigating Overthinking in Large Reasoning Models via Manifold Steering</h3>
<ul>
<li><strong>Authors: </strong>Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22411">https://arxiv.org/abs/2505.22411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22411">https://arxiv.org/pdf/2505.22411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22411]] Mitigating Overthinking in Large Reasoning Models via Manifold Steering(https://arxiv.org/abs/2505.22411)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: this https URL.</li>
</ul>

<h3>Title: GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control</h3>
<ul>
<li><strong>Authors: </strong>Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, Shangbang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22421">https://arxiv.org/abs/2505.22421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22421">https://arxiv.org/pdf/2505.22421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22421]] GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control(https://arxiv.org/abs/2505.22421)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control.</li>
</ul>

<h3>Title: Scaling Reasoning without Attention</h3>
<ul>
<li><strong>Authors: </strong>Xueliang Zhao, Wei Wu, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22425">https://arxiv.org/abs/2505.22425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22425">https://arxiv.org/pdf/2505.22425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22425]] Scaling Reasoning without Attention(https://arxiv.org/abs/2505.22425)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning.</li>
</ul>

<h3>Title: RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network</h3>
<ul>
<li><strong>Authors: </strong>Van-Tin Luu, Yon-Lin Cai, Vu-Hoang Tran, Wei-Chen Chiu, Yi-Ting Chen, Ching-Chun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22427">https://arxiv.org/abs/2505.22427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22427">https://arxiv.org/pdf/2505.22427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22427]] RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network(https://arxiv.org/abs/2505.22427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a groundbreaking approach - the first online automatic geometric calibration method for radar and camera systems. Given the significant data sparsity and measurement uncertainty in radar height data, achieving automatic calibration during system operation has long been a challenge. To address the sparsity issue, we propose a Dual-Perspective representation that gathers features from both frontal and bird's-eye views. The frontal view contains rich but sensitive height information, whereas the bird's-eye view provides robust features against height uncertainty. We thereby propose a novel Selective Fusion Mechanism to identify and fuse reliable features from both perspectives, reducing the effect of height uncertainty. Moreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism to explicitly find location correspondences through cross-modal matching. During the training phase, we also design a Noise-Resistant Matcher to provide better supervision and enhance the robustness of the matching mechanism against sparsity and height uncertainty. Our experimental results, tested on the nuScenes dataset, demonstrate that our method significantly outperforms previous radar-camera auto-calibration methods, as well as existing state-of-the-art LiDAR-camera calibration techniques, establishing a new benchmark for future research. The code is available at this https URL.</li>
</ul>

<h3>Title: RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kun Li, Yunxiang Li, Tianhua Zhang, Hongyin Luo, Xixin Wu, James Glass, Helen Meng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22430">https://arxiv.org/abs/2505.22430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22430">https://arxiv.org/pdf/2505.22430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22430]] RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning(https://arxiv.org/abs/2505.22430)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Robust evaluation is critical for deploying trustworthy retrieval-augmented generation (RAG) systems. However, current LLM-based evaluation frameworks predominantly rely on directly prompting resource-intensive models with complex multi-stage prompts, underutilizing models' reasoning capabilities and introducing significant computational cost. In this paper, we present RAG-Zeval (RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness and correctness evaluation as a rule-guided reasoning task. Our approach trains evaluators with reinforcement learning, facilitating compact models to generate comprehensive and sound assessments with detailed explanation in one-pass. We introduce a ranking-based outcome reward mechanism, using preference judgments rather than absolute scores, to address the challenge of obtaining precise pointwise reward signals. To this end, we synthesize the ranking references by generating quality-controlled responses with zero human annotation. Experiments demonstrate RAG-Zeval's superior performance, achieving the strongest correlation with human judgments and outperforming baselines that rely on LLMs with 10-100 times more parameters. Our approach also exhibits superior interpretability in response evaluation.</li>
</ul>

<h3>Title: Distance Transform Guided Mixup for Alzheimer's Detection</h3>
<ul>
<li><strong>Authors: </strong>Zobia Batool, Huseyin Ozkan, Erchan Aptoula</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22434">https://arxiv.org/abs/2505.22434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22434">https://arxiv.org/pdf/2505.22434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22434]] Distance Transform Guided Mixup for Alzheimer's Detection(https://arxiv.org/abs/2505.22434)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Alzheimer's detection efforts aim to develop accurate models for early disease diagnosis. Significant advances have been achieved with convolutional neural networks and vision transformer based approaches. However, medical datasets suffer heavily from class imbalance, variations in imaging protocols, and limited dataset diversity, which hinder model generalization. To overcome these challenges, this study focuses on single-domain generalization by extending the well-known mixup method. The key idea is to compute the distance transform of MRI scans, separate them spatially into multiple layers and then combine layers stemming from distinct samples to produce augmented images. The proposed approach generates diverse data while preserving the brain's structure. Experimental results show generalization performance improvement across both ADNI and AIBL datasets.</li>
</ul>

<h3>Title: Does Johnny Get the Message? Evaluating Cybersecurity Notifications for Everyday Users</h3>
<ul>
<li><strong>Authors: </strong>Victor Jüttner, Erik Buchmann</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22435">https://arxiv.org/abs/2505.22435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22435">https://arxiv.org/pdf/2505.22435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22435]] Does Johnny Get the Message? Evaluating Cybersecurity Notifications for Everyday Users(https://arxiv.org/abs/2505.22435)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Due to the increasing presence of networked devices in everyday life, not only cybersecurity specialists but also end users benefit from security applications such as firewalls, vulnerability scanners, and intrusion detection systems. Recent approaches use large language models (LLMs) to rewrite brief, technical security alerts into intuitive language and suggest actionable measures, helping everyday users understand and respond appropriately to security risks. However, it remains an open question how well such alerts are explained to users. LLM outputs can also be hallucinated, inconsistent, or misleading. In this work, we introduce the Human-Centered Security Alert Evaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity notifications to support researchers who want to compare notifications generated for everyday users, improve them, or analyze the capabilities of different LLMs in explaining cybersecurity issues. We demonstrate HCSAEF through three use cases, which allow us to quantify the impact of prompt design, model selection, and output consistency. Our findings indicate that HCSAEF effectively differentiates generated notifications along dimensions such as intuitiveness, urgency, and correctness.</li>
</ul>

<h3>Title: On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Liyao Tang, Zhe Chen, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22444">https://arxiv.org/abs/2505.22444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22444">https://arxiv.org/pdf/2505.22444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22444]] On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation(https://arxiv.org/abs/2505.22444)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The emergence of large-scale pre-trained point cloud models has significantly advanced 3D scene understanding, but adapting these models to specific downstream tasks typically demands full fine-tuning, incurring high computational and storage costs. Parameter-efficient fine-tuning (PEFT) techniques, successful in natural language processing and 2D vision tasks, would underperform when naively applied to 3D point cloud models due to significant geometric and spatial distribution shifts. Existing PEFT methods commonly treat points as orderless tokens, neglecting important local spatial structures and global geometric contexts in 3D modeling. To bridge this gap, we introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT module specifically designed for 3D point cloud transformers. GEM explicitly integrates fine-grained local positional encodings with a lightweight latent attention mechanism to capture comprehensive global context, thereby effectively addressing the spatial and geometric distribution mismatch. Extensive experiments demonstrate that GEM achieves performance comparable to or sometimes even exceeding full fine-tuning, while only updating 1.6% of the model's parameters, fewer than other PEFT methods. With significantly reduced training time and memory requirements, our approach thus sets a new benchmark for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point cloud models. Code will be released.</li>
</ul>

<h3>Title: NFR: Neural Feature-Guided Non-Rigid Shape Registration</h3>
<ul>
<li><strong>Authors: </strong>Puhua Jiang, Zhangquan Chen, Mingze Sun, Ruqi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22445">https://arxiv.org/abs/2505.22445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22445">https://arxiv.org/pdf/2505.22445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22445]] NFR: Neural Feature-Guided Non-Rigid Shape Registration(https://arxiv.org/abs/2505.22445)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel learning-based framework for 3D shape registration, which overcomes the challenges of significant non-rigid deformation and partiality undergoing among input shapes, and, remarkably, requires no correspondence annotation during training. Our key insight is to incorporate neural features learned by deep learning-based shape matching networks into an iterative, geometric shape registration pipeline. The advantage of our approach is two-fold -- On one hand, neural features provide more accurate and semantically meaningful correspondence estimation than spatial features (e.g., coordinates), which is critical in the presence of large non-rigid deformations; On the other hand, the correspondences are dynamically updated according to the intermediate registrations and filtered by consistency prior, which prominently robustify the overall pipeline. Empirical results show that, with as few as dozens of training shapes of limited variability, our pipeline achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching and partial shape matching across varying settings, but also delivers high-quality correspondences between unseen challenging shape pairs that undergo both significant extrinsic and intrinsic deformations, in which case neither traditional registration methods nor intrinsic methods work.</li>
</ul>

<h3>Title: Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sizai Hou, Songze Li, Baturalp Buyukates</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22447">https://arxiv.org/abs/2505.22447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22447">https://arxiv.org/pdf/2505.22447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22447]] Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models(https://arxiv.org/abs/2505.22447)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, steal, membership infer, federate, large language model</a></li>
<li><strong>Abstract: </strong>Prompt learning is a crucial technique for adapting pre-trained multimodal language models (MLLMs) to user tasks. Federated prompt personalization (FPP) is further developed to address data heterogeneity and local overfitting, however, it exposes personalized prompts - valuable intellectual assets - to privacy risks like prompt stealing or membership inference attacks. Widely-adopted techniques like differential privacy add noise to prompts, whereas degrading personalization performance. We propose SecFPP, a secure FPP protocol harmonizing generalization, personalization, and privacy guarantees. SecFPP employs hierarchical prompt adaptation with domain-level and class-level components to handle multi-granular data imbalance. For privacy, it uses a novel secret-sharing-based adaptive clustering algorithm for domain-level adaptation while keeping class-level components private. While theoretically and empirically secure, SecFPP achieves state-of-the-art accuracy under severe heterogeneity in data distribution. Extensive experiments show it significantly outperforms both non-private and privacy-preserving baselines, offering a superior privacy-performance trade-off.</li>
</ul>

<h3>Title: Private Lossless Multiple Release</h3>
<ul>
<li><strong>Authors: </strong>Joel Daniel Andersson, Lukas Retschmeier, Boel Nelson, Rasmus Pagh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22449">https://arxiv.org/abs/2505.22449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22449">https://arxiv.org/pdf/2505.22449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22449]] Private Lossless Multiple Release(https://arxiv.org/abs/2505.22449)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Koufogiannis et al. (2016) showed a $\textit{gradual release}$ result for Laplace noise-based differentially private mechanisms: given an $\varepsilon$-DP release, a new release with privacy parameter $\varepsilon' > \varepsilon$ can be computed such that the combined privacy loss of both releases is at most $\varepsilon'$ and the distribution of the latter is the same as a single release with parameter $\varepsilon'$. They also showed gradual release techniques for Gaussian noise, later also explored by Whitehouse et al. (2022). In this paper, we consider a more general $\textit{multiple release}$ setting in which analysts hold private releases with different privacy parameters corresponding to different access/trust levels. These releases are determined one by one, with privacy parameters in arbitrary order. A multiple release is $\textit{lossless}$ if having access to a subset $S$ of the releases has the same privacy guarantee as the least private release in $S$, and each release has the same distribution as a single release with the same privacy parameter. Our main result is that lossless multiple release is possible for a large class of additive noise mechanisms. For the Gaussian mechanism we give a simple method for lossless multiple release with a short, self-contained analysis that does not require knowledge of the mathematics of Brownian motion. We also present lossless multiple release for the Laplace and Poisson mechanisms. Finally, we consider how to efficiently do gradual release of sparse histograms, and present a mechanism with running time independent of the number of dimensions.</li>
</ul>

<h3>Title: Position: All Current Generative Fidelity and Diversity Metrics are Flawed</h3>
<ul>
<li><strong>Authors: </strong>Ossi Räisä, Boris van Breugel, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22450">https://arxiv.org/abs/2505.22450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22450">https://arxiv.org/pdf/2505.22450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22450]] Position: All Current Generative Fidelity and Diversity Metrics are Flawed(https://arxiv.org/abs/2505.22450)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Any method's development and practical application is limited by our ability to measure its reliability. The popularity of generative modeling emphasizes the importance of good synthetic data metrics. Unfortunately, previous works have found many failure cases in current metrics, for example lack of outlier robustness and unclear lower and upper bounds. We propose a list of desiderata for synthetic data metrics, and a suite of sanity checks: carefully chosen simple experiments that aim to detect specific and known generative modeling failure modes. Based on these desiderata and the results of our checks, we arrive at our position: all current generative fidelity and diversity metrics are flawed. This significantly hinders practical use of synthetic data. Our aim is to convince the research community to spend more effort in developing metrics, instead of models. Additionally, through analyzing how current metrics fail, we provide practitioners with guidelines on how these metrics should (not) be used.</li>
</ul>

<h3>Title: Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO</h3>
<ul>
<li><strong>Authors: </strong>Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22453">https://arxiv.org/abs/2505.22453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22453">https://arxiv.org/pdf/2505.22453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22453]] Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO(https://arxiv.org/abs/2505.22453)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9 %$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at this https URL.</li>
</ul>

<h3>Title: Universal Domain Adaptation for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Seun-An Choe, Keon-Hee Park, Jinwoo Choi, Gyeong-Moon Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22458">https://arxiv.org/abs/2505.22458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22458">https://arxiv.org/pdf/2505.22458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22458]] Universal Domain Adaptation for Semantic Segmentation(https://arxiv.org/abs/2505.22458)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to transfer knowledge from labeled source data to unlabeled target data. However, traditional UDA-SS methods assume that category settings between source and target domains are known, which is unrealistic in real-world scenarios. This leads to performance degradation if private classes exist. To address this limitation, we propose Universal Domain Adaptation for Semantic Segmentation (UniDA-SS), achieving robust adaptation even without prior knowledge of category settings. We define the problem in the UniDA-SS scenario as low confidence scores of common classes in the target domain, which leads to confusion with private classes. To solve this problem, we propose UniMAP: UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework composed of two key components. First, Domain-Specific Prototype-based Distinction (DSPD) divides each class into two domain-specific prototypes, enabling finer separation of domain-specific features and enhancing the identification of common classes across domains. Second, Target-based Image Matching (TIM) selects a source image containing the most common-class pixels based on the target pseudo-label and pairs it in a batch to promote effective learning of common classes. We also introduce a new UniDA-SS benchmark and demonstrate through various experiments that UniMAP significantly outperforms baselines. The code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Zobia Batool, Huseyin Ozkan, Erchan Aptoula</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22465">https://arxiv.org/abs/2505.22465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22465">https://arxiv.org/pdf/2505.22465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22465]] Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning(https://arxiv.org/abs/2505.22465)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Although Alzheimer's disease detection via MRIs has advanced significantly thanks to contemporary deep learning models, challenges such as class imbalance, protocol variations, and limited dataset diversity often hinder their generalization capacity. To address this issue, this article focuses on the single domain generalization setting, where given the data of one domain, a model is designed and developed with maximal performance w.r.t. an unseen domain of distinct distribution. Since brain morphology is known to play a crucial role in Alzheimer's diagnosis, we propose the use of learnable pseudo-morphological modules aimed at producing shape-aware, anatomically meaningful class-specific augmentations in combination with a supervised contrastive learning module to extract robust class-specific representations. Experiments conducted across three datasets show improved performance and generalization capacity, especially under class imbalance and imaging protocol variations. The source code will be made available upon acceptance at this https URL.</li>
</ul>

<h3>Title: Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Sohrabbeig, Omid Ardakanian, Petr Musilek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22474">https://arxiv.org/abs/2505.22474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22474">https://arxiv.org/pdf/2505.22474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22474]] Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis(https://arxiv.org/abs/2505.22474)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The forecasting of multivariate urban data presents a complex challenge due to the intricate dependencies between various urban metrics such as weather, air pollution, carbon intensity, and energy demand. This paper introduces a novel multivariate time-series forecasting model that utilizes advanced Graph Neural Networks (GNNs) to capture spatial dependencies among different time-series variables. The proposed model incorporates a decomposition-based preprocessing step, isolating trend, seasonal, and residual components to enhance the accuracy and interpretability of forecasts. By leveraging the dynamic capabilities of GNNs, the model effectively captures interdependencies and improves the forecasting performance. Extensive experiments on real-world datasets, including electricity usage, weather metrics, carbon intensity, and air pollution data, demonstrate the effectiveness of the proposed approach across various forecasting scenarios. The results highlight the potential of the model to optimize smart infrastructure systems, contributing to energy-efficient urban development and enhanced public well-being.</li>
</ul>

<h3>Title: Understanding Adversarial Training with Energy-based Models</h3>
<ul>
<li><strong>Authors: </strong>Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, Iacopo Masi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22486">https://arxiv.org/abs/2505.22486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22486">https://arxiv.org/pdf/2505.22486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22486]] Understanding Adversarial Training with Energy-based Models(https://arxiv.org/abs/2505.22486)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>We aim at using Energy-based Model (EBM) framework to better understand adversarial training (AT) in classifiers, and additionally to analyze the intrinsic generative capabilities of robust classifiers. By viewing standard classifiers through an energy lens, we begin by analyzing how the energies of adversarial examples, generated by various attacks, differ from those of the natural samples. The central focus of our work is to understand the critical phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT from an energy perspective. We analyze the impact of existing AT approaches on the energy of samples during training and observe that the behavior of the ``delta energy' -- change in energy between original sample and its adversarial counterpart -- diverges significantly when CO or RO occurs. After a thorough analysis of these energy dynamics and their relationship with overfitting, we propose a novel regularizer, the Delta Energy Regularizer (DER), designed to smoothen the energy landscape during training. We demonstrate that DER is effective in mitigating both CO and RO across multiple benchmarks. We further show that robust classifiers, when being used as generative models, have limits in handling trade-off between image quality and variability. We propose an improved technique based on a local class-wise principal component analysis (PCA) and energy-based guidance for better class-specific initialization and adaptive stopping, enhancing sample diversity and generation quality. Considering that we do not explicitly train for generative modeling, we achieve a competitive Inception Score (IS) and Fréchet inception distance (FID) compared to hybrid discriminative-generative models.</li>
</ul>

<h3>Title: Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Zhou, Josiah P. Hanna, Jin Zhu, Ying Yang, Chengchun Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22492">https://arxiv.org/abs/2505.22492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22492">https://arxiv.org/pdf/2505.22492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22492]] Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation(https://arxiv.org/abs/2505.22492)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.</li>
</ul>

<h3>Title: ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods</h3>
<ul>
<li><strong>Authors: </strong>Michal Kmicikiewicz, Vincent Fortuin, Ewa Szczurek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22494">https://arxiv.org/abs/2505.22494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22494">https://arxiv.org/pdf/2505.22494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22494]] ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods(https://arxiv.org/abs/2505.22494)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Designing protein sequences of both high fitness and novelty is a challenging task in data-efficient protein engineering. Exploration beyond wild-type neighborhoods often leads to biologically implausible sequences or relies on surrogate models that lose fidelity in novel regions. Here, we propose ProSpero, an active learning framework in which a frozen pre-trained generative model is guided by a surrogate updated from oracle feedback. By integrating fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling, our approach enables exploration beyond wild-type neighborhoods while preserving biological plausibility. We show that our framework remains effective even when the surrogate is misspecified. ProSpero consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.</li>
</ul>

<h3>Title: The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector</h3>
<ul>
<li><strong>Authors: </strong>Aixuan Li, Mochu Xiang, Jing Zhang, Yuchao Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22499">https://arxiv.org/abs/2505.22499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22499">https://arxiv.org/pdf/2505.22499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22499]] The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector(https://arxiv.org/abs/2505.22499)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>3D object detection is a critical component in autonomous driving systems. It allows real-time recognition and detection of vehicles, pedestrians and obstacles under varying environmental conditions. Among existing methods, 3D object detection in the Bird's Eye View (BEV) has emerged as the mainstream framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D adversarial attacks are investigated, where attacks are placed in 3D environments to evaluate the model performance, e.g., putting a film on a car, clothing a pedestrian. The vulnerability of 3D object detection models to 3D adversarial attacks serves as an important indicator to evaluate the robustness of the model against perturbations. To investigate this vulnerability, we generate non-invasive 3D adversarial objects tailored for real-world attack scenarios. Our method verifies the existence of universal adversarial objects that are spatially consistent across time and camera views. Specifically, we employ differentiable rendering techniques to accurately model the spatial relationship between adversarial objects and the target vehicle. Furthermore, we introduce an occlusion-aware module to enhance visual consistency and realism under different viewpoints. To maintain attack effectiveness across multiple frames, we design a BEV spatial feature-guided optimization strategy. Experimental results demonstrate that our approach can reliably suppress vehicle predictions from state-of-the-art 3D object detectors, serving as an important tool to test robustness of 3D object detection models before deployment. Moreover, the generated adversarial objects exhibit strong generalization capabilities, retaining its effectiveness at various positions and distances in the scene.</li>
</ul>

<h3>Title: EvolveSearch: An Iterative Self-Evolving Search Agent</h3>
<ul>
<li><strong>Authors: </strong>Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22501">https://arxiv.org/abs/2505.22501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22501">https://arxiv.org/pdf/2505.22501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22501]] EvolveSearch: An Iterative Self-Evolving Search Agent(https://arxiv.org/abs/2505.22501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has transformed the landscape of agentic information seeking capabilities through the integration of tools such as search engines and web browsers. However, current mainstream approaches for enabling LLM web search proficiency face significant challenges: supervised fine-tuning struggles with data production in open-search domains, while RL converges quickly, limiting their data utilization efficiency. To address these issues, we propose EvolveSearch, a novel iterative self-evolution framework that combines SFT and RL to enhance agentic web search capabilities without any external human-annotated reasoning data. Extensive experiments on seven multi-hop question-answering (MHQA) benchmarks demonstrate that EvolveSearch consistently improves performance across iterations, ultimately achieving an average improvement of 4.7\% over the current state-of-the-art across seven benchmarks, opening the door to self-evolution agentic capabilities in open web search domains.</li>
</ul>

<h3>Title: Sparsification and Reconstruction from the Perspective of Representation Geometry</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Sun, Bingzhe Wu, Zhile Yang, Chengke Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22506">https://arxiv.org/abs/2505.22506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22506">https://arxiv.org/pdf/2505.22506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22506]] Sparsification and Reconstruction from the Perspective of Representation Geometry(https://arxiv.org/abs/2505.22506)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic interpretability, aiming to identify interpretable monosemantic features. However, how does sparse encoding organize the representations of activation vector from language models? What is the relationship between this organizational paradigm and feature disentanglement as well as reconstruction performance? To address these questions, we propose the SAEMA, which validates the stratified structure of the representation by observing the variability of the rank of the symmetric semipositive definite (SSPD) matrix corresponding to the modal tensor unfolded along the latent tensor with the level of noise added to the residual stream. To systematically investigate how sparse encoding alters representational structures, we define local and global representations, demonstrating that they amplify inter-feature distinctions by merging similar semantic features and introducing additional dimensionality. Furthermore, we intervene the global representation from an optimization perspective, proving a significant causal relationship between their separability and the reconstruction performance. This study explains the principles of sparsity from the perspective of representational geometry and demonstrates the impact of changes in representational structure on reconstruction performance. Particularly emphasizes the necessity of understanding representations and incorporating representational constraints, providing empirical references for developing new interpretable tools and improving SAEs. The code is available at \hyperlink{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Multi-MLLM Knowledge Distillation for Out-of-Context News Detection</h3>
<ul>
<li><strong>Authors: </strong>Yimeng Gu, Zhao Tong, Ignacio Castro, Shu Wu, Gareth Tyson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22517">https://arxiv.org/abs/2505.22517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22517">https://arxiv.org/pdf/2505.22517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22517]] Multi-MLLM Knowledge Distillation for Out-of-Context News Detection(https://arxiv.org/abs/2505.22517)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal out-of-context news is a type of misinformation in which the image is used outside of its original context. Many existing works have leveraged multimodal large language models (MLLMs) for detecting out-of-context news. However, observing the limited zero-shot performance of smaller MLLMs, they generally require label-rich fine-tuning and/or expensive API calls to GPT models to improve the performance, which is impractical in low-resource scenarios. In contrast, we aim to improve the performance of small MLLMs in a more label-efficient and cost-effective manner. To this end, we first prompt multiple teacher MLLMs to generate both label predictions and corresponding rationales, which collectively serve as the teachers' knowledge. We then introduce a two-stage knowledge distillation framework to transfer this knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the student model using all training data. In Stage 2, we further fine-tune the student model using both LoRA fine-tuning and DPO on the data points where teachers' predictions conflict. This two-stage strategy reduces annotation costs and helps the student model uncover subtle patterns in more challenging cases. Experimental results demonstrate that our approach achieves state-of-the-art performance using less than 10% labeled data.</li>
</ul>

<h3>Title: PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuan Zhang, Feng Chen, Yaolei Qi, Guanyu Yang, Huazhu Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22522">https://arxiv.org/abs/2505.22522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22522">https://arxiv.org/pdf/2505.22522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22522]] PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation(https://arxiv.org/abs/2505.22522)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Pathology image segmentation across multiple centers encounters significant challenges due to diverse sources of heterogeneity including imaging modalities, organs, and scanning equipment, whose variability brings representation bias and impedes the development of generalizable segmentation models. In this paper, we propose PathFL, a novel multi-alignment Federated Learning framework for pathology image segmentation that addresses these challenges through three-level alignment strategies of image, feature, and model aggregation. Firstly, at the image level, a collaborative style enhancement module aligns and diversifies local data by facilitating style information exchange across clients. Secondly, at the feature level, an adaptive feature alignment module ensures implicit alignment in the representation space by infusing local features with global insights, promoting consistency across heterogeneous client features learning. Finally, at the model aggregation level, a stratified similarity aggregation strategy hierarchically aligns and aggregates models on the server, using layer-specific similarity to account for client discrepancies and enhance global generalization. Comprehensive evaluations on four sets of heterogeneous pathology image datasets, encompassing cross-source, cross-modality, cross-organ, and cross-scanner variations, validate the effectiveness of our PathFL in achieving better performance and robustness against data heterogeneity.</li>
</ul>

<h3>Title: PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Junwen Chen, Heyang Jiang, Yanbin Wang, Keming Wu, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, Yuhui Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22523">https://arxiv.org/abs/2505.22523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22523">https://arxiv.org/pdf/2505.22523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22523]] PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models(https://arxiv.org/abs/2505.22523)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery.</li>
</ul>

<h3>Title: Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo</h3>
<ul>
<li><strong>Authors: </strong>Chinmay Pani, Zijing Ou, Yingzhen Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22524">https://arxiv.org/abs/2505.22524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22524">https://arxiv.org/pdf/2505.22524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22524]] Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo(https://arxiv.org/abs/2505.22524)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have become highly effective across various domains. However, real-world applications often require the generative process to adhere to certain constraints but without task-specific fine-tuning. To this end, we propose a training-free method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution at the test time. Our approach leverages twisted SMC with an approximate locally optimal proposal, obtained via a first-order Taylor expansion of the reward function. To address the challenge of ill-defined gradients in discrete spaces, we incorporate a Gumbel-Softmax relaxation, enabling efficient gradient-based approximation within the discrete generative framework. Empirical results on both synthetic datasets and image modelling validate the effectiveness of our approach.</li>
</ul>

<h3>Title: Training RL Agents for Multi-Objective Network Defense Tasks</h3>
<ul>
<li><strong>Authors: </strong>Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, Ahmed Ridley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22531">https://arxiv.org/abs/2505.22531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22531">https://arxiv.org/pdf/2505.22531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22531]] Training RL Agents for Multi-Objective Network Defense Tasks(https://arxiv.org/abs/2505.22531)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Open-ended learning (OEL) -- which emphasizes training agents that achieve broad capability over narrow competency -- is emerging as a paradigm to develop artificial intelligence (AI) agents to achieve robustness and generalization. However, despite promising results that demonstrate the benefits of OEL, applying OEL to develop autonomous agents for real-world cybersecurity applications remains a challenge. We propose a training approach, inspired by OEL, to develop autonomous network defenders. Our results demonstrate that like in other domains, OEL principles can translate into more robust and generalizable agents for cyber defense. To apply OEL to network defense, it is necessary to address several technical challenges. Most importantly, it is critical to provide a task representation approach over a broad universe of tasks that maintains a consistent interface over goals, rewards and action spaces. This way, the learning agent can train with varying network conditions, attacker behaviors, and defender goals while being able to build on previously gained knowledge. With our tools and results, we aim to fundamentally impact research that applies AI to solve cybersecurity problems. Specifically, as researchers develop gyms and benchmarks for cyber defense, it is paramount that they consider diverse tasks with consistent representations, such as those we propose in our work.</li>
</ul>

<h3>Title: TabularQGAN: A Quantum Generative Model for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Pallavi Bhardwaj, Caitlin Jones, Lasse Dierich, Aleksandar Vučković</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22533">https://arxiv.org/abs/2505.22533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22533">https://arxiv.org/pdf/2505.22533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22533]] TabularQGAN: A Quantum Generative Model for Tabular Data(https://arxiv.org/abs/2505.22533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel quantum generative model for synthesizing tabular data. Synthetic data is valuable in scenarios where real-world data is scarce or private, it can be used to augment or replace existing datasets. Real-world enterprise data is predominantly tabular and heterogeneous, often comprising a mixture of categorical and numerical features, making it highly relevant across various industries such as healthcare, finance, and software. We propose a quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz to effectively model tabular data. The proposed approach is tested on the MIMIC III healthcare and Adult Census datasets, with extensive benchmarking against leading classical models, CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models. Additionally, we evaluate the generalization capabilities of the models using two custom-designed metrics that demonstrate the ability of the proposed quantum model to generate useful and novel samples. To our knowledge, this is one of the first demonstrations of a successful quantum generative model for handling tabular data, indicating that this task could be well-suited to quantum computers.</li>
</ul>

<h3>Title: A Human-Centric Approach to Explainable AI for Personalized Education</h3>
<ul>
<li><strong>Authors: </strong>Vinitra Swamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22541">https://arxiv.org/abs/2505.22541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22541">https://arxiv.org/pdf/2505.22541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22541]] A Human-Centric Approach to Explainable AI for Personalized Education(https://arxiv.org/abs/2505.22541)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Deep neural networks form the backbone of artificial intelligence research, with potential to transform the human experience in areas ranging from autonomous driving to personal assistants, healthcare to education. However, their integration into the daily routines of real-world classrooms remains limited. It is not yet common for a teacher to assign students individualized homework targeting their specific weaknesses, provide students with instant feedback, or simulate student responses to a new exam question. While these models excel in predictive performance, this lack of adoption can be attributed to a significant weakness: the lack of explainability of model decisions, leading to a lack of trust from students, parents, and teachers. This thesis aims to bring human needs to the forefront of eXplainable AI (XAI) research, grounded in the concrete use case of personalized learning and teaching. We frame the contributions along two verticals: technical advances in XAI and their aligned human studies. We investigate explainability in AI for education, revealing systematic disagreements between post-hoc explainers and identifying a need for inherently interpretable model architectures. We propose four novel technical contributions in interpretability with a multimodal modular architecture (MultiModN), an interpretable mixture-of-experts model (InterpretCC), adversarial training for explainer stability, and a theory-driven LLM-XAI framework to present explanations to students (iLLuMinaTE), which we evaluate in diverse settings with professors, teachers, learning scientists, and university students. By combining empirical evaluations of existing explainers with novel architectural designs and human studies, our work lays a foundation for human-centric AI systems that balance state-of-the-art performance with built-in transparency and trust.</li>
</ul>

<h3>Title: Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Long Hui, Wai Lok Yeung</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22551">https://arxiv.org/abs/2505.22551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22551">https://arxiv.org/pdf/2505.22551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22551]] Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification(https://arxiv.org/abs/2505.22551)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Limited DXA access hinders osteoporosis screening. This proof-of-concept study proposes using widely available knee X-rays for opportunistic Bone Mineral Density (BMD) estimation via deep learning, emphasizing robust uncertainty quantification essential for clinical use. An EfficientNet model was trained on the OAI dataset to predict BMD from bilateral knee radiographs. Two Test-Time Augmentation (TTA) methods were compared: traditional averaging and a multi-sample approach. Crucially, Split Conformal Prediction was implemented to provide statistically rigorous, patient-specific prediction intervals with guaranteed coverage. Results showed a Pearson correlation of 0.68 (traditional TTA). While traditional TTA yielded better point predictions, the multi-sample approach produced slightly tighter confidence intervals (90%, 95%, 99%) while maintaining coverage. The framework appropriately expressed higher uncertainty for challenging cases. Although anatomical mismatch between knee X-rays and standard DXA limits immediate clinical use, this method establishes a foundation for trustworthy AI-assisted BMD screening using routine radiographs, potentially improving early osteoporosis detection.</li>
</ul>

<h3>Title: ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM</h3>
<ul>
<li><strong>Authors: </strong>Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22552">https://arxiv.org/abs/2505.22552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22552">https://arxiv.org/pdf/2505.22552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22552]] ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM(https://arxiv.org/abs/2505.22552)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones.</li>
</ul>

<h3>Title: MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Yanyi Qu, Haoyang Ma, Wenhui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22555">https://arxiv.org/abs/2505.22555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22555">https://arxiv.org/pdf/2505.22555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22555]] MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism(https://arxiv.org/abs/2505.22555)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human pose estimation based on Channel State Information (CSI) has emerged as a promising approach for non-intrusive and precise human activity monitoring, yet faces challenges including accurate multi-person pose recognition and effective CSI feature learning. This paper presents MultiFormer, a wireless sensing system that accurately estimates human pose through CSI. The proposed system adopts a Transformer based time-frequency dual-token feature extractor with multi-head self-attention. This feature extractor is able to model inter-subcarrier correlations and temporal dependencies of the CSI. The extracted CSI features and the pose probability heatmaps are then fused by Multi-Stage Feature Fusion Network (MSFN) to enforce the anatomical constraints. Extensive experiments conducted on on the public MM-Fi dataset and our self-collected dataset show that the MultiFormer achieves higher accuracy over state-of-the-art approaches, especially for high-mobility keypoints (wrists, elbows) that are particularly difficult for previous methods to accurately estimate.</li>
</ul>

<h3>Title: Geometric Hyena Networks for Large-scale Equivariant Learning</h3>
<ul>
<li><strong>Authors: </strong>Artem Moskalev, Mangal Prakash, Junjie Xu, Tianyu Cui, Rui Liao, Tommaso Mansi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22560">https://arxiv.org/abs/2505.22560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22560">https://arxiv.org/pdf/2505.22560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22560]] Geometric Hyena Networks for Large-scale Equivariant Learning(https://arxiv.org/abs/2505.22560)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.</li>
</ul>

<h3>Title: Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Yu Lei, Xingyang Ge, Yi Zhang, Yiming Yang, Bolei Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22563">https://arxiv.org/abs/2505.22563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22563">https://arxiv.org/pdf/2505.22563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22563]] Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings(https://arxiv.org/abs/2505.22563)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in LLMs align with the dynamic neural responses during human sentence comprehension. By comparing hierarchical embeddings from 14 publicly available LLMs with fMRI data collected from participants, who were exposed to a naturalistic narrative story, we constructed sentence-level neural prediction models to precisely identify the model layers most significantly correlated with brain region activations. Results show that improvements in model performance drive the evolution of representational architectures toward brain-like hierarchies, particularly achieving stronger functional and anatomical correspondence at higher semantic abstraction levels.</li>
</ul>

<h3>Title: Universal Visuo-Tactile Video Understanding for Embodied Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yifan Xie, Mingyang Li, Shoujie Li, Xingting Li, Guangyu Chen, Fei Ma, Fei Richard Yu, Wenbo Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22566">https://arxiv.org/abs/2505.22566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22566">https://arxiv.org/pdf/2505.22566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22566]] Universal Visuo-Tactile Video Understanding for Embodied Interaction(https://arxiv.org/abs/2505.22566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Tactile perception is essential for embodied agents to understand physical attributes of objects that cannot be determined through visual inspection alone. While existing approaches have made progress in visual and language modalities for physical understanding, they fail to effectively incorporate tactile information that provides crucial haptic feedback for real-world interaction. In this paper, we present VTV-LLM, the first multi-modal large language model for universal Visuo-Tactile Video (VTV) understanding that bridges the gap between tactile perception and natural language. To address the challenges of cross-sensor and cross-modal integration, we contribute VTV150K, a comprehensive dataset comprising 150,000 video frames from 100 diverse objects captured across three different tactile sensors (GelSight Mini, DIGIT, and Tac3D), annotated with four fundamental tactile attributes (hardness, protrusion, elasticity, and friction). We develop a novel three-stage training paradigm that includes VTV enhancement for robust visuo-tactile representation, VTV-text alignment for cross-modal correspondence, and text prompt finetuning for natural language generation. Our framework enables sophisticated tactile reasoning capabilities including feature assessment, comparative analysis, scenario-based decision making and so on. Experimental evaluations demonstrate that VTV-LLM achieves superior performance in tactile video understanding tasks, establishing a foundation for more intuitive human-machine interaction in tactile domains.</li>
</ul>

<h3>Title: ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dmitrii Sorokin, Maksim Nakhodnov, Andrey Kuznetsov, Aibek Alanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22569">https://arxiv.org/abs/2505.22569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22569">https://arxiv.org/pdf/2505.22569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22569]] ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models(https://arxiv.org/abs/2505.22569)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have led to impressive image generation capabilities, but aligning these models with human preferences remains challenging. Reward-based fine-tuning using models trained on human feedback improves alignment but often harms diversity, producing less varied outputs. In this work, we address this trade-off with two contributions. First, we introduce \textit{combined generation}, a novel sampling strategy that applies a reward-tuned diffusion model only in the later stages of the generation process, while preserving the base model for earlier steps. This approach mitigates early-stage overfitting and helps retain global structure and diversity. Second, we propose \textit{ImageReFL}, a fine-tuning method that improves image diversity with minimal loss in quality by training on real images and incorporating multiple regularizers, including diffusion and ReFL losses. Our approach outperforms conventional reward tuning methods on standard quality and diversity metrics. A user study further confirms that our method better balances human preference alignment and visual diversity. The source code can be found at this https URL .</li>
</ul>

<h3>Title: Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Hoang Pham, Khac-Hoai Nam Bui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22571">https://arxiv.org/abs/2505.22571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22571">https://arxiv.org/pdf/2505.22571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22571]] Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2505.22571)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.</li>
</ul>

<h3>Title: Fusion Steering: Prompt-Specific Activation Control</h3>
<ul>
<li><strong>Authors: </strong>Waldemar Chang, Alhassan Yasin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22572">https://arxiv.org/abs/2505.22572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22572">https://arxiv.org/pdf/2505.22572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22572]] Fusion Steering: Prompt-Specific Activation Control(https://arxiv.org/abs/2505.22572)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present Fusion Steering, an activation steering methodology that improves factual accuracy in large language models (LLMs) for question-answering (QA) tasks. This approach introduces flexible steering configurations, including full-layer steering and segmented steering. Unlike traditional methods constrained to single-layer or fixed-layer operations, Fusion Steering employs dynamic injection of prompt-specific activation deltas across all transformer layers. These activation deltas are derived from reference completions that combine the ground-truth answer with a model-generated explanation to facilitate semantically enriched, example-specific steering. The injection weights are optimized per prompt using Optuna, targeting a joint objective that balances token overlap (factual alignment) and perplexity (fluency proxy). Evaluation employs a composite score integrating token overlap and LLM-graded quality, encompassing factual accuracy, coherence, and relevance. Empirical results on 260 SimpleQA prompts (selected from 500 where the baseline failed) showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring $\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at 16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully correct responses from 0.0% to 13.1%. These findings highlight the strengths of segmented, dynamic intervention strategies and the promise of per-prompt, full-network activation control. Fusion Steering is also amenable to sparse representations, such as Neuronpedia or sparse crosscoders, suggesting a promising direction for interpretable and scalable activation-level control in LLMs.</li>
</ul>

<h3>Title: Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22582">https://arxiv.org/abs/2505.22582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22582">https://arxiv.org/pdf/2505.22582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22582]] Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts(https://arxiv.org/abs/2505.22582)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs. The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages. To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts). Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages. To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer. Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts. Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens. Experimental results show that our method outperforms the previous state-of-the-art baseline with 60% fewer experts in the single-expansion setting and with 33.3% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method.</li>
</ul>

<h3>Title: Precise In-Parameter Concept Erasure in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22586">https://arxiv.org/abs/2505.22586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22586">https://arxiv.org/pdf/2505.22586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22586]] Precise In-Parameter Concept Erasure in Large Language Models(https://arxiv.org/abs/2505.22586)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.</li>
</ul>

<h3>Title: Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Erxin Yu, Jing Li, Ming Liao, Qi Zhu, Boyang Xue, Minghui Xu, Baojun Wang, Lanqing Hong, Fei Mi, Lifeng Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22591">https://arxiv.org/abs/2505.22591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22591">https://arxiv.org/pdf/2505.22591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22591]] Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning(https://arxiv.org/abs/2505.22591)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.</li>
</ul>

<h3>Title: SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Huang, Zunnan Xu, Jun Zhou, Ting Liu, Yicheng Xiao, Mingwen Ou, Bowen Ji, Xiu Li, Kehong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22596">https://arxiv.org/abs/2505.22596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22596">https://arxiv.org/pdf/2505.22596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22596]] SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning(https://arxiv.org/abs/2505.22596)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Leveraging multimodal large models for image segmentation has become a prominent research direction. However, existing approaches typically rely heavily on manually annotated datasets that include explicit reasoning processes, which are costly and time-consuming to produce. Recent advances suggest that reinforcement learning (RL) can endow large models with reasoning capabilities without requiring such reasoning-annotated data. In this paper, we propose SAM-R1, a novel framework that enables multimodal large models to perform fine-grained reasoning in image understanding tasks. Our approach is the first to incorporate fine-grained segmentation settings during the training of multimodal reasoning models. By integrating task-specific, fine-grained rewards with a tailored optimization objective, we further enhance the model's reasoning and segmentation alignment. We also leverage the Segment Anything Model (SAM) as a strong and flexible reward provider to guide the learning process. With only 3k training samples, SAM-R1 achieves strong performance across multiple benchmarks, demonstrating the effectiveness of reinforcement learning in equipping multimodal models with segmentation-oriented reasoning capabilities.</li>
</ul>

<h3>Title: Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22604">https://arxiv.org/abs/2505.22604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22604">https://arxiv.org/pdf/2505.22604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22604]] Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective(https://arxiv.org/abs/2505.22604)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Rapid advances in Artificial Intelligence Generated Images (AIGI) have facilitated malicious use, such as forgery and misinformation. Therefore, numerous methods have been proposed to detect fake images. Although such detectors have been proven to be universally vulnerable to adversarial attacks, defenses in this field are scarce. In this paper, we first identify that adversarial training (AT), widely regarded as the most effective defense, suffers from performance collapse in AIGI detection. Through an information-theoretic lens, we further attribute the cause of collapse to feature entanglement, which disrupts the preservation of feature-label mutual information. Instead, standard detectors show clear feature separation. Motivated by this difference, we propose Training-free Robust Detection via Information-theoretic Measures (TRIM), the first training-free adversarial defense for AIGI detection. TRIM builds on standard detectors and quantifies feature shifts using prediction entropy and KL divergence. Extensive experiments across multiple datasets and attacks validate the superiority of our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%) on ProGAN (GenImage), while well maintaining original accuracy.</li>
</ul>

<h3>Title: Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook</h3>
<ul>
<li><strong>Authors: </strong>Banafsheh Saber Latibari, Najmeh Nazari, Avesta Sasan, Houman Homayoun, Pratik Satam, Soheil Salehi, Hossein Sayadi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22605">https://arxiv.org/abs/2505.22605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22605">https://arxiv.org/pdf/2505.22605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22605]] Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook(https://arxiv.org/abs/2505.22605)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, transformer</a></li>
<li><strong>Abstract: </strong>The rise of hardware-level security threats, such as side-channel attacks, hardware Trojans, and firmware vulnerabilities, demands advanced detection mechanisms that are more intelligent and adaptive. Traditional methods often fall short in addressing the complexity and evasiveness of modern attacks, driving increased interest in machine learning-based solutions. Among these, Transformer models, widely recognized for their success in natural language processing and computer vision, have gained traction in the security domain due to their ability to model complex dependencies, offering enhanced capabilities in identifying vulnerabilities, detecting anomalies, and reinforcing system integrity. This survey provides a comprehensive review of recent advancements on the use of Transformers in hardware security, examining their application across key areas such as side-channel analysis, hardware Trojan detection, vulnerability classification, device fingerprinting, and firmware security. Furthermore, we discuss the practical challenges of applying Transformers to secure hardware systems, and highlight opportunities and future research directions that position them as a foundation for next-generation hardware-assisted security. These insights pave the way for deeper integration of AI-driven techniques into hardware security frameworks, enabling more resilient and intelligent defenses.</li>
</ul>

<h3>Title: RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yuchi Wang, Yishuo Cai, Shuhuai Ren, Sihan Yang, Linli Yao, Yuanxin Liu, Yuanxing Zhang, Pengfei Wan, Xu Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22613">https://arxiv.org/abs/2505.22613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22613">https://arxiv.org/pdf/2505.22613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22613]] RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction(https://arxiv.org/abs/2505.22613)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at this https URL.</li>
</ul>

<h3>Title: Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding</h3>
<ul>
<li><strong>Authors: </strong>Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22618">https://arxiv.org/abs/2505.22618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22618">https://arxiv.org/pdf/2505.22618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22618]] Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding(https://arxiv.org/abs/2505.22618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.</li>
</ul>

<h3>Title: Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions</h3>
<ul>
<li><strong>Authors: </strong>Yijun Shen, Delong Chen, Fan Liu, Xingyu Wang, Chuanyi Zhang, Liang Yao, Yuhui Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22627">https://arxiv.org/abs/2505.22627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22627">https://arxiv.org/pdf/2505.22627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22627]] Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions(https://arxiv.org/abs/2505.22627)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While densely annotated image captions significantly facilitate the learning of robust vision-language alignment, methodologies for systematically optimizing human annotation efforts remain underexplored. We introduce Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize the number of annotated samples and improve their comprehensiveness under fixed budget constraints (e.g., total human annotation time). The framework is built upon two key insights. First, sequential annotation reduces redundant workload compared to conventional parallel annotation, as subsequent annotators only need to annotate the ``residual'' -- the missing visual information that previous annotations have not covered. Second, humans process textual input faster by reading while outputting annotations with much higher throughput via talking; thus a multimodal interface enables optimized efficiency. We evaluate our framework from two aspects: intrinsic evaluations that assess the comprehensiveness of semantic units, obtained by parsing detailed captions into object-attribute trees and analyzing their effective connections; extrinsic evaluation measures the practical usage of the annotated captions in facilitating vision-language alignment. Experiments with eight participants show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel method.</li>
</ul>

<h3>Title: Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ziling Cheng, Meng Cao, Marc-Antoine Rondeau, Jackie Chi Kit Cheung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22630">https://arxiv.org/abs/2505.22630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22630">https://arxiv.org/pdf/2505.22630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22630]] Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs(https://arxiv.org/abs/2505.22630)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The widespread success of large language models (LLMs) on NLP benchmarks has been accompanied by concerns that LLMs function primarily as stochastic parrots that reproduce texts similar to what they saw during pre-training, often erroneously. But what is the nature of their errors, and do these errors exhibit any regularities? In this work, we examine irrelevant context hallucinations, in which models integrate misleading contextual cues into their predictions. Through behavioral analysis, we show that these errors result from a structured yet flawed mechanism that we term class-based (mis)generalization, in which models combine abstract class cues with features extracted from the query or context to derive answers. Furthermore, mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types reveal that this behavior is reflected in the model's internal computations: (i) abstract class representations are constructed in lower layers before being refined into specific answers in higher layers, (ii) feature selection is governed by two competing circuits -- one prioritizing direct query-based reasoning, the other incorporating contextual cues -- whose relative influences determine the final output. Our findings provide a more nuanced perspective on the stochastic parrot argument: through form-based training, LLMs can exhibit generalization leveraging abstractions, albeit in unreliable ways based on contextual cues -- what we term stochastic chameleons.</li>
</ul>

<h3>Title: Spatial Knowledge Graph-Guided Multimodal Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22633">https://arxiv.org/abs/2505.22633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22633">https://arxiv.org/pdf/2505.22633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22633]] Spatial Knowledge Graph-Guided Multimodal Synthesis(https://arxiv.org/abs/2505.22633)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. In this work, we introduce SKG2Data, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which is subsequently utilized to guide multimodal data synthesis. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, not only enhance the spatial perception and reasoning abilities of MLLMs but also exhibit strong generalization capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence.</li>
</ul>

<h3>Title: Learning Composable Chains-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Fangcong Yin, Zeyu Leo Liu, Liu Leqi, Xi Ye, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22635">https://arxiv.org/abs/2505.22635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22635">https://arxiv.org/pdf/2505.22635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22635]] Learning Composable Chains-of-Thought(https://arxiv.org/abs/2505.22635)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train "atomic CoT" models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget.</li>
</ul>

<h3>Title: ObjectClear: Complete Object Removal via Object-Effect Attention</h3>
<ul>
<li><strong>Authors: </strong>Jixin Zhao, Shangchen Zhou, Zhouxia Wang, Peiqing Yang, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22636">https://arxiv.org/abs/2505.22636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22636">https://arxiv.org/pdf/2505.22636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22636]] ObjectClear: Complete Object Removal via Object-Effect Attention(https://arxiv.org/abs/2505.22636)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Object removal requires eliminating not only the target object but also its effects, such as shadows and reflections. However, diffusion-based inpainting methods often produce artifacts, hallucinate content, alter background, and struggle to remove object effects accurately. To address this challenge, we introduce a new dataset for OBject-Effect Removal, named OBER, which provides paired images with and without object effects, along with precise masks for both objects and their associated visual artifacts. The dataset comprises high-quality captured and simulated data, covering diverse object categories and complex multi-object scenes. Building on OBER, we propose a novel framework, ObjectClear, which incorporates an object-effect attention mechanism to guide the model toward the foreground removal regions by learning attention masks, effectively decoupling foreground removal from background reconstruction. Furthermore, the predicted attention map enables an attention-guided fusion strategy during inference, greatly preserving background details. Extensive experiments demonstrate that ObjectClear outperforms existing methods, achieving improved object-effect removal quality and background fidelity, especially in complex scenarios.</li>
</ul>

<h3>Title: SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes</h3>
<ul>
<li><strong>Authors: </strong>Denis Donadel, Gabriele Crestanello, Giulio Morandini, Daniele Antonioli, Mauro Conti, Massimo Merro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22638">https://arxiv.org/abs/2505.22638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22638">https://arxiv.org/pdf/2505.22638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22638]] SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes(https://arxiv.org/abs/2505.22638)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>Industrial Control Systems (ICS) manage critical infrastructures like power grids and water treatment plants. Cyberattacks on ICSs can disrupt operations, causing severe economic, environmental, and safety issues. For example, undetected pollution in a water plant can put the lives of thousands at stake. ICS researchers have increasingly turned to honeypots -- decoy systems designed to attract attackers, study their behaviors, and eventually improve defensive mechanisms. However, existing ICS honeypots struggle to replicate the ICS physical process, making them susceptible to detection. Accurately simulating the noise in ICS physical processes is challenging because different factors produce it, including sensor imperfections and external interferences. In this paper, we propose SimProcess, a novel framework to rank the fidelity of ICS simulations by evaluating how closely they resemble real-world and noisy physical processes. It measures the simulation distance from a target system by estimating the noise distribution with machine learning models like Random Forest. Unlike existing solutions that require detailed mathematical models or are limited to simple systems, SimProcess operates with only a timeseries of measurements from the real system, making it applicable to a broader range of complex dynamic systems. We demonstrate the framework's effectiveness through a case study using real-world power grid data from the EPIC testbed. We compare the performance of various simulation methods, including static and generative noise techniques. Our model correctly classifies real samples with a recall of up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best distribution to simulate our power systems, together with a generative solution provided by an autoencoder, thereby helping developers to improve honeypot fidelity. Additionally, we make our code publicly available.</li>
</ul>

<h3>Title: SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Dekai Zhu, Yixuan Hu, Youquan Liu, Dongyue Lu, Lingdong Kong, Slobodan Ilic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22643">https://arxiv.org/abs/2505.22643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22643">https://arxiv.org/pdf/2505.22643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22643]] SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation(https://arxiv.org/abs/2505.22643)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Leveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, we propose Spiral, a novel range-view LiDAR diffusion model that simultaneously generates depth, reflectance images, and semantic maps. Furthermore, we introduce novel semantic-aware metrics to evaluate the quality of the generated labeled range-view data. Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, outperforming two-step methods that combine the generative and segmentation models. Additionally, we validate that range images generated by Spiral can be effectively used for synthetic data augmentation in the downstream segmentation training, significantly reducing the labeling effort on LiDAR data.</li>
</ul>

<h3>Title: On the Intractability of Chaotic Symbolic Walks: Toward a Non-Algebraic Post-Quantum Hardness Assumption</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Aly Bouke</a></li>
<li><strong>Subjects: </strong>cs.CR, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22644">https://arxiv.org/abs/2505.22644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22644">https://arxiv.org/pdf/2505.22644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22644]] On the Intractability of Chaotic Symbolic Walks: Toward a Non-Algebraic Post-Quantum Hardness Assumption(https://arxiv.org/abs/2505.22644)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Most classical and post-quantum cryptographic assumptions, including integer factorization, discrete logarithms, and Learning with Errors (LWE), rely on algebraic structures such as rings or vector spaces. While mathematically powerful, these structures can be exploited by quantum algorithms or advanced algebraic attacks, raising a pressing need for structure-free alternatives. To address this gap, we introduce the Symbolic Path Inversion Problem (SPIP), a new computational hardness assumption based on symbolic trajectories generated by contractive affine maps with bounded noise over Z2. Unlike traditional systems, SPIP is inherently non-algebraic and relies on chaotic symbolic evolution and rounding-induced non-injectivity to render inversion computationally infeasible. We prove that SPIP is PSPACE-hard and #P-hard, and demonstrate through empirical simulation that even short symbolic sequences (e.g., n = 3, m = 2) can produce over 500 valid trajectories for a single endpoint, with exponential growth reaching 2256 paths for moderate parameters. A quantum security analysis further shows that Grover-style search offers no practical advantage due to oracle ambiguity and verification instability. These results position SPIP as a viable foundation for post-quantum cryptography that avoids the vulnerabilities of algebraic symmetry while offering scalability, unpredictability, and resistance to both classical and quantum inversion.</li>
</ul>

<h3>Title: Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese</h3>
<ul>
<li><strong>Authors: </strong>Hanjia Lyu, Jiebo Luo, Jian Kang, Allison Koenecke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22645">https://arxiv.org/abs/2505.22645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22645">https://arxiv.org/pdf/2505.22645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22645]] Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese(https://arxiv.org/abs/2505.22645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (this https URL).</li>
</ul>

<h3>Title: The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason</h3>
<ul>
<li><strong>Authors: </strong>Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22653">https://arxiv.org/abs/2505.22653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22653">https://arxiv.org/pdf/2505.22653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22653]] The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason(https://arxiv.org/abs/2505.22653)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at this https URL.</li>
</ul>

<h3>Title: 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22657">https://arxiv.org/abs/2505.22657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22657">https://arxiv.org/pdf/2505.22657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22657]] 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model(https://arxiv.org/abs/2505.22657)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.</li>
</ul>

<h3>Title: GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qingchen Yu, Zifan Zheng, Ding Chen, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22661">https://arxiv.org/abs/2505.22661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22661">https://arxiv.org/pdf/2505.22661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22661]] GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning(https://arxiv.org/abs/2505.22661)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The evaluation of large language models (LLMs) has traditionally relied on static benchmarks, a paradigm that poses two major limitations: (1) predefined test sets lack adaptability to diverse application domains, and (2) standardized evaluation protocols often fail to capture fine-grained assessments of domain-specific knowledge and contextual reasoning abilities. To overcome these challenges, we propose GuessArena, an adaptive evaluation framework grounded in adversarial game-based interactions. Inspired by the interactive structure of the Guess Who I Am? game, our framework seamlessly integrates dynamic domain knowledge modeling with progressive reasoning assessment to improve evaluation fidelity. Empirical studies across five vertical domains-finance, healthcare, manufacturing, information technology, and education-demonstrate that GuessArena effectively distinguishes LLMs in terms of domain knowledge coverage and reasoning chain completeness. Compared to conventional benchmarks, our method provides substantial advantages in interpretability, scalability, and scenario adaptability.</li>
</ul>

<h3>Title: AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu, Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, Xia Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22662">https://arxiv.org/abs/2505.22662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22662">https://arxiv.org/pdf/2505.22662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22662]] AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models(https://arxiv.org/abs/2505.22662)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The reasoning-capable large language models (LLMs) demonstrate strong performance on complex reasoning tasks but often suffer from overthinking, generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy reasoning questions, thereby increasing inference cost and latency. Recent approaches attempt to address this challenge by manually deciding when to apply long or short reasoning. However, they lack the flexibility to adapt CoT length dynamically based on question complexity. In this paper, we propose Auto Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that enables LLMs to dynamically compress their generated reasoning path based on the complexity of the reasoning question. AutoL2S enables a learned paradigm, in which LLMs themselves can decide when longer reasoning is necessary and when shorter reasoning suffices, by training on data annotated with our proposed method, which includes both long and short CoT paths and a special <EASY> token. We then use <EASY> token to indicate when the model can skip generating lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs' ability to generate shorter CoT reasoning paths with improved quality after training. Extensive evaluation results show that AutoL2S reduces the length of reasoning generation by up to 57% without compromising performance, demonstrating the effectiveness of AutoL2S for scalable and efficient LLM reasoning.</li>
</ul>

<h3>Title: Zero-Shot Vision Encoder Grafting via LLM Surrogates</h3>
<ul>
<li><strong>Authors: </strong>Kaiyu Yue, Vasu Singla, Menglin Jia, John Kirchenbauer, Rifaa Qadri, Zikui Cai, Abhinav Bhatele, Furong Huang, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.22664">https://arxiv.org/abs/2505.22664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.22664">https://arxiv.org/pdf/2505.22664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.22664]] Zero-Shot Vision Encoder Grafting via LLM Surrogates(https://arxiv.org/abs/2505.22664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small "surrogate models" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
