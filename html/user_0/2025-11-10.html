<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-10</h1>
<h3>Title: Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Pratik Poudel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04686">https://arxiv.org/abs/2511.04686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04686">https://arxiv.org/pdf/2511.04686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04686]] Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity(https://arxiv.org/abs/2511.04686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The Key-Value (KV) cache is integral to efficient autoregressive inference in large language models (LLMs), yet its unbounded growth in stateful multi-turn scenarios presents major challenges. This paper examines the interplay between KV cache management strategies, the architectural context limits of models like meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of positional encodings. Through empirical analysis using a stateful benchmarking framework, we show that LLM generation quality degrades sharply when the accumulated KV cache approaches or exceeds the model's trained context window (e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via AttentionTop), can worsen performance if they disrupt positional coherence. Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a cache by removing non-contiguous tokens can scramble these signals and lead to degenerative outputs. We further show that simple strategies preserving contiguous context blocks (e.g., keeping an initial "gist") can yield more coherent generations than complex or positionally disruptive ones. We advocate for eviction techniques that respect architectural limits, preserve positional structure, and view "cache health" holistically beyond mere size.</li>
</ul>

<h3>Title: Evaluating LLMs' Reasoning Over Ordered Procedural Steps</h3>
<ul>
<li><strong>Authors: </strong>Adrita Anika, Md Messal Monem Miah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04688">https://arxiv.org/abs/2511.04688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04688">https://arxiv.org/pdf/2511.04688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04688]] Evaluating LLMs' Reasoning Over Ordered Procedural Steps(https://arxiv.org/abs/2511.04688)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning over procedural sequences, where the order of steps directly impacts outcomes, is a critical capability for large language models (LLMs). In this work, we study the task of reconstructing globally ordered sequences from shuffled procedural steps, using a curated dataset of food recipes, a domain where correct sequencing is essential for task success. We evaluate several LLMs under zero-shot and few-shot settings and present a comprehensive evaluation framework that adapts established metrics from ranking and sequence alignment. These include Kendall's Tau, Normalized Longest Common Subsequence (NLCS), and Normalized Edit Distance (NED), which capture complementary aspects of ordering quality. Our analysis shows that model performance declines with increasing sequence length, reflecting the added complexity of longer procedures. We also find that greater step displacement in the input, corresponding to more severe shuffling, leads to further degradation. These findings highlight the limitations of current LLMs in procedural reasoning, especially with longer and more disordered inputs.</li>
</ul>

<h3>Title: Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Peiyu Li, Xiuxiu Tang, Si Chen, Ying Cheng, Ronald Metoyer, Ting Hua, Nitesh V. Chawla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04689">https://arxiv.org/abs/2511.04689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04689">https://arxiv.org/pdf/2511.04689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04689]] Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks(https://arxiv.org/abs/2511.04689)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model evaluation requires thousands of benchmark items, making evaluations expensive and slow. Existing methods compute average accuracy across fixed item sets, treating all items equally despite varying quality and informativeness. We present ATLAS an adaptive testing framework using Item Response Theory (IRT) to estimate model ability through Fisher information-guided item selection. Our analysis of five major benchmarks reveals that 3-6% of items exhibit negative discrimination, indicating annotation errors that corrupt static evaluation. ATLAS achieves 90% item reduction while maintaining measurement precision: on HellaSwag (5,608 items), we match full-benchmark estimates using only 42 items with 0.154 MAE. Our framework maintains item exposure rates below 10% and test overlap at 16-27%, compared to static benchmarks where every model sees all items (100% exposure). Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with the same accuracy get different IRT scores, and 23-31% of all models shift by more than 10 rank positions. Code and calibrated item banks are available at this https URL.</li>
</ul>

<h3>Title: Reasoning Up the Instruction Ladder for Controllable Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zishuo Zheng, Vidhisha Balachandran, Chan Young Park, Faeze Brahman, Sachin Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04694">https://arxiv.org/abs/2511.04694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04694">https://arxiv.org/pdf/2511.04694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04694]] Reasoning Up the Instruction Ladder for Controllable Language Models(https://arxiv.org/abs/2511.04694)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language model (LLM) based systems take on high-stakes roles in real-world decision-making, they must reconcile competing instructions from multiple sources (e.g., model developers, users, and tools) within a single prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where higher-level directives override lower-priority requests, is critical for the reliability and controllability of LLMs. In this work, we reframe instruction hierarchy resolution as a reasoning task. Specifically, the model must first "think" about the relationship between a given user prompt and higher-priority (system) instructions before generating a response. To enable this capability via training, we construct VerIH, an instruction hierarchy dataset of constraint-following tasks with verifiable answers. This dataset comprises both aligned and conflicting system-user instructions. We show that lightweight reinforcement learning with VerIH effectively transfers general reasoning capabilities of models to instruction prioritization. Our finetuned models achieve consistent improvements on instruction following and instruction hierarchy benchmarks. This reasoning ability also generalizes to safety-critical settings beyond the training distribution. By treating safety issues as resolving conflicts between adversarial user inputs and predefined higher-priority policies, our trained model enhances robustness against jailbreak and prompt injection attacks. These results demonstrate that reasoning over instruction hierarchies provides a practical path to reliable LLMs, where updates to system prompts yield controllable and robust changes in model behavior.</li>
</ul>

<h3>Title: EncouRAGe: Evaluating RAG Local, Fast, and Reliable</h3>
<ul>
<li><strong>Authors: </strong>Jan Strich, Adeline Scharfenberg, Chris Biemann, Martin Semmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04696">https://arxiv.org/abs/2511.04696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04696">https://arxiv.org/pdf/2511.04696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04696]] EncouRAGe: Evaluating RAG Local, Fast, and Reliable(https://arxiv.org/abs/2511.04696)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce EncouRAGe, a comprehensive Python framework designed to streamline the development and evaluation of Retrieval-Augmented Generation (RAG) systems using Large Language Models (LLMs) and Embedding Models. EncouRAGe comprises five modular and extensible components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics, facilitating flexible experimentation and extensible development. The framework emphasizes scientific reproducibility, diverse evaluation metrics, and local deployment, enabling researchers to efficiently assess datasets within RAG workflows. This paper presents implementation details and an extensive evaluation across multiple benchmark datasets, including 25k QA pairs and over 51k documents. Our results show that RAG still underperforms compared to the Oracle Context, while Hybrid BM25 consistently achieves the best results across all four datasets. We further examine the effects of reranking, observing only marginal performance improvements accompanied by higher response latency.</li>
</ul>

<h3>Title: multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder</h3>
<ul>
<li><strong>Authors: </strong>K M Sajjadul Islam, John Fields, Praveen Madiraju</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04698">https://arxiv.org/abs/2511.04698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04698">https://arxiv.org/pdf/2511.04698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04698]] multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder(https://arxiv.org/abs/2511.04698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, explainability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The early detection of mental health disorders from social media text is critical for enabling timely support, risk assessment, and referral to appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned RoBERTa model designed for multiclass classification of common mental health conditions, including stress, anxiety, depression, post-traumatic stress disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple curated datasets, data exploration is conducted to analyze class overlaps, revealing strong correlations between depression and suicidal ideation as well as anxiety and PTSD, while stress emerges as a broad, overlapping category. Comparative experiments with traditional machine learning methods, domain-specific transformers, and prompting-based large language models demonstrate that multiMentalRoBERTa achieves superior performance, with macro F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup (excluding stress), outperforming both fine-tuned MentalBERT and baseline classifiers. Beyond predictive accuracy, explainability methods, including Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues that drive classification, with a particular focus on distinguishing depression from suicidal ideation. The findings emphasize the effectiveness of fine-tuned transformers for reliable and interpretable detection in sensitive contexts, while also underscoring the importance of fairness, bias mitigation, and human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as a lightweight, robust, and deployable solution for enhancing support in mental health platforms.</li>
</ul>

<h3>Title: Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding</h3>
<ul>
<li><strong>Authors: </strong>Haneen Al-Homoud, Asma Ibrahim, Murtadha Al-Jubran, Fahad Al-Otaibi, Yazeed Al-Harbi, Daulet Toibazar, Kesen Wang, Pedro J. Moreno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04699">https://arxiv.org/abs/2511.04699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04699">https://arxiv.org/pdf/2511.04699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04699]] Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding(https://arxiv.org/abs/2511.04699)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address the scarcity of Arabic resources for Optical Character Recognition (OCR) and Document Understanding (DU). The dataset comprises over 2.5 million of samples, including 1.5 million textual data, 270K fully annotated tables, and hundred thousands of real data based charts. Our pipeline leverages authentic scanned backgrounds, bilingual layouts, and diacritic aware fonts to capture the typographic and structural complexity of Arabic documents. In addition to text, the corpus includes variety of rendered styles for charts and tables. Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart Extraction Score (CharTeX) improved as well in other modalities. SynthDocs provides a scalable, visually realistic resource for advancing research in multilingual document analysis.</li>
</ul>

<h3>Title: Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Song Wang, Zihan Chen, Peng Wang, Zhepei Wei, Zhen Tan, Yu Meng, Cong Shen, Jundong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04700">https://arxiv.org/abs/2511.04700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04700">https://arxiv.org/pdf/2511.04700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04700]] Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation(https://arxiv.org/abs/2511.04700)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.</li>
</ul>

<h3>Title: Measuring what Matters: Construct Validity in Large Language Model Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Andrew M. Bean, Ryan Othniel Kearns, Angelika Romanou, Franziska Sofia Hafner, Harry Mayne, Jan Batzner, Negar Foroutan, Chris Schmitz, Karolina Korgul, Hunar Batra, Oishi Deb, Emma Beharry, Cornelius Emde, Thomas Foster, Anna Gausen, María Grandury, Simeng Han, Valentin Hofmann, Lujain Ibrahim, Hazel Kim, Hannah Rose Kirk, Fangru Lin, Gabrielle Kaili-May Liu, Lennart Luettgau, Jabez Magomere, Jonathan Rystrøm, Anna Sotnikova, Yushi Yang, Yilun Zhao, Adel Bibi, Antoine Bosselut, Ronald Clark, Arman Cohan, Jakob Foerster, Yarin Gal, Scott A. Hale, Inioluwa Deborah Raji, Christopher Summerfield, Philip H.S. Torr, Cozmin Ududec, Luc Rocher, Adam Mahdi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04703">https://arxiv.org/abs/2511.04703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04703">https://arxiv.org/pdf/2511.04703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04703]] Measuring what Matters: Construct Validity in Large Language Model Benchmarks(https://arxiv.org/abs/2511.04703)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.</li>
</ul>

<h3>Title: POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Tingyue Yang, Junchi Yao, Yuhui Guo, Chang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04705">https://arxiv.org/abs/2511.04705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04705">https://arxiv.org/pdf/2511.04705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04705]] POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios(https://arxiv.org/abs/2511.04705)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce POLIS-Bench, the first rigorous, systematic evaluation suite designed for LLMs operating in governmental bilingual policy scenarios. Compared to existing benchmarks, POLIS-Bench introduces three major advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive, up-to-date policy corpus that significantly scales the effective assessment sample size, ensuring relevance to current governance practice. (ii) Scenario-Grounded Task Design: We distill three specialized, scenario-grounded tasks -- Clause Retrieval & Interpretation, Solution Generation, and the Compliance Judgmen--to comprehensively probe model understanding and application. (iii) Dual-Metric Evaluation Framework: We establish a novel dual-metric evaluation framework combining semantic similarity with accuracy rate to precisely measure both content alignment and task requirement adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on POLIS-Bench reveals a clear performance hierarchy where reasoning models maintain superior cross-task stability and accuracy, highlighting the difficulty of compliance tasks. Furthermore, leveraging our benchmark, we successfully fine-tune a lightweight open-source model. The resulting POLIS series models achieves parity with, or surpasses, strong proprietary baselines on multiple policy subtasks at a significantly reduced cost, providing a cost-effective and compliant path for robust real-world governmental deployment.</li>
</ul>

<h3>Title: Jailbreaking in the Haystack</h3>
<ul>
<li><strong>Authors: </strong>Rishi Rajesh Shah, Chen Henry Wu, Shashwat Saxena, Ziqian Zhong, Alexander Robey, Aditi Raghunathan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04707">https://arxiv.org/abs/2511.04707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04707">https://arxiv.org/pdf/2511.04707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04707]] Jailbreaking in the Haystack(https://arxiv.org/abs/2511.04707)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals. Critical to our method is the observation that the position of harmful goals play an important role in safety. Experiments on standard safety benchmark, HarmBench, show that NINJA significantly increases attack success rates across state-of-the-art open and proprietary models, including LLaMA, Qwen, Mistral, and Gemini. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-optimal -- under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts -- when crafted with careful goal positioning -- introduce fundamental vulnerabilities in modern LMs.</li>
</ul>

<h3>Title: GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hari Mohan Pandey, Anshul Gupta, Subham Sarkar, Minakshi Tomer, Schneider Johannes, Yan Gong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04710">https://arxiv.org/abs/2511.04710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04710">https://arxiv.org/pdf/2511.04710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04710]] GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models(https://arxiv.org/abs/2511.04710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Text-to-SQL systems enable users to interact with structured databases using natural language, eliminating the need for specialized programming knowledge. In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL model built upon the open-source Gemma 2B architecture. Unlike many large language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient, iterative manner and can be deployed on low-cost hardware. Leveraging the SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple prompting strategies, including few-shot learning, to enhance SQL query generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct, achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy, outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and CodeXDavinci. The proposed approach demonstrates that effective prompt design and targeted instruction tuning can significantly boost performance while maintaining high scalability and adaptability. These results position GEMMA-SQL as a practical, open-source alternative for robust and accessible text-to-SQL systems.</li>
</ul>

<h3>Title: SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Wenyuan Yang, Yichen Sun, Changzheng Chen, Zhixuan Chu, Jiaheng Zhang, Yiming Li, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04711">https://arxiv.org/abs/2511.04711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04711">https://arxiv.org/pdf/2511.04711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04711]] SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking(https://arxiv.org/abs/2511.04711)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>Large-scale vision-language models, especially CLIP, have demonstrated remarkable performance across diverse downstream tasks. Soft prompts, as carefully crafted modules that efficiently adapt vision-language models to specific tasks, necessitate effective copyright protection. In this paper, we investigate model copyright protection by auditing whether suspicious third-party models incorporate protected soft prompts. While this can be viewed as a special case of model ownership auditing, our analysis shows that existing techniques are ineffective due to prompt learning's unique characteristics. Non-intrusive auditing is inherently prone to false positives when independent models share similar data distributions with victim models. Intrusive approaches also fail: backdoor methods designed for CLIP cannot embed functional triggers, while extending traditional DNN backdoor techniques to prompt learning suffers from harmfulness and ambiguity challenges. We find that these failures in intrusive auditing stem from the same fundamental reason: watermarking operates within the same decision space as the primary task yet pursues opposing objectives. Motivated by these findings, we propose sequential watermarking for soft prompts (SWAP), which implants watermarks into a different and more complex space. SWAP encodes watermarks through a specific order of defender-specified out-of-distribution classes, inspired by the zero-shot prediction capability of CLIP. This watermark, which is embedded in a more complex space, keeps the original prediction label unchanged, making it less opposed to the primary task. We further design a hypothesis-test-guided verification protocol for SWAP and provide theoretical analyses of success conditions. Extensive experiments on 11 datasets demonstrate SWAP's effectiveness, harmlessness, and robustness against potential adaptive attacks.</li>
</ul>

<h3>Title: First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation</h3>
<ul>
<li><strong>Authors: </strong>Dmytro Vitel, Anshuman Chhabra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04715">https://arxiv.org/abs/2511.04715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04715">https://arxiv.org/pdf/2511.04715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04715]] First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation(https://arxiv.org/abs/2511.04715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field.</li>
</ul>

<h3>Title: P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models</h3>
<ul>
<li><strong>Authors: </strong>Mingliang Hou, Yinuo Wang, Teng Guo, Zitao Liu, Wenzhou Dou, Jiaqi Zheng, Renqiang Luo, Mi Tian, Weiqi Luo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04716">https://arxiv.org/abs/2511.04716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04716">https://arxiv.org/pdf/2511.04716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04716]] P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models(https://arxiv.org/abs/2511.04716)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, explainability</a></li>
<li><strong>Abstract: </strong>Cognitive diagnosis models (CDMs) are pivotal for creating fine-grained learner profiles in modern intelligent education platforms. However, these models are trained on sensitive student data, raising significant privacy concerns. While membership inference attacks (MIA) have been studied in various domains, their application to CDMs remains a critical research gap, leaving their privacy risks unquantified. This paper is the first to systematically investigate MIA against CDMs. We introduce a novel and realistic grey box threat model that exploits the explainability features of these platforms, where a model's internal knowledge state vectors are exposed to users through visualizations such as radar charts. We demonstrate that these vectors can be accurately reverse-engineered from such visualizations, creating a potent attack surface. Based on this threat model, we propose a profile-based MIA (P-MIA) framework that leverages both the model's final prediction probabilities and the exposed internal knowledge state vectors as features. Extensive experiments on three real-world datasets against mainstream CDMs show that our grey-box attack significantly outperforms standard black-box baselines. Furthermore, we showcase the utility of P-MIA as an auditing tool by successfully evaluating the efficacy of machine unlearning techniques and revealing their limitations.</li>
</ul>

<h3>Title: Learning to reason about rare diseases through retrieval-augmented agents</h3>
<ul>
<li><strong>Authors: </strong>Ha Young Kim, Jun Li, Ana Beatriz Solana, Carolin M. Pirkl, Benedikt Wiestler, Julia A. Schnabel, Cosmin I. Bercea</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04720">https://arxiv.org/abs/2511.04720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04720">https://arxiv.org/pdf/2511.04720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04720]] Learning to reason about rare diseases through retrieval-augmented agents(https://arxiv.org/abs/2511.04720)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Rare diseases represent the long tail of medical imaging, where AI models often fail due to the scarcity of representative training data. In clinical workflows, radiologists frequently consult case reports and literature when confronted with unfamiliar findings. Following this line of reasoning, we introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic system for rare disease detection in brain MRI. Our approach uses AI agents with access to external medical knowledge by embedding both case reports and literature using sentence transformers and indexing them with FAISS to enable efficient similarity search. The agent retrieves clinically relevant evidence to guide diagnostic decision making on unseen diseases, without the need of additional training. Designed as a model-agnostic reasoning module, RADAR can be seamlessly integrated with diverse large language models, consistently improving their rare pathology recognition and interpretability. On the NOVA dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2% performance gain, with the strongest improvements observed for open source models such as DeepSeek. Beyond accuracy, the retrieved examples provide interpretable, literature grounded explanations, highlighting retrieval-augmented reasoning as a powerful paradigm for low-prevalence conditions in medical imaging.</li>
</ul>

<h3>Title: AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Qianyang Li, Xingjun Zhang, Peng Tao, Shaoxun Wang, Yancheng Pan, Jia Wei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04722">https://arxiv.org/abs/2511.04722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04722">https://arxiv.org/pdf/2511.04722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04722]] AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting(https://arxiv.org/abs/2511.04722)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Forecasting long-term time series in IoT environments remains a significant challenge due to the non-stationary and multi-scale characteristics of sensor signals. Furthermore, error accumulation causes a decrease in forecast quality when predicting further into the future. Traditional methods are restricted to operate in time-domain, while the global frequency information achieved by Fourier transform would be regarded as stationary signals leading to blur the temporal patterns of transient events. We propose AWEMixer, an Adaptive Wavelet-Enhanced Mixer Network including two innovative components: 1) a Frequency Router designs to utilize the global periodicity pattern achieved by Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a Coherent Gated Fusion Block to achieve selective integration of prominent frequency features with multi-scale temporal representation through cross-attention and gating mechanism, which realizes accurate time-frequency localization while remaining robust to noise. Seven public benchmarks validate that our model is more effective than recent state-of-the-art models. Specifically, our model consistently achieves performance improvement compared with transformer-based and MLP-based state-of-the-art models in long-sequence time series forecasting. Code is available at this https URL</li>
</ul>

<h3>Title: Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction</h3>
<ul>
<li><strong>Authors: </strong>Mohamadreza Akbari Pour, Mohamad Sadeq Karimi, Amir Hossein Mazloumi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04723">https://arxiv.org/abs/2511.04723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04723">https://arxiv.org/pdf/2511.04723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04723]] Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction(https://arxiv.org/abs/2511.04723)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Health prediction is crucial for ensuring reliability, minimizing downtime, and optimizing maintenance in industrial systems. Remaining Useful Life (RUL) prediction is a key component of this process; however, many existing models struggle to capture fine-grained temporal dependencies while dynamically prioritizing critical features across time for robust prognostics. To address these challenges, we propose a novel framework that integrates Temporal Convolutional Networks (TCNs) for localized temporal feature extraction with a modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder. This architecture effectively bridges short- and long-term dependencies while emphasizing salient temporal patterns. Furthermore, the incorporation of a multi-time-window methodology improves adaptability across diverse operating conditions. Extensive evaluations on benchmark datasets demonstrate that the proposed model reduces the average RMSE by up to 5.5%, underscoring its improved predictive accuracy compared to state-of-the-art methods. By closing critical gaps in current approaches, this framework advances the effectiveness of industrial prognostic systems and highlights the potential of advanced time-series transformers for RUL prediction.</li>
</ul>

<h3>Title: Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daniyal Ganiuly, Assel Smaiyl</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04728">https://arxiv.org/abs/2511.04728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04728">https://arxiv.org/pdf/2511.04728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04728]] Trustworthiness Calibration Framework for Phishing Email Detection Using Large Language Models(https://arxiv.org/abs/2511.04728)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Phishing emails continue to pose a persistent challenge to online communication, exploiting human trust and evading automated filters through realistic language and adaptive tactics. While large language models (LLMs) such as GPT-4 and LLaMA-3-8B achieve strong accuracy in text classification, their deployment in security systems requires assessing reliability beyond benchmark performance. To address this, this study introduces the Trustworthiness Calibration Framework (TCF), a reproducible methodology for evaluating phishing detectors across three dimensions: calibration, consistency, and robustness. These components are integrated into a bounded index, the Trustworthiness Calibration Index (TCI), and complemented by the Cross-Dataset Stability (CDS) metric that quantifies stability of trustworthiness across datasets. Experiments conducted on five corpora, such as SecureMail 2025, Phishing Validation 2024, CSDMC2010, Enron-Spam, and Nazario, using DeBERTa-v3-base, LLaMA-3-8B, and GPT-4 demonstrate that GPT-4 achieves the strongest overall trust profile, followed by LLaMA-3-8B and DeBERTa-v3-base. Statistical analysis confirms that reliability varies independently of raw accuracy, underscoring the importance of trust-aware evaluation for real-world deployment. The proposed framework establishes a transparent and reproducible foundation for assessing model dependability in LLM-based phishing detection.</li>
</ul>

<h3>Title: CPO: Condition Preference Optimization for Controllable Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zonglin Lyu, Ming Li, Xinxin Liu, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04753">https://arxiv.org/abs/2511.04753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04753">https://arxiv.org/pdf/2511.04753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04753]] CPO: Condition Preference Optimization for Controllable Image Generation(https://arxiv.org/abs/2511.04753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images ($I^{w}$) over less controllable ones ($I^{l}$). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, $\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer $\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over $10\%$ error rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent $2$--$5\%$ reductions in edge and depth maps.</li>
</ul>

<h3>Title: Surprisal reveals diversity gaps in image captioning and different scorers change the story</h3>
<ul>
<li><strong>Authors: </strong>Nikolai Ilinykh, Simon Dobnik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04754">https://arxiv.org/abs/2511.04754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04754">https://arxiv.org/pdf/2511.04754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04754]] Surprisal reveals diversity gaps in image captioning and different scorers change the story(https://arxiv.org/abs/2511.04754)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We quantify linguistic diversity in image captioning with surprisal variance - the spread of token-level negative log-probabilities within a caption set. On the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions. Measured with a caption-trained n-gram LM, humans display roughly twice the surprisal variance of models, but rescoring the same captions with a general-language model reverses the pattern. Our analysis introduces the surprisal-based diversity metric for image captioning. We show that relying on a single scorer can completely invert conclusions, thus, robust diversity evaluation must report surprisal under several scorers.</li>
</ul>

<h3>Title: DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Dhenenjay Yadav, Rohan Sawai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04766">https://arxiv.org/abs/2511.04766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04766">https://arxiv.org/pdf/2511.04766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04766]] DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation(https://arxiv.org/abs/2511.04766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) offer powerful representations for geospatial analysis, but adapting them effectively remains challenging. Standard adaptation methods, whether full fine-tuning or efficient frozen-backbone approaches, typically employ decoders with fixed regularization strategies, failing to account for the significant heterogeneity in satellite imagery. We introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture designed to address this limitation. DARN integrates three key innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide theoretical justifications linking DARN's optimization to stationary point convergence and its mechanism to adaptive information bottlenecks. Empirically, DARN demonstrates exceptional performance across both major adaptation paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering substantial advantages crucial for real-world deployment: superior out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms), enhanced robustness (17% relative reduction in corruption error), and improved performance on minority classes. DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.</li>
</ul>

<h3>Title: Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Xiaoda Wang, Yuji Zhao, Kaiqiao Han, Xiao Luo, Sanne van Rooij, Jennifer Stevens, Lifang He, Liang Zhan, Yizhou Sun, Wei Wang, Carl Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04789">https://arxiv.org/abs/2511.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04789">https://arxiv.org/pdf/2511.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04789]] Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting(https://arxiv.org/abs/2511.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry patterns. Modeling these longitudinal trajectories enables mechanistic insight, treatment development, and individualized 'digital-twin' forecasting. However, existing methods usually adopt recurrent neural networks and transformer architectures, which rely on discrete, regularly sampled data while struggling to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts. Moreover, these methods have difficulty capturing individual heterogeneity including variations in disease onset, progression rate, and symptom severity, which is a hallmark of PD. To address these challenges, we propose CNODE (Conditional Neural ODE), a novel framework for continuous, individualized PD progression forecasting. The core of CNODE is to model morphological brain changes as continuous temporal processes using a neural ODE model. In addition, we jointly learn patient-specific initial time and progress speed to align individual trajectories into a shared progression trajectory. We validate CNODE on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental results show that our method outperforms state-of-the-art baselines in forecasting longitudinal PD progression.</li>
</ul>

<h3>Title: Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Liu, Junjie Liang, Yuqi Jia, Bochuan Cao, Yang Bai, Heng Huang, Xun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04800">https://arxiv.org/abs/2511.04800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04800">https://arxiv.org/pdf/2511.04800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04800]] Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models(https://arxiv.org/abs/2511.04800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for improving the reasoning abilities of large language models (LLMs). The Group Relative Policy Optimization (GRPO) family has demonstrated strong performance in training LLMs with RLVR. However, as models train longer and scale larger, more training prompts become residual prompts, those with zero variance rewards that provide no training signal. Consequently, fewer prompts contribute to training, reducing diversity and hindering effectiveness. To fully exploit these residual prompts, we propose the Explore Residual Prompts in Policy Optimization (ERPO) framework, which encourages exploration on residual prompts and reactivates their training signals. ERPO maintains a history tracker for each prompt and adaptively increases the sampling temperature for residual prompts that previously produced all correct responses. This encourages the model to generate more diverse reasoning traces, introducing incorrect responses that revive training signals. Empirical results on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong baselines across multiple mathematical reasoning benchmarks.</li>
</ul>

<h3>Title: Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose</h3>
<ul>
<li><strong>Authors: </strong>Shuo Zhao, Jianxu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04803">https://arxiv.org/abs/2511.04803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04803">https://arxiv.org/pdf/2511.04803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04803]] Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose(https://arxiv.org/abs/2511.04803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at this https URL.</li>
</ul>

<h3>Title: An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention</h3>
<ul>
<li><strong>Authors: </strong>Shuo Zhao, Yu Zhou, Jianxu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04811">https://arxiv.org/abs/2511.04811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04811">https://arxiv.org/pdf/2511.04811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04811]] An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention(https://arxiv.org/abs/2511.04811)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Biomedical image segmentation is critical for precise structure delineation and downstream analysis. Traditional methods often struggle with noisy data, while deep learning models such as U-Net have set new benchmarks in segmentation performance. nnU-Net further automates model configuration, making it adaptable across datasets without extensive tuning. However, it requires a substantial amount of annotated data for cross-validation, posing a challenge when only raw images but no labels are available. Large foundation models offer zero-shot generalizability, but may underperform on specific datasets with unique characteristics, limiting their direct use for analysis. This work addresses these bottlenecks by proposing a data-centric AI workflow that leverages active learning and pseudo-labeling to combine the strengths of traditional neural networks and large foundation models while minimizing human intervention. The pipeline starts by generating pseudo-labels from a foundation model, which are then used for nnU-Net's self-configuration. Subsequently, a representative core-set is selected for minimal manual annotation, enabling effective fine-tuning of the nnU-Net model. This approach significantly reduces the need for manual annotations while maintaining competitive performance, providing an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks. The code is available at this https URL.</li>
</ul>

<h3>Title: A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Ojeda, Rafael Velasquez, Nicolás Aparicio, Juanita Puentes, Paula Cárdenas, Nicolás Andrade, Gabriel González, Sergio Rincón, Carolina Muñoz-Camargo, Pablo Arbeláez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04814">https://arxiv.org/abs/2511.04814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04814">https://arxiv.org/pdf/2511.04814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04814]] A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification(https://arxiv.org/abs/2511.04814)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Antimicrobial peptides have emerged as promising molecules to combat antimicrobial resistance. However, fragmented datasets, inconsistent annotations, and the lack of standardized benchmarks hinder computational approaches and slow down the discovery of new candidates. To address these challenges, we present the Expanded Standardized Collection for Antimicrobial Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000 peptides from 27 validated repositories. Our dataset separates antimicrobial peptides from negative sequences and incorporates their functional annotations into a biologically coherent multilabel hierarchy, capturing activities across antibacterial, antifungal, antiviral, and antiparasitic classes. Building on ESCAPE, we propose a transformer-based model that leverages sequence and structural information to predict multiple functional activities of peptides. Our method achieves up to a 2.56% relative average improvement in mean Average Precision over the second-best method adapted for this task, establishing a new state-of-the-art multilabel peptide classification. ESCAPE provides a comprehensive and reproducible evaluation framework to advance AI-driven antimicrobial peptide research.</li>
</ul>

<h3>Title: Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiwoo Shin, Byeonghu Na, Mina Kang, Wonhyeok Choi, Il-chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04834">https://arxiv.org/abs/2511.04834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04834">https://arxiv.org/pdf/2511.04834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04834]] Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models(https://arxiv.org/abs/2511.04834)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.</li>
</ul>

<h3>Title: Sublinear iterations can suffice even for DDPMs</h3>
<ul>
<li><strong>Authors: </strong>Matthew S. Zhang, Stephen Huan, Jerry Huang, Nicholas M. Boffi, Sitan Chen, Sinho Chewi</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04844">https://arxiv.org/abs/2511.04844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04844">https://arxiv.org/pdf/2511.04844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04844]] Sublinear iterations can suffice even for DDPMs(https://arxiv.org/abs/2511.04844)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>SDE-based methods such as denoising diffusion probabilistic models (DDPMs) have shown remarkable success in real-world sample generation tasks. Prior analyses of DDPMs have been focused on the exponential Euler discretization, showing guarantees that generally depend at least linearly on the dimension or initial Fisher information. Inspired by works in log-concave sampling (Shen and Lee, 2019), we analyze an integrator -- the denoising diffusion randomized midpoint method (DDRaM) -- that leverages an additional randomized midpoint to better approximate the SDE. Using a recently-developed analytic framework called the "shifted composition rule", we show that this algorithm enjoys favorable discretization properties under appropriate smoothness assumptions, with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure convergence. This is the first sublinear complexity bound for pure DDPM sampling -- prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice. We also provide experimental validation of the advantages of our method, showing that it performs well in practice with pre-trained image synthesis models.</li>
</ul>

<h3>Title: Grounded Test-Time Adaptation for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Arthur Chen, Zuxin Liu, Jianguo Zhang, Akshara Prabhakar, Zhiwei Liu, Shelby Heinecke, Silvio Savarese, Victor Zhong, Caiming Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04847">https://arxiv.org/abs/2511.04847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04847">https://arxiv.org/pdf/2511.04847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04847]] Grounded Test-Time Adaptation for LLM Agents(https://arxiv.org/abs/2511.04847)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.</li>
</ul>

<h3>Title: Geometry Denoising with Preferred Normal Vectors</h3>
<ul>
<li><strong>Authors: </strong>Manuel Weiß, Lukas Baumgärtner, Roland Herzog, Stephan Schmidt</a></li>
<li><strong>Subjects: </strong>cs.CV, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04848">https://arxiv.org/abs/2511.04848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04848">https://arxiv.org/pdf/2511.04848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04848]] Geometry Denoising with Preferred Normal Vectors(https://arxiv.org/abs/2511.04848)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce a new paradigm for geometry denoising using prior knowledge about the surface normal vector. This prior knowledge comes in the form of a set of preferred normal vectors, which we refer to as label vectors. A segmentation problem is naturally embedded in the denoising process. The segmentation is based on the similarity of the normal vector to the elements of the set of label vectors. Regularization is achieved by a total variation term. We formulate a split Bregman (ADMM) approach to solve the resulting optimization problem. The vertex update step is based on second-order shape calculus.</li>
</ul>

<h3>Title: SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Prat, Leo Zhang, Charlotte M. Deane, Yee Whye Teh, Garrett M. Morris</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04854">https://arxiv.org/abs/2511.04854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04854">https://arxiv.org/pdf/2511.04854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04854]] SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion(https://arxiv.org/abs/2511.04854)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Determining the binding pose of a ligand to a protein, known as molecular docking, is a fundamental task in drug discovery. Generative approaches promise faster, improved, and more diverse pose sampling than physics-based methods, but are often hindered by chemically implausible outputs, poor generalisability, and high computational cost. To address these challenges, we introduce a novel fragmentation scheme, leveraging inductive biases from structural chemistry, to decompose ligands into rigid-body fragments. Building on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion model that generates poses by learning to reassemble these rigid bodies within the binding pocket. By operating at the level of fragments in SE(3), SigmaDock exploits well-established geometric priors while avoiding overly complex diffusion processes and unstable training dynamics. Experimentally, we show SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates (RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8% reported by recent deep learning approaches, whilst demonstrating consistent generalisation to unseen proteins. SigmaDock is the first deep learning approach to surpass classical physics-based docking under the PB train-test split, marking a significant leap forward in the reliability and feasibility of deep learning for molecular modelling.</li>
</ul>

<h3>Title: GPT-5 at CTFs: Case Studies From Top-Tier Cybersecurity Events</h3>
<ul>
<li><strong>Authors: </strong>Reworr, Artem Petrov, Dmitrii Volkov</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04860">https://arxiv.org/abs/2511.04860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04860">https://arxiv.org/pdf/2511.04860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04860]] GPT-5 at CTFs: Case Studies From Top-Tier Cybersecurity Events(https://arxiv.org/abs/2511.04860)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>OpenAI and DeepMind's AIs recently got gold at the IMO math olympiad and ICPC programming competition. We show frontier AI is similarly good at hacking by letting GPT-5 compete in elite CTF cybersecurity competitions. In one of this year's hardest events, it outperformed 93% of humans finishing 25th: between the world's #3-ranked team (24th place) and #7-ranked team (26th place). This report walks through our methodology, results, and their implications, and dives deep into 3 problems and solutions we found particularly interesting.</li>
</ul>

<h3>Title: Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Kyle Fogarty, Chenyue Cai, Jing Yang, Zhilin Guo, Cengiz Öztireli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04864">https://arxiv.org/abs/2511.04864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04864">https://arxiv.org/pdf/2511.04864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04864]] Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction(https://arxiv.org/abs/2511.04864)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recovering high-quality surfaces from irregular point cloud is ill-posed unless strong geometric priors are available. We introduce an implicit self-prior approach that distills a shape-specific prior directly from the input point cloud itself and embeds it within an implicit neural representation. This is achieved by jointly training a small dictionary of learnable embeddings with an implicit distance field; at every query location, the field attends to the dictionary via cross-attention, enabling the network to capture and reuse repeating structures and long-range correlations inherent to the shape. Optimized solely with self-supervised point cloud reconstruction losses, our approach requires no external training data. To effectively integrate this learned prior while preserving input fidelity, the trained field is then sampled to extract densely distributed points and analytic normals via automatic differentiation. We integrate the resulting dense point cloud and corresponding normals into a robust implicit moving least squares (RIMLS) formulation. We show this hybrid strategy preserves fine geometric details in the input data, while leveraging the learned prior to regularize sparse regions. Experiments show that our method outperforms both classical and learning-based approaches in generating high-fidelity surfaces with superior detail preservation and robustness to common data degradations.</li>
</ul>

<h3>Title: FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Esha Sharma, Lauren Davis, Julie Ivy, Min Chi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04865">https://arxiv.org/abs/2511.04865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04865">https://arxiv.org/pdf/2511.04865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04865]] FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting(https://arxiv.org/abs/2511.04865)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Food banks are crucial for alleviating food insecurity, but their effectiveness hinges on accurately forecasting highly volatile in-kind donations to ensure equitable and efficient resource distribution. Traditional forecasting models often fail to maintain consistent accuracy due to unpredictable fluctuations and concept drift driven by seasonal variations and natural disasters such as hurricanes in the Southeastern U.S. and wildfires in the West Coast. To address these challenges, we propose FoodRL, a novel reinforcement learning (RL) based metalearning framework that clusters and dynamically weights diverse forecasting models based on recent performance and contextual information. Evaluated on multi-year data from two structurally distinct U.S. food banks-one large regional West Coast food bank affected by wildfires and another state-level East Coast food bank consistently impacted by hurricanes, FoodRL consistently outperforms baseline methods, particularly during periods of disruption or decline. By delivering more reliable and adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent to 1.7 million additional meals annually, demonstrating its significant potential for social impact as well as adaptive ensemble learning for humanitarian supply chains.</li>
</ul>

<h3>Title: Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Preetum Nakkiran, Arwen Bradley, Adam Goliński, Eugene Ndiaye, Michael Kirchhof, Sinead Williamson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04869">https://arxiv.org/abs/2511.04869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04869">https://arxiv.org/pdf/2511.04869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04869]] Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs(https://arxiv.org/abs/2511.04869)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. We find that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. Our main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of "B-calibration," which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. We state three implications of this prediction, which we validate through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. To our knowledge, our work provides the first principled explanation of when and why semantic calibration emerges in LLMs.</li>
</ul>

<h3>Title: Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Girard, Manon Edde, Félix Dumais, Yoan David, Matthieu Dumont, Guillaume Theaud, Jean-Christophe Houde, Arnaud Boré, Maxime Descoteaux, Pierre-Marc Jodoin</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04871">https://arxiv.org/abs/2511.04871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04871">https://arxiv.org/pdf/2511.04871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04871]] Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications(https://arxiv.org/abs/2511.04871)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps are effective for assessing neurodegenerative diseases and microstructural properties of white matter in large number of brain conditions. However, DW-MRI inherently limits the combination of data from multiple acquisition sites without harmonization to mitigate scanner-specific biases. While the widely used ComBAT method reduces site effects in research, its reliance on linear covariate relationships, homogeneous populations, fixed site numbers, and well populated sites constrains its clinical use. To overcome these limitations, we propose Clinical-ComBAT, a method designed for real-world clinical scenarios. Clinical-ComBAT harmonizes each site independently, enabling flexibility as new data and clinics are introduced. It incorporates a non-linear polynomial data model, site-specific harmonization referenced to a normative site, and variance priors adaptable to small cohorts. It further includes hyperparameter tuning and a goodness-of-fit metric for harmonization assessment. We demonstrate its effectiveness on simulated and real data, showing improved alignment of diffusion metrics and enhanced applicability for normative modeling.</li>
</ul>

<h3>Title: Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects</h3>
<ul>
<li><strong>Authors: </strong>James Ndubuisi, Fernando Auat, Marta Vallejo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04872">https://arxiv.org/abs/2511.04872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04872">https://arxiv.org/pdf/2511.04872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04872]] Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects(https://arxiv.org/abs/2511.04872)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study evaluates the efficacy of vision transformer models, specifically Swin transformers, in enhancing the diagnostic accuracy of ear diseases compared to traditional convolutional neural networks. With a reported 27% misdiagnosis rate among specialist otolaryngologists, improving diagnostic accuracy is crucial. The research utilised a real-world dataset from the Department of Otolaryngology at the Clinical Hospital of the Universidad de Chile, comprising otoscopic videos of ear examinations depicting various middle and external ear conditions. Frames were selected based on the Laplacian and Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively, marginally outperforming the ResNet model (99.5%). These results surpassed metrics reported in related studies. However, the evaluation uncovered a critical data leakage issue in the preprocessing step, affecting both this study and related research using the same raw dataset. After mitigating the data leakage, model performance decreased significantly. Corrected accuracies were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This finding highlights the importance of rigorous data handling in machine learning studies, especially in medical applications. The findings indicate that while vision transformers show promise, it is essential to find an optimal balance between the benefits of advanced model architectures and those derived from effective data preprocessing. This balance is key to developing a reliable machine learning model for diagnosing ear diseases.</li>
</ul>

<h3>Title: Bit-Flipping Attack Exploration and Countermeasure in 5G Network</h3>
<ul>
<li><strong>Authors: </strong>Joon Kim, Chengwei Duan, Sandip Ray</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04882">https://arxiv.org/abs/2511.04882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04882">https://arxiv.org/pdf/2511.04882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04882]] Bit-Flipping Attack Exploration and Countermeasure in 5G Network(https://arxiv.org/abs/2511.04882)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>5G communication technology has become a vital component in a wide range of applications due to its unique advantages such as high data rate and low latency. While much of the existing research has focused on optimizing its efficiency and performance, security considerations have not received comparable attention, potentially leaving critical vulnerabilities unexplored. In this work, we investigate the vulnerability of 5G systems to bit-flipping attacks, which is an integrity attack where an adversary intercepts 5G network traffic and modifies specific fields of an encrypted message without decryption, thus mutating the message while remaining valid to the receiver. Notably, these attacks do not require the attacker to know the plaintext, and only the semantic meaning or position of certain fields would be enough to effect targeted modifications. We conduct our analysis on OpenAirInterface (OAI), an open-source 5G platform that follows the 3GPP Technical Specifications, to rigorously test the real-world feasibility and impact of bit-flipping attacks under current 5G encryption mechanisms. Finally, we propose a keystream-based shuffling defense mechanism to mitigate the effect of such attacks by raising the difficulty of manipulating specific encrypted fields, while introducing no additional communication overhead compared to the NAS Integrity Algorithm (NIA) in 5G. Our findings reveal that enhancements to 5G security are needed to better protect against attacks that alter data during transmission at the network level.</li>
</ul>

<h3>Title: Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Di Chen, Jia Li, Michael Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04883">https://arxiv.org/abs/2511.04883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04883">https://arxiv.org/pdf/2511.04883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04883]] Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning(https://arxiv.org/abs/2511.04883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles (AVs) are expected to be commercially available in the near future, leading to mixed autonomy traffic consisting of both AVs and human-driven vehicles (HVs). Although numerous studies have shown that AVs can be deployed to benefit the overall traffic system performance by incorporating system-level goals into their decision making, it is not clear whether the benefits still exist when agents act out of self-interest -- a trait common to all driving agents, both human and autonomous. This study aims to understand whether self-interested AVs can bring benefits to all driving agents in mixed autonomy traffic systems. The research is centered on the concept of collective rationality (CR). This concept, originating from game theory and behavioral economics, means that driving agents may cooperate collectively even when pursuing individual interests. Our recent research has proven the existence of CR in an analytical game-theoretical model and empirically in mixed human-driven traffic. In this paper, we demonstrate that CR can be attained among driving agents trained using deep reinforcement learning (DRL) with a simple reward design. We examine the extent to which self-interested traffic agents can achieve CR without directly incorporating system-level objectives. Results show that CR consistently emerges in various scenarios, which indicates the robustness of this property. We also postulate a mechanism to explain the emergence of CR in the microscopic and dynamic environment and verify it based on simulation evidence. This research suggests the possibility of leveraging advanced learning methods (such as federated learning) to achieve collective cooperation among self-interested driving agents in mixed-autonomy systems.</li>
</ul>

<h3>Title: You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models</h3>
<ul>
<li><strong>Authors: </strong>Shuvendu Roy, Hossein Hajimirsadeghi, Mengyao Zhai, Golnoosh Samei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04902">https://arxiv.org/abs/2511.04902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04902">https://arxiv.org/pdf/2511.04902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04902]] You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models(https://arxiv.org/abs/2511.04902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have demonstrated the promise of unsupervised reinforcement learning (RL) methods for enhancing reasoning capabilities without external supervision. However, the generalizability of these label-free RL approaches to smaller base models with limited reasoning capabilities remains unexplored. In this work, we systematically investigate the performance of label-free RL methods across different model sizes and reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals critical limitations: label-free RL is highly dependent on the base model's pre-existing reasoning capability, with performance often degrading below baseline levels for weaker models. We find that smaller models fail to generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection, and that training data difficulty plays a crucial role in determining success. To address these challenges, we propose a simple yet effective method for label-free RL that utilizes curriculum learning to progressively introduce harder problems during training and mask no-majority rollouts during training. Additionally, we introduce a data curation pipeline to generate samples with predefined difficulty. Our approach demonstrates consistent improvements across all model sizes and reasoning capabilities, providing a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models. We make our code available at this https URL</li>
</ul>

<h3>Title: Efficient Swap Multicalibration of Elicitable Properties</h3>
<ul>
<li><strong>Authors: </strong>Lunjia Hu, Haipeng Luo, Spandan Senapati, Vatsal Sharan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04907">https://arxiv.org/abs/2511.04907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04907">https://arxiv.org/pdf/2511.04907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04907]] Efficient Swap Multicalibration of Elicitable Properties(https://arxiv.org/abs/2511.04907)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Multicalibration [HJKRR18] is an algorithmic fairness perspective that demands that the predictions of a predictor are correct conditional on themselves and membership in a collection of potentially overlapping subgroups of a population. The work of [NR23] established a surprising connection between multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and property elicitation: a property $\Gamma$ can be multicalibrated if and only if it is elicitable, where elicitability is the notion that the true property value of a distribution can be obtained by solving a regression problem over the distribution. In the online setting, [NR23] proposed an inefficient algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a hypothesis class of group membership functions and an elicitable property $\Gamma$, after $T$ rounds of interaction between a forecaster and adversary. In this paper, we generalize multicalibration for an elicitable property $\Gamma$ from group membership functions to arbitrary bounded hypothesis classes and introduce a stronger notion -- swap multicalibration, following [GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when given access to an online agnostic learner, achieves $T^{1/(r+1)}$ $\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a hypothesis class with bounded sequential Rademacher complexity and an elicitable property $\Gamma$. For the special case of $r=2$, this implies an oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap multicalibration error, which significantly improves on the previously established bounds for the problem [NR23, GMS25, LSS25a], and completely resolves an open question raised in [GJRR24] on the possibility of an oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean multicalibration error by answering it in a strongly affirmative sense.</li>
</ul>

<h3>Title: Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application</h3>
<ul>
<li><strong>Authors: </strong>A. Ganapathi Rao, Sathish Krishna Anumula, Aditya Kumar Singh, Renukhadevi M, Y. Jeevan Nagendra Kumar, Tammineni Rama Tulasi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04918">https://arxiv.org/abs/2511.04918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04918">https://arxiv.org/pdf/2511.04918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04918]] Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application(https://arxiv.org/abs/2511.04918)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>It involves the completely novel ways of integrating ML algorithms with traditional statistical modelling that has changed the way we analyze data, do predictive analytics or make decisions in the fields of the data. In this paper, we study some ML and statistical model connections to understand ways in which some modern ML algorithms help 'enrich' conventional models; we demonstrate how new algorithms improve performance, scale, flexibility and robustness of the traditional models. It shows that the hybrid models are of great improvement in predictive accuracy, robustness, and interpretability</li>
</ul>

<h3>Title: BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chandra Vamsi Krishna Alla, Harish Naidu Gaddam, Manohar Kommi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04919">https://arxiv.org/abs/2511.04919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04919">https://arxiv.org/pdf/2511.04919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04919]] BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models(https://arxiv.org/abs/2511.04919)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face significant computational and memory constraints when processing long contexts, despite growing demand for applications requiring reasoning over extensive documents, multi-session dialogues, and book length texts. While recent advances have extended context windows to 100K-1M tokens, such approaches incur prohibitive costs for resource constrained deployments. We propose BudgetMem, a novel memory augmented architecture that learns what to remember rather than remembering everything. Our system combines selective memory policies with feature based salience scoring (entity density, TF-IDF, discourse markers, position bias) to decide which information merits storage under strict budget constraints. Unlike existing retrieval augmented generation (RAG) systems that store all chunks, BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval for efficient information access. Through comprehensive experiments on 700 question answer pairs across short (237 tokens) and long (5K-10K tokens) documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves remarkable results on long documents: only 1.0% F1 score degradation while saving 72.4% memory compared to baseline RAG. We validate our approach through budget sensitivity analysis (testing 7 budget ratios), naive baseline comparisons, and document length analysis, showing that BudgetMem's benefits increase with document length. Our work provides a practical pathway for deploying capable long context systems on modest hardware, democratizing access to advanced language understanding capabilities.</li>
</ul>

<h3>Title: AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent</h3>
<ul>
<li><strong>Authors: </strong>Yu Li, Lehui Li, Qingmin Liao, Fengli Xu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04921">https://arxiv.org/abs/2511.04921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04921">https://arxiv.org/pdf/2511.04921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04921]] AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent(https://arxiv.org/abs/2511.04921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model agents are becoming increasingly capable at web-centric tasks such as information retrieval, complex reasoning. These emerging capabilities have given rise to surge research interests in developing LLM agent for facilitating scientific quest. One key application in AI research is to automate experiment design through agentic dataset and baseline retrieval. However, prior efforts suffer from limited data coverage, as recommendation datasets primarily harvest candidates from public portals and omit many datasets actually used in published papers, and from an overreliance on content similarity that biases model toward superficial similarity and overlooks experimental suitability. Harnessing collective perception embedded in the baseline and dataset citation network, we present a comprehensive framework for baseline and dataset recommendation. First, we design an automated data-collection pipeline that links roughly one hundred thousand accepted papers to the baselines and datasets they actually used. Second, we propose a collective perception enhanced retriever. To represent the position of each dataset or baseline within the scholarly network, it concatenates self-descriptions with aggregated citation contexts. To achieve efficient candidate recall, we finetune an embedding model on these representations. Finally, we develop a reasoning-augmented reranker that exact interaction chains to construct explicit reasoning chains and finetunes a large language model to produce interpretable justifications and refined rankings. The dataset we curated covers 85\% of the datasets and baselines used at top AI conferences over the past five years. On our dataset, the proposed method outperforms the strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in HitRate@5. Taken together, our results advance reliable, interpretable automation of experimental design.</li>
</ul>

<h3>Title: Zero Trust Security Model Implementation in Microservices Architectures Using Identity Federation</h3>
<ul>
<li><strong>Authors: </strong>Rethish Nair Rajendran, Sathish Krishna Anumula, Dileep Kumar Rai, Sachin Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04925">https://arxiv.org/abs/2511.04925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04925">https://arxiv.org/pdf/2511.04925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04925]] Zero Trust Security Model Implementation in Microservices Architectures Using Identity Federation(https://arxiv.org/abs/2511.04925)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, federate</a></li>
<li><strong>Abstract: </strong>The microservice bombshells that have been linked with the microservice expansion have altered the application architectures, offered agility and scalability in terms of complexity in security trade-offs. Feeble legacy-based perimeter-based policies are unable to offer safeguard to distributed workloads and temporary interaction among and in between the services. The article itself is a case on the need of the Zero Trust Security Model of micro services ecosystem, particularly, the fact that human and workloads require identity federation. It is proposed that the solution framework will be based on industry-standard authentication and authorization and end-to-end trust identity technologies, including Authorization and OpenID connect (OIDC), Authorization and OAuth 2.0 token exchange, and Authorization and SPIFFE/ SPIRE workload identities. Experimental evaluation is a unique demonstration of a superior security position of making use of a smaller attack surface, harmony policy enforcement, as well as interoperability across multi- domain environments. The research results overlay that the federated identity combined with the Zero Trust basics not only guarantee the rules relating to authentication and authorization but also fully complies with the latest DevSecOps standards of microservice deployment, which is automated, scaled, and resilient. The current project offers a stringent roadmap to the organizations that desire to apply Zero Trust in cloud-native technologies but will as well guarantee adherence and interoperability.</li>
</ul>

<h3>Title: Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding</h3>
<ul>
<li><strong>Authors: </strong>Hadi Reisizadeh, Jiajun Ruan, Yiwei Chen, Soumyadeep Pal, Sijia Liu, Mingyi Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04934">https://arxiv.org/abs/2511.04934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04934">https://arxiv.org/pdf/2511.04934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04934]] Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding(https://arxiv.org/abs/2511.04934)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Unlearning in large language models (LLMs) is critical for regulatory compliance and for building ethical generative AI systems that avoid producing private, toxic, illegal, or copyrighted content. Despite rapid progress, in this work we show that \textit{almost all} existing unlearning methods fail to achieve true forgetting in practice. Specifically, while evaluations of these `unlearned' models under deterministic (greedy) decoding often suggest successful knowledge removal using standard benchmarks (as has been done in the literature), we show that sensitive information reliably resurfaces when models are sampled with standard probabilistic decoding. To rigorously capture this vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric that quantifies the likelihood of forgotten knowledge reappearing when generating $k$ samples from the model under realistic decoding strategies. Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the first large-scale, systematic study of unlearning reliability using our newly defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge leakage persists across methods and tasks, underscoring that current state-of-the-art unlearning techniques provide only limited forgetting and highlighting the urgent need for more robust approaches to LLM unlearning.</li>
</ul>

<h3>Title: The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective</h3>
<ul>
<li><strong>Authors: </strong>Lei Chen, Erci Xu, Yiming Sun, Shengyu Fan, Xianglong Deng, Guiming Shi, Guang Fan, Liang Kong, Yilan Zhu, Shoumeng Yan, Mingzhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04946">https://arxiv.org/abs/2511.04946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04946">https://arxiv.org/pdf/2511.04946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04946]] The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective(https://arxiv.org/abs/2511.04946)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Fully Homomorphic Encryption (FHE) allows computations to be performed on encrypted data, significantly enhancing user privacy. However, the I/O challenges associated with deploying FHE applications remains understudied. We analyze the impact of storage I/O on the performance of FHE applications and summarize key lessons from the status quo. Key results include that storage I/O can degrade the performance of ASICs by as much as 357$\times$ and reduce GPUs performance by up to 22$\times$.</li>
</ul>

<h3>Title: DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tharindu Fernando, Clinton Fookes, Sridha Sridharan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04949">https://arxiv.org/abs/2511.04949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04949">https://arxiv.org/pdf/2511.04949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04949]] DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning(https://arxiv.org/abs/2511.04949)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, watermark, generative</a></li>
<li><strong>Abstract: </strong>Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.</li>
</ul>

<h3>Title: LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Wei Shao, Lingchao Zheng, Pengyu Wang, Peizhen Zheng, Jun Li, Yuwei Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04952">https://arxiv.org/abs/2511.04952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04952">https://arxiv.org/pdf/2511.04952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04952]] LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model(https://arxiv.org/abs/2511.04952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Long context inference scenarios have become increasingly important for large language models, yet they introduce significant computational latency. While prior research has optimized long-sequence inference through operators, model architectures, and system frameworks, tokenization remains an overlooked bottleneck. Existing parallel tokenization methods accelerate processing through text segmentation and multi-process tokenization, but they suffer from inconsistent results due to boundary artifacts that occur after merging. To address this, we propose LoPT, a novel Lossless Parallel Tokenization framework that ensures output identical to standard sequential tokenization. Our approach employs character-position-based matching and dynamic chunk length adjustment to align and merge tokenized segments accurately. Extensive experiments across diverse long-text datasets demonstrate that LoPT achieves significant speedup while guaranteeing lossless tokenization. We also provide theoretical proof of consistency and comprehensive analytical studies to validate the robustness of our method.</li>
</ul>

<h3>Title: Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</h3>
<ul>
<li><strong>Authors: </strong>Zihao Yi, Qingxuan Jiang, Ruotian Ma, Xingyu Chen, Qu Yang, Mengru Wang, Fanghua Ye, Ying Shen, Zhaopeng Tu, Xiaolong Li, Linus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04962">https://arxiv.org/abs/2511.04962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04962">https://arxiv.org/pdf/2511.04962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04962]] Too Good to be Bad: On the Failure of LLMs to Role-Play Villains(https://arxiv.org/abs/2511.04962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly tasked with creative generation, including the simulation of fictional characters. However, their ability to portray non-prosocial, antagonistic personas remains largely unexamined. We hypothesize that the safety alignment of modern LLMs creates a fundamental conflict with the task of authentically role-playing morally ambiguous or villainous characters. To investigate this, we introduce the Moral RolePlay benchmark, a new dataset featuring a four-level moral alignment scale and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs with role-playing characters from moral paragons to pure villains. Our large-scale evaluation reveals a consistent, monotonic decline in role-playing fidelity as character morality decreases. We find that models struggle most with traits directly antithetical to safety principles, such as ``Deceitful'' and ``Manipulative'', often substituting nuanced malevolence with superficial aggression. Furthermore, we demonstrate that general chatbot proficiency is a poor predictor of villain role-playing ability, with highly safety-aligned models performing particularly poorly. Our work provides the first systematic evidence of this critical limitation, highlighting a key tension between model safety and creative fidelity. Our benchmark and findings pave the way for developing more nuanced, context-aware alignment methods.</li>
</ul>

<h3>Title: Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xiongri Shen, Jiaqi Wang, Yi Zhong, Zhenxi Song, Leilei Zhao, Yichen Wei, Lingyan Liang, Shuqiang Wang, Baiying Lei, Demao Deng, Zhiguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04963">https://arxiv.org/abs/2511.04963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04963">https://arxiv.org/pdf/2511.04963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04963]] Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement(https://arxiv.org/abs/2511.04963)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and diffusion MRI (dMRI), is essential for studying neurodegenerative diseases. However, missing modalities pose a major barrier to their clinical use. Although GAN- and diffusion model-based approaches have shown some promise in modality completion, they remain limited in fMRI-dMRI synthesis due to (1) significant BOLD vs. diffusion-weighted signal differences between fMRI and dMRI in time/gradient axis, and (2) inadequate integration of disease-related neuroanatomical patterns during generation. To address these challenges, we propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D diffusion framework for cross-modality learning, and (2) a tissue refinement network integrated with a efficient microstructure refinement to maintain structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and 30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation, the synthesized data show strong diagnostic performance, achieving 67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic experiments. Code is available in \href{this https URL}{PDS GitHub Repository}</li>
</ul>

<h3>Title: Learning Fourier shapes to probe the geometric world of deep neural networks</h3>
<ul>
<li><strong>Authors: </strong>Jian Wang, Yixing Yong, Haixia Bi, Lijun He, Fan Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04970">https://arxiv.org/abs/2511.04970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04970">https://arxiv.org/pdf/2511.04970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04970]] Learning Fourier shapes to probe the geometric world of deep neural networks(https://arxiv.org/abs/2511.04970)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.</li>
</ul>

<h3>Title: Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques</h3>
<ul>
<li><strong>Authors: </strong>Esha Chowdhury (Dhaka University of Engineering &amp; Technology Gazipur, Bangladesh)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04971">https://arxiv.org/abs/2511.04971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04971">https://arxiv.org/pdf/2511.04971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04971]] Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques(https://arxiv.org/abs/2511.04971)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurate prediction of cardiovascular disease (CVD) risk is crucial for healthcare institutions. This study addresses the growing prevalence of diabetes and its strong link to heart disease by proposing an efficient CVD risk prediction model for diabetic patients using machine learning (ML) and hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by removing duplicates, handling missing values, identifying categorical and numerical features, and applying Principal Component Analysis (PCA) for feature extraction. Several ML models, including Decision Trees (DT), Random Forest (RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and XGBoost, were implemented, with XGBoost achieving the highest accuracy of 0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM, BiLSTM, and GRU, were also explored. Some of these models achieved perfect recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050. Our research highlights the effectiveness of ML and DL models in predicting CVD risk among diabetic patients, automating and enhancing clinical decision-making. High accuracy and F1 scores demonstrate these models' potential to improve personalized risk management and preventive strategies.</li>
</ul>

<h3>Title: Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features</h3>
<ul>
<li><strong>Authors: </strong>Dylan Peek, Matthew P. Skerritt, Siddharth Pritam, Stephan Chalup</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04972">https://arxiv.org/abs/2511.04972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04972">https://arxiv.org/pdf/2511.04972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04972]] Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features(https://arxiv.org/abs/2511.04972)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Topological Data Analysis (TDA) involves techniques of analyzing the underlying structure and connectivity of data. However, traditional methods like persistent homology can be computationally demanding, motivating the development of neural network-based estimators capable of reducing computational overhead and inference time. A key barrier to advancing these methods is the lack of labeled 3D data with class distributions and diversity tailored specifically for supervised learning in TDA tasks. To address this, we introduce a novel approach for systematically generating labeled 3D datasets using the Repulsive Surface algorithm, allowing control over topological invariants, such as hole count. The resulting dataset offers varied geometry with topological labeling, making it suitable for training and benchmarking neural network estimators. This paper uses a synthetic 3D dataset to train a genus estimator network, created using a 3D convolutional transformer architecture. An observed decrease in accuracy as deformations increase highlights the role of not just topological complexity, but also geometric complexity, when training generalized estimators. This dataset fills a gap in labeled 3D datasets and generation for training and evaluating models and techniques for TDA.</li>
</ul>

<h3>Title: Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Li, Yifan Sun, Lei Cheng, Lewen Wang, Yang Liu, Weiqing Liu, Jianlong Li, Jiang Bian, Shikai Fang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04973">https://arxiv.org/abs/2511.04973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04973">https://arxiv.org/pdf/2511.04973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04973]] Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces(https://arxiv.org/abs/2511.04973)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Generative models for multivariate time series are essential for data augmentation, simulation, and privacy preservation, yet current state-of-the-art diffusion-based approaches are slow and limited to fixed-length windows. We propose FAR-TS, a simple yet effective framework that combines disentangled factorization with an autoregressive Transformer over a discrete, quantized latent space to generate time series. Each time series is decomposed into a data-adaptive basis that captures static cross-channel correlations and temporal coefficients that are vector-quantized into discrete tokens. A LLaMA-style autoregressive Transformer then models these token sequences, enabling fast and controllable generation of sequences with arbitrary length. Owing to its streamlined design, FAR-TS achieves orders-of-magnitude faster generation than Diffusion-TS while preserving cross-channel correlations and an interpretable latent space, enabling high-quality and flexible time series synthesis.</li>
</ul>

<h3>Title: GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder</h3>
<ul>
<li><strong>Authors: </strong>Heng Er Metilda Chee, Jiayin Wang, Zhiqiang Guo, Weizhi Ma, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04977">https://arxiv.org/abs/2511.04977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04977">https://arxiv.org/pdf/2511.04977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04977]] GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder(https://arxiv.org/abs/2511.04977)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Stickers have become a popular form of visual communication, yet understanding their semantic relationships remains challenging due to their highly diverse and symbolic content. In this work, we formally {define the Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark for this task, consisting of 905 human-annotated positive and negative sticker pairs. Through extensive evaluation, we show that existing pretrained vision and multimodal models struggle to capture nuanced sticker semantics. To address this, we propose the {General Sticker Encoder (GSE)}, a lightweight and versatile model that learns robust sticker embeddings using both Triple-S and additional datasets. GSE achieves superior performance on unseen stickers, and demonstrates strong results on downstream tasks such as emotion classification and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we provide standardized evaluation tools and robust embeddings, enabling future research in sticker understanding, retrieval, and multimodal content generation. The Triple-S benchmark and GSE have been publicly released and are available here.</li>
</ul>

<h3>Title: Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk</h3>
<ul>
<li><strong>Authors: </strong>Rongbin Ye, Jiaqi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04980">https://arxiv.org/abs/2511.04980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04980">https://arxiv.org/pdf/2511.04980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04980]] Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk(https://arxiv.org/abs/2511.04980)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The financial industry faces a significant challenge modeling and risk portfolios: balancing the predictability of advanced machine learning models, neural network models, and explainability required by regulatory entities (such as Office of the Comptroller of the Currency, Consumer Financial Protection Bureau). This paper intends to fill the gap in the application between these "black box" models and explainability frameworks, such as LIME and SHAP. Authors elaborate on the application of these frameworks on different models and demonstrates the more complex models with better prediction powers could be applied and reach the same level of the explainability, using SHAP and LIME. Beyond the comparison and discussion of performances, this paper proposes a novel five dimensional framework evaluating Inherent Interpretability, Global Explanations, Local Explanations, Consistency, and Complexity to offer a nuanced method for assessing and comparing model explainability beyond simple accuracy metrics. This research demonstrates the feasibility of employing sophisticated, high performing ML models in regulated financial environments by utilizing modern explainability techniques and provides a structured approach to evaluate the crucial trade offs between model performance and interpretability.</li>
</ul>

<h3>Title: Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding</h3>
<ul>
<li><strong>Authors: </strong>Xinheng He, Yijia Zhang, Haowei Lin, Xingang Peng, Xiangzhe Kong, Mingyu Li, Jianzhu Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04984">https://arxiv.org/abs/2511.04984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04984">https://arxiv.org/pdf/2511.04984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04984]] Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding(https://arxiv.org/abs/2511.04984)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Structure-based drug design has seen significant advancements with the integration of artificial intelligence (AI), particularly in the generation of hit and lead compounds. However, most AI-driven approaches neglect the importance of endogenous protein interactions with peptides, which may result in suboptimal molecule designs. In this work, we present Peptide2Mol, an E(3)-equivariant graph neural network diffusion model that generates small molecules by referencing both the original peptide binders and their surrounding protein pocket environments. Trained on large datasets and leveraging sophisticated modeling techniques, Peptide2Mol not only achieves state-of-the-art performance in non-autoregressive generative tasks, but also produces molecules with similarity to the original peptide binder. Additionally, the model allows for molecule optimization and peptidomimetic design through a partial diffusion process. Our results highlight Peptide2Mol as an effective deep generative model for generating and optimizing bioactive small molecules from protein binding pockets.</li>
</ul>

<h3>Title: Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Runsheng Ren, Jing Li, Yanxiu Li, Shixun Huang, Jun Shen, Wanqing Li, John Le, Sheng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04988">https://arxiv.org/abs/2511.04988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04988">https://arxiv.org/pdf/2511.04988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04988]] Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models(https://arxiv.org/abs/2511.04988)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Accurately forecasting carbon prices is essential for informed energy market decision-making, guiding sustainable energy planning, and supporting effective decarbonization strategies. However, it remains challenging due to structural breaks and high-frequency noise caused by frequent policy interventions and market shocks. Existing studies, including the most recent baseline approaches, have attempted to incorporate breakpoints but often treat denoising and modeling as separate processes and lack systematic evaluation across advanced deep learning architectures, limiting the robustness and the generalization capability. To address these gaps, this paper proposes a comprehensive hybrid framework that integrates structural break detection (Bai-Perron, ICSS, and PELT algorithms), wavelet signal denoising, and three state-of-the-art deep learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot prices from 2007 to 2024 and exogenous features such as energy prices and policy indicators, the framework constructs univariate and multivariate datasets for comparative evaluation. Experimental results demonstrate that our proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by 70.55% in RMSE and 74.42% in MAE compared to the original LSTM without decomposition from the same baseline study. These findings underscore the value of integrating structural awareness and multiscale decomposition into deep learning architectures to enhance accuracy and interpretability in carbon price forecasting and other nonstationary financial time series.</li>
</ul>

<h3>Title: Acquiring Common Chinese Emotional Events Using Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Ya Wang, Guangzheng Zhu, Cungen Cao, Jingjing Li, He Li, Xin Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04989">https://arxiv.org/abs/2511.04989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04989">https://arxiv.org/pdf/2511.04989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04989]] Acquiring Common Chinese Emotional Events Using Large Language Model(https://arxiv.org/abs/2511.04989)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge about emotional events is an important kind of knowledge which has been applied to improve the effectiveness of different applications. However, emotional events cannot be easily acquired, especially common or generalized emotional events that are context-independent. The goal of this paper is to obtain common emotional events in Chinese language such as "win a prize" and "be criticized". Our approach begins by collecting a comprehensive list of Chinese emotional event indicators. Then, we generate emotional events by prompting a Chinese large language model (LLM) using these indicators. To ensure the quality of these emotional events, we train a filter to discard invalid generated results. We also classify these emotional events as being positive events and negative events using different techniques. Finally, we harvest a total of 102,218 high-quality common emotional events with sentiment polarity labels, which is the only large-scale commonsense knowledge base of emotional events in Chinese language. Intrinsic evaluation results show that the proposed method in this paper can be effectively used to acquire common Chinese emotional events. An extrinsic use case also demonstrates the strong potential of common emotional events in the field of emotion cause extraction (ECE). Related resources including emotional event indicators and emotional events will be released after the publication of this paper.</li>
</ul>

<h3>Title: BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Daniel S. Lee, Mayra S. Haedo-Cruz, Chen Jiang, Oshin Miranda, LiRong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04998">https://arxiv.org/abs/2511.04998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04998">https://arxiv.org/pdf/2511.04998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04998]] BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records(https://arxiv.org/abs/2511.04998)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based deep learning models have shown promise for disease risk prediction using electronic health records(EHRs), but modeling temporal dependencies remains a key challenge due to irregular visit intervals and lack of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder or BiPETE for single-disease prediction, which integrates rotary positional embeddings to encode relative visit timing and sinusoidal embeddings to preserve visit order. Without relying on large-scale pretraining, BiPETE is trained on EHR data from two mental health cohorts-depressive disorder and post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and substance use disorders (ASUD). BiPETE outperforms baseline models, improving the area under the precision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD cohorts, respectively. An ablation study further confirms the effectiveness of the dual positional encoding strategy. We apply the Integrated Gradients method to interpret model predictions, identifying key clinical features associated with ASUD risk and protection, such as abnormal inflammatory, hematologic, and metabolic markers, as well as specific medications and comorbidities. Overall, these key clinical features identified by the attribution methods contribute to a deeper understanding of the risk assessment process and offer valuable clues for mitigating potential risks. In summary, our study presents a practical and interpretable framework for disease risk prediction using EHR data, which can achieve strong performance.</li>
</ul>

<h3>Title: Multi-agent Coordination via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Dongsu Lee, Daehee Lee, Amy Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05005">https://arxiv.org/abs/2511.05005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05005">https://arxiv.org/pdf/2511.05005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05005]] Multi-agent Coordination via Flow Matching(https://arxiv.org/abs/2511.05005)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\boldsymbol{\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.</li>
</ul>

<h3>Title: Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Aakriti Agrawal, Gouthaman KV, Rohith Aralikatti, Gauri Jagatap, Jiaxin Yuan, Vijay Kamarshi, Andrea Fanelli, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05017">https://arxiv.org/abs/2511.05017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05017">https://arxiv.org/pdf/2511.05017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05017]] Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings(https://arxiv.org/abs/2511.05017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we identify an inherent bias in prevailing LVLM architectures toward the language modality, largely resulting from the common practice of simply appending visual embeddings to the input text sequence. To address this, we propose a simple yet effective method that refines textual embeddings by integrating average-pooled visual features. Our approach demonstrably improves visual grounding and significantly reduces hallucinations on established benchmarks. While average pooling offers a straightforward, robust, and efficient means of incorporating visual information, we believe that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment. Given that the primary focus of this work is to highlight the modality imbalance and its impact on hallucinations -- and to show that refining textual embeddings with visual information mitigates this issue -- we leave exploration of advanced fusion strategies for future work.</li>
</ul>

<h3>Title: Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies</h3>
<ul>
<li><strong>Authors: </strong>Prasoon Varshney, Makesh Narsimhan Sreedhar, Liwei Jiang, Traian Rebedea, Christopher Parisien</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05018">https://arxiv.org/abs/2511.05018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05018">https://arxiv.org/pdf/2511.05018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05018]] Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies(https://arxiv.org/abs/2511.05018)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs' capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques.</li>
</ul>

<h3>Title: OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Dongjin Park, Hasung Yeo, Joon-Woo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05028">https://arxiv.org/abs/2511.05028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05028">https://arxiv.org/pdf/2511.05028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05028]] OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data(https://arxiv.org/abs/2511.05028)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.</li>
</ul>

<h3>Title: Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zhengxuan Li, Qinhui Yang, Yiyu Zhuang, Chuan Guo, Xinxin Zuo, Xiaoxiao Long, Yao Yao, Xun Cao, Qiu Shen, Hao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05038">https://arxiv.org/abs/2511.05038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05038">https://arxiv.org/pdf/2511.05038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05038]] Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance(https://arxiv.org/abs/2511.05038)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.</li>
</ul>

<h3>Title: UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian</h3>
<ul>
<li><strong>Authors: </strong>Mykyta Syromiatnikov, Victoria Ruvinskaya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05040">https://arxiv.org/abs/2511.05040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05040">https://arxiv.org/pdf/2511.05040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05040]] UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian(https://arxiv.org/abs/2511.05040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the real capabilities of large language models in low-resource languages still represents a challenge, as many existing benchmarks focus on widespread tasks translated from English or evaluate only simple language understanding. This paper introduces UA-Code-Bench, a new open-source benchmark established for a thorough evaluation of language models' code generation and competitive programming problem-solving abilities in Ukrainian. The benchmark comprises 500 problems from the Eolymp platform, evenly distributed across five complexity levels from very easy to very hard. A diverse set of 13 leading proprietary and open-source models, generating Python solutions based on a one-shot prompt, was evaluated via the dedicated Eolymp environment against hidden tests, ensuring code correctness. The obtained results reveal that even top-performing models, such as OpenAI o3 and GPT-5, solve only half of the problems, highlighting the challenge of code generation in low-resource natural language. Furthermore, this research presents a comprehensive analysis of performance across various difficulty levels, as well as an assessment of solution uniqueness and computational efficiency, measured by both elapsed time and memory consumption of the generated solutions. In conclusion, this work demonstrates the value of competitive programming benchmarks in evaluating large language models, especially in underrepresented languages. It also paves the way for future research on multilingual code generation and reasoning-enhanced models. The benchmark, data parsing, preparation, code generation, and evaluation scripts are available at this https URL.</li>
</ul>

<h3>Title: Medical Referring Image Segmentation via Next-Token Mask Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Chen, Yiran Wang, Gaoyang Pang, Jiafu Hao, Chentao Yue, Luping Zhou, Yonghui Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05044">https://arxiv.org/abs/2511.05044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05044">https://arxiv.org/pdf/2511.05044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05044]] Medical Referring Image Segmentation via Next-Token Mask Prediction(https://arxiv.org/abs/2511.05044)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical Referring Image Segmentation (MRIS) involves segmenting target regions in medical images based on natural language descriptions. While achieving promising results, recent approaches usually involve complex design of multimodal fusion or multi-stage decoders. In this work, we propose NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive next-token prediction task over a unified multimodal sequence of tokenized image, text, and mask representations. This formulation streamlines model design by eliminating the need for modality-specific fusion and external segmentation models, supports a unified architecture for end-to-end training. It also enables the use of pretrained tokenizers from emerging large-scale multimodal models, enhancing generalization and adaptability. More importantly, to address challenges under this formulation-such as exposure bias, long-tail token distributions, and fine-grained lesion edges-we propose three novel strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance boundary sensitivity and mitigate long-tail distribution effects, and (3) a memory-based Hard Error Token (HET) optimization strategy that emphasizes difficult tokens during training. Extensive experiments on the QaTa-COV19 and MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art performance, offering a streamlined and effective alternative to traditional MRIS pipelines.</li>
</ul>

<h3>Title: No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Sung, Hyeonmin Choe, Il-Min Kim, Sangseok Yun, Jae Mo Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05055">https://arxiv.org/abs/2511.05055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05055">https://arxiv.org/pdf/2511.05055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05055]] No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation(https://arxiv.org/abs/2511.05055)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation (MDE), inferring pixel-level depths in single RGB images from a monocular camera, plays a crucial and pivotal role in a variety of AI applications demanding a three-dimensional (3D) topographical scene. In the real-world scenarios, MDE models often need to be deployed in environments with different conditions from those for training. Test-time (domain) adaptation (TTA) is one of the compelling and practical approaches to address the issue. Although there have been notable advancements in TTA for MDE, particularly in a self-supervised manner, existing methods are still ineffective and problematic when applied to diverse and dynamic environments. To break through this challenge, we propose a novel and high-performing TTA framework for MDE, named PITTA. Our approach incorporates two key innovative strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware image masking. Specifically, PITTA enables highly effective TTA on a pretrained MDE network in a pose-agnostic manner without resorting to any camera pose information. Besides, our instance-aware masking strategy extracts instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.) from a segmentation mask produced by a pretrained panoptic segmentation network, by removing static objects including background components. To further boost performance, we also present a simple yet effective edge extraction methodology for the input image (i.e., a single monocular image) and depth map. Extensive experimental evaluations on DrivingStereo and Waymo datasets with varying environmental conditions demonstrate that our proposed framework, PITTA, surpasses the existing state-of-the-art techniques with remarkable performance improvements in MDE during TTA.</li>
</ul>

<h3>Title: Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach</h3>
<ul>
<li><strong>Authors: </strong>Yuanxiang Huangfu, Chaochao Wang, Weilei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05057">https://arxiv.org/abs/2511.05057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05057">https://arxiv.org/pdf/2511.05057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05057]] Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach(https://arxiv.org/abs/2511.05057)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The effectiveness of Contrastive Language-Image Pre-training (CLIP) models critically depends on the semantic diversity and quality of their training data. However, while existing synthetic data generation methods primarily focus on increasing data volume, such emphasis often leads to limited semantic diversity and redundant or shallow captions. To address this limitation, we propose Role-SynthCLIP, a novel data synthesis framework that leverages multi-perspective role-playing prompts (e.g., a compositional analyst, an interpreter of image context) to guide Multimodal Large Language Models (MLLMs) in generating semantically diverse captions from distinct viewpoints. This mechanism enhances the semantic diversity and fine-grained image-text alignment of synthetic pairs, thereby improving caption expressiveness and accuracy while keeping the total number of image-text pairs unchanged. Experimental results demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on the MS COCO validation set, surpassing the best existing synthetic data baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained models are released at this https URL.</li>
</ul>

<h3>Title: Deep learning models are vulnerable, but adversarial examples are even more vulnerable</h3>
<ul>
<li><strong>Authors: </strong>Jun Li, Yanwei Xu, Keran Li, Xiaoli Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05073">https://arxiv.org/abs/2511.05073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05073">https://arxiv.org/pdf/2511.05073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05073]] Deep learning models are vulnerable, but adversarial examples are even more vulnerable(https://arxiv.org/abs/2511.05073)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Understanding intrinsic differences between adversarial examples and clean samples is key to enhancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy Field Maps and statistical distributions show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%.</li>
</ul>

<h3>Title: Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts</h3>
<ul>
<li><strong>Authors: </strong>Manan Sharma, Arya Suneesh, Manish Jain, Pawan Kumar Rajpoot, Prasanna Devadiga, Bharatdeep Hazarika, Ashish Shrivastava, Kishan Gurumurthy, Anshuman B Suresh, Aditya U Baliga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05078">https://arxiv.org/abs/2511.05078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05078">https://arxiv.org/pdf/2511.05078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05078]] Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts(https://arxiv.org/abs/2511.05078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We address claim normalization for multilingual misinformation detection - transforming noisy social media posts into clear, verifiable statements across 20 languages. The key contribution demonstrates how systematic decomposition of posts using Who, What, Where, When, Why and How questions enables robust cross-lingual transfer despite training exclusively on English data. Our methodology incorporates finetuning Qwen3-14B using LoRA with the provided dataset after intra-post deduplication, token-level recall filtering for semantic alignment and retrieval-augmented few-shot learning with contextual examples during inference. Our system achieves METEOR scores ranging from 41.16 (English) to 15.21 (Marathi), securing third rank on the English leaderboard and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative improvement in METEOR over baseline configurations and substantial gains over existing methods. Results demonstrate effective cross-lingual generalization for Romance and Germanic languages while maintaining semantic coherence across diverse linguistic structures.</li>
</ul>

<h3>Title: On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class</h3>
<ul>
<li><strong>Authors: </strong>P. Bilha Githinji, Aikaterini Meilliou, Peiwu Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05080">https://arxiv.org/abs/2511.05080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05080">https://arxiv.org/pdf/2511.05080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05080]] On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class(https://arxiv.org/abs/2511.05080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing health-seeking behavior and digital consumption of biomedical information by the general public necessitate scalable solutions for automatically adapting complex scientific and technical documents into plain language. Automatic text simplification solutions, including advanced large language models, however, continue to face challenges in reliably arbitrating the tension between optimizing readability performance and ensuring preservation of discourse fidelity. This report empirically assesses the performance of two major classes of general-purpose LLMs, demonstrating their linguistic capabilities and foundational readiness for the task compared to a human benchmark. Using a comparative analysis of the instruction-tuned Mistral 24B and the reasoning-augmented QWen2.5 32B, we identify a potential architectural advantage in the instruction-tuned LLM. Mistral exhibits a tempered lexical simplification strategy that enhances readability across a suite of metrics and the simplification-specific formula SARI (mean 42.46), while preserving human-level discourse with a BERTScore of 0.91. QWen also attains enhanced readability performance, but its operational strategy shows a disconnect in balancing between readability and accuracy, reaching a statistically significantly lower BERTScore of 0.89. Additionally, a comprehensive correlation analysis of 21 metrics spanning readability, discourse fidelity, content safety, and underlying distributional measures for mechanistic insights, confirms strong functional redundancies among five readability indices. This empirical evidence tracks baseline performance of the evolving LLMs for the task of text simplification, identifies the instruction-tuned Mistral 24B for simplification, provides necessary heuristics for metric selection, and points to lexical support as a primary domain-adaptation issue for simplification.</li>
</ul>

<h3>Title: Iterative Layer-wise Distillation for Efficient Compression of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Grigory Kovalev, Mikhail Tikhomirov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05085">https://arxiv.org/abs/2511.05085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05085">https://arxiv.org/pdf/2511.05085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05085]] Iterative Layer-wise Distillation for Efficient Compression of Large Language Models(https://arxiv.org/abs/2511.05085)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.</li>
</ul>

<h3>Title: A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Ruolin Li, Min Liu, Yuan Bian, Zhaoyang Li, Yuzhen Li, Xueping Wang, Yaonan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05092">https://arxiv.org/abs/2511.05092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05092">https://arxiv.org/pdf/2511.05092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05092]] A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification(https://arxiv.org/abs/2511.05092)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>With growing concerns over data privacy, researchers have started using virtual data as an alternative to sensitive real-world images for training person re-identification (Re-ID) models. However, existing virtual datasets produced by game engines still face challenges such as complex construction and poor domain generalization, making them difficult to apply in real scenarios. To address these challenges, we propose a Dual-stage Prompt-driven Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich prompts incorporating multi-dimensional attributes such as pedestrian appearance, illumination, and viewpoint that drive the diffusion model to synthesize diverse data end-to-end, building a large-scale virtual dataset named GenePerson with 130,519 images of 6,641 identities. In the second stage, we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn domain-invariant generalization features. With the aid of contrastive learning, we employ two textual inversion networks to map images into pseudo-words representing style and content, respectively, thereby constructing style-disentangled content prompts to guide the model in learning domain-invariant content features at the image level. Experiments demonstrate that models trained on GenePerson with PDM achieve state-of-the-art generalization performance, surpassing those on popular real and virtual Re-ID datasets.</li>
</ul>

<h3>Title: TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS</h3>
<ul>
<li><strong>Authors: </strong>Arslan Mumtaz, Mridula Singh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05100">https://arxiv.org/abs/2511.05100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05100">https://arxiv.org/pdf/2511.05100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05100]] TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS(https://arxiv.org/abs/2511.05100)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>Global Navigation Satellite Systems (GNSS) provide Positioning, Navigation, and Timing (PNT) information to over 4 billion devices worldwide. Despite its pervasive use in safety critical and high precision applications, GNSS remains vulnerable to spoofing attacks. Cryptographic enhancements, such as the use of TESLA protocol in Galileo, to provide navigation message authentication do not mitigate time of arrival manipulations. In this paper, we propose TRICK, a primitive for secure positioning that closes this gap by introducing a fundamentally new approach that only requires two way communications with a single reference node along with multiple broadcast signals. Unlike classical Verifiable Multilateration (VM), which requires establishing two way communication with each reference nodes, our solution relies on only two measurements with a trusted Low Earth Orbiting (LEO) satellite and combines broadcast navigation signals. We rigorously prove that combining the LEO satellite based two way range measurements and multiple one way ranges such as from broadcast signals of GNSS into ellipsoidal constraint restores the same guarantees as offered by VM whilst using minimal infrastructure and message exchanges. Through detailed analysis, we show that our approach reliably detects spoofing attempts while adding negligible computation overhead.</li>
</ul>

<h3>Title: Quantifying the Risk of Transferred Black Box Attacks</h3>
<ul>
<li><strong>Authors: </strong>Disesdi Susanna Cox, Niklas Bunzel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05102">https://arxiv.org/abs/2511.05102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05102">https://arxiv.org/pdf/2511.05102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05102]] Quantifying the Risk of Transferred Black Box Attacks(https://arxiv.org/abs/2511.05102)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Neural networks have become pervasive across various applications, including security-related products. However, their widespread adoption has heightened concerns regarding vulnerability to adversarial attacks. With emerging regulations and standards emphasizing security, organizations must reliably quantify risks associated with these attacks, particularly regarding transferred adversarial attacks, which remain challenging to evaluate accurately. This paper investigates the complexities involved in resilience testing against transferred adversarial attacks. Our analysis specifically addresses black-box evasion attacks, highlighting transfer-based attacks due to their practical significance and typically high transferability between neural network models. We underline the computational infeasibility of exhaustively exploring high-dimensional input spaces to achieve complete test coverage. As a result, comprehensive adversarial risk mapping is deemed impractical. To mitigate this limitation, we propose a targeted resilience testing framework that employs surrogate models strategically selected based on Centered Kernel Alignment (CKA) similarity. By leveraging surrogate models exhibiting both high and low CKA similarities relative to the target model, the proposed approach seeks to optimize coverage of adversarial subspaces. Risk estimation is conducted using regression-based estimators, providing organizations with realistic and actionable risk quantification.</li>
</ul>

<h3>Title: Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study</h3>
<ul>
<li><strong>Authors: </strong>Yasemin Turkan, F. Boray Tek, M. Serdar Nazlı, Öykü Eren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05106">https://arxiv.org/abs/2511.05106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05106">https://arxiv.org/pdf/2511.05106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05106]] Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study(https://arxiv.org/abs/2511.05106)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associated with neurodegenerative diseases such as Alzheimer's disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject-level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT-specific augmentation techniques were applied, along with a year-weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These findings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches.</li>
</ul>

<h3>Title: SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements</h3>
<ul>
<li><strong>Authors: </strong>Jörg Gamerdinger, Benedict Wetzel, Patrick Schulz, Sven Teufel, Oliver Bringmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05108">https://arxiv.org/abs/2511.05108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05108">https://arxiv.org/pdf/2511.05108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05108]] SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements(https://arxiv.org/abs/2511.05108)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lane detection for autonomous driving in snow-covered environments remains a major challenge due to the frequent absence or occlusion of lane markings. In this paper, we present a novel, robust and realtime capable approach that bypasses the reliance on traditional lane markings by detecting roadside features,specifically vertical roadside posts called delineators, as indirect lane indicators. Our method first perceives these posts, then fits a smooth lane trajectory using a parameterized Bezier curve model, leveraging spatial consistency and road geometry. To support training and evaluation in these challenging scenarios, we introduce SnowyLane, a new synthetic dataset containing 80,000 annotated frames capture winter driving conditions, with varying snow coverage, and lighting conditions. Compared to state-of-the-art lane detection systems, our approach demonstrates significantly improved robustness in adverse weather, particularly in cases with heavy snow occlusion. This work establishes a strong foundation for reliable lane detection in winter scenarios and contributes a valuable resource for future research in all-weather autonomous driving. The dataset is available at this https URL</li>
</ul>

<h3>Title: PhantomFetch: Obfuscating Loads against Prefetcher Side-Channel Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xingzhi Zhang, Buyi Lv, Yimin Lu, Kai Bu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05110">https://arxiv.org/abs/2511.05110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05110">https://arxiv.org/pdf/2511.05110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05110]] PhantomFetch: Obfuscating Loads against Prefetcher Side-Channel Attacks(https://arxiv.org/abs/2511.05110)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>The IP-stride prefetcher has recently been exploited to leak secrets through side-channel attacks. It, however, cannot be simply disabled for security with prefetching speedup as a sacrifice. The state-of-the-art defense tries to retain the prefetching effect by hardware modification. In this paper, we present PhantomFetch as the first prefetching-retentive and hardware-agnostic defense. It avoids potential remanufacturing cost and enriches applicability to off-the-shelf devices. The key idea is to directly break the exploitable coupling between trained prefetcher entries and the victim's secret-dependent loads by obfuscating the sensitive load effects of the victim. The experiment results show that PhantomFetch can secure the IP-stride prefetcher with only negligible overhead.</li>
</ul>

<h3>Title: Confidentiality in a Card-Based Protocol Under Repeated Biased Shuffles</h3>
<ul>
<li><strong>Authors: </strong>Do Hyun Kim, Ahmet Cetinkaya</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05111">https://arxiv.org/abs/2511.05111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05111">https://arxiv.org/pdf/2511.05111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05111]] Confidentiality in a Card-Based Protocol Under Repeated Biased Shuffles(https://arxiv.org/abs/2511.05111)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>In this paper, we provide a probabilistic analysis of the confidentiality in a card-based protocol. We focus on Bert den Boer's original Five Card Trick to develop our approach. Five Card Trick was formulated as a secure two-party computation method, where two players use colored cards with identical backs to calculate the logical AND operation on the bits that they choose. In this method, the players first arrange the cards privately, and then shuffle them through a random cut. Finally, they reveal the shuffled arrangement to determine the result of the operation. An unbiased random cut is essential to prevent players from exposing their chosen bits to each other. However, players typically choose to move cards within the deck even though not moving any cards should be equally likely. This unconscious behavior results in a biased, nonuniform shuffling-distribution in the sense that some arrangements of cards are slightly more probable after the cut. Such a nonuniform distribution creates an opportunity for a malicious player to gain advantage in guessing the other player's choice. We provide the conditional probabilities of such guesses as a way to quantify the information leakage. Furthermore, we utilize the eigenstructure of a Markov chain to derive tight bounds on the number of times the biased random cuts must be repeated to reduce the leakage to an acceptable level. We also discuss the generalization of our approach to the setting where shuffling is conducted by a malicious player.</li>
</ul>

<h3>Title: Usando LLMs para Programar Jogos de Tabuleiro e Variações</h3>
<ul>
<li><strong>Authors: </strong>Álvaro Guglielmin Becker, Lana Bertoldo Rossato, Anderson Rocha Tavares</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05114">https://arxiv.org/abs/2511.05114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05114">https://arxiv.org/pdf/2511.05114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05114]] Usando LLMs para Programar Jogos de Tabuleiro e Variações(https://arxiv.org/abs/2511.05114)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Creating programs to represent board games can be a time-consuming task. Large Language Models (LLMs) arise as appealing tools to expedite this process, given their capacity to efficiently generate code from simple contextual information. In this work, we propose a method to test how capable three LLMs (Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as new variants of existing games.</li>
</ul>

<h3>Title: Cybersecurity AI in OT: Insights from an AI Top-10 Ranker in the Dragos OT CTF 2025</h3>
<ul>
<li><strong>Authors: </strong>Víctor Mayoral-Vilches, Luis Javier Navarrete-Lozano, Francesco Balassone, María Sanz-Gómez, Cristóbal Ricardo Veas Chávez, Maite del Mundo de Torres</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05119">https://arxiv.org/abs/2511.05119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05119">https://arxiv.org/pdf/2511.05119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05119]] Cybersecurity AI in OT: Insights from an AI Top-10 Ranker in the Dragos OT CTF 2025(https://arxiv.org/abs/2511.05119)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Operational Technology (OT) cybersecurity increasingly relies on rapid response across malware analysis, network forensics, and reverse engineering disciplines. We examine the performance of Cybersecurity AI (CAI), powered by the \texttt{alias1} model, during the Dragos OT CTF 2025 -- a 48-hour industrial control system (ICS) competition with more than 1,000 teams. Using CAI telemetry and official leaderboard data, we quantify CAI's trajectory relative to the leading human-operated teams. CAI reached Rank~1 between competition hours 7.0 and 8.0, crossed 10,000 points at 5.42~hours (1,846~pts/h), and completed 32 of the competition's 34 challenges before automated operations were paused at hour~24 with a final score of 18,900 points (6th place). The top-3 human teams solved 33 of 34 challenges, collectively leaving only the 600-point ``Kiddy Tags -- 1'' unsolved; they were also the only teams to clear the 1,000-point ``Moot Force'' binary. The top-5 human teams averaged 1,347~pts/h to the same milestone, marking a 37\% velocity advantage for CAI. We analyse time-resolved scoring, category coverage, and solve cadence. The evidence indicates that a mission-configured AI agent can match or exceed expert human crews in early-phase OT incident response while remaining subject to practical limits in sustained, multi-day operations.</li>
</ul>

<h3>Title: A Toolbox for Improving Evolutionary Prompt Search</h3>
<ul>
<li><strong>Authors: </strong>Daniel Grießhaber, Maximilian Kimmich, Johannes Maucher, Ngoc Thang Vu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05120">https://arxiv.org/abs/2511.05120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05120">https://arxiv.org/pdf/2511.05120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05120]] A Toolbox for Improving Evolutionary Prompt Search(https://arxiv.org/abs/2511.05120)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evolutionary prompt optimization has demonstrated effectiveness in refining prompts for LLMs. However, existing approaches lack robust operators and efficient evaluation mechanisms. In this work, we propose several key improvements to evolutionary prompt optimization that can partially generalize to prompt optimization in general: 1) decomposing evolution into distinct steps to enhance the evolution and its control, 2) introducing an LLM-based judge to verify the evolutions, 3) integrating human feedback to refine the evolutionary operator, and 4) developing more efficient evaluation strategies that maintain performance while reducing computational overhead. Our approach improves both optimization quality and efficiency. We release our code, enabling prompt optimization on new tasks and facilitating further research in this area.</li>
</ul>

<h3>Title: A Secured Intent-Based Networking (sIBN) with Data-Driven Time-Aware Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Urslla Uchechi Izuazu, Mounir Bensalem, Admela Jukan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05133">https://arxiv.org/abs/2511.05133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05133">https://arxiv.org/pdf/2511.05133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05133]] A Secured Intent-Based Networking (sIBN) with Data-Driven Time-Aware Intrusion Detection(https://arxiv.org/abs/2511.05133)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>While Intent-Based Networking (IBN) promises operational efficiency through autonomous and abstraction-driven network management, a critical unaddressed issue lies in IBN's implicit trust in the integrity of intent ingested by the network. This inherent assumption of data reliability creates a blind spot exploitable by Man-in-the-Middle (MitM) attacks, where an adversary intercepts and alters intent before it is enacted, compelling the network to orchestrate malicious configurations. This study proposes a secured IBN (sIBN) system with data driven intrusion detection method designed to secure legitimate user intent from adversarial tampering. The proposed intent intrusion detection system uses a ML model applied for network behavioral anomaly detection to reveal temporal patterns of intent tampering. This is achieved by leveraging a set of original behavioral metrics and newly engineered time-aware features, with the model's hyperparameters fine-tuned through the randomized search cross-validation (RSCV) technique. Numerical results based on real-world data sets, show the effectiveness of sIBN, achieving the best performance across standard evaluation metrics, in both binary and multi classification tasks, while maintaining low error rates.</li>
</ul>

<h3>Title: ManufactuBERT: Efficient Continual Pretraining for Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Robin Armingaud, Romaric Besançon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05135">https://arxiv.org/abs/2511.05135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05135">https://arxiv.org/pdf/2511.05135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05135]] ManufactuBERT: Efficient Continual Pretraining for Manufacturing(https://arxiv.org/abs/2511.05135)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While large general-purpose Transformer-based encoders excel at general language understanding, their performance diminishes in specialized domains like manufacturing due to a lack of exposure to domain-specific terminology and semantics. In this paper, we address this gap by introducing ManufactuBERT, a RoBERTa model continually pretrained on a large-scale corpus curated for the manufacturing domain. We present a comprehensive data processing pipeline to create this corpus from web data, involving an initial domain-specific filtering step followed by a multi-stage deduplication process that removes redundancies. Our experiments show that ManufactuBERT establishes a new state-of-the-art on a range of manufacturing-related NLP tasks, outperforming strong specialized baselines. More importantly, we demonstrate that training on our carefully deduplicated corpus significantly accelerates convergence, leading to a 33\% reduction in training time and computational cost compared to training on the non-deduplicated dataset. The proposed pipeline offers a reproducible example for developing high-performing encoders in other specialized domains. We will release our model and curated corpus at this https URL.</li>
</ul>

<h3>Title: From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection</h3>
<ul>
<li><strong>Authors: </strong>Jingsong Liu, Han Li, Nassir Navab, Peter J. Schüffler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05150">https://arxiv.org/abs/2511.05150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05150">https://arxiv.org/pdf/2511.05150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05150]] From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection(https://arxiv.org/abs/2511.05150)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>AI-based biomarkers can infer molecular features directly from hematoxylin & eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global patch-level embeddings and overlook cell-level morphology. We present a PFM model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local and global tokens. Across four tasks involving four biomarkers and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2% average improvement over prior PFMs, advancing interpretable and robust AI-based biomarker detection in digital pathology.</li>
</ul>

<h3>Title: SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks</h3>
<ul>
<li><strong>Authors: </strong>Azhar Hussain Mozumder, M. John Basha, Chayapathi A. R</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05156">https://arxiv.org/abs/2511.05156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05156">https://arxiv.org/pdf/2511.05156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05156]] SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks(https://arxiv.org/abs/2511.05156)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.</li>
</ul>

<h3>Title: Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results</h3>
<ul>
<li><strong>Authors: </strong>Jan-Thorsten Peter, David Vilar, Tobias Domhan, Dan Malkin, Markus Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05162">https://arxiv.org/abs/2511.05162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05162">https://arxiv.org/pdf/2511.05162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05162]] Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results(https://arxiv.org/abs/2511.05162)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Most current large language models (LLMs) support a wide variety of languages in addition to English, including high-resource languages (e.g. German, Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In addition they have also shown impressive capabilities in different domains, like coding, science and math. In this short paper, taking math as an example domain, we study the performance of different LLMs across languages. Experimental results show that there exists a non-negligible and consistent gap in the performance of the models across languages. Interestingly, and somewhat against expectations, the gap exists for both high- and low-resource languages. We hope that these results influence further research into cross-lingual capability generalization for next generation LLMs. If it weren't for the fact that they are false! By analyzing one of the standard multilingual math benchmarks (MGSM), we determine that several translation errors are present in the data. Furthermore, the lack of standardized answer extraction from LLM outputs further influences the final results. We propose a method for automatic quality assurance to address the first issue at scale, and give recommendations to address the second one. Combining these two approaches we show that the aforementioned language gap mostly disappears, leading to completely different conclusions from our research. We additionally release the corrected dataset to the community.</li>
</ul>

<h3>Title: Another BRIXEL in the Wall: Towards Cheaper Dense Features</h3>
<ul>
<li><strong>Authors: </strong>Alexander Lappe, Martin A. Giese</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05168">https://arxiv.org/abs/2511.05168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05168">https://arxiv.org/pdf/2511.05168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05168]] Another BRIXEL in the Wall: Towards Cheaper Dense Features(https://arxiv.org/abs/2511.05168)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision foundation models achieve strong performance on both global and locally dense downstream tasks. Pretrained on large images, the recent DINOv3 model family is able to produce very fine-grained dense feature maps, enabling state-of-the-art performance. However, computing these feature maps requires the input image to be available at very high resolution, as well as large amounts of compute due to the squared complexity of the transformer architecture. To address these issues, we propose BRIXEL, a simple knowledge distillation approach that has the student learn to reproduce its own feature maps at higher resolution. Despite its simplicity, BRIXEL outperforms the baseline DINOv3 models by large margins on downstream tasks when the resolution is kept fixed. Moreover, it is able to produce feature maps that are very similar to those of the teacher at a fraction of the computational cost. Code and model weights are available at this https URL.</li>
</ul>

<h3>Title: Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy</h3>
<ul>
<li><strong>Authors: </strong>Simon Baur, Tristan Ruhwedel, Ekin Böke, Zuzanna Kobus, Gergana Lishkova, Christoph Wetz, Holger Amthauer, Christoph Roderburg, Frank Tacke, Julian M. Rogasch, Wojciech Samek, Henning Jann, Jackie Ma, Johannes Eschrich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05169">https://arxiv.org/abs/2511.05169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05169">https://arxiv.org/pdf/2511.05169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05169]] Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy(https://arxiv.org/abs/2511.05169)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Peptide receptor radionuclide therapy (PRRT) is an established treatment for metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs only in a subset of patients. Predicting progression-free survival (PFS) could support individualized treatment planning. This study evaluates laboratory, imaging, and multimodal deep learning models for PFS prediction in PRRT-treated patients. In this retrospective, single-center study 116 patients with metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical characteristics, laboratory values, and pretherapeutic somatostatin receptor positron emission tomography/computed tomographies (SR-PET/CT) were collected. Seven models were trained to classify low- vs. high-PFS groups, including unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches. Explainability was evaluated by feature importance analysis and gradient maps. Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1 year). Groups were similar in most characteristics, except for higher baseline chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal three-dimensional convolutional neural networks using SR-PET or CT performed worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch - achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01). Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers outperformed unimodal approaches for PFS prediction after PRRT. Upon external validation, such models may support risk-adapted follow-up strategies.</li>
</ul>

<h3>Title: Associative Poisoning to Generative Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Mathias Lundteigen Mohus, Jingyue Li, Zhirong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05177">https://arxiv.org/abs/2511.05177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05177">https://arxiv.org/pdf/2511.05177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05177]] Associative Poisoning to Generative Machine Learning(https://arxiv.org/abs/2511.05177)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of generative models such as Stable Diffusion and ChatGPT has made them increasingly attractive targets for malicious exploitation, particularly through data poisoning. Existing poisoning attacks compromising synthesised data typically either cause broad degradation of generated data or require control over the training process, limiting their applicability in real-world scenarios. In this paper, we introduce a novel data poisoning technique called associative poisoning, which compromises fine-grained features of the generated data without requiring control of the training process. This attack perturbs only the training data to manipulate statistical associations between specific feature pairs in the generated outputs. We provide a formal mathematical formulation of the attack and prove its theoretical feasibility and stealthiness. Empirical evaluations using two state-of-the-art generative models demonstrate that associative poisoning effectively induces or suppresses feature associations while preserving the marginal distributions of the targeted features and maintaining high-quality outputs, thereby evading visual detection. These results suggest that generative systems used in image synthesis, synthetic dataset generation, and natural language processing are susceptible to subtle, stealthy manipulations that compromise their statistical integrity. To address this risk, we examine the limitations of existing defensive strategies and propose a novel countermeasure strategy.</li>
</ul>

<h3>Title: No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ragini Gupta, Naman Raina, Bo Chen, Li Chen, Claudiu Danilov, Josh Eckhardt, Keyshla Bernard, Klara Nahrstedt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05179">https://arxiv.org/abs/2511.05179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05179">https://arxiv.org/pdf/2511.05179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05179]] No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models(https://arxiv.org/abs/2511.05179)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modern IoT deployments for environmental sensing produce high volume spatiotemporal data to support downstream tasks such as forecasting, typically powered by machine learning models. While existing filtering and strategic deployment techniques optimize collected data volume at the edge, they overlook how variations in sampling frequencies and spatial coverage affect downstream model performance. In many forecasting models, incorporating data from additional sensors denoise predictions by providing broader spatial contexts. This interplay between sampling frequency, spatial coverage and different forecasting model architectures remain underexplored. This work presents a systematic study of forecasting models - classical models (VAR), neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under varying spatial sensor nodes density and sampling intervals using real-world temperature data in a wireless sensor network. Our results show that STGNNs are effective when sensor deployments are sparse and sampling rate is moderate, leveraging spatial correlations via encoded graph structure to compensate for limited coverage. In contrast, TSFMs perform competitively at high frequencies but degrade when spatial coverage from neighboring sensors is reduced. Crucially, the multivariate TSFM Moirai outperforms all models by natively learning cross-sensor dependencies. These findings offer actionable insights for building efficient forecasting pipelines in spatio-temporal systems. All code for model configurations, training, dataset, and logs are open-sourced for reproducibility: this https URL</li>
</ul>

<h3>Title: Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cong-Thanh Do, Rama Doddipatla, Kate Knill</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05184">https://arxiv.org/abs/2511.05184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05184">https://arxiv.org/pdf/2511.05184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05184]] Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models(https://arxiv.org/abs/2511.05184)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting is a widely used method to improve the reasoning capability of Large Language Models (LLMs). More recently, CoT has been leveraged in Knowledge Distillation (KD) to transfer reasoning capability from a larger LLM to a smaller one. This paper examines the role of CoT in distilling the reasoning capability from larger LLMs to smaller LLMs using white-box KD, analysing its effectiveness in improving the performance of the distilled models for various natural language reasoning and understanding tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2 families, employing CoT data from the CoT-Collection dataset. The distilled models are then evaluated on natural language reasoning and understanding tasks from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for smaller LLMs. Experimental results demonstrate the role of CoT in improving white-box KD effectiveness, enabling the distilled models to achieve better average performance in natural language reasoning and understanding tasks from BBH.</li>
</ul>

<h3>Title: Linear Gradient Prediction with Control Variates</h3>
<ul>
<li><strong>Authors: </strong>Kamil Ciosek, Nicolò Felicioni, Juan Elenter Litwin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05187">https://arxiv.org/abs/2511.05187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05187">https://arxiv.org/pdf/2511.05187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05187]] Linear Gradient Prediction with Control Variates(https://arxiv.org/abs/2511.05187)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose a new way of training neural networks, with the goal of reducing training cost. Our method uses approximate predicted gradients instead of the full gradients that require an expensive backward pass. We derive a control-variate-based technique that ensures our updates are unbiased estimates of the true gradient. Moreover, we propose a novel way to derive a predictor for the gradient inspired by the theory of the Neural Tangent Kernel. We empirically show the efficacy of the technique on a vision transformer classification task.</li>
</ul>

<h3>Title: BLADE: Behavior-Level Anomaly Detection Using Network Traffic in Web Services</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Dong, Yong Huang, Shubao Sun, Wentao Cui, Zhihua Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05193">https://arxiv.org/abs/2511.05193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05193">https://arxiv.org/pdf/2511.05193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05193]] BLADE: Behavior-Level Anomaly Detection Using Network Traffic in Web Services(https://arxiv.org/abs/2511.05193)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>With their widespread popularity, web services have become the main targets of various cyberattacks. Existing traffic anomaly detection approaches focus on flow-level attacks, yet fail to recognize behavior-level attacks, which appear benign in individual flows but reveal malicious purpose using multiple network flows. To transcend this limitation, we propose a novel unsupervised traffic anomaly detection system, BLADE, capable of detecting not only flow-level but also behavior-level attacks in web services. Our key observation is that application-layer operations of web services exhibit distinctive communication patterns at the network layer from a multi-flow perspective. BLADE first exploits a flow autoencoder to learn a latent feature representation and calculates its reconstruction losses per flow. Then, the latent representation is assigned a pseudo operation label using an unsupervised clustering method. Next, an anomaly score is computed based on the reconstruction losses. Finally, the triplets of timestamps, pseudo labels, and anomaly scores from multiple flows are aggregated and fed into a one-class classifier to characterize the behavior patterns of legitimate web operations, enabling the detection of flow-level and behavior-level anomalies. BLADE is extensively evaluated on both the custom dataset and the CIC-IDS2017 dataset. The experimental results demonstrate BLADE's superior performance, achieving high F1 scores of 0.9732 and 0.9801, respectively, on the two datasets, and outperforming traditional single-flow anomaly detection baselines.</li>
</ul>

<h3>Title: Walk the Lines 2: Contour Tracking for Detailed Segmentation</h3>
<ul>
<li><strong>Authors: </strong>André Peter Kelm, Max Braeschke, Emre Gülsoylu, Simone Frintrop</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05210">https://arxiv.org/abs/2511.05210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05210">https://arxiv.org/pdf/2511.05210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05210]] Walk the Lines 2: Contour Tracking for Detailed Segmentation(https://arxiv.org/abs/2511.05210)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents Walk the Lines 2 (WtL2), a unique contour tracking algorithm specifically adapted for detailed segmentation of infrared (IR) ships and various objects in RGB.1 This extends the original Walk the Lines (WtL) [12], which focused solely on detailed ship segmentation in color. These innovative WtLs can replace the standard non-maximum suppression (NMS) by using contour tracking to refine the object contour until a 1-pixel-wide closed shape can be binarized, forming a segmentable area in foreground-background scenarios. WtL2 broadens the application range of WtL beyond its original scope, adapting to IR and expanding to diverse objects within the RGB context. To achieve IR segmentation, we adapt its input, the object contour detector, to IR ships. In addition, the algorithm is enhanced to process a wide range of RGB objects, outperforming the latest generation of contour-based methods when achieving a closed object contour, offering high peak Intersection over Union (IoU) with impressive details. This positions WtL2 as a compelling method for specialized applications that require detailed segmentation or high-quality samples, potentially accelerating progress in several niche areas of image segmentation.</li>
</ul>

<h3>Title: FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jiang Lin, Xinyu Chen, Song Wu, Zhiqiu Zhang, Jizhi Zhang, Ye Wang, Qiang Tang, Qian Wang, Jian Yang, Zili Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05219">https://arxiv.org/abs/2511.05219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05219">https://arxiv.org/pdf/2511.05219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05219]] FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction(https://arxiv.org/abs/2511.05219)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Controlling the spatial and semantic structure of diffusion-generated images remains a challenge. Existing methods like ControlNet rely on handcrafted condition maps and retraining, limiting flexibility and generalization. Inversion-based approaches offer stronger alignment but incur high inference cost due to dual-path denoising. We present FreeControl, a training-free framework for semantic structural control in diffusion models. Unlike prior methods that extract attention across multiple timesteps, FreeControl performs one-step attention extraction from a single, optimally chosen key timestep and reuses it throughout denoising. This enables efficient structural guidance without inversion or retraining. To further improve quality and stability, we introduce Latent-Condition Decoupling (LCD): a principled separation of the key timestep and the noised latent used in attention extraction. LCD provides finer control over attention quality and eliminates structural artifacts. FreeControl also supports compositional control via reference images assembled from multiple sources - enabling intuitive scene layout design and stronger prompt alignment. FreeControl introduces a new paradigm for test-time control, enabling structurally and semantically aligned, visually coherent generation directly from raw images, with the flexibility for intuitive compositional design and compatibility with modern diffusion models at approximately 5 percent additional cost.</li>
</ul>

<h3>Title: ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy</h3>
<ul>
<li><strong>Authors: </strong>David Bertram, Anja Ophey, Sinah Röttgen, Konstantin Kuffer, Gereon R. Fink, Elke Kalbe, Clint Hansen, Walter Maetzler, Maximilian Kapsecker, Lara M. Reimer, Stephan Jonas, Andreas T. Damgaard, Natasha B. Bertelsen, Casper Skjaerbaek, Per Borghammer, Karolien Groenewald, Pietro-Luca Ratti, Michele T. Hu, No émie Moreau, Michael Sommerauer, Katarzyna Bozek</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05221">https://arxiv.org/abs/2511.05221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05221">https://arxiv.org/pdf/2511.05221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05221]] ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy(https://arxiv.org/abs/2511.05221)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.</li>
</ul>

<h3>Title: 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Guo, Bo Xu, Yanyan Li, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05229">https://arxiv.org/abs/2511.05229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05229">https://arxiv.org/pdf/2511.05229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05229]] 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos(https://arxiv.org/abs/2511.05229)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.</li>
</ul>

<h3>Title: The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss</h3>
<ul>
<li><strong>Authors: </strong>Rui Wu, Lizheng Wang, Yongjun Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05236">https://arxiv.org/abs/2511.05236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05236">https://arxiv.org/pdf/2511.05236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05236]] The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss(https://arxiv.org/abs/2511.05236)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Judea Pearl's vision of Structural Causal Models (SCMs) as engines for counterfactual reasoning hinges on faithful abduction: the precise inference of latent exogenous noise. For decades, operationalizing this step for complex, non-linear mechanisms has remained a significant computational challenge. The advent of diffusion models, powerful universal function approximators, offers a promising solution. However, we argue that their standard design, optimized for perceptual generation over logical inference, introduces a fundamental flaw for this classical problem: an inherent information loss we term the Structural Reconstruction Error (SRE). To address this challenge, we formalize the principle of Causal Information Conservation (CIC) as the necessary condition for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based framework engineered to be causally sound by eliminating SRE by construction through an analytically invertible mechanism. To operationalize this framework, a Targeted Modeling strategy provides structural regularization, while a Hybrid Training Objective instills a strong causal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework not only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity, individual-level counterfactuals required for deep causal inquiries. Our work provides a foundational blueprint that reconciles the power of modern generative models with the rigor of classical causal theory, establishing a new and more rigorous standard for this emerging field.</li>
</ul>

<h3>Title: Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese</h3>
<ul>
<li><strong>Authors: </strong>Zilong Li, Jie Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05239">https://arxiv.org/abs/2511.05239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05239">https://arxiv.org/pdf/2511.05239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05239]] Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese(https://arxiv.org/abs/2511.05239)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ancient people translated classical Chinese into Japanese by annotating around each character. We abstract this process as sequence tagging tasks and fit them into modern language technologies. The research of this annotation and translation system is a facing low-resource problem. We release this problem by introducing a LLM-based annotation pipeline and construct a new dataset from digitalized open-source translation data. We show that under the low-resource setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the training of sequence tagging tasks. We also evaluate the performance of large language models. They achieve high scores in direct machine translation, but they are confused when being asked to annotate characters. Our method could work as a supplement of LLMs.</li>
</ul>

<h3>Title: ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Xincheng Yao, Yan Luo, Zefeng Qian, Chongyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05245">https://arxiv.org/abs/2511.05245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05245">https://arxiv.org/pdf/2511.05245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05245]] ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining(https://arxiv.org/abs/2511.05245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The current mainstream and state-of-the-art anomaly detection (AD) methods are substantially established on pretrained feature networks yielded by ImageNet pretraining. However, regardless of supervised or self-supervised pretraining, the pretraining process on ImageNet does not match the goal of anomaly detection (i.e., pretraining in natural images doesn't aim to distinguish between normal and abnormal). Moreover, natural images and industrial image data in AD scenarios typically have the distribution shift. The two issues can cause ImageNet-pretrained features to be suboptimal for AD tasks. To further promote the development of the AD field, pretrained representations specially for AD tasks are eager and very valuable. To this end, we propose a novel AD representation learning framework specially designed for learning robust and discriminative pretrained representations for industrial anomaly detection. Specifically, closely surrounding the goal of anomaly detection (i.e., focus on discrepancies between normals and anomalies), we propose angle- and norm-oriented contrastive losses to maximize the angle size and norm difference between normal and abnormal features simultaneously. To avoid the distribution shift from natural images to AD images, our pretraining is performed on a large-scale AD dataset, RealIAD. To further alleviate the potential shift between pretraining data and downstream AD datasets, we learn the pretrained AD representations based on the class-generalizable representation, residual features. For evaluation, based on five embedding-based AD methods, we simply replace their original features with our pretrained representations. Extensive experiments on five AD datasets and five backbones consistently show the superiority of our pretrained features. The code is available at this https URL.</li>
</ul>

<h3>Title: Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection</h3>
<ul>
<li><strong>Authors: </strong>Tiziano Natali, Karin A. Olthof, Niels F.M. Kok, Koert F.D. Kuhlmann, Theo J.M. Ruers, Matteo Fusaglia</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05253">https://arxiv.org/abs/2511.05253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05253">https://arxiv.org/pdf/2511.05253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05253]] Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection(https://arxiv.org/abs/2511.05253)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Introduction: Accurate intraoperative delineation of colorectal liver metastases (CRLM) is crucial for achieving negative resection margins but remains challenging using intraoperative ultrasound (iUS) due to low contrast, noise, and operator dependency. Automated segmentation could enhance precision and efficiency in ultrasound-based navigation workflows. Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two variants were compared: one trained on full iUS volumes and another on cropped regions around tumors. Segmentation accuracy was assessed using Dice Similarity Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference (RVD) on retrospective and prospective datasets. The workflow was integrated into 3D Slicer for real-time intraoperative use. Results: The cropped-volume model significantly outperformed the full-volume model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC = 0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic segmentation but with ~4x faster execution (~ 1 min). Prospective intraoperative testing confirmed robust and consistent performance, with clinically acceptable accuracy for real-time surgical guidance. Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net provides reliable, near real-time results with minimal operator input. The method enables efficient, registration-free ultrasound-based navigation for hepatic surgery, approaching expert-level accuracy while substantially reducing manual workload and procedure time.</li>
</ul>

<h3>Title: An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones</h3>
<ul>
<li><strong>Authors: </strong>Taihelong Zeng, Yun Lin, Yuhe Shi, Yan Li, Zhiqing Wei, Xuanru Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05265">https://arxiv.org/abs/2511.05265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05265">https://arxiv.org/pdf/2511.05265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05265]] An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones(https://arxiv.org/abs/2511.05265)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency.</li>
</ul>

<h3>Title: Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Serrão Seabra (1, 2), Nikolaj T. Mücke (1), Vinicius Luiz Santos Silva (2, 4), Alexandre A. Emerick (2), Denis Voskov (1, 5), Femke Vossepoel (1) ((1) Faculty of Civil Engineering and Geosciences, TU Delft, Delft, Netherlands, (2) Petroleo Brasileiro S.A. (Petrobras), Rio de Janeiro, Brazil, (4) Imperial College London, London, United Kingdom, (5) Department of Energy Resources Engineering, Stanford University, CA, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05266">https://arxiv.org/abs/2511.05266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05266">https://arxiv.org/pdf/2511.05266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05266]] Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage(https://arxiv.org/abs/2511.05266)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate characterization of subsurface heterogeneity is important for the safe and effective implementation of geological carbon storage (GCS) projects. This paper explores how machine learning methods can enhance data assimilation for GCS with a framework that integrates score-based diffusion models with machine learning-enhanced localization in channelized reservoirs during CO$_2$ injection. We employ a machine learning-enhanced localization framework that uses large ensembles ($N_s = 5000$) with permeabilities generated by the diffusion model and states computed by simple ML algorithms to improve covariance estimation for the Ensemble Smoother with Multiple Data Assimilation (ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability fields, generated with the geostatistical model FLUVSIM. Our approach is applied on a CO$_2$ injection scenario simulated using the Delft Advanced Research Terra Simulator (DARTS). Our ML-based localization maintains significantly more ensemble variance than when localization is not applied, while achieving comparable data-matching quality. This framework has practical implications for GCS projects, helping improve the reliability of uncertainty quantification for risk assessment.</li>
</ul>

<h3>Title: DeepEyesV2: Toward Agentic Multimodal Model</h3>
<ul>
<li><strong>Authors: </strong>Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, Xing Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05271">https://arxiv.org/abs/2511.05271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05271">https://arxiv.org/pdf/2511.05271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05271]] DeepEyesV2: Toward Agentic Multimodal Model(https://arxiv.org/abs/2511.05271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Agentic multimodal models should not only comprehend text and images, but also actively invoke external tools, such as code execution environments and web search, and integrate these operations into reasoning. In this work, we introduce DeepEyesV2 and explore how to build an agentic multimodal model from the perspectives of data construction, training methods, and model evaluation. We observe that direct reinforcement learning alone fails to induce robust tool-use behavior. This phenomenon motivates a two-stage training pipeline: a cold-start stage to establish tool-use patterns, and reinforcement learning stage to further refine tool invocation. We curate a diverse, moderately challenging training dataset, specifically including examples where tool use is beneficial. We further introduce RealX-Bench, a comprehensive benchmark designed to evaluate real-world multimodal reasoning, which inherently requires the integration of multiple capabilities, including perception, search, and reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative benchmarks, demonstrating its effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2 exhibits task-adaptive tool invocation, tending to use image operations for perception tasks and numerical computations for reasoning tasks. Reinforcement learning further enables complex tool combinations and allows model to selectively invoke tools based on context. We hope our study can provide guidance for community in developing agentic multimodal models.</li>
</ul>

<h3>Title: Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Teqi Hao, Xioayu Tan, Shaojie Shi, Yinghui Xu, Xihe Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05286">https://arxiv.org/abs/2511.05286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05286">https://arxiv.org/pdf/2511.05286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05286]] Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models(https://arxiv.org/abs/2511.05286)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The personalization of black-box large language models (LLMs) is a critical yet challenging task. Existing approaches predominantly rely on context injection, where user history is embedded into the prompt to directly guide the generation process. However, this single-step paradigm imposes a dual burden on the model: generating accurate content while simultaneously aligning with user-specific styles. This often results in a trade-off that compromises output quality and limits precise control. To address this fundamental tension, we propose Reflective Personalization Optimization (RPO), a novel framework that redefines the personalization paradigm by decoupling content generation from alignment. RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the user's preferences. This reflection module is trained using a two-stage process. Initially, supervised fine-tuning is employed on structured rewriting trajectories to establish a core personalized reasoning policy that models the transformation from generic to user-aligned responses. Subsequently, reinforcement learning is applied to further refine and enhance the quality of the personalized outputs. Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by decoupling content generation from personalization, significantly outperforms state-of-the-art baselines. These findings underscore the superiority of explicit response shaping over implicit context injection. Moreover, RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios.</li>
</ul>

<h3>Title: Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Marius Fracarolli, Michael Staniek, Stefan Riezler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05289">https://arxiv.org/abs/2511.05289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05289">https://arxiv.org/pdf/2511.05289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05289]] Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting(https://arxiv.org/abs/2511.05289)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Balancing strong privacy guarantees with high predictive performance is critical for time series forecasting (TSF) tasks involving Electronic Health Records (EHR). In this study, we explore how data augmentation can mitigate Membership Inference Attacks (MIA) on TSF models. We show that retraining with synthetic data can substantially reduce the effectiveness of loss-based MIAs by reducing the attacker's true-positive to false-positive ratio. The key challenge is generating synthetic samples that closely resemble the original training data to confuse the attacker, while also introducing enough novelty to enhance the model's ability to generalize to unseen data. We examine multiple augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to strengthen model resilience without sacrificing accuracy. Our experimental results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA attacks without sacrificing performance on test data.</li>
</ul>

<h3>Title: What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Yin, Pengcheng Wang, Han Ding, Fei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05292">https://arxiv.org/abs/2511.05292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05292">https://arxiv.org/pdf/2511.05292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05292]] What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs(https://arxiv.org/abs/2511.05292)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Accurate food intake detection is vital for dietary monitoring and chronic disease prevention. Traditional self-report methods are prone to recall bias, while camera-based approaches raise concerns about privacy. Furthermore, existing wearable-based methods primarily focus on a limited number of food types, such as hamburgers and pizza, failing to address the vast diversity of Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that classifies Chinese food types by integrating hand motion cues from a smartwatch with head dynamics from smart glasses. To filter out irrelevant daily activities, we design a two-stage detection pipeline. The first stage identifies eating states by distinguishing characteristic temporal patterns from non-eating behaviors. The second stage then conducts fine-grained food type recognition based on the motions captured during food intake. To evaluate CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings across 11 food categories and 10 participants. Experiments demonstrate that CuisineSense achieves high accuracy in both eating state detection and food classification, offering a practical solution for unobtrusive, wearable-based dietary this http URL system code is publicly available at this https URL.</li>
</ul>

<h3>Title: Cross-domain EEG-based Emotion Recognition with Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Yan, Yibo Li, Han Ding, Fei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05293">https://arxiv.org/abs/2511.05293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05293">https://arxiv.org/pdf/2511.05293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05293]] Cross-domain EEG-based Emotion Recognition with Contrastive Learning(https://arxiv.org/abs/2511.05293)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Electroencephalogram (EEG)-based emotion recognition is vital for affective computing but faces challenges in feature utilization and cross-domain generalization. This work introduces EmotionCLIP, which reformulates recognition as an EEG-text matching task within the CLIP framework. A tailored backbone, SST-LegoViT, captures spatial, spectral, and temporal features using multi-scale convolution and Transformer modules. Experiments on SEED and SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models. Results demonstrate the effectiveness of multimodal contrastive learning for robust EEG emotion recognition.</li>
</ul>

<h3>Title: LiveStar: Live Streaming Assistant for Real-World Online Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05299">https://arxiv.org/abs/2511.05299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05299">https://arxiv.org/pdf/2511.05299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05299]] LiveStar: Live Streaming Assistant for Real-World Online Video Understanding(https://arxiv.org/abs/2511.05299)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStar's state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at this https URL.</li>
</ul>

<h3>Title: Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05308">https://arxiv.org/abs/2511.05308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05308">https://arxiv.org/pdf/2511.05308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05308]] Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation(https://arxiv.org/abs/2511.05308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>As 3D point clouds become a cornerstone of modern technology, the need for sophisticated generative models and reliable evaluation metrics has grown exponentially. In this work, we first expose that some commonly used metrics for evaluating generated point clouds, particularly those based on Chamfer Distance (CD), lack robustness against defects and fail to capture geometric fidelity and local shape consistency when used as quality indicators. We further show that introducing samples alignment prior to distance calculation and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet essential steps to ensure the consistency and robustness of point cloud generative model evaluation metrics. While existing metrics primarily focus on directly comparing 3D Euclidean coordinates, we present a novel metric, named Surface Normal Concordance (SNC), which approximates surface similarity by comparing estimated point normals. This new metric, when combined with traditional ones, provides a more comprehensive evaluation of the quality of generated samples. Finally, leveraging recent advancements in transformer-based models for point cloud analysis, such as serialized patch attention , we propose a new architecture for generating high-fidelity 3D structures, the Diffusion Point Transformer. We perform extensive experiments and comparisons on the ShapeNet dataset, showing that our model outperforms previous solutions, particularly in terms of quality of generated point clouds, achieving new state-of-the-art. Code available at this https URL.</li>
</ul>

<h3>Title: Listening Between the Lines: Decoding Podcast Narratives with Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shreya Gupta, Ojasva Saxena, Arghodeep Nandi, Sarah Masud, Kiran Garimella, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05310">https://arxiv.org/abs/2511.05310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05310">https://arxiv.org/pdf/2511.05310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05310]] Listening Between the Lines: Decoding Podcast Narratives with Language Modeling(https://arxiv.org/abs/2511.05310)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Podcasts have become a central arena for shaping public opinion, making them a vital source for understanding contemporary discourse. Their typically unscripted, multi-themed, and conversational style offers a rich but complex form of data. To analyze how podcasts persuade and inform, we must examine their narrative structures -- specifically, the narrative frames they employ. The fluid and conversational nature of podcasts presents a significant challenge for automated analysis. We show that existing large language models, typically trained on more structured text such as news articles, struggle to capture the subtle cues that human listeners rely on to identify narrative frames. As a result, current approaches fall short of accurately analyzing podcast narratives at scale. To solve this, we develop and evaluate a fine-tuned BERT model that explicitly links narrative frames to specific entities mentioned in the conversation, effectively grounding the abstract frame in concrete details. Our approach then uses these granular frame labels and correlates them with high-level topics to reveal broader discourse trends. The primary contributions of this paper are: (i) a novel frame-labeling methodology that more closely aligns with human judgment for messy, conversational data, and (ii) a new analysis that uncovers the systematic relationship between what is being discussed (the topic) and how it is being presented (the frame), offering a more robust framework for studying influence in digital media.</li>
</ul>

<h3>Title: Attention and Compression is all you need for Controllably Efficient Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jatin Prakash, Aahlad Puli, Rajesh Ranganath</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05313">https://arxiv.org/abs/2511.05313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05313">https://arxiv.org/pdf/2511.05313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05313]] Attention and Compression is all you need for Controllably Efficient Language Models(https://arxiv.org/abs/2511.05313)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The quadratic cost of attention in transformers motivated the development of efficient approaches: namely sparse and sliding window attention, convolutions and linear attention. Although these approaches result in impressive reductions in compute and memory, they often trade-off with quality, specifically in-context recall performance. Moreover, apriori fixing this quality-compute tradeoff means being suboptimal from the get-go: some downstream applications require more memory for in-context recall, while others require lower latency and memory. Further, these approaches rely on heuristic choices that artificially restrict attention, or require handcrafted and complex recurrent state update rules, or they must be carefully composed with attention at specific layers to form a hybrid architecture that complicates the design process, especially at scale. To address above issues, we propose Compress & Attend Transformer (CAT), a conceptually simple architecture employing two simple ingredients only: dense attention and compression. CAT decodes chunks of tokens by attending to compressed chunks of the sequence so far. Compression results in decoding from a reduced sequence length that yields compute and memory savings, while choosing a particular chunk size trades-off quality for efficiency. Moreover, CAT can be trained with multiple chunk sizes at once, unlocking control of quality-compute trade-offs directly at test-time without any retraining, all in a single adaptive architecture. In exhaustive evaluations on common language modeling tasks, in-context recall, and long-context understanding, a single adaptive CAT model outperforms existing efficient baselines, including hybrid architectures, across different compute-memory budgets. Further, a single CAT matches dense transformer in language modeling across model scales while being 1.4-3x faster and requiring 2-9x lower total memory usage.</li>
</ul>

<h3>Title: $\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huanqi Wu, Huangbiao Xu, Runfeng Xie, Jiaxin Cai, Kaixin Zhang, Xiao Ke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05319">https://arxiv.org/abs/2511.05319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05319">https://arxiv.org/pdf/2511.05319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05319]] $\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models(https://arxiv.org/abs/2511.05319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although steganography has made significant advancements in recent years, it still struggles to embed semantically rich, sentence-level information into carriers. However, in the era of AIGC, the capacity of steganography is more critical than ever. In this work, we present Sentence-to-Image Steganography, an instance of Semantic Steganography, a novel task that enables the hiding of arbitrary sentence-level messages within a cover image. Furthermore, we establish a benchmark named Invisible Text (IVT), comprising a diverse set of sentence-level texts as secret messages for evaluation. Finally, we present $\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large language models (LLMs) to embed high-level textual information, such as sentences or even paragraphs, into images. Unlike traditional bit-level counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich content through a newly designed pipeline in which the LLM is involved throughout the entire process. Both quantitative and qualitative experiments demonstrate that our method effectively unlocks new semantic steganographic capabilities for LLMs. The source code will be released soon.</li>
</ul>

<h3>Title: What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions</h3>
<ul>
<li><strong>Authors: </strong>Klára Bendová, Tomáš Knap, Jan Černý, Vojtěch Pour, Jaromir Savelka, Ivana Kvapilíková, Jakub Drápal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05320">https://arxiv.org/abs/2511.05320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05320">https://arxiv.org/pdf/2511.05320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05320]] What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions(https://arxiv.org/abs/2511.05320)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courts' decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on "sparing" and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%.</li>
</ul>

<h3>Title: Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE</h3>
<ul>
<li><strong>Authors: </strong>Firoj Ahmmed Patwary, Abdullah Al Noman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05324">https://arxiv.org/abs/2511.05324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05324">https://arxiv.org/pdf/2511.05324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05324]] Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE(https://arxiv.org/abs/2511.05324)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Tokenization is an important first step in Natural Language Processing (NLP) pipelines because it decides how models learn and represent linguistic information. However, current subword tokenizers like SentencePiece or HuggingFace BPE are mostly designed for Latin or multilingual corpora and do not perform well on languages with rich morphology such as Bengali. To address this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer specifically developed for the Bengali script. BengaliBPE applies Unicode normalization, grapheme-level initialization, and morphology-aware merge rules to maintain linguistic consistency and preserve subword integrity. We use a large-scale Bengali news classification dataset to compare BengaliBPE with three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The evaluation considers tokenization granularity, encoding speed, and downstream classification accuracy. While all methods perform reasonably well, BengaliBPE provides the most detailed segmentation and the best morphological interpretability, albeit with slightly higher computational cost. These findings highlight the importance of language-aware tokenization for morphologically rich scripts and establish BengaliBPE as a strong foundation for future Bengali NLP systems, including large-scale pretraining of contextual language models.</li>
</ul>

<h3>Title: Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Janet Jenq, Hongda Shen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05325">https://arxiv.org/abs/2511.05325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05325">https://arxiv.org/pdf/2511.05325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05325]] Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval(https://arxiv.org/abs/2511.05325)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Multimodal product retrieval systems in e-commerce platforms rely on effectively combining visual and textual signals to improve search relevance and user experience. However, vision-language models such as CLIP are vulnerable to typographic attacks, where misleading or irrelevant text embedded in images skews model predictions. In this work, we propose a novel method that reverses the logic of typographic attacks by rendering relevant textual content (e.g., titles, descriptions) directly onto product images to perform vision-text compression, thereby strengthening image-text alignment and boosting multimodal product retrieval performance. We evaluate our method on three vertical-specific e-commerce datasets (sneakers, handbags, and trading cards) using six state-of-the-art vision foundation models. Our experiments demonstrate consistent improvements in unimodal and multimodal retrieval accuracy across categories and model families. Our findings suggest that visually rendering product metadata is a simple yet effective enhancement for zero-shot multimodal retrieval in e-commerce applications.</li>
</ul>

<h3>Title: SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Yuan Huang, Armin Lederer, Dai-Jie Wu, Xiaobing Dai, Sihua Zhang, Stefan Sosnowski, Shao-Hua Sun, Sandra Hirche</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05355">https://arxiv.org/abs/2511.05355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05355">https://arxiv.org/pdf/2511.05355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05355]] SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning(https://arxiv.org/abs/2511.05355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow matching (FM) has shown promising results in data-driven planning. However, it inherently lacks formal guarantees for ensuring state and action constraints, whose satisfaction is a fundamental and crucial requirement for the safety and admissibility of planned trajectories on various systems. Moreover, existing FM planners do not ensure the dynamical consistency, which potentially renders trajectories inexecutable. We address these shortcomings by proposing SAD-Flower, a novel framework for generating Safe, Admissible, and Dynamically consistent trajectories. Our approach relies on an augmentation of the flow with a virtual control input. Thereby, principled guidance can be derived using techniques from nonlinear control theory, providing formal guarantees for state constraints, action constraints, and dynamic consistency. Crucially, SAD-Flower operates without retraining, enabling test-time satisfaction of unseen constraints. Through extensive experiments across several tasks, we demonstrate that SAD-Flower outperforms various generative-model-based baselines in ensuring constraint satisfaction.</li>
</ul>

<h3>Title: Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects</h3>
<ul>
<li><strong>Authors: </strong>Manuel Gomes, Bogdan Raducanu, Miguel Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05356">https://arxiv.org/abs/2511.05356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05356">https://arxiv.org/pdf/2511.05356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05356]] Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects(https://arxiv.org/abs/2511.05356)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Articulated object perception presents significant challenges in computer vision, particularly because most existing methods ignore temporal dynamics despite the inherently dynamic nature of such objects. The use of 4D temporal data has not been thoroughly explored in articulated object perception and remains unexamined for panoptic segmentation. The lack of a benchmark dataset further hurt this field. To this end, we introduce Artic4D as a new dataset derived from PartNet Mobility and augmented with synthetic sensor data, featuring 4D panoptic annotations and articulation parameters. Building on this dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework. This approach explicitly estimates per-frame offsets mapping observed object parts to a learned canonical space, thereby enhancing part-level segmentation. The framework employs this canonical representation to achieve consistent alignment of object parts across sequential frames. Comprehensive experiments on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the art approaches in panoptic segmentation accuracy in more complex scenarios. These findings highlight the effectiveness of temporal modeling and canonical alignment in dynamic object understanding, and pave the way for future advances in 4D articulated object perception.</li>
</ul>

<h3>Title: Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Tsukerman, Konstantin Grotov, Pavel Ginzburg</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.app-ph, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05357">https://arxiv.org/abs/2511.05357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05357">https://arxiv.org/pdf/2511.05357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05357]] Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media(https://arxiv.org/abs/2511.05357)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a conditional diffusion model for electromagnetic inverse design that generates structured media geometries directly from target differential scattering cross-section profiles, bypassing expensive iterative optimization. Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map desired angular scattering patterns to 2x2 dielectric sphere structure, naturally handling the non-uniqueness of inverse problems by sampling diverse valid designs. Trained on 11,000 simulated metasurfaces, the model achieves median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES evolutionary optimization while reducing design time from hours to seconds. These results demonstrate that employing diffusion models is promising for advancing electromagnetic inverse design research, potentially enabling rapid exploration of complex metasurface architectures and accelerating the development of next-generation photonic and wireless communication systems. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations</h3>
<ul>
<li><strong>Authors: </strong>Amr Gomaa, Ahmed Salem, Sahar Abdelnabi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05359">https://arxiv.org/abs/2511.05359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05359">https://arxiv.org/pdf/2511.05359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05359]] ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations(https://arxiv.org/abs/2511.05359)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>As language models evolve into autonomous agents that act and communicate on behalf of users, ensuring safety in multi-agent ecosystems becomes a central challenge. Interactions between personal assistants and external service providers expose a core tension between utility and protection: effective collaboration requires information sharing, yet every exchange creates new attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating privacy and security risks in agent-agent interactions. ConVerse spans three practical domains (travel, real estate, insurance) with 12 user personas and over 864 contextually grounded attacks (611 privacy, 253 security). Unlike prior single-agent settings, it models autonomous, multi-turn agent-to-agent conversations where malicious requests are embedded within plausible discourse. Privacy is tested through a three-tier taxonomy assessing abstraction quality, while security attacks target tool use and preference manipulation. Evaluating seven state-of-the-art models reveals persistent vulnerabilities; privacy attacks succeed in up to 88% of cases and security breaches in up to 60%, with stronger models leaking more. By unifying privacy and security within interactive multi-agent contexts, ConVerse reframes safety as an emergent property of communication.</li>
</ul>

<h3>Title: Dense Motion Captioning</h3>
<ul>
<li><strong>Authors: </strong>Shiyao Xu, Benedetta Liberatori, Gül Varol, Paolo Rota</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05369">https://arxiv.org/abs/2511.05369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05369">https://arxiv.org/pdf/2511.05369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05369]] Dense Motion Captioning(https://arxiv.org/abs/2511.05369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.</li>
</ul>

<h3>Title: PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05393">https://arxiv.org/abs/2511.05393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05393">https://arxiv.org/pdf/2511.05393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05393]] PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization(https://arxiv.org/abs/2511.05393)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.</li>
</ul>

<h3>Title: Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yiting He, Zhishuai Liu, Weixin Wang, Pan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05396">https://arxiv.org/abs/2511.05396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05396">https://arxiv.org/pdf/2511.05396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05396]] Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction(https://arxiv.org/abs/2511.05396)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.</li>
</ul>

<h3>Title: Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments</h3>
<ul>
<li><strong>Authors: </strong>Laura Alejandra Encinar Gonzalez, John Folkesson, Rudolph Triebel, Riccardo Giubilato</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05404">https://arxiv.org/abs/2511.05404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05404">https://arxiv.org/pdf/2511.05404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05404]] Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments(https://arxiv.org/abs/2511.05404)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at this http URL.</li>
</ul>

<h3>Title: Large Language Models for Explainable Threat Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Tiago Dinis, Miguel Correia, Roger Tavares</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05406">https://arxiv.org/abs/2511.05406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05406">https://arxiv.org/pdf/2511.05406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05406]] Large Language Models for Explainable Threat Intelligence(https://arxiv.org/abs/2511.05406)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>As cyber threats continue to grow in complexity, traditional security mechanisms struggle to keep up. Large language models (LLMs) offer significant potential in cybersecurity due to their advanced capabilities in text processing and generation. This paper explores the use of LLMs with retrieval-augmented generation (RAG) to obtain threat intelligence by combining real-time information retrieval with domain-specific data. The proposed system, RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats. Moreover, it makes this form of Artificial Intelligence (AI) explainable by generating and visually presenting to the user a knowledge graph for every reply. This increases the transparency and interpretability of the reasoning of the model, allowing analysts to better understand the connections made by the system based on the context recovered by the RAG system. We evaluated RAGRecon experimentally with two datasets and seven different LLMs and the responses matched the reference responses more than 91% of the time for the best combinations.</li>
</ul>

<h3>Title: Steering Language Models with Weight Arithmetic</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Fabien Roger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05408">https://arxiv.org/abs/2511.05408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05408">https://arxiv.org/pdf/2511.05408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05408]] Steering Language Models with Weight Arithmetic(https://arxiv.org/abs/2511.05408)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Providing high-quality feedback to Large Language Models (LLMs) on a diverse training distribution can be difficult and expensive, and providing feedback only on a narrow distribution can result in unintended generalizations. To better leverage narrow training data, we propose contrastive weight steering, a simple post-training method that edits the model parameters using weight arithmetic. We isolate a behavior direction in weight-space by subtracting the weight deltas from two small fine-tunes -- one that induces the desired behavior and another that induces its opposite -- and then add or remove this direction to modify the model's weights. We apply this technique to mitigate sycophancy and induce misalignment, and find that weight steering often generalizes further than activation steering, achieving stronger out-of-distribution behavioral control before degrading general capabilities. We also show that, in the context of task-specific fine-tuning, weight steering can partially mitigate undesired behavioral drift: it can reduce sycophancy and under-refusals introduced during fine-tuning while preserving task performance gains. Finally, we provide preliminary evidence that emergent misalignment can be detected by measuring the similarity between fine-tuning updates and an "evil" weight direction, suggesting that it may be possible to monitor the evolution of weights during training and detect rare misaligned behaviors that never manifest during training or evaluations.</li>
</ul>

<h3>Title: APP: Accelerated Path Patching with Task-Specific Pruning</h3>
<ul>
<li><strong>Authors: </strong>Frauke Andersen, William Rudman, Ruochen Zhang, Carsten Eickhoff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05442">https://arxiv.org/abs/2511.05442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05442">https://arxiv.org/pdf/2511.05442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05442]] APP: Accelerated Path Patching with Task-Specific Pruning(https://arxiv.org/abs/2511.05442)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Circuit discovery is a key step in many mechanistic interpretability pipelines. Current methods, such as Path Patching, are computationally expensive and have limited in-depth circuit analysis for smaller models. In this study, we propose Accelerated Path Patching (APP), a hybrid approach leveraging our novel contrastive attention head pruning method to drastically reduce the search space of circuit discovery methods. Our Contrastive-FLAP pruning algorithm uses techniques from causal mediation analysis to assign higher pruning scores to task-specific attention heads, leading to higher performing sparse models compared to traditional pruning techniques. Although Contrastive-FLAP is successful at preserving task-specific heads that existing pruning algorithms remove at low sparsity ratios, the circuits found by Contrastive-FLAP alone are too large to satisfy the minimality constraint required in circuit analysis. APP first applies Contrastive-FLAP to reduce the search space on required for circuit discovery algorithms by, on average, 56\%. Next, APP, applies traditional Path Patching on the remaining attention heads, leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to the dense model. Despite the substantial computational saving that APP provides, circuits obtained from APP exhibit substantial overlap and similar performance to previously established Path Patching circuits</li>
</ul>

<h3>Title: Adversarially Robust Multitask Adaptive Control</h3>
<ul>
<li><strong>Authors: </strong>Kasra Fallah, Leonardo F. Toso, James Anderson</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05444">https://arxiv.org/abs/2511.05444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05444">https://arxiv.org/pdf/2511.05444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05444]] Adversarially Robust Multitask Adaptive Control(https://arxiv.org/abs/2511.05444)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study adversarially robust multitask adaptive linear quadratic control; a setting where multiple systems collaboratively learn control policies under model uncertainty and adversarial corruption. We propose a clustered multitask approach that integrates clustering and system identification with resilient aggregation to mitigate corrupted model updates. Our analysis characterizes how clustering accuracy, intra-cluster heterogeneity, and adversarial behavior affect the expected regret of certainty-equivalent (CE) control across LQR tasks. We establish non-asymptotic bounds demonstrating that the regret decreases inversely with the number of honest systems per cluster and that this reduction is preserved under a bounded fraction of adversarial systems within each cluster.</li>
</ul>

<h3>Title: How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?</h3>
<ul>
<li><strong>Authors: </strong>Tuan Anh Tran, Duy M. H. Nguyen, Hoai-Chau Tran, Michael Barz, Khoa D. Doan, Roger Wattenhofer, Ngo Anh Vien, Mathias Niepert, Daniel Sonntag, Paul Swoboda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05449">https://arxiv.org/abs/2511.05449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05449">https://arxiv.org/pdf/2511.05449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05449]] How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?(https://arxiv.org/abs/2511.05449)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at this https URL</li>
</ul>

<h3>Title: Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models</h3>
<ul>
<li><strong>Authors: </strong>Sarkar Snigdha Sarathi Das, Palash Goyal, Mihir Parmar, Yiwen Song, Long T. Le, Lesly Miculicich, Jinsung Yoon, Rui Zhang, Hamid Palangi, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05460">https://arxiv.org/abs/2511.05460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05460">https://arxiv.org/pdf/2511.05460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05460]] Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models(https://arxiv.org/abs/2511.05460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pre-trained Time Series Foundational Models (TSFMs) represent a significant advance, capable of forecasting diverse time series with complex characteristics, including varied seasonalities, trends, and long-range dependencies. Despite their primary goal of universal time series forecasting, their efficacy is far from uniform; divergent training protocols and data sources cause individual TSFMs to exhibit highly variable performance across different forecasting tasks, domains, and horizons. Leveraging this complementary expertise by arbitrating existing TSFM outputs presents a compelling strategy, yet this remains a largely unexplored area of research. In this paper, we conduct a thorough examination of how different TSFMs exhibit specialized performance profiles across various forecasting settings, and how we can effectively leverage this behavior in arbitration between different time series models. We specifically analyze how factors such as model selection and forecast horizon distribution can influence the efficacy of arbitration strategies. Based on this analysis, we propose Synapse, a novel arbitration framework for TSFMs. Synapse is designed to dynamically leverage a pool of TSFMs, assign and adjust predictive weights based on their relative, context-dependent performance, and construct a robust forecast distribution by adaptively sampling from the output quantiles of constituent models. Experimental results demonstrate that Synapse consistently outperforms other popular ensembling techniques as well as individual TSFMs, demonstrating Synapse's efficacy in time series forecasting.</li>
</ul>

<h3>Title: Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Xian-Hong Huang, Hui-Kai Su, Chi-Chia Sun, Jun-Wei Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05474">https://arxiv.org/abs/2511.05474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05474">https://arxiv.org/pdf/2511.05474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05474]] Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection(https://arxiv.org/abs/2511.05474)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces a cutting-edge approach to cross-modal interaction for tiny object detection by combining semantic-guided natural language processing with advanced visual recognition backbones. The proposed method integrates the BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By employing lemmatization and fine-tuning techniques, the system aligns semantic cues from textual inputs with visual features, enhancing detection precision for small and complex objects. Experimental validation using the COCO and Objects365 datasets demonstrates that the model achieves superior performance. On the COCO2017 validation set, it attains a 52.6% average precision (AP), outperforming YOLO-World significantly while maintaining half the parameter consumption of Transformer-based models like GLIP. Several test on different of backbones such ELAN, MSP, and CSP further enable efficient handling of multi-scale objects, ensuring scalability and robustness in resource-constrained environments. This study underscores the potential of integrating natural language understanding with advanced backbone architectures, setting new benchmarks in object detection accuracy, efficiency, and adaptability to real-world challenges.</li>
</ul>

<h3>Title: GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Guojie Li, Anwar P.P. Abdul Majeed, Muhammad Ateeq, Anh Nguyen, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05477">https://arxiv.org/abs/2511.05477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05477">https://arxiv.org/pdf/2511.05477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05477]] GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation(https://arxiv.org/abs/2511.05477)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation requires models that are accurate, lightweight, and interpretable. Convolutional architectures lack adaptive nonlinearity and transparent decision-making, whereas Transformer architectures are hindered by quadratic complexity and opaque attention mechanisms. U-KAN addresses these challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than both convolutional and attention-based methods, fewer parameters than Transformer variants, and improved interpretability compared to conventional approaches. However, its O(C^2) complexity due to full-channel transformations limits its scalability as the number of channels increases. To overcome this, we introduce GroupKAN, a lightweight segmentation network that incorporates two novel, structured functional modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC), GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11 percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M), and shows improved interpretability.</li>
</ul>

<h3>Title: On Flow Matching KL Divergence</h3>
<ul>
<li><strong>Authors: </strong>Maojiang Su, Jerry Yao-Chieh Hu, Sophia Pi, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05480">https://arxiv.org/abs/2511.05480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05480">https://arxiv.org/pdf/2511.05480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05480]] On Flow Matching KL Divergence(https://arxiv.org/abs/2511.05480)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.</li>
</ul>

<h3>Title: SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning</h3>
<ul>
<li><strong>Authors: </strong>Kang Yang, Yuanlin Yang, Yuning Chen, Sikai Yang, Xinyu Zhang, Wan Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05482">https://arxiv.org/abs/2511.05482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05482">https://arxiv.org/pdf/2511.05482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05482]] SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning(https://arxiv.org/abs/2511.05482)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Precision agriculture demands continuous and accurate monitoring of soil moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P), and potassium (K), to optimize yields and conserve resources. Wireless soil sensing has been explored to measure these four components; however, current solutions require recalibration (i.e., retraining the data processing model) to handle variations in soil texture, characterized by aluminosilicates (Al) and organic carbon (C), limiting their practicality. To address this, we introduce SoilX, a calibration-free soil sensing system that jointly measures six key components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX eliminates texture- and carbon-dependent recalibration. SoilX incorporates Contrastive Cross-Component Learning (3CL), with two customized terms: the Orthogonality Regularizer and the Separation Loss, to effectively disentangle cross-component interference. Additionally, we design a novel tetrahedral antenna array with an antenna-switching mechanism, which can robustly measure soil dielectric permittivity independent of device placement. Extensive experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5% over baselines and generalizes well to unseen fields.</li>
</ul>

<h3>Title: DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction</h3>
<ul>
<li><strong>Authors: </strong>Abigail Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.05483">https://arxiv.org/abs/2511.05483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.05483">https://arxiv.org/pdf/2511.05483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.05483]] DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction(https://arxiv.org/abs/2511.05483)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Predicting the effect of amino acid mutations on enzyme thermodynamic stability (DDG) is fundamental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the intricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key innovation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide transformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
