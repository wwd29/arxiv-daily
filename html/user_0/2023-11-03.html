<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: healthAIChain: Improving security and safety using Blockchain Technology applications in AI-based healthcare systems. (arXiv:2311.00842v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00842">http://arxiv.org/abs/2311.00842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00842]] healthAIChain: Improving security and safety using Blockchain Technology applications in AI-based healthcare systems(http://arxiv.org/abs/2311.00842)</code></li>
<li>Summary: <p>Blockchain as a digital ledger for keeping records of digital transactions
and other information, it is secure and decentralized technology. The globally
growing number of digital population every day possesses a significant threat
to online data including the medical and patients data. After bitcoin,
blockchain technology has emerged into a general-purpose technology with
applications in medical industries and healthcare. Blockchain can promote
highly configurable openness while retaining the highest security standards for
critical data of medical patients. Referred to as distributed record keeping
for healthcare systems which makes digital assets unalterable and transparent
via a cryptographic hash and decentralized network. The study delves into the
security and safety improvement associated with implementing blockchain in
AI-based healthcare systems. Blockchain-enabled AI tackles the existing issues
related to security, performance efficiencies, and safety in healthcare
systems. We have also examined the Artificial Intelligence in healthcare and
medical industry, potential areas, open questions concerning the blockchain in
healthcare systems. Finally, the article proposed an AI-based healthcare
blockchain model (healthAIChain) to improve patients data and security.
</p></li>
</ul>

<h3>Title: Artificial Intelligence Ethics Education in Cybersecurity: Challenges and Opportunities: a focus group report. (arXiv:2311.00903v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00903">http://arxiv.org/abs/2311.00903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00903]] Artificial Intelligence Ethics Education in Cybersecurity: Challenges and Opportunities: a focus group report(http://arxiv.org/abs/2311.00903)</code></li>
<li>Summary: <p>The emergence of AI tools in cybersecurity creates many opportunities and
uncertainties. A focus group with advanced graduate students in cybersecurity
revealed the potential depth and breadth of the challenges and opportunities.
The salient issues are access to open source or free tools, documentation,
curricular diversity, and clear articulation of ethical principles for AI
cybersecurity education. Confronting the "black box" mentality in AI
cybersecurity work is also of the greatest importance, doubled by deeper and
prior education in foundational AI work. Systems thinking and effective
communication were considered relevant areas of educational improvement. Future
AI educators and practitioners need to address these issues by implementing
rigorous technical training curricula, clear documentation, and frameworks for
ethically monitoring AI combined with critical and system's thinking and
communication skills.
</p></li>
</ul>

<h3>Title: A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence. (arXiv:2311.01154v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01154">http://arxiv.org/abs/2311.01154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01154]] A Review of Digital Twins and their Application in Cybersecurity based on Artificial Intelligence(http://arxiv.org/abs/2311.01154)</code></li>
<li>Summary: <p>The potential of digital twin technology is yet to be fully realized due to
its diversity and untapped potential. Digital twins enable systems' analysis,
design, optimization, and evolution to be performed digitally or in conjunction
with a cyber-physical approach to improve speed, accuracy, and efficiency over
traditional engineering methods. Industry 4.0, factories of the future, and
digital twins continue to benefit from the technology and provide enhanced
efficiency within existing systems. Due to the lack of information and security
standards associated with the transition to cyber digitization, cybercriminals
have been able to take advantage of the situation. Access to a digital twin of
a product or service is equivalent to threatening the entire collection. There
is a robust interaction between digital twins and artificial intelligence
tools, which leads to strong interaction between these technologies, so it can
be used to improve the cybersecurity of these digital platforms based on their
integration with these technologies. This study aims to investigate the role of
artificial intelligence in providing cybersecurity for digital twin versions of
various industries, as well as the risks associated with these versions. In
addition, this research serves as a road map for researchers and others
interested in cybersecurity and digital security.
</p></li>
</ul>

<h3>Title: Emergent (In)Security of Multi-Cloud Environments. (arXiv:2311.01247v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01247">http://arxiv.org/abs/2311.01247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01247]] Emergent (In)Security of Multi-Cloud Environments(http://arxiv.org/abs/2311.01247)</code></li>
<li>Summary: <p>As organizations increasingly use cloud services to host their IT
infrastructure, there is a need to share data among these cloud hosted services
and systems. A majority of IT organizations have workloads spread across
different cloud service providers, growing their multi-cloud environments. When
an organization grows their multi-cloud environment, the threat vectors and
vulnerabilities for their cloud systems and services grow as well. The increase
in the number of attack vectors creates a challenge of how to prioritize
mitigations and countermeasures to best defend a multi-cloud environment
against attacks. Utilizing multiple industry standard risk analysis tools, we
conducted an analysis of multi-cloud threat vectors enabling calculation and
prioritization for the identified mitigations and countermeasures. The
prioritizations from the analysis showed that authentication and architecture
are the highest risk areas of threat vectors. Armed with this data, IT managers
are able to more appropriately budget cybersecurity expenditure to implement
the most impactful mitigations and countermeasures.
</p></li>
</ul>

<h3>Title: Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01344">http://arxiv.org/abs/2311.01344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01344]] Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers(http://arxiv.org/abs/2311.01344)</code></li>
<li>Summary: <p>Model extraction is a growing concern for the security of AI systems. For
deep neural network models, the architecture is the most important information
an adversary aims to recover. Being a sequence of repeated computation blocks,
neural network models deployed on edge-devices will generate distinctive
side-channel leakages. The latter can be exploited to extract critical
information when targeted platforms are physically accessible. By combining
theoretical knowledge about deep learning practices and analysis of a
widespread implementation library (ARM CMSIS-NN), our purpose is to answer this
critical question: how far can we extract architecture information by simply
examining an EM side-channel trace? For the first time, we propose an
extraction methodology for traditional MLP and CNN models running on a high-end
32-bit microcontroller (Cortex-M7) that relies only on simple pattern
recognition analysis. Despite few challenging cases, we claim that, contrary to
parameters extraction, the complexity of the attack is relatively low and we
highlight the urgent need for practicable protections that could fit the strong
memory and latency requirements of such platforms.
</p></li>
</ul>

<h3>Title: Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability. (arXiv:2311.01406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01406">http://arxiv.org/abs/2311.01406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01406]] Analysis of Information Propagation in Ethereum Network Using Combined Graph Attention Network and Reinforcement Learning to Optimize Network Efficiency and Scalability(http://arxiv.org/abs/2311.01406)</code></li>
<li>Summary: <p>Blockchain technology has revolutionized the way information is propagated in
decentralized networks. Ethereum plays a pivotal role in facilitating smart
contracts and decentralized applications. Understanding information propagation
dynamics in Ethereum is crucial for ensuring network efficiency, security, and
scalability. In this study, we propose an innovative approach that utilizes
Graph Convolutional Networks (GCNs) to analyze the information propagation
patterns in the Ethereum network. The first phase of our research involves data
collection from the Ethereum blockchain, consisting of blocks, transactions,
and node degrees. We construct a transaction graph representation using
adjacency matrices to capture the node embeddings; while our major contribution
is to develop a combined Graph Attention Network (GAT) and Reinforcement
Learning (RL) model to optimize the network efficiency and scalability. It
learns the best actions to take in various network states, ultimately leading
to improved network efficiency, throughput, and optimize gas limits for block
processing. In the experimental evaluation, we analyze the performance of our
model on a large-scale Ethereum dataset. We investigate effectively aggregating
information from neighboring nodes capturing graph structure and updating node
embeddings using GCN with the objective of transaction pattern prediction,
accounting for varying network loads and number of blocks. Not only we design a
gas limit optimization model and provide the algorithm, but also to address
scalability, we demonstrate the use and implementation of sparse matrices in
GraphConv, GraphSAGE, and GAT. The results indicate that our designed GAT-RL
model achieves superior results compared to other GCN models in terms of
performance. It effectively propagates information across the network,
optimizing gas limits for block processing and improving network efficiency.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: InfoGuard: A Design and Usability Study of User-Controlled Application-Independent Encryption for Privacy-Conscious Users. (arXiv:2311.00812v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00812">http://arxiv.org/abs/2311.00812</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00812]] InfoGuard: A Design and Usability Study of User-Controlled Application-Independent Encryption for Privacy-Conscious Users(http://arxiv.org/abs/2311.00812)</code></li>
<li>Summary: <p>Billions of secure messaging users have adopted end-to-end encryption (E2EE).
Nevertheless, challenges remain. Most communication applications do not provide
E2EE, and application silos prevent interoperability. Our qualitative analysis
of privacy-conscious users' discussions of E2EE on Reddit reveals concerns
about trusting client applications with plaintext, lack of clear indicators
about how encryption works, high cost to switch apps, and concerns that most
apps are not open source. We propose InfoGuard, a system enabling E2EE for
user-to-user communication in any application. InfoGuard allows users to
trigger encryption on any textbox, even if the application does not support
E2EE. InfoGuard encrypts text before it reaches the application, eliminating
the client app's access to plaintext. InfoGuard also incorporates visible
encryption to make it easier for users to understand that their data is being
encrypted and give them greater confidence in the system's security. The design
enables fine-grained encryption, allowing specific sensitive data items to be
encrypted while the rest remains visible to the server. Participants in our
user study found InfoGuard usable and trustworthy, expressing a willingness to
adopt it.
</p></li>
</ul>

<h3>Title: Reputation Systems for Supply Chains: The Challenge of Achieving Privacy Preservation. (arXiv:2311.01060v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01060">http://arxiv.org/abs/2311.01060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01060]] Reputation Systems for Supply Chains: The Challenge of Achieving Privacy Preservation(http://arxiv.org/abs/2311.01060)</code></li>
<li>Summary: <p>Consumers frequently interact with reputation systems to rate products,
services, and deliveries. While past research extensively studied different
conceptual approaches to realize such systems securely and
privacy-preservingly, these concepts are not yet in use in business-to-business
environments. In this paper, (1) we thus outline which specific challenges
privacy-cautious stakeholders in volatile supply chain networks introduce, (2)
give an overview of the diverse landscape of privacy-preserving reputation
systems and their properties, and (3) based on well-established concepts from
supply chain information systems and cryptography, we further propose an
initial concept that accounts for the aforementioned challenges by utilizing
fully homomorphic encryption. For future work, we identify the need of
evaluating whether novel systems address the supply chain-specific privacy and
confidentiality needs.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01349">http://arxiv.org/abs/2311.01349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01349]] Unreading Race: Purging Protected Features from Chest X-ray Embeddings(http://arxiv.org/abs/2311.01349)</code></li>
<li>Summary: <p>Purpose: To analyze and remove protected feature effects in chest radiograph
embeddings of deep learning models.
</p>
<p>Materials and Methods: An orthogonalization is utilized to remove the
influence of protected features (e.g., age, sex, race) in chest radiograph
embeddings, ensuring feature-independent results. To validate the efficacy of
the approach, we retrospectively study the MIMIC and CheXpert datasets using
three pre-trained models, namely a supervised contrastive, a self-supervised
contrastive, and a baseline classifier model. Our statistical analysis involves
comparing the original versus the orthogonalized embeddings by estimating
protected feature influences and evaluating the ability to predict race, age,
or sex using the two types of embeddings.
</p>
<p>Results: Our experiments reveal a significant influence of protected features
on predictions of pathologies. Applying orthogonalization removes these feature
effects. Apart from removing any influence on pathology classification, while
maintaining competitive predictive performance, orthogonalized embeddings
further make it infeasible to directly predict protected attributes and
mitigate subgroup disparities.
</p>
<p>Conclusion: The presented work demonstrates the successful application and
evaluation of the orthogonalization technique in the domain of chest X-ray
classification.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer. (arXiv:2311.01106v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01106">http://arxiv.org/abs/2311.01106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01106]] In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer(http://arxiv.org/abs/2311.01106)</code></li>
<li>Summary: <p>Enabling machine learning classifiers to defer their decision to a downstream
expert when the expert is more accurate will ensure improved safety and
performance. This objective can be achieved with the learning-to-defer
framework which aims to jointly learn how to classify and how to defer to the
expert. In recent studies, it has been theoretically shown that popular
estimators for learning to defer parameterized with softmax provide unbounded
estimates for the likelihood of deferring which makes them uncalibrated.
However, it remains unknown whether this is due to the widely used softmax
parameterization and if we can find a softmax-based estimator that is both
statistically consistent and possesses a valid probability estimator. In this
work, we first show that the cause of the miscalibrated and unbounded estimator
in prior literature is due to the symmetric nature of the surrogate losses used
and not due to softmax. We then propose a novel statistically consistent
asymmetric softmax-based surrogate loss that can produce valid estimates
without the issue of unboundedness. We further analyze the non-asymptotic
properties of our method and empirically validate its performance and
calibration on benchmark datasets.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly. (arXiv:2311.01323v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01323">http://arxiv.org/abs/2311.01323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01323]] Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly(http://arxiv.org/abs/2311.01323)</code></li>
<li>Summary: <p>The adversarial vulnerability of deep neural networks (DNNs) has drawn great
attention due to the security risk of applying these models in real-world
applications. Based on transferability of adversarial examples, an increasing
number of transfer-based methods have been developed to fool black-box DNN
models whose architecture and parameters are inaccessible. Although tremendous
effort has been exerted, there still lacks a standardized benchmark that could
be taken advantage of to compare these methods systematically, fairly, and
practically. Our investigation shows that the evaluation of some methods needs
to be more reasonable and more thorough to verify their effectiveness, to
avoid, for example, unfair comparison and insufficient consideration of
possible substitute/victim models. Therefore, we establish a transfer-based
attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we
evaluate and compare them comprehensively on 25 popular substitute/victim
models on ImageNet. New insights about the effectiveness of these methods are
gained and guidelines for future evaluations are provided. Code at:
https://github.com/qizhangli/TA-Bench.
</p></li>
</ul>

<h3>Title: Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems. (arXiv:2311.00859v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00859">http://arxiv.org/abs/2311.00859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00859]] Optimal Cost Constrained Adversarial Attacks For Multiple Agent Systems(http://arxiv.org/abs/2311.00859)</code></li>
<li>Summary: <p>Finding optimal adversarial attack strategies is an important topic in
reinforcement learning and the Markov decision process. Previous studies
usually assume one all-knowing coordinator (attacker) for whom attacking
different recipient (victim) agents incurs uniform costs. However, in reality,
instead of using one limitless central attacker, the attacks often need to be
performed by distributed attack agents. We formulate the problem of performing
optimal adversarial agent-to-agent attacks using distributed attack agents, in
which we impose distinct cost constraints on each different attacker-victim
pair. We propose an optimal method integrating within-step static constrained
attack-resource allocation optimization and between-step dynamic programming to
achieve the optimal adversarial attack in a multi-agent system. Our numerical
results show that the proposed attacks can significantly reduce the rewards
received by the attacked agents.
</p></li>
</ul>

<h3>Title: MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training. (arXiv:2311.00919v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00919">http://arxiv.org/abs/2311.00919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00919]] MIST: Defending Against Membership Inference Attacks Through Membership-Invariant Subspace Training(http://arxiv.org/abs/2311.00919)</code></li>
<li>Summary: <p>In Member Inference (MI) attacks, the adversary try to determine whether an
instance is used to train a machine learning (ML) model. MI attacks are a major
privacy concern when using private data to train ML models. Most MI attacks in
the literature take advantage of the fact that ML models are trained to fit the
training data well, and thus have very low loss on training instances. Most
defenses against MI attacks therefore try to make the model fit the training
data less well. Doing so, however, generally results in lower accuracy. We
observe that training instances have different degrees of vulnerability to MI
attacks. Most instances will have low loss even when not included in training.
For these instances, the model can fit them well without concerns of MI
attacks. An effective defense only needs to (possibly implicitly) identify
instances that are vulnerable to MI attacks and avoids overfitting them. A
major challenge is how to achieve such an effect in an efficient training
process. Leveraging two distinct recent advancements in representation
learning: counterfactually-invariant representations and subspace learning
methods, we introduce a novel Membership-Invariant Subspace Training (MIST)
method to defend against MI attacks. MIST avoids overfitting the vulnerable
instances without significant impact on other instances. We have conducted
extensive experimental studies, comparing MIST with various other
state-of-the-art (SOTA) MI defenses against several SOTA MI attacks. We find
that MIST outperforms other defenses while resulting in minimal reduction in
testing accuracy.
</p></li>
</ul>

<h3>Title: Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. (arXiv:2311.01011v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01011">http://arxiv.org/abs/2311.01011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01011]] Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game(http://arxiv.org/abs/2311.01011)</code></li>
<li>Summary: <p>While Large Language Models (LLMs) are increasingly being used in real-world
applications, they remain vulnerable to prompt injection attacks: malicious
third party prompts that subvert the intent of the system designer. To help
researchers study this problem, we present a dataset of over 126,000 prompt
injection attacks and 46,000 prompt-based "defenses" against prompt injection,
all created by players of an online game called Tensor Trust. To the best of
our knowledge, this is currently the largest dataset of human-generated
adversarial examples for instruction-following LLMs. The attacks in our dataset
have a lot of easily interpretable stucture, and shed light on the weaknesses
of LLMs. We also use the dataset to create a benchmark for resistance to two
types of prompt injection, which we refer to as prompt extraction and prompt
hijacking. Our benchmark results show that many models are vulnerable to the
attack strategies in the Tensor Trust dataset. Furthermore, we show that some
attack strategies from the dataset generalize to deployed LLM-based
applications, even though they have a very different set of constraints to the
game. We release all data and source code at https://tensortrust.ai/paper
</p></li>
</ul>

<h3>Title: Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent. (arXiv:2311.01205v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01205">http://arxiv.org/abs/2311.01205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01205]] Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go Indifferent(http://arxiv.org/abs/2311.01205)</code></li>
<li>Summary: <p>Prior attacks on graph neural networks have mostly focused on graph poisoning
and evasion, neglecting the network's weights and biases. Traditional
weight-based fault injection attacks, such as bit flip attacks used for
convolutional neural networks, do not consider the unique properties of graph
neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip
attack designed specifically for graph neural networks. Our attack targets the
learnable neighborhood aggregation functions in quantized message passing
neural networks, degrading their ability to distinguish graph structures and
losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest
that exploiting mathematical properties specific to certain graph neural
network architectures can significantly increase their vulnerability to bit
flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive
Graph Isomorphism Networks trained on various graph property prediction
datasets to random output by flipping only a small fraction of the network's
bits, demonstrating its higher destructive power compared to a bit flip attack
transferred from convolutional neural networks. Our attack is transparent and
motivated by theoretical insights which are confirmed by extensive empirical
results.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Automatic counting of planting microsites via local visual detection and global count estimation. (arXiv:2311.00796v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00796">http://arxiv.org/abs/2311.00796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00796]] Automatic counting of planting microsites via local visual detection and global count estimation(http://arxiv.org/abs/2311.00796)</code></li>
<li>Summary: <p>In forest industry, mechanical site preparation by mounding is widely used
prior to planting operations. One of the main problems when planning planting
operations is the difficulty in estimating the number of mounds present on a
planting block, as their number may greatly vary depending on site
characteristics. This estimation is often carried out through field surveys by
several forestry workers. However, this procedure is prone to error and
slowness. Motivated by recent advances in UAV imagery and artificial
intelligence, we propose a fully automated framework to estimate the number of
mounds on a planting block. Using computer vision and machine learning, we
formulate the counting task as a supervised learning problem using two
prediction models. A local detection model is firstly used to detect visible
mounds based on deep features, while a global prediction function is
subsequently applied to provide a final estimation based on block-level
features. To evaluate the proposed method, we constructed a challenging UAV
dataset representing several plantation blocks with different characteristics.
The performed experiments demonstrated the robustness of the proposed method,
which outperforms manual methods in precision, while significantly reducing
time and cost.
</p></li>
</ul>

<h3>Title: Beyond Still Images: Robust Multi-Stream Spatiotemporal Networks. (arXiv:2311.00800v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00800">http://arxiv.org/abs/2311.00800</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00800]] Beyond Still Images: Robust Multi-Stream Spatiotemporal Networks(http://arxiv.org/abs/2311.00800)</code></li>
<li>Summary: <p>A defining characteristic of natural vision is its ability to withstand a
variety of input alterations, resulting in the creation of an invariant
representation of the surroundings. While convolutional neural networks exhibit
resilience to certain forms of spatial input variation, modifications in the
spatial and temporal aspects can significantly affect the representations of
video content in deep neural networks. Inspired by the resilience of natural
vision to input variations, we employ a simple multi-stream model to explore
its potential to address spatiotemporal changes by including temporal features.
Our primary goal is to introduce a video-trained model and evaluate its
robustness to diverse image and video inputs, with a particular focus on
exploring the role of temporal features in invariant recognition. Results show
that including videos and the temporal stream during training mitigates the
decline in accuracy and mAP in image and video understanding tasks by 1.36% and
3.14%, respectively.
</p></li>
</ul>

<h3>Title: VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization. (arXiv:2311.00807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00807">http://arxiv.org/abs/2311.00807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00807]] VQA-GEN: A Visual Question Answering Benchmark for Domain Generalization(http://arxiv.org/abs/2311.00807)</code></li>
<li>Summary: <p>Visual question answering (VQA) models are designed to demonstrate
visual-textual reasoning capabilities. However, their real-world applicability
is hindered by a lack of comprehensive benchmark datasets. Existing domain
generalization datasets for VQA exhibit a unilateral focus on textual shifts
while VQA being a multi-modal task contains shifts across both visual and
textual domains. We propose VQA-GEN, the first ever multi-modal benchmark
dataset for distribution shift generated through a shift induced pipeline.
Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing
methods to joint multi-modal distribution shifts. validating that comprehensive
multi-modal shifts are critical for robust VQA generalization. Models trained
on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming
the value of VQA-GEN. Further, we analyze the importance of each shift
technique of our pipeline contributing to the generalization of the model.
</p></li>
</ul>

<h3>Title: RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection. (arXiv:2311.00917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00917">http://arxiv.org/abs/2311.00917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00917]] RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection(http://arxiv.org/abs/2311.00917)</code></li>
<li>Summary: <p>Deep learning (DL) networks have achieved remarkable performance in infrared
small target detection (ISTD). However, these structures exhibit a deficiency
in interpretability and are widely regarded as black boxes, as they disregard
domain knowledge in ISTD. To alleviate this issue, this work proposes an
interpretable deep network for detecting infrared dim targets, dubbed RPCANet.
Specifically, our approach formulates the ISTD task as sparse target
extraction, low-rank background estimation, and image reconstruction in a
relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the
iterative optimization updating steps into a deep-learning framework,
time-consuming and complex matrix calculations are replaced by theory-guided
neural networks. RPCANet detects targets with clear interpretability and
preserves the intrinsic image feature, instead of directly transforming the
detection task into a matrix decomposition problem. Extensive experiments
substantiate the effectiveness of our deep unfolding framework and demonstrate
its trustworthy results, surpassing baseline methods in both qualitative and
quantitative evaluations.
</p></li>
</ul>

<h3>Title: Detecting Generated Images by Real Images Only. (arXiv:2311.00962v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00962">http://arxiv.org/abs/2311.00962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00962]] Detecting Generated Images by Real Images Only(http://arxiv.org/abs/2311.00962)</code></li>
<li>Summary: <p>As deep learning technology continues to evolve, the images yielded by
generative models are becoming more and more realistic, triggering people to
question the authenticity of images. Existing generated image detection methods
detect visual artifacts in generated images or learn discriminative features
from both real and generated images by massive training. This learning paradigm
will result in efficiency and generalization issues, making detection methods
always lag behind generation methods. This paper approaches the generated image
detection problem from a new perspective: Start from real images. By finding
the commonality of real images and mapping them to a dense subspace in feature
space, the goal is that generated images, regardless of their generative model,
are then projected outside the subspace. As a result, images from different
generative models can be detected, solving some long-existing problems in the
field. Experimental results show that although our method was trained only by
real images and uses 99.9\% less training data than other deep learning-based
methods, it can compete with state-of-the-art methods and shows excellent
performance in detecting emerging generative models with high inference
efficiency. Moreover, the proposed method shows robustness against various
post-processing. These advantages allow the method to be used in real-world
scenarios.
</p></li>
</ul>

<h3>Title: NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks. (arXiv:2311.01022v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01022">http://arxiv.org/abs/2311.01022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01022]] NeuroWrite: Predictive Handwritten Digit Classification using Deep Neural Networks(http://arxiv.org/abs/2311.01022)</code></li>
<li>Summary: <p>The rapid evolution of deep neural networks has revolutionized the field of
machine learning, enabling remarkable advancements in various domains. In this
article, we introduce NeuroWrite, a unique method for predicting the
categorization of handwritten digits using deep neural networks. Our model
exhibits outstanding accuracy in identifying and categorising handwritten
digits by utilising the strength of convolutional neural networks (CNNs) and
recurrent neural networks (RNNs).In this article, we give a thorough
examination of the data preparation methods, network design, and training
methods used in NeuroWrite. By implementing state-of-the-art techniques, we
showcase how NeuroWrite can achieve high classification accuracy and robust
generalization on handwritten digit datasets, such as MNIST. Furthermore, we
explore the model's potential for real-world applications, including digit
recognition in digitized documents, signature verification, and automated
postal code recognition. NeuroWrite is a useful tool for computer vision and
pattern recognition because of its performance and adaptability.The
architecture, training procedure, and evaluation metrics of NeuroWrite are
covered in detail in this study, illustrating how it can improve a number of
applications that call for handwritten digit classification. The outcomes show
that NeuroWrite is a promising method for raising the bar for deep neural
network-based handwritten digit recognition.
</p></li>
</ul>

<h3>Title: H-NeXt: The next step towards roto-translation invariant networks. (arXiv:2311.01111v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01111">http://arxiv.org/abs/2311.01111</a></li>
<li>Code URL: https://github.com/karellat/h-next</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01111]] H-NeXt: The next step towards roto-translation invariant networks(http://arxiv.org/abs/2311.01111)</code></li>
<li>Summary: <p>The widespread popularity of equivariant networks underscores the
significance of parameter efficient models and effective use of training data.
At a time when robustness to unseen deformations is becoming increasingly
important, we present H-NeXt, which bridges the gap between equivariance and
invariance. H-NeXt is a parameter-efficient roto-translation invariant network
that is trained without a single augmented image in the training set. Our
network comprises three components: an equivariant backbone for learning
roto-translation independent features, an invariant pooling layer for
discarding roto-translation information, and a classification layer. H-NeXt
outperforms the state of the art in classification on unaugmented training sets
and augmented test sets of MNIST and CIFAR-10.
</p></li>
</ul>

<h3>Title: Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration. (arXiv:2311.01202v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01202">http://arxiv.org/abs/2311.01202</a></li>
<li>Code URL: https://github.com/ivanxie416/cmignet</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01202]] Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration(http://arxiv.org/abs/2311.01202)</code></li>
<li>Summary: <p>The majority of point cloud registration methods currently rely on extracting
features from points. However, these methods are limited by their dependence on
information obtained from a single modality of points, which can result in
deficiencies such as inadequate perception of global features and a lack of
texture information. Actually, humans can employ visual information learned
from 2D images to comprehend the 3D world. Based on this fact, we present a
novel Cross-Modal Information-Guided Network (CMIGNet), which obtains global
shape perception through cross-modal information to achieve precise and robust
point cloud registration. Specifically, we first incorporate the projected
images from the point clouds and fuse the cross-modal features using the
attention mechanism. Furthermore, we employ two contrastive learning
strategies, namely overlapping contrastive learning and cross-modal contrastive
learning. The former focuses on features in overlapping regions, while the
latter emphasizes the correspondences between 2D and 3D features. Finally, we
propose a mask prediction module to identify keypoints in the point clouds.
Extensive experiments on several benchmark datasets demonstrate that our
network achieves superior registration performance.
</p></li>
</ul>

<h3>Title: Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning. (arXiv:2311.01227v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01227">http://arxiv.org/abs/2311.01227</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01227]] Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning(http://arxiv.org/abs/2311.01227)</code></li>
<li>Summary: <p>This paper introduces a two-stage framework designed to enhance long-tail
class incremental learning, enabling the model to progressively learn new
classes, while mitigating catastrophic forgetting in the context of long-tailed
data distributions. Addressing the challenge posed by the under-representation
of tail classes in long-tail class incremental learning, our approach achieves
classifier alignment by leveraging global variance as an informative measure
and class prototypes in the second stage. This process effectively captures
class properties and eliminates the need for data balancing or additional layer
tuning. Alongside traditional class incremental learning losses in the first
stage, the proposed approach incorporates mixup classes to learn robust feature
representations, ensuring smoother boundaries. The proposed framework can
seamlessly integrate as a module with any class incremental learning method to
effectively handle long-tail class incremental learning scenarios. Extensive
experimentation on the CIFAR-100 and ImageNet-Subset datasets validates the
approach's efficacy, showcasing its superiority over state-of-the-art
techniques across various long-tail CIL settings.
</p></li>
</ul>

<h3>Title: Robust Identity Perceptual Watermark Against Deepfake Face Swapping. (arXiv:2311.01357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01357">http://arxiv.org/abs/2311.01357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01357]] Robust Identity Perceptual Watermark Against Deepfake Face Swapping(http://arxiv.org/abs/2311.01357)</code></li>
<li>Summary: <p>Notwithstanding offering convenience and entertainment to society, Deepfake
face swapping has caused critical privacy issues with the rapid development of
deep generative models. Due to imperceptible artifacts in high-quality
synthetic images, passive detection models against face swapping in recent
years usually suffer performance damping regarding the generalizability issue.
Therefore, several studies have been attempted to proactively protect the
original images against malicious manipulations by inserting invisible signals
in advance. However, the existing proactive defense approaches demonstrate
unsatisfactory results with respect to visual quality, detection accuracy, and
source tracing ability. In this study, we propose the first robust identity
perceptual watermarking framework that concurrently performs detection and
source tracing against Deepfake face swapping proactively. We assign identity
semantics regarding the image contents to the watermarks and devise an
unpredictable and unreversible chaotic encryption system to ensure watermark
confidentiality. The watermarks are encoded and recovered by jointly training
an encoder-decoder framework along with adversarial image manipulations.
Extensive experiments demonstrate state-of-the-art performance against Deepfake
face swapping under both cross-dataset and cross-manipulation settings.
</p></li>
</ul>

<h3>Title: CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar. (arXiv:2311.01423v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01423">http://arxiv.org/abs/2311.01423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01423]] CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar(http://arxiv.org/abs/2311.01423)</code></li>
<li>Summary: <p>Robust perception is a vital component for ensuring safe autonomous and
assisted driving. Automotive radar (77 to 81 GHz), which offers
weather-resilient sensing, provides a complementary capability to the vision-
or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radar
tensors contain rich spatiotemporal semantics besides 3D location information.
The majority of previous methods take in 3D (Doppler-range-azimuth) RF radar
tensors, allowing prediction of an object's location, heading angle, and size
in bird's-eye-view (BEV). However, they lack the ability to at the same time
infer objects' size, orientation, and identity in the 3D space. To overcome
this limitation, we propose an efficient joint architecture called
CenterRadarNet, designed to facilitate high-resolution representation learning
from 4D (Doppler-range-azimuth-elevation) radar data for 3D object detection
and re-identification (re-ID) tasks. As a single-stage 3D object detector,
CenterRadarNet directly infers the BEV object distribution confidence maps,
corresponding 3D bounding box attributes, and appearance embedding for each
pixel. Moreover, we build an online tracker utilizing the learned appearance
embedding for re-ID. CenterRadarNet achieves the state-of-the-art result on the
K-Radar 3D object detection benchmark. In addition, we present the first 3D
object-tracking result using radar on the K-Radar dataset V2. In diverse
driving scenarios, CenterRadarNet shows consistent, robust performance,
emphasizing its wide applicability.
</p></li>
</ul>

<h3>Title: Transformation Decoupling Strategy based on Screw Theory for Deterministic Point Cloud Registration with Gravity Prior. (arXiv:2311.01432v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01432">http://arxiv.org/abs/2311.01432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01432]] Transformation Decoupling Strategy based on Screw Theory for Deterministic Point Cloud Registration with Gravity Prior(http://arxiv.org/abs/2311.01432)</code></li>
<li>Summary: <p>Point cloud registration is challenging in the presence of heavy outlier
correspondences. This paper focuses on addressing the robust
correspondence-based registration problem with gravity prior that often arises
in practice. The gravity directions are typically obtained by inertial
measurement units (IMUs) and can reduce the degree of freedom (DOF) of rotation
from 3 to 1. We propose a novel transformation decoupling strategy by
leveraging screw theory. This strategy decomposes the original 4-DOF problem
into three sub-problems with 1-DOF, 2-DOF, and 1-DOF, respectively, thereby
enhancing the computation efficiency. Specifically, the first 1-DOF represents
the translation along the rotation axis and we propose an interval
stabbing-based method to solve it. The second 2-DOF represents the pole which
is an auxiliary variable in screw theory and we utilize a branch-and-bound
method to solve it. The last 1-DOF represents the rotation angle and we propose
a global voting method for its estimation. The proposed method sequentially
solves three consensus maximization sub-problems, leading to efficient and
deterministic registration. In particular, it can even handle the
correspondence-free registration problem due to its significant robustness.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method is more efficient and robust than state-of-the-art methods,
even when dealing with outlier rates exceeding 99%.
</p></li>
</ul>

<h3>Title: Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01441">http://arxiv.org/abs/2311.01441</a></li>
<li>Code URL: https://github.com/andyz245/discreteadversarialdistillation</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01441]] Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models(http://arxiv.org/abs/2311.01441)</code></li>
<li>Summary: <p>We propose a conceptually simple and lightweight framework for improving the
robustness of vision models through the combination of knowledge distillation
and data augmentation. We address the conjecture that larger models do not make
for better teachers by showing strong gains in out-of-distribution robustness
when distilling from pretrained foundation models. Following this finding, we
propose Discrete Adversarial Distillation (DAD), which leverages a robust
teacher to generate adversarial examples and a VQGAN to discretize them,
creating more informative samples than standard data augmentation techniques.
We provide a theoretical framework for the use of a robust teacher in the
knowledge distillation with data augmentation setting and demonstrate strong
gains in out-of-distribution robustness and clean accuracy across different
student architectures. Notably, our method adds minor computational overhead
compared to similar techniques and can be easily combined with other data
augmentations for further improvements.
</p></li>
</ul>

<h3>Title: CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation. (arXiv:2311.01447v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01447">http://arxiv.org/abs/2311.01447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01447]] CADSim: Robust and Scalable in-the-wild 3D Reconstruction for Controllable Sensor Simulation(http://arxiv.org/abs/2311.01447)</code></li>
<li>Summary: <p>Realistic simulation is key to enabling safe and scalable development of %
self-driving vehicles. A core component is simulating the sensors so that the
entire autonomy system can be tested in simulation. Sensor simulation involves
modeling traffic participants, such as vehicles, with high quality appearance
and articulated geometry, and rendering them in real time. The self-driving
industry has typically employed artists to build these assets. However, this is
expensive, slow, and may not reflect reality. Instead, reconstructing assets
automatically from sensor data collected in the wild would provide a better
path to generating a diverse and large set with good real-world coverage.
Nevertheless, current reconstruction approaches struggle on in-the-wild sensor
data, due to its sparsity and noise. To tackle these issues, we present CADSim,
which combines part-aware object-class priors via a small set of CAD models
with differentiable rendering to automatically reconstruct vehicle geometry,
including articulated wheels, with high-quality appearance. Our experiments
show our method recovers more accurate shapes from sparse data compared to
existing approaches. Importantly, it also trains and renders efficiently. We
demonstrate our reconstructed vehicles in several applications, including
accurate testing of autonomy perception systems.
</p></li>
</ul>

<h3>Title: Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition. (arXiv:2311.00906v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00906">http://arxiv.org/abs/2311.00906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00906]] Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition(http://arxiv.org/abs/2311.00906)</code></li>
<li>Summary: <p>Active learning, a widely adopted technique for enhancing machine learning
models in text and image classification tasks with limited annotation
resources, has received relatively little attention in the domain of Named
Entity Recognition (NER). The challenge of data imbalance in NER has hindered
the effectiveness of active learning, as sequence labellers lack sufficient
learning signals. To address these challenges, this paper presents a novel
reweighting-based active learning strategy that assigns dynamic smoothed
weights to individual tokens. This adaptable strategy is compatible with
various token-level acquisition functions and contributes to the development of
robust active learners. Experimental results on multiple corpora demonstrate
the substantial performance improvement achieved by incorporating our
re-weighting strategy into existing acquisition functions, validating its
practical efficacy.
</p></li>
</ul>

<h3>Title: DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01070">http://arxiv.org/abs/2311.01070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01070]] DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts(http://arxiv.org/abs/2311.01070)</code></li>
<li>Summary: <p>Whisper is a multitask and multilingual speech model covering 99 languages.
It yields commendable automatic speech recognition (ASR) results in a subset of
its covered languages, but the model still under-performs on a non-negligible
number of under-represented languages, a problem exacerbated in smaller model
versions. In this work, we propose DistilWhisper, an approach able to bridge
the performance gap in ASR for these languages while retaining the advantages
of multitask and multilingual capabilities. Our approach involves two key
strategies: lightweight modular ASR fine-tuning of whisper-small using
language-specific experts, and knowledge distillation from whisper-large-v2.
This dual approach allows us to effectively boost ASR performance while keeping
the robustness inherited from the multitask and multilingual pre-training.
Results demonstrate that our approach is more effective than standard
fine-tuning or LoRA adapters, boosting performance in the targeted languages
for both in- and out-of-domain test sets, while introducing only a negligible
parameter overhead at inference.
</p></li>
</ul>

<h3>Title: Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance. (arXiv:2311.01108v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01108">http://arxiv.org/abs/2311.01108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01108]] Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance(http://arxiv.org/abs/2311.01108)</code></li>
<li>Summary: <p>Adopting a two-stage paradigm of pretraining followed by fine-tuning,
Pretrained Language Models (PLMs) have achieved substantial advancements in the
field of natural language processing. However, in real-world scenarios, data
labels are often noisy due to the complex annotation process, making it
essential to develop strategies for fine-tuning PLMs with such noisy labels. To
this end, we introduce an innovative approach for fine-tuning PLMs using noisy
labels, which incorporates the guidance of Large Language Models (LLMs) like
ChatGPT. This guidance assists in accurately distinguishing between clean and
noisy samples and provides supplementary information beyond the noisy labels,
thereby boosting the learning process during fine-tuning PLMs. Extensive
experiments on synthetic and real-world noisy datasets further demonstrate the
superior advantages of our framework over the state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01270">http://arxiv.org/abs/2311.01270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01270]] People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection(http://arxiv.org/abs/2311.01270)</code></li>
<li>Summary: <p>NLP models are used in a variety of critical social computing tasks, such as
detecting sexist, racist, or otherwise hateful content. Therefore, it is
imperative that these models are robust to spurious features. Past work has
attempted to tackle such spurious features using training data augmentation,
including Counterfactually Augmented Data (CADs). CADs introduce minimal
changes to existing training data points and flip their labels; training on
them may reduce model dependency on spurious features. However, manually
generating CADs can be time-consuming and expensive. Hence in this work, we
assess if this task can be automated using generative NLP models. We
automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate
their usefulness in improving model robustness compared to manually-generated
CADs. By testing both model performance on multiple out-of-domain test sets and
individual data point efficacy, our results show that while manual CADs are
still the most effective, CADs generated by ChatGPT come a close second. One
key reason for the lower performance of automated methods is that the changes
they introduce are often insufficient to flip the original label.
</p></li>
</ul>

<h3>Title: Investigating Relative Performance of Transfer and Meta Learning. (arXiv:2311.00727v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00727">http://arxiv.org/abs/2311.00727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00727]] Investigating Relative Performance of Transfer and Meta Learning(http://arxiv.org/abs/2311.00727)</code></li>
<li>Summary: <p>Over the past decade, the field of machine learning has experienced
remarkable advancements. While image recognition systems have achieved
impressive levels of accuracy, they continue to rely on extensive training
datasets. Additionally, a significant challenge has emerged in the form of poor
out-of-distribution performance, which necessitates retraining neural networks
when they encounter conditions that deviate from their training data. This
limitation has notably contributed to the slow progress in self-driving car
technology. These pressing issues have sparked considerable interest in methods
that enable neural networks to learn effectively from limited data. This paper
presents the outcomes of an extensive investigation designed to compare two
distinct approaches, transfer learning and meta learning, as potential
solutions to this problem. The overarching objective was to establish a robust
criterion for selecting the most suitable method in diverse machine learning
scenarios. Building upon prior research, I expanded the comparative analysis by
introducing a new meta learning method into the investigation. Subsequently, I
assessed whether the findings remained consistent under varying conditions.
Finally, I delved into the impact of altering the size of the training dataset
on the relative performance of these methods. This comprehensive exploration
has yielded insights into the conditions favoring each approach, thereby
facilitating the development of a criterion for selecting the most appropriate
method in any given situation
</p></li>
</ul>

<h3>Title: Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning. (arXiv:2311.00737v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00737">http://arxiv.org/abs/2311.00737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00737]] Real-Time Magnetic Tracking and Diagnosis of COVID-19 via Machine Learning(http://arxiv.org/abs/2311.00737)</code></li>
<li>Summary: <p>The COVID-19 pandemic underscored the importance of reliable, noninvasive
diagnostic tools for robust public health interventions. In this work, we fused
magnetic respiratory sensing technology (MRST) with machine learning (ML) to
create a diagnostic platform for real-time tracking and diagnosis of COVID-19
and other respiratory diseases. The MRST precisely captures breathing patterns
through three specific breath testing protocols: normal breath, holding breath,
and deep breath. We collected breath data from both COVID-19 patients and
healthy subjects in Vietnam using this platform, which then served to train and
validate ML models. Our evaluation encompassed multiple ML algorithms,
including support vector machines and deep learning models, assessing their
ability to diagnose COVID-19. Our multi-model validation methodology ensures a
thorough comparison and grants the adaptability to select the most optimal
model, striking a balance between diagnostic precision with model
interpretability. The findings highlight the exceptional potential of our
diagnostic tool in pinpointing respiratory anomalies, achieving over 90%
accuracy. This innovative sensor technology can be seamlessly integrated into
healthcare settings for patient monitoring, marking a significant enhancement
for the healthcare infrastructure.
</p></li>
</ul>

<h3>Title: Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00865">http://arxiv.org/abs/2311.00865</a></li>
<li>Code URL: https://github.com/mgerstgrasser/super</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00865]] Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning(http://arxiv.org/abs/2311.00865)</code></li>
<li>Summary: <p>We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized
Experience Relay, in which agents share with other agents a limited number of
transitions they observe during training. The intuition behind this is that
even a small number of relevant experiences from other agents could help each
agent learn. Unlike many other multi-agent RL algorithms, this approach allows
for largely decentralized training, requiring only a limited communication
channel between agents. We show that our approach outperforms baseline
no-sharing decentralized training and state-of-the art multi-agent RL
algorithms. Further, sharing only a small number of highly relevant experiences
outperforms sharing all experiences between agents, and the performance uplift
from selective experience sharing is robust across a range of hyperparameters
and DQN variants. A reference implementation of our algorithm is available at
https://github.com/mgerstgrasser/super.
</p></li>
</ul>

<h3>Title: Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00983">http://arxiv.org/abs/2311.00983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00983]] Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks(http://arxiv.org/abs/2311.00983)</code></li>
<li>Summary: <p>Inventory Routing Problem (IRP) is a crucial challenge in supply chain
management as it involves optimizing efficient route selection while
considering the uncertainty of inventory demand planning. To solve IRPs,
usually a two-stage approach is employed, where demand is predicted using
machine learning techniques first, and then an optimization algorithm is used
to minimize routing costs. Our experiment shows machine learning models fall
short of achieving perfect accuracy because inventory levels are influenced by
the dynamic business environment, which, in turn, affects the optimization
problem in the next stage, resulting in sub-optimal decisions. In this paper,
we formulate and propose a decision-focused learning-based approach to solving
real-world IRPs. This approach directly integrates inventory prediction and
routing optimization within an end-to-end system potentially ensuring a robust
supply chain strategy.
</p></li>
</ul>

<h3>Title: Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy. (arXiv:2311.01002v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01002">http://arxiv.org/abs/2311.01002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01002]] Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy(http://arxiv.org/abs/2311.01002)</code></li>
<li>Summary: <p>Data pruning, which aims to downsize a large training set into a small
informative subset, is crucial for reducing the enormous computational costs of
modern deep learning. Though large-scale data collections invariably contain
annotation noise and numerous robust learning methods have been developed, data
pruning for the noise-robust learning scenario has received little attention.
With state-of-the-art Re-labeling methods that self-correct erroneous labels
while training, it is challenging to identify which subset induces the most
accurate re-labeling of erroneous labels in the entire training set. In this
paper, we formalize the problem of data pruning with re-labeling. We first show
that the likelihood of a training example being correctly re-labeled is
proportional to the prediction confidence of its neighborhood in the subset.
Therefore, we propose a novel data pruning algorithm, Prune4Rel, that finds a
subset maximizing the total neighborhood confidence of all training examples,
thereby maximizing the re-labeling accuracy and generalization performance.
Extensive experiments on four real and one synthetic noisy datasets show that
\algname{} outperforms the baselines with Re-labeling models by up to 9.1% as
well as those with a standard model by up to 21.6%.
</p></li>
</ul>

<h3>Title: Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01047">http://arxiv.org/abs/2311.01047</a></li>
<li>Code URL: https://github.com/bhagyapuranik/texp_for_robustness</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01047]] Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective(http://arxiv.org/abs/2311.01047)</code></li>
<li>Summary: <p>State-of-the-art techniques for enhancing robustness of deep networks mostly
rely on empirical risk minimization with suitable data augmentation. In this
paper, we propose a complementary approach motivated by communication theory,
aimed at enhancing the signal-to-noise ratio at the output of a neural network
layer via neural competition during learning and inference. In addition to
minimization of a standard end-to-end cost, neurons compete to sparsely
represent layer inputs by maximization of a tilted exponential (TEXP) objective
function for the layer. TEXP learning can be interpreted as maximum likelihood
estimation of matched filters under a Gaussian model for data noise. Inference
in a TEXP layer is accomplished by replacing batch norm by a tilted softmax,
which can be interpreted as computation of posterior probabilities for the
competing signaling hypotheses represented by each neuron. After providing
insights via simplified models, we show, by experimentation on standard image
datasets, that TEXP learning and inference enhances robustness against noise
and other common corruptions, without requiring data augmentation. Further
cumulative gains in robustness against this array of distortions can be
obtained by appropriately combining TEXP with data augmentation techniques.
</p></li>
</ul>

<h3>Title: Combating Bilateral Edge Noise for Robust Link Prediction. (arXiv:2311.01196v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01196">http://arxiv.org/abs/2311.01196</a></li>
<li>Code URL: https://github.com/tmlr-group/rgib</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01196]] Combating Bilateral Edge Noise for Robust Link Prediction(http://arxiv.org/abs/2311.01196)</code></li>
<li>Summary: <p>Although link prediction on graphs has achieved great success with the
development of graph neural networks (GNNs), the potential robustness under the
edge noise is still less investigated. To close this gap, we first conduct an
empirical study to disclose that the edge noise bilaterally perturbs both input
topology and target label, yielding severe performance degradation and
representation collapse. To address this dilemma, we propose an
information-theory-guided principle, Robust Graph Information Bottleneck
(RGIB), to extract reliable supervision signals and avoid representation
collapse. Different from the basic information bottleneck, RGIB further
decouples and balances the mutual dependence among graph topology, target
labels, and representation, building new learning objectives for robust
representation against the bilateral noise. Two instantiations, RGIB-SSL and
RGIB-REP, are explored to leverage the merits of different methodologies, i.e.,
self-supervised learning and data reparameterization, for implicit and explicit
data denoising, respectively. Extensive experiments on six datasets and three
GNNs with diverse noisy scenarios verify the effectiveness of our RGIB
instantiations. The code is publicly available at:
https://github.com/tmlr-group/RGIB.
</p></li>
</ul>

<h3>Title: A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. (arXiv:2311.01329v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01329">http://arxiv.org/abs/2311.01329</a></li>
<li>Code URL: https://github.com/kaiyan289/tailo</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01329]] A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories(http://arxiv.org/abs/2311.01329)</code></li>
<li>Summary: <p>Offline imitation from observations aims to solve MDPs where only
task-specific expert states and task-agnostic non-expert state-action pairs are
available. Offline imitation is useful in real-world scenarios where arbitrary
interactions are costly and expert actions are unavailable. The
state-of-the-art "DIstribution Correction Estimation" (DICE) methods minimize
divergence of state occupancy between expert and learner policies and retrieve
a policy with weighted behavior cloning; however, their results are unstable
when learning from incomplete trajectories, due to a non-robust optimization in
the dual domain. To address the issue, in this paper, we propose
Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a
discounted sum along the future trajectory as the weight for weighted behavior
cloning. The terms for the sum are scaled by the output of a discriminator,
which aims to identify expert states. Despite simplicity, TAILO works well if
there exist trajectories or segments of expert behavior in the task-agnostic
data, a common assumption in prior work. In experiments across multiple
testbeds, we find TAILO to be more robust and effective, particularly with
incomplete trajectories.
</p></li>
</ul>

<h3>Title: Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01412">http://arxiv.org/abs/2311.01412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01412]] Castor: Causal Temporal Regime Structure Learning(http://arxiv.org/abs/2311.01412)</code></li>
<li>Summary: <p>The task of uncovering causal relationships among multivariate time series
data stands as an essential and challenging objective that cuts across a broad
array of disciplines ranging from climate science to healthcare. Such data
entails linear or non-linear relationships, and usually follow multiple a
priori unknown regimes. Existing causal discovery methods can infer summary
causal graphs from heterogeneous data with known regimes, but they fall short
in comprehensively learning both regimes and the corresponding causal graph. In
this paper, we introduce CASTOR, a novel framework designed to learn causal
relationships in heterogeneous time series data composed of various regimes,
each governed by a distinct causal graph. Through the maximization of a score
function via the EM algorithm, CASTOR infers the number of regimes and learns
linear or non-linear causal relationships in each regime. We demonstrate the
robust convergence properties of CASTOR, specifically highlighting its
proficiency in accurately identifying unique regimes. Empirical evidence,
garnered from exhaustive synthetic experiments and two real-world benchmarks,
confirm CASTOR's superior performance in causal discovery compared to baseline
methods. By learning a full temporal causal graph for each regime, CASTOR
establishes itself as a distinctly interpretable method for causal discovery in
heterogeneous time series.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Are These the Same Apple? Comparing Images Based on Object Intrinsics. (arXiv:2311.00750v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00750">http://arxiv.org/abs/2311.00750</a></li>
<li>Code URL: https://github.com/s-tian/cute</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00750]] Are These the Same Apple? Comparing Images Based on Object Intrinsics(http://arxiv.org/abs/2311.00750)</code></li>
<li>Summary: <p>The human visual system can effortlessly recognize an object under different
extrinsic factors such as lighting, object poses, and background, yet current
computer vision systems often struggle with these variations. An important step
to understanding and improving artificial vision systems is to measure image
similarity purely based on intrinsic object properties that define object
identity. This problem has been studied in the computer vision literature as
re-identification, though mostly restricted to specific object categories such
as people and cars. We propose to extend it to general object categories,
exploring an image similarity metric based on object intrinsics. To benchmark
such measurements, we collect the Common paired objects Under differenT
Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different
extrinsic factors such as lighting, poses, and imaging conditions. While
existing methods such as LPIPS and CLIP scores do not measure object intrinsics
well, we find that combining deep features learned from contrastive
self-supervised learning with foreground filtering is a simple yet effective
approach to approximating the similarity. We conduct an extensive survey of
pre-trained features and foreground extraction methods to arrive at a strong
baseline that best measures intrinsic object-centric image similarity among
current methods. Finally, we demonstrate that our approach can aid in
downstream applications such as acting as an analog for human subjects and
improving generalizable re-identification. Please see our project website at
https://s-tian.github.io/projects/cute/ for visualizations of the data and
demos of our metric.
</p></li>
</ul>

<h3>Title: M&M3D: Multi-Dataset Training and Efficient Network for Multi-view 3D Object Detection. (arXiv:2311.00986v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00986">http://arxiv.org/abs/2311.00986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00986]] M&M3D: Multi-Dataset Training and Efficient Network for Multi-view 3D Object Detection(http://arxiv.org/abs/2311.00986)</code></li>
<li>Summary: <p>In this research, I proposed a network structure for multi-view 3D object
detection using camera-only data and a Bird's-Eye-View map. My work is based on
a current key challenge domain adaptation and visual data transfer. Although
many excellent camera-only 3D object detection has been continuously proposed,
many research work risk dramatic performance drop when the networks are trained
on the source domain but tested on a different target domain. Then I found it
is very surprising that predictions on bounding boxes and classes are still
replied to on 2D networks. Based on the domain gap assumption on various 3D
datasets, I found they still shared a similar data extraction on the same BEV
map size and camera data transfer. Therefore, to analyze the domain gap
influence on the current method and to make good use of 3D space information
among the dataset and the real world, I proposed a transfer learning method and
Transformer construction to study the 3D object detection on NuScenes-mini and
Lyft. Through multi-dataset training and a detection head from the Transformer,
the network demonstrated good data migration performance and efficient
detection performance by using 3D anchor query and 3D positional information.
Relying on only a small amount of source data and the existing large model
pre-training weights, the efficient network manages to achieve competitive
results on the new target domain. Moreover, my study utilizes 3D information as
available semantic information and 2D multi-view image features blending into
the visual-language transfer design. In the final 3D anchor box prediction and
object classification, my network achieved good results on standard metrics of
3D object detection, which differs from dataset-specific models on each
training domain without any fine-tuning.
</p></li>
</ul>

<h3>Title: Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01004">http://arxiv.org/abs/2311.01004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01004]] Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning(http://arxiv.org/abs/2311.01004)</code></li>
<li>Summary: <p>With the development of multimodality and large language models, the deep
learning-based technique for medical image captioning holds the potential to
offer valuable diagnostic recommendations. However, current generic text and
image pre-trained models do not yield satisfactory results when it comes to
describing intricate details within medical images. In this paper, we present a
novel medical image captioning method guided by the segment anything model
(SAM) to enable enhanced encoding with both general and detailed feature
extraction. In addition, our approach employs a distinctive pre-training
strategy with mixed semantic learning to simultaneously capture both the
overall information and finer details within medical images. We demonstrate the
effectiveness of this approach, as it outperforms the pre-trained BLIP2 model
on various evaluation metrics for generating descriptions of medical images.
</p></li>
</ul>

<h3>Title: Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations. (arXiv:2311.01188v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01188">http://arxiv.org/abs/2311.01188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01188]] Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations(http://arxiv.org/abs/2311.01188)</code></li>
<li>Summary: <p>Estimating building footprint maps from geospatial data is of paramount
importance in urban planning, development, disaster management, and various
other applications. Deep learning methodologies have gained prominence in
building segmentation maps, offering the promise of precise footprint
extraction without extensive post-processing. However, these methods face
challenges in generalization and label efficiency, particularly in remote
sensing, where obtaining accurate labels can be both expensive and
time-consuming. To address these challenges, we propose terrain-aware
self-supervised learning, tailored to remote sensing, using digital elevation
models from LiDAR data. We propose to learn a model to differentiate between
bare Earth and superimposed structures enabling the network to implicitly learn
domain-relevant features without the need for extensive pixel-level
annotations. We test the effectiveness of our approach by evaluating building
segmentation performance on test datasets with varying label fractions.
Remarkably, with only 1% of the labels (equivalent to 25 labeled examples), our
method improves over ImageNet pre-training, showing the advantage of leveraging
unlabeled data for feature extraction in the domain of remote sensing. The
performance improvement is more pronounced in few-shot scenarios and gradually
closes the gap with ImageNet pre-training as the label fraction increases. We
test on a dataset characterized by substantial distribution shifts and labeling
errors to demonstrate the generalizability of our approach. When compared to
other baselines, including ImageNet pretraining and more complex architectures,
our approach consistently performs better, demonstrating the efficiency and
effectiveness of self-supervised terrain-aware feature learning.
</p></li>
</ul>

<h3>Title: Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing. (arXiv:2311.00835v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00835">http://arxiv.org/abs/2311.00835</a></li>
<li>Code URL: https://github.com/yanlinf/casent</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00835]] Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing(http://arxiv.org/abs/2311.00835)</code></li>
<li>Summary: <p>Ultra-fine entity typing plays a crucial role in information extraction by
predicting fine-grained semantic types for entity mentions in text. However,
this task poses significant challenges due to the massive number of entity
types in the output space. The current state-of-the-art approaches, based on
standard multi-label classifiers or cross-encoder models, suffer from poor
generalization performance or inefficient inference. In this paper, we present
CASENT, a seq2seq model designed for ultra-fine entity typing that predicts
ultra-fine types with calibrated confidence scores. Our model takes an entity
mention as input and employs constrained beam search to generate multiple types
autoregressively. The raw sequence probabilities associated with the predicted
types are then transformed into confidence scores using a novel calibration
method. We conduct extensive experiments on the UFET dataset which contains
over 10k types. Our method outperforms the previous state-of-the-art in terms
of F1 score and calibration error, while achieving an inference speedup of over
50 times. Additionally, we demonstrate the generalization capabilities of our
model by evaluating it in zero-shot and few-shot settings on five specialized
domain entity typing datasets that are unseen during training. Remarkably, our
model outperforms large language models with 10 times more parameters in the
zero-shot setting, and when fine-tuned on 50 examples, it significantly
outperforms ChatGPT on all datasets. Our code, models and demo are available at
https://github.com/yanlinf/CASENT.
</p></li>
</ul>

<h3>Title: Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods. (arXiv:2311.01428v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01428">http://arxiv.org/abs/2311.01428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01428]] Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods(http://arxiv.org/abs/2311.01428)</code></li>
<li>Summary: <p>Dementia, a prevalent neurodegenerative condition, is a major manifestation
of Alzheimer's disease (AD). As the condition progresses from mild to severe,
it significantly impairs the individual's ability to perform daily tasks
independently, necessitating the need for timely and accurate AD
classification. Machine learning or deep learning models have emerged as
effective tools for this purpose. In this study, we suggested an approach for
classifying the four stages of dementia using RF, SVM, and CNN algorithms,
augmented with watershed segmentation for feature extraction from MRI images.
Our results reveal that SVM with watershed features achieves an impressive
accuracy of 96.25%, surpassing other classification methods. The ADNI dataset
is utilized to evaluate the effectiveness of our method, and we observed that
the inclusion of watershed segmentation contributes to the enhanced performance
of the models.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Dynamic Fair Federated Learning Based on Reinforcement Learning. (arXiv:2311.00959v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00959">http://arxiv.org/abs/2311.00959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00959]] Dynamic Fair Federated Learning Based on Reinforcement Learning(http://arxiv.org/abs/2311.00959)</code></li>
<li>Summary: <p>Federated learning enables a collaborative training and optimization of
global models among a group of devices without sharing local data samples.
However, the heterogeneity of data in federated learning can lead to unfair
representation of the global model across different devices. To address the
fairness issue in federated learning, we propose a dynamic q fairness federated
learning algorithm with reinforcement learning, called DQFFL. DQFFL aims to
mitigate the discrepancies in device aggregation and enhance the fairness of
treatment for all groups involved in federated learning. To quantify fairness,
DQFFL leverages the performance of the global federated model on each device
and incorporates {\alpha}-fairness to transform the preservation of fairness
during federated aggregation into the distribution of client weights in the
aggregation process. Considering the sensitivity of parameters in measuring
fairness, we propose to utilize reinforcement learning for dynamic parameters
during aggregation. Experimental results demonstrate that our DQFFL outperforms
the state-of-the-art methods in terms of overall performance, fairness and
convergence speed.
</p></li>
</ul>

<h3>Title: Federated Linear Bandits with Finite Adversarial Actions. (arXiv:2311.00973v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00973">http://arxiv.org/abs/2311.00973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00973]] Federated Linear Bandits with Finite Adversarial Actions(http://arxiv.org/abs/2311.00973)</code></li>
<li>Summary: <p>We study a federated linear bandits model, where $M$ clients communicate with
a central server to solve a linear contextual bandits problem with finite
adversarial action sets that may be different across clients. To address the
unique challenges of adversarial finite action sets, we propose the
FedSupLinUCB algorithm, which extends the principles of SupLinUCB and OFUL
algorithms in linear contextual bandits. We prove that FedSupLinUCB achieves a
total regret of $\tilde{O}(\sqrt{d T})$, where $T$ is the total number of arm
pulls from all clients, and $d$ is the ambient dimension of the linear model.
This matches the minimax lower bound and thus is order-optimal (up to polylog
terms). We study both asynchronous and synchronous cases and show that the
communication cost can be controlled as $O(d M^2 \log(d)\log(T))$ and
$O(\sqrt{d^3 M^3} \log(d))$, respectively. The FedSupLinUCB design is further
extended to two scenarios: (1) variance-adaptive, where a total regret of
$\tilde{O} (\sqrt{d \sum \nolimits_{t=1}^{T} \sigma_t^2})$ can be achieved with
$\sigma_t^2$ being the noise variance of round $t$; and (2) adversarial
corruption, where a total regret of $\tilde{O}(\sqrt{dT} + d C_p)$ can be
achieved with $C_p$ being the total corruption budget. Experiment results
corroborate the theoretical analysis and demonstrate the effectiveness of
FedSupLinUCB on both synthetic and real-world datasets.
</p></li>
</ul>

<h3>Title: Federated Learning on Edge Sensing Devices: A Review. (arXiv:2311.01201v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01201">http://arxiv.org/abs/2311.01201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01201]] Federated Learning on Edge Sensing Devices: A Review(http://arxiv.org/abs/2311.01201)</code></li>
<li>Summary: <p>The ability to monitor ambient characteristics, interact with them, and
derive information about the surroundings has been made possible by the rapid
proliferation of edge sensing devices like IoT, mobile, and wearable devices
and their measuring capabilities with integrated sensors. Even though these
devices are small and have less capacity for data storage and processing, they
produce vast amounts of data. Some example application areas where sensor data
is collected and processed include healthcare, environmental (including air
quality and pollution levels), automotive, industrial, aerospace, and
agricultural applications. These enormous volumes of sensing data collected
from the edge devices are analyzed using a variety of Machine Learning (ML) and
Deep Learning (DL) approaches. However, analyzing them on the cloud or a server
presents challenges related to privacy, hardware, and connectivity limitations.
Federated Learning (FL) is emerging as a solution to these problems while
preserving privacy by jointly training a model without sharing raw data. In
this paper, we review the FL strategies from the perspective of edge sensing
devices to get over the limitations of conventional machine learning
techniques. We focus on the key FL principles, software frameworks, and
testbeds. We also explore the current sensor technologies, properties of the
sensing devices and sensing applications where FL is utilized. We conclude with
a discussion on open issues and future research directions on FL for further
studies
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: SmoothHess: ReLU Network Feature Interactions via Stein's Lemma. (arXiv:2311.00858v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00858">http://arxiv.org/abs/2311.00858</a></li>
<li>Code URL: https://github.com/maxtorop/smoothhess</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00858]] SmoothHess: ReLU Network Feature Interactions via Stein's Lemma(http://arxiv.org/abs/2311.00858)</code></li>
<li>Summary: <p>Several recent methods for interpretability model feature interactions by
looking at the Hessian of a neural network. This poses a challenge for ReLU
networks, which are piecewise-linear and thus have a zero Hessian almost
everywhere. We propose SmoothHess, a method of estimating second-order
interactions through Stein's Lemma. In particular, we estimate the Hessian of
the network convolved with a Gaussian through an efficient sampling algorithm,
requiring only network gradient calls. SmoothHess is applied post-hoc, requires
no modifications to the ReLU network architecture, and the extent of smoothing
can be controlled explicitly. We provide a non-asymptotic bound on the sample
complexity of our estimation procedure. We validate the superior ability of
SmoothHess to capture interactions on benchmark datasets and a real-world
medical spirometry dataset.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: On Manipulating Scene Text in the Wild with Diffusion Models. (arXiv:2311.00734v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00734">http://arxiv.org/abs/2311.00734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00734]] On Manipulating Scene Text in the Wild with Diffusion Models(http://arxiv.org/abs/2311.00734)</code></li>
<li>Summary: <p>Diffusion models have gained attention for image editing yielding impressive
results in text-to-image tasks. On the downside, one might notice that
generated images of stable diffusion models suffer from deteriorated details.
This pitfall impacts image editing tasks that require information preservation
e.g., scene text editing. As a desired result, the model must show the
capability to replace the text on the source image to the target text while
preserving the details e.g., color, font size, and background. To leverage the
potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene
Text manipulation Network so-called DBEST. Specifically, we design two
adaptation strategies, namely one-shot style adaptation and text-recognition
guidance. In experiments, we thoroughly assess and compare our proposed method
against state-of-the-arts on various scene text datasets, then provide
extensive ablation studies for each granularity to analyze our performance
gain. Also, we demonstrate the effectiveness of our proposed method to
synthesize scene text indicated by competitive Optical Character Recognition
(OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and
ICDAR2013 datasets for character-level evaluation.
</p></li>
</ul>

<h3>Title: Towards High-quality HDR Deghosting with Conditional Diffusion Models. (arXiv:2311.00932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00932">http://arxiv.org/abs/2311.00932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00932]] Towards High-quality HDR Deghosting with Conditional Diffusion Models(http://arxiv.org/abs/2311.00932)</code></li>
<li>Summary: <p>High Dynamic Range (HDR) images can be recovered from several Low Dynamic
Range (LDR) images by existing Deep Neural Networks (DNNs) techniques. Despite
the remarkable progress, DNN-based methods still generate ghosting artifacts
when LDR images have saturation and large motion, which hinders potential
applications in real-world scenarios. To address this challenge, we formulate
the HDR deghosting problem as an image generation that leverages LDR features
as the diffusion model's condition, consisting of the feature condition
generator and the noise predictor. Feature condition generator employs
attention and Domain Feature Alignment (DFA) layer to transform the
intermediate features to avoid ghosting artifacts. With the learned features as
conditions, the noise predictor leverages a stochastic iterative denoising
process for diffusion models to generate an HDR image by steering the sampling
process. Furthermore, to mitigate semantic confusion caused by the saturation
problem of LDR images, we design a sliding window noise estimator to sample
smooth noise in a patch-based manner. In addition, an image space loss is
proposed to avoid the color distortion of the estimated HDR results. We
empirically evaluate our model on benchmark datasets for HDR imaging. The
results demonstrate that our approach achieves state-of-the-art performances
and well generalization to real-world images.
</p></li>
</ul>

<h3>Title: Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00938">http://arxiv.org/abs/2311.00938</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00938]] Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance(http://arxiv.org/abs/2311.00938)</code></li>
<li>Summary: <p>Diffusion models have emerged as a pivotal advancement in generative models,
setting new standards to the quality of the generated instances. In the current
paper we aim to underscore a discrepancy between conventional training methods
and the desired conditional sampling behavior of these models. While the
prevalent classifier-free guidance technique works well, it's not without
flaws. At higher values for the guidance scale parameter $w$, we often get out
of distribution samples and mode collapse, whereas at lower values for $w$ we
may not get the desired specificity. To address these challenges, we introduce
an updated loss function that better aligns training objectives with sampling
behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our
method's ability to produce higher quality samples with fewer sampling
timesteps, and be more robust to the choice of guidance scale $w$. We also
experiment with fine-tuning Stable Diffusion on the proposed loss, to provide
early evidence that large diffusion models may also benefit from this refined
loss function.
</p></li>
</ul>

<h3>Title: Gaussian Mixture Solvers for Diffusion Models. (arXiv:2311.00941v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00941">http://arxiv.org/abs/2311.00941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00941]] Gaussian Mixture Solvers for Diffusion Models(http://arxiv.org/abs/2311.00941)</code></li>
<li>Summary: <p>Recently, diffusion models have achieved great success in generative tasks.
Sampling from diffusion models is equivalent to solving the reverse diffusion
stochastic differential equations (SDEs) or the corresponding probability flow
ordinary differential equations (ODEs). In comparison, SDE-based solvers can
generate samples of higher quality and are suited for image translation tasks
like stroke-based synthesis. During inference, however, existing SDE-based
solvers are severely constrained by the efficiency-effectiveness dilemma. Our
investigation suggests that this is because the Gaussian assumption in the
reverse transition kernel is frequently violated (even in the case of simple
mixture data) given a limited number of discretization steps. To overcome this
limitation, we introduce a novel class of SDE-based solvers called
\emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver
estimates the first three-order moments and optimizes the parameters of a
Gaussian mixture transition kernel using generalized methods of moments in each
step during sampling. Empirically, our solver outperforms numerous SDE-based
solvers in terms of sample quality in image generation and stroke-based
synthesis in various diffusion models, which validates the motivation and
effectiveness of GMS. Our code is available at
https://github.com/Guohanzhong/GMS.
</p></li>
</ul>

<h3>Title: Optimal Noise pursuit for Augmenting Text-to-Video Generation. (arXiv:2311.00949v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00949">http://arxiv.org/abs/2311.00949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00949]] Optimal Noise pursuit for Augmenting Text-to-Video Generation(http://arxiv.org/abs/2311.00949)</code></li>
<li>Summary: <p>Despite the remarkable progress in text-to-video generation, existing
diffusion-based models often exhibit instability in terms of noise during
inference. Specifically, when different noises are fed for the given text,
these models produce videos that differ significantly in terms of both frame
quality and temporal consistency. With this observation, we posit that there
exists an optimal noise matched to each textual input; however, the widely
adopted strategies of random noise sampling often fail to capture it. In this
paper, we argue that the optimal noise can be approached through inverting the
groundtruth video using the established noise-video mapping derived from the
diffusion model. Nevertheless, the groundtruth video for the text prompt is not
available during inference. To address this challenge, we propose to
approximate the optimal noise via a search and inversion pipeline. Given a text
prompt, we initially search for a video from a predefined candidate pool that
closely relates to the text prompt. Subsequently, we invert the searched video
into the noise space, which serves as an improved noise prompt for the textual
input. In addition to addressing noise, we also observe that the text prompt
with richer details often leads to higher-quality videos. Motivated by this, we
further design a semantic-preserving rewriter to enrich the text prompt, where
a reference-guided rewriting is devised for reasonable details compensation,
and a denoising with a hybrid semantics strategy is proposed to preserve the
semantic consistency. Extensive experiments on the WebVid-10M benchmark show
that our proposed method can improve the text-to-video models with a clear
margin, while introducing no optimization burden.
</p></li>
</ul>

<h3>Title: VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning. (arXiv:2311.00990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00990">http://arxiv.org/abs/2311.00990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00990]] VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning(http://arxiv.org/abs/2311.00990)</code></li>
<li>Summary: <p>Customized text-to-video generation aims to generate text-guided videos with
customized user-given subjects, which has gained increasing attention recently.
However, existing works are primarily limited to generating videos for a single
subject, leaving the more challenging problem of customized multi-subject
text-to-video generation largely unexplored. In this paper, we fill this gap
and propose a novel VideoDreamer framework. VideoDreamer can generate
temporally consistent text-guided videos that faithfully preserve the visual
features of the given multiple subjects. Specifically, VideoDreamer leverages
the pretrained Stable Diffusion with latent-code motion dynamics and temporal
cross-frame attention as the base video generator. The video generator is
further customized for the given multiple subjects by the proposed Disen-Mix
Finetuning and Human-in-the-Loop Re-finetuning strategy, which can tackle the
attribute binding problem of multi-subject generation. We also introduce
MultiStudioBench, a benchmark for evaluating customized multi-subject
text-to-video generation models. Extensive experiments demonstrate the
remarkable ability of VideoDreamer to generate videos with new content such as
new events and backgrounds, tailored to the customized multiple subjects. Our
project page is available at https://videodreamer23.github.io/.
</p></li>
</ul>

<h3>Title: Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs. (arXiv:2311.01015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01015">http://arxiv.org/abs/2311.01015</a></li>
<li>Code URL: https://github.com/jpthu17/graphmotion</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01015]] Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs(http://arxiv.org/abs/2311.01015)</code></li>
<li>Summary: <p>Most text-driven human motion generation methods employ sequential modeling
approaches, e.g., transformer, to extract sentence-level text representations
automatically and implicitly for human motion synthesis. However, these compact
text representations may overemphasize the action names at the expense of other
important properties and lack fine-grained details to guide the synthesis of
subtly distinct motion. In this paper, we propose hierarchical semantic graphs
for fine-grained control over motion generation. Specifically, we disentangle
motion descriptions into hierarchical semantic graphs including three levels of
motions, actions, and specifics. Such global-to-local structures facilitate a
comprehensive understanding of motion description and fine-grained control of
motion generation. Correspondingly, to leverage the coarse-to-fine topology of
hierarchical semantic graphs, we decompose the text-to-motion diffusion process
into three semantic levels, which correspond to capturing the overall motion,
local actions, and action specifics. Extensive experiments on two benchmark
human motion datasets, including HumanML3D and KIT, with superior performances,
justify the efficacy of our method. More encouragingly, by modifying the edge
weights of hierarchical semantic graphs, our method can continuously refine the
generated motion, which may have a far-reaching impact on the community. Code
and pre-training weights are available at
https://github.com/jpthu17/GraphMotion.
</p></li>
</ul>

<h3>Title: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01017">http://arxiv.org/abs/2311.01017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01017]] Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion(http://arxiv.org/abs/2311.01017)</code></li>
<li>Summary: <p>Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
</p></li>
</ul>

<h3>Title: Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning. (arXiv:2311.01018v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01018">http://arxiv.org/abs/2311.01018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01018]] Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning(http://arxiv.org/abs/2311.01018)</code></li>
<li>Summary: <p>Training diffusion models on limited datasets poses challenges in terms of
limited generation capacity and expressiveness, leading to unsatisfactory
results in various downstream tasks utilizing pretrained diffusion models, such
as domain translation and text-guided image manipulation. In this paper, we
propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a
methodology to address these challenges by leveraging diverse features from
diffusion models pretrained on large source datasets. SDFT distills more
general features (shape, colors, etc.) and less domain-specific features
(texture, fine details, etc) from the source model, allowing successful
knowledge transfer without disturbing the training process on target datasets.
The proposed method is not constrained by the specific architecture of the
model and thus can be generally adopted to existing frameworks. Experimental
results demonstrate that SDFT enhances the expressiveness of the diffusion
model with limited datasets, resulting in improved generation capabilities
across various downstream tasks.
</p></li>
</ul>

<h3>Title: Infusion: Internal Diffusion for Video Inpainting. (arXiv:2311.01090v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01090">http://arxiv.org/abs/2311.01090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01090]] Infusion: Internal Diffusion for Video Inpainting(http://arxiv.org/abs/2311.01090)</code></li>
<li>Summary: <p>Video inpainting is the task of filling a desired region in a video in a
visually convincing manner. It is a very challenging task due to the high
dimensionality of the signal and the temporal consistency required for
obtaining convincing results. Recently, diffusion models have shown impressive
results in modeling complex data distributions, including images and videos.
Diffusion models remain nonetheless very expensive to train and perform
inference with, which strongly restrict their application to video. We show
that in the case of video inpainting, thanks to the highly auto-similar nature
of videos, the training of a diffusion model can be restricted to the video to
inpaint and still produce very satisfying results. This leads us to adopt an
internal learning approch, which also allows for a greatly reduced network
size. We call our approach "Infusion": an internal learning algorithm for video
inpainting through diffusion. Due to our frugal network, we are able to propose
the first video inpainting approach based purely on diffusion. Other methods
require supporting elements such as optical flow estimation, which limits their
performance in the case of dynamic textures for example. We introduce a new
method for efficient training and inference of diffusion models in the context
of internal learning. We split the diffusion process into different learning
intervals which greatly simplifies the learning steps. We show qualititative
and quantitative results, demonstrating that our method reaches
state-of-the-art performance, in particular in the case of dynamic backgrounds
and textures.
</p></li>
</ul>

<h3>Title: Optimal Transport-Guided Conditional Score-Based Diffusion Models. (arXiv:2311.01226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01226">http://arxiv.org/abs/2311.01226</a></li>
<li>Code URL: https://github.com/xjtu-xgu/otcs</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01226]] Optimal Transport-Guided Conditional Score-Based Diffusion Models(http://arxiv.org/abs/2311.01226)</code></li>
<li>Summary: <p>Conditional score-based diffusion model (SBDM) is for conditional generation
of target data with paired data as condition, and has achieved great success in
image translation. However, it requires the paired data as condition, and there
would be insufficient paired data provided in real-world applications. To
tackle the applications with partially paired or even unpaired dataset, we
propose a novel Optimal Transport-guided Conditional Score-based diffusion
model (OTCS) in this paper. We build the coupling relationship for the unpaired
or partially paired dataset based on $L_2$-regularized unsupervised or
semi-supervised optimal transport, respectively. Based on the coupling
relationship, we develop the objective for training the conditional score-based
model for unpaired or partially paired settings, which is based on a
reformulation and generalization of the conditional SBDM for paired setting.
With the estimated coupling relationship, we effectively train the conditional
score-based model by designing a ``resampling-by-compatibility'' strategy to
choose the sampled data with high compatibility as guidance. Extensive
experiments on unpaired super-resolution and semi-paired image-to-image
translation demonstrated the effectiveness of the proposed OTCS model. From the
viewpoint of optimal transport, OTCS provides an approach to transport data
across distributions, which is a challenge for OT on large-scale datasets. We
theoretically prove that OTCS realizes the data transport in OT with a
theoretical bound. Code is available at \url{https://github.com/XJTU-XGU/OTCS}.
</p></li>
</ul>

<h3>Title: DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01295">http://arxiv.org/abs/2311.01295</a></li>
<li>Code URL: https://github.com/wenxuan-bao/dp-mix</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01295]] DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning(http://arxiv.org/abs/2311.01295)</code></li>
<li>Summary: <p>Data augmentation techniques, such as simple image transformations and
combinations, are highly effective at improving the generalization of computer
vision models, especially when training data is limited. However, such
techniques are fundamentally incompatible with differentially private learning
approaches, due to the latter's built-in assumption that each training image's
contribution to the learned model is bounded. In this paper, we investigate why
naive applications of multi-sample data augmentation techniques, such as mixup,
fail to achieve good performance and propose two novel data augmentation
techniques specifically designed for the constraints of differentially private
learning. Our first technique, DP-Mix_Self, achieves SoTA classification
performance across a range of datasets and settings by performing mixup on
self-augmented data. Our second technique, DP-Mix_Diff, further improves
performance by incorporating synthetic data from a pre-trained diffusion model
into the mixup process. We open-source the code at
https://github.com/wenxuan-Bao/DP-Mix.
</p></li>
</ul>

<h3>Title: The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing. (arXiv:2311.01410v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01410">http://arxiv.org/abs/2311.01410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01410]] The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing(http://arxiv.org/abs/2311.01410)</code></li>
<li>Summary: <p>We present a unified probabilistic formulation for diffusion-based image
editing, where a latent variable is edited in a task-specific manner and
generally deviates from the corresponding marginal distribution induced by the
original stochastic or ordinary differential equation (SDE or ODE). Instead, it
defines a corresponding SDE or ODE for editing. In the formulation, we prove
that the Kullback-Leibler divergence between the marginal distributions of the
two SDEs gradually decreases while that for the ODEs remains as the time
approaches zero, which shows the promise of SDE in image editing. Inspired by
it, we provide the SDE counterparts for widely used ODE baselines in various
tasks including inpainting and image-to-image translation, where SDE shows a
consistent and substantial improvement. Moreover, we propose SDE-Drag -- a
simple yet effective method built upon the SDE formulation for point-based
content dragging. We build a challenging benchmark (termed DragBench) with
open-set natural, art, and AI-generated images for evaluation. A user study on
DragBench indicates that SDE-Drag significantly outperforms our ODE baseline,
existing diffusion-based methods, and the renowned DragGAN. Our results
demonstrate the superiority and versatility of SDE in image editing and push
the boundary of diffusion-based editing methods.
</p></li>
</ul>

<h3>Title: Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00797">http://arxiv.org/abs/2311.00797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00797]] Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling(http://arxiv.org/abs/2311.00797)</code></li>
<li>Summary: <p>We study the tipping point collective dynamics of an adaptive
susceptible-infected-susceptible (SIS) epidemiological network in a
data-driven, machine learning-assisted manner. We identify a
parameter-dependent effective stochastic differential equation (eSDE) in terms
of physically meaningful coarse mean-field variables through a deep-learning
ResNet architecture inspired by numerical stochastic integrators. We construct
an approximate effective bifurcation diagram based on the identified drift term
of the eSDE and contrast it with the mean-field SIS model bifurcation diagram.
We observe a subcritical Hopf bifurcation in the evolving network's effective
SIS dynamics, that causes the tipping point behavior; this takes the form of
large amplitude collective oscillations that spontaneously -- yet rarely --
arise from the neighborhood of a (noisy) stationary state. We study the
statistics of these rare events both through repeated brute force simulations
and by using established mathematical/computational tools exploiting the
right-hand-side of the identified SDE. We demonstrate that such a collective
SDE can also be identified (and the rare events computations also performed) in
terms of data-driven coarse observables, obtained here via manifold learning
techniques, in particular Diffusion Maps. The workflow of our study is
straightforwardly applicable to other complex dynamics problems exhibiting
tipping point dynamics.
</p></li>
</ul>

<h3>Title: Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction. (arXiv:2311.01033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01033">http://arxiv.org/abs/2311.01033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01033]] Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction(http://arxiv.org/abs/2311.01033)</code></li>
<li>Summary: <p>Continuous-time long-term event prediction plays an important role in many
application scenarios. Most existing works rely on autoregressive frameworks to
predict event sequences, which suffer from error accumulation, thus
compromising prediction quality. Inspired by the success of denoising diffusion
probabilistic models, we propose a diffusion-based non-autoregressive temporal
point process model for long-term event prediction in continuous time. Instead
of generating events one at a time in an autoregressive way, our model predicts
the future event sequence entirely as a whole. In order to perform diffusion
processes on event sequences, we develop a bidirectional map between target
event sequences and the Euclidean vector space. Furthermore, we design a novel
denoising network to capture both sequential and contextual features for better
sample quality. Extensive experiments are conducted to prove the superiority of
our proposed model over state-of-the-art methods on long-term event prediction
in continuous time. To the best of our knowledge, this is the first work to
apply diffusion methods to long-term event prediction problems.
</p></li>
</ul>

<h3>Title: Add and Thin: Diffusion for Temporal Point Processes. (arXiv:2311.01139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01139">http://arxiv.org/abs/2311.01139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01139]] Add and Thin: Diffusion for Temporal Point Processes(http://arxiv.org/abs/2311.01139)</code></li>
<li>Summary: <p>Autoregressive neural networks within the temporal point process (TPP)
framework have become the standard for modeling continuous-time event data.
Even though these models can expressively capture event sequences in a
one-step-ahead fashion, they are inherently limited for long-term forecasting
applications due to the accumulation of errors caused by their sequential
nature. To overcome these limitations, we derive ADD-THIN, a principled
probabilistic denoising diffusion model for TPPs that operates on entire event
sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles
data with discrete and continuous components. In experiments on synthetic and
real-world datasets, our model matches the state-of-the-art TPP models in
density estimation and strongly outperforms them in forecasting.
</p></li>
</ul>

<h3>Title: Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01223">http://arxiv.org/abs/2311.01223</a></li>
<li>Code URL: https://github.com/apexrl/diff4rlsurvey</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01223]] Diffusion Models for Reinforcement Learning: A Survey(http://arxiv.org/abs/2311.01223)</code></li>
<li>Summary: <p>Diffusion models have emerged as a prominent class of generative models,
surpassing previous methods regarding sample quality and training stability.
Recent works have shown the advantages of diffusion models in improving
reinforcement learning (RL) solutions, including as trajectory planners,
expressive policy classes, data synthesizers, etc. This survey aims to provide
an overview of the advancements in this emerging field and hopes to inspire new
avenues of research. First, we examine several challenges encountered by
current RL algorithms. Then, we present a taxonomy of existing methods based on
the roles played by diffusion models in RL and explore how the existing
challenges are addressed. We further outline successful applications of
diffusion models in various RL-related tasks while discussing the limitations
of current approaches. Finally, we conclude the survey and offer insights into
future research directions, focusing on enhancing model performance and
applying diffusion models to broader tasks. We are actively maintaining a
GitHub repository for papers and other related resources in applying diffusion
models in RL: https://github.com/apexrl/Diff4RLSurvey .
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00729">http://arxiv.org/abs/2311.00729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00729]] ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection(http://arxiv.org/abs/2311.00729)</code></li>
<li>Summary: <p>Temporal action detection (TAD) involves the localization and classification
of action instances within untrimmed videos. While standard TAD follows fully
supervised learning with closed-set setting on large training data, recent
zero-shot TAD methods showcase the promising of open-set setting by leveraging
large-scale contrastive visual-language (ViL) pretrained models. However,
existing zero-shot TAD methods have limitations on how to properly construct
the strong relationships between two interdependent tasks of localization and
classification and adapt ViL model to video understanding. In this work, we
present ZEETAD, featuring two modules: dual-localization and zero-shot proposal
classification. The former is a Transformer-based module that detects action
events while selectively collecting crucial semantic embeddings for later
recognition. The latter one, CLIP-based module, generates semantic embeddings
from text and frame inputs for each temporal unit. Additionally, we enhance
discriminative capability on unseen classes by minimally updating the frozen
CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and
ActivityNet-1.3 datasets demonstrate our approach's superior performance in
zero-shot TAD and effective knowledge transfer from ViL models to unseen action
categories.
</p></li>
</ul>

<h3>Title: Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding. (arXiv:2311.01091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01091">http://arxiv.org/abs/2311.01091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01091]] Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding(http://arxiv.org/abs/2311.01091)</code></li>
<li>Summary: <p>Panoptic narrative grounding (PNG) aims to segment things and stuff objects
in an image described by noun phrases of a narrative caption. As a multimodal
task, an essential aspect of PNG is the visual-linguistic interaction between
image and caption. The previous two-stage method aggregates visual contexts
from offline-generated mask proposals to phrase features, which tend to be
noisy and fragmentary. The recent one-stage method aggregates only pixel
contexts from image features to phrase features, which may incur semantic
misalignment due to lacking object priors. To realize more comprehensive
visual-linguistic interaction, we propose to enrich phrases with coupled pixel
and object contexts by designing a Phrase-Pixel-Object Transformer Decoder
(PPO-TD), where both fine-grained part details and coarse-grained entity clues
are aggregated to phrase features. In addition, we also propose a PhraseObject
Contrastive Loss (POCL) to pull closer the matched phrase-object pairs and push
away unmatched ones for aggregating more precise object contexts from more
phrase-relevant object tokens. Extensive experiments on the PNG benchmark show
our method achieves new state-of-the-art performance with large margins.
</p></li>
</ul>

<h3>Title: Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation. (arXiv:2311.01092v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01092">http://arxiv.org/abs/2311.01092</a></li>
<li>Code URL: https://github.com/medhk23/omnifm-dr</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01092]] Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation(http://arxiv.org/abs/2311.01092)</code></li>
<li>Summary: <p>The emergence of multi-modal deep learning models has made significant
impacts on clinical applications in the last decade. However, the majority of
models are limited to single-tasking, without considering disease diagnosis is
indeed a multi-task procedure. Here, we demonstrate a unified transformer model
specifically designed for multi-modal clinical tasks by incorporating
customized instruction tuning. We first compose a multi-task training dataset
comprising 13.4 million instruction and ground-truth pairs (with approximately
one million radiographs) for the customized tuning, involving both image- and
pixel-level tasks. Thus, we can unify the various vision-intensive tasks in a
single training framework with homogeneous model inputs and outputs to increase
clinical interpretability in one reading. Finally, we demonstrate the overall
superior performance of our model compared to prior arts on various chest X-ray
benchmarks across multi-tasks in both direct inference and finetuning settings.
Three radiologists further evaluate the generated reports against the recorded
ones, which also exhibit the enhanced explainability of our multi-task model.
</p></li>
</ul>

<h3>Title: AiluRus: A Scalable ViT Framework for Dense Prediction. (arXiv:2311.01197v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01197">http://arxiv.org/abs/2311.01197</a></li>
<li>Code URL: https://github.com/caddyless/ailurus</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01197]] AiluRus: A Scalable ViT Framework for Dense Prediction(http://arxiv.org/abs/2311.01197)</code></li>
<li>Summary: <p>Vision transformers (ViTs) have emerged as a prevalent architecture for
vision tasks owing to their impressive performance. However, when it comes to
handling long token sequences, especially in dense prediction tasks that
require high-resolution input, the complexity of ViTs increases significantly.
Notably, dense prediction tasks, such as semantic segmentation or object
detection, emphasize more on the contours or shapes of objects, while the
texture inside objects is less informative. Motivated by this observation, we
propose to apply adaptive resolution for different regions in the image
according to their importance. Specifically, at the intermediate layer of the
ViT, we utilize a spatial-aware density-based clustering algorithm to select
representative tokens from the token sequence. Once the representative tokens
are determined, we proceed to merge other tokens into their closest
representative token. Consequently, semantic similar tokens are merged together
to form low-resolution regions, while semantic irrelevant tokens are preserved
independently as high-resolution regions. This strategy effectively reduces the
number of tokens, allowing subsequent layers to handle a reduced token sequence
and achieve acceleration. We evaluate our proposed method on three different
datasets and observe promising performance. For example, the "Segmenter ViT-L"
model can be accelerated by 48% FPS without fine-tuning, while maintaining the
performance. Additionally, our method can be applied to accelerate fine-tuning
as well. Experimental results demonstrate that we can save 52% training time
while accelerating 2.46 times FPS with only a 0.09% performance drop. The code
is available at https://github.com/caddyless/ailurus/tree/main.
</p></li>
</ul>

<h3>Title: Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification. (arXiv:2311.01212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01212">http://arxiv.org/abs/2311.01212</a></li>
<li>Code URL: https://github.com/henulwy/stbdip</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01212]] Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral Image Classification(http://arxiv.org/abs/2311.01212)</code></li>
<li>Summary: <p>Cross-domain few-shot hyperspectral image classification focuses on learning
prior knowledge from a large number of labeled samples from source domain and
then transferring the knowledge to the tasks which contain only few labeled
samples in target domains. Following the metric-based manner, many current
methods first extract the features of the query and support samples, and then
directly predict the classes of query samples according to their distance to
the support samples or prototypes. The relations between samples have not been
fully explored and utilized. Different from current works, this paper proposes
to learn sample relations from different views and take them into the model
learning process, to improve the cross-domain few-shot hyperspectral image
classification. Building on current DCFSL method which adopts a domain
discriminator to deal with domain-level distribution difference, the proposed
method applys contrastive learning to learn the class-level sample relations to
obtain more discriminable sample features. In addition, it adopts a transformer
based cross-attention learning module to learn the set-level sample relations
and acquire the attentions from query samples to support samples. Our
experimental results have demonstrated the contribution of the multi-view
relation learning mechanism for few-shot hyperspectral image classification
when compared with the state of the art methods.
</p></li>
</ul>

<h3>Title: FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01240">http://arxiv.org/abs/2311.01240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01240]] FacadeNet: Conditional Facade Synthesis via Selective Editing(http://arxiv.org/abs/2311.01240)</code></li>
<li>Summary: <p>We introduce FacadeNet, a deep learning approach for synthesizing building
facade images from diverse viewpoints. Our method employs a conditional GAN,
taking a single view of a facade along with the desired viewpoint information
and generates an image of the facade from the distinct viewpoint. To precisely
modify view-dependent elements like windows and doors while preserving the
structure of view-independent components such as walls, we introduce a
selective editing module. This module leverages image embeddings extracted from
a pre-trained vision transformer. Our experiments demonstrated state-of-the-art
performance on building facade generation, surpassing alternative methods.
</p></li>
</ul>

<h3>Title: Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition. (arXiv:2311.01283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01283">http://arxiv.org/abs/2311.01283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01283]] Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition(http://arxiv.org/abs/2311.01283)</code></li>
<li>Summary: <p>This paper presents a study on improving human action recognition through the
utilization of knowledge distillation, and the combination of CNN and ViT
models. The research aims to enhance the performance and efficiency of smaller
student models by transferring knowledge from larger teacher models. The
proposed method employs a Transformer vision network as the student model,
while a convolutional network serves as the teacher model. The teacher model
extracts local image features, whereas the student model focuses on global
features using an attention mechanism. The Vision Transformer (ViT)
architecture is introduced as a robust framework for capturing global
dependencies in images. Additionally, advanced variants of ViT, namely PVT,
Convit, MVIT, Swin Transformer, and Twins, are discussed, highlighting their
contributions to computer vision tasks. The ConvNeXt model is introduced as a
teacher model, known for its efficiency and effectiveness in computer vision.
The paper presents performance results for human action recognition on the
Stanford 40 dataset, comparing the accuracy and mAP of student models trained
with and without knowledge distillation. The findings illustrate that the
suggested approach significantly improves the accuracy and mAP when compared to
training networks under regular settings. These findings emphasize the
potential of combining local and global features in action recognition tasks.
</p></li>
</ul>

<h3>Title: Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01310">http://arxiv.org/abs/2311.01310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01310]] Scattering Vision Transformer: Spectral Mixing Matters(http://arxiv.org/abs/2311.01310)</code></li>
<li>Summary: <p>Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
</p></li>
</ul>

<h3>Title: Efficient Vision Transformer for Accurate Traffic Sign Detection. (arXiv:2311.01429v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01429">http://arxiv.org/abs/2311.01429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01429]] Efficient Vision Transformer for Accurate Traffic Sign Detection(http://arxiv.org/abs/2311.01429)</code></li>
<li>Summary: <p>This research paper addresses the challenges associated with traffic sign
detection in self-driving vehicles and driver assistance systems. The
development of reliable and highly accurate algorithms is crucial for the
widespread adoption of traffic sign recognition and detection (TSRD) in diverse
real-life scenarios. However, this task is complicated by suboptimal traffic
images affected by factors such as camera movement, adverse weather conditions,
and inadequate lighting. This study specifically focuses on traffic sign
detection methods and introduces the application of the Transformer model,
particularly the Vision Transformer variants, to tackle this task. The
Transformer's attention mechanism, originally designed for natural language
processing, offers improved parallel efficiency. Vision Transformers have
demonstrated success in various domains, including autonomous driving, object
detection, healthcare, and defense-related applications. To enhance the
efficiency of the Transformer model, the research proposes a novel strategy
that integrates a locality inductive bias and a transformer module. This
includes the introduction of the Efficient Convolution Block and the Local
Transformer Block, which effectively capture short-term and long-term
dependency information, thereby improving both detection speed and accuracy.
Experimental evaluations demonstrate the significant advancements achieved by
this approach, particularly when applied to the GTSDB dataset.
</p></li>
</ul>

<h3>Title: tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis. (arXiv:2311.00732v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00732">http://arxiv.org/abs/2311.00732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00732]] tmn at #SMM4H 2023: Comparing Text Preprocessing Techniques for Detecting Tweets Self-reporting a COVID-19 Diagnosis(http://arxiv.org/abs/2311.00732)</code></li>
<li>Summary: <p>The paper describes a system developed for Task 1 at SMM4H 2023. The goal of
the task is to automatically distinguish tweets that self-report a COVID-19
diagnosis (for example, a positive test, clinical diagnosis, or
hospitalization) from those that do not. We investigate the use of different
techniques for preprocessing tweets using four transformer-based models. The
ensemble of fine-tuned language models obtained an F1-score of 84.5%, which is
4.1% higher than the average value.
</p></li>
</ul>

<h3>Title: Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00871">http://arxiv.org/abs/2311.00871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00871]] Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models(http://arxiv.org/abs/2311.00871)</code></li>
<li>Summary: <p>Transformer models, notably large language models (LLMs), have the remarkable
ability to perform in-context learning (ICL) -- to perform new tasks when
prompted with unseen input-output examples without any explicit model training.
In this work, we study how effectively transformers can bridge between their
pretraining data mixture, comprised of multiple distinct task families, to
identify and learn new tasks in-context which are both inside and outside the
pretraining distribution. Building on previous work, we investigate this
question in a controlled setting, where we study transformer models trained on
sequences of $(x, f(x))$ pairs rather than natural language. Our empirical
results show transformers demonstrate near-optimal unsupervised model selection
capabilities, in their ability to first in-context identify different task
families and in-context learn within them when the task families are
well-represented in their pretraining data. However when presented with tasks
or functions which are out-of-domain of their pretraining data, we demonstrate
various failure modes of transformers and degradation of their generalization
for even simple extrapolation tasks. Together our results highlight that the
impressive ICL abilities of high-capacity sequence models may be more closely
tied to the coverage of their pretraining data mixtures than inductive biases
that create fundamental generalization capabilities.
</p></li>
</ul>

<h3>Title: COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning. (arXiv:2311.00886v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00886">http://arxiv.org/abs/2311.00886</a></li>
<li>Code URL: https://github.com/google-research/google-research</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00886]] COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning(http://arxiv.org/abs/2311.00886)</code></li>
<li>Summary: <p>Estimation of temporal counterfactual outcomes from observed history is
crucial for decision-making in many domains such as healthcare and e-commerce,
particularly when randomized controlled trials (RCTs) suffer from high cost or
impracticality. For real-world datasets, modeling time-dependent confounders is
challenging due to complex dynamics, long-range dependencies and both past
treatments and covariates affecting the future outcomes. In this paper, we
introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach
that integrates self-supervised learning for improved historical
representations. The proposed framework combines temporal and feature-wise
attention with a component-wise contrastive loss tailored for temporal
treatment outcome observations, yielding superior performance in estimation
accuracy and generalization to out-of-distribution data compared to existing
models, as validated by empirical results on both synthetic and real-world
datasets.
</p></li>
</ul>

<h3>Title: Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01442">http://arxiv.org/abs/2311.01442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01442]] Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models(http://arxiv.org/abs/2311.01442)</code></li>
<li>Summary: <p>Deep learning models, particularly Transformers, have achieved impressive
results in various domains, including time series forecasting. While existing
time series literature primarily focuses on model architecture modifications
and data augmentation techniques, this paper explores the training schema of
deep learning models for time series; how models are trained regardless of
their architecture. We perform extensive experiments to investigate the
occurrence of deep double descent in several Transformer models trained on
public time series data sets. We demonstrate epoch-wise deep double descent and
that overfitting can be reverted using more epochs. Leveraging these findings,
we achieve state-of-the-art results for long sequence time series forecasting
in nearly 70% of the 72 benchmarks tested. This suggests that many models in
the literature may possess untapped potential. Additionally, we introduce a
taxonomy for classifying training schema modifications, covering data
augmentation, model inputs, model targets, time series per model, and
computational budget.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00735">http://arxiv.org/abs/2311.00735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00735]] PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network(http://arxiv.org/abs/2311.00735)</code></li>
<li>Summary: <p>Positron emission tomography (PET), as an imaging technique with high
biochemical sensitivity, has been widely used in diagnosis of encephalopathy
and brain science research used in brain disease diagnosis and brain science
research. Since different tracers present different effects on the same focal
area, the choice of tracers is getting more significant for PET imaging.
Nowadays, with the wide application of PET imaging in neuropsychiatric
treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to
be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field.
However, due to the complexity of its preparation and other limitations, DOPA
is far less widely used than FDG. To address this issue, a tracer conversion
invertible neural network (TC-INN) for image projection is developed to map FDG
images to DOPA images through deep learning. More diagnostic information is
obtained by generating PET images from FDG to DOPA. Specifically, the proposed
TC-INN consists of two separate phases, one for training the traceable data,
the other for re-building the new data. The reference DOPA PET image is used as
the learning target for the corresponding network during the training process
of tracer conversion. Mean-while, the invertible network iteratively estimates
the resultant DOPA PET data and compares it to the reference DOPA PET data.
Notably, the reversible model employed variable enhancement techniques to
achieve better power generation. Moreover, image registration needs to be
performed before training due to the angular deviation of the acquired FDG and
DOPA data information. Experimental results show generative ability in mapping
be-tween FDG images and DOPA images. It demonstrates great potential for PET
image conversion in the case of limited tracer applications.
</p></li>
</ul>

<h3>Title: A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision. (arXiv:2311.00995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00995">http://arxiv.org/abs/2311.00995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00995]] A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision(http://arxiv.org/abs/2311.00995)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) have been workhorse generative models
for last many years, especially in the research field of computer vision.
Accordingly, there have been many significant advancements in the theory and
application of GAN models, which are notoriously hard to train, but produce
good results if trained well. There have been many a surveys on GANs,
organizing the vast GAN literature from various focus and perspectives.
However, none of the surveys brings out the important chronological aspect: how
the multiple challenges of employing GAN models were solved one-by-one over
time, across multiple landmark research works. This survey intends to bridge
that gap and present some of the landmark research works on the theory and
application of GANs, in chronological order.
</p></li>
</ul>

<h3>Title: Novel View Synthesis from a Single RGBD Image for Indoor Scenes. (arXiv:2311.01065v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01065">http://arxiv.org/abs/2311.01065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01065]] Novel View Synthesis from a Single RGBD Image for Indoor Scenes(http://arxiv.org/abs/2311.01065)</code></li>
<li>Summary: <p>In this paper, we propose an approach for synthesizing novel view images from
a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an
interesting computer vision task with extensive applications. Methods using
multiple images has been well-studied, exemplary ones include training
scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo
(MVS) and 3D rendering pipelines. However, both are either computationally
intensive or non-generalizable across different scenes, limiting their
practical value. Conversely, the depth information embedded in RGBD images
unlocks 3D potential from a singular view, simplifying NVS. The widespread
availability of compact, affordable stereo cameras, and even LiDARs in
contemporary devices like smartphones, makes capturing RGBD images more
accessible than ever. In our method, we convert an RGBD image into a point
cloud and render it from a different viewpoint, then formulate the NVS task
into an image translation problem. We leveraged generative adversarial networks
to style-transfer the rendered image, achieving a result similar to a
photograph taken from the new perspective. We explore both unsupervised
learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate
the qualitative results. Our method circumvents the limitations of traditional
multi-image techniques, holding significant promise for practical, real-time
applications in NVS.
</p></li>
</ul>

<h3>Title: Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network. (arXiv:2311.01192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01192">http://arxiv.org/abs/2311.01192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01192]] Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network(http://arxiv.org/abs/2311.01192)</code></li>
<li>Summary: <p>Along with generative AI, interest in scene graph generation (SGG), which
comprehensively captures the relationships and interactions between objects in
an image and creates a structured graph-based representation, has significantly
increased in recent years. However, relying on object-centric and dichotomous
relationships, existing SGG methods have a limited ability to accurately
predict detailed relationships. To solve these problems, a new approach to the
modeling multiobject relationships, called edge dual scene graph generation
(EdgeSGG), is proposed herein. EdgeSGG is based on a edge dual scene graph and
Dual Message Passing Neural Network (DualMPNN), which can capture rich
contextual interactions between unconstrained objects. To facilitate the
learning of edge dual scene graphs with a symmetric graph structure, the
proposed DualMPNN learns both object- and relation-centric features for more
accurately predicting relation-aware contexts and allows fine-grained
relational updates between objects. A comparative experiment with
state-of-the-art (SoTA) methods was conducted using two public datasets for SGG
operations and six metrics for three subtasks. Compared with SoTA approaches,
the proposed model exhibited substantial performance improvements across all
SGG subtasks. Furthermore, experiment on long-tail distributions revealed that
incorporating the relationships between objects effectively mitigates existing
long-tail problems.
</p></li>
</ul>

<h3>Title: Multi-dimensional data refining strategy for effective fine-tuning LLMs. (arXiv:2311.01049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01049">http://arxiv.org/abs/2311.01049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01049]] Multi-dimensional data refining strategy for effective fine-tuning LLMs(http://arxiv.org/abs/2311.01049)</code></li>
<li>Summary: <p>Data is a cornerstone for fine-tuning large language models, yet acquiring
suitable data remains challenging. Challenges encompassed data scarcity,
linguistic diversity, and domain-specific content. This paper presents lessons
learned while crawling and refining data tailored for fine-tuning Vietnamese
language models. Crafting such a dataset, while accounting for linguistic
intricacies and striking a balance between inclusivity and accuracy, demands
meticulous planning. Our paper presents a multidimensional strategy including
leveraging existing datasets in the English language and developing customized
data-crawling scripts with the assistance of generative AI tools. A fine-tuned
LLM model for the Vietnamese language, which was produced using resultant
datasets, demonstrated good performance while generating Vietnamese news
articles from prompts. The study offers practical solutions and guidance for
future fine-tuning models in languages like Vietnamese.
</p></li>
</ul>

<h3>Title: Generative Input: Towards Next-Generation Input Methods Paradigm. (arXiv:2311.01166v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01166">http://arxiv.org/abs/2311.01166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01166]] Generative Input: Towards Next-Generation Input Methods Paradigm(http://arxiv.org/abs/2311.01166)</code></li>
<li>Summary: <p>Since the release of ChatGPT, generative models have achieved tremendous
success and become the de facto approach for various NLP tasks. However, its
application in the field of input methods remains under-explored. Many neural
network approaches have been applied to the construction of Chinese input
method engines(IMEs).Previous research often assumed that the input pinyin was
correct and focused on Pinyin-to-character(P2C) task, which significantly falls
short of meeting users' demands. Moreover, previous research could not leverage
user feedback to optimize the model and provide personalized results. In this
study, we propose a novel Generative Input paradigm named GeneInput. It uses
prompts to handle all input scenarios and other intelligent auxiliary input
functions, optimizing the model with user feedback to deliver personalized
results. The results demonstrate that we have achieved state-of-the-art
performance for the first time in the Full-mode Key-sequence to
Characters(FK2C) task. We propose a novel reward model training method that
eliminates the need for additional manual annotations and the performance
surpasses GPT-4 in tasks involving intelligent association and conversational
assistance. Compared to traditional paradigms, GeneInput not only demonstrates
superior performance but also exhibits enhanced robustness, scalability, and
online learning capabilities.
</p></li>
</ul>

<h3>Title: Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information. (arXiv:2311.01326v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01326">http://arxiv.org/abs/2311.01326</a></li>
<li>Code URL: https://github.com/screemix/kgc-t5-with-neighbors</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01326]] Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information(http://arxiv.org/abs/2311.01326)</code></li>
<li>Summary: <p>Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which
limits their potential performance. Knowledge Graph Completion (KGC) techniques
aim to address this issue. However, traditional KGC methods are computationally
intensive and impractical for large-scale KGs, necessitating the learning of
dense node embeddings and computing pairwise distances. Generative
transformer-based language models (e.g., T5 and recent KGT5) offer a promising
solution as they can predict the tail nodes directly. In this study, we propose
to include node neighborhoods as additional information to improve KGC methods
based on language models. We examine the effects of this imputation and show
that, on both inductive and transductive Wikidata subsets, our method
outperforms KGT5 and conventional KGC approaches. We also provide an extensive
analysis of the impact of neighborhood on model prediction and show its
importance. Furthermore, we point the way to significantly improve KGC through
more effective neighborhood selection.
</p></li>
</ul>

<h3>Title: VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01191">http://arxiv.org/abs/2311.01191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01191]] VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification(http://arxiv.org/abs/2311.01191)</code></li>
<li>Summary: <p>Class imbalance in graph data poses significant challenges for node
classification. Existing methods, represented by SMOTE-based approaches,
partially alleviate this issue but still exhibit limitations during imbalanced
scenario construction. Self-supervised learning (SSL) offers a promising
solution by synthesizing minority nodes from the data itself, yet its potential
remains unexplored. In this paper, we analyze the limitations of SMOTE-based
approaches and introduce VIGraph, a novel SSL model based on the
self-supervised Variational Graph Auto-Encoder (VGAE) that leverages
Variational Inference (VI) to generate minority nodes. Specifically, VIGraph
strictly adheres to the concept of imbalance when constructing imbalanced
graphs and utilizes the generative VGAE to generate minority nodes. Moreover,
VIGraph introduces a novel Siamese contrastive strategy at the decoding phase
to improve the overall quality of generated nodes. VIGraph can generate
high-quality nodes without reintegrating them into the original graph,
eliminating the "Generating, Reintegrating, and Retraining" process found in
SMOTE-based methods. Experiments on multiple real-world datasets demonstrate
that VIGraph achieves promising results for class-imbalanced node
classification tasks.
</p></li>
</ul>

<h3>Title: Monotone Generative Modeling via a Gromov-Monge Embedding. (arXiv:2311.01375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01375">http://arxiv.org/abs/2311.01375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01375]] Monotone Generative Modeling via a Gromov-Monge Embedding(http://arxiv.org/abs/2311.01375)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) are powerful tools for creating new
content, but they face challenges such as sensitivity to starting conditions
and mode collapse. To address these issues, we propose a deep generative model
that utilizes the Gromov-Monge embedding (GME). It helps identify the
low-dimensional structure of the underlying measure of the data and then maps
it, while preserving its geometry, into a measure in a low-dimensional latent
space, which is then optimally transported to the reference measure. We
guarantee the preservation of the underlying geometry by the GME and
$c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic
embedding cost employed by the GME. The latter property is a first step in
guaranteeing better robustness to initialization of parameters and mode
collapse. Numerical experiments demonstrate the effectiveness of our approach
in generating high-quality images, avoiding mode collapse, and exhibiting
robustness to different starting conditions.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Incorporating Language-Driven Appearance Knowledge Units with Visual Cues in Pedestrian Detection. (arXiv:2311.01025v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01025">http://arxiv.org/abs/2311.01025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01025]] Incorporating Language-Driven Appearance Knowledge Units with Visual Cues in Pedestrian Detection(http://arxiv.org/abs/2311.01025)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown their capability in understanding
contextual and semantic information regarding appearance knowledge of
instances. In this paper, we introduce a novel approach to utilize the strength
of an LLM in understanding contextual appearance variations and to leverage its
knowledge into a vision model (here, pedestrian detection). While pedestrian
detection is considered one of crucial tasks directly related with our safety
(e.g., intelligent driving system), it is challenging because of varying
appearances and poses in diverse scenes. Therefore, we propose to formulate
language-driven appearance knowledge units and incorporate them with visual
cues in pedestrian detection. To this end, we establish description corpus
which includes numerous narratives describing various appearances of
pedestrians and others. By feeding them through an LLM, we extract appearance
knowledge sets that contain the representations of appearance variations. After
that, we perform a task-prompting process to obtain appearance knowledge units
which are representative appearance knowledge guided to be relevant to a
downstream pedestrian detection task. Finally, we provide plentiful appearance
information by integrating the language-driven knowledge units with visual
cues. Through comprehensive experiments with various pedestrian detectors, we
verify the effectiveness of our method showing noticeable performance gains and
achieving state-of-the-art detection performance.
</p></li>
</ul>

<h3>Title: Long Story Short: a Summarize-then-Search Method for Long Video Question Answering. (arXiv:2311.01233v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01233">http://arxiv.org/abs/2311.01233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01233]] Long Story Short: a Summarize-then-Search Method for Long Video Question Answering(http://arxiv.org/abs/2311.01233)</code></li>
<li>Summary: <p>Large language models such as GPT-3 have demonstrated an impressive
capability to adapt to new tasks without requiring task-specific training data.
This capability has been particularly effective in settings such as narrative
question answering, where the diversity of tasks is immense, but the available
supervision data is small. In this work, we investigate if such language models
can extend their zero-shot reasoning abilities to long multimodal narratives in
multimedia content such as drama, movies, and animation, where the story plays
an essential role. We propose Long Story Short, a framework for narrative video
QA that first summarizes the narrative of the video to a short plot and then
searches parts of the video relevant to the question. We also propose to
enhance visual matching with CLIPCheck. Our model outperforms state-of-the-art
supervised models by a large margin, highlighting the potential of zero-shot QA
for long videos.
</p></li>
</ul>

<h3>Title: Can Large Language Models Design Accurate Label Functions?. (arXiv:2311.00739v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00739">http://arxiv.org/abs/2311.00739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00739]] Can Large Language Models Design Accurate Label Functions?(http://arxiv.org/abs/2311.00739)</code></li>
<li>Summary: <p>Programmatic weak supervision methodologies facilitate the expedited labeling
of extensive datasets through the use of label functions (LFs) that encapsulate
heuristic data sources. Nonetheless, the creation of precise LFs necessitates
domain expertise and substantial endeavors. Recent advances in pre-trained
language models (PLMs) have exhibited substantial potential across diverse
tasks. However, the capacity of PLMs to autonomously formulate accurate LFs
remains an underexplored domain. In this research, we address this gap by
introducing DataSculpt, an interactive framework that harnesses PLMs for the
automated generation of LFs. Within DataSculpt, we incorporate an array of
prompting techniques, instance selection strategies, and LF filtration methods
to explore the expansive design landscape. Ultimately, we conduct a thorough
assessment of DataSculpt's performance on 12 real-world datasets, encompassing
a range of tasks. This evaluation unveils both the strengths and limitations of
contemporary PLMs in LF design.
</p></li>
</ul>

<h3>Title: Task-Agnostic Low-Rank Adapters for Unseen English Dialects. (arXiv:2311.00915v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00915">http://arxiv.org/abs/2311.00915</a></li>
<li>Code URL: https://github.com/zedian/hyperlora</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00915]] Task-Agnostic Low-Rank Adapters for Unseen English Dialects(http://arxiv.org/abs/2311.00915)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are trained on corpora disproportionally
weighted in favor of Standard American English. As a result, speakers of other
dialects experience significantly more failures when interacting with these
technologies. In practice, these speakers often accommodate their speech to be
better understood. Our work shares the belief that language technologies should
be designed to accommodate the diversity in English dialects and not the other
way around. However, prior works on dialect struggle with generalizing to
evolving and emerging dialects in a scalable manner. To fill this gap, our
method, HyperLoRA, leverages expert linguistic knowledge to enable
resource-efficient adaptation via hypernetworks. By disentangling
dialect-specific and cross-dialectal information, HyperLoRA improves
generalization to unseen dialects in a task-agnostic fashion. Not only is
HyperLoRA more scalable in the number of parameters, but it also achieves the
best or most competitive performance across 5 dialects in a zero-shot setting.
In this way, our approach facilitates access to language technology for
billions of English dialect speakers who are traditionally underrepresented.
</p></li>
</ul>

<h3>Title: Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia. (arXiv:2311.00998v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00998">http://arxiv.org/abs/2311.00998</a></li>
<li>Code URL: https://github.com/exqrch/indonesiannmt</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00998]] Replicable Benchmarking of Neural Machine Translation (NMT) on Low-Resource Local Languages in Indonesia(http://arxiv.org/abs/2311.00998)</code></li>
<li>Summary: <p>Neural machine translation (NMT) for low-resource local languages in
Indonesia faces significant challenges, including the need for a representative
benchmark and limited data availability. This work addresses these challenges
by comprehensively analyzing training NMT systems for four low-resource local
languages in Indonesia: Javanese, Sundanese, Minangkabau, and Balinese. Our
study encompasses various training approaches, paradigms, data sizes, and a
preliminary study into using large language models for synthetic low-resource
languages parallel data generation. We reveal specific trends and insights into
practical strategies for low-resource language translation. Our research
demonstrates that despite limited computational resources and textual data,
several of our NMT systems achieve competitive performances, rivaling the
translation quality of zero-shot gpt-3.5-turbo. These findings significantly
advance NMT for low-resource languages, offering valuable guidance for
researchers in similar contexts.
</p></li>
</ul>

<h3>Title: Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism. (arXiv:2311.01041v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01041">http://arxiv.org/abs/2311.01041</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01041]] Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism(http://arxiv.org/abs/2311.01041)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated impressive language
understanding and generation capabilities, enabling them to answer a wide range
of questions across various domains. However, these models are not flawless and
often produce responses that contain errors or misinformation. These
inaccuracies, commonly referred to as hallucinations, render LLMs unreliable
and even unusable in many scenarios. In this paper, our focus is on mitigating
the issue of hallucination in LLMs, particularly in the context of
question-answering. Instead of attempting to answer all questions, we explore a
refusal mechanism that instructs LLMs to refuse to answer challenging questions
in order to avoid errors. We then propose a simple yet effective solution
called Learn to Refuse (L2R), which incorporates the refusal mechanism to
enable LLMs to recognize and refuse to answer questions that they find
difficult to address. To achieve this, we utilize a structured knowledge base
to represent all the LLM's understanding of the world, enabling it to provide
traceable gold knowledge. This knowledge base is separate from the LLM and
initially empty, and it is progressively expanded with validated knowledge.
When an LLM encounters questions outside its domain, the system recognizes its
knowledge scope and determines whether it can answer the question
independently. Additionally, we introduce a method for automatically and
efficiently expanding the knowledge base of LLMs. Through qualitative and
quantitative analysis, we demonstrate that our approach enhances the
controllability and reliability of LLMs.
</p></li>
</ul>

<h3>Title: Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model. (arXiv:2311.01149v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01149">http://arxiv.org/abs/2311.01149</a></li>
<li>Code URL: https://github.com/casia-lm/chinesewebtext</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01149]] Chinesewebtext: Large-scale high-quality Chinese web text extracted with effective evaluation model(http://arxiv.org/abs/2311.01149)</code></li>
<li>Summary: <p>During the development of large language models (LLMs), the scale and quality
of the pre-training data play a crucial role in shaping LLMs' capabilities. To
accelerate the research of LLMs, several large-scale datasets, such as C4 [1],
Pile [2], RefinedWeb [3] and WanJuan [4], have been released to the public.
However, most of the released corpus focus mainly on English, and there is
still lack of complete tool-chain for extracting clean texts from web data.
Furthermore, fine-grained information of the corpus, e.g. the quality of each
text, is missing. To address these challenges, we propose in this paper a new
complete tool-chain EvalWeb to extract Chinese clean texts from noisy web data.
First, similar to previous work, manually crafted rules are employed to discard
explicit noisy texts from the raw crawled web contents. Second, a well-designed
evaluation model is leveraged to assess the remaining relatively clean data,
and each text is assigned a specific quality score. Finally, we can easily
utilize an appropriate threshold to select the high-quality pre-training data
for Chinese. Using our proposed approach, we release the largest and latest
large-scale high-quality Chinese web text ChineseWebText, which consists of
1.42 TB and each text is associated with a quality score, facilitating the LLM
researchers to choose the data according to the desired quality thresholds. We
also release a much cleaner subset of 600 GB Chinese data with the quality
exceeding 90%.
</p></li>
</ul>

<h3>Title: Revisiting the Knowledge Injection Frameworks. (arXiv:2311.01150v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01150">http://arxiv.org/abs/2311.01150</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01150]] Revisiting the Knowledge Injection Frameworks(http://arxiv.org/abs/2311.01150)</code></li>
<li>Summary: <p>In recent years, large language models (LLMs), such as GPTs, have attained
great impact worldwide. However, how to adapt these LLMs to better suit the
vertical domain-specific tasks by utilizing external knowledge remains not
completely solved. Indeed, there have emerged a few works on this line where
most of them rely on an alignment heuristic that is built to inject the
corresponding knowledge tuple into the associated text sample.
</p>
<p>However, despite the promise, we identify a pivotal problem in this work
ubiquitously. Simply put, we find that injecting unaligned (i.e., random)
knowledge tuple into the LLMs achieves comparable (and sometimes better)
results than the aligned knowledge being injected. We therefore take a thorough
investigation of this frustrating finding on a variety of related prior work
and further provide a chain of potential interpretations for the phenomenon.
Based on all that, we offer a simple remediated technique. Briefly, the core of
this technique is rooted in an ideological emphasis on the pruning and
purification of the external knowledge base to be injected into LLMs. At last,
we show that by integrating this technique into most (if not all) knowledge
injection frameworks and recent LLMs, it manages to overcome the aforementioned
sanity problem and further pushes the boundary of the performance of the
domain-adaptive LLMs.
</p></li>
</ul>

<h3>Title: Predicting Question-Answering Performance of Large Language Models through Semantic Consistency. (arXiv:2311.01152v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01152">http://arxiv.org/abs/2311.01152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01152]] Predicting Question-Answering Performance of Large Language Models through Semantic Consistency(http://arxiv.org/abs/2311.01152)</code></li>
<li>Summary: <p>Semantic consistency of a language model is broadly defined as the model's
ability to produce semantically-equivalent outputs, given
semantically-equivalent inputs. We address the task of assessing
question-answering (QA) semantic consistency of contemporary large language
models (LLMs) by manually creating a benchmark dataset with high-quality
paraphrases for factual questions, and release the dataset to the community.
</p>
<p>We further combine the semantic consistency metric with additional
measurements suggested in prior work as correlating with LLM QA accuracy, for
building and evaluating a framework for factual QA reference-less performance
prediction -- predicting the likelihood of a language model to accurately
answer a question. Evaluating the framework on five contemporary LLMs, we
demonstrate encouraging, significantly outperforming baselines, results.
</p></li>
</ul>

<h3>Title: FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01282">http://arxiv.org/abs/2311.01282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01282]] FlashDecoding++: Faster Large Language Model Inference on GPUs(http://arxiv.org/abs/2311.01282)</code></li>
<li>Summary: <p>As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and &gt;50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
</p>
<p>We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</p></li>
</ul>

<h3>Title: AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01305">http://arxiv.org/abs/2311.01305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01305]] AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models(http://arxiv.org/abs/2311.01305)</code></li>
<li>Summary: <p>Large language models(LLMs) exhibit excellent performance across a variety of
tasks, but they come with significant computational and storage costs.
Quantizing these models is an effective way to alleviate this issue. However,
existing methods struggle to strike a balance between model accuracy and
hardware efficiency. This is where we introduce AWEQ, a post-training method
that requires no additional training overhead. AWEQ excels in both
ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization.
There is an observation that weight quantization is less challenging than
activation quantization. AWEQ transfers the difficulty of activation
quantization to weights using channel equalization, achieving a balance between
the quantization difficulties of both, and thereby maximizing performance. We
have further refined the equalization method to mitigate quantization bias
error, ensuring the robustness of the model. Extensive experiments on popular
models such as LLaMA and OPT demonstrate that AWEQ outperforms all existing
post-training quantization methods for large models.
</p></li>
</ul>

<h3>Title: The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models. (arXiv:2311.01307v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01307">http://arxiv.org/abs/2311.01307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01307]] The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models(http://arxiv.org/abs/2311.01307)</code></li>
<li>Summary: <p>Large Language Models (LLMs) make natural interfaces to factual knowledge,
but their usefulness is limited by their tendency to deliver inconsistent
answers to semantically equivalent questions. For example, a model might
predict both "Anne Redpath passed away in Edinburgh." and "Anne Redpath's life
ended in London." In this work, we identify potential causes of inconsistency
and evaluate the effectiveness of two mitigation strategies: up-scaling and
augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas
models show that both strategies reduce inconsistency while retrieval
augmentation is considerably more efficient. We further consider and
disentangle the consistency contributions of different components of Atlas. For
all LMs evaluated we find that syntactical form and other evaluation task
artifacts impact consistency. Taken together, our results provide a better
understanding of the factors affecting the factual consistency of language
models.
</p></li>
</ul>

<h3>Title: Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01007">http://arxiv.org/abs/2311.01007</a></li>
<li>Code URL: https://github.com/clinicalml/onboarding_human_ai</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01007]] Effective Human-AI Teams via Learned Natural Language Rules and Onboarding(http://arxiv.org/abs/2311.01007)</code></li>
<li>Summary: <p>People are relying on AI agents to assist them with various tasks. The human
must know when to rely on the agent, collaborate with the agent, or ignore its
suggestions. In this work, we propose to learn rules grounded in data regions
and described in natural language that illustrate how the human should
collaborate with the AI. Our novel region discovery algorithm finds local
regions in the data as neighborhoods in an embedding space that corrects the
human prior. Each region is then described using an iterative and contrastive
procedure where a large language model describes the region. We then teach
these rules to the human via an onboarding stage. Through user studies on
object detection and question-answering tasks, we show that our method can lead
to more accurate human-AI teams. We also evaluate our region discovery and
description algorithms separately.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Concatenated Masked Autoencoders as Spatial-Temporal Learner. (arXiv:2311.00961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00961">http://arxiv.org/abs/2311.00961</a></li>
<li>Code URL: https://github.com/minhoooo1/catmae</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00961]] Concatenated Masked Autoencoders as Spatial-Temporal Learner(http://arxiv.org/abs/2311.00961)</code></li>
<li>Summary: <p>Learning representations from videos requires understanding continuous motion
and visual correspondences between frames. In this paper, we introduce the
Concatenated Masked Autoencoders (CatMAE) as a spatial-temporal learner for
self-supervised video representation learning. For the input sequence of video
frames, CatMAE keeps the initial frame unchanged while applying substantial
masking (95%) to subsequent frames. The encoder in CatMAE is responsible for
encoding visible patches for each frame individually; subsequently, for each
masked frame, the decoder leverages visible patches from both previous and
current frames to reconstruct the original image. Our proposed method enables
the model to estimate the motion information between visible patches, match the
correspondences between preceding and succeeding frames, and ultimately learn
the evolution of scenes. Furthermore, we propose a new data augmentation
strategy, Video-Reverse (ViRe), which uses reversed video frames as the model's
reconstruction targets. This further encourages the model to utilize continuous
motion details and correspondences to complete the reconstruction, thereby
enhancing the model's capabilities. Compared to the most advanced pre-training
methods, CatMAE achieves a leading level in video segmentation tasks and action
recognition tasks.
</p></li>
</ul>

<h3>Title: Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation. (arXiv:2311.00979v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00979">http://arxiv.org/abs/2311.00979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00979]] Overhead Line Defect Recognition Based on Unsupervised Semantic Segmentation(http://arxiv.org/abs/2311.00979)</code></li>
<li>Summary: <p>Overhead line inspection greatly benefits from defect recognition using
visible light imagery. Addressing the limitations of existing feature
extraction techniques and the heavy data dependency of deep learning
approaches, this paper introduces a novel defect recognition framework. This is
built on the Faster RCNN network and complemented by unsupervised semantic
segmentation. The approach involves identifying the type and location of the
target equipment, utilizing semantic segmentation to differentiate between the
device and its backdrop, and finally employing similarity measures and logical
rules to categorize the type of defect. Experimental results indicate that this
methodology focuses more on the equipment rather than the defects when
identifying issues in overhead lines. This leads to a notable enhancement in
accuracy and exhibits impressive adaptability. Thus, offering a fresh
perspective for automating the inspection of distribution network equipment.
</p></li>
</ul>

<h3>Title: CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation. (arXiv:2311.00987v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00987">http://arxiv.org/abs/2311.00987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00987]] CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation(http://arxiv.org/abs/2311.00987)</code></li>
<li>Summary: <p>The advancement of computer vision has pushed visual analysis tasks from
still images to the video domain. In recent years, video instance segmentation,
which aims to track and segment multiple objects in video frames, has drawn
much attention for its potential applications in various emerging areas such as
autonomous driving, intelligent transportation, and smart retail. In this
paper, we propose an effective framework for instance-level visual analysis on
video frames, which can simultaneously conduct object detection, instance
segmentation, and multi-object tracking. The core idea of our method is
collaborative multi-task learning which is achieved by a novel structure, named
associative connections among detection, segmentation, and tracking task heads
in an end-to-end learnable CNN. These additional connections allow information
propagation across multiple related tasks, so as to benefit these tasks
simultaneously. We evaluate the proposed method extensively on KITTI MOTS and
MOTS Challenge datasets and obtain quite encouraging results.
</p></li>
</ul>

<h3>Title: Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation. (arXiv:2311.01023v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01023">http://arxiv.org/abs/2311.01023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01023]] Augmentation is AUtO-Net: Augmentation-Driven Contrastive Multiview Learning for Medical Image Segmentation(http://arxiv.org/abs/2311.01023)</code></li>
<li>Summary: <p>The utilisation of deep learning segmentation algorithms that learn complex
organs and tissue patterns and extract essential regions of interest from the
noisy background to improve the visual ability for medical image diagnosis has
achieved impressive results in Medical Image Computing (MIC). This thesis
focuses on retinal blood vessel segmentation tasks, providing an extensive
literature review of deep learning-based medical image segmentation approaches
while comparing the methodologies and empirical performances. The work also
examines the limitations of current state-of-the-art methods by pointing out
the two significant existing limitations: data size constraints and the
dependency on high computational resources. To address such problems, this work
proposes a novel efficient, simple multiview learning framework that
contrastively learns invariant vessel feature representation by comparing with
multiple augmented views by various transformations to overcome data shortage
and improve generalisation ability. Moreover, the hybrid network architecture
integrates the attention mechanism into a Convolutional Neural Network to
further capture complex continuous curvilinear vessel structures. The result
demonstrates the proposed method validated on the CHASE-DB1 dataset, attaining
the highest F1 score of 83.46% and the highest Intersection over Union (IOU)
score of 71.62% with UNet structure, surpassing existing benchmark UNet-based
methods by 1.95% and 2.8%, respectively. The combination of the metrics
indicates the model detects the vessel object accurately with a highly
coincidental location with the ground truth. Moreover, the proposed approach
could be trained within 30 minutes by consuming less than 3 GB GPU RAM, and
such characteristics support the efficient implementation for real-world
applications and deployments.
</p></li>
</ul>

<h3>Title: A deep learning experiment for semantic segmentation of overlapping characters in palimpsests. (arXiv:2311.01130v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01130">http://arxiv.org/abs/2311.01130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01130]] A deep learning experiment for semantic segmentation of overlapping characters in palimpsests(http://arxiv.org/abs/2311.01130)</code></li>
<li>Summary: <p>Palimpsests refer to historical manuscripts where erased writings have been
partially covered by the superimposition of a second writing. By employing
imaging techniques, e.g., multispectral imaging, it becomes possible to
identify features that are imperceptible to the naked eye, including faded and
erased inks. When dealing with overlapping inks, Artificial Intelligence
techniques can be utilized to disentangle complex nodes of overlapping letters.
In this work, we propose deep learning-based semantic segmentation as a method
for identifying and segmenting individual letters in overlapping characters.
The experiment was conceived as a proof of concept, focusing on the palimpsests
of the Ars Grammatica by Prisciano as a case study. Furthermore, caveats and
prospects of our approach combined with multispectral imaging are also
discussed.
</p></li>
</ul>

<h3>Title: AeroPath: An airway segmentation benchmark dataset with challenging pathology. (arXiv:2311.01138v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01138">http://arxiv.org/abs/2311.01138</a></li>
<li>Code URL: https://github.com/raidionics/aeropath</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01138]] AeroPath: An airway segmentation benchmark dataset with challenging pathology(http://arxiv.org/abs/2311.01138)</code></li>
<li>Summary: <p>To improve the prognosis of patients suffering from pulmonary diseases, such
as lung cancer, early diagnosis and treatment are crucial. The analysis of CT
images is invaluable for diagnosis, whereas high quality segmentation of the
airway tree are required for intervention planning and live guidance during
bronchoscopy. Recently, the Multi-domain Airway Tree Modeling (ATM'22)
challenge released a large dataset, both enabling training of deep-learning
based models and bringing substantial improvement of the state-of-the-art for
the airway segmentation task. However, the ATM'22 dataset includes few patients
with severe pathologies affecting the airway tree anatomy. In this study, we
introduce a new public benchmark dataset (AeroPath), consisting of 27 CT images
from patients with pathologies ranging from emphysema to large tumors, with
corresponding trachea and bronchi annotations. Second, we present a multiscale
fusion design for automatic airway segmentation. Models were trained on the
ATM'22 dataset, tested on the AeroPath dataset, and further evaluated against
competitive open-source methods. The same performance metrics as used in the
ATM'22 challenge were used to benchmark the different considered approaches.
Lastly, an open web application is developed, to easily test the proposed model
on new data. The results demonstrated that our proposed architecture predicted
topologically correct segmentations for all the patients included in the
AeroPath dataset. The proposed method is robust and able to handle various
anomalies, down to at least the fifth airway generation. In addition, the
AeroPath dataset, featuring patients with challenging pathologies, will
contribute to development of new state-of-the-art methods. The AeroPath dataset
and the web application are made openly available.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
