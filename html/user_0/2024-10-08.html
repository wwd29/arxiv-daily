<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-08</h1>
<h3>Title: Post-Quantum Cryptography Anonymous Scheme -- PQCWC: Post-Quantum Cryptography Winternitz-Chen</h3>
<ul>
<li><strong>Authors: </strong>Abel C. H. Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.NI, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03678">https://arxiv.org/abs/2410.03678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03678">https://arxiv.org/pdf/2410.03678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03678]] Post-Quantum Cryptography Anonymous Scheme -- PQCWC: Post-Quantum Cryptography Winternitz-Chen(https://arxiv.org/abs/2410.03678)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>As quantum computing technology matures, it poses a threat to the security of mainstream asymmetric cryptographic methods. In response, the National Institute of Standards and Technology released the final version of post-quantum cryptographic (PQC) algorithm standards in August 2024. These post-quantum cryptographic algorithms are primarily based on lattice-based and hash-based cryptography. Therefore, this study proposes the Post-Quantum Cryptography Winternitz-Chen (PQCWC) anonymous scheme, aimed at exploring the design of anonymous schemes based on PQC for future applications in privacy protection. The anonymous scheme designed in this study is mainly built on the Winternitz signature scheme, which can prevent the original public key from being exposed in the certificate. Furthermore, the PQCWC anonymous scheme integrates the butterfly key expansion mechanism, proposing the first hash-based butterfly key expansion mechanism in the world, achieving anonymity for both the registration authority and the certificate authority, thereby fully protecting privacy. In the experimental environment, this study compares various hash algorithms, including Secure Hash Algorithm-1 (SHA-1), the SHA-2 series, the SHA-3 series, and the BLAKE series. The results demonstrate that the proposed anonymous scheme can achieve anonymity without increasing key length, signature length, key generation time, signature generation time, or signature verification time.</li>
</ul>

<h3>Title: LCM: Log Conformal Maps for Robust Representation Learning to Mitigate Perspective Distortion</h3>
<ul>
<li><strong>Authors: </strong>Meenakshi Subhash Chippa, Prakash Chandra Chhipa, Kanjar De, Marcus Liwicki, Rajkumar Saini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03686">https://arxiv.org/abs/2410.03686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03686">https://arxiv.org/pdf/2410.03686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03686]] LCM: Log Conformal Maps for Robust Representation Learning to Mitigate Perspective Distortion(https://arxiv.org/abs/2410.03686)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Perspective distortion (PD) leads to substantial alterations in the shape, size, orientation, angles, and spatial relationships of visual elements in images. Accurately determining camera intrinsic and extrinsic parameters is challenging, making it hard to synthesize perspective distortion effectively. The current distortion correction methods involve removing distortion and learning vision tasks, thus making it a multi-step process, often compromising performance. Recent work leverages the Möbius transform for mitigating perspective distortions (MPD) to synthesize perspective distortions without estimating camera parameters. An essential downside of using the Möbius transform is that it requires tuning multiple interdependent and interrelated parameters and involving complex arithmetic operations, leading to substantial computational complexity. To address these challenges, we propose Log Conformal Maps (LCM), a method leveraging the logarithmic function to approximate perspective distortions with fewer parameters and reduced computational complexity. We provide a theoretical foundation complemented with experiments to demonstrate that LCM with fewer parameters approximates the MPD. We show that LCM integrates well with supervised and self-supervised representation learning, outperform standard models, and matches the state-of-the-art performance in mitigating perspective distortion over multiple benchmarks, namely Imagenet-PD, Imagenet-E, and Imagenet-X. Further LCM demonstrate seamless integration with person re-identification and improved the performance. Source code shall be released soon.</li>
</ul>

<h3>Title: Gradient Boosting Decision Trees on Medical Diagnosis over Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>A. Yarkın Yıldız, Asli Kalayci</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03705">https://arxiv.org/abs/2410.03705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03705">https://arxiv.org/pdf/2410.03705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03705]] Gradient Boosting Decision Trees on Medical Diagnosis over Tabular Data(https://arxiv.org/abs/2410.03705)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Medical diagnosis is a crucial task in the medical field, in terms of providing accurate classification and respective treatments. Having near-precise decisions based on correct diagnosis can affect a patient's life itself, and may extremely result in a catastrophe if not classified correctly. Several traditional machine learning (ML), such as support vector machines (SVMs) and logistic regression, and state-of-the-art tabular deep learning (DL) methods, including TabNet and TabTransformer, have been proposed and used over tabular medical datasets. Additionally, due to the superior performances, lower computational costs, and easier optimization over different tasks, ensemble methods have been used in the field more recently. They offer a powerful alternative in terms of providing successful medical decision-making processes in several diagnosis tasks. In this study, we investigated the benefits of ensemble methods, especially the Gradient Boosting Decision Tree (GBDT) algorithms in medical classification tasks over tabular data, focusing on XGBoost, CatBoost, and LightGBM. The experiments demonstrate that GBDT methods outperform traditional ML and deep neural network architectures and have the highest average rank over several benchmark tabular medical diagnosis datasets. Furthermore, they require much less computational power compared to DL models, creating the optimal methodology in terms of high performance and lower complexity.</li>
</ul>

<h3>Title: Performance Evaluation of Tokenizers in Large Language Models for the Assamese Language</h3>
<ul>
<li><strong>Authors: </strong>Sagar Tamang, Dibya Jyoti Bora</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03718">https://arxiv.org/abs/2410.03718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03718">https://arxiv.org/pdf/2410.03718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03718]] Performance Evaluation of Tokenizers in Large Language Models for the Assamese Language(https://arxiv.org/abs/2410.03718)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training of a tokenizer plays an important role in the performance of deep learning models. This research aims to understand the performance of tokenizers in five state-of-the-art (SOTA) large language models (LLMs) in the Assamese language of India. The research is important to understand the multi-lingual support for a low-resourced language such as Assamese. Our research reveals that the tokenizer of SUTRA from Two AI performs the best with an average Normalized Sequence Length (NSL) value of 0.45, closely followed by the tokenizer of GPT-4o from Open AI with an average NSL value of 0.54, followed by Gemma 2, Meta Llama 3.1, and Mistral Large Instruct 2407 with an average NSL value of 0.82, 1.4, and 1.48 respectively.</li>
</ul>

<h3>Title: FluentEditor+: Text-based Speech Editing by Modeling Local Hierarchical Acoustic Smoothness and Global Prosody Consistency</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Jiatian Xi, Ziyue Jiang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03719">https://arxiv.org/abs/2410.03719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03719">https://arxiv.org/pdf/2410.03719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03719]] FluentEditor+: Text-based Speech Editing by Modeling Local Hierarchical Acoustic Smoothness and Global Prosody Consistency(https://arxiv.org/abs/2410.03719)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Text-based speech editing (TSE) allows users to modify speech by editing the corresponding text and performing operations such as cutting, copying, and pasting to generate updated audio without altering the original recording directly. Text-based speech editing (TSE) allows users to modify speech by editing the corresponding text and performing operations such as cutting, copying, and pasting to generate updated audio without altering the original recording directly. While current TSE techniques focus on minimizing discrepancies between generated speech and reference targets within edited segments, they often neglect the importance of maintaining both local and global fluency in the context of the original discourse. Additionally, seamlessly integrating edited segments with unaltered portions of the audio remains challenging, typically requiring support from text-to-speech (TTS) systems. This paper introduces a novel approach, FluentEditor$\tiny +$, designed to overcome these limitations. FluentEditor$\tiny +$ employs advanced feature extraction techniques to capture both acoustic and prosodic characteristics, ensuring fluent transitions between edited and unedited regions. The model ensures segmental acoustic smoothness and global prosody consistency, allowing seamless splicing of speech while preserving the coherence and naturalness of the output. Extensive experiments on the VCTK and LibriTTS datasets show that FluentEditor$\tiny +$ surpasses existing TTS-based methods, including Editspeech, Campnet, $A^3T$ FluentSpeech, and Fluenteditor, in both fluency and prosody. Ablation studies further highlight the contributions of each module to the overall effectiveness of the system.</li>
</ul>

<h3>Title: Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development</h3>
<ul>
<li><strong>Authors: </strong>Andrew Katz, Gabriella Coloyan Fleming, Joyce Main</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03721">https://arxiv.org/abs/2410.03721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03721">https://arxiv.org/pdf/2410.03721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03721]] Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development(https://arxiv.org/abs/2410.03721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper aims to answer one central question: to what extent can open-source generative text models be used in a workflow to approximate thematic analysis in social science research? To answer this question, we present the Generative AI-enabled Theme Organization and Structuring (GATOS) workflow, which uses open-source machine learning techniques, natural language processing tools, and generative text models to facilitate thematic analysis. To establish validity of the method, we present three case studies applying the GATOS workflow, leveraging these models and techniques to inductively create codebooks similar to traditional procedures using thematic analysis. Specifically, we investigate the extent to which a workflow comprising open-source models and tools can inductively produce codebooks that approach the known space of themes and sub-themes. To address the challenge of gleaning insights from these texts, we combine open-source generative text models, retrieval-augmented generation, and prompt engineering to identify codes and themes in large volumes of text, i.e., generate a qualitative codebook. The process mimics an inductive coding process that researchers might use in traditional thematic analysis by reading text one unit of analysis at a time, considering existing codes already in the codebook, and then deciding whether or not to generate a new code based on whether the extant codebook provides adequate thematic coverage. We demonstrate this workflow using three synthetic datasets from hypothetical organizational research settings: a study of teammate feedback in teamwork settings, a study of organizational cultures of ethical behavior, and a study of employee perspectives about returning to their offices after the pandemic. We show that the GATOS workflow is able to identify themes in the text that were used to generate the original synthetic datasets.</li>
</ul>

<h3>Title: Neurosymbolic AI approach to Attribution in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Deepa Tilwani, Revathy Venkataramanan, Amit P. Sheth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03726">https://arxiv.org/abs/2410.03726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03726">https://arxiv.org/pdf/2410.03726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03726]] Neurosymbolic AI approach to Attribution in Large Language Models(https://arxiv.org/abs/2410.03726)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Attribution in large language models (LLMs) remains a significant challenge, particularly in ensuring the factual accuracy and reliability of the generated outputs. Current methods for citation or attribution, such as those employed by tools like this http URL and Bing Search-integrated LLMs, attempt to ground responses by providing real-time search results and citations. However, so far, these approaches suffer from issues such as hallucinations, biases, surface-level relevance matching, and the complexity of managing vast, unfiltered knowledge sources. While tools like this http URL dynamically integrate web-based information and citations, they often rely on inconsistent sources such as blog posts or unreliable sources, which limits their overall reliability. We present that these challenges can be mitigated by integrating Neurosymbolic AI (NesyAI), which combines the strengths of neural networks with structured symbolic reasoning. NesyAI offers transparent, interpretable, and dynamic reasoning processes, addressing the limitations of current attribution methods by incorporating structured symbolic knowledge with flexible, neural-based learning. This paper explores how NesyAI frameworks can enhance existing attribution models, offering more reliable, interpretable, and adaptable systems for LLMs.</li>
</ul>

<h3>Title: FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"</h3>
<ul>
<li><strong>Authors: </strong>Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03727">https://arxiv.org/abs/2410.03727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03727">https://arxiv.org/pdf/2410.03727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03727]] FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"(https://arxiv.org/abs/2410.03727)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved this http URL is available at: \url{this https URL}.</li>
</ul>

<h3>Title: Unsupervised Human Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Dilek Hakkani Tur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03731">https://arxiv.org/abs/2410.03731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03731">https://arxiv.org/pdf/2410.03731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03731]] Unsupervised Human Preference Learning(https://arxiv.org/abs/2410.03731)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local "steering wheel" model that directs the outputs of a much larger foundation model, producing content tailored to an individual's preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data and compute-efficient manner, our approach paves the way for highly personalized language model applications.</li>
</ul>

<h3>Title: ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fillipe dos Santos Silva, Gabriel Kenzo Kakimoto, Julio Cesar dos Reis, Marcelo S. Reis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03738">https://arxiv.org/abs/2410.03738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03738">https://arxiv.org/pdf/2410.03738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03738]] ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation(https://arxiv.org/abs/2410.03738)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Cluster analysis plays a crucial role in various domains and applications, such as customer segmentation in marketing. These contexts often involve multimodal data, including both tabular and textual datasets, making it challenging to represent hidden patterns for obtaining meaningful clusters. This study introduces ERASMO, a framework designed to fine-tune a pretrained language model on textually encoded tabular data and generate embeddings from the fine-tuned model. ERASMO employs a textual converter to transform tabular data into a textual format, enabling the language model to process and understand the data more effectively. Additionally, ERASMO produces contextually rich and structurally representative embeddings through techniques such as random feature sequence shuffling and number verbalization. Extensive experimental evaluations were conducted using multiple datasets and baseline approaches. Our results demonstrate that ERASMO fully leverages the specific context of each tabular dataset, leading to more precise and nuanced embeddings for accurate clustering. This approach enhances clustering performance by capturing complex relationship patterns within diverse tabular data.</li>
</ul>

<h3>Title: Language Enhanced Model for Eye (LEME): An Open-Source Ophthalmology-Specific Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Aidan Gilson, Xuguang Ai, Qianqian Xie, Sahana Srinivasan, Krithi Pushpanathan, Maxwell B. Singer, Jimin Huang, Hyunjae Kim, Erping Long, Peixing Wan, Luciano V. Del Priore, Lucila Ohno-Machado, Hua Xu, Dianbo Liu, Ron A. Adelman, Yih-Chung Tham, Qingyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03740">https://arxiv.org/abs/2410.03740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03740">https://arxiv.org/pdf/2410.03740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03740]] Language Enhanced Model for Eye (LEME): An Open-Source Ophthalmology-Specific Large Language Model(https://arxiv.org/abs/2410.03740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are poised to revolutionize healthcare. Ophthalmology-specific LLMs remain scarce and underexplored. We introduced an open-source, specialized LLM for ophthalmology, termed Language Enhanced Model for Eye (LEME). LEME was initially pre-trained on the Llama2 70B framework and further fine-tuned with a corpus of ~127,000 non-copyrighted training instances curated from ophthalmology-specific case reports, abstracts, and open-source study materials. We benchmarked LEME against eight other LLMs, namely, GPT-3.5, GPT-4, three Llama2 models (7B, 13B, 70B), PMC-LLAMA 13B, Meditron 70B, and EYE-Llama (another ophthalmology-specific LLM). Evaluations included four internal validation tasks: abstract completion, fill-in-the-blank, multiple-choice questions (MCQ), and short-answer QA. External validation tasks encompassed long-form QA, MCQ, patient EHR summarization, and clinical QA. Evaluation metrics included Rouge-L scores, accuracy, and expert evaluation of correctness, completeness, and readability. In internal validations, LEME consistently outperformed its counterparts, achieving Rouge-L scores of 0.20 in abstract completion (all p<0.05), 0.82 in fill-in-the-blank (all p<0.0001), and 0.22 in short-answer QA (all p<0.0001, except versus GPT-4). In external validations, LEME excelled in long-form QA with a Rouge-L of 0.19 (all p<0.0001), ranked second in MCQ accuracy (0.68; all p<0.0001), and scored highest in EHR summarization and clinical QA (ranging from 4.24 to 4.83 out of 5 for correctness, completeness, and readability). LEME's emphasis on robust fine-tuning and the use of non-copyrighted data represents a breakthrough in open-source ophthalmology-specific LLMs, offering the potential to revolutionize execution of clinical tasks while democratizing research collaboration.</li>
</ul>

<h3>Title: Beyond Scalar Reward Model: Learning Generative Judge from Preference Data</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03742">https://arxiv.org/abs/2410.03742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03742">https://arxiv.org/pdf/2410.03742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03742]] Beyond Scalar Reward Model: Learning Generative Judge from Preference Data(https://arxiv.org/abs/2410.03742)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Learning from preference feedback is a common practice for aligning large language models~(LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference or reward. However, scalar models lack interpretability and are known to be susceptible to biases in datasets. This paper investigates leveraging the generation capability of LLMs to address both limitations in one shot. Specifically, we prompt the pre-trained LLM to generate positive and negative judgments, both supported with rationales in natural language form. The self-generated contrastive judgment pairs are used to train the generative judge with Direct Preference Optimization (DPO). This proposal of training the generative Judge using self-generated Contrastive judgments (Con-J) ensures natural interpretability due to the generated rationales together with the judgments, as well as high robustness against bias without the need for an additional reward head. Experimental results show that the performance of Con-J is comparable to the scalar reward model trained on the same collection of preference data, and demonstrate its superior interpretability and robustness in encoding human preferences.</li>
</ul>

<h3>Title: Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging</h3>
<ul>
<li><strong>Authors: </strong>Yiming Ju, Ziyi Ni, Xingrun Xing, Zhixiong Zeng, hanyu Zhao, Siqi Fan, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03743">https://arxiv.org/abs/2410.03743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03743">https://arxiv.org/pdf/2410.03743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03743]] Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging(https://arxiv.org/abs/2410.03743)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is crucial for adapting Large Language Models (LLMs) to specific tasks. In this work, we demonstrate that the order of training data can lead to significant training imbalances, potentially resulting in performance degradation. Consequently, we propose to mitigate this imbalance by merging SFT models fine-tuned with different data orders, thereby enhancing the overall effectiveness of SFT. Additionally, we introduce a novel technique, "parameter-selection merging," which outperforms traditional weighted-average methods on five datasets. Further, through analysis and ablation studies, we validate the effectiveness of our method and identify the sources of performance improvements.</li>
</ul>

<h3>Title: Khattat: Enhancing Readability and Concept Representation of Semantic Typography</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Hussein, Alaa Elsetohy, Sama Hadhoud, Tameem Bakr, Yasser Rohaim, Badr AlKhamissi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03748">https://arxiv.org/abs/2410.03748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03748">https://arxiv.org/pdf/2410.03748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03748]] Khattat: Enhancing Readability and Concept Representation of Semantic Typography(https://arxiv.org/abs/2410.03748)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Designing expressive typography that visually conveys a word's meaning while maintaining readability is a complex task, known as semantic typography. It involves selecting an idea, choosing an appropriate font, and balancing creativity with legibility. We introduce an end-to-end system that automates this process. First, a Large Language Model (LLM) generates imagery ideas for the word, useful for abstract concepts like freedom. Then, the FontCLIP pre-trained model automatically selects a suitable font based on its semantic understanding of font attributes. The system identifies optimal regions of the word for morphing and iteratively transforms them using a pre-trained diffusion model. A key feature is our OCR-based loss function, which enhances readability and enables simultaneous stylization of multiple characters. We compare our method with other baselines, demonstrating great readability enhancement and versatility across multiple languages and writing scripts.</li>
</ul>

<h3>Title: SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Juan Pablo Muñoz, Jinjie Yuan, Nilesh Jain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03750">https://arxiv.org/abs/2410.03750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03750">https://arxiv.org/pdf/2410.03750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03750]] SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models(https://arxiv.org/abs/2410.03750)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large pre-trained models (LPMs), such as large language models, have become ubiquitous and are employed in many applications. These models are often adapted to a desired domain or downstream task through a fine-tuning stage. This paper proposes SQFT, an end-to-end solution for low-precision sparse parameter-efficient fine-tuning of LPMs, allowing for effective model manipulation in resource-constrained environments. Additionally, an innovative strategy enables the merging of sparse weights with low-rank adapters without losing sparsity and accuracy, overcoming the limitations of previous approaches. SQFT also addresses the challenge of having quantized weights and adapters with different numerical precisions, enabling merging in the desired numerical format without sacrificing accuracy. Multiple adaptation scenarios, models, and comprehensive sparsity levels demonstrate the effectiveness of SQFT. Models and code are available at this https URL.</li>
</ul>

<h3>Title: Recent Advances in Speech Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03751">https://arxiv.org/abs/2410.03751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03751">https://arxiv.org/pdf/2410.03751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03751]] Recent Advances in Speech Language Models: A Survey(https://arxiv.org/abs/2410.03751)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize the evaluation metrics for SpeechLMs, and discuss the challenges and future research directions in this rapidly evolving field.</li>
</ul>

<h3>Title: Denoising with a Joint-Embedding Predictive Architecture</h3>
<ul>
<li><strong>Authors: </strong>Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03755">https://arxiv.org/abs/2410.03755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03755">https://arxiv.org/pdf/2410.03755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03755]] Denoising with a Joint-Embedding Predictive Architecture(https://arxiv.org/abs/2410.03755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Joint-embedding predictive architectures (JEPAs) have shown substantial promise in self-supervised representation learning, yet their application in generative modeling remains underexplored. Conversely, diffusion models have demonstrated significant efficacy in modeling arbitrary probability distributions. In this paper, we introduce Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), pioneering the integration of JEPA within generative modeling. By recognizing JEPA as a form of masked image modeling, we reinterpret it as a generalized next-token prediction strategy, facilitating data generation in an auto-regressive manner. Furthermore, we incorporate diffusion loss to model the per-token probability distribution, enabling data generation in a continuous space. We also adapt flow matching loss as an alternative to diffusion loss, thereby enhancing the flexibility of D-JEPA. Empirically, with increased GFLOPs, D-JEPA consistently achieves lower FID scores with fewer training epochs, indicating its good scalability. Our base, large, and huge models outperform all previous generative models across all scales on class-conditional ImageNet benchmarks. Beyond image generation, D-JEPA is well-suited for other continuous data modeling, including video and audio.</li>
</ul>

<h3>Title: HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03761">https://arxiv.org/abs/2410.03761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03761">https://arxiv.org/pdf/2410.03761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03761]] HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation(https://arxiv.org/abs/2410.03761)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we present HiReview, a novel framework for hierarchical taxonomy-driven automatic literature review generation. With the exponential growth of academic documents, manual literature reviews have become increasingly labor-intensive and time-consuming, while traditional summarization models struggle to generate comprehensive document reviews effectively. Large language models (LLMs), with their powerful text processing capabilities, offer a potential solution; however, research on incorporating LLMs for automatic document generation remains limited. To address key challenges in large-scale automatic literature review generation (LRG), we propose a two-stage taxonomy-then-generation approach that combines graph-based hierarchical clustering with retrieval-augmented LLMs. First, we retrieve the most relevant sub-community within the citation network, then generate a hierarchical taxonomy tree by clustering papers based on both textual content and citation relationships. In the second stage, an LLM generates coherent and contextually accurate summaries for clusters or topics at each hierarchical level, ensuring comprehensive coverage and logical organization of the literature. Extensive experiments demonstrate that HiReview significantly outperforms state-of-the-art methods, achieving superior hierarchical organization, content relevance, and factual accuracy in automatic literature review generation tasks.</li>
</ul>

<h3>Title: Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression</h3>
<ul>
<li><strong>Authors: </strong>Jingcun Wang, Yu-Guang Chen, Ing-Chao Lin, Bing Li, Grace Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03765">https://arxiv.org/abs/2410.03765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03765">https://arxiv.org/pdf/2410.03765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03765]] Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression(https://arxiv.org/abs/2410.03765)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable breakthroughs. However, the huge number of parameters in LLMs require significant amount of memory storage in inference, which prevents their practical deployment in many applications. To reduce memory storage of LLMs, singular value decomposition (SVD) provides a promising solution to approximate weight matrices for compressing LLMs. In this paper, we take a step further to explore parameter sharing across different layers with SVD to achieve more effective compression for LLMs. Specifically, weight matrices in different layers are decomposed and represented as a linear combination of a set of shared basis vectors and unique coefficients. The types of weight matrices and the layer selection for basis sharing are examined when compressing LLMs to maintain the performance. Comprehensive experiments demonstrate that Basis Sharing outperforms state-of-the-art SVD-based compression approaches and parameter sharing techniques, especially under large compression ratios. Code is available at: this https URL</li>
</ul>

<h3>Title: Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yohan Mathew, Ollie Matthews, Robert McCarthy, Joan Velja, Christian Schroeder de Witt, Dylan Cope, Nandi Schoots</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03768">https://arxiv.org/abs/2410.03768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03768">https://arxiv.org/pdf/2410.03768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03768]] Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs(https://arxiv.org/abs/2410.03768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render collusion practically undetectable. This underscores the need for evaluation frameworks to monitor and mitigate steganographic collusion capabilities. We address a crucial gap in the literature by demonstrating, for the first time, that robust steganographic collusion in LLMs can arise indirectly from optimization pressure. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography. Importantly, we find that emergent steganographic collusion can be robust to both passive steganalytic oversight of model outputs and active mitigation through communication paraphrasing. We contribute a novel model evaluation framework and discuss limitations and future work. Our findings imply that effective risk mitigation from steganographic collusion post-deployment requires innovation in passive and active oversight techniques.</li>
</ul>

<h3>Title: SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Li, Jingyu Lu, Chuangxin Chu, Tianyu Zeng, Yujia Zheng, Mei Li, Haotian Huang, Bin Wu, Zuoxian Liu, Kai Ma, Xuejing Yuan, Xingkai Wang, Keyan Ding, Huajun Chen, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03769">https://arxiv.org/abs/2410.03769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03769">https://arxiv.org/pdf/2410.03769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03769]] SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks(https://arxiv.org/abs/2410.03769)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have had a transformative impact on a variety of scientific tasks across disciplines such as biology, chemistry, medicine, and physics. However, ensuring the safety alignment of these models in scientific research remains an underexplored area, with existing benchmarks primarily focus on textual content and overlooking key scientific representations such as molecular, protein, and genomic languages. Moreover, the safety mechanisms of LLMs in scientific tasks are insufficiently studied. To address these limitations, we introduce SciSafeEval, a comprehensive benchmark designed to evaluate the safety alignment of LLMs across a range of scientific tasks. SciSafeEval spans multiple scientific languages - including textual, molecular, protein, and genomic - and covers a wide range of scientific domains. We evaluate LLMs in zero-shot, few-shot and chain-of-thought settings, and introduce a 'jailbreak' enhancement feature that challenges LLMs equipped with safety guardrails, rigorously testing their defenses against malicious intention. Our benchmark surpasses existing safety datasets in both scale and scope, providing a robust platform for assessing the safety and performance of LLMs in scientific contexts. This work aims to facilitate the responsible development and deployment of LLMs, promoting alignment with safety and ethical standards in scientific research.</li>
</ul>

<h3>Title: A Two-Stage Proactive Dialogue Generator for Efficient Clinical Information Collection Using Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xueshen Li, Xinlong Hou, Nirupama Ravi, Ziyi Huang, Yu Gan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03770">https://arxiv.org/abs/2410.03770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03770">https://arxiv.org/pdf/2410.03770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03770]] A Two-Stage Proactive Dialogue Generator for Efficient Clinical Information Collection Using Large Language Model(https://arxiv.org/abs/2410.03770)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient patient-doctor interaction is among the key factors for a successful disease diagnosis. During the conversation, the doctor could query complementary diagnostic information, such as the patient's symptoms, previous surgery, and other related information that goes beyond medical evidence data (test results) to enhance disease diagnosis. However, this procedure is usually time-consuming and less-efficient, which can be potentially optimized through computer-assisted systems. As such, we propose a diagnostic dialogue system to automate the patient information collection procedure. By exploiting medical history and conversation logic, our conversation agents, particularly the doctor agent, can pose multi-round clinical queries to effectively collect the most relevant disease diagnostic information. Moreover, benefiting from our two-stage recommendation structure, carefully designed ranking criteria, and interactive patient agent, our model is able to overcome the under-exploration and non-flexible challenges in dialogue generation. Our experimental results on a real-world medical conversation dataset show that our model can generate clinical queries that mimic the conversation style of real doctors, with efficient fluency, professionalism, and safety, while effectively collecting relevant disease diagnostic information.</li>
</ul>

<h3>Title: Precision Knowledge Editing: Enhancing Safety in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03772">https://arxiv.org/abs/2410.03772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03772">https://arxiv.org/pdf/2410.03772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03772]] Precision Knowledge Editing: Enhancing Safety in Large Language Models(https://arxiv.org/abs/2410.03772)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities, but they also pose risks related to the generation of toxic or harmful content. This work introduces Precision Knowledge Editing (PKE), an advanced technique that builds upon existing knowledge editing methods to more effectively identify and modify toxic parameter regions within LLMs. By leveraging neuron weight tracking and activation pathway tracing, PKE achieves finer granularity in toxic content management compared to previous methods like Detoxifying Instance Neuron Modification (DINM). Our experiments demonstrate that PKE significantly reduces the attack success rate (ASR) across various models, including Llama2-7b and Llama-3-8b-instruct, while maintaining overall model performance. Additionally, we also compared the performance of some closed-source models (gpt-4-0613 and Claude 3 Sonnet) in our experiments, and found that models adjusted using our method far outperformed the closed-source models in terms of safety. This research contributes to the ongoing efforts to make LLMs safer and more reliable for real-world applications.</li>
</ul>

<h3>Title: Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Bálint Csanády, Lóránt Nagy, Dániel Boros, Iván Ivkovic, Dávid Kovács, Dalma Tóth-Lakits, László Márkus, András Lukács</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03776">https://arxiv.org/abs/2410.03776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03776">https://arxiv.org/pdf/2410.03776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03776]] Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks(https://arxiv.org/abs/2410.03776)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a purely deep neural network-based approach for estimating long memory parameters of time series models that incorporate the phenomenon of long-range dependence. Parameters, such as the Hurst exponent, are critical in characterizing the long-range dependence, roughness, and self-similarity of stochastic processes. The accurate and fast estimation of these parameters holds significant importance across various scientific disciplines, including finance, physics, and engineering. We harnessed efficient process generators to provide high-quality synthetic training data, enabling the training of scale-invariant 1D Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) models. Our neural models outperform conventional statistical methods, even those augmented with neural networks. The precision, speed, consistency, and robustness of our estimators are demonstrated through experiments involving fractional Brownian motion (fBm), the Autoregressive Fractionally Integrated Moving Average (ARFIMA) process, and the fractional Ornstein-Uhlenbeck (fOU) process. We believe that our work will inspire further research in the field of stochastic process modeling and parameter estimation using deep learning techniques.</li>
</ul>

<h3>Title: Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Yao, Han Wu, Mingyang Liu, Sichun Luo, Xiongwei Han, Jie Liu, Zhijiang Guo, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03777">https://arxiv.org/abs/2410.03777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03777">https://arxiv.org/pdf/2410.03777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03777]] Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling(https://arxiv.org/abs/2410.03777)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \textsc{Uni}on \textsc{T}op-$k$ \textsc{E}nsembling (\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling.</li>
</ul>

<h3>Title: SGW-based Multi-Task Learning in Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Ruiyuan Zhang, Yuyao Chen, Yuchi Huo, Jiaxiang Liu, Dianbing Xi, Jie Liu, Chao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03778">https://arxiv.org/abs/2410.03778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03778">https://arxiv.org/pdf/2410.03778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03778]] SGW-based Multi-Task Learning in Vision Tasks(https://arxiv.org/abs/2410.03778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Multi-task-learning(MTL) is a multi-target optimization task. Neural networks try to realize each target using a shared interpretative space within MTL. However, as the scale of datasets expands and the complexity of tasks increases, knowledge sharing becomes increasingly challenging. In this paper, we first re-examine previous cross-attention MTL methods from the perspective of noise. We theoretically analyze this issue and identify it as a flaw in the cross-attention mechanism. To address this issue, we propose an information bottleneck knowledge extraction module (KEM). This module aims to reduce inter-task interference by constraining the flow of information, thereby reducing computational complexity. Furthermore, we have employed neural collapse to stabilize the knowledge-selection process. That is, before input to KEM, we projected the features into ETF space. This mapping makes our method more robust. We implemented and conducted comparative experiments with this method on multiple datasets. The results demonstrate that our approach significantly outperforms existing methods in multi-task learning.</li>
</ul>

<h3>Title: DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, Dongyoon Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03782">https://arxiv.org/abs/2410.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03782">https://arxiv.org/pdf/2410.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03782]] DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation(https://arxiv.org/abs/2410.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adapting a pre-trained foundation model on downstream tasks should ensure robustness against distribution shifts without the need to retrain the whole model. Although existing weight interpolation methods are simple yet effective, we argue their static nature limits downstream performance while achieving efficiency. In this work, we propose DaWin, a training-free dynamic weight interpolation method that leverages the entropy of individual models over each unlabeled test sample to assess model expertise, and compute per-sample interpolation coefficients dynamically. Unlike previous works that typically rely on additional training to learn such coefficients, our approach requires no training. Then, we propose a mixture modeling approach that greatly reduces inference overhead raised by dynamic interpolation. We validate DaWin on the large-scale visual recognition benchmarks, spanning 14 tasks across robust fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and multi-task learning with eight classification tasks. Results demonstrate that DaWin achieves significant performance gain in considered settings, with minimal computational overhead. We further discuss DaWin's analytic behavior to explain its empirical success.</li>
</ul>

<h3>Title: Improving Neural Optimal Transport via Displacement Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Jaemoo Choi, Yongxin Chen, Jaewoong Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03783">https://arxiv.org/abs/2410.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03783">https://arxiv.org/pdf/2410.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03783]] Improving Neural Optimal Transport via Displacement Interpolation(https://arxiv.org/abs/2410.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimal Transport (OT) theory investigates the cost-minimizing transport map that moves a source distribution to a target distribution. Recently, several approaches have emerged for learning the optimal transport map for a given cost function using neural networks. We refer to these approaches as the OT Map. OT Map provides a powerful tool for diverse machine learning tasks, such as generative modeling and unpaired image-to-image translation. However, existing methods that utilize max-min optimization often experience training instability and sensitivity to hyperparameters. In this paper, we propose a novel method to improve stability and achieve a better approximation of the OT Map by exploiting displacement interpolation, dubbed Displacement Interpolation Optimal Transport Model (DIOTM). We derive the dual formulation of displacement interpolation at specific time $t$ and prove how these dual problems are related across time. This result allows us to utilize the entire trajectory of displacement interpolation in learning the OT Map. Our method improves the training stability and achieves superior results in estimating optimal transport maps. We demonstrate that DIOTM outperforms existing OT-based models on image-to-image translation tasks.</li>
</ul>

<h3>Title: Accelerating Deep Learning with Fixed Time Budget</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Asif Khan, Ridha Hamila, Hamid Menouar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03790">https://arxiv.org/abs/2410.03790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03790">https://arxiv.org/pdf/2410.03790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03790]] Accelerating Deep Learning with Fixed Time Budget(https://arxiv.org/abs/2410.03790)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The success of modern deep learning is attributed to two key elements: huge amounts of training data and large model sizes. Where a vast amount of data allows the model to learn more features, the large model architecture boosts the learning capability of the model. However, both these factors result in prolonged training time. In some practical applications such as edge-based learning and federated learning, limited-time budgets necessitate more efficient training methods. This paper proposes an effective technique for training arbitrary deep learning models within fixed time constraints utilizing sample importance and dynamic ranking. The proposed method is extensively evaluated in both classification and regression tasks in computer vision. The results consistently show clear gains achieved by the proposed method in improving the learning performance of various state-of-the-art deep learning models in both regression and classification tasks.</li>
</ul>

<h3>Title: Searching for Best Practices in Medical Transcription with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jiafeng Li, Yanda Mu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03797">https://arxiv.org/abs/2410.03797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03797">https://arxiv.org/pdf/2410.03797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03797]] Searching for Best Practices in Medical Transcription with Large Language Model(https://arxiv.org/abs/2410.03797)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The transcription of medical monologues, especially those containing a high density of specialized terminology and delivered with a distinct accent, presents a significant challenge for existing automated systems. This paper introduces a novel approach leveraging a Large Language Model (LLM) to generate highly accurate medical transcripts from audio recordings of doctors' monologues, specifically focusing on Indian accents. Our methodology integrates advanced language modeling techniques to lower the Word Error Rate (WER) and ensure the precise recognition of critical medical terms. Through rigorous testing on a comprehensive dataset of medical recordings, our approach demonstrates substantial improvements in both overall transcription accuracy and the fidelity of key medical terminologies. These results suggest that our proposed system could significantly aid in clinical documentation processes, offering a reliable tool for healthcare providers to streamline their transcription needs while maintaining high standards of accuracy.</li>
</ul>

<h3>Title: Self-Powered LLM Modality Expansion for Large Speech-Text Models</h3>
<ul>
<li><strong>Authors: </strong>Tengfei Yu, Xuebo Liu, Zhiyi Hou, Liang Ding, Dacheng Tao, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03798">https://arxiv.org/abs/2410.03798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03798">https://arxiv.org/pdf/2410.03798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03798]] Self-Powered LLM Modality Expansion for Large Speech-Text Models(https://arxiv.org/abs/2410.03798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable performance across diverse tasks, indicating their potential for expansion into large speech-text models (LSMs) by integrating speech capabilities. Although unified speech-text pre-training and multimodal data instruction-tuning offer considerable benefits, these methods generally entail significant resource demands and tend to overfit specific tasks. This study aims to refine the use of speech datasets for LSM training by addressing the limitations of vanilla instruction tuning. We explore the instruction-following dynamics within LSMs, identifying a critical issue termed speech anchor bias-a tendency for LSMs to over-rely on speech inputs, mistakenly interpreting the entire speech modality as directives, thereby neglecting textual instructions. To counteract this bias, we introduce a self-powered LSM that leverages augmented automatic speech recognition data generated by the model itself for more effective instruction tuning. Our experiments across a range of speech-based tasks demonstrate that self-powered LSM mitigates speech anchor bias and improves the fusion of speech and text modalities in LSMs. Data, code and scripts are freely available at this https URL.</li>
</ul>

<h3>Title: Text-guided Diffusion Model for 3D Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanchen Luo, Junfeng Fang, Sihang Li, Zhiyuan Liu, Jiancan Wu, An Zhang, Wenjie Du, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03803">https://arxiv.org/abs/2410.03803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03803">https://arxiv.org/pdf/2410.03803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03803]] Text-guided Diffusion Model for 3D Molecule Generation(https://arxiv.org/abs/2410.03803)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The de novo generation of molecules with targeted properties is crucial in biology, chemistry, and drug discovery. Current generative models are limited to using single property values as conditions, struggling with complex customizations described in detailed human language. To address this, we propose the text guidance instead, and introduce TextSMOG, a new Text-guided Small Molecule Generation Approach via 3D Diffusion Model which integrates language and diffusion models for text-guided small molecule generation. This method uses textual conditions to guide molecule generation, enhancing both stability and diversity. Experimental results show TextSMOG's proficiency in capturing and utilizing information from textual descriptions, making it a powerful tool for generating 3D molecular structures in response to complex textual customizations.</li>
</ul>

<h3>Title: Mixture of Attentions For Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Zimmer, Milan Gritta, Gerasimos Lampouras, Haitham Bou Ammar, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03804">https://arxiv.org/abs/2410.03804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03804">https://arxiv.org/pdf/2410.03804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03804]] Mixture of Attentions For Speculative Decoding(https://arxiv.org/abs/2410.03804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy. Speculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel. Small models that utilise activations from the LLM currently achieve the fastest decoding speeds. However, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. To address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD. Our novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server. In a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%. In a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process.</li>
</ul>

<h3>Title: Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Aguilera-Martos, Andrés Herrera-Poyatos, Julián Luengo, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03805">https://arxiv.org/abs/2410.03805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03805">https://arxiv.org/pdf/2410.03805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03805]] Local Attention Mechanism: Boosting the Transformer Architecture for Long-Sequence Time Series Forecasting(https://arxiv.org/abs/2410.03805)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have become the leading choice in natural language processing over other deep learning architectures. This trend has also permeated the field of time series analysis, especially for long-horizon forecasting, showcasing promising results both in performance and running time. In this paper, we introduce Local Attention Mechanism (LAM), an efficient attention mechanism tailored for time series analysis. This mechanism exploits the continuity properties of time series to reduce the number of attention scores computed. We present an algorithm for implementing LAM in tensor algebra that runs in time and memory O(nlogn), significantly improving upon the O(n^2) time and memory complexity of traditional attention mechanisms. We also note the lack of proper datasets to evaluate long-horizon forecast models. Thus, we propose a novel set of datasets to improve the evaluation of models addressing long-horizon forecasting challenges. Our experimental analysis demonstrates that the vanilla transformer architecture magnified with LAM surpasses state-of-the-art models, including the vanilla attention mechanism. These results confirm the effectiveness of our approach and highlight a range of future challenges in long-sequence time series forecasting.</li>
</ul>

<h3>Title: Metadata Matters for Time Series: Informative Forecasting with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Li Zhang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03806">https://arxiv.org/abs/2410.03806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03806">https://arxiv.org/pdf/2410.03806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03806]] Metadata Matters for Time Series: Informative Forecasting with Transformers(https://arxiv.org/abs/2410.03806)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Time series forecasting is prevalent in extensive real-world applications, such as financial analysis and energy planning. Previous studies primarily focus on time series modality, endeavoring to capture the intricate variations and dependencies inherent in time series. Beyond numerical time series data, we notice that metadata (e.g.~dataset and variate descriptions) also carries valuable information essential for forecasting, which can be used to identify the application scenario and provide more interpretable knowledge than digit sequences. Inspired by this observation, we propose a Metadata-informed Time Series Transformer (MetaTST), which incorporates multiple levels of context-specific metadata into Transformer forecasting models to enable informative time series forecasting. To tackle the unstructured nature of metadata, MetaTST formalizes them into natural languages by pre-designed templates and leverages large language models (LLMs) to encode these texts into metadata tokens as a supplement to classic series tokens, resulting in an informative embedding. Further, a Transformer encoder is employed to communicate series and metadata tokens, which can extend series representations by metadata information for more accurate forecasting. This design also allows the model to adaptively learn context-specific patterns across various scenarios, which is particularly effective in handling large-scale, diverse-scenario forecasting tasks. Experimentally, MetaTST achieves state-of-the-art compared to advanced time series models and LLM-based methods on widely acknowledged short- and long-term forecasting benchmarks, covering both single-dataset individual and multi-dataset joint training settings.</li>
</ul>

<h3>Title: Can Mamba Always Enjoy the "Free Lunch"?</h3>
<ul>
<li><strong>Authors: </strong>Ruifeng Ren, Zhicong Li, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03810">https://arxiv.org/abs/2410.03810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03810">https://arxiv.org/pdf/2410.03810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03810]] Can Mamba Always Enjoy the "Free Lunch"?(https://arxiv.org/abs/2410.03810)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers have been the cornerstone of current Large Language Models (LLMs); however, its linear growth in overhead during inference with respect to sequence length poses challenges for modeling long sequences. In this context, Mamba has gradually attracted attention due to its constant-level size during inference and existing empirical results have shown that it can perform comparably to Transformers in sequence modeling while offering significant savings. However, one may ask that, can Mamba always enjoy the ``free lunch"? In this paper, we focus on analyzing the expressive ability of Mamba from a theoretical standpoint. First, inspired by the connection between Mamba and linear attention, we investigate potential shortcomings of the Mamba when performing the COPY operation. Our results indicate that Mamba with constant size may encounter bottlenecks when handling COPY, while it can achieve perfect performance when the size scales linearly with sequence length. Based on this observation, we analyze Mamba's ability to tackle DP problems when equipped with Chain of Thought (CoT). Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is comparable to standard and efficient Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our results contribute to a deeper understanding of Mamba.</li>
</ul>

<h3>Title: EvenNICER-SLAM: Event-based Neural Implicit Encoding SLAM</h3>
<ul>
<li><strong>Authors: </strong>Shi Chen, Danda Pani Paudel, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03812">https://arxiv.org/abs/2410.03812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03812">https://arxiv.org/pdf/2410.03812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03812]] EvenNICER-SLAM: Event-based Neural Implicit Encoding SLAM(https://arxiv.org/abs/2410.03812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The advancement of dense visual simultaneous localization and mapping (SLAM) has been greatly facilitated by the emergence of neural implicit representations. Neural implicit encoding SLAM, a typical example of which is NICE-SLAM, has recently demonstrated promising results in large-scale indoor scenes. However, these methods typically rely on temporally dense RGB-D image streams as input in order to function properly. When the input source does not support high frame rates or the camera movement is too fast, these methods often experience crashes or significant degradation in tracking and mapping accuracy. In this paper, we propose EvenNICER-SLAM, a novel approach that addresses this issue through the incorporation of event cameras. Event cameras are bio-inspired cameras that respond to intensity changes instead of absolute brightness. Specifically, we integrated an event loss backpropagation stream into the NICE-SLAM pipeline to enhance camera tracking with insufficient RGB-D input. We found through quantitative evaluation that EvenNICER-SLAM, with an inclusion of higher-frequency event image input, significantly outperforms NICE-SLAM with reduced RGB-D input frequency. Our results suggest the potential for event cameras to improve the robustness of dense SLAM systems against fast camera motion in real-world scenarios.</li>
</ul>

<h3>Title: Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in SAR Imaging Based on MSTAR Data</h3>
<ul>
<li><strong>Authors: </strong>Shahrokh Hamidi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03816">https://arxiv.org/abs/2410.03816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03816">https://arxiv.org/pdf/2410.03816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03816]] Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in SAR Imaging Based on MSTAR Data(https://arxiv.org/abs/2410.03816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The statistical analysis of land clutter for Synthetic Aperture Radar (SAR) imaging has become an increasingly important subject for research and investigation. It is also absolutely necessary for designing robust algorithms capable of performing the task of target detection in the background clutter. Any attempt to extract the energy of the desired targets from the land clutter requires complete knowledge of the statistical properties of the background clutter. In this paper, the spatial as well as the temporal characteristics of the land clutter are studied. Since the data for each image has been collected based on a different aspect angle; therefore, the temporal analysis contains variation in the aspect angle. Consequently, the temporal analysis includes the characteristics of the radar cross section with respect to the aspect angle based on which the data has been collected. In order to perform the statistical analysis, several well-known and relevant distributions, namely, Weibull, Log-normal, Gamma, and Rayleigh are considered as prime candidates to model the land clutter. The goodness-of-fit test is based on the Kullback-Leibler (KL) Divergence metric. The detailed analysis presented in this paper demonstrates that the Weibull distribution is a more accurate fit for the temporal-aspect-angle statistical analysis while the Rayleigh distribution models the spatial characteristics of the background clutter with higher accuracy. Finally, based on the aforementioned statistical analyses and by utilizing the Constant False Alarm Rate (CFAR) algorithm, we perform target detection in land clutter. The overall verification of the analysis is performed by exploiting the Moving and Stationary Target Acquisition and Recognition (MSTAR) data-set, which has been collected in spotlight mode at X-band, and the results are presented.</li>
</ul>

<h3>Title: Large Language Models can be Strong Self-Detoxifiers</h3>
<ul>
<li><strong>Authors: </strong>Ching-Yun Ko, Pin-Yu Chen, Payel Das, Youssef Mroueh, Soham Dan, Georgios Kollias, Subhajit Chaudhury, Tejaswini Pedapati, Luca Daniel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03818">https://arxiv.org/abs/2410.03818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03818">https://arxiv.org/pdf/2410.03818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03818]] Large Language Models can be Strong Self-Detoxifiers(https://arxiv.org/abs/2410.03818)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reducing the likelihood of generating harmful and toxic output is an essential task when aligning large language models (LLMs). Existing methods mainly rely on training an external reward model (i.e., another language model) or fine-tuning the LLM using self-generated data to influence the outcome. In this paper, we show that LLMs have the capability of self-detoxification without the use of an additional reward model or re-training. We propose \textit{Self-disciplined Autoregressive Sampling (SASA)}, a lightweight controlled decoding algorithm for toxicity reduction of LLMs. SASA leverages the contextual representations from an LLM to learn linear subspaces characterizing toxic v.s. non-toxic output in analytical forms. When auto-completing a response token-by-token, SASA dynamically tracks the margin of the current output to steer the generation away from the toxic subspace, by adjusting the autoregressive sampling strategy. Evaluated on LLMs of different scale and nature, namely Llama-3.1-Instruct (8B), Llama-2 (7B), and GPT2-L models with the RealToxicityPrompts, BOLD, and AttaQ benchmarks, SASA markedly enhances the quality of the generated sentences relative to the original models and attains comparable performance to state-of-the-art detoxification techniques, significantly reducing the toxicity level by only using the LLM's internal representations.</li>
</ul>

<h3>Title: MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion</h3>
<ul>
<li><strong>Authors: </strong>Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03825">https://arxiv.org/abs/2410.03825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03825">https://arxiv.org/pdf/2410.03825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03825]] MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion(https://arxiv.org/abs/2410.03825)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.</li>
</ul>

<h3>Title: Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation</h3>
<ul>
<li><strong>Authors: </strong>Chu Fei Luo, Radin Shayanfar, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03829">https://arxiv.org/abs/2410.03829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03829">https://arxiv.org/pdf/2410.03829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03829]] Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation(https://arxiv.org/abs/2410.03829)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Misinformation, defined as false or inaccurate information, can result in significant societal harm when it is spread with malicious or even innocuous intent. The rapid online information exchange necessitates advanced detection mechanisms to mitigate misinformation-induced harm. Existing research, however, has predominantly focused on assessing veracity, overlooking the legal implications and social consequences of misinformation. In this work, we take a novel angle to consolidate the definition of misinformation detection using legal issues as a measurement of societal ramifications, aiming to bring interdisciplinary efforts to tackle misinformation and its consequence. We introduce a new task: Misinformation with Legal Consequence (MisLC), which leverages definitions from a wide range of legal domains covering 4 broader legal topics and 11 fine-grained legal issues, including hate speech, election laws, and privacy regulations. For this task, we advocate a two-step dataset curation approach that utilizes crowd-sourced checkworthiness and expert evaluations of misinformation. We provide insights about the MisLC task through empirical evidence, from the problem definition to experiments and expert involvement. While the latest large language models and retrieval-augmented generation are effective baselines for the task, we find they are still far from replicating expert performance.</li>
</ul>

<h3>Title: Learning Code Preference via Synthetic Evolution</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Liu, Thanh Nguyen, Mingyue Shang, Hantian Ding, Xiaopeng Li, Yu Yu, Varun Kumar, Zijian Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03837">https://arxiv.org/abs/2410.03837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03837">https://arxiv.org/pdf/2410.03837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03837]] Learning Code Preference via Synthetic Evolution(https://arxiv.org/abs/2410.03837)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently demonstrated remarkable coding capabilities. However, assessing code generation based on well-formed properties and aligning it with developer preferences remains challenging. In this paper, we explore two key questions under the new challenge of code preference learning: (i) How do we train models to predict meaningful preferences for code? and (ii) How do human and LLM preferences align with verifiable code properties and developer code tastes? To this end, we propose CodeFavor, a framework for training pairwise code preference models from synthetic evolution data, including code commits and code critiques. To evaluate code preferences, we introduce CodePrefBench, a benchmark comprising 1364 rigorously curated code preference tasks to cover three verifiable properties-correctness, efficiency, and security-along with human preference. Our evaluation shows that CodeFavor holistically improves the accuracy of model-based code preferences by up to 28.8%. Meanwhile, CodeFavor models can match the performance of models with 6-9x more parameters while being 34x more cost-effective. We also rigorously validate the design choices in CodeFavor via a comprehensive set of controlled experiments. Furthermore, we discover the prohibitive costs and limitations of human-based code preference: despite spending 23.4 person-minutes on each task, 15.1-40.3% of tasks remain unsolved. Compared to model-based preference, human preference tends to be more accurate under the objective of code correctness, while being sub-optimal for non-functional objectives.</li>
</ul>

<h3>Title: ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD</h3>
<ul>
<li><strong>Authors: </strong>Aviral Kaintura, Palaniappan R, Shui Song Luar, Indira Iyer Almeida</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03845">https://arxiv.org/abs/2410.03845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03845">https://arxiv.org/pdf/2410.03845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03845]] ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD(https://arxiv.org/abs/2410.03845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-source Electronic Design Automation (EDA) tools are rapidly transforming chip design by addressing key barriers of commercial EDA tools such as complexity, costs, and access. Recent advancements in Large Language Models (LLMs) have further enhanced efficiency in chip design by providing user assistance across a range of tasks like setup, decision-making, and flow automation. This paper introduces ORAssistant, a conversational assistant for OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to improve the user experience for the OpenROAD flow, from RTL-GDSII by providing context-specific responses to common user queries, including installation, command usage, flow setup, and execution, in prose format. Currently, ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and KLayout. The data model is built from publicly available documentation and GitHub resources. The proposed architecture is scalable, supporting extensions to other open-source tools, operating modes, and LLM models. We use Google Gemini as the base LLM model to build and test ORAssistant. Early evaluation results of the RAG-based model show notable improvements in performance and accuracy compared to non-fine-tuned LLMs.</li>
</ul>

<h3>Title: Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Chen, Stylios Moscholios</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03848">https://arxiv.org/abs/2410.03848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03848">https://arxiv.org/pdf/2410.03848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03848]] Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style(https://arxiv.org/abs/2410.03848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), such as GPT series and Llama series have demonstrated strong capabilities in natural language processing, contextual understanding, and text generation. In recent years, researchers are trying to enhance the abilities of LLMs in performing various tasks, and numerous studies have proved that well-designed prompts can significantly improve the performance of LLMs on these tasks. This study compares the language style imitation ability of three different large language models under the guidance of the same zero-shot prompt. It also involves comparing the imitation ability of the same large language model when guided by three different prompts individually. Additionally, by applying a Tree-of-Thoughts (ToT) Prompting method to Llama 3, a conversational AI with the language style of a real person was created. In this study, three evaluation methods were used to evaluate LLMs and prompts. The results show that Llama 3 performs best at imitating language styles, and that the ToT prompting method is the most effective to guide it in imitating language styles. Using a ToT framework, Llama 3 was guided to interact with users in the language style of a specific individual without altering its core parameters, thereby creating a text-based conversational AI that reflects the language style of the individual.</li>
</ul>

<h3>Title: A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research</h3>
<ul>
<li><strong>Authors: </strong>Teresa Salazar, Helder Araújo, Alberto Cano, Pedro Henriques Abreu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03855">https://arxiv.org/abs/2410.03855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03855">https://arxiv.org/pdf/2410.03855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03855]] A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research(https://arxiv.org/abs/2410.03855)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Group fairness in machine learning is a critical area of research focused on achieving equitable outcomes across different groups defined by sensitive attributes such as race or gender. Federated learning, a decentralized approach to training machine learning models across multiple devices or organizations without sharing raw data, amplifies the need for fairness due to the heterogeneous data distributions across clients, which can exacerbate biases. The intersection of federated learning and group fairness has attracted significant interest, with 47 research works specifically dedicated to addressing this issue. However, no dedicated survey has focused comprehensively on group fairness in federated learning. In this work, we present an in-depth survey on this topic, addressing the critical challenges and reviewing related works in the field. We create a novel taxonomy of these approaches based on key criteria such as data partitioning, location, and applied strategies. Additionally, we explore broader concerns related to this problem and investigate how different approaches handle the complexities of various sensitive groups and their intersections. Finally, we review the datasets and applications commonly used in current research. We conclude by highlighting key areas for future research, emphasizing the need for more methods to address the complexities of achieving group fairness in federated systems.</li>
</ul>

<h3>Title: Detecting Machine-Generated Long-Form Content with Latent-Space Variables</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tian, Zeyu Pan, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03856">https://arxiv.org/abs/2410.03856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03856">https://arxiv.org/pdf/2410.03856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03856]] Detecting Machine-Generated Long-Form Content with Latent-Space Variables(https://arxiv.org/abs/2410.03856)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing capability of large language models (LLMs) to generate fluent long-form texts is presenting new challenges in distinguishing machine-generated outputs from human-written ones, which is crucial for ensuring authenticity and trustworthiness of expressions. Existing zero-shot detectors primarily focus on token-level distributions, which are vulnerable to real-world domain shifts, including different prompting and decoding strategies, and adversarial attacks. We propose a more robust method that incorporates abstract elements, such as event transitions, as key deciding factors to detect machine versus human texts by training a latent-space model on sequences of events or topics derived from human-written texts. In three different domains, machine-generated texts, which are originally inseparable from human texts on the token level, can be better distinguished with our latent-space model, leading to a 31% improvement over strong baselines such as DetectGPT. Our analysis further reveals that, unlike humans, modern LLMs like GPT-4 generate event triggers and their transitions differently, an inherent disparity that helps our method to robustly detect machine-generated texts.</li>
</ul>

<h3>Title: You Know What I'm Saying -- Jailbreak Attack via Implicit Reference</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Wu, Lingrui Mei, Ruibin Yuan, Lujun Li, Wei Xue, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03857">https://arxiv.org/abs/2410.03857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03857">https://arxiv.org/pdf/2410.03857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03857]] You Know What I'm Saying -- Jailbreak Attack via Implicit Reference(https://arxiv.org/abs/2410.03857)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>While recent advancements in large language model (LLM) alignment have enabled the effective identification of malicious objectives involving scene nesting and keyword rewriting, our study reveals that these methods remain inadequate at detecting malicious objectives expressed through context within nested harmless objectives. This study identifies a previously overlooked vulnerability, which we term Attack via Implicit Reference (AIR). AIR decomposes a malicious objective into permissible objectives and links them through implicit references within the context. This method employs multiple related harmless objectives to generate malicious content without triggering refusal responses, thereby effectively bypassing existing detection this http URL experiments demonstrate AIR's effectiveness across state-of-the-art LLMs, achieving an attack success rate (ASR) exceeding 90% on most models, including GPT-4o, Claude-3.5-Sonnet, and Qwen-2-72B. Notably, we observe an inverse scaling phenomenon, where larger models are more vulnerable to this attack method. These findings underscore the urgent need for defense mechanisms capable of understanding and preventing contextual attacks. Furthermore, we introduce a cross-model attack strategy that leverages less secure models to generate malicious contexts, thereby further increasing the ASR when targeting other this http URL code and jailbreak artifacts can be found at this https URL.</li>
</ul>

<h3>Title: MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Leo Bringer, Joey Wilson, Kira Barton, Maani Ghaffari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03860">https://arxiv.org/abs/2410.03860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03860">https://arxiv.org/pdf/2410.03860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03860]] MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty(https://arxiv.org/abs/2410.03860)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a Multi-modal Diffusion model for Motion Prediction (MDMP) that integrates and synchronizes skeletal data and textual descriptions of actions to generate refined long-term motion predictions with quantifiable uncertainty. Existing methods for motion forecasting or motion generation rely solely on either prior motions or text prompts, facing limitations with precision or control, particularly over extended durations. The multi-modal nature of our approach enhances the contextual understanding of human motion, while our graph-based transformer framework effectively capture both spatial and temporal motion dynamics. As a result, our model consistently outperforms existing generative techniques in accurately predicting long-term motions. Additionally, by leveraging diffusion models' ability to capture different modes of prediction, we estimate uncertainty, significantly improving spatial awareness in human-robot interactions by incorporating zones of presence with varying confidence levels for each body joint.</li>
</ul>

<h3>Title: Improving Mapper's Robustness by Varying Resolution According to Lens-Space Density</h3>
<ul>
<li><strong>Authors: </strong>Kaleb D. Ruscitti, Leland McInnes</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03862">https://arxiv.org/abs/2410.03862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03862">https://arxiv.org/pdf/2410.03862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03862]] Improving Mapper's Robustness by Varying Resolution According to Lens-Space Density(https://arxiv.org/abs/2410.03862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose an improvement to the Mapper algorithm that removes the assumption of a single resolution scale across semantic space, and improves the robustness of the results under change of parameters. This eases parameter selection, especially for datasets with highly variable local density in the Morse function $f$ used for Mapper. This is achieved by incorporating this density into the choice of cover for Mapper. Furthermore, we prove that for covers with some natural hypotheses, the graph output by Mapper still converges in bottleneck distance to the Reeb graph of the Rips complex of the data, but captures more topological features than when using the usual Mapper cover. Finally, we discuss implementation details, and include the results of computational experiments. We also provide an accompanying reference implementation.</li>
</ul>

<h3>Title: Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Kuiyi Gao, Zihan Jia, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03869">https://arxiv.org/abs/2410.03869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03869">https://arxiv.org/pdf/2410.03869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03869]] Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step(https://arxiv.org/abs/2410.03869)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion</a></li>
<li><strong>Abstract: </strong>Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years. Despite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material. To assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process. Specifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries. To evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements. Experiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%). Further, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95% of CoJ attack. We release our dataset and code to facilitate the AI safety research.</li>
</ul>

<h3>Title: A Federated Distributionally Robust Support Vector Machine with Mixture of Wasserstein Balls Ambiguity Set for Distributed Fault Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Michael Ibrahim, Heraldo Rozas, Nagi Gebraeel, Weijun Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03877">https://arxiv.org/abs/2410.03877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03877">https://arxiv.org/pdf/2410.03877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03877]] A Federated Distributionally Robust Support Vector Machine with Mixture of Wasserstein Balls Ambiguity Set for Distributed Fault Diagnosis(https://arxiv.org/abs/2410.03877)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The training of classification models for fault diagnosis tasks using geographically dispersed data is a crucial task for original parts manufacturers (OEMs) seeking to provide long-term service contracts (LTSCs) to their customers. Due to privacy and bandwidth constraints, such models must be trained in a federated fashion. Moreover, due to harsh industrial settings the data often suffers from feature and label uncertainty. Therefore, we study the problem of training a distributionally robust (DR) support vector machine (SVM) in a federated fashion over a network comprised of a central server and $G$ clients without sharing data. We consider the setting where the local data of each client $g$ is sampled from a unique true distribution $\mathbb{P}_g$, and the clients can only communicate with the central server. We propose a novel Mixture of Wasserstein Balls (MoWB) ambiguity set that relies on local Wasserstein balls centered at the empirical distribution of the data at each client. We study theoretical aspects of the proposed ambiguity set, deriving its out-of-sample performance guarantees and demonstrating that it naturally allows for the separability of the DR problem. Subsequently, we propose two distributed optimization algorithms for training the global FDR-SVM: i) a subgradient method-based algorithm, and ii) an alternating direction method of multipliers (ADMM)-based algorithm. We derive the optimization problems to be solved by each client and provide closed-form expressions for the computations performed by the central server during each iteration for both algorithms. Finally, we thoroughly examine the performance of the proposed algorithms in a series of numerical experiments utilizing both simulation data and popular real-world datasets.</li>
</ul>

<h3>Title: SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhang, Zhiyang Xu, Ying Shen, Parisa Kordjamshidi, Lifu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03878">https://arxiv.org/abs/2410.03878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03878">https://arxiv.org/pdf/2410.03878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03878]] SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models(https://arxiv.org/abs/2410.03878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Integrating the 3D world into large language models (3D-based LLMs) has been a promising research direction for 3D scene understanding. However, current 3D-based LLMs fall short in situated understanding due to two key limitations: 1) existing 3D datasets are constructed from a global perspective of the 3D scenes and lack situated context. 2) the architectures of existing 3D-based LLMs lack explicit alignment between the spatial representations of 3D scenes and natural language, limiting their performance in tasks requiring precise spatial reasoning. We address these issues by introducing a scalable situated 3D dataset, named Spartun3D, that incorporates various situated spatial reasoning tasks. Furthermore, we propose Spartun3D-LLM, built on an existing 3D-based LLM but integrated with a novel situated spatial alignment module, aiming to enhance the alignment between 3D visual representations and their corresponding textual descriptions. Experimental results demonstrate that both our proposed dataset and alignment module significantly enhance the situated spatial understanding of 3D-based LLMs.</li>
</ul>

<h3>Title: DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction</h3>
<ul>
<li><strong>Authors: </strong>Xinwei Zhang, Zhiqi Bu, Borja Balle, Mingyi Hong, Meisam Razaviyayn, Vahab Mirrokni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03883">https://arxiv.org/abs/2410.03883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03883">https://arxiv.org/pdf/2410.03883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03883]] DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction(https://arxiv.org/abs/2410.03883)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Differential privacy (DP) offers a robust framework for safeguarding individual data privacy. To utilize DP in training modern machine learning models, differentially private optimizers have been widely used in recent years. A popular approach to privatize an optimizer is to clip the individual gradients and add sufficiently large noise to the clipped gradient. This approach led to the development of DP optimizers that have comparable performance with their non-private counterparts in fine-tuning tasks or in tasks with a small number of training parameters. However, a significant performance drop is observed when these optimizers are applied to large-scale training. This degradation stems from the substantial noise injection required to maintain DP, which disrupts the optimizer's dynamics. This paper introduces DiSK, a novel framework designed to significantly enhance the performance of DP optimizers. DiSK employs Kalman filtering, a technique drawn from control and signal processing, to effectively denoise privatized gradients and generate progressively refined gradient estimations. To ensure practicality for large-scale training, we simplify the Kalman filtering process, minimizing its memory and computational demands. We establish theoretical privacy-utility trade-off guarantees for DiSK, and demonstrate provable improvements over standard DP optimizers like DPSGD in terms of iteration complexity upper-bound. Extensive experiments across diverse tasks, including vision tasks such as CIFAR-100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and DART, validate the effectiveness of DiSK. The results showcase its ability to significantly improve the performance of DP optimizers, surpassing state-of-the-art results under the same privacy constraints on several benchmarks.</li>
</ul>

<h3>Title: KidLM: Advancing Language Models for Children -- Early Insights and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Mir Tafseer Nayeem, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03884">https://arxiv.org/abs/2410.03884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03884">https://arxiv.org/pdf/2410.03884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03884]] KidLM: Advancing Language Models for Children -- Early Insights and Future Directions(https://arxiv.org/abs/2410.03884)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children's unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.</li>
</ul>

<h3>Title: Towards Cost Sensitive Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Junier Oliva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03892">https://arxiv.org/abs/2410.03892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03892">https://arxiv.org/pdf/2410.03892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03892]] Towards Cost Sensitive Decision Making(https://arxiv.org/abs/2410.03892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world situations allow for the acquisition of additional relevant information when making decisions with limited or uncertain data. However, traditional RL approaches either require all features to be acquired beforehand (e.g. in a MDP) or regard part of them as missing data that cannot be acquired (e.g. in a POMDP). In this work, we consider RL models that may actively acquire features from the environment to improve the decision quality and certainty, while automatically balancing the cost of feature acquisition process and the reward of task decision process. We propose the Active-Acquisition POMDP and identify two types of the acquisition process for different application domains. In order to assist the agent in the actively-acquired partially-observed environment and alleviate the exploration-exploitation dilemma, we develop a model-based approach, where a deep generative model is utilized to capture the dependencies of the features and impute the unobserved features. The imputations essentially represent the beliefs of the agent. Equipped with the dynamics model, we develop hierarchical RL algorithms to resolve both types of the AA-POMDPs. Empirical results demonstrate that our approach achieves considerably better performance than existing POMDP-RL solutions.</li>
</ul>

<h3>Title: Improving Node Representation by Boosting Target-Aware Contrastive Loss</h3>
<ul>
<li><strong>Authors: </strong>Ying-Chun Lin, Jennifer Neville</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03901">https://arxiv.org/abs/2410.03901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03901">https://arxiv.org/pdf/2410.03901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03901]] Improving Node Representation by Boosting Target-Aware Contrastive Loss(https://arxiv.org/abs/2410.03901)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Graphs model complex relationships between entities, with nodes and edges capturing intricate connections. Node representation learning involves transforming nodes into low-dimensional embeddings. These embeddings are typically used as features for downstream tasks. Therefore, their quality has a significant impact on task performance. Existing approaches for node representation learning span (semi-)supervised, unsupervised, and self-supervised paradigms. In graph domains, (semi-)supervised learning often only optimizes models based on class labels, neglecting other abundant graph signals, which limits generalization. While self-supervised or unsupervised learning produces representations that better capture underlying graph signals, the usefulness of these captured signals for downstream target tasks can vary. To bridge this gap, we introduce Target-Aware Contrastive Learning (Target-aware CL) which aims to enhance target task performance by maximizing the mutual information between the target task and node representations with a self-supervised learning process. This is achieved through a sampling function, XGBoost Sampler (XGSampler), to sample proper positive examples for the proposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware CL increases the mutual information between the target task and node representations, such that model generalization is improved. Additionally, XGSampler enhances the interpretability of each signal by showing the weights for sampling the proper positive examples. We show experimentally that XTCL significantly improves the performance on two target tasks: node classification and link prediction tasks, compared to state-of-the-art models.</li>
</ul>

<h3>Title: PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lemei Zhang, Peng Liu, Marcus Tiedemann Oekland Henriksboe, Even W. Lauvrak, Jon Atle Gulla, Heri Ramampiaro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03905">https://arxiv.org/abs/2410.03905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03905">https://arxiv.org/pdf/2410.03905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03905]] PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models(https://arxiv.org/abs/2410.03905)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Natural Language Processing in recent years, numerous studies have shown that generic summaries generated by Large Language Models (LLMs) can sometimes surpass those annotated by experts, such as journalists, according to human evaluations. However, there is limited research on whether these generic summaries meet the individual needs of ordinary people. The biggest obstacle is the lack of human-annotated datasets from the general public. Existing work on personalized summarization often relies on pseudo datasets created from generic summarization datasets or controllable tasks that focus on specific named entities or other aspects, such as the length and specificity of generated summaries, collected from hypothetical tasks without the annotators' initiative. To bridge this gap, we propose a high-quality, personalized, manually annotated abstractive summarization dataset called PersonalSum. This dataset is the first to investigate whether the focus of public readers differs from the generic summaries generated by LLMs. It includes user profiles, personalized summaries accompanied by source sentences from given articles, and machine-generated generic summaries along with their sources. We investigate several personal signals - entities/topics, plot, and structure of articles - that may affect the generation of personalized summaries using LLMs in a few-shot in-context learning scenario. Our preliminary results and analysis indicate that entities/topics are merely one of the key factors that impact the diverse preferences of users, and personalized summarization remains a significant challenge for existing LLMs.</li>
</ul>

<h3>Title: ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities</h3>
<ul>
<li><strong>Authors: </strong>Ying Su, Zhan Ling, Haochen Shi, Jiayang Cheng, Yauwai Yim, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03907">https://arxiv.org/abs/2410.03907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03907">https://arxiv.org/pdf/2410.03907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03907]] ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities(https://arxiv.org/abs/2410.03907)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models~(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability. However, there is still lack of study on how vision language models~(VLMs) behave when multi-modal task inputs are considered. Counterfactual planning that evaluates the model's reasoning ability over alternative task situations are also under exploited. In order to evaluate the planning ability of both multi-modal and counterfactual aspects, we propose ActPlan-1K. ActPlan-1K is a multi-modal planning benchmark constructed based on ChatGPT and household activity simulator iGibson2. The benchmark consists of 153 activities and 1,187 instances. Each instance describing one activity has a natural language task description and multiple environment images from the simulator. The gold plan of each instance is action sequences over the objects in provided scenes. Both the correctness and commonsense satisfaction are evaluated on typical VLMs. It turns out that current VLMs are still struggling at generating human-level procedural plans for both normal activities and counterfactual activities. We further provide automatic evaluation metrics by finetuning over BLEURT model to facilitate future research on our benchmark.</li>
</ul>

<h3>Title: Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Amey Hengle, Atharva Kulkarni, Shantanu Patankar, Madhumitha Chandrasekaran, Sneha D'Silva, Jemima Jacob, Rashmi Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03908">https://arxiv.org/abs/2410.03908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03908">https://arxiv.org/pdf/2410.03908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03908]] Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis(https://arxiv.org/abs/2410.03908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this study, we introduce ANGST, a novel, first-of-its kind benchmark for depression-anxiety comorbidity classification from social media posts. Unlike contemporary datasets that often oversimplify the intricate interplay between different mental health disorders by treating them as isolated conditions, ANGST enables multi-label classification, allowing each post to be simultaneously identified as indicating depression and/or anxiety. Comprising 2876 meticulously annotated posts by expert psychologists and an additional 7667 silver-labeled posts, ANGST posits a more representative sample of online mental health discourse. Moreover, we benchmark ANGST using various state-of-the-art language models, ranging from Mental-BERT to GPT-4. Our results provide significant insights into the capabilities and limitations of these models in complex diagnostic scenarios. While GPT-4 generally outperforms other models, none achieve an F1 score exceeding 72% in multi-class comorbid classification, underscoring the ongoing challenges in applying language models to mental health diagnostics.</li>
</ul>

<h3>Title: Distribution Guided Active Feature Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Junier Oliva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03915">https://arxiv.org/abs/2410.03915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03915">https://arxiv.org/pdf/2410.03915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03915]] Distribution Guided Active Feature Acquisition(https://arxiv.org/abs/2410.03915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Human agents routinely reason on instances with incomplete and muddied data (and weigh the cost of obtaining further features). In contrast, much of ML is devoted to the unrealistic, sterile environment where all features are observed and further information on an instance is obviated. Here we extend past static ML and develop an active feature acquisition (AFA) framework that interacts with the environment to obtain new information on-the-fly and can: 1) make inferences on an instance in the face of incomplete features, 2) determine a plan for feature acquisitions to obtain additional information on the instance at hand. We build our AFA framework on a backbone of understanding the information and conditional dependencies that are present in the data. First, we show how to build generative models that can capture dependencies over arbitrary subsets of features and employ these models for acquisitions in a greedy scheme. After, we show that it is possible to guide the training of RL agents for AFA via side-information and auxiliary rewards stemming from our generative models. We also examine two important factors for deploying AFA models in real-world scenarios, namely interpretability and robustness. Extensive experiments demonstrate the state-of-the-art performance of our AFA framework.</li>
</ul>

<h3>Title: Online Posterior Sampling with a Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Branislav Kveton, Boris Oreshkin, Youngsuk Park, Aniket Deshmukh, Rui Song</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03919">https://arxiv.org/abs/2410.03919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03919">https://arxiv.org/pdf/2410.03919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03919]] Online Posterior Sampling with a Diffusion Prior(https://arxiv.org/abs/2410.03919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Posterior sampling in contextual bandits with a Gaussian prior can be implemented exactly or approximately using the Laplace approximation. The Gaussian prior is computationally efficient but it cannot describe complex distributions. In this work, we propose approximate posterior sampling algorithms for contextual bandits with a diffusion model prior. The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse process, which are estimated in a closed form using the Laplace approximation. Our approximations are motivated by posterior sampling with a Gaussian prior, and inherit its simplicity and efficiency. They are asymptotically consistent and perform well empirically on a variety of contextual bandit problems.</li>
</ul>

<h3>Title: C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits</h3>
<ul>
<li><strong>Authors: </strong>Maaz Bin Musa, Steven M. Winston, Garrison Allen, Jacob Schiller, Kevin Moore, Sean Quick, Johnathan Melvin, Padmini Srinivasan, Mihailis E. Diamantis, Rishab Nithyanand</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03925">https://arxiv.org/abs/2410.03925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03925">https://arxiv.org/pdf/2410.03925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03925]] C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits(https://arxiv.org/abs/2410.03925)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The development of tools and techniques to analyze and extract organizations data habits from privacy policies are critical for scalable regulatory compliance audits. Unfortunately, these tools are becoming increasingly limited in their ability to identify compliance issues and fixes. After all, most were developed using regulation-agnostic datasets of annotated privacy policies obtained from a time before the introduction of landmark privacy regulations such as EUs GDPR and Californias CCPA. In this paper, we describe the first open regulation-aware dataset of expert-annotated privacy policies, C3PA (CCPA Privacy Policy Provision Annotations), aimed to address this challenge. C3PA contains over 48K expert-labeled privacy policy text segments associated with responses to CCPA-specific disclosure mandates from 411 unique organizations. We demonstrate that the C3PA dataset is uniquely suited for aiding automated audits of compliance with CCPA-related disclosure mandates.</li>
</ul>

<h3>Title: Clustering Alzheimer's Disease Subtypes via Similarity Learning and Graph Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Wei, Shu Yang, Davoud Ataee Tarzanagh, Jingxuan Bao, Jia Xu, Patryk Orzechowski, Joost B. Wagenaar, Qi Long, Li Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03937">https://arxiv.org/abs/2410.03937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03937">https://arxiv.org/pdf/2410.03937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03937]] Clustering Alzheimer's Disease Subtypes via Similarity Learning and Graph Diffusion(https://arxiv.org/abs/2410.03937)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a complex neurodegenerative disorder that affects millions of people worldwide. Due to the heterogeneous nature of AD, its diagnosis and treatment pose critical challenges. Consequently, there is a growing research interest in identifying homogeneous AD subtypes that can assist in addressing these challenges in recent years. In this study, we aim to identify subtypes of AD that represent distinctive clinical features and underlying pathology by utilizing unsupervised clustering with graph diffusion and similarity learning. We adopted SIMLR, a multi-kernel similarity learning framework, and graph diffusion to perform clustering on a group of 829 patients with AD and mild cognitive impairment (MCI, a prodromal stage of AD) based on their cortical thickness measurements extracted from magnetic resonance imaging (MRI) scans. Although the clustering approach we utilized has not been explored for the task of AD subtyping before, it demonstrated significantly better performance than several commonly used clustering methods. Specifically, we showed the power of graph diffusion in reducing the effects of noise in the subtype detection. Our results revealed five subtypes that differed remarkably in their biomarkers, cognitive status, and some other clinical features. To evaluate the resultant subtypes further, a genetic association study was carried out and successfully identified potential genetic underpinnings of different AD subtypes. Our source code is available at: this https URL.</li>
</ul>

<h3>Title: AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Artur Kasymov, Marcin Sendera, Michał Stypułkowski, Maciej Zięba, Przemysław Spurek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03941">https://arxiv.org/abs/2410.03941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03941">https://arxiv.org/pdf/2410.03941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03941]] AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models(https://arxiv.org/abs/2410.03941)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) is a fine-tuning technique that can be applied to conditional generative diffusion models. LoRA utilizes a small number of context examples to adapt the model to a specific domain, character, style, or concept. However, due to the limited data utilized during training, the fine-tuned model performance is often characterized by strong context bias and a low degree of variability in the generated images. To solve this issue, we introduce AutoLoRA, a novel guidance technique for diffusion models fine-tuned with the LoRA approach. Inspired by other guidance techniques, AutoLoRA searches for a trade-off between consistency in the domain represented by LoRA weights and sample diversity from the base conditional diffusion model. Moreover, we show that incorporating classifier-free guidance for both LoRA fine-tuned and base models leads to generating samples with higher diversity and better quality. The experimental results for several fine-tuned LoRA domains show superiority over existing guidance techniques on selected metrics.</li>
</ul>

<h3>Title: Unidirectional Key Update in Updatable Encryption, Revisited</h3>
<ul>
<li><strong>Authors: </strong>M. Jurkiewicz, K. Prabucka</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03948">https://arxiv.org/abs/2410.03948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03948">https://arxiv.org/pdf/2410.03948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03948]] Unidirectional Key Update in Updatable Encryption, Revisited(https://arxiv.org/abs/2410.03948)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>In this paper we construct a new efficient updatable encryption (UE) scheme based on FrodoPKE learning with errors key encapsulation. We analyse the security of the proposed scheme in the backward-leak uni-directional setting within the rand-ind-eu-cpa model. Since the underlying computationally hard problem here is LWE, the scheme is secure against both classical and quantum attacks.</li>
</ul>

<h3>Title: A Brain-Inspired Regularizer for Adversarial Robustness</h3>
<ul>
<li><strong>Authors: </strong>Elie Attias, Cengiz Pehlevan, Dina Obeid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03952">https://arxiv.org/abs/2410.03952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03952">https://arxiv.org/pdf/2410.03952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03952]] A Brain-Inspired Regularizer for Adversarial Robustness(https://arxiv.org/abs/2410.03952)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNNs) excel in many visual tasks, but they tend to be sensitive to slight input perturbations that are imperceptible to the human eye, often resulting in task failures. Recent studies indicate that training CNNs with regularizers that promote brain-like representations, using neural recordings, can improve model robustness. However, the requirement to use neural data severely restricts the utility of these methods. Is it possible to develop regularizers that mimic the computational function of neural regularizers without the need for neural recordings, thereby expanding the usability and effectiveness of these techniques? In this work, we inspect a neural regularizer introduced in Li et al. (2019) to extract its underlying strength. The regularizer uses neural representational similarities, which we find also correlate with pixel similarities. Motivated by this finding, we introduce a new regularizer that retains the essence of the original but is computed using image pixel similarities, eliminating the need for neural recordings. We show that our regularization method 1) significantly increases model robustness to a range of black box attacks on various datasets and 2) is computationally inexpensive and relies only on original datasets. Our work explores how biologically motivated loss functions can be used to drive the performance of artificial neural networks.</li>
</ul>

<h3>Title: LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity</h3>
<ul>
<li><strong>Authors: </strong>Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03953">https://arxiv.org/abs/2410.03953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03953">https://arxiv.org/pdf/2410.03953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03953]] LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity(https://arxiv.org/abs/2410.03953)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Combining large language models during training or at inference time has shown substantial performance gain over component LLMs. This paper presents LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique properties: (i) We introduce the focal diversity metric to capture the diversity-performance correlation among component LLMs of an ensemble. (ii) We develop a diversity-optimized ensemble pruning algorithm to select the top-k sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii) We generate new output for each prompt query by utilizing a learn-to-ensemble approach, which learns to detect and resolve the output inconsistency among all component LLMs of an ensemble. Extensive evaluation on four different benchmarks shows good performance gain over the best LLM ensemble methods: (i) In constrained solution set problems, LLM-TOPLA outperforms the best-performing ensemble (Mixtral) by 2.2\% in accuracy on MMLU and the best-performing LLM ensemble (MoreAgent) on GSM8k by 2.1\%. (ii) In generative tasks, LLM-TOPLA outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by $3.9\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available at this https URL</li>
</ul>

<h3>Title: Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gang Li, Wendi Yu, Yao Yao, Wei Tong, Yingbin Liang, Qihang Lin, Tianbao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03955">https://arxiv.org/abs/2410.03955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03955">https://arxiv.org/pdf/2410.03955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03955]] Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models(https://arxiv.org/abs/2410.03955)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>In the real world, a learning-enabled system usually undergoes multiple cycles of model development to enhance the system's ability to handle difficult or emerging tasks. This continual model development process raises a significant issue that the model development for acquiring new or improving existing capabilities may inadvertently lose capabilities of the old model, also known as catastrophic forgetting. Existing continual learning studies focus on mitigating catastrophic forgetting by trading off performance on previous tasks and new tasks to ensure good average performance. However, they are inadequate for many applications especially in safety-critical domains, as failure to strictly preserve the performance of the old model not only introduces safety risks and uncertainties but also imposes substantial expenses in the re-improving and re-validation of existing properties. To address this issue, we introduce model developmental safety as a guarantee of a learning system such that in the model development process the new model should strictly preserve the existing protected capabilities of the old model while improving its performance on target tasks. To ensure the model developmental safety, we present a safety-centric framework by formulating the model developmental safety as data-dependent constraints. Under this framework, we study how to develop a pretrained vision-language model (aka the CLIP model) for acquiring new capabilities or improving existing capabilities of image classification. We propose an efficient constrained optimization algorithm with theoretical guarantee and use its insights to finetune a CLIP model with task-dependent heads for promoting the model developmental safety. Our experiments on improving vision perception capabilities on autonomous driving and scene recognition datasets demonstrate the efficacy of the proposed approach.</li>
</ul>

<h3>Title: Variational Language Concepts for Interpreting Foundation Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hengyi Wang, Shiwei Tan, Zhiqing Hong, Desheng Zhang, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03964">https://arxiv.org/abs/2410.03964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03964">https://arxiv.org/pdf/2410.03964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03964]] Variational Language Concepts for Interpreting Foundation Language Models(https://arxiv.org/abs/2410.03964)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Foundation Language Models (FLMs) such as BERT and its variants have achieved remarkable success in natural language processing. To date, the interpretability of FLMs has primarily relied on the attention weights in their self-attention layers. However, these attention weights only provide word-level interpretations, failing to capture higher-level structures, and are therefore lacking in readability and intuitiveness. To address this challenge, we first provide a formal definition of conceptual interpretation and then propose a variational Bayesian framework, dubbed VAriational Language Concept (VALC), to go beyond word-level interpretations and provide concept-level interpretations. Our theoretical analysis shows that our VALC finds the optimal language concepts to interpret FLM predictions. Empirical results on several real-world datasets show that our method can successfully provide conceptual interpretation for FLMs.</li>
</ul>

<h3>Title: Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions</h3>
<ul>
<li><strong>Authors: </strong>Jianxin Zhang, Josh Viktorov, Doosan Jung, Emily Pitler</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03973">https://arxiv.org/abs/2410.03973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03973">https://arxiv.org/pdf/2410.03973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03973]] Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions(https://arxiv.org/abs/2410.03973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural Stochastic Differential Equations (Neural SDEs) have emerged as powerful mesh-free generative models for continuous stochastic processes, with critical applications in fields such as finance, physics, and biology. Previous state-of-the-art methods have relied on adversarial training, such as GANs, or on minimizing distance measures between processes using signature kernels. However, GANs suffer from issues like instability, mode collapse, and the need for specialized training techniques, while signature kernel-based methods require solving linear PDEs and backpropagating gradients through the solver, whose computational complexity scales quadratically with the discretization steps. In this paper, we identify a novel class of strictly proper scoring rules for comparing continuous Markov processes. This theoretical finding naturally leads to a novel approach called Finite Dimensional Matching (FDM) for training Neural SDEs. Our method leverages the Markov property of SDEs to provide a computationally efficient training objective. This scoring rule allows us to bypass the computational overhead associated with signature kernels and reduces the training complexity from $O(D^2)$ to $O(D)$ per epoch, where $D$ represents the number of discretization steps of the process. We demonstrate that FDM achieves superior performance, consistently outperforming existing methods in terms of both computational efficiency and generative quality.</li>
</ul>

<h3>Title: MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task</h3>
<ul>
<li><strong>Authors: </strong>Juraj Juraska, Daniel Deutsch, Mara Finkelstein, Markus Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03983">https://arxiv.org/abs/2410.03983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03983">https://arxiv.org/pdf/2410.03983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03983]] MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task(https://arxiv.org/abs/2410.03983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we present the MetricX-24 submissions to the WMT24 Metrics Shared Task and provide details on the improvements we made over the previous version of MetricX. Our primary submission is a hybrid reference-based/-free metric, which can score a translation irrespective of whether it is given the source segment, the reference, or both. The metric is trained on previous WMT data in a two-stage fashion, first on the DA ratings only, then on a mixture of MQM and DA ratings. The training set in both stages is augmented with synthetic examples that we created to make the metric more robust to several common failure modes, such as fluent but unrelated translation, or undertranslation. We demonstrate the benefits of the individual modifications via an ablation study, and show a significant performance increase over MetricX-23 on the WMT23 MQM ratings, as well as our new synthetic challenge set.</li>
</ul>

<h3>Title: Mamba Capsule Routing Towards Part-Whole Relational Camouflaged Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Dingwen Zhang, Liangbo Cheng, Yi Liu, Xinggang Wang, Junwei Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03987">https://arxiv.org/abs/2410.03987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03987">https://arxiv.org/pdf/2410.03987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03987]] Mamba Capsule Routing Towards Part-Whole Relational Camouflaged Object Detection(https://arxiv.org/abs/2410.03987)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The part-whole relational property endowed by Capsule Networks (CapsNets) has been known successful for camouflaged object detection due to its segmentation integrity. However, the previous Expectation Maximization (EM) capsule routing algorithm with heavy computation and large parameters obstructs this trend. The primary attribution behind lies in the pixel-level capsule routing. Alternatively, in this paper, we propose a novel mamba capsule routing at the type level. Specifically, we first extract the implicit latent state in mamba as capsule vectors, which abstract type-level capsules from pixel-level versions. These type-level mamba capsules are fed into the EM routing algorithm to get the high-layer mamba capsules, which greatly reduce the computation and parameters caused by the pixel-level capsule routing for part-whole relationships exploration. On top of that, to retrieve the pixel-level capsule features for further camouflaged prediction, we achieve this on the basis of the low-layer pixel-level capsules with the guidance of the correlations from adjacent-layer type-level mamba capsules. Extensive experiments on three widely used COD benchmark datasets demonstrate that our method significantly outperforms state-of-the-arts. Code has been available on this https URL\_capsule.</li>
</ul>

<h3>Title: On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abhilasha Sancheti, Haozhe An, Rachel Rudinger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03996">https://arxiv.org/abs/2410.03996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03996">https://arxiv.org/pdf/2410.03996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03996]] On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models(https://arxiv.org/abs/2410.03996)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study the presence of heteronormative biases and prejudice against interracial romantic relationships in large language models by performing controlled name-replacement experiments for the task of relationship prediction. We show that models are less likely to predict romantic relationships for (a) same-gender character pairs than different-gender pairs; and (b) intra/inter-racial character pairs involving Asian names as compared to Black, Hispanic, or White names. We examine the contextualized embeddings of first names and find that gender for Asian names is less discernible than non-Asian names. We discuss the social implications of our findings, underlining the need to prioritize the development of inclusive and equitable technology.</li>
</ul>

<h3>Title: Impact of Regularization on Calibration and Robustness: from the Representation Space Perspective</h3>
<ul>
<li><strong>Authors: </strong>Jonghyun Park, Juyeop Kim, Jong-Seok Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03999">https://arxiv.org/abs/2410.03999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03999">https://arxiv.org/pdf/2410.03999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03999]] Impact of Regularization on Calibration and Robustness: from the Representation Space Perspective(https://arxiv.org/abs/2410.03999)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that regularization techniques using soft labels, e.g., label smoothing, Mixup, and CutMix, not only enhance image classification accuracy but also improve model calibration and robustness against adversarial attacks. However, the underlying mechanisms of such improvements remain underexplored. In this paper, we offer a novel explanation from the perspective of the representation space (i.e., the space of the features obtained at the penultimate layer). Our investigation first reveals that the decision regions in the representation space form cone-like shapes around the origin after training regardless of the presence of regularization. However, applying regularization causes changes in the distribution of features (or representation vectors). The magnitudes of the representation vectors are reduced and subsequently the cosine similarities between the representation vectors and the class centers (minimal loss points for each class) become higher, which acts as a central mechanism inducing improved calibration and robustness. Our findings provide new insights into the characteristics of the high-dimensional representation space in relation to training and regularization using soft labels.</li>
</ul>

<h3>Title: Take It Easy: Label-Adaptive Self-Rationalization for Fact Verification and Explanation Generation</h3>
<ul>
<li><strong>Authors: </strong>Jing Yang, Anderson Rocha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04002">https://arxiv.org/abs/2410.04002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04002">https://arxiv.org/pdf/2410.04002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04002]] Take It Easy: Label-Adaptive Self-Rationalization for Fact Verification and Explanation Generation(https://arxiv.org/abs/2410.04002)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computational methods to aid journalists in the task often require adapting a model to specific domains and generating explanations. However, most automated fact-checking methods rely on three-class datasets, which do not accurately reflect real-world misinformation. Moreover, fact-checking explanations are often generated based on text summarization of evidence, failing to address the relationship between the claim and the evidence. To address these issues, we extend the self-rationalization method--typically used in natural language inference (NLI) tasks--to fact verification. We propose a label-adaptive learning approach: first, we fine-tune a model to learn veracity prediction with annotated labels (step-1 model). Then, we fine-tune the step-1 model again to learn self-rationalization, using the same data and additional annotated explanations. Our results show that our label-adaptive approach improves veracity prediction by more than ten percentage points (Macro F1) on both the PubHealth and AVeriTec datasets, outperforming the GPT-4 model. Furthermore, to address the high cost of explanation annotation, we generated 64 synthetic explanations from three large language models: GPT-4-turbo, GPT-3.5-turbo, and Llama-3-8B and few-shot fine-tune our step-1 model. The few-shot synthetic explanation fine-tuned model performed comparably to the fully fine-tuned self-rationalization model, demonstrating the potential of low-budget learning with synthetic data. Our label-adaptive self-rationalization approach presents a promising direction for future research on real-world explainable fact-checking with different labeling schemes.</li>
</ul>

<h3>Title: ASPIRER: Bypassing System Prompts With Permutation-based Backdoors in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lu Yan, Siyuan Cheng, Xuan Chen, Kaiyuan Zhang, Guangyu Shen, Zhuo Zhang, Xiangyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04009">https://arxiv.org/abs/2410.04009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04009">https://arxiv.org/pdf/2410.04009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04009]] ASPIRER: Bypassing System Prompts With Permutation-based Backdoors in LLMs(https://arxiv.org/abs/2410.04009)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become integral to many applications, with system prompts serving as a key mechanism to regulate model behavior and ensure ethical outputs. In this paper, we introduce a novel backdoor attack that systematically bypasses these system prompts, posing significant risks to the AI supply chain. Under normal conditions, the model adheres strictly to its system prompts. However, our backdoor allows malicious actors to circumvent these safeguards when triggered. Specifically, we explore a scenario where an LLM provider embeds a covert trigger within the base model. A downstream deployer, unaware of the hidden trigger, fine-tunes the model and offers it as a service to users. Malicious actors can purchase the trigger from the provider and use it to exploit the deployed model, disabling system prompts and achieving restricted outcomes. Our attack utilizes a permutation trigger, which activates only when its components are arranged in a precise order, making it computationally challenging to detect or reverse-engineer. We evaluate our approach on five state-of-the-art models, demonstrating that our method achieves an attack success rate (ASR) of up to 99.50% while maintaining a clean accuracy (CACC) of 98.58%, even after defensive fine-tuning. These findings highlight critical vulnerabilities in LLM deployment pipelines and underscore the need for stronger defenses.</li>
</ul>

<h3>Title: Hyperbolic Fine-tuning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Menglin Yang, Aosong Feng, Bo Xiong, Jihong Liu, Irwin King, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04010">https://arxiv.org/abs/2410.04010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04010">https://arxiv.org/pdf/2410.04010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04010]] Hyperbolic Fine-tuning for Large Language Models(https://arxiv.org/abs/2410.04010)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs. In this study, we first investigate the non-Euclidean characteristics of LLMs. Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. Building on the observation, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. However, we found that this fine-tuning in hyperbolic space cannot be achieved with naive application of exponential and logarithmic maps, when the embedding and weight matrices both reside in Euclidean space. To address this technique issue, we introduce a new method called hyperbolic low-rank efficient fine-tuning, HypLoRA, that performs low-rank adaptation directly on the hyperbolic manifold, avoiding the cancellation effect caused by the exponential and logarithmic maps, thus preserving the hyperbolic modeling capabilities. Through extensive experiments, we demonstrate that HypLoRA significantly enhances the performance of LLMs on reasoning tasks, particularly for complex reasoning problems. In particular, HypLoRA improves the performance in the complex AQuA dataset by up to 13.0%, showcasing its effectiveness in handling complex reasoning challenges</li>
</ul>

<h3>Title: A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Houquan Zhou, Zhenghua Li, Bo Zhang, Chen Li, Shaopeng Lai, Ji Zhang, Fei Huang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04027">https://arxiv.org/abs/2410.04027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04027">https://arxiv.org/pdf/2410.04027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04027]] A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models(https://arxiv.org/abs/2410.04027)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work proposes a simple training-free prompt-free approach to leverage large language models (LLMs) for the Chinese spelling correction (CSC) task, which is totally different from all previous CSC approaches. The key idea is to use an LLM as a pure language model in a conventional manner. The LLM goes through the input sentence from the beginning, and at each inference step, produces a distribution over its vocabulary for deciding the next token, given a partial sentence. To ensure that the output sentence remains faithful to the input sentence, we design a minimal distortion model that utilizes pronunciation or shape similarities between the original and replaced characters. Furthermore, we propose two useful reward strategies to address practical challenges specific to the CSC task. Experiments on five public datasets demonstrate that our approach significantly improves LLM performance, enabling them to compete with state-of-the-art domain-general CSC models.</li>
</ul>

<h3>Title: SyllableLM: Learning Coarse Semantic Units for Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alan Baade, Puyuan Peng, David Harwath</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04029">https://arxiv.org/abs/2410.04029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04029">https://arxiv.org/pdf/2410.04029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04029]] SyllableLM: Learning Coarse Semantic Units for Speech Language Models(https://arxiv.org/abs/2410.04029)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.</li>
</ul>

<h3>Title: ForgeryTTT: Zero-Shot Image Manipulation Localization with Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Weihuang Liu, Xi Shen, Chi-Man Pun, Xiaodong Cun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04032">https://arxiv.org/abs/2410.04032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04032">https://arxiv.org/pdf/2410.04032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04032]] ForgeryTTT: Zero-Shot Image Manipulation Localization with Test-Time Training(https://arxiv.org/abs/2410.04032)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Social media is increasingly plagued by realistic fake images, making it hard to trust content. Previous algorithms to detect these fakes often fail in new, real-world scenarios because they are trained on specific datasets. To address the problem, we introduce ForgeryTTT, the first method leveraging test-time training (TTT) to identify manipulated regions in images. The proposed approach fine-tunes the model for each individual test sample, improving its performance. ForgeryTTT first employs vision transformers as a shared image encoder to learn both classification and localization tasks simultaneously during the training-time training using a large synthetic dataset. Precisely, the localization head predicts a mask to highlight manipulated areas. Given such a mask, the input tokens can be divided into manipulated and genuine groups, which are then fed into the classification head to distinguish between manipulated and genuine parts. During test-time training, the predicted mask from the localization head is used for the classification head to update the image encoder for better adaptation. Additionally, using the classical dropout strategy in each token group significantly improves performance and efficiency. We test ForgeryTTT on five standard benchmarks. Despite its simplicity, ForgeryTTT achieves a 20.1% improvement in localization accuracy compared to other zero-shot methods and a 4.3% improvement over non-zero-shot techniques. Our code and data will be released upon publication.</li>
</ul>

<h3>Title: BlockFound: Customized blockchain foundation model for anomaly detection</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Yu, Xian Wu, Hao Liu, Wenbo Guo, Xinyu Xing</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04039">https://arxiv.org/abs/2410.04039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04039">https://arxiv.org/pdf/2410.04039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04039]] BlockFound: Customized blockchain foundation model for anomaly detection(https://arxiv.org/abs/2410.04039)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose BlockFound, a customized foundation model for anomaly blockchain transaction detection. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models, BlockFound introduces a series of customized designs to model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized mask language learning mechanism for pretraining with RoPE embedding and FlashAttention for handling longer sequences. After training the foundation model, we further design a novel detection method for anomaly detection. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockFound's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockFound is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work not only provides new foundation models for blockchain but also sets a new benchmark for applying LLMs in blockchain data.</li>
</ul>

<h3>Title: Neuron-Level Sequential Editing for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Houcheng Jiang, Junfeng Fang, Tianyu Zhang, An Zhang, Ruipeng Wang, Tao Liang, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04045">https://arxiv.org/abs/2410.04045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04045">https://arxiv.org/pdf/2410.04045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04045]] Neuron-Level Sequential Editing for Large Language Models(https://arxiv.org/abs/2410.04045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work explores sequential model editing in large language models (LLMs), a critical task that involves modifying internal knowledge within LLMs continuously through multi-round editing, each incorporating updates or corrections to adjust the model outputs without the need for costly retraining. Existing model editing methods, especially those that alter model parameters, typically focus on single-round editing and often face significant challenges in sequential model editing-most notably issues of model forgetting and failure. To address these challenges, we introduce a new model editing method, namely \textbf{N}euron-level \textbf{S}equential \textbf{E}diting (NSE), tailored for supporting sequential model editing. Specifically, we optimize the target layer's hidden states using the model's original weights to prevent model failure. Furthermore, we iteratively select neurons in multiple layers for editing based on their activation values to mitigate model forgetting. Our empirical experiments demonstrate that NSE significantly outperforms current modifying parameters model editing methods, marking a substantial advancement in the field of sequential model editing. Our code is released on \url{this https URL}.</li>
</ul>

<h3>Title: Lane Detection System for Driver Assistance in Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Kauan Divino Pouso Mariano, Fernanda de Castro Fernandes, Luan Gabriel Silva Oliveira, Lyan Eduardo Sakuno Rodrigues, Matheus Andrade Brandão</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04046">https://arxiv.org/abs/2410.04046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04046">https://arxiv.org/pdf/2410.04046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04046]] Lane Detection System for Driver Assistance in Vehicles(https://arxiv.org/abs/2410.04046)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This work presents the development of a lane detection system aimed at assisting the driving of conventional and autonomous vehicles. The system was implemented using traditional computer vision techniques, focusing on robustness and efficiency to operate in real-time, even under adverse conditions such as worn-out lanes and weather variations. The methodology employs an image processing pipeline that includes camera calibration, distortion correction, perspective transformation, and binary image generation. Lane detection is performed using sliding window techniques and segmentation based on gradients and color channels, enabling the precise identification of lanes in various road scenarios. The results indicate that the system can effectively detect and track lanes, performing well under different lighting conditions and road surfaces. However, challenges were identified in extreme situations, such as intense shadows and sharp curves. It is concluded that, despite its limitations, the traditional computer vision approach shows significant potential for application in driver assistance systems and autonomous navigation, with room for future improvements.</li>
</ul>

<h3>Title: Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution</h3>
<ul>
<li><strong>Authors: </strong>Wen Ye, Yizhou Zhang, Wei Yang, Lumingyuan Tang, Defu Cao, Jie Cai, Yan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04047">https://arxiv.org/abs/2410.04047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04047">https://arxiv.org/pdf/2410.04047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04047]] Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution(https://arxiv.org/abs/2410.04047)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent decades, there has been substantial advances in time series models and benchmarks across various individual tasks, such as time series forecasting, classification, and anomaly detection. Meanwhile, compositional reasoning in time series prevalent in real-world applications (e.g., decision-making and compositional question answering) is in great demand. Unlike simple tasks that primarily focus on predictive accuracy, compositional reasoning emphasizes the synthesis of diverse information from both time series data and various domain knowledge, making it distinct and extremely more challenging. In this paper, we introduce Compositional Time Series Reasoning, a new task of handling intricate multistep reasoning tasks from time series data. Specifically, this new task focuses on various question instances requiring structural and compositional reasoning abilities on time series data, such as decision-making and compositional question answering. As an initial attempt to tackle this novel task, we developed TS-Reasoner, a program-aided approach that utilizes large language model (LLM) to decompose a complex task into steps of programs that leverage existing time series models and numerical subroutines. Unlike existing reasoning work which only calls off-the-shelf modules, TS-Reasoner allows for the creation of custom modules and provides greater flexibility to incorporate domain knowledge as well as user-specified constraints. We demonstrate the effectiveness of our method through a comprehensive set of experiments. These promising results indicate potential opportunities in the new task of time series reasoning and highlight the need for further research.</li>
</ul>

<h3>Title: Large Language Models can Achieve Social Balance</h3>
<ul>
<li><strong>Authors: </strong>Pedro Cisneros-Velarde</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA, cs.SI, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04054">https://arxiv.org/abs/2410.04054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04054">https://arxiv.org/pdf/2410.04054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04054]] Large Language Models can Achieve Social Balance(https://arxiv.org/abs/2410.04054)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Social balance is a concept in sociology which states that if every three individuals in a population achieve certain structures of positive or negative interactions, then the whole population ends up in one faction of positive interactions or divided between two or more antagonistic factions. In this paper, we consider a group of interacting large language models (LLMs) and study how, after continuous interactions, they can achieve social balance. Across three different LLM models, we found that social balance depends on (i) whether interactions are updated based on "relationships", "appraisals", or "opinions"; (ii) whether agents update their interactions based on homophily or influence from their peers; and (iii) the number of simultaneous interactions the LLMs consider. When social balance is achieved, its particular structure of positive or negative interactions depends on these three conditions and are different across LLM models and sizes. The stability of interactions and the justification for their update also vary across models. Thus, social balance is driven by the pre-training and alignment particular to each LLM model.</li>
</ul>

<h3>Title: Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04055">https://arxiv.org/abs/2410.04055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04055">https://arxiv.org/pdf/2410.04055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04055]] Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks(https://arxiv.org/abs/2410.04055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Vision-Language Models (VLMs) have shown remarkable abilities in visual and language reasoning tasks, they invariably generate flawed responses. Self-correction that instructs models to refine their outputs presents a promising solution to this issue. Previous studies have mainly concentrated on Large Language Models (LLMs), while the self-correction abilities of VLMs, particularly concerning both visual and linguistic information, remain largely unexamined. This study investigates the self-correction capabilities of VLMs during both inference and fine-tuning stages. We introduce a Self-Correction Learning (SCL) approach that enables VLMs to learn from their self-generated self-correction data through Direct Preference Optimization (DPO) without relying on external feedback, facilitating self-improvement. Specifically, we collect preferred and disfavored samples based on the correctness of initial and refined responses, which are obtained by two-turn self-correction with VLMs during the inference stage. Experimental results demonstrate that although VLMs struggle to self-correct effectively during iterative inference without additional fine-tuning and external feedback, they can enhance their performance and avoid previous mistakes through preference fine-tuning when their self-generated self-correction data are categorized into preferred and disfavored samples. This study emphasizes that self-correction is not merely a refinement process; rather, it should enhance the reasoning abilities of models through additional training, enabling them to generate high-quality responses directly without further refinement.</li>
</ul>

<h3>Title: LoRTA: Low Rank Tensor Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Hounie, Charilaos Kanatsoulis, Arnuv Tandon, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04060">https://arxiv.org/abs/2410.04060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04060">https://arxiv.org/pdf/2410.04060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04060]] LoRTA: Low Rank Tensor Adaptation of Large Language Models(https://arxiv.org/abs/2410.04060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning (PEFT) method that effectively adapts large pre-trained models for downstream tasks. LoRA parameterizes model updates using low-rank matrices at each layer, significantly reducing the number of trainable parameters and, consequently, resource requirements during fine-tuning. However, the lower bound on the number of trainable parameters remains high due to the use of the low-rank matrix model. In this paper, we address this limitation by proposing a novel approach that employs a low rank tensor parametrization for model updates. The proposed low rank tensor model can significantly reduce the number of trainable parameters, while also allowing for finer-grained control over adapter size. Our experiments on Natural Language Understanding, Instruction Tuning, Preference Optimization and Protein Folding benchmarks demonstrate that our method is both efficient and effective for fine-tuning large language models, achieving a substantial reduction in the number of parameters while maintaining comparable performance.</li>
</ul>

<h3>Title: Unique ID based Trust Scheme for Improved IoV Wireless Sensor Network Security Against Power Controlled Sybil Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jae-Dong Kim, Dabin Kim, Minseok Ko, Jong-Moon Chung</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04063">https://arxiv.org/abs/2410.04063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04063">https://arxiv.org/pdf/2410.04063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04063]] Unique ID based Trust Scheme for Improved IoV Wireless Sensor Network Security Against Power Controlled Sybil Attacks(https://arxiv.org/abs/2410.04063)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Wireless sensor networks (WSN) are widely used in vehicular networks to support Vehicle-to-Everything (V2X) communications. Wireless sensors in vehicular networks support sensing and monitoring of various environmental factors and vehicle movement, which can help to enhance traffic management, road safety, and transportation efficiency. However, WSNs face security challenges due to their distributed nature and resource limited modules. In Sybil attacks, attackers create multiple fake identities to disrupt network operations (e.g., denial-of-service (DoS)), which is one of the major security concerns in WSNs. Defensive techniques have been proposed, which recently include a received signal strength indicator (RSSI) profiling scheme that improves the performance and is not affected by internal forgeable information. However, even this new RSSI based robust detection scheme was found to be vulnerable when Sybil attackers are mobile or intentionally manipulate their radio transmission power in addition to their device address. In this paper, a unique identification based trust path routing scheme (UITrust) is proposed, which uses the device's physically invariable unique identifiers and routing path trust level estimations to avoid power-controlled Sybil attacks, where the simulation results show the proposed scheme can provide a significant improvement compared to existing schemes.</li>
</ul>

<h3>Title: Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Pesaran Zadeh, Juyeon Kim, Jin-Hwa Kim, Gunhee Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04064">https://arxiv.org/abs/2410.04064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04064">https://arxiv.org/pdf/2410.04064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04064]] Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback(https://arxiv.org/abs/2410.04064)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks. We make the code and dataset available at this https URL.</li>
</ul>

<h3>Title: ECon: On the Detection and Resolution of Evidence Conflicts</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jiayang, Chunkit Chan, Qianqian Zhuang, Lin Qiu, Tianhang Zhang, Tengxiao Liu, Yangqiu Song, Yue Zhang, Pengfei Liu, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04068">https://arxiv.org/abs/2410.04068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04068">https://arxiv.org/pdf/2410.04068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04068]] ECon: On the Detection and Resolution of Evidence Conflicts(https://arxiv.org/abs/2410.04068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rise of large language models (LLMs) has significantly influenced the quality of information in decision-making systems, leading to the prevalence of AI-generated content and challenges in detecting misinformation and managing conflicting information, or "inter-evidence conflicts." This study introduces a method for generating diverse, validated evidence conflicts to simulate real-world misinformation scenarios. We evaluate conflict detection methods, including Natural Language Inference (NLI) models, factual consistency (FC) models, and LLMs, on these conflicts (RQ1) and analyze LLMs' conflict resolution behaviors (RQ2). Our key findings include: (1) NLI and LLM models exhibit high precision in detecting answer conflicts, though weaker models suffer from low recall; (2) FC models struggle with lexically similar answer conflicts, while NLI and LLM models handle these better; and (3) stronger models like GPT-4 show robust performance, especially with nuanced conflicts. For conflict resolution, LLMs often favor one piece of conflicting evidence without justification and rely on internal knowledge if they have prior beliefs.</li>
</ul>

<h3>Title: $\epsilon$-VAE: Denoising as Visual Decoding</h3>
<ul>
<li><strong>Authors: </strong>Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04081">https://arxiv.org/abs/2410.04081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04081">https://arxiv.org/pdf/2410.04081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04081]] $\epsilon$-VAE: Denoising as Visual Decoding(https://arxiv.org/abs/2410.04081)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.</li>
</ul>

<h3>Title: Cross Resolution Encoding-Decoding For Detection Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ashish Kumar, Jaesik Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04088">https://arxiv.org/abs/2410.04088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04088">https://arxiv.org/pdf/2410.04088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04088]] Cross Resolution Encoding-Decoding For Detection Transformers(https://arxiv.org/abs/2410.04088)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Detection Transformers (DETR) are renowned object detection pipelines, however computationally efficient multiscale detection using DETR is still challenging. In this paper, we propose a Cross-Resolution Encoding-Decoding (CRED) mechanism that allows DETR to achieve the accuracy of high-resolution detection while having the speed of low-resolution detection. CRED is based on two modules; Cross Resolution Attention Module (CRAM) and One Step Multiscale Attention (OSMA). CRAM is designed to transfer the knowledge of low-resolution encoder output to a high-resolution feature. While OSMA is designed to fuse multiscale features in a single step and produce a feature map of a desired resolution enriched with multiscale information. When used in prominent DETR methods, CRED delivers accuracy similar to the high-resolution DETR counterpart in roughly 50% fewer FLOPs. Specifically, state-of-the-art DN-DETR, when used with CRED (calling CRED-DETR), becomes 76% faster, with ~50% reduced FLOPs than its high-resolution counterpart with 202 G FLOPs on MS-COCO benchmark. We plan to release pretrained CRED-DETRs for use by the community. Code: this https URL</li>
</ul>

<h3>Title: Designing Concise ConvNets with Columnar Stages</h3>
<ul>
<li><strong>Authors: </strong>Ashish Kumar, Jaesik Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04089">https://arxiv.org/abs/2410.04089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04089">https://arxiv.org/pdf/2410.04089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04089]] Designing Concise ConvNets with Columnar Stages(https://arxiv.org/abs/2410.04089)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the era of vision Transformers, the recent success of VanillaNet shows the huge potential of simple and concise convolutional neural networks (ConvNets). Where such models mainly focus on runtime, it is also crucial to simultaneously focus on other aspects, e.g., FLOPs, parameters, etc, to strengthen their utility further. To this end, we introduce a refreshing ConvNet macro design called Columnar Stage Network (CoSNet). CoSNet has a systematically developed simple and concise structure, smaller depth, low parameter count, low FLOPs, and attention-less operations, well suited for resource-constrained deployment. The key novelty of CoSNet is deploying parallel convolutions with fewer kernels fed by input replication, using columnar stacking of these convolutions, and minimizing the use of 1x1 convolution layers. Our comprehensive evaluations show that CoSNet rivals many renowned ConvNets and Transformer designs under resource-constrained scenarios. Code: this https URL</li>
</ul>

<h3>Title: Cross-Lingual Query-by-Example Spoken Term Detection: A Transformer-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Allahdadi Fatemeh, Mahdian Toroghi Rahil, Zareian Hassan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04091">https://arxiv.org/abs/2410.04091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04091">https://arxiv.org/pdf/2410.04091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04091]] Cross-Lingual Query-by-Example Spoken Term Detection: A Transformer-Based Approach(https://arxiv.org/abs/2410.04091)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Query-by-example spoken term detection (QbE-STD) is typically constrained by transcribed data scarcity and language specificity. This paper introduces a novel, language-agnostic QbE-STD model leveraging image processing techniques and transformer architecture. By employing a pre-trained XLSR-53 network for feature extraction and a Hough transform for detection, our model effectively searches for user-defined spoken terms within any audio file. Experimental results across four languages demonstrate significant performance gains (19-54%) over a CNN-based baseline. While processing time is improved compared to DTW, accuracy remains inferior. Notably, our model offers the advantage of accurately counting query term repetitions within the target audio.</li>
</ul>

<h3>Title: BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts</h3>
<ul>
<li><strong>Authors: </strong>Maria-Eleni Zoumpoulidi, Georgios Paraskevopoulos, Alexandros Potamianos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04094">https://arxiv.org/abs/2410.04094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04094">https://arxiv.org/pdf/2410.04094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04094]] BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts(https://arxiv.org/abs/2410.04094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the continuous progress of Large Language Models (LLMs) across various tasks, their performance on mathematical problems and reasoning tasks remains limited. This limitation can be attributed, among other factors, to the inherent difficulty of these problems and the fact that solutions often consist of multiple steps, potentially of varying nature, making it challenging for a single prompting technique to execute all required steps. To address this, we introduce BloomWise, a new prompting technique, inspired by Bloom's Taxonomy, aiming to improve LLMs' performance in solving such problems by encouraging them to approach the problem starting from simple, i.e., remembering, and progressing to higher cognitive skills, i.e., analyzing, until the correct solution is reached. The decision regarding the need to employ more sophisticated cognitive skills is based on self-evaluation performed by the LLM. Thus, we encourage the LLM to deploy the appropriate cognitive processes. In extensive experiments across 4 popular math reasoning datasets, we have demonstrated the effectiveness of our proposed approach. We also present extensive ablations, analyzing the strengths of each module within our system.</li>
</ul>

<h3>Title: A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Wang, Shiyu Liu, Jianheng Huang, Zheng Wang, Yixuan Liao, Xiaoxin Chen, Junfeng Yao, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04103">https://arxiv.org/abs/2410.04103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04103">https://arxiv.org/pdf/2410.04103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04103]] A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models(https://arxiv.org/abs/2410.04103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to the continuous emergence of new data, version updates have become an indispensable requirement for Large Language Models (LLMs). The training paradigms for version updates of LLMs include pre-training from scratch (PTFS) and continual pre-training (CPT). Preliminary experiments demonstrate that PTFS achieves better pre-training performance, while CPT has lower training cost. Moreover, their performance and training cost gaps widen progressively with version updates. To investigate the underlying reasons for this phenomenon, we analyze the effect of learning rate adjustments during the two stages of CPT: preparing an initialization checkpoint and continual pre-training based on this checkpoint. We find that a large learning rate in the first stage and a complete learning rate decay process in the second stage are crucial for version updates of LLMs. Hence, we propose a learning rate path switching training paradigm. Our paradigm comprises one main path, where we pre-train a LLM with the maximal learning rate, and multiple branching paths, each of which corresponds to an update of the LLM with newly-added training data. Extensive experiments demonstrate the effectiveness and generalization of our paradigm. Particularly, when training four versions of LLMs, our paradigm reduces the total training cost to 58% compared to PTFS, while maintaining comparable pre-training performance.</li>
</ul>

<h3>Title: Exploring LLM-based Data Annotation Strategies for Medical Dialogue Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chengfeng Dou, Ying Zhang, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhengwei Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04112">https://arxiv.org/abs/2410.04112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04112">https://arxiv.org/pdf/2410.04112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04112]] Exploring LLM-based Data Annotation Strategies for Medical Dialogue Preference Alignment(https://arxiv.org/abs/2410.04112)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research examines the use of Reinforcement Learning from AI Feedback (RLAIF) techniques to improve healthcare dialogue models, with the aim of tackling the challenges of preference-aligned data annotation while reducing the reliance on medical experts. We argue that the primary challenges in current RLAIF research for healthcare are the limitations of automated evaluation methods and the difficulties in accurately representing physician preferences. To address these challenges, we present a new evaluation framework based on standardized patient examinations. This framework is designed to objectively assess the effectiveness of large language models (LLMs) in guiding users and following instructions, enabling a comprehensive comparison across different models. Furthermore, our investigation of effective ways to express physician preferences using Constitutional AI algorithms highlighted the particular effectiveness of flowcharts. Utilizing this finding, we introduce an innovative agent-based approach for annotating preference data. This approach autonomously creates medical dialogue flows tailored to the patient's condition, demonstrates strong generalization abilities, and reduces the need for expert involvement. Our results show that the agent-based approach outperforms existing RLAIF annotation methods in standardized patient examinations and surpasses current open source medical dialogue LLMs in various test scenarios.</li>
</ul>

<h3>Title: Rethinking Fair Representation Learning for Performance-Sensitive Tasks</h3>
<ul>
<li><strong>Authors: </strong>Charles Jones, Fabio de Sousa Ribeiro, Mélanie Roschewitz, Daniel C. Castro, Ben Glocker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04120">https://arxiv.org/abs/2410.04120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04120">https://arxiv.org/pdf/2410.04120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04120]] Rethinking Fair Representation Learning for Performance-Sensitive Tasks(https://arxiv.org/abs/2410.04120)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We investigate the prominent class of fair representation learning methods for bias mitigation. Using causal reasoning to define and formalise different sources of dataset bias, we reveal important implicit assumptions inherent to these methods. We prove fundamental limitations on fair representation learning when evaluation data is drawn from the same distribution as training data and run experiments across a range of medical modalities to examine the performance of fair representation learning under distribution shifts. Our results explain apparent contradictions in the existing literature and reveal how rarely considered causal and statistical aspects of the underlying data affect the validity of fair representation learning. We raise doubts about current evaluation practices and the applicability of fair representation learning methods in performance-sensitive settings. We argue that fine-grained analysis of dataset biases should play a key role in the field moving forward.</li>
</ul>

<h3>Title: From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression</h3>
<ul>
<li><strong>Authors: </strong>Eunseong Choi, Sunkyung Lee, Minjin Choi, June Park, Jongwuk Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04139">https://arxiv.org/abs/2410.04139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04139">https://arxiv.org/pdf/2410.04139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04139]] From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression(https://arxiv.org/abs/2410.04139)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved significant performance gains using advanced prompting techniques over various tasks. However, the increasing length of prompts leads to high computational costs and often obscures crucial information. Prompt compression has been proposed to alleviate these issues, but it faces challenges in (i) capturing the global context and (ii) training the compressor effectively. To tackle these challenges, we introduce a novel prompt compression method, namely Reading To Compressing (R2C), utilizing the Fusion-in-Decoder (FiD) architecture to identify the important information in the prompt. Specifically, the cross-attention scores of the FiD are used to discern essential chunks and sentences from the prompt. R2C effectively captures the global context without compromising semantic consistency while detouring the necessity of pseudo-labels for training the compressor. Empirical results show that R2C retains key contexts, enhancing the LLM performance by 6% in out-of-domain evaluations while reducing the prompt length by 80%.</li>
</ul>

<h3>Title: Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher</h3>
<ul>
<li><strong>Authors: </strong>Yong Guo, Shulian Zhang, Haolin Pan, Jing Liu, Yulun Zhang, Jian Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04140">https://arxiv.org/abs/2410.04140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04140">https://arxiv.org/pdf/2410.04140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04140]] Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher(https://arxiv.org/abs/2410.04140)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Knowledge distillation aims to transfer knowledge from a large teacher model to a compact student counterpart, often coming with a significant performance gap between them. We find that a too-large performance gap can hamper the training process, which is also verified in recent studies. To address this, we propose a Gap Preserving Distillation (GPD) method that trains an additional dynamic teacher model from scratch along with training the student to bridge this gap. In this way, it becomes possible to maintain a reasonable performance gap between teacher and student during the whole distillation process. To further strengthen distillation from the dynamic teacher to the student, we develop a hard strategy by enforcing them to share parameters and encouraging parameter inheritance. Besides hard strategy, we also build the soft bidirectional mappings between them which are built on an Inverse Reparameterization (IR) method and a Channel-Branch Reparameterization (CBR) strategy. We highlight that our IR is able to initialize a larger dynamic teacher with an arbitrary expansion ratio, while preserving exactly the same accuracy as the given student model. In this way, it guarantees that the dynamic teacher and student start from the same point and avoid a too large gap in early stage of training. As for our CBR, with parameter-sharing, it directly extracts an effective student model from the well-learned dynamic teacher without any post-training, making our method highly flexible for model deployment. In the experiments, GPD significantly outperforms existing distillation methods on top of both CNNs and transformers architectures, achieving up to 1.58% accuracy improvement. Interestingly, GPD also generalizes well to the scenarios without a pre-trained teacher, including training from scratch and fine-tuning, yielding a large improvement of 1.80% and 0.89% on ResNet18, respectively.</li>
</ul>

<h3>Title: ConDa: Fast Federated Unlearning with Contribution Dampening</h3>
<ul>
<li><strong>Authors: </strong>Vikram S Chundawat, Pushkar Niroula, Prasanna Dhungana, Stefan Schoepf, Murari Mandal, Alexandra Brintrup</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04144">https://arxiv.org/abs/2410.04144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04144">https://arxiv.org/pdf/2410.04144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04144]] ConDa: Fast Federated Unlearning with Contribution Dampening(https://arxiv.org/abs/2410.04144)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has enabled collaborative model training across decentralized data sources or clients. While adding new participants to a shared model does not pose great technical hurdles, the removal of a participant and their related information contained in the shared model remains a challenge. To address this problem, federated unlearning has emerged as a critical research direction, seeking to remove information from globally trained models without harming the model performance on the remaining data. Most modern federated unlearning methods use costly approaches such as the use of remaining clients data to retrain the global model or methods that would require heavy computation on client or server side. We introduce Contribution Dampening (ConDa), a framework that performs efficient unlearning by tracking down the parameters which affect the global model for each client and performs synaptic dampening on the parameters of the global model that have privacy infringing contributions from the forgetting client. Our technique does not require clients data or any kind of retraining and it does not put any computational overhead on either the client or server side. We perform experiments on multiple datasets and demonstrate that ConDa is effective to forget a client's data. In experiments conducted on the MNIST, CIFAR10, and CIFAR100 datasets, ConDa proves to be the fastest federated unlearning method, outperforming the nearest state of the art approach by at least 100x. Our emphasis is on the non-IID Federated Learning setting, which presents the greatest challenge for unlearning. Additionally, we validate ConDa's robustness through backdoor and membership inference attacks. We envision this work as a crucial component for FL in adhering to legal and ethical requirements.</li>
</ul>

<h3>Title: Can the Variation of Model Weights be used as a Criterion for Self-Paced Multilingual NMT?</h3>
<ul>
<li><strong>Authors: </strong>Àlex R. Atrio, Alexis Allemann, Ljiljana Dolamic, Andrei Popescu-Belis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04147">https://arxiv.org/abs/2410.04147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04147">https://arxiv.org/pdf/2410.04147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04147]] Can the Variation of Model Weights be used as a Criterion for Self-Paced Multilingual NMT?(https://arxiv.org/abs/2410.04147)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Many-to-one neural machine translation systems improve over one-to-one systems when training data is scarce. In this paper, we design and test a novel algorithm for selecting the language of minibatches when training such systems. The algorithm changes the language of the minibatch when the weights of the model do not evolve significantly, as measured by the smoothed KL divergence between all layers of the Transformer network. This algorithm outperforms the use of alternating monolingual batches, but not the use of shuffled batches, in terms of translation quality (measured with BLEU and COMET) and convergence speed.</li>
</ul>

<h3>Title: Toxic Subword Pruning for Dialogue Response Generation on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyuan Lu, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04155">https://arxiv.org/abs/2410.04155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04155">https://arxiv.org/pdf/2410.04155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04155]] Toxic Subword Pruning for Dialogue Response Generation on Large Language Models(https://arxiv.org/abs/2410.04155)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How to defend large language models (LLMs) from generating toxic content is an important research area. Yet, most research focused on various model training techniques to remediate LLMs by updating their weights. A typical related research area is safety alignment. This however is often costly and tedious and can expose the model to even more problems such as catastrophic forgetting if the trainings are not carefully handled by experienced NLP practitioners. We thus propose a simple yet effective and novel algorithm, namely \textbf{Tox}ic Subword \textbf{Prun}ing (ToxPrune) to prune the subword contained by the toxic words from BPE in trained LLMs. In contrast to the previous work that demonstrates pruning BPE tokens as harmful to the task of machine translation, we surprisingly found its usefulness in preventing toxic content from being generated on LLMs. Fortunately, our findings suggest that ToxPrune simultaneously improves the toxic language model NSFW-3B on the task of dialogue response generation obviously. We surprisingly found that ToxPrune can even obviously improve official Llama-3.1-6B in the metric of dialogue diversity. Extensive automatic results and human evaluation indicate that ToxPrune could be helpful for both remediating toxic LLMs and improving non-toxic LLMs on the task of dialogue response generation.\footnote{We plan to release the resources to facilitate future work.}</li>
</ul>

<h3>Title: Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Keda Tao, Jinjin Gu, Yulun Zhang, Xiucheng Wang, Nan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04161">https://arxiv.org/abs/2410.04161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04161">https://arxiv.org/pdf/2410.04161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04161]] Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model(https://arxiv.org/abs/2410.04161)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the generation of false facial attributes and identities often associated with generative face restoration methods. By incorporating a dual-control adapter and a two-stage training strategy, our method effectively utilizes multi-modal prior information for targeted restoration tasks. We also present the Reface-HQ dataset, comprising over 23,000 high-resolution facial images across 5,000 identities, to address the need for reference face training images. Our approach achieves superior visual quality in restoring facial details under severe degradation and allows for controlled restoration processes, enhancing the accuracy of identity preservation and attribute correction. Including negative quality samples and attribute prompts in the training further refines the model's ability to generate detailed and perceptually accurate images.</li>
</ul>

<h3>Title: Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling</h3>
<ul>
<li><strong>Authors: </strong>Huije Lee, Hoyun Song, Jisu Shin, Sukmin Cho, SeungYoon Han, Jong C. Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04164">https://arxiv.org/abs/2410.04164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04164">https://arxiv.org/pdf/2410.04164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04164]] Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling(https://arxiv.org/abs/2410.04164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Trolling in online communities typically involves disruptive behaviors such as provoking anger and manipulating discussions, leading to a polarized atmosphere and emotional distress. Robust moderation is essential for mitigating these negative impacts and maintaining a healthy and constructive community atmosphere. However, effectively addressing trolls is difficult because their behaviors vary widely and require different response strategies (RSs) to counter them. This diversity makes it challenging to choose an appropriate RS for each specific situation. To address this challenge, our research investigates whether humans have preferred strategies tailored to different types of trolling behaviors. Our findings reveal a correlation between the types of trolling encountered and the preferred RS. In this paper, we introduce a methodology for generating counter-responses to trolls by recommending appropriate RSs, supported by a dataset aligning these strategies with human preferences across various troll contexts. The experimental results demonstrate that our proposed approach guides constructive discussion and reduces the negative effects of trolls, thereby enhancing the online community environment.</li>
</ul>

<h3>Title: Preference Optimization as Probabilistic Inference</h3>
<ul>
<li><strong>Authors: </strong>Abbas Abdolmaleki, Bilal Piot, Bobak Shahriari, Jost Tobias Springenberg, Tim Hertweck, Rishabh Joshi, Junhyuk Oh, Michael Bloesch, Thomas Lampe, Nicolas Heess, Jonas Buchli, Martin Riedmiller</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04166">https://arxiv.org/abs/2410.04166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04166">https://arxiv.org/pdf/2410.04166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04166]] Preference Optimization as Probabilistic Inference(https://arxiv.org/abs/2410.04166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing preference optimization methods are mainly designed for directly learning from human feedback with the assumption that paired examples (preferred vs. dis-preferred) are available. In contrast, we propose a method that can leverage unpaired preferred or dis-preferred examples, and works even when only one type of feedback (positive or negative) is available. This flexibility allows us to apply it in scenarios with varying forms of feedback and models, including training generative language models based on human feedback as well as training policies for sequential decision-making problems, where learned (value) functions are available. Our approach builds upon the probabilistic framework introduced in (Dayan and Hinton, 1997), which proposes to use expectation-maximization (EM) to directly optimize the probability of preferred outcomes (as opposed to classic expected reward maximization). To obtain a practical algorithm, we identify and address a key limitation in current EM-based methods: when applied to preference optimization, they solely maximize the likelihood of preferred examples, while neglecting dis-preferred samples. We show how one can extend EM algorithms to explicitly incorporate dis-preferred outcomes, leading to a novel, theoretically grounded, preference optimization algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback.</li>
</ul>

<h3>Title: IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Shitong Shao, Zikai Zhou, Lichen Bai, Haoyi Xiond, Zeke Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04171">https://arxiv.org/abs/2410.04171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04171">https://arxiv.org/pdf/2410.04171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04171]] IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis(https://arxiv.org/abs/2410.04171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The multi-step sampling mechanism, a key feature of visual diffusion models, has significant potential to replicate the success of OpenAI's Strawberry in enhancing performance by increasing the inference computational cost. Sufficient prior studies have demonstrated that correctly scaling up computation in the sampling process can successfully lead to improved generation quality, enhanced image editing, and compositional generalization. While there have been rapid advancements in developing inference-heavy algorithms for improved image generation, relatively little work has explored inference scaling laws in video diffusion models (VDMs). Furthermore, existing research shows only minimal performance gains that are perceptible to the naked eye. To address this, we design a novel training-free algorithm IV-Mixed Sampler that leverages the strengths of image diffusion models (IDMs) to assist VDMs surpass their current capabilities. The core of IV-Mixed Sampler is to use IDMs to significantly enhance the quality of each video frame and VDMs ensure the temporal coherence of the video during the sampling process. Our experiments have demonstrated that IV-Mixed Sampler achieves state-of-the-art performance on 4 benchmarks including UCF-101-FVD, MSR-VTT-FVD, Chronomagic-Bench-150, and Chronomagic-Bench-1649. For example, the open-source Animatediff with IV-Mixed Sampler reduces the UMT-FVD score from 275.2 to 228.6, closing to 223.1 from the closed-source Pika-2.0.</li>
</ul>

<h3>Title: DiDOTS: Knowledge Distillation from Large-Language-Models for Dementia Obfuscation in Transcribed Speech</h3>
<ul>
<li><strong>Authors: </strong>Dominika Woszczyk, Soteris Demetriou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04188">https://arxiv.org/abs/2410.04188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04188">https://arxiv.org/pdf/2410.04188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04188]] DiDOTS: Knowledge Distillation from Large-Language-Models for Dementia Obfuscation in Transcribed Speech(https://arxiv.org/abs/2410.04188)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Dementia is a sensitive neurocognitive disorder affecting tens of millions of people worldwide and its cases are expected to triple by 2050. Alarmingly, recent advancements in dementia classification make it possible for adversaries to violate affected individuals' privacy and infer their sensitive condition from speech transcriptions. Existing obfuscation methods in text have never been applied for dementia and depend on the availability of large labeled datasets which are challenging to collect for sensitive medical attributes. In this work, we bridge this research gap and tackle the above issues by leveraging Large-Language-Models (LLMs) with diverse prompt designs (zero-shot, few-shot, and knowledge-based) to obfuscate dementia in speech transcripts. Our evaluation shows that LLMs are more effective dementia obfuscators compared to competing methods. However, they have billions of parameters which renders them hard to train, store and share, and they are also fragile suffering from hallucination, refusal and contradiction effects among others. To further mitigate these, we propose a novel method, DiDOTS. DiDOTS distills knowledge from LLMs using a teacher-student paradigm and parameter-efficient fine-tuning. DiDOTS has one order of magnitude fewer parameters compared to its teacher LLM and can be fine-tuned using three orders of magnitude less parameters compared to full fine-tuning. Our evaluation shows that compared to prior work DiDOTS retains the performance of LLMs achieving 1.3x and 2.2x improvement in privacy performance on two datasets, while humans rate it as better in preserving utility even when compared to state-of-the-art paraphrasing models.</li>
</ul>

<h3>Title: Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiting Dong, Guobin Shen, Dongcheng Zhao, Xiang He, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04190">https://arxiv.org/abs/2410.04190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04190">https://arxiv.org/pdf/2410.04190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04190]] Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models(https://arxiv.org/abs/2410.04190)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) remain vulnerable to jailbreak attacks that bypass their safety mechanisms. Existing attack methods are fixed or specifically tailored for certain models and cannot flexibly adjust attack strength, which is critical for generalization when attacking models of various sizes. We introduce a novel scalable jailbreak attack that preempts the activation of an LLM's safety policies by occupying its computational resources. Our method involves engaging the LLM in a resource-intensive preliminary task - a Character Map lookup and decoding process - before presenting the target instruction. By saturating the model's processing capacity, we prevent the activation of safety protocols when processing the subsequent instruction. Extensive experiments on state-of-the-art LLMs demonstrate that our method achieves a high success rate in bypassing safety measures without requiring gradient access, manual prompt engineering. We verified our approach offers a scalable attack that quantifies attack strength and adapts to different model scales at the optimal strength. We shows safety policies of LLMs might be more susceptible to resource constraints. Our findings reveal a critical vulnerability in current LLM safety designs, highlighting the need for more robust defense strategies that account for resource-intense condition.</li>
</ul>

<h3>Title: Accelerating Diffusion Models with One-to-Many Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Linfeng Zhang, Kaisheng Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04191">https://arxiv.org/abs/2410.04191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04191">https://arxiv.org/pdf/2410.04191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04191]] Accelerating Diffusion Models with One-to-Many Knowledge Distillation(https://arxiv.org/abs/2410.04191)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Significant advancements in image generation have been made with diffusion models. Nevertheless, when contrasted with previous generative models, diffusion models face substantial computational overhead, leading to failure in real-time generation. Recent approaches have aimed to accelerate diffusion models by reducing the number of sampling steps through improved sampling techniques or step distillation. However, the methods to diminish the computational cost for each timestep remain a relatively unexplored area. Observing the fact that diffusion models exhibit varying input distributions and feature distributions at different timesteps, we introduce one-to-many knowledge distillation (O2MKD), which distills a single teacher diffusion model into multiple student diffusion models, where each student diffusion model is trained to learn the teacher's knowledge for a subset of continuous timesteps. Experiments on CIFAR10, LSUN Church, CelebA-HQ with DDPM and COCO30K with Stable Diffusion show that O2MKD can be applied to previous knowledge distillation and fast sampling methods to achieve significant acceleration. Codes will be released in Github.</li>
</ul>

<h3>Title: Parametric Taylor series based latent dynamics identification neural networks</h3>
<ul>
<li><strong>Authors: </strong>Xinlei Lin, Dunhui Xiao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04193">https://arxiv.org/abs/2410.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04193">https://arxiv.org/pdf/2410.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04193]] Parametric Taylor series based latent dynamics identification neural networks(https://arxiv.org/abs/2410.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Numerical solving parameterised partial differential equations (P-PDEs) is highly practical yet computationally expensive, driving the development of reduced-order models (ROMs). Recently, methods that combine latent space identification techniques with deep learning algorithms (e.g., autoencoders) have shown great potential in describing the dynamical system in the lower dimensional latent space, for example, LaSDI, gLaSDI and GPLaSDI. In this paper, a new parametric latent identification of nonlinear dynamics neural networks, P-TLDINets, is introduced, which relies on a novel neural network structure based on Taylor series expansion and ResNets to learn the ODEs that govern the reduced space dynamics. During the training process, Taylor series-based Latent Dynamic Neural Networks (TLDNets) and identified equations are trained simultaneously to generate a smoother latent space. In order to facilitate the parameterised study, a $k$-nearest neighbours (KNN) method based on an inverse distance weighting (IDW) interpolation scheme is introduced to predict the identified ODE coefficients using local information. Compared to other latent dynamics identification methods based on autoencoders, P-TLDINets remain the interpretability of the model. Additionally, it circumvents the building of explicit autoencoders, avoids dependency on specific grids, and features a more lightweight structure, which is easy to train with high generalisation capability and accuracy. Also, it is capable of using different scales of meshes. P-TLDINets improve training speeds nearly hundred times compared to GPLaSDI and gLaSDI, maintaining an $L_2$ error below $2\%$ compared to high-fidelity models.</li>
</ul>

<h3>Title: Consistent Autoformalization for Constructing Mathematical Libraries</h3>
<ul>
<li><strong>Authors: </strong>Lan Zhang, Xin Quan, Andre Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04194">https://arxiv.org/abs/2410.04194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04194">https://arxiv.org/pdf/2410.04194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04194]] Consistent Autoformalization for Constructing Mathematical Libraries(https://arxiv.org/abs/2410.04194)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types.</li>
</ul>

<h3>Title: CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints</h3>
<ul>
<li><strong>Authors: </strong>Anirudh Atmakuru, Jatin Nainani, Rohith Siddhartha Reddy Bheemreddy, Anirudh Lakkaraju, Zonghai Yao, Hamed Zamani, Haw-Shiuan Chang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04197">https://arxiv.org/abs/2410.04197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04197">https://arxiv.org/pdf/2410.04197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04197]] CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints(https://arxiv.org/abs/2410.04197)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating the creativity of large language models (LLMs) in story writing is difficult because LLM-generated stories could seemingly look creative but be very similar to some existing stories in their huge and proprietary training corpus. To overcome this challenge, we introduce a novel benchmark dataset with varying levels of prompt specificity: CS4 ($\mathbf{C}$omparing the $\mathbf{S}$kill of $\mathbf{C}$reating $\mathbf{S}$tories by $\mathbf{C}$ontrolling the $\mathbf{S}$ynthesized $\mathbf{C}$onstraint $\mathbf{S}$pecificity). By increasing the number of requirements/constraints in the prompt, we can increase the prompt specificity and hinder LLMs from retelling high-quality narratives in their training data. Consequently, CS4 empowers us to indirectly measure the LLMs' creativity without human annotations. Our experiments on LLaMA, Gemma, and Mistral not only highlight the creativity challenges LLMs face when dealing with highly specific prompts but also reveal that different LLMs perform very differently under different numbers of constraints and achieve different balances between the model's instruction-following ability and narrative coherence. Additionally, our experiments on OLMo suggest that Learning from Human Feedback (LHF) can help LLMs select better stories from their training data but has limited influence in boosting LLMs' ability to produce creative stories that are unseen in the training corpora. The benchmark is released at this https URL.</li>
</ul>

<h3>Title: LongGenBench: Long-context Generation Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xiang Liu, Peijie Dong, Xuming Hu, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04199">https://arxiv.org/abs/2410.04199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04199">https://arxiv.org/pdf/2410.04199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04199]] LongGenBench: Long-context Generation Benchmark(https://arxiv.org/abs/2410.04199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.</li>
</ul>

<h3>Title: IT$^3$: Idempotent Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Nikita Durasov, Assaf Shocher, Doruk Oner, Gal Chechik, Alexei A. Efros, Pascal Fua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04201">https://arxiv.org/abs/2410.04201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04201">https://arxiv.org/pdf/2410.04201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04201]] IT$^3$: Idempotent Test-Time Training(https://arxiv.org/abs/2410.04201)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces Idempotent Test-Time Training (IT$^3$), a novel approach to addressing the challenge of distribution shift. While supervised-learning methods assume matching train and test distributions, this is rarely the case for machine learning systems deployed in the real world. Test-Time Training (TTT) approaches address this by adapting models during inference, but they are limited by a domain specific auxiliary task. IT$^3$ is based on the universal property of idempotence. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, that is $f(f(x))=f(x)$. At training, the model receives an input $x$ along with another signal that can either be the ground truth label $y$ or a neutral "don't know" signal $0$. At test time, the additional signal can only be $0$. When sequentially applying the model, first predicting $y_0 = f(x, 0)$ and then $y_1 = f(x, y_0)$, the distance between $y_0$ and $y_1$ measures certainty and indicates out-of-distribution input $x$ if high. We use this distance, that can be expressed as $||f(x, f(x, 0)) - f(x, 0)||$ as our TTT loss during inference. By carefully optimizing this objective, we effectively train $f(x,\cdot)$ to be idempotent, projecting the internal representation of the input onto the training distribution. We demonstrate the versatility of our approach across various tasks, including corrupted image classification, aerodynamic predictions, tabular data with missing information, age prediction from face, and large-scale aerial photo segmentation. Moreover, these tasks span different architectures such as MLPs, CNNs, and GNNs.</li>
</ul>

<h3>Title: Exploring Strengths and Weaknesses of Super-Resolution Attack in Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Davide Alessandro Coccomini, Roberto Caldelli, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04205">https://arxiv.org/abs/2410.04205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04205">https://arxiv.org/pdf/2410.04205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04205]] Exploring Strengths and Weaknesses of Super-Resolution Attack in Deepfake Detection(https://arxiv.org/abs/2410.04205)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Image manipulation is rapidly evolving, allowing the creation of credible content that can be used to bend reality. Although the results of deepfake detectors are promising, deepfakes can be made even more complicated to detect through adversarial attacks. They aim to further manipulate the image to camouflage deepfakes' artifacts or to insert signals making the image appear pristine. In this paper, we further explore the potential of super-resolution attacks based on different super-resolution techniques and with different scales that can impact the performance of deepfake detectors with more or less intensity. We also evaluated the impact of the attack on more diverse datasets discovering that the super-resolution process is effective in hiding the artifacts introduced by deepfake generation models but fails in hiding the traces contained in fully synthetic images. Finally, we propose some changes to the detectors' training process to improve their robustness to this kind of attack.</li>
</ul>

<h3>Title: Learning on LoRAs: GL-Equivariant Processing of Low-Rank Weight Spaces for Large Finetuned Models</h3>
<ul>
<li><strong>Authors: </strong>Theo (Moe)Putterman, Derek Lim, Yoav Gelberg, Stefanie Jegelka, Haggai Maron</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04207">https://arxiv.org/abs/2410.04207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04207">https://arxiv.org/pdf/2410.04207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04207]] Learning on LoRAs: GL-Equivariant Processing of Low-Rank Weight Spaces for Large Finetuned Models(https://arxiv.org/abs/2410.04207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Low-rank adaptations (LoRAs) have revolutionized the finetuning of large foundation models, enabling efficient adaptation even with limited computational resources. The resulting proliferation of LoRAs presents exciting opportunities for applying machine learning techniques that take these low-rank weights themselves as inputs. In this paper, we investigate the potential of Learning on LoRAs (LoL), a paradigm where LoRA weights serve as input to machine learning models. For instance, an LoL model that takes in LoRA weights as inputs could predict the performance of the finetuned model on downstream tasks, detect potentially harmful finetunes, or even generate novel model edits without traditional training methods. We first identify the inherent parameter symmetries of low rank decompositions of weights, which differ significantly from the parameter symmetries of standard neural networks. To efficiently process LoRA weights, we develop several symmetry-aware invariant or equivariant LoL models, using tools such as canonicalization, invariant featurization, and equivariant layers. We finetune thousands of text-to-image diffusion models and language models to collect datasets of LoRAs. In numerical experiments on these datasets, we show that our LoL architectures are capable of processing low rank weight decompositions to predict CLIP score, finetuning data attributes, finetuning data membership, and accuracy on downstream tasks.</li>
</ul>

<h3>Title: Equivariant Neural Functional Networks for Transformers</h3>
<ul>
<li><strong>Authors: </strong>Viet-Hoang Tran, Thieu N. Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, Tan Minh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04209">https://arxiv.org/abs/2410.04209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04209">https://arxiv.org/pdf/2410.04209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04209]] Equivariant Neural Functional Networks for Transformers(https://arxiv.org/abs/2410.04209)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper systematically explores neural functional networks (NFN) for transformer architectures. NFN are specialized neural networks that treat the weights, gradients, or sparsity patterns of a deep neural network (DNN) as input data and have proven valuable for tasks such as learnable optimizers, implicit data representations, and weight editing. While NFN have been extensively developed for MLP and CNN, no prior work has addressed their design for transformers, despite the importance of transformers in modern deep learning. This paper aims to address this gap by providing a systematic study of NFN for transformers. We first determine the maximal symmetric group of the weights in a multi-head attention module as well as a necessary and sufficient condition under which two sets of hyperparameters of the multi-head attention module define the same function. We then define the weight space of transformer architectures and its associated group action, which leads to the design principles for NFN in transformers. Based on these, we introduce Transformer-NFN, an NFN that is equivariant under this group action. Additionally, we release a dataset of more than 125,000 Transformers model checkpoints trained on two datasets with two different tasks, providing a benchmark for evaluating Transformer-NFN and encouraging further research on transformer training and performance.</li>
</ul>

<h3>Title: Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension</h3>
<ul>
<li><strong>Authors: </strong>Ning Wang, Zekun Li, Tongxin Bai, Guoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04211">https://arxiv.org/abs/2410.04211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04211">https://arxiv.org/pdf/2410.04211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04211]] Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension(https://arxiv.org/abs/2410.04211)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modeling long sequences is crucial for various large-scale models; however, extending existing architectures to handle longer sequences presents significant technical and resource challenges. In this paper, we propose an efficient and flexible attention architecture that enables the extension of context lengths in large language models with reduced computational resources and fine-tuning time compared to other excellent methods. Specifically, we introduce correlation-aware selection and merging mechanisms to facilitate efficient sparse attention. In addition, we also propose a novel data augmentation technique involving positional encodings to enhance generalization to unseen positions. The results are as follows: First, using a single A100, we achieve fine-tuning on Llama2-7B with a sequence length of 32K, which is more efficient than other methods that rely on subsets for regression. Second, we present a comprehensive method for extending context lengths across the pre-training, fine-tuning, and inference phases. During pre-training, our attention mechanism partially breaks translation invariance during token selection, so we apply positional encodings only to the selected tokens. This approach achieves relatively high performance and significant extrapolation capabilities. For fine-tuning, we introduce Cyclic, Randomly Truncated, and Dynamically Growing NTK Positional Embedding (CRD NTK). This design allows fine-tuning with a sequence length of only 16K, enabling models such as Llama2-7B and Mistral-7B to perform inference with context lengths of up to 1M or even arbitrary lengths. Our method achieves 100\% accuracy on the passkey task with a context length of 4M and maintains stable perplexity at a 1M context length. This represents at least a 64-fold reduction in resource requirements compared to traditional full-attention mechanisms, while still achieving competitive performance.</li>
</ul>

<h3>Title: TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Liu, Xingchao Yang, Tomoya Akiyama, Yuantian Huang, Qiaoge Li, Shigeru Kuriyama, Takafumi Taketomi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04221">https://arxiv.org/abs/2410.04221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04221">https://arxiv.org/pdf/2410.04221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04221]] TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation(https://arxiv.org/abs/2410.04221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, (i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between speech audio and gesture motion, we implement a hierarchical joint embedding space (AuMoCLIP); (ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our codes and pretrained models are available: \url{this https URL}</li>
</ul>

<h3>Title: Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning</h3>
<ul>
<li><strong>Authors: </strong>Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04223">https://arxiv.org/abs/2410.04223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04223">https://arxiv.org/pdf/2410.04223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04223]] Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning(https://arxiv.org/abs/2410.04223)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.</li>
</ul>

<h3>Title: Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Jianze Li, Jiezhang Cao, Zichen Zou, Xiongfei Su, Xin Yuan, Yulun Zhang, Yong Guo, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04224">https://arxiv.org/abs/2410.04224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04224">https://arxiv.org/pdf/2410.04224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04224]] Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution(https://arxiv.org/abs/2410.04224)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have been achieving excellent performance for real-world image super-resolution (Real-ISR) with considerable computational costs. Current approaches are trying to derive one-step diffusion models from multi-step counterparts through knowledge distillation. However, these methods incur substantial training costs and may constrain the performance of the student model by the teacher's limitations. To tackle these issues, we propose DFOSD, a Distillation-Free One-Step Diffusion model. Specifically, we propose a noise-aware discriminator (NAD) to participate in adversarial training, further enhancing the authenticity of the generated content. Additionally, we improve the perceptual loss with edge-aware DISTS (EA-DISTS) to enhance the model's ability to generate fine details. Our experiments demonstrate that, compared with previous diffusion-based methods requiring dozens or even hundreds of steps, our DFOSD attains comparable or even superior results in both quantitative metrics and qualitative evaluations. Our DFOSD also abtains higher performance and efficiency compared with other one-step diffusion methods. We will release code and models at \url{this https URL}.</li>
</ul>

<h3>Title: Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zi Wang, Divyam Anshumaan, Ashish Hooda, Yudong Chen, Somesh Jha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04234">https://arxiv.org/abs/2410.04234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04234">https://arxiv.org/pdf/2410.04234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04234]] Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks(https://arxiv.org/abs/2410.04234)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Optimization methods are widely employed in deep learning to identify and mitigate undesired model responses. While gradient-based techniques have proven effective for image models, their application to language models is hindered by the discrete nature of the input space. This study introduces a novel optimization approach, termed the \emph{functional homotopy} method, which leverages the functional duality between model training and input generation. By constructing a series of easy-to-hard optimization problems, we iteratively solve these problems using principles derived from established homotopy methods. We apply this approach to jailbreak attack synthesis for large language models (LLMs), achieving a $20\%-30\%$ improvement in success rate over existing methods in circumventing established safe open-source models such as Llama-2 and Llama-3.</li>
</ul>

<h3>Title: Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations</h3>
<ul>
<li><strong>Authors: </strong>Sagi Shaier, Ari Kobren, Philip Ogren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04241">https://arxiv.org/abs/2410.04241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04241">https://arxiv.org/pdf/2410.04241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04241]] Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations(https://arxiv.org/abs/2410.04241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Resolving knowledge conflicts is a crucial challenge in Question Answering (QA) tasks, as the internet contains numerous conflicting facts and opinions. While some research has made progress in tackling ambiguous settings where multiple valid answers exist, these approaches often neglect to provide source citations, leaving users to evaluate the factuality of each answer. On the other hand, existing work on citation generation has focused on unambiguous settings with single answers, failing to address the complexity of real-world scenarios. Despite the importance of both aspects, no prior research has combined them, leaving a significant gap in the development of QA systems. In this work, we bridge this gap by proposing the novel task of QA with source citation in ambiguous settings, where multiple valid answers exist. To facilitate research in this area, we create a comprehensive framework consisting of: (1) five novel datasets, obtained by augmenting three existing reading comprehension datasets with citation meta-data across various ambiguous settings, such as distractors and paraphrasing; (2) the first ambiguous multi-hop QA dataset featuring real-world, naturally occurring contexts; (3) two new metrics to evaluate models' performances; and (4) several strong baselines using rule-based, prompting, and finetuning approaches over five large language models. We hope that this new task, datasets, metrics, and baselines will inspire the community to push the boundaries of QA research and develop more trustworthy and interpretable systems.</li>
</ul>

<h3>Title: Implicit to Explicit Entropy Regularization: Benchmarking ViT Fine-tuning under Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Maria Marrium, Arif Mahmood, Mohammed Bennamoun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04256">https://arxiv.org/abs/2410.04256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04256">https://arxiv.org/pdf/2410.04256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04256]] Implicit to Explicit Entropy Regularization: Benchmarking ViT Fine-tuning under Noisy Labels(https://arxiv.org/abs/2410.04256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Automatic annotation of large-scale datasets can introduce noisy training data labels, which adversely affect the learning process of deep neural networks (DNNs). Consequently, Noisy Labels Learning (NLL) has become a critical research field for Convolutional Neural Networks (CNNs), though it remains less explored for Vision Transformers (ViTs). In this study, we evaluate the vulnerability of ViT fine-tuning to noisy labels and compare its robustness with CNNs. We also investigate whether NLL methods developed for CNNs are equally effective for ViTs. Using linear probing and MLP-K fine-tuning, we benchmark two ViT backbones (ViT-B/16 and ViT-L/16) using three commonly used classification losses: Cross Entropy (CE), Focal Loss (FL), and Mean Absolute Error (MAE), alongside six robust NLL methods: GCE, SCE, NLNL, APL, NCE+AGCE, and ANL-CE. The evaluation is conducted across six datasets including MNIST, CIFAR-10/100, WebVision, Clothing1M, and Food-101N. Furthermore, we explore whether implicit prediction entropy minimization contributes to ViT robustness against noisy labels, noting a general trend of prediction entropy reduction across most NLL methods. Building on this observation, we examine whether explicit entropy minimization could enhance ViT resilience to noisy labels. Our findings indicate that incorporating entropy regularization enhances the performance of established loss functions such as CE and FL, as well as the robustness of the six studied NLL methods across both ViT backbones.</li>
</ul>

<h3>Title: DeFoG: Discrete Flow Matching for Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiming Qin, Manuel Madeira, Dorina Thanou, Pascal Frossard</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04263">https://arxiv.org/abs/2410.04263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04263">https://arxiv.org/pdf/2410.04263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04263]] DeFoG: Discrete Flow Matching for Graph Generation(https://arxiv.org/abs/2410.04263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Graph generation is fundamental in diverse scientific applications, due to its ability to reveal the underlying distribution of complex data, and eventually generate new, realistic data points. Despite the success of diffusion models in this domain, those face limitations in sampling efficiency and flexibility, stemming from the tight coupling between the training and sampling stages. To address this, we propose DeFoG, a novel framework using discrete flow matching for graph generation. DeFoG employs a flow-based approach that features an efficient linear interpolation noising process and a flexible denoising process based on a continuous-time Markov chain formulation. We leverage an expressive graph transformer and ensure desirable node permutation properties to respect graph symmetry. Crucially, our framework enables a disentangled design of the training and sampling stages, enabling more effective and efficient optimization of model performance. We navigate this design space by introducing several algorithmic improvements that boost the model performance, consistently surpassing existing diffusion models. We also theoretically demonstrate that, for general discrete data, discrete flow models can faithfully replicate the ground truth distribution - a result that naturally extends to graph data and reinforces DeFoG's foundations. Extensive experiments show that DeFoG achieves state-of-the-art results on synthetic and molecular datasets, improving both training and sampling efficiency over diffusion models, and excels in conditional generation on a digital pathology dataset.</li>
</ul>

<h3>Title: AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text</h3>
<ul>
<li><strong>Authors: </strong>Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04265">https://arxiv.org/abs/2410.04265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04265">https://arxiv.org/pdf/2410.04265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04265]] AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text(https://arxiv.org/abs/2410.04265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains.</li>
</ul>

<h3>Title: Constructing Cloze Questions Generatively</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Sun (1), Jie Wang (2)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04266">https://arxiv.org/abs/2410.04266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04266">https://arxiv.org/pdf/2410.04266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04266]] Constructing Cloze Questions Generatively(https://arxiv.org/abs/2410.04266)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We present a generative method called CQG for constructing cloze questions from a given article using neural networks and WordNet, with an emphasis on generating multigram distractors. Built on sense disambiguation, text-to-text transformation, WordNet's synset taxonomies and lexical labels, CQG selects an answer key for a given sentence, segments it into a sequence of instances, generates instance-level distractor candidates (IDCs) using a transformer and sibling this http URL then removes inappropriate IDCs, ranks the remaining IDCs based on contextual embedding similarities, as well as synset and lexical relatedness, forms distractor candidates by combinatorially replacing instances with the corresponding top-ranked IDCs, and checks if they are legitimate phrases. Finally, it selects top-ranked distractor candidates based on contextual semantic similarities to the answer key. Experiments show that this method significantly outperforms SOTA results. Human judges also confirm the high qualities of the generated distractors.</li>
</ul>

<h3>Title: RoQLlama: A Lightweight Romanian Adapted Language Model</h3>
<ul>
<li><strong>Authors: </strong>George-Andrei Dima, Andrei-Marius Avram, Cristian-George Crăciun, Dumitru-Clementin Cercel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04269">https://arxiv.org/abs/2410.04269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04269">https://arxiv.org/pdf/2410.04269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04269]] RoQLlama: A Lightweight Romanian Adapted Language Model(https://arxiv.org/abs/2410.04269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The remarkable achievements obtained by open-source large language models (LLMs) in recent years have predominantly been concentrated on tasks involving the English language. In this paper, we aim to advance the performance of Llama2 models on Romanian tasks. We tackle the problem of reduced computing resources by using QLoRA for training. We release RoQLlama-7b, a quantized LLM, which shows equal or improved results compared to its full-sized counterpart when tested on seven Romanian downstream tasks in the zero-shot setup. Also, it consistently achieves higher average scores across all few-shot prompts. Additionally, we introduce a novel Romanian dataset, namely RoMedQA, which contains single-choice medical questions in Romanian.</li>
</ul>

<h3>Title: Fundamental Limitations on Subquadratic Alternatives to Transformers</h3>
<ul>
<li><strong>Authors: </strong>Josh Alman, Hantao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CC, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04271">https://arxiv.org/abs/2410.04271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04271">https://arxiv.org/pdf/2410.04271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04271]] Fundamental Limitations on Subquadratic Alternatives to Transformers(https://arxiv.org/abs/2410.04271)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models such as Mamba were designed to replace attention with an almost linear time alternative. In this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time - whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason - cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.</li>
</ul>

<h3>Title: Mechanistic Behavior Editing of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Joykirat Singh, Subhabrata Dutta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04277">https://arxiv.org/abs/2410.04277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04277">https://arxiv.org/pdf/2410.04277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04277]] Mechanistic Behavior Editing of Language Models(https://arxiv.org/abs/2410.04277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models trained on web-scale text acquire language generation abilities that can solve a wide range of tasks, particularly when task knowledge is refined into the generative prior using in-context examples. However, spurious features learned from noisy data hinder their generalizability. Supervised finetuning can introduce task specificity, but introduce data inefficiency. Prior studies indicate that (i) noisy neural circuitries coexist with generalizable ones within LLMs, and (ii) finetuning typically enhances (or suppresses) existing abilities without introducing newer ones. Building upon these, we propose TaRot, a novel method for task adaptation. TaRot intervenes in the neural circuitries using learnable rotation matrices that are optimized using Bayesian Optimization, on labelled samples in the order of standard few-shot prompting examples. Experiments on multiple classification and generation tasks using LLMs of varying sizes reveal the efficacy of TaRot, improving upon both zero- as well as few-shot performance, with average improvements (across models and tasks) of 23.81% and 11.15%, respectively. The source code is available at this https URL</li>
</ul>

<h3>Title: Black Boxes and Looking Glasses: Multilevel Symmetries, Reflection Planes, and Convex Optimization in Deep Networks</h3>
<ul>
<li><strong>Authors: </strong>Emi Zeger, Mert Pilanci</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04279">https://arxiv.org/abs/2410.04279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04279">https://arxiv.org/pdf/2410.04279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04279]] Black Boxes and Looking Glasses: Multilevel Symmetries, Reflection Planes, and Convex Optimization in Deep Networks(https://arxiv.org/abs/2410.04279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We show that training deep neural networks (DNNs) with absolute value activation and arbitrary input dimension can be formulated as equivalent convex Lasso problems with novel features expressed using geometric algebra. This formulation reveals geometric structures encoding symmetry in neural networks. Using the equivalent Lasso form of DNNs, we formally prove a fundamental distinction between deep and shallow networks: deep networks inherently favor symmetric structures in their fitted functions, with greater depth enabling multilevel symmetries, i.e., symmetries within symmetries. Moreover, Lasso features represent distances to hyperplanes that are reflected across training points. These reflection hyperplanes are spanned by training data and are orthogonal to optimal weight vectors. Numerical experiments support theory and demonstrate theoretically predicted features when training networks using embeddings generated by Large Language Models.</li>
</ul>

<h3>Title: Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis</h3>
<ul>
<li><strong>Authors: </strong>Mengfang Sun, Wenying Sun, Ying Sun, Shaobo Liu, Mohan Jiang, Zhen Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04283">https://arxiv.org/abs/2410.04283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04283">https://arxiv.org/pdf/2410.04283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04283]] Applying Hybrid Graph Neural Networks to Strengthen Credit Risk Analysis(https://arxiv.org/abs/2410.04283)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to credit risk prediction by employing Graph Convolutional Neural Networks (GCNNs) to assess the creditworthiness of borrowers. Leveraging the power of big data and artificial intelligence, the proposed method addresses the challenges faced by traditional credit risk assessment models, particularly in handling imbalanced datasets and extracting meaningful features from complex relationships. The paper begins by transforming raw borrower data into graph-structured data, where borrowers and their relationships are represented as nodes and edges, respectively. A classic subgraph convolutional model is then applied to extract local features, followed by the introduction of a hybrid GCNN model that integrates both local and global convolutional operators to capture a comprehensive representation of node features. The hybrid model incorporates an attention mechanism to adaptively select features, mitigating issues of over-smoothing and insufficient feature consideration. The study demonstrates the potential of GCNNs in improving the accuracy of credit risk prediction, offering a robust solution for financial institutions seeking to enhance their lending decision-making processes.</li>
</ul>

<h3>Title: Unveiling the Impact of Local Homophily on GNN Fairness: In-Depth Analysis and New Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Donald Loveland, Danai Koutra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04287">https://arxiv.org/abs/2410.04287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04287">https://arxiv.org/pdf/2410.04287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04287]] Unveiling the Impact of Local Homophily on GNN Fairness: In-Depth Analysis and New Benchmarks(https://arxiv.org/abs/2410.04287)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) often struggle to generalize when graphs exhibit both homophily (same-class connections) and heterophily (different-class connections). Specifically, GNNs tend to underperform for nodes with local homophily levels that differ significantly from the global homophily level. This issue poses a risk in user-centric applications where underrepresented homophily levels are present. Concurrently, fairness within GNNs has received substantial attention due to the potential amplification of biases via message passing. However, the connection between local homophily and fairness in GNNs remains underexplored. In this work, we move beyond global homophily and explore how local homophily levels can lead to unfair predictions. We begin by formalizing the challenge of fair predictions for underrepresented homophily levels as an out-of-distribution (OOD) problem. We then conduct a theoretical analysis that demonstrates how local homophily levels can alter predictions for differing sensitive attributes. We additionally introduce three new GNN fairness benchmarks, as well as a novel semi-synthetic graph generator, to empirically study the OOD problem. Across extensive analysis we find that two factors can promote unfairness: (a) OOD distance, and (b) heterophilous nodes situated in homophilous graphs. In cases where these two conditions are met, fairness drops by up to 24% on real world datasets, and 30% in semi-synthetic datasets. Together, our theoretical insights, empirical analysis, and algorithmic contributions unveil a previously overlooked source of unfairness rooted in the graph's homophily information.</li>
</ul>

<h3>Title: Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods</h3>
<ul>
<li><strong>Authors: </strong>Daniel Otero, Rafael Mateus, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04289">https://arxiv.org/abs/2410.04289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04289">https://arxiv.org/pdf/2410.04289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04289]] Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods(https://arxiv.org/abs/2410.04289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate anomaly detection is critical in vision-based infrastructure inspection, where it helps prevent costly failures and enhances safety. Self-Supervised Learning (SSL) offers a promising approach by learning robust representations from unlabeled data. However, its application in anomaly detection remains underexplored. This paper addresses this gap by providing a comprehensive evaluation of SSL methods for real-world anomaly detection, focusing on sewer infrastructure. Using the Sewer-ML dataset, we evaluate lightweight models such as ViT-Tiny and ResNet-18 across SSL frameworks, including BYOL, Barlow Twins, SimCLR, DINO, and MAE, under varying class imbalance levels. Through 250 experiments, we rigorously assess the performance of these SSL methods to ensure a robust and comprehensive evaluation. Our findings highlight the superiority of joint-embedding methods like SimCLR and Barlow Twins over reconstruction-based approaches such as MAE, which struggle to maintain performance under class imbalance. Furthermore, we find that the SSL model choice is more critical than the backbone architecture. Additionally, we emphasize the need for better label-free assessments of SSL representations, as current methods like RankMe fail to adequately evaluate representation quality, making cross-validation without labels infeasible. Despite the remaining performance gap between SSL and supervised models, these findings highlight the potential of SSL to enhance anomaly detection, paving the way for further research in this underexplored area of SSL applications.</li>
</ul>

<h3>Title: Integrating Physics-Informed Deep Learning and Numerical Methods for Robust Dynamics Discovery and Parameter Estimation</h3>
<ul>
<li><strong>Authors: </strong>Caitlin Ho, Andrea Arnold</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04299">https://arxiv.org/abs/2410.04299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04299">https://arxiv.org/pdf/2410.04299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04299]] Integrating Physics-Informed Deep Learning and Numerical Methods for Robust Dynamics Discovery and Parameter Estimation(https://arxiv.org/abs/2410.04299)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Incorporating a priori physics knowledge into machine learning leads to more robust and interpretable algorithms. In this work, we combine deep learning techniques and classic numerical methods for differential equations to solve two challenging problems in dynamical systems theory: dynamics discovery and parameter estimation. Results demonstrate the effectiveness of the proposed approaches on a suite of test problems exhibiting oscillatory and chaotic dynamics. When comparing the performance of various numerical schemes, such as the Runge-Kutta and linear multistep families of methods, we observe promising results in predicting the system dynamics and estimating physical parameters, given appropriate choices of spatial and temporal discretization schemes and numerical method orders.</li>
</ul>

<h3>Title: Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Quyen Tran, Minh Le, Tuan Truong, Dinh Phung, Linh Ngo, Thien Nguyen, Nhat Ho, Trung Le</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04327">https://arxiv.org/abs/2410.04327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04327">https://arxiv.org/pdf/2410.04327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04327]] Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning(https://arxiv.org/abs/2410.04327)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Drawing inspiration from human learning behaviors, this work proposes a novel approach to mitigate catastrophic forgetting in Prompt-based Continual Learning models by exploiting the relationships between continuously emerging class data. We find that applying human habits of organizing and connecting information can serve as an efficient strategy when training deep learning models. Specifically, by building a hierarchical tree structure based on the expanding set of labels, we gain fresh insights into the data, identifying groups of similar classes could easily cause confusion. Additionally, we delve deeper into the hidden connections between classes by exploring the original pretrained model's behavior through an optimal transport-based approach. From these insights, we propose a novel regularization loss function that encourages models to focus more on challenging knowledge areas, thereby enhancing overall performance. Experimentally, our method demonstrated significant superiority over the most robust state-of-the-art models on various benchmarks.</li>
</ul>

<h3>Title: Gradient Routing: Masking Gradients to Localize Computation in Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Alex Cloud, Jacob Goldman-Wetzler, Evžen Wybitul, Joseph Miller, Alexander Matt Turner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04332">https://arxiv.org/abs/2410.04332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04332">https://arxiv.org/pdf/2410.04332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04332]] Gradient Routing: Masking Gradients to Localize Computation in Neural Networks(https://arxiv.org/abs/2410.04332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural networks are trained primarily based on their inputs and outputs, without regard for their internal mechanisms. These neglected mechanisms determine properties that are critical for safety, like (i) transparency; (ii) the absence of sensitive information or harmful capabilities; and (iii) reliable generalization of goals beyond the training distribution. To address this shortcoming, we introduce gradient routing, a training method that isolates capabilities to specific subregions of a neural network. Gradient routing applies data-dependent, weighted masks to gradients during backpropagation. These masks are supplied by the user in order to configure which parameters are updated by which data points. We show that gradient routing can be used to (1) learn representations which are partitioned in an interpretable way; (2) enable robust unlearning via ablation of a pre-specified network subregion; and (3) achieve scalable oversight of a reinforcement learner by localizing modules responsible for different behaviors. Throughout, we find that gradient routing localizes capabilities even when applied to a limited, ad-hoc subset of the data. We conclude that the approach holds promise for challenging, real-world applications where quality data are scarce.</li>
</ul>

<h3>Title: ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Shuhao Gu, Mengdi Zhao, Bowen Zhang, Liangdong Wang, Jijie Li, Guang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04335">https://arxiv.org/abs/2410.04335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04335">https://arxiv.org/pdf/2410.04335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04335]] ReTok: Replacing Tokenizer to Enhance Representation Efficiency in Large Language Model(https://arxiv.org/abs/2410.04335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tokenizer is an essential component for large language models (LLMs), and a tokenizer with a high compression rate can improve the model's representation and processing efficiency. However, the tokenizer cannot ensure high compression rate in all scenarios, and an increase in the average input and output lengths will increases the training and inference costs of the model. Therefore, it is crucial to find ways to improve the model's efficiency with minimal cost while maintaining the model's performance. In this work, we propose a method to improve model representation and processing efficiency by replacing the tokenizers of LLMs. We propose replacing and reinitializing the parameters of the model's input and output layers with the parameters of the original model, and training these parameters while keeping other parameters fixed. We conducted experiments on different LLMs, and the results show that our method can maintain the performance of the model after replacing the tokenizer, while significantly improving the decoding speed for long texts.</li>
</ul>

<h3>Title: Inference Scaling for Long-Context Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04343">https://arxiv.org/abs/2410.04343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04343">https://arxiv.org/pdf/2410.04343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04343]] Inference Scaling for Long-Context Retrieval Augmented Generation(https://arxiv.org/abs/2410.04343)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.</li>
</ul>

<h3>Title: Ordinal Preference Optimization: Aligning Human Preferences via NDCG</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhao, Yixin Wang, Mingzhang Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04346">https://arxiv.org/abs/2410.04346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04346">https://arxiv.org/pdf/2410.04346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04346]] Ordinal Preference Optimization: Aligning Human Preferences via NDCG(https://arxiv.org/abs/2410.04346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning Large Language Models (LLMs) with diverse human preferences is a pivotal technique for controlling model behaviors and enhancing generation quality. Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), and their variants optimize language models by pairwise comparisons. However, when multiple responses are available, these approaches fall short of leveraging the extensive information in the ranking given by the reward models or human feedback. In this work, we propose a novel listwise approach named Ordinal Preference Optimization (OPO), which employs the Normalized Discounted Cumulative Gain (NDCG), a widely-used ranking metric, to better utilize relative proximity within ordinal multiple responses. We develop an end-to-end preference optimization algorithm by approximating NDCG with a differentiable surrogate loss. This approach builds a connection between ranking models in information retrieval and the alignment problem. In aligning multi-response datasets assigned with ordinal rewards, OPO outperforms existing pairwise and listwise approaches on evaluation sets and general benchmarks like AlpacaEval. Moreover, we demonstrate that increasing the pool of negative samples can enhance model performance by reducing the adverse effects of trivial negatives.</li>
</ul>

<h3>Title: Latent Feature Mining for Predictive Model Enhancement with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bingxuan Li, Pengyi Shi, Amy Ward</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04347">https://arxiv.org/abs/2410.04347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04347">https://arxiv.org/pdf/2410.04347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04347]] Latent Feature Mining for Predictive Model Enhancement with Large Language Models(https://arxiv.org/abs/2410.04347)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Predictive modeling often faces challenges due to limited data availability and quality, especially in domains where collected features are weakly correlated with outcomes and where additional feature collection is constrained by ethical or practical difficulties. Traditional machine learning (ML) models struggle to incorporate unobserved yet critical factors. In this work, we introduce an effective approach to formulate latent feature mining as text-to-text propositional logical reasoning. We propose FLAME (Faithful Latent Feature Mining for Predictive Model Enhancement), a framework that leverages large language models (LLMs) to augment observed features with latent features and enhance the predictive power of ML models in downstream tasks. Our framework is generalizable across various domains with necessary domain-specific adaptation, as it is designed to incorporate contextual information unique to each area, ensuring effective transfer to different areas facing similar data availability challenges. We validate our framework with two case studies: (1) the criminal justice system, a domain characterized by limited and ethically challenging data collection; (2) the healthcare domain, where patient privacy concerns and the complexity of medical data limit comprehensive feature collection. Our results show that inferred latent features align well with ground truth labels and significantly enhance the downstream classifier.</li>
</ul>

<h3>Title: TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights</h3>
<ul>
<li><strong>Authors: </strong>Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, Philip S. Yu, Meng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04350">https://arxiv.org/abs/2410.04350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04350">https://arxiv.org/pdf/2410.04350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04350]] TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights(https://arxiv.org/abs/2410.04350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. However, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences between tokens, which may affect optimization efficiency and make it difficult to achieve optimal results. In this work, we propose that the optimal data for DPO has equal expected rewards for each token in winning and losing responses, as there is no difference in token importance. However, since the optimal dataset is unavailable in practice, we propose using the original dataset for importance sampling to achieve unbiased optimization. Accordingly, we propose a token-level importance sampling DPO objective named TIS-DPO that assigns importance weights to each token based on its reward. Inspired by previous works, we estimate the token importance weights using the difference in prediction probabilities from a pair of contrastive LLMs. We explore three methods to construct these contrastive LLMs: (1) guiding the original LLM with contrastive prompts, (2) training two separate LLMs using winning and losing responses, and (3) performing forward and reverse DPO training with winning and losing responses. Experiments show that TIS-DPO significantly outperforms various baseline methods on harmlessness and helpfulness alignment and summarization tasks. We also visualize the estimated weights, demonstrating their ability to identify key token positions.</li>
</ul>

<h3>Title: Enhancing Android Malware Detection: The Influence of ChatGPT on Decision-centric Task</h3>
<ul>
<li><strong>Authors: </strong>Yao Li, Sen Fang, Tao Zhang, Haipeng Cai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04352">https://arxiv.org/abs/2410.04352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04352">https://arxiv.org/pdf/2410.04352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04352]] Enhancing Android Malware Detection: The Influence of ChatGPT on Decision-centric Task(https://arxiv.org/abs/2410.04352)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>With the rise of large language models, such as ChatGPT, non-decisional models have been applied to various tasks. Moreover, ChatGPT has drawn attention to the traditional decision-centric task of Android malware detection. Despite effective detection methods proposed by scholars, they face low interpretability issues. Specifically, while these methods excel in classifying applications as benign or malicious and can detect malicious behavior, they often fail to provide detailed explanations for the decisions they make. This challenge raises concerns about the reliability of existing detection schemes and questions their true ability to understand complex data. In this study, we investigate the influence of the non-decisional model, ChatGPT, on the traditional decision-centric task of Android malware detection. We choose three state-of-the-art solutions, Drebin, XMAL, and MaMaDroid, conduct a series of experiments on publicly available datasets, and carry out a comprehensive comparison and analysis. Our findings indicate that these decision-driven solutions primarily rely on statistical patterns within datasets to make decisions, rather than genuinely understanding the underlying data. In contrast, ChatGPT, as a non-decisional model, excels in providing comprehensive analysis reports, substantially enhancing interpretability. Furthermore, we conduct surveys among experienced developers. The result highlights developers' preference for ChatGPT, as it offers in-depth insights and enhances efficiency and understanding of challenges. Meanwhile, these studies and analyses offer profound insights, presenting developers with a novel perspective on Android malware detection--enhancing the reliability of detection results from a non-decisional perspective.</li>
</ul>

<h3>Title: VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide</h3>
<ul>
<li><strong>Authors: </strong>Dohun Lee, Bryan S Kim, Geon Yeong Park, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04364">https://arxiv.org/abs/2410.04364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04364">https://arxiv.org/pdf/2410.04364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04364]] VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide(https://arxiv.org/abs/2410.04364)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: this http URL</li>
</ul>

<h3>Title: Algorithmic Capabilities of Random Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ziqian Zhong, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04368">https://arxiv.org/abs/2410.04368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04368">https://arxiv.org/pdf/2410.04368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04368]] Algorithmic Capabilities of Random Transformers(https://arxiv.org/abs/2410.04368)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained. Code is available at this https URL.</li>
</ul>

<h3>Title: DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ke Sun, Shen Chen, Taiping Yao, Hong Liu, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04372">https://arxiv.org/abs/2410.04372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04372">https://arxiv.org/pdf/2410.04372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04372]] DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion(https://arxiv.org/abs/2410.04372)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. Our Codes are available in this https URL.</li>
</ul>

<h3>Title: Suspiciousness of Adversarial Texts to Human</h3>
<ul>
<li><strong>Authors: </strong>Shakila Mahjabin Tonni, Pedro Faustini, Mark Dras</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04377">https://arxiv.org/abs/2410.04377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04377">https://arxiv.org/pdf/2410.04377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04377]] Suspiciousness of Adversarial Texts to Human(https://arxiv.org/abs/2410.04377)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial examples pose a significant challenge to deep neural networks (DNNs) across both image and text domains, with the intent to degrade model performance through meticulously altered inputs. Adversarial texts, however, are distinct from adversarial images due to their requirement for semantic similarity and the discrete nature of the textual contents. This study delves into the concept of human suspiciousness, a quality distinct from the traditional focus on imperceptibility found in image-based adversarial examples. Unlike images, where adversarial changes are meant to be indistinguishable to the human eye, textual adversarial content must often remain undetected or non-suspicious to human readers, even when the text's purpose is to deceive NLP systems or bypass filters. In this research, we expand the study of human suspiciousness by analyzing how individuals perceive adversarial texts. We gather and publish a novel dataset of Likert-scale human evaluations on the suspiciousness of adversarial sentences, crafted by four widely used adversarial attack methods and assess their correlation with the human ability to detect machine-generated alterations. Additionally, we develop a regression-based model to quantify suspiciousness and establish a baseline for future research in reducing the suspiciousness in adversarial text generation. We also demonstrate how the regressor-generated suspicious scores can be incorporated into adversarial generation methods to produce texts that are less likely to be perceived as computer-generated. We make our human suspiciousness annotated data and our code available.</li>
</ul>

<h3>Title: Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification</h3>
<ul>
<li><strong>Authors: </strong>Yijia Chang, Hanrui Jiang, Chao Lin, Xinyi Huang, Jian Weng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04397">https://arxiv.org/abs/2410.04397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04397">https://arxiv.org/pdf/2410.04397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04397]] Towards Understanding and Enhancing Security of Proof-of-Training for DNN Model Ownership Verification(https://arxiv.org/abs/2410.04397)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>The great economic values of deep neural networks (DNNs) urge AI enterprises to protect their intellectual property (IP) for these models. Recently, proof-of-training (PoT) has been proposed as a promising solution to DNN IP protection, through which AI enterprises can utilize the record of DNN training process as their ownership proof. To prevent attackers from forging ownership proof, a secure PoT scheme should be able to distinguish honest training records from those forged by attackers. Although existing PoT schemes provide various distinction criteria, these criteria are based on intuitions or observations. The effectiveness of these criteria lacks clear and comprehensive analysis, resulting in existing schemes initially deemed secure being swiftly compromised by simple ideas. In this paper, we make the first move to identify distinction criteria in the style of formal methods, so that their effectiveness can be explicitly demonstrated. Specifically, we conduct systematic modeling to cover a wide range of attacks and then theoretically analyze the distinctions between honest and forged training records. The analysis results not only induce a universal distinction criterion, but also provide detailed reasoning to demonstrate its effectiveness in defending against attacks covered by our model. Guided by the criterion, we propose a generic PoT construction that can be instantiated into concrete schemes. This construction sheds light on the realization that trajectory matching algorithms, previously employed in data distillation, possess significant advantages in PoT construction. Experimental results demonstrate that our scheme can resist attacks that have compromised existing PoT schemes, which corroborates its superiority in security.</li>
</ul>

<h3>Title: Lens: Rethinking Multilingual Enhancement for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weixiang Zhao, Yulin Hu, Jiahe Guo, Xingyu Sui, Tongtong Wu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04407">https://arxiv.org/abs/2410.04407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04407">https://arxiv.org/pdf/2410.04407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04407]] Lens: Rethinking Multilingual Enhancement for Large Language Models(https://arxiv.org/abs/2410.04407)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the growing global demand for large language models (LLMs) that serve users from diverse linguistic backgrounds, most cutting-edge LLMs remain predominantly English-centric. This creates a performance gap across languages, restricting access to advanced AI services for non-English speakers. Current methods to enhance multilingual capabilities largely rely on data-driven post-training techniques, such as multilingual instruction tuning or continual pre-training. However, these approaches encounter significant challenges, including the scarcity of high-quality multilingual datasets and the limited enhancement of multilingual capabilities. They often suffer from off-target issues and catastrophic forgetting of central language abilities. To this end, we propose Lens, a novel approach to enhance multilingual capabilities of LLMs by leveraging their internal language representation spaces. Specially, Lens operates by manipulating the hidden representations within the language-agnostic and language-specific subspaces from top layers of LLMs. Using the central language as a pivot, the target language is drawn closer to it within the language-agnostic subspace, allowing it to inherit well-established semantic representations. Meanwhile, in the language-specific subspace, the representations of the target and central languages are pushed apart, enabling the target language to express itself distinctly. Extensive experiments on one English-centric and two multilingual LLMs demonstrate that Lens effectively improves multilingual performance without sacrificing the original central language capabilities of the backbone model, achieving superior results with much fewer computational resources compared to existing post-training approaches.</li>
</ul>

<h3>Title: CoVLM: Leveraging Consensus from Vision-Language Models for Semi-supervised Multi-modal Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Devank, Jayateja Kalla, Soma Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04426">https://arxiv.org/abs/2410.04426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04426">https://arxiv.org/pdf/2410.04426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04426]] CoVLM: Leveraging Consensus from Vision-Language Models for Semi-supervised Multi-modal Fake News Detection(https://arxiv.org/abs/2410.04426)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we address the real-world, challenging task of out-of-context misinformation detection, where a real image is paired with an incorrect caption for creating fake news. Existing approaches for this task assume the availability of large amounts of labeled data, which is often impractical in real-world, since it requires extensive manual intervention and domain expertise. In contrast, since obtaining a large corpus of unlabeled image-text pairs is much easier, here, we propose a semi-supervised protocol, where the model has access to a limited number of labeled image-text pairs and a large corpus of unlabeled pairs. Additionally, the occurrence of fake news being much lesser compared to the real ones, the datasets tend to be highly imbalanced, thus making the task even more challenging. Towards this goal, we propose a novel framework, Consensus from Vision-Language Models (CoVLM), which generates robust pseudo-labels for unlabeled pairs using thresholds derived from the labeled data. This approach can automatically determine the right threshold parameters of the model for selecting the confident pseudo-labels. Experimental results on benchmark datasets across challenging conditions and comparisons with state-of-the-art approaches demonstrate the effectiveness of our framework.</li>
</ul>

<h3>Title: CAPEEN: Image Captioning with Early Exits and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Divya Jyoti Bajpai, Manjesh Kumar Hanawal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04433">https://arxiv.org/abs/2410.04433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04433">https://arxiv.org/pdf/2410.04433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04433]] CAPEEN: Image Captioning with Early Exits and Knowledge Distillation(https://arxiv.org/abs/2410.04433)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have made significant progress in recognizing visual elements and generating descriptive text in image-captioning tasks. However, their improved performance comes from increased computational burden and inference latency. Early Exit (EE) strategies can be used to enhance their efficiency, but their adaptation presents challenges in image captioning as it requires varying levels of semantic information for accurate predictions. To overcome this, we introduce CAPEEN to improve the performance of EE strategies using knowledge distillation. Inference in CAPEEN is completed at intermediary layers if prediction confidence exceeds a predefined value learned from the training data. To account for real-world deployments, where target distributions could drift from that of training samples, we introduce a variant A-CAPEEN to adapt the thresholds on the fly using Multiarmed bandits framework. Experiments on the MS COCO and Flickr30k datasets show that CAPEEN gains speedup of 1.77x while maintaining competitive performance compared to the final layer, and A-CAPEEN additionally offers robustness against distortions. The source code is available at this https URL</li>
</ul>

<h3>Title: A Mathematical Explanation of UNet</h3>
<ul>
<li><strong>Authors: </strong>Xue-Cheng Tai, Hao Liu, Raymond H. Chan, Lingfeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04434">https://arxiv.org/abs/2410.04434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04434">https://arxiv.org/pdf/2410.04434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04434]] A Mathematical Explanation of UNet(https://arxiv.org/abs/2410.04434)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The UNet architecture has transformed image segmentation. UNet's versatility and accuracy have driven its widespread adoption, significantly advancing fields reliant on machine learning problems with images. In this work, we give a clear and concise mathematical explanation of UNet. We explain what is the meaning and function of each of the components of UNet. We will show that UNet is solving a control problem. We decompose the control variables using multigrid methods. Then, operator-splitting techniques is used to solve the problem, whose architecture exactly recovers the UNet architecture. Our result shows that UNet is a one-step operator-splitting algorithm for the control problem.</li>
</ul>

<h3>Title: Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Li, Guohao Li, Zhibin Lan, Xue Xu, Wanru Zhuang, Jiachen Liu, Xinyan Xiao, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04439">https://arxiv.org/abs/2410.04439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04439">https://arxiv.org/pdf/2410.04439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04439]] Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training(https://arxiv.org/abs/2410.04439)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate texts, and lack of support for Chinese text, but their development shows promising potential. In this paper, we propose a series of methods, aiming to empower backbone models to generate visual texts in English and Chinese. We first conduct a preliminary study revealing that Byte Pair Encoding (BPE) tokenization and the insufficient learning of cross-attention modules restrict the performance of the backbone models. Based on these observations, we make the following improvements: (1) We design a mixed granularity input strategy to provide more suitable text representations; (2) We propose to augment the conventional training objective with three glyph-aware training losses, which enhance the learning of cross-attention modules and encourage the model to focus on visual texts. Through experiments, we demonstrate that our methods can effectively empower backbone models to generate semantic relevant, aesthetically appealing, and accurate visual text images, while maintaining their fundamental image generation quality.</li>
</ul>

<h3>Title: Automated Detection of Defects on Metal Surfaces using Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Toqa Alaa, Mostafa Kotb, Arwa Zakaria, Mariam Diab, Walid Gomaa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04440">https://arxiv.org/abs/2410.04440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04440">https://arxiv.org/pdf/2410.04440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04440]] Automated Detection of Defects on Metal Surfaces using Vision Transformers(https://arxiv.org/abs/2410.04440)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Metal manufacturing often results in the production of defective products, leading to operational challenges. Since traditional manual inspection is time-consuming and resource-intensive, automatic solutions are needed. The study utilizes deep learning techniques to develop a model for detecting metal surface defects using Vision Transformers (ViTs). The proposed model focuses on the classification and localization of defects using a ViT for feature extraction. The architecture branches into two paths: classification and localization. The model must approach high classification accuracy while keeping the Mean Square Error (MSE) and Mean Absolute Error (MAE) as low as possible in the localization process. Experimental results show that it can be utilized in the process of automated defects detection, improve operational efficiency, and reduce errors in metal manufacturing.</li>
</ul>

<h3>Title: TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Peiyuan Liu, Beiliang Wu, Yifan Hu, Naiqi Li, Tao Dai, Jigang Bao, Shu-tao Xia</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04442">https://arxiv.org/abs/2410.04442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04442">https://arxiv.org/pdf/2410.04442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04442]] TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting(https://arxiv.org/abs/2410.04442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Non-stationarity poses significant challenges for multivariate time series forecasting due to the inherent short-term fluctuations and long-term trends that can lead to spurious regressions or obscure essential long-term relationships. Most existing methods either eliminate or retain non-stationarity without adequately addressing its distinct impacts on short-term and long-term modeling. Eliminating non-stationarity is essential for avoiding spurious regressions and capturing local dependencies in short-term modeling, while preserving it is crucial for revealing long-term cointegration across variates. In this paper, we propose TimeBridge, a novel framework designed to bridge the gap between non-stationarity and dependency modeling in long-term time series forecasting. By segmenting input series into smaller patches, TimeBridge applies Integrated Attention to mitigate short-term non-stationarity and capture stable dependencies within each variate, while Cointegrated Attention preserves non-stationarity to model long-term cointegration across variates. Extensive experiments show that TimeBridge consistently achieves state-of-the-art performance in both short-term and long-term forecasting. Additionally, TimeBridge demonstrates exceptional performance in financial forecasting on the CSI 500 and S&P 500 indices, further validating its robustness and effectiveness. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Optimising for the Unknown: Domain Alignment for Cephalometric Landmark Detection</h3>
<ul>
<li><strong>Authors: </strong>Julian Wyatt, Irina Voiculescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04445">https://arxiv.org/abs/2410.04445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04445">https://arxiv.org/pdf/2410.04445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04445]] Optimising for the Unknown: Domain Alignment for Cephalometric Landmark Detection(https://arxiv.org/abs/2410.04445)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Cephalometric Landmark Detection is the process of identifying key areas for cephalometry. Each landmark is a single GT point labelled by a clinician. A machine learning model predicts the probability locus of a landmark represented by a heatmap. This work, for the 2024 CL-Detection MICCAI Challenge, proposes a domain alignment strategy with a regional facial extraction module and an X-ray artefact augmentation procedure. The challenge ranks our method's results as the best in MRE of 1.186mm and third in the 2mm SDR of 82.04% on the online validation leaderboard. The code is available at this https URL.</li>
</ul>

<h3>Title: Attention Shift: Steering AI Away from Unsafe Content</h3>
<ul>
<li><strong>Authors: </strong>Shivank Garg, Manyana Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04447">https://arxiv.org/abs/2410.04447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04447">https://arxiv.org/pdf/2410.04447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04447]] Attention Shift: Steering AI Away from Unsafe Content(https://arxiv.org/abs/2410.04447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study investigates the generation of unsafe or harmful content in state-of-the-art generative models, focusing on methods for restricting such generations. We introduce a novel training-free approach using attention reweighing to remove unsafe concepts without additional training during inference. We compare our method against existing ablation methods, evaluating the performance on both, direct and adversarial jailbreak prompts, using qualitative and quantitative metrics. We hypothesize potential reasons for the observed results and discuss the limitations and broader implications of content restriction.</li>
</ul>

<h3>Title: Video Summarization Techniques: A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Toqa Alaa, Ahmad Mongy, Assem Bakr, Mariam Diab, Walid Gomaa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04449">https://arxiv.org/abs/2410.04449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04449">https://arxiv.org/pdf/2410.04449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04449]] Video Summarization Techniques: A Comprehensive Review(https://arxiv.org/abs/2410.04449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid expansion of video content across a variety of industries, including social media, education, entertainment, and surveillance, has made video summarization an essential field of study. The current work is a survey that explores the various approaches and methods created for video summarizing, emphasizing both abstractive and extractive strategies. The process of extractive summarization involves the identification of key frames or segments from the source video, utilizing methods such as shot boundary recognition, and clustering. On the other hand, abstractive summarization creates new content by getting the essential content from the video, using machine learning models like deep neural networks and natural language processing, reinforcement learning, attention mechanisms, generative adversarial networks, and multi-modal learning. We also include approaches that incorporate the two methodologies, along with discussing the uses and difficulties encountered in real-world implementations. The paper also covers the datasets used to benchmark these techniques. This review attempts to provide a state-of-the-art thorough knowledge of the current state and future directions of video summarization research.</li>
</ul>

<h3>Title: MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Zhentao Xie, Jiabao Zhao, Yilei Wang, Jinxin Shi, Yanhong Bai, Xingjiao Wu, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04452">https://arxiv.org/abs/2410.04452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04452">https://arxiv.org/pdf/2410.04452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04452]] MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems(https://arxiv.org/abs/2410.04452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Detecting cognitive biases in large language models (LLMs) is a fascinating task that aims to probe the existing cognitive biases within these models. Current methods for detecting cognitive biases in language models generally suffer from incomplete detection capabilities and a restricted range of detectable bias types. To address this issue, we introduced the 'MindScope' dataset, which distinctively integrates static and dynamic elements. The static component comprises 5,170 open-ended questions spanning 72 cognitive bias categories. The dynamic component leverages a rule-based, multi-agent communication framework to facilitate the generation of multi-round dialogues. This framework is flexible and readily adaptable for various psychological experiments involving LLMs. In addition, we introduce a multi-agent detection method applicable to a wide range of detection tasks, which integrates Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement learning-based decision module. Demonstrating substantial effectiveness, this method has shown to improve detection accuracy by as much as 35.10% compared to GPT-4. Codes and appendix are available at this https URL.</li>
</ul>

<h3>Title: CopyLens: Dynamically Flagging Copyrighted Sub-Dataset Contributions to LLM Outputs</h3>
<ul>
<li><strong>Authors: </strong>Qichao Ma, Rui-Jie Zhu, Peiye Liu, Renye Yan, Fahong Zhang, Ling Liang, Meng Li, Zhaofei Yu, Zongwei Wang, Yimao Cai, Tiejun Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04454">https://arxiv.org/abs/2410.04454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04454">https://arxiv.org/pdf/2410.04454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04454]] CopyLens: Dynamically Flagging Copyrighted Sub-Dataset Contributions to LLM Outputs(https://arxiv.org/abs/2410.04454)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become pervasive due to their knowledge absorption and text-generation capabilities. Concurrently, the copyright issue for pretraining datasets has been a pressing concern, particularly when generation includes specific styles. Previous methods either focus on the defense of identical copyrighted outputs or find interpretability by individual tokens with computational burdens. However, the gap between them exists, where direct assessments of how dataset contributions impact LLM outputs are missing. Once the model providers ensure copyright protection for data holders, a more mature LLM community can be established. To address these limitations, we introduce CopyLens, a new framework to analyze how copyrighted datasets may influence LLM responses. Specifically, a two-stage approach is employed: First, based on the uniqueness of pretraining data in the embedding space, token representations are initially fused for potential copyrighted texts, followed by a lightweight LSTM-based network to analyze dataset contributions. With such a prior, a contrastive-learning-based non-copyright OOD detector is designed. Our framework can dynamically face different situations and bridge the gap between current copyright detection methods. Experiments show that CopyLens improves efficiency and accuracy by 15.2% over our proposed baseline, 58.7% over prompt engineering methods, and 0.21 AUC over OOD detection baselines.</li>
</ul>

<h3>Title: Improved Off-policy Reinforcement Learning in Biological Sequence Design</h3>
<ul>
<li><strong>Authors: </strong>Hyeonah Kim, Minsu Kim, Taeyoung Yun, Sanghyeok Choi, Emmanuel Bengio, Alex Hernández-García, Jinkyoo Park</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04461">https://arxiv.org/abs/2410.04461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04461">https://arxiv.org/pdf/2410.04461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04461]] Improved Off-policy Reinforcement Learning in Biological Sequence Design(https://arxiv.org/abs/2410.04461)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Designing biological sequences with desired properties is a significant challenge due to the combinatorially vast search space and the high cost of evaluating each candidate sequence. To address these challenges, reinforcement learning (RL) methods, such as GFlowNets, utilize proxy models for rapid reward evaluation and annotated data for policy training. Although these approaches have shown promise in generating diverse and novel sequences, the limited training data relative to the vast search space often leads to the misspecification of proxy for out-of-distribution inputs. We introduce $\delta$-Conservative Search, a novel off-policy search method for training GFlowNets designed to improve robustness against proxy misspecification. The key idea is to incorporate conservativeness, controlled by parameter $\delta$, to constrain the search to reliable regions. Specifically, we inject noise into high-score offline sequences by randomly masking tokens with a Bernoulli distribution of parameter $\delta$ and then denoise masked tokens using the GFlowNet policy. Additionally, $\delta$ is adaptively adjusted based on the uncertainty of the proxy model for each data point. This enables the reflection of proxy uncertainty to determine the level of conservativeness. Experimental results demonstrate that our method consistently outperforms existing machine learning methods in discovering high-score sequences across diverse tasks-including DNA, RNA, protein, and peptide design-especially in large-scale scenarios.</li>
</ul>

<h3>Title: Tensor-Train Point Cloud Compression and Efficient Approximate Nearest-Neighbor Search</h3>
<ul>
<li><strong>Authors: </strong>Georgii Novikov, Alexander Gneushev, Alexey Kadeishvili, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04462">https://arxiv.org/abs/2410.04462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04462">https://arxiv.org/pdf/2410.04462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04462]] Tensor-Train Point Cloud Compression and Efficient Approximate Nearest-Neighbor Search(https://arxiv.org/abs/2410.04462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Nearest-neighbor search in large vector databases is crucial for various machine learning applications. This paper introduces a novel method using tensor-train (TT) low-rank tensor decomposition to efficiently represent point clouds and enable fast approximate nearest-neighbor searches. We propose a probabilistic interpretation and utilize density estimation losses like Sliced Wasserstein to train TT decompositions, resulting in robust point cloud compression. We reveal an inherent hierarchical structure within TT point clouds, facilitating efficient approximate nearest-neighbor searches. In our paper, we provide detailed insights into the methodology and conduct comprehensive comparisons with existing methods. We demonstrate its effectiveness in various scenarios, including out-of-distribution (OOD) detection problems and approximate nearest-neighbor (ANN) search tasks.</li>
</ul>

<h3>Title: Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information</h3>
<ul>
<li><strong>Authors: </strong>Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng Lu, Libo Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04463">https://arxiv.org/abs/2410.04463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04463">https://arxiv.org/pdf/2410.04463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04463]] Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information(https://arxiv.org/abs/2410.04463)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) has become a vital technique for enhancing the performance of Large Language Models (LLMs), attracting increasing attention from researchers. One stream of approaches focuses on the iterative enhancement of LLMs by continuously verifying and refining their reasoning outputs for desired quality. Despite its impressive results, this paradigm faces two critical issues: (1) Simple verification methods: The current paradigm relies solely on a single verification method. (2) Wrong Information Ignorance: Traditional paradigms directly ignore wrong information during reasoning and refine the logic paths from scratch each time. To address these challenges, we propose Wrong-of-Thought (WoT), which includes two core modules: (1) Multi-Perspective Verification: A multi-perspective verification method for accurately refining the reasoning process and result, and (2) Wrong Information Utilization: Utilizing wrong information to alert LLMs and reduce the probability of LLMs making same mistakes. Experiments on 8 popular datasets and 5 LLMs demonstrate that WoT surpasses all previous baselines. In addition, WoT exhibits powerful capabilities in difficult computation tasks.</li>
</ul>

<h3>Title: Revisiting In-context Learning Inference Circuit in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04468">https://arxiv.org/abs/2410.04468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04468">https://arxiv.org/pdf/2410.04468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04468]] Revisiting In-context Learning Inference Circuit in Large Language Models(https://arxiv.org/abs/2410.04468)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.</li>
</ul>

<h3>Title: Collapsed Language Models Promote Fairness</h3>
<ul>
<li><strong>Authors: </strong>Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04472">https://arxiv.org/abs/2410.04472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04472">https://arxiv.org/pdf/2410.04472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04472]] Collapsed Language Models Promote Fairness(https://arxiv.org/abs/2410.04472)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more. Despite the development, it is nontrivial to reach a principled understanding of fairness and an effective algorithm that can consistently debias language models. In this work, by rigorous evaluations of Neural Collapse -- a learning phenomenon happen in last-layer representations and classifiers in deep networks -- on fairness-related words, we find that debiased language models exhibit collapsed alignment between token representations and word embeddings. More importantly, this observation inspires us to design a principled fine-tuning method that can effectively improve fairness in a wide range of debiasing methods, while still preserving the performance of language models on standard natural language understanding tasks. We attach our code at this https URL .</li>
</ul>

<h3>Title: A Large-Scale Exploit Instrumentation Study of AI/ML Supply Chain Attacks in Hugging Face Models</h3>
<ul>
<li><strong>Authors: </strong>Beatrice Casey, Joanna C. S. Santos, Mehdi Mirakhorli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04490">https://arxiv.org/abs/2410.04490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04490">https://arxiv.org/pdf/2410.04490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04490]] A Large-Scale Exploit Instrumentation Study of AI/ML Supply Chain Attacks in Hugging Face Models(https://arxiv.org/abs/2410.04490)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The development of machine learning (ML) techniques has led to ample opportunities for developers to develop and deploy their own models. Hugging Face serves as an open source platform where developers can share and download other models in an effort to make ML development more collaborative. In order for models to be shared, they first need to be serialized. Certain Python serialization methods are considered unsafe, as they are vulnerable to object injection. This paper investigates the pervasiveness of these unsafe serialization methods across Hugging Face, and demonstrates through an exploitation approach, that models using unsafe serialization methods can be exploited and shared, creating an unsafe environment for ML developers. We investigate to what extent Hugging Face is able to flag repositories and files using unsafe serialization methods, and develop a technique to detect malicious models. Our results show that Hugging Face is home to a wide range of potentially vulnerable models.</li>
</ul>

<h3>Title: Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification</h3>
<ul>
<li><strong>Authors: </strong>Zhaorui Tan, Xi Yang, Qiufeng Wang, Anh Nguyen, Kaizhu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04492">https://arxiv.org/abs/2410.04492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04492">https://arxiv.org/pdf/2410.04492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04492]] Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification(https://arxiv.org/abs/2410.04492)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels</h3>
<ul>
<li><strong>Authors: </strong>Vy Nguyen, Chau Pham</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04501">https://arxiv.org/abs/2410.04501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04501">https://arxiv.org/pdf/2410.04501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04501]] Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels(https://arxiv.org/abs/2410.04501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing frequency of suicidal thoughts highlights the importance of early detection and intervention. Social media platforms, where users often share personal experiences and seek help, could be utilized to identify individuals at risk. However, the large volume of daily posts makes manual review impractical. This paper explores the use of Large Language Models (LLMs) to automatically detect suicidal content in text-based social media posts. We propose a novel method for generating pseudo-labels for unlabeled data by prompting LLMs, along with traditional classification fine-tuning techniques to enhance label accuracy. To create a strong suicide detection model, we develop an ensemble approach involving prompting with Qwen2-72B-Instruct, and using fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate our approach on the dataset of the Suicide Ideation Detection on Social Media Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we conduct a comprehensive analysis to assess the impact of different models and fine-tuning strategies on detection performance. Experimental results show that the ensemble model significantly improves the detection accuracy, by 5% points compared with the individual models. It achieves a weight F1 score of 0.770 on the public test set, and 0.731 on the private test set, providing a promising solution for identifying suicidal content in social media. Our analysis shows that the choice of LLMs affects the prompting performance, with larger models providing better accuracy. Our code and checkpoints are publicly available at this https URL.</li>
</ul>

<h3>Title: MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network</h3>
<ul>
<li><strong>Authors: </strong>Doanh C. Bui, Jin Tae Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04507">https://arxiv.org/abs/2410.04507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04507">https://arxiv.org/pdf/2410.04507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04507]] MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network(https://arxiv.org/abs/2410.04507)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Whole slide image (WSI) classification is a crucial problem for cancer diagnostics in clinics and hospitals. A WSI, acquired at gigapixel size, is commonly tiled into patches and processed by multiple-instance learning (MIL) models. Previous MIL-based models designed for this problem have only been evaluated on individual tasks for specific organs, and the ability to handle multiple tasks within a single model has not been investigated. In this study, we propose MECFormer, a generative Transformer-based model designed to handle multiple tasks within one model. To leverage the power of learning multiple tasks simultaneously and to enhance the model's effectiveness in focusing on each individual task, we introduce an Expert Consultation Network, a projection layer placed at the beginning of the Transformer-based model. Additionally, to enable flexible classification, autoregressive decoding is incorporated by a language decoder for WSI classification. Through extensive experiments on five datasets involving four different organs, one cancer classification task, and four cancer subtyping tasks, MECFormer demonstrates superior performance compared to individual state-of-the-art multiple-instance learning models.</li>
</ul>

<h3>Title: ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection</h3>
<ul>
<li><strong>Authors: </strong>Yibo Yan, Shen Wang, Jiahao Huo, Hang Li, Boyan Li, Jiamin Su, Xiong Gao, Yi-Fan Zhang, Tianlong Xu, Zhendong Chu, Aoxiao Zhong, Kun Wang, Hui Xiong, Philip S. Yu, Xuming Hu, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04509">https://arxiv.org/abs/2410.04509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04509">https://arxiv.org/pdf/2410.04509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04509]] ErrorRadar: Benchmarking Complex Mathematical Reasoning of Multimodal Large Language Models Via Error Detection(https://arxiv.org/abs/2410.04509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the field of Multimodal Large Language Models (MLLMs) continues to evolve, their potential to revolutionize artificial intelligence is particularly promising, especially in addressing mathematical reasoning tasks. Current mathematical benchmarks predominantly focus on evaluating MLLMs' problem-solving ability, yet there is a crucial gap in addressing more complex scenarios such as error detection, for enhancing reasoning capability in complicated settings. To fill this gap, we formally formulate the new task: multimodal error detection, and introduce ErrorRadar, the first benchmark designed to assess MLLMs' capabilities in such a task. ErrorRadar evaluates two sub-tasks: error step identification and error categorization, providing a comprehensive framework for evaluating MLLMs' complex mathematical reasoning ability. It consists of 2,500 high-quality multimodal K-12 mathematical problems, collected from real-world student interactions in an educational organization, with rigorous annotation and rich metadata such as problem type and error category. Through extensive experiments, we evaluated both open-source and closed-source representative MLLMs, benchmarking their performance against educational expert evaluators. Results indicate significant challenges still remain, as GPT-4o with best performance is still around 10% behind human evaluation. The dataset will be available upon acceptance.</li>
</ul>

<h3>Title: Realizing Video Summarization from the Path of Language-based Semantic Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kuan-Chen Mu, Zhi-Yi Chin, Wei-Chen Chiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04511">https://arxiv.org/abs/2410.04511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04511">https://arxiv.org/pdf/2410.04511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04511]] Realizing Video Summarization from the Path of Language-based Semantic Understanding(https://arxiv.org/abs/2410.04511)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent development of Video-based Large Language Models (VideoLLMs), has significantly advanced video summarization by aligning video features and, in some cases, audio features with Large Language Models (LLMs). Each of these VideoLLMs possesses unique strengths and weaknesses. Many recent methods have required extensive fine-tuning to overcome the limitations of these models, which can be resource-intensive. In this work, we observe that the strengths of one VideoLLM can complement the weaknesses of another. Leveraging this insight, we propose a novel video summarization framework inspired by the Mixture of Experts (MoE) paradigm, which operates as an inference-time algorithm without requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs to generate comprehensive and coherent textual summaries. It effectively combines visual and audio content, provides detailed background descriptions, and excels at identifying keyframes, which enables more semantically meaningful retrieval compared to traditional computer vision approaches that rely solely on visual information, all without the need for additional fine-tuning. Moreover, the resulting summaries enhance performance in downstream tasks such as summary video generation, either through keyframe selection or in combination with text-to-image models. Our language-driven approach offers a semantically rich alternative to conventional methods and provides flexibility to incorporate newer VideoLLMs, enhancing adaptability and performance in video summarization tasks.</li>
</ul>

<h3>Title: DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Xuan Gong, Tianshi Ming, Xinpeng Wang, Zhihua Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04514">https://arxiv.org/abs/2410.04514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04514">https://arxiv.org/pdf/2410.04514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04514]] DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination(https://arxiv.org/abs/2410.04514)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention $M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention outlier tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs. The code of our method will be released soon.</li>
</ul>

<h3>Title: RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference</h3>
<ul>
<li><strong>Authors: </strong>Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04519">https://arxiv.org/abs/2410.04519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04519">https://arxiv.org/pdf/2410.04519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04519]] RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference(https://arxiv.org/abs/2410.04519)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have brought a great breakthrough to the natural language processing (NLP) community, while leading the challenge of handling concurrent customer queries due to their high throughput demands. Data multiplexing addresses this by merging multiple inputs into a single composite input, allowing more efficient inference through a shared forward pass. However, as distinguishing individuals from a composite input is challenging, conventional methods typically require training the entire backbone, yet still suffer from performance degradation. In this paper, we introduce RevMUX, a parameter-efficient data multiplexing framework that incorporates a reversible design in the multiplexer, which can be reused by the demultiplexer to perform reverse operations and restore individual samples for classification. Extensive experiments on four datasets and three types of LLM backbones demonstrate the effectiveness of RevMUX for enhancing LLM inference efficiency while retaining a satisfactory classification performance.</li>
</ul>

<h3>Title: Dynamic Post-Hoc Neural Ensemblers</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Pineda Arango, Maciej Janowski, Lennart Purucker, Arber Zela, Frank Hutter, Josif Grabocka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04520">https://arxiv.org/abs/2410.04520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04520">https://arxiv.org/pdf/2410.04520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04520]] Dynamic Post-Hoc Neural Ensemblers(https://arxiv.org/abs/2410.04520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ensemble methods are known for enhancing the accuracy and robustness of machine learning models by combining multiple base learners. However, standard approaches like greedy or random ensembles often fall short, as they assume a constant weight across samples for the ensemble members. This can limit expressiveness and hinder performance when aggregating the ensemble predictions. In this study, we explore employing neural networks as ensemble methods, emphasizing the significance of dynamic ensembling to leverage diverse model predictions adaptively. Motivated by the risk of learning low-diversity ensembles, we propose regularizing the model by randomly dropping base model predictions during the training. We demonstrate this approach lower bounds the diversity within the ensemble, reducing overfitting and improving generalization capabilities. Our experiments showcase that the dynamic neural ensemblers yield competitive results compared to strong baselines in computer vision, natural language processing, and tabular data.</li>
</ul>

<h3>Title: MC-CoT: A Modular Collaborative CoT Framework for Zero-shot Medical-VQA with LLM and MLLM Integration</h3>
<ul>
<li><strong>Authors: </strong>Lai Wei, Wenkai Wang, Xiaoyu Shen, Yu Xie, Zhihao Fan, Xiaojin Zhang, Zhongyu Wei, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04521">https://arxiv.org/abs/2410.04521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04521">https://arxiv.org/pdf/2410.04521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04521]] MC-CoT: A Modular Collaborative CoT Framework for Zero-shot Medical-VQA with LLM and MLLM Integration(https://arxiv.org/abs/2410.04521)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In recent advancements, multimodal large language models (MLLMs) have been fine-tuned on specific medical image datasets to address medical visual question answering (Med-VQA) tasks. However, this common approach of task-specific fine-tuning is costly and necessitates separate models for each downstream task, limiting the exploration of zero-shot capabilities. In this paper, we introduce MC-CoT, a modular cross-modal collaboration Chain-of-Thought (CoT) framework designed to enhance the zero-shot performance of MLLMs in Med-VQA by leveraging large language models (LLMs). MC-CoT improves reasoning and information extraction by integrating medical knowledge and task-specific guidance, where LLM provides various complex medical reasoning chains and MLLM provides various observations of medical images based on instructions of the LLM. Our experiments on datasets such as SLAKE, VQA-RAD, and PATH-VQA show that MC-CoT surpasses standalone MLLMs and various multimodality CoT frameworks in recall rate and accuracy. These findings highlight the importance of incorporating background information and detailed guidance in addressing complex zero-shot Med-VQA tasks.</li>
</ul>

<h3>Title: Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yanrui Du, Sendong Zhao, Jiawei Cao, Ming Ma, Danyang Zhao, Fenglei Fan, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04524">https://arxiv.org/abs/2410.04524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04524">https://arxiv.org/pdf/2410.04524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04524]] Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning(https://arxiv.org/abs/2410.04524)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Instruction Fine-Tuning (IFT) has become an essential method for adapting base Large Language Models (LLMs) into variants for professional and private use. However, researchers have raised concerns over a significant decrease in LLMs' security following IFT, even when the IFT process involves entirely benign instructions (termed Benign IFT). Our study represents a pioneering effort to mitigate the security risks arising from Benign IFT. Specifically, we conduct a Module Robustness Analysis, aiming to investigate how LLMs' internal modules contribute to their security. Based on our analysis, we propose a novel IFT strategy, called the Modular Layer-wise Learning Rate (ML-LR) strategy. In our analysis, we implement a simple security feature classifier that serves as a proxy to measure the robustness of modules (e.g. $Q$/$K$/$V$, etc.). Our findings reveal that the module robustness shows clear patterns, varying regularly with the module type and the layer depth. Leveraging these insights, we develop a proxy-guided search algorithm to identify a robust subset of modules, termed Mods$_{Robust}$. During IFT, the ML-LR strategy employs differentiated learning rates for Mods$_{Robust}$ and the rest modules. Our experimental results show that in security assessments, the application of our ML-LR strategy significantly mitigates the rise in harmfulness of LLMs following Benign IFT. Notably, our ML-LR strategy has little impact on the usability or expertise of LLMs following Benign IFT. Furthermore, we have conducted comprehensive analyses to verify the soundness and flexibility of our ML-LR strategy.</li>
</ul>

<h3>Title: FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Siqiao Xue, Tingting Chen, Fan Zhou, Qingyang Dai, Zhixuan Chu, Hongyuan Mei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04526">https://arxiv.org/abs/2410.04526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04526">https://arxiv.org/pdf/2410.04526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04526]] FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering(https://arxiv.org/abs/2410.04526)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce FAMMA, an open-source benchmark for financial multilingual multimodal question answering (QA). Our benchmark aims to evaluate the abilities of multimodal large language models (MLLMs) in answering questions that require advanced financial knowledge and sophisticated reasoning. It includes 1,758 meticulously collected question-answer pairs from university textbooks and exams, spanning 8 major subfields in finance including corporate finance, asset management, and financial engineering. Some of the QA pairs are written in Chinese or French, while a majority of them are in English. These questions are presented in a mixed format combining text and heterogeneous image types, such as charts, tables, and diagrams. We evaluate a range of state-of-the-art MLLMs on our benchmark, and our analysis shows that FAMMA poses a significant challenge for these models. Even advanced systems like GPT-4o and Claude-35-Sonnet achieve only 42\% accuracy. Additionally, the open-source Qwen2-VL lags notably behind its proprietary counterparts. Lastly, we explore GPT o1-style reasoning chains to enhance the models' reasoning capabilities, which significantly improve error correction. Our FAMMA benchmark will facilitate future research to develop expert systems in financial QA. The leaderboard is available at this https URL .</li>
</ul>

<h3>Title: In-Place Panoptic Radiance Field Segmentation with Perceptual Prior for 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Shenghao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04529">https://arxiv.org/abs/2410.04529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04529">https://arxiv.org/pdf/2410.04529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04529]] In-Place Panoptic Radiance Field Segmentation with Perceptual Prior for 3D Scene Understanding(https://arxiv.org/abs/2410.04529)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate 3D scene representation and panoptic understanding are essential for applications such as virtual reality, robotics, and autonomous driving. However, challenges persist with existing methods, including precise 2D-to-3D mapping, handling complex scene characteristics like boundary ambiguity and varying scales, and mitigating noise in panoptic pseudo-labels. This paper introduces a novel perceptual-prior-guided 3D scene representation and panoptic understanding method, which reformulates panoptic understanding within neural radiance fields as a linear assignment problem involving 2D semantics and instance recognition. Perceptual information from pre-trained 2D panoptic segmentation models is incorporated as prior guidance, thereby synchronizing the learning processes of appearance, geometry, and panoptic understanding within neural radiance fields. An implicit scene representation and understanding model is developed to enhance generalization across indoor and outdoor scenes by extending the scale-encoded cascaded grids within a reparameterized domain distillation framework. This model effectively manages complex scene attributes and generates 3D-consistent scene representations and panoptic understanding outcomes for various scenes. Experiments and ablation studies under challenging conditions, including synthetic and real-world scenes, demonstrate the proposed method's effectiveness in enhancing 3D scene representation and panoptic segmentation accuracy.</li>
</ul>

<h3>Title: On Evaluating LLMs' Capabilities as Functional Approximators: A Bayesian Perspective</h3>
<ul>
<li><strong>Authors: </strong>Shoaib Ahmed Siddiqui, Yanzhi Chen, Juyeon Heo, Menglin Xia, Adrian Weller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04541">https://arxiv.org/abs/2410.04541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04541">https://arxiv.org/pdf/2410.04541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04541]] On Evaluating LLMs' Capabilities as Functional Approximators: A Bayesian Perspective(https://arxiv.org/abs/2410.04541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent works have successfully applied Large Language Models (LLMs) to function modeling tasks. However, the reasons behind this success remain unclear. In this work, we propose a new evaluation framework to comprehensively assess LLMs' function modeling abilities. By adopting a Bayesian perspective of function modeling, we discover that LLMs are relatively weak in understanding patterns in raw data, but excel at utilizing prior knowledge about the domain to develop a strong understanding of the underlying function. Our findings offer new insights about the strengths and limitations of LLMs in the context of function modeling.</li>
</ul>

<h3>Title: Pullback Flow Matching on Data Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Friso de Kruiff, Erik Bekkers, Ozan Öktem, Carola-Bibiane Schönlieb, Willem Diepeveen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04543">https://arxiv.org/abs/2410.04543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04543">https://arxiv.org/pdf/2410.04543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04543]] Pullback Flow Matching on Data Manifolds(https://arxiv.org/abs/2410.04543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose Pullback Flow Matching (PFM), a novel framework for generative modeling on data manifolds. Unlike existing methods that assume or learn restrictive closed-form manifold mappings for training Riemannian Flow Matching (RFM) models, PFM leverages pullback geometry and isometric learning to preserve the underlying manifold's geometry while enabling efficient generation and precise interpolation in latent space. This approach not only facilitates closed-form mappings on the data manifold but also allows for designable latent spaces, using assumed metrics on both data and latent manifolds. By enhancing isometric learning through Neural ODEs and proposing a scalable training objective, we achieve a latent space more suitable for interpolation, leading to improved manifold learning and generative performance. We demonstrate PFM's effectiveness through applications in synthetic data, protein dynamics and protein sequence data, generating novel proteins with specific properties. This method shows strong potential for drug discovery and materials science, where generating novel samples with specific properties is of great interest.</li>
</ul>

<h3>Title: How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyan Li, Chen Liang, Jing Peng, Ming Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04545">https://arxiv.org/abs/2410.04545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04545">https://arxiv.org/pdf/2410.04545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04545]] How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?(https://arxiv.org/abs/2410.04545)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people's perceptions of the writing on various aspects, including their evaluation on the quality of the writing and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals' quality evaluations of the same writing. Indeed, factors such as an individual's writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI's content generation assistance among the top-ranked writings.</li>
</ul>

<h3>Title: Bisimulation metric for Model Predictive Control</h3>
<ul>
<li><strong>Authors: </strong>Yutaka Shimizu, Masayoshi Tomizuka</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04553">https://arxiv.org/abs/2410.04553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04553">https://arxiv.org/pdf/2410.04553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04553]] Bisimulation metric for Model Predictive Control(https://arxiv.org/abs/2410.04553)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Model-based reinforcement learning has shown promise for improving sample efficiency and decision-making in complex environments. However, existing methods face challenges in training stability, robustness to noise, and computational efficiency. In this paper, we propose Bisimulation Metric for Model Predictive Control (BS-MPC), a novel approach that incorporates bisimulation metric loss in its objective function to directly optimize the encoder. This time-step-wise direct optimization enables the learned encoder to extract intrinsic information from the original state space while discarding irrelevant details and preventing the gradients and errors from diverging. BS-MPC improves training stability, robustness against input noise, and computational efficiency by reducing training time. We evaluate BS-MPC on both continuous control and image-based tasks from the DeepMind Control Suite, demonstrating superior performance and robustness compared to state-of-the-art baseline methods.</li>
</ul>

<h3>Title: $\texttt{dattri}$: A Library for Efficient Data Attribution</h3>
<ul>
<li><strong>Authors: </strong>Junwei Deng, Ting-Wei Li, Shiyuan Zhang, Shixuan Liu, Yijun Pan, Hao Huang, Xinhe Wang, Pingbang Hu, Xingjian Zhang, Jiaqi W. Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04555">https://arxiv.org/abs/2410.04555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04555">https://arxiv.org/pdf/2410.04555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04555]] $\texttt{dattri}$: A Library for Efficient Data Attribution(https://arxiv.org/abs/2410.04555)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce $\texttt{dattri}$, an open-source data attribution library that addresses the above needs. Specifically, $\texttt{dattri}$ highlights three novel design features. Firstly, $\texttt{dattri}$ proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, $\texttt{dattri}$ modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessian-vector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, $\texttt{dattri}$ provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed $\texttt{dattri}$ library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of $\texttt{dattri}$ is available at this https URL.</li>
</ul>

<h3>Title: Watermarking Decision Tree Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Stefano Calzavara, Lorenzo Cazzaro, Donald Gera, Salvatore Orlando</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04570">https://arxiv.org/abs/2410.04570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04570">https://arxiv.org/pdf/2410.04570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04570]] Watermarking Decision Tree Ensembles(https://arxiv.org/abs/2410.04570)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, watermark</a></li>
<li><strong>Abstract: </strong>Protecting the intellectual property of machine learning models is a hot topic and many watermarking schemes for deep neural networks have been proposed in the literature. Unfortunately, prior work largely neglected the investigation of watermarking techniques for other types of models, including decision tree ensembles, which are a state-of-the-art model for classification tasks on non-perceptual data. In this paper, we present the first watermarking scheme designed for decision tree ensembles, focusing in particular on random forest models. We discuss watermark creation and verification, presenting a thorough security analysis with respect to possible attacks. We finally perform an experimental evaluation of the proposed scheme, showing excellent results in terms of accuracy and security against the most relevant threats.</li>
</ul>

<h3>Title: EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?</h3>
<ul>
<li><strong>Authors: </strong>Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, John Langford, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04571">https://arxiv.org/abs/2410.04571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04571">https://arxiv.org/pdf/2410.04571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04571]] EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?(https://arxiv.org/abs/2410.04571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>How can we harness the collective capabilities of multiple Large Language Models (LLMs) to create an even more powerful model? This question forms the foundation of our research, where we propose an innovative approach to weak-to-strong (w2s) generalization-a critical problem in AI alignment. Our work introduces an easy-to-hard (e2h) framework for studying the feasibility of w2s generalization, where weak models trained on simpler tasks collaboratively supervise stronger models on more complex tasks. This setup mirrors real-world challenges, where direct human supervision is limited. To achieve this, we develop a novel AdaBoost-inspired ensemble method, demonstrating that an ensemble of weak supervisors can enhance the performance of stronger LLMs across classification and generative tasks on difficult QA datasets. In several cases, our ensemble approach matches the performance of models trained on ground-truth data, establishing a new benchmark for w2s generalization. We observe an improvement of up to 14% over existing baselines and average improvements of 5% and 4% for binary classification and generative tasks, respectively. This research points to a promising direction for enhancing AI through collective supervision, especially in scenarios where labeled data is sparse or insufficient.</li>
</ul>

<h3>Title: Enhancing 3D Human Pose Estimation Amidst Severe Occlusion with Dual Transformer Fusion</h3>
<ul>
<li><strong>Authors: </strong>Mehwish Ghafoor, Arif Mahmood, Muhammad Bilal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04574">https://arxiv.org/abs/2410.04574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04574">https://arxiv.org/pdf/2410.04574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04574]] Enhancing 3D Human Pose Estimation Amidst Severe Occlusion with Dual Transformer Fusion(https://arxiv.org/abs/2410.04574)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the field of 3D Human Pose Estimation from monocular videos, the presence of diverse occlusion types presents a formidable challenge. Prior research has made progress by harnessing spatial and temporal cues to infer 3D poses from 2D joint observations. This paper introduces a Dual Transformer Fusion (DTF) algorithm, a novel approach to obtain a holistic 3D pose estimation, even in the presence of severe occlusions. Confronting the issue of occlusion-induced missing joint data, we propose a temporal interpolation-based occlusion guidance mechanism. To enable precise 3D Human Pose Estimation, our approach leverages the innovative DTF architecture, which first generates a pair of intermediate views. Each intermediate-view undergoes spatial refinement through a self-refinement schema. Subsequently, these intermediate-views are fused to yield the final 3D human pose estimation. The entire system is end-to-end trainable. Through extensive experiments conducted on the Human3.6M and MPI-INF-3DHP datasets, our method's performance is rigorously evaluated. Notably, our approach outperforms existing state-of-the-art methods on both datasets, yielding substantial improvements. The code is available here: this https URL.</li>
</ul>

<h3>Title: Robustness Reprogramming for Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Hou, MohamadAli Torkamani, Hamid Krim, Xiaorui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04577">https://arxiv.org/abs/2410.04577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04577">https://arxiv.org/pdf/2410.04577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04577]] Robustness Reprogramming for Representation Learning(https://arxiv.org/abs/2410.04577)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters? To explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness of our approaches. This work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics.</li>
</ul>

<h3>Title: Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Jiang, Cao Xiao, Minhao Jiang, Parminder Bhatia, Taha Kass-Hout, Jimeng Sun, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04585">https://arxiv.org/abs/2410.04585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04585">https://arxiv.org/pdf/2410.04585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04585]] Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval(https://arxiv.org/abs/2410.04585)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional retrieval-augmented generation (RAG) methods attempt to address these limitations but frequently retrieve sparse or irrelevant information, undermining prediction accuracy. We introduce KARE, a novel framework that integrates knowledge graph (KG) community-level retrieval with LLM reasoning to enhance healthcare predictions. KARE constructs a comprehensive multi-source KG by integrating biomedical databases, clinical literature, and LLM-generated insights, and organizes it using hierarchical graph community detection and summarization for precise and contextually relevant information retrieval. Our key innovations include: (1) a dense medical knowledge structuring approach enabling accurate retrieval of relevant information; (2) a dynamic knowledge retrieval mechanism that enriches patient contexts with focused, multi-faceted medical insights; and (3) a reasoning-enhanced prediction framework that leverages these enriched contexts to produce both accurate and interpretable clinical predictions. Extensive experiments demonstrate that KARE outperforms leading models by up to 10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and readmission predictions. In addition to its impressive prediction accuracy, our framework leverages the reasoning capabilities of LLMs, enhancing the trustworthiness of clinical predictions.</li>
</ul>

<h3>Title: Hammer: Robust Function-Calling for On-Device Language Models via Function Masking</h3>
<ul>
<li><strong>Authors: </strong>Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04587">https://arxiv.org/abs/2410.04587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04587">https://arxiv.org/pdf/2410.04587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04587]] Hammer: Robust Function-Calling for On-Device Language Models via Function Masking(https://arxiv.org/abs/2410.04587)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function calling capabilities. This paper identifies a critical gap in existing function calling models, where performance varies significantly across benchmarks, often due to being misled by specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models' sensitivity to irrelevant functions and incorporates function masking techniques to minimize misleading. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving sota results. Our open source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function calling performance.</li>
</ul>

<h3>Title: ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Yi, Jaeyoung Lim, Juyong Yoon</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04601">https://arxiv.org/abs/2410.04601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04601">https://arxiv.org/pdf/2410.04601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04601]] ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks(https://arxiv.org/abs/2410.04601)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automated generation of scientific protocols executable by robots can significantly accelerate scientific research processes. Large Language Models (LLMs) excel at Scientific Protocol Formulation Tasks (SPFT), but the evaluation of their capabilities rely on human evaluation. Here, we propose a flexible, automatic framework to evaluate LLM's capability on SPFT: ProtocoLLM. This framework prompts the target model and GPT-4 to extract pseudocode from biology protocols using only predefined lab actions and evaluates the output of target model using LLAM-EVAL, the pseudocode generated by GPT-4 serving as a baseline and Llama-3 acting as the evaluator. Our adaptable prompt-based evaluation method, LLAM-EVAL, offers significant flexibility in terms of evaluation model, material, criteria, and is free of cost. We evaluate GPT variations, Llama, Mixtral, Gemma, Cohere, and Gemini. Overall, we find that GPT and Cohere is a powerful scientific protocol formulators. We also introduce BIOPROT 2.0, a dataset with biology protocols and corresponding pseudocodes, which can aid LLMs in formulation and evaluation of SPFT. Our work is extensible to assess LLMs on SPFT across various domains and other fields that require protocol generation for specific goals.</li>
</ul>

<h3>Title: Privacy's Peril: Unmasking the Unregulated Underground Market of Data Brokers and the Suggested Framework</h3>
<ul>
<li><strong>Authors: </strong>Rabia Bajwa, Farah Tasnur Meem</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04606">https://arxiv.org/abs/2410.04606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04606">https://arxiv.org/pdf/2410.04606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04606]] Privacy's Peril: Unmasking the Unregulated Underground Market of Data Brokers and the Suggested Framework(https://arxiv.org/abs/2410.04606)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The internet is a common place for businesses to collect and store as much client data as possible and computer storage capacity has increased exponentially due to this trend. Businesses utilize this data to enhance customer satisfaction, generate revenue, boost sales, and increase profile. However, the emerging sector of data brokers is plagued with legal challenges. In part I, we will look at what a data broker is, how it collects information, the data industry, and some of the difficulties it encounters. In Part II, we will look at potential options for regulating data brokers. All options are provided in light of the EU General Data Protection Regulation (GDPR). In Part III, we shall present our analysis and findings.</li>
</ul>

<h3>Title: VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Harshit, Tolga Tasdizen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04609">https://arxiv.org/abs/2410.04609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04609">https://arxiv.org/pdf/2410.04609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04609]] VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models(https://arxiv.org/abs/2410.04609)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The recent developments in deep learning led to the integration of natural language processing (NLP) with computer vision, resulting in powerful integrated Vision and Language Models (VLMs). Despite their remarkable capabilities, these models are frequently regarded as black boxes within the machine learning research community. This raises a critical question: which parts of an image correspond to specific segments of text, and how can we decipher these associations? Understanding these connections is essential for enhancing model transparency, interpretability, and trustworthiness. To answer this question, we present an image-text aligned human visual attention dataset that maps specific associations between image regions and corresponding text segments. We then compare the internal heatmaps generated by VL models with this dataset, allowing us to analyze and better understand the model's decision-making process. This approach aims to enhance model transparency, interpretability, and trustworthiness by providing insights into how these models align visual and linguistic information. We conducted a comprehensive study on text-guided visual saliency detection in these VL models. This study aims to understand how different models prioritize and focus on specific visual elements in response to corresponding text segments, providing deeper insights into their internal mechanisms and improving our ability to interpret their outputs.</li>
</ul>

<h3>Title: Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF</h3>
<ul>
<li><strong>Authors: </strong>Zhaolin Gao, Wenhao Zhan, Jonathan D. Chang, Gokul Swamy, Kianté Brantley, Jason D. Lee, Wen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04612">https://arxiv.org/abs/2410.04612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04612">https://arxiv.org/pdf/2410.04612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04612]] Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF(https://arxiv.org/abs/2410.04612)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn reinforcement learning from human feedback (RLHF) methods to the multi-turn setting by treating all prior dialogue turns as a long context. Such approaches suffer from covariate shift: the conversations in the training set have previous turns generated by some reference policy, which means that low training error may not necessarily correspond to good performance when the learner is actually in the conversation loop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient policy optimization approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single model to estimate $Q$-values and trains on self-generated data, addressing the covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of regression tasks on iteratively collected datasets, enabling ease of implementation. Theoretically, we prove that REFUEL can match the performance of any policy covered by the training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our model. REFUEL consistently outperforms state-of-the-art methods such as DPO and REBEL across various settings. Furthermore, despite having only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found at this https URL, and models trained by REFUEL can be found at this https URL.</li>
</ul>

<h3>Title: LRQ-Fact: LLM-Generated Relevant Questions for Multimodal Fact-Checking</h3>
<ul>
<li><strong>Authors: </strong>Alimohammad Beigi, Bohan Jiang, Dawei Li, Tharindu Kumarage, Zhen Tan, Pouya Shaeri, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04616">https://arxiv.org/abs/2410.04616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04616">https://arxiv.org/pdf/2410.04616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04616]] LRQ-Fact: LLM-Generated Relevant Questions for Multimodal Fact-Checking(https://arxiv.org/abs/2410.04616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human fact-checkers have specialized domain knowledge that allows them to formulate precise questions to verify information accuracy. However, this expert-driven approach is labor-intensive and is not scalable, especially when dealing with complex multimodal misinformation. In this paper, we propose a fully-automated framework, LRQ-Fact, for multimodal fact-checking. Firstly, the framework leverages Vision-Language Models (VLMs) and Large Language Models (LLMs) to generate comprehensive questions and answers for probing multimodal content. Next, a rule-based decision-maker module evaluates both the original content and the generated questions and answers to assess the overall veracity. Extensive experiments on two benchmarks show that LRQ-Fact improves detection accuracy for multimodal misinformation. Moreover, we evaluate its generalizability across different model backbones, offering valuable insights for further refinement.</li>
</ul>

<h3>Title: Evaluation of Code LLMs on Geospatial Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Piotr Gramacki, Bruno Martins, Piotr Szymański</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04617">https://arxiv.org/abs/2410.04617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04617">https://arxiv.org/pdf/2410.04617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04617]] Evaluation of Code LLMs on Geospatial Code Generation(https://arxiv.org/abs/2410.04617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a "mentor" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.</li>
</ul>

<h3>Title: Towards Unsupervised Blind Face Restoration using Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Tianshu Kuai, Sina Honari, Igor Gilitschenski, Alex Levinshtein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04618">https://arxiv.org/abs/2410.04618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04618">https://arxiv.org/pdf/2410.04618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04618]] Towards Unsupervised Blind Face Restoration using Diffusion Prior(https://arxiv.org/abs/2410.04618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration methods have shown remarkable performance, particularly when trained on large-scale synthetic datasets with supervised learning. These datasets are often generated by simulating low-quality face images with a handcrafted image degradation pipeline. The models trained on such synthetic degradations, however, cannot deal with inputs of unseen degradations. In this paper, we address this issue by using only a set of input images, with unknown degradations and without ground truth targets, to fine-tune a restoration model that learns to map them to clean and contextually consistent outputs. We utilize a pre-trained diffusion model as a generative prior through which we generate high quality images from the natural image distribution while maintaining the input image content through consistency constraints. These generated images are then used as pseudo targets to fine-tune a pre-trained restoration model. Unlike many recent approaches that employ diffusion models at test time, we only do so during training and thus maintain an efficient inference-time performance. Extensive experiments show that the proposed approach can consistently improve the perceptual quality of pre-trained blind face restoration models while maintaining great consistency with the input contents. Our best model also achieves the state-of-the-art results on both synthetic and real-world datasets.</li>
</ul>

<h3>Title: Punctuation Prediction for Polish Texts using Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jakub Pokrywka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04621">https://arxiv.org/abs/2410.04621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04621">https://arxiv.org/pdf/2410.04621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04621]] Punctuation Prediction for Polish Texts using Transformers(https://arxiv.org/abs/2410.04621)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Speech recognition systems typically output text lacking punctuation. However, punctuation is crucial for written text comprehension. To tackle this problem, Punctuation Prediction models are developed. This paper describes a solution for Poleval 2022 Task 1: Punctuation Prediction for Polish Texts, which scores 71.44 Weighted F1. The method utilizes a single HerBERT model finetuned to the competition data and an external dataset.</li>
</ul>

<h3>Title: Control Large Language Models via Divide and Conquer</h3>
<ul>
<li><strong>Authors: </strong>Bingxuan Li, Yiwei Wang, Tao Meng, Kai-Wei Chang, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04628">https://arxiv.org/abs/2410.04628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04628">https://arxiv.org/pdf/2410.04628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04628]] Control Large Language Models via Divide and Conquer(https://arxiv.org/abs/2410.04628)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates controllable generation for large language models (LLMs) with prompt-based control, focusing on Lexically Constrained Generation (LCG). We systematically evaluate the performance of LLMs on satisfying lexical constraints with prompt-based control, as well as their efficacy in downstream applications. We conclude that LLMs face significant challenges in consistently satisfying lexical constraints with prompt-based control. We identified three key limitations of LLMs for LCG, including (1) position bias, where LLMs tend to satisfy constraints that appear in specific positions within the input; (2) low responsiveness to decoding parameters, which render minimal impact on control of LLMs; and (3) struggle with handling the inherent complexity of certain constraints (e.g., compound words). To address these issues, we introduce a Divide and Conquer Generation strategy, effective for both white-box and black-box LLMs, to enhance LLMs performance in LCG tasks, which demonstrates over 90% improvement on success rate in the most challenging LCG task. Our analysis provides valuable insights into the performance of LLMs in LCG with prompt-based control, and our proposed strategy offers a pathway to more sophisticated and customized text generation applications.</li>
</ul>

<h3>Title: Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for Robust Ground-View Scene Rendering</h3>
<ul>
<li><strong>Authors: </strong>Yonghan Lee, Jaehoon Choi, Dongki Jung, Jaeseong Yun, Soohyun Ryu, Dinesh Manocha, Suyong Yeon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04646">https://arxiv.org/abs/2410.04646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04646">https://arxiv.org/pdf/2410.04646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04646]] Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for Robust Ground-View Scene Rendering(https://arxiv.org/abs/2410.04646)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a novel-view rendering algorithm, Mode-GS, for ground-robot trajectory datasets. Our approach is based on using anchored Gaussian splats, which are designed to overcome the limitations of existing 3D Gaussian splatting algorithms. Prior neural rendering methods suffer from severe splat drift due to scene complexity and insufficient multi-view observation, and can fail to fix splats on the true geometry in ground-robot datasets. Our method integrates pixel-aligned anchors from monocular depths and generates Gaussian splats around these anchors using residual-form Gaussian decoders. To address the inherent scale ambiguity of monocular depth, we parameterize anchors with per-view depth-scales and employ scale-consistent depth loss for online scale calibration. Our method results in improved rendering performance, based on PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns, and achieves state-of-the-art rendering performance on the R3LIVE odometry dataset and the Tanks and Temples dataset.</li>
</ul>

<h3>Title: AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Dewei Hu, Hao Li, Han Liu, Jiacheng Wang, Xing Yao, Daiwei Lu, Ipek Oguz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04648">https://arxiv.org/abs/2410.04648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04648">https://arxiv.org/pdf/2410.04648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04648]] AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation(https://arxiv.org/abs/2410.04648)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning has shown remarkable performance in medical image segmentation. However, despite its promise, deep learning has many challenges in practice due to its inability to effectively transition to unseen domains, caused by the inherent data distribution shift and the lack of manual annotations to guide domain adaptation. To tackle this problem, we present an unsupervised domain adaptation (UDA) method named AdaptDiff that enables a retinal vessel segmentation network trained on fundus photography (FP) to produce satisfactory results on unseen modalities (e.g., OCT-A) without any manual labels. For all our target domains, we first adopt a segmentation model trained on the source domain to create pseudo-labels. With these pseudo-labels, we train a conditional semantic diffusion probabilistic model to represent the target domain distribution. Experimentally, we show that even with low quality pseudo-labels, the diffusion model can still capture the conditional semantic information. Subsequently, we sample on the target domain with binary vessel masks from the source domain to get paired data, i.e., target domain synthetic images conditioned on the binary vessel map. Finally, we fine-tune the pre-trained segmentation network using the synthetic paired data to mitigate the domain gap. We assess the effectiveness of AdaptDiff on seven publicly available datasets across three distinct modalities. Our results demonstrate a significant improvement in segmentation performance across all unseen datasets. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains</h3>
<ul>
<li><strong>Authors: </strong>Shane E. Loeffler, Zan Ahmad, Syed Yusuf Ali, Carolyna Yamamoto, Dan M. Popescu, Alana Yee, Yash Lal, Natalia Trayanova, Mauro Maggioni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.SP, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04655">https://arxiv.org/abs/2410.04655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04655">https://arxiv.org/pdf/2410.04655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04655]] Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains(https://arxiv.org/abs/2410.04655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Predicting time-dependent dynamics of complex systems governed by non-linear partial differential equations (PDEs) with varying parameters and domains is a challenging task motivated by applications across various fields. We introduce a novel family of neural operators based on our Graph Fourier Neural Kernels, designed to learn solution generators for nonlinear PDEs in which the highest-order term is diffusive, across multiple domains and parameters. G-FuNK combines components that are parameter- and domain-adapted with others that are not. The domain-adapted components are constructed using a weighted graph on the discretized domain, where the graph Laplacian approximates the highest-order diffusive term, ensuring boundary condition compliance and capturing the parameter and domain-specific behavior. Meanwhile, the learned components transfer across domains and parameters via Fourier Neural Operators. This approach naturally embeds geometric and directional information, improving generalization to new test domains without need for retraining the network. To handle temporal dynamics, our method incorporates an integrated ODE solver to predict the evolution of the system. Experiments show G-FuNK's capability to accurately approximate heat, reaction diffusion, and cardiac electrophysiology equations across various geometries and anisotropic diffusivity fields. G-FuNK achieves low relative errors on unseen domains and fiber fields, significantly accelerating predictions compared to traditional finite-element solvers.</li>
</ul>

<h3>Title: ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyue Wang, Chi Chen, Fuwen Luo, Yurui Dong, Yuanchi Zhang, Yuzhuang Xu, Xiaolong Wang, Peng Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04659">https://arxiv.org/abs/2410.04659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04659">https://arxiv.org/pdf/2410.04659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04659]] ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models(https://arxiv.org/abs/2410.04659)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. Since comprehensively assessing active perception is challenging, we focus on a specialized form of Visual Question Answering (VQA) that eases the evaluation yet challenging for existing MLLMs. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 27 models, including proprietary and open-source models, and observe that the ability to read and comprehend multiple images simultaneously plays a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that our benchmark could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.</li>
</ul>

<h3>Title: Federated Learning Nodes Can Reconstruct Peers' Image Data</h3>
<ul>
<li><strong>Authors: </strong>Ethan Wilson, Kai Yue, Chau-Wai Wong, Huaiyu Dai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04661">https://arxiv.org/abs/2410.04661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04661">https://arxiv.org/pdf/2410.04661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04661]] Federated Learning Nodes Can Reconstruct Peers' Image Data(https://arxiv.org/abs/2410.04661)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, federate, diffusion</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a privacy-preserving machine learning framework that enables multiple nodes to train models on their local data and periodically average weight updates to benefit from other nodes' training. Each node's goal is to collaborate with other nodes to improve the model's performance while keeping its training data private. However, this framework does not guarantee data privacy. Prior work has shown that the gradient-sharing steps in FL can be vulnerable to data reconstruction attacks from an honest-but-curious central server. In this work, we show that an honest-but-curious node/client can also launch attacks to reconstruct peers' image data in a centralized system, presenting a severe privacy risk. We demonstrate that a single client can silently reconstruct other clients' private images using diluted information available within consecutive updates. We leverage state-of-the-art diffusion models to enhance the perceptual quality and recognizability of the reconstructed images, further demonstrating the risk of information leakage at a semantic level. This highlights the need for more robust privacy-preserving mechanisms that protect against silent client-side attacks during federated training.</li>
</ul>

<h3>Title: Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates</h3>
<ul>
<li><strong>Authors: </strong>Chaithanya Bandi, Hari Bandi, Abir Harrasse</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04663">https://arxiv.org/abs/2410.04663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04663">https://arxiv.org/pdf/2410.04663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04663]] Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates(https://arxiv.org/abs/2410.04663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores optimal architectures for evaluating the outputs of large language models (LLMs) using LLMs themselves. We propose a novel framework that interprets LLMs as advocates within an ensemble of interacting agents, allowing them to defend their answers and reach conclusions through a judge and jury system. This approach offers a more dynamic and comprehensive evaluation process compared to traditional human-based assessments or automated metrics. We discuss the motivation behind this framework, its key components, and comparative advantages. We also present a probabilistic model to evaluate the error reduction achieved by iterative advocate systems. Finally, we outline experiments to validate the effectiveness of multi-advocate architectures and discuss future research directions.</li>
</ul>

<h3>Title: CAR: Controllable Autoregressive Modeling for Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Yao, Jialin Li, Yifeng Zhou, Yong Liu, Xi Jiang, Chengjie Wang, Feng Zheng, Yuexian Zou, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04671">https://arxiv.org/abs/2410.04671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04671">https://arxiv.org/pdf/2410.04671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04671]] CAR: Controllable Autoregressive Modeling for Visual Generation(https://arxiv.org/abs/2410.04671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Controllable generation, which enables fine-grained control over generated outputs, has emerged as a critical focus in visual generative models. Currently, there are two primary technical approaches in visual generation: diffusion models and autoregressive models. Diffusion models, as exemplified by ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas autoregressive models, despite showcasing impressive generative quality and scalability, remain underexplored in terms of controllability and flexibility. In this study, we introduce Controllable AutoRegressive Modeling (CAR), a novel, plug-and-play framework that integrates conditional control into multi-scale latent variable modeling, enabling efficient control generation within a pre-trained visual autoregressive model. CAR progressively refines and captures control representations, which are injected into each autoregressive step of the pre-trained model to guide the generation process. Our approach demonstrates excellent controllability across various types of conditions and delivers higher image quality compared to previous methods. Additionally, CAR achieves robust generalization with significantly fewer training resources compared to those required for pre-training the model. To the best of our knowledge, we are the first to propose a control framework for pre-trained autoregressive visual generation models.</li>
</ul>

<h3>Title: On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Su, Yushu Li, Nanqing Liu, Kui Jia, Xulei Yang, Chuan-Sheng Foo, Xun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04682">https://arxiv.org/abs/2410.04682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04682">https://arxiv.org/pdf/2410.04682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04682]] On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning(https://arxiv.org/abs/2410.04682)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) updates the model weights during the inference stage using testing data to enhance generalization. However, this practice exposes TTA to adversarial risks. Existing studies have shown that when TTA is updated with crafted adversarial test samples, also known as test-time poisoned data, the performance on benign samples can deteriorate. Nonetheless, the perceived adversarial risk may be overstated if the poisoned data is generated under overly strong assumptions. In this work, we first review realistic assumptions for test-time data poisoning, including white-box versus grey-box attacks, access to benign data, attack budget, and more. We then propose an effective and realistic attack method that better produces poisoned samples without access to benign samples, and derive an effective in-distribution attack objective. We also design two TTA-aware attack objectives. Our benchmarks of existing attack methods reveal that the TTA methods are more robust than previously believed. In addition, we analyze effective defense strategies to help develop adversarially robust TTA methods.</li>
</ul>

<h3>Title: Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment Whole-Body Organs in CT with Light-Weighted Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Vince Zhu, Zhanghexuan Ji, Dazhou Guo, Puyang Wang, Yingda Xia, Le Lu, Xianghua Ye, Wei Zhu, Dakai Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04689">https://arxiv.org/abs/2410.04689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04689">https://arxiv.org/pdf/2410.04689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04689]] Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment Whole-Body Organs in CT with Light-Weighted Adaptation(https://arxiv.org/abs/2410.04689)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Deep segmentation networks achieve high performance when trained on specific datasets. However, in clinical practice, it is often desirable that pretrained segmentation models can be dynamically extended to enable segmenting new organs without access to previous training datasets or without training from scratch. This would ensure a much more efficient model development and deployment paradigm accounting for the patient privacy and data storage issues. This clinically preferred process can be viewed as a continual semantic segmentation (CSS) problem. Previous CSS works would either experience catastrophic forgetting or lead to unaffordable memory costs as models expand. In this work, we propose a new continual whole-body organ segmentation model with light-weighted low-rank adaptation (LoRA). We first train and freeze a pyramid vision transformer (PVT) base segmentation model on the initial task, then continually add light-weighted trainable LoRA parameters to the frozen model for each new learning task. Through a holistically exploration of the architecture modification, we identify three most important layers (i.e., patch-embedding, multi-head attention and feed forward layers) that are critical in adapting to the new segmentation tasks, while retaining the majority of the pretrained parameters fixed. Our proposed model continually segments new organs without catastrophic forgetting and meanwhile maintaining a low parameter increasing rate. Continually trained and tested on four datasets covering different body parts of a total of 121 organs, results show that our model achieves high segmentation accuracy, closely reaching the PVT and nnUNet upper bounds, and significantly outperforms other regularization-based CSS methods. When comparing to the leading architecture-based CSS method, our model has a substantial lower parameter increasing rate while achieving comparable performance.</li>
</ul>

<h3>Title: Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04691">https://arxiv.org/abs/2410.04691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04691">https://arxiv.org/pdf/2410.04691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04691]] Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning(https://arxiv.org/abs/2410.04691)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning and in-context learning (ICL) are two prevalent methods in imbuing large language models with task-specific knowledge. It is commonly believed that fine-tuning can surpass ICL given sufficient training samples as it allows the model to adjust its internal parameters based on the data. However, this paper presents a counterintuitive finding: For tasks with implicit patterns, ICL captures these patterns significantly better than fine-tuning. We developed several datasets featuring implicit patterns, such as sequences determining answers through parity or identifying reducible terms in calculations. We then evaluated the models' understanding of these patterns under both fine-tuning and ICL across models ranging from 0.5B to 7B parameters. The results indicate that models employing ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning, despite utilizing thousands of times more training samples than ICL, achieved only limited improvements. We also proposed circuit shift theory from a mechanistic interpretability's view to explain why ICL wins.</li>
</ul>

<h3>Title: MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lei Wang, Shan Dong, Yuhui Xu, Hanze Dong, Yalu Wang, Amrita Saha, Ee-Peng Lim, Caiming Xiong, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04698">https://arxiv.org/abs/2410.04698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04698">https://arxiv.org/pdf/2410.04698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04698]] MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs(https://arxiv.org/abs/2410.04698)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.</li>
</ul>

<h3>Title: The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?</h3>
<ul>
<li><strong>Authors: </strong>Alexander S. Choi, Syeda Sabrina Akter, JP Singh, Antonios Anastasopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04699">https://arxiv.org/abs/2410.04699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04699">https://arxiv.org/pdf/2410.04699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04699]] The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?(https://arxiv.org/abs/2410.04699)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages-Topic Discovery and Topic Assignment-integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.</li>
</ul>

<h3>Title: Tight Stability, Convergence, and Robustness Bounds for Predictive Coding Networks</h3>
<ul>
<li><strong>Authors: </strong>Ankur Mali, Tommaso Salvatori, Alexander Ororbia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04708">https://arxiv.org/abs/2410.04708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04708">https://arxiv.org/pdf/2410.04708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04708]] Tight Stability, Convergence, and Robustness Bounds for Predictive Coding Networks(https://arxiv.org/abs/2410.04708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Energy-based learning algorithms, such as predictive coding (PC), have garnered significant attention in the machine learning community due to their theoretical properties, such as local operations and biologically plausible mechanisms for error correction. In this work, we rigorously analyze the stability, robustness, and convergence of PC through the lens of dynamical systems theory. We show that, first, PC is Lyapunov stable under mild assumptions on its loss and residual energy functions, which implies intrinsic robustness to small random perturbations due to its well-defined energy-minimizing dynamics. Second, we formally establish that the PC updates approximate quasi-Newton methods by incorporating higher-order curvature information, which makes them more stable and able to converge with fewer iterations compared to models trained via backpropagation (BP). Furthermore, using this dynamical framework, we provide new theoretical bounds on the similarity between PC and other algorithms, i.e., BP and target propagation (TP), by precisely characterizing the role of higher-order derivatives. These bounds, derived through detailed analysis of the Hessian structures, show that PC is significantly closer to quasi-Newton updates than TP, providing a deeper understanding of the stability and efficiency of PC compared to conventional learning methods.</li>
</ul>

<h3>Title: Rule-based Data Selection for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaomin Li, Mingye Gao, Zhiwei Zhang, Chang Yue, Hong Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04715">https://arxiv.org/abs/2410.04715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04715">https://arxiv.org/pdf/2410.04715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04715]] Rule-based Data Selection for Large Language Models(https://arxiv.org/abs/2410.04715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quality of training data significantly impacts the performance of large language models (LLMs). There are increasing studies using LLMs to rate and select data based on several human-crafted metrics (rules). However, these conventional rule-based approaches often depend too heavily on human heuristics, lack effective metrics for assessing rules, and exhibit limited adaptability to new tasks. In our study, we introduce an innovative rule-based framework that utilizes the orthogonality of score vectors associated with rules as a novel metric for rule evaluations. Our approach includes an automated pipeline that first uses LLMs to generate a diverse set of rules, encompassing various rating dimensions to evaluate data quality. Then it rates a batch of data based on these rules and uses the determinantal point process (DPP) from random matrix theory to select the most orthogonal score vectors, thereby identifying a set of independent rules. These rules are subsequently used to evaluate all data, selecting samples with the highest average scores for downstream tasks such as LLM training. We verify the effectiveness of our method through two experimental setups: 1) comparisons with ground truth ratings and 2) benchmarking LLMs trained with the chosen data. Our comprehensive experiments cover a range of scenarios, including general pre-training and domain-specific fine-tuning in areas such as IMDB, Medical, Math, and Code. The outcomes demonstrate that our DPP-based rule rating method consistently outperforms other approaches, including rule-free rating, uniform sampling, importance resampling, and QuRating, in terms of both rating precision and model performance.</li>
</ul>

<h3>Title: $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization</h3>
<ul>
<li><strong>Authors: </strong>Dylan Zhang, Justin Wang, Francois Charton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04717">https://arxiv.org/abs/2410.04717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04717">https://arxiv.org/pdf/2410.04717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04717]] $\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization(https://arxiv.org/abs/2410.04717)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization $\textbf{only emerges}$ when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $\textit{$\textbf{specialist}$}$ and $\textit{$\textbf{generalist}$}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.</li>
</ul>

<h3>Title: ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction</h3>
<ul>
<li><strong>Authors: </strong>Hyungjin Chung, Dohun Lee, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04721">https://arxiv.org/abs/2410.04721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04721">https://arxiv.org/pdf/2410.04721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04721]] ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction(https://arxiv.org/abs/2410.04721)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive models (ARMs) and diffusion models (DMs) represent two leading paradigms in generative modeling, each excelling in distinct areas: ARMs in global context modeling and long-sequence generation, and DMs in generating high-quality local contexts, especially for continuous data such as images and short videos. However, ARMs often suffer from exponential error accumulation over long sequences, leading to physically implausible results, while DMs are limited by their local context generation capabilities. In this work, we introduce Autoregressive Coherent multimodal generation with Diffusion Correction (ACDC), a zero-shot approach that combines the strengths of both ARMs and DMs at the inference stage without the need for additional fine-tuning. ACDC leverages ARMs for global context generation and memory-conditioned DMs for local correction, ensuring high-quality outputs by correcting artifacts in generated multimodal tokens. In particular, we propose a memory module based on large language models (LLMs) that dynamically adjusts the conditioning texts for the DMs, preserving crucial global context information. Our experiments on multimodal tasks, including coherent multi-frame story generation and autoregressive video generation, demonstrate that ACDC effectively mitigates the accumulation of errors and significantly enhances the quality of generated outputs, achieving superior performance while remaining agnostic to specific ARM and DM architectures. Project page: this https URL</li>
</ul>

<h3>Title: ProtoNAM: Prototypical Neural Additive Models for Interpretable Deep Tabular Learning</h3>
<ul>
<li><strong>Authors: </strong>Guangzhi Xiong, Sanchit Sinha, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04723">https://arxiv.org/abs/2410.04723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04723">https://arxiv.org/pdf/2410.04723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04723]] ProtoNAM: Prototypical Neural Additive Models for Interpretable Deep Tabular Learning(https://arxiv.org/abs/2410.04723)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Generalized additive models (GAMs) have long been a powerful white-box tool for the intelligible analysis of tabular data, revealing the influence of each feature on the model predictions. Despite the success of neural networks (NNs) in various domains, their application as NN-based GAMs in tabular data analysis remains suboptimal compared to tree-based ones, and the opacity of encoders in NN-GAMs also prevents users from understanding how networks learn the functions. In this work, we propose a new deep tabular learning method, termed Prototypical Neural Additive Model (ProtoNAM), which introduces prototypes into neural networks in the framework of GAMs. With the introduced prototype-based feature activation, ProtoNAM can flexibly model the irregular mapping from tabular features to the outputs while maintaining the explainability of the final prediction. We also propose a gradient-boosting inspired hierarchical shape function modeling method, facilitating the discovery of complex feature patterns and bringing transparency into the learning process of each network layer. Our empirical evaluations demonstrate that ProtoNAM outperforms all existing NN-based GAMs, while providing additional insights into the shape function learned for each feature. The source code of ProtoNAM is available at \url{this https URL}.</li>
</ul>

<h3>Title: Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang, Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04727">https://arxiv.org/abs/2410.04727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04727">https://arxiv.org/pdf/2410.04727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04727]] Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models(https://arxiv.org/abs/2410.04727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Numerous recent works target to extend effective context length for language models and various methods, tasks and benchmarks exist to measure model's effective memorization length. However, through thorough investigations, we find limitations for currently existing evaluations on model's memorization capability. We provide an extensive survey for limitations in this work and propose a new method called forgetting curve to measure the memorization capability of long-context models. We show that forgetting curve has the advantage of being robust to the tested corpus and the experimental settings, of not relying on prompts and can be applied to any model size. We apply our forgetting curve to a large variety of models involving both transformer and RNN/SSM based architectures. Our measurement provides empirical evidence for the effectiveness of transformer extension techniques while raises questions for the effective length of RNN/SSM based models. We also examine the difference between our measurement and existing benchmarks as well as popular metrics for various models. Our code and results can be found at this https URL.</li>
</ul>

<h3>Title: Efficient transformer with reinforced position embedding for language models</h3>
<ul>
<li><strong>Authors: </strong>Yen-Che Hsiao, Abhishek Dutta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04731">https://arxiv.org/abs/2410.04731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04731">https://arxiv.org/pdf/2410.04731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04731]] Efficient transformer with reinforced position embedding for language models(https://arxiv.org/abs/2410.04731)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an efficient transformer architecture that uses reinforced positional embedding to obtain superior performance with half the number of encoder decoder layers. We demonstrate that concatenating positional encoding with trainable token embeddings, normalizing columns in the token embedding matrix, and using the normalized token embedding matrix as the value of the attention layer improve the training and validation loss and the training time in an encoder-decoder Transformer model for a Portuguese-English translation task with 10 epochs or 12 hours of training across 10 trials. Our method, with roughly a threefold parameter reduction compared to the baseline model, yields a mean training loss of 1.21, a mean validation loss of 1.51, and an average training time of 1352.27 seconds per epoch, surpassing the baseline model with the same embedding dimension that employs addition of positional encoding and token embeddings, which achieves a mean training loss of 1.96, a validation loss of 2.18, and an average training time of 4297.79 seconds per epoch. Additionally, we evaluated our proposed architecture and the baseline across 14 diverse translation datasets from TensorFlow. The results indicate that our method consistently achieves lower or comparable training and validation losses, suggesting enhanced learning efficiency.</li>
</ul>

<h3>Title: PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners</h3>
<ul>
<li><strong>Authors: </strong>Yujin Tang, Lu Qi, Fei Xie, Xiangtai Li, Chao Ma, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04733">https://arxiv.org/abs/2410.04733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04733">https://arxiv.org/pdf/2410.04733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04733]] PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners(https://arxiv.org/abs/2410.04733)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Spatiotemporal predictive learning methods generally fall into two categories: recurrent-based approaches, which face challenges in parallelization and performance, and recurrent-free methods, which employ convolutional neural networks (CNNs) as encoder-decoder architectures. These methods benefit from strong inductive biases but often at the expense of scalability and generalization. This paper proposes PredFormer, a pure transformer-based framework for spatiotemporal predictive learning. Motivated by the Vision Transformers (ViT) design, PredFormer leverages carefully designed Gated Transformer blocks, following a comprehensive analysis of 3D attention mechanisms, including full-, factorized-, and interleaved- spatial-temporal attention. With its recurrent-free, transformer-based design, PredFormer is both simple and efficient, significantly outperforming previous methods by large margins. Extensive experiments on synthetic and real-world datasets demonstrate that PredFormer achieves state-of-the-art performance. On Moving MNIST, PredFormer achieves a 51.3% reduction in MSE relative to SimVP. For TaxiBJ, the model decreases MSE by 33.1% and boosts FPS from 533 to 2364. Additionally, on WeatherBench, it reduces MSE by 11.1% while enhancing FPS from 196 to 404. These performance gains in both accuracy and efficiency demonstrate PredFormer's potential for real-world applications. The source code will be released at this https URL.</li>
</ul>

<h3>Title: TLDR: Token-Level Detective Reward Model for Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04734">https://arxiv.org/abs/2410.04734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04734">https://arxiv.org/pdf/2410.04734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04734]] TLDR: Token-Level Detective Reward Model for Large Vision Language Models(https://arxiv.org/abs/2410.04734)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a $\textbf{T}$oken-$\textbf{L}$evel $\textbf{D}$etective $\textbf{R}$eward Model ($\textbf{TLDR}$) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data.</li>
</ul>

<h3>Title: Diffusion Models in 3D Vision: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhen Wang, Dongyuan Li, Renhe Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04738">https://arxiv.org/abs/2410.04738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04738">https://arxiv.org/pdf/2410.04738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04738]] Diffusion Models in 3D Vision: A Survey(https://arxiv.org/abs/2410.04738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, 3D vision has become a crucial field within computer vision, powering a wide range of applications such as autonomous driving, robotics, augmented reality (AR), and medical imaging. This field relies on the accurate perception, understanding, and reconstruction of 3D scenes from 2D data sources like images and videos. Diffusion models, originally designed for 2D generative tasks, offer the potential for more flexible, probabilistic approaches that can better capture the variability and uncertainty present in real-world 3D data. However, traditional methods often struggle with efficiency and scalability. In this paper, we review the state-of-the-art approaches that leverage diffusion models for 3D visual tasks, including but not limited to 3D object generation, shape completion, point cloud reconstruction, and scene understanding. We provide an in-depth discussion of the underlying mathematical principles of diffusion models, outlining their forward and reverse processes, as well as the various architectural advancements that enable these models to work with 3D datasets. We also discuss the key challenges in applying diffusion models to 3D vision, such as handling occlusions and varying point densities, and the computational demands of high-dimensional data. Finally, we discuss potential solutions, including improving computational efficiency, enhancing multimodal fusion, and exploring the use of large-scale pretraining for better generalization across 3D tasks. This paper serves as a foundation for future exploration and development in this rapidly evolving field.</li>
</ul>

<h3>Title: PSA: Private Set Alignment for Secure and Collaborative Analytics on Large-Scale Data</h3>
<ul>
<li><strong>Authors: </strong>Jiabo Wang, Elmo Xuyun Huang, Pu Duan, Huaxiong Wang, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04746">https://arxiv.org/abs/2410.04746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04746">https://arxiv.org/pdf/2410.04746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04746]] PSA: Private Set Alignment for Secure and Collaborative Analytics on Large-Scale Data(https://arxiv.org/abs/2410.04746)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Enforcement of privacy regulation is essential for collaborative data analytics. In this work, we address a scenario in which two companies expect to securely join their datasets with respect to their common customers to maximize data insights. Apart from the necessary protection of raw data, it becomes more challenging to protect the identities and attributes of common customers, as it requires participants to align their records associated with common customers without knowing who they are. We proposed a solution, dubbed PSA, for this scenario, which is effectively applicable to real-world use cases, such as evaluating advertising conversion using data from both publishers and merchants. The contributions of this work are threefold: 1. We defined the notion of PSA with two levels of privacy protection and proposed novel PSA protocols based on the modified oblivious switching network, which leverages efficient symmetric key operations and offline precomputation to save online run time. 2. We implemented and benchmarked the proposed protocols in different network conditions by joining two datasets, each at the scale of one million records, in 35.5 sec on a single thread with a network bandwidth of 500 Mbps, resulting in an X100 improvement over the existing Homomorphic based protocols. 3. We give new proof for an algorithm of quasi-linear complexity that constructs an oblivious switching network to achieve a target permutation distinct from the existing one in the literature.</li>
</ul>

<h3>Title: LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies</h3>
<ul>
<li><strong>Authors: </strong>Ameer Hamza, Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04749">https://arxiv.org/abs/2410.04749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04749">https://arxiv.org/pdf/2410.04749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04749]] LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies(https://arxiv.org/abs/2410.04749)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>Generating Natural Language Explanations (NLEs) for model predictions on medical images, particularly those depicting thoracic pathologies, remains a critical and challenging task. Existing methodologies often struggle due to general models' insufficient domain-specific medical knowledge and privacy concerns associated with retrieval-based augmentation techniques. To address these issues, we propose a novel Vision-Language framework augmented with a Knowledge Graph (KG)-based datastore, which enhances the model's understanding by incorporating additional domain-specific medical knowledge essential for generating accurate and informative NLEs. Our framework employs a KG-based retrieval mechanism that not only improves the precision of the generated explanations but also preserves data privacy by avoiding direct data retrieval. The KG datastore is designed as a plug-and-play module, allowing for seamless integration with various model architectures. We introduce and evaluate three distinct frameworks within this paradigm: KG-LLaVA, which integrates the pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are validated on the MIMIC-NLE dataset, where they achieve state-of-the-art results, underscoring the effectiveness of KG augmentation in generating high-quality NLEs for thoracic pathologies.</li>
</ul>

<h3>Title: Intriguing Properties of Large Language and Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Yechan Hwang, Ho-Jin Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04751">https://arxiv.org/abs/2410.04751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04751">https://arxiv.org/pdf/2410.04751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04751]] Intriguing Properties of Large Language and Vision Models(https://arxiv.org/abs/2410.04751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.</li>
</ul>

<h3>Title: Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zimu Wang, Lei Xia, Wei Wang, Xinya Du</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04752">https://arxiv.org/abs/2410.04752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04752">https://arxiv.org/pdf/2410.04752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04752]] Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering(https://arxiv.org/abs/2410.04752)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>As an essential task in information extraction (IE), Event-Event Causal Relation Extraction (ECRE) aims to identify and classify the causal relationships between event mentions in natural language texts. However, existing research on ECRE has highlighted two critical challenges, including the lack of document-level modeling and causal hallucinations. In this paper, we propose a Knowledge-guided binary Question Answering (KnowQA) method with event structures for ECRE, consisting of two stages: Event Structure Construction and Binary Question Answering. We conduct extensive experiments under both zero-shot and fine-tuning settings with large language models (LLMs) on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the usefulness of event structures on document-level ECRE and the effectiveness of KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only the effectiveness but also the high generalizability and low inconsistency of our method, particularly when with complete event structures after fine-tuning the models.</li>
</ul>

<h3>Title: A Comprehensive Study on GDPR-Oriented Analysis of Privacy Policies: Taxonomy, Corpus and GDPR Concept Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Peng Tang, Xin Li, Yuxin Chen, Weidong Qiu, Haochen Mei, Allison Holmes, Fenghua Li, Shujun Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04754">https://arxiv.org/abs/2410.04754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04754">https://arxiv.org/pdf/2410.04754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04754]] A Comprehensive Study on GDPR-Oriented Analysis of Privacy Policies: Taxonomy, Corpus and GDPR Concept Classifiers(https://arxiv.org/abs/2410.04754)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine learning based classifiers that take a privacy policy as the input and predict relevant concepts are useful in different applications such as (semi-)automated compliance analysis against requirements of the EU GDPR. In all past studies, such classifiers produce a concept label per segment (e.g., sentence or paragraph) and their performances were evaluated by using a dataset of labeled segments without considering the privacy policy they belong to. However, such an approach could overestimate the performance in real-world settings, where all segments in a new privacy policy are supposed to be unseen. Additionally, we also observed other research gaps, including the lack of a more complete GDPR taxonomy and the less consideration of hierarchical information in privacy policies. To fill such research gaps, we developed a more complete GDPR taxonomy, created the first corpus of labeled privacy policies with hierarchical information, and conducted the most comprehensive performance evaluation of GDPR concept classifiers for privacy policies. Our work leads to multiple novel findings, including the confirmed inappropriateness of splitting training and test sets at the segment level, the benefits of considering hierarchical information, and the limitations of the "one size fits all" approach, and the significance of testing cross-corpus generalizability.</li>
</ul>

<h3>Title: WTCL-Dehaze: Rethinking Real-world Image Dehazing via Wavelet Transform and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Divine Joseph Appiah, Donghai Guan, Abdul Nasser Kasule, Mingqiang Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04762">https://arxiv.org/abs/2410.04762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04762">https://arxiv.org/pdf/2410.04762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04762]] WTCL-Dehaze: Rethinking Real-world Image Dehazing via Wavelet Transform and Contrastive Learning(https://arxiv.org/abs/2410.04762)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Images captured in hazy outdoor conditions often suffer from colour distortion, low contrast, and loss of detail, which impair high-level vision tasks. Single image dehazing is essential for applications such as autonomous driving and surveillance, with the aim of restoring image clarity. In this work, we propose WTCL-Dehaze an enhanced semi-supervised dehazing network that integrates Contrastive Loss and Discrete Wavelet Transform (DWT). We incorporate contrastive regularization to enhance feature representation by contrasting hazy and clear image pairs. Additionally, we utilize DWT for multi-scale feature extraction, effectively capturing high-frequency details and global structures. Our approach leverages both labelled and unlabelled data to mitigate the domain gap and improve generalization. The model is trained on a combination of synthetic and real-world datasets, ensuring robust performance across different scenarios. Extensive experiments demonstrate that our proposed algorithm achieves superior performance and improved robustness compared to state-of-the-art single image dehazing methods on both benchmark datasets and real-world images.</li>
</ul>

<h3>Title: Double Oracle Neural Architecture Search for Game Theoretic Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Aye Phyu Phyu Aung, Xinrun Wang, Ruiyu Wang, Hau Chan, Bo An, Xiaoli Li, J. Senthilnath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04764">https://arxiv.org/abs/2410.04764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04764">https://arxiv.org/pdf/2410.04764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04764]] Double Oracle Neural Architecture Search for Game Theoretic Deep Learning Models(https://arxiv.org/abs/2410.04764)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new approach to train deep learning models using game theory concepts including Generative Adversarial Networks (GANs) and Adversarial Training (AT) where we deploy a double-oracle framework using best response oracles. GAN is essentially a two-player zero-sum game between the generator and the discriminator. The same concept can be applied to AT with attacker and classifier as players. Training these models is challenging as a pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium is difficult as training algorithms for both GAN and AT have a large-scale strategy space. Extending our preliminary model DO-GAN, we propose the methods to apply the double oracle framework concept to Adversarial Neural Architecture Search (NAS for GAN) and Adversarial Training (NAS for AT) algorithms. We first generalize the players' strategies as the trained models of generator and discriminator from the best response oracles. We then compute the meta-strategies using a linear program. For scalability of the framework where multiple network models of best responses are stored in the memory, we prune the weakly-dominated players' strategies to keep the oracles from becoming intractable. Finally, we conduct experiments on MNIST, CIFAR-10 and TinyImageNet for DONAS-GAN. We also evaluate the robustness under FGSM and PGD attacks on CIFAR-10, SVHN and TinyImageNet for DONAS-AT. We show that all our variants have significant improvements in both subjective qualitative evaluation and quantitative metrics, compared with their respective base architectures.</li>
</ul>

<h3>Title: Granular Ball Twin Support Vector Machine</h3>
<ul>
<li><strong>Authors: </strong>A. Quadir, M. Sajid, M. Tanveer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04774">https://arxiv.org/abs/2410.04774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04774">https://arxiv.org/pdf/2410.04774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04774]] Granular Ball Twin Support Vector Machine(https://arxiv.org/abs/2410.04774)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood Estimator in Mixture ModelsTwin support vector machine (TSVM) is an emerging machine learning model with versatile applicability in classification and regression endeavors. Nevertheless, TSVM confronts noteworthy challenges: $(i)$ the imperative demand for matrix inversions presents formidable obstacles to its efficiency and applicability on large-scale datasets; $(ii)$ the omission of the structural risk minimization (SRM) principle in its primal formulation heightens the vulnerability to overfitting risks; and $(iii)$ the TSVM exhibits a high susceptibility to noise and outliers, and also demonstrates instability when subjected to resampling. In view of the aforementioned challenges, we propose the granular ball twin support vector machine (GBTSVM). GBTSVM takes granular balls, rather than individual data points, as inputs to construct a classifier. These granular balls, characterized by their coarser granularity, exhibit robustness to resampling and reduced susceptibility to the impact of noise and outliers. We further propose a novel large-scale granular ball twin support vector machine (LS-GBTSVM). LS-GBTSVM's optimization formulation ensures two critical facets: $(i)$ it eliminates the need for matrix inversions, streamlining the LS-GBTSVM's computational efficiency, and $(ii)$ it incorporates the SRM principle through the incorporation of regularization terms, effectively addressing the issue of overfitting. The proposed LS-GBTSVM exemplifies efficiency, scalability for large datasets, and robustness against noise and outliers. We conduct a comprehensive evaluation of the GBTSVM and LS-GBTSVM models on benchmark datasets from UCI, KEEL, and NDC datasets. Our experimental findings and statistical analyses affirm the superior generalization prowess of the proposed GBTSVM and LS-GBTSVM models.</li>
</ul>

<h3>Title: MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models (MLLMs)</h3>
<ul>
<li><strong>Authors: </strong>Shih-Han Chou, Shivam Chandhok, James J. Little, Leonid Sigal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04778">https://arxiv.org/abs/2410.04778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04778">https://arxiv.org/pdf/2410.04778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04778]] MM-R$^3$: On (In-)Consistency of Multi-modal Large Language Models (MLLMs)(https://arxiv.org/abs/2410.04778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the advent of Large Language Models (LLMs) and Multimodal (Visio-lingual) LLMs, a flurry of research has emerged, analyzing the performance of such models across a diverse array of tasks. While most studies focus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models through task accuracy (e.g., Visual Question Answering, grounding) across various datasets, our work explores the related but complementary aspect of consistency - the ability of an MLLM model to produce semantically similar or identical responses to semantically similar queries. We note that consistency is a fundamental prerequisite (necessary but not sufficient condition) for robustness and trust in MLLMs. Humans, in particular, are known to be highly consistent (even if not always accurate) in their responses, and consistency is inherently expected from AI systems. Armed with this perspective, we propose the MM-R$^3$ benchmark, which analyses the performance in terms of consistency and accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image Restyling, and Context Reasoning. Our analysis reveals that consistency does not always align with accuracy, indicating that models with higher accuracy are not necessarily more consistent, and vice versa. Furthermore, we propose a simple yet effective mitigation strategy in the form of an adapter module trained to minimize inconsistency across prompts. With our proposed strategy, we are able to achieve absolute improvements of 5.7% and 12.5%, on average on widely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their existing counterparts.</li>
</ul>

<h3>Title: Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality</h3>
<ul>
<li><strong>Authors: </strong>Guanyu Zhou, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04780">https://arxiv.org/abs/2410.04780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04780">https://arxiv.org/pdf/2410.04780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04780]] Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality(https://arxiv.org/abs/2410.04780)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: this https URL</li>
</ul>

<h3>Title: Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Jiahuan Li, Yiqing Cao, Shujian Huang, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04784">https://arxiv.org/abs/2410.04784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04784">https://arxiv.org/pdf/2410.04784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04784]] Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge(https://arxiv.org/abs/2410.04784)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Having been trained on massive pretraining data, large language models have shown excellent performance on many knowledge-intensive tasks. However, pretraining data tends to contain misleading and even conflicting information, and it is intriguing to understand how LLMs handle these noisy data during training. In this study, we systematically analyze LLMs' learning preferences for data with conflicting knowledge. We find that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts. This finding is generalizable across models and languages and is more evident in larger models. An in-depth analysis reveals that LLMs tend to trust data with features that signify consistency with the majority of data, and it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data.</li>
</ul>

<h3>Title: Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Mónica Apellaniz Portos, Roberto Labadie-Tamayo, Claudius Stemmler, Erwin Feyersinger, Andreas Babic, Franziska Bruckner, Vrääth Öhner, Matthias Zeppelzauer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04789">https://arxiv.org/abs/2410.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04789">https://arxiv.org/pdf/2410.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04789]] Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning(https://arxiv.org/abs/2410.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present an approach for the analysis of hybrid visual compositions in animation in the domain of ephemeral film. We combine ideas from semi-supervised and weakly supervised learning to train a model that can segment hybrid compositions without requiring pre-labeled segmentation masks. We evaluate our approach on a set of ephemeral films from 13 film archives. Results demonstrate that the proposed learning strategy yields a performance close to a fully supervised baseline. On a qualitative level the performed analysis provides interesting insights on hybrid compositions in animation film.</li>
</ul>

<h3>Title: Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing Thai Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dahyun Kim, Sukyung Lee, Yungi Kim, Attapol Rutherford, Chanjun Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04795">https://arxiv.org/abs/2410.04795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04795">https://arxiv.org/pdf/2410.04795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04795]] Representing the Under-Represented: Cultural and Core Capability Benchmarks for Developing Thai Large Language Models(https://arxiv.org/abs/2410.04795)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has highlighted the need for robust evaluation frameworks that assess their core capabilities, such as reasoning, knowledge, and commonsense, leading to the inception of certain widely-used benchmark suites such as the H6 benchmark. However, these benchmark suites are primarily built for the English language, and there exists a lack thereof for under-represented languages, in terms of LLM development, such as Thai. On the other hand, developing LLMs for Thai should also include enhancing the cultural understanding as well as core capabilities. To address these dual challenge in Thai LLM research, we propose two key benchmarks: Thai-H6 and Thai Cultural and Linguistic Intelligence Benchmark (ThaiCLI). Through a thorough evaluation of various LLMs with multi-lingual capabilities, we provide a comprehensive analysis of the proposed benchmarks and how they contribute to Thai LLM development. Furthermore, we will make both the datasets and evaluation code publicly available to encourage further research and development for Thai LLMs.</li>
</ul>

<h3>Title: DAPE V2: Process Attention Score as Feature Map for Length Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04798">https://arxiv.org/abs/2410.04798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04798">https://arxiv.org/pdf/2410.04798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04798]] DAPE V2: Process Attention Score as Feature Map for Length Extrapolation(https://arxiv.org/abs/2410.04798)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens, in contrast to earlier feed-forward neural networks. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem. The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.</li>
</ul>

<h3>Title: Transforming Color: A Novel Image Colorization Method</h3>
<ul>
<li><strong>Authors: </strong>Hamza Shafiq, Bumshik Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04799">https://arxiv.org/abs/2410.04799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04799">https://arxiv.org/pdf/2410.04799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04799]] Transforming Color: A Novel Image Colorization Method(https://arxiv.org/abs/2410.04799)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel method for image colorization that utilizes a color transformer and generative adversarial networks (GANs) to address the challenge of generating visually appealing colorized images. Conventional approaches often struggle with capturing long-range dependencies and producing realistic colorizations. The proposed method integrates a transformer architecture to capture global information and a GAN framework to improve visual quality. In this study, a color encoder that utilizes a random normal distribution to generate color features is applied. These features are then integrated with grayscale image features to enhance the overall representation of the images. Our method demonstrates superior performance compared with existing approaches by utilizing the capacity of the transformer, which can capture long-range dependencies and generate a realistic colorization of the GAN. Experimental results show that the proposed network significantly outperforms other state-of-the-art colorization techniques, highlighting its potential for image colorization. This research opens new possibilities for precise and visually compelling image colorization in domains such as digital restoration and historical image analysis.</li>
</ul>

<h3>Title: Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering</h3>
<ul>
<li><strong>Authors: </strong>Kazumoto Nakamura, Yuji Nozawa, Yu-Chieh Lin, Kengo Nakata, Youyang Ng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04801">https://arxiv.org/abs/2410.04801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04801">https://arxiv.org/pdf/2410.04801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04801]] Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering(https://arxiv.org/abs/2410.04801)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The goal of this paper is to improve the performance of pretrained Vision Transformer (ViT) models, particularly DINOv2, in image clustering task without requiring re-training or fine-tuning. As model size increases, high-norm artifacts anomaly appears in the patches of multi-head attention. We observe that this anomaly leads to reduced accuracy in zero-shot image clustering. These artifacts are characterized by disproportionately large values in the attention map compared to other patch tokens. To address these artifacts, we propose an approach called Inference-Time Attention Engineering (ITAE), which manipulates attention function during inference. Specifically, we identify the artifacts by investigating one of the Query-Key-Value (QKV) patches in the multi-head attention and attenuate their corresponding attention values inside the pretrained models. ITAE shows improved clustering accuracy on multiple datasets by exhibiting more expressive features in latent space. Our findings highlight the potential of ITAE as a practical solution for reducing artifacts in pretrained ViT models and improving model performance in clustering tasks without the need for re-training or fine-tuning.</li>
</ul>

<h3>Title: Timer-XL: Long-Context Transformers for Unified Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04803">https://arxiv.org/abs/2410.04803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04803">https://arxiv.org/pdf/2410.04803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04803]] Timer-XL: Long-Context Transformers for Unified Time Series Forecasting(https://arxiv.org/abs/2410.04803)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We present Timer-XL, a generative Transformer for unified time series forecasting. To uniformly predict 1D and 2D time series, we generalize next token prediction, predominantly adopted for causal generation of 1D sequences, to multivariate next token prediction. The proposed paradigm uniformly formulates various forecasting scenarios as a long-context generation problem. We opt for the generative Transformer, which can capture global-range and causal dependencies while providing contextual flexibility, to implement unified forecasting on univariate series characterized by non-stationarity, multivariate time series with complicated dynamics and correlations, and covariate-informed contexts that include both endogenous and exogenous variables. Technically, we propose a universal TimeAttention to facilitate generative Transformers on time series, which can effectively capture fine-grained intra- and inter-series dependencies of flattened time series tokens (patches) and is further strengthened by position embeddings in both temporal and variable dimensions. Timer-XL achieves state-of-the-art performance across challenging forecasting benchmarks through a unified approach. As a large time series model, it demonstrates notable model transferability by large-scale pre-training, as well as contextual flexibility in token lengths, positioning it as a one-for-all forecaster.</li>
</ul>

<h3>Title: FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haokun Chen, Hang Li, Yao Zhang, Gengyuan Zhang, Jinhe Bi, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04810">https://arxiv.org/abs/2410.04810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04810">https://arxiv.org/pdf/2410.04810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04810]] FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models(https://arxiv.org/abs/2410.04810)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, diffusion</a></li>
<li><strong>Abstract: </strong>One-Shot Federated Learning (OSFL), a special decentralized machine learning paradigm, has recently gained significant attention. OSFL requires only a single round of client data or model upload, which reduces communication costs and mitigates privacy threats compared to traditional FL. Despite these promising prospects, existing methods face challenges due to client data heterogeneity and limited data quantity when applied to real-world OSFL systems. Recently, Latent Diffusion Models (LDM) have shown remarkable advancements in synthesizing high-quality images through pretraining on large-scale datasets, thereby presenting a potential solution to overcome these issues. However, directly applying pretrained LDM to heterogeneous OSFL results in significant distribution shifts in synthetic data, leading to performance degradation in classification models trained on such data. This issue is particularly pronounced in rare domains, such as medical imaging, which are underrepresented in LDM's pretraining data. To address this challenge, we propose Federated Bi-Level Personalization (FedBiP), which personalizes the pretrained LDM at both instance-level and concept-level. Hereby, FedBiP synthesizes images following the client's local data distribution without compromising the privacy regulations. FedBiP is also the first approach to simultaneously address feature space heterogeneity and client data scarcity in OSFL. Our method is validated through extensive experiments on three OSFL benchmarks with feature space heterogeneity, as well as on challenging medical and satellite image datasets with label heterogeneity. The results demonstrate the effectiveness of FedBiP, which substantially outperforms other OSFL methods.</li>
</ul>

<h3>Title: Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Zhu, Jinhui Hou, Hui Liu, Huanqiang Zeng, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04811">https://arxiv.org/abs/2410.04811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04811">https://arxiv.org/pdf/2410.04811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04811]] Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration(https://arxiv.org/abs/2410.04811)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The differential equation-based image restoration approach aims to establish learnable trajectories connecting high-quality images to a tractable distribution, e.g., low-quality images or a Gaussian distribution. In this paper, we reformulate the trajectory optimization of this kind of method, focusing on enhancing both reconstruction quality and efficiency. Initially, we navigate effective restoration paths through a reinforcement learning process, gradually steering potential trajectories toward the most precise options. Additionally, to mitigate the considerable computational burden associated with iterative sampling, we propose cost-aware trajectory distillation to streamline complex paths into several manageable steps with adaptable sizes. Moreover, we fine-tune a foundational diffusion model (FLUX) with 12B parameters by using our algorithms, producing a unified framework for handling 7 kinds of image restoration tasks. Extensive experiments showcase the significant superiority of the proposed method, achieving a maximum PSNR improvement of 2.1 dB over state-of-the-art methods, while also greatly enhancing visual perceptual quality. Project page: \url{this https URL}.</li>
</ul>

<h3>Title: Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS, nlin.CD, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04814">https://arxiv.org/abs/2410.04814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04814">https://arxiv.org/pdf/2410.04814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04814]] Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data(https://arxiv.org/abs/2410.04814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In science, we are often interested in obtaining a generative model of the underlying system dynamics from observed time series. While powerful methods for dynamical systems reconstruction (DSR) exist when data come from a single domain, how to best integrate data from multiple dynamical regimes and leverage it for generalization is still an open question. This becomes particularly important when individual time series are short, and group-level information may help to fill in for gaps in single-domain data. At the same time, averaging is not an option in DSR, as it will wipe out crucial dynamical properties (e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is needed that enables to efficiently harvest group-level (multi-domain) information while retaining all single-domain dynamical characteristics. Here we provide such a hierarchical approach and showcase it on popular DSR benchmarks, as well as on neuroscientific and medical time series. In addition to faithful reconstruction of all individual dynamical regimes, our unsupervised methodology discovers common low-dimensional feature spaces in which datasets with similar dynamics cluster. The features spanning these spaces were further dynamically highly interpretable, surprisingly in often linear relation to control parameters that govern the dynamics of the underlying system. Finally, we illustrate transfer learning and generalization to new parameter regimes.</li>
</ul>

<h3>Title: Resource-Efficient Multiview Perception: Integrating Semantic Masking with Masked Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Kosta Dakic, Kanchana Thilakarathna, Rodrigo N. Calheiros, Teng Joon Lim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04817">https://arxiv.org/abs/2410.04817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04817">https://arxiv.org/pdf/2410.04817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04817]] Resource-Efficient Multiview Perception: Integrating Semantic Masking with Masked Autoencoders(https://arxiv.org/abs/2410.04817)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multiview systems have become a key technology in modern computer vision, offering advanced capabilities in scene understanding and analysis. However, these systems face critical challenges in bandwidth limitations and computational constraints, particularly for resource-limited camera nodes like drones. This paper presents a novel approach for communication-efficient distributed multiview detection and tracking using masked autoencoders (MAEs). We introduce a semantic-guided masking strategy that leverages pre-trained segmentation models and a tunable power function to prioritize informative image regions. This approach, combined with an MAE, reduces communication overhead while preserving essential visual information. We evaluate our method on both virtual and real-world multiview datasets, demonstrating comparable performance in terms of detection and tracking performance metrics compared to state-of-the-art techniques, even at high masking ratios. Our selective masking algorithm outperforms random masking, maintaining higher accuracy and precision as the masking ratio increases. Furthermore, our approach achieves a significant reduction in transmission data volume compared to baseline methods, thereby balancing multiview tracking performance with communication efficiency.</li>
</ul>

<h3>Title: MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaichen Huang, Jiahao Huo, Yibo Yan, Kun Wang, Yutao Yue, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04819">https://arxiv.org/abs/2410.04819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04819">https://arxiv.org/pdf/2410.04819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04819]] MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models(https://arxiv.org/abs/2410.04819)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, multimodal large language models (MLLMs) have significantly advanced, integrating more modalities into diverse applications. However, the lack of explainability remains a major barrier to their use in scenarios requiring decision transparency. Current neuron-level explanation paradigms mainly focus on knowledge localization or language- and domain-specific analyses, leaving the exploration of multimodality largely unaddressed. To tackle these challenges, we propose MINER, a transferable framework for mining modality-specific neurons (MSNs) in MLLMs, which comprises four stages: (1) modality separation, (2) importance score calculation, (3) importance score aggregation, (4) modality-specific neuron selection. Extensive experiments across six benchmarks and two representative MLLMs show that (I) deactivating ONLY 2% of MSNs significantly reduces MLLMs performance (0.56 to 0.24 for Qwen2-VL, 0.69 to 0.31 for Qwen2-Audio), (II) different modalities mainly converge in the lower layers, (III) MSNs influence how key information from various modalities converges to the last token, (IV) two intriguing phenomena worth further investigation, i.e., semantic probing and semantic telomeres. The source code is available at this URL.</li>
</ul>

<h3>Title: CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models</h3>
<ul>
<li><strong>Authors: </strong>Songning Lai, Jiayu Yang, Yu Huang, Lijie Hu, Tianlang Xue, Zhangyi Hu, Jiaxu Li, Haicheng Liao, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04823">https://arxiv.org/abs/2410.04823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04823">https://arxiv.org/pdf/2410.04823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04823]] CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models(https://arxiv.org/abs/2410.04823)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal, interpretability</a></li>
<li><strong>Abstract: </strong>Despite the transformative impact of deep learning across multiple domains, the inherent opacity of these models has driven the development of Explainable Artificial Intelligence (XAI). Among these efforts, Concept Bottleneck Models (CBMs) have emerged as a key approach to improve interpretability by leveraging high-level semantic information. However, CBMs, like other machine learning models, are susceptible to security threats, particularly backdoor attacks, which can covertly manipulate model behaviors. Understanding that the community has not yet studied the concept level backdoor attack of CBM, because of "Better the devil you know than the devil you don't know.", we introduce CAT (Concept-level Backdoor ATtacks), a methodology that leverages the conceptual representations within CBMs to embed triggers during training, enabling controlled manipulation of model predictions at inference time. An enhanced attack pattern, CAT+, incorporates a correlation function to systematically select the most effective and stealthy concept triggers, thereby optimizing the attack's impact. Our comprehensive evaluation framework assesses both the attack success rate and stealthiness, demonstrating that CAT and CAT+ maintain high performance on clean data while achieving significant targeted effects on backdoored datasets. This work underscores the potential security risks associated with CBMs and provides a robust testing methodology for future security assessments.</li>
</ul>

<h3>Title: Rationale-Aware Answer Verification by Pairwise Self-Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Akira Kawabata, Saku Sugawara</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04838">https://arxiv.org/abs/2410.04838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04838">https://arxiv.org/pdf/2410.04838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04838]] Rationale-Aware Answer Verification by Pairwise Self-Evaluation(https://arxiv.org/abs/2410.04838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier's ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.</li>
</ul>

<h3>Title: Strong Model Collapse</h3>
<ul>
<li><strong>Authors: </strong>Elvis Dohmatob, Yunzhen Feng, Julia Kempe</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04840">https://arxiv.org/abs/2410.04840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04840">https://arxiv.org/pdf/2410.04840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04840]] Strong Model Collapse(https://arxiv.org/abs/2410.04840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish the existance of a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1\% of the total training dataset) can still lead to model collapse: larger and larger training sets do not enhance performance. We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and feed-forward neural networks for images.</li>
</ul>

<h3>Title: A Simple Image Segmentation Framework via In-Context Examples</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Chenchen Jing, Hengtao Li, Muzhi Zhu, Hao Chen, Xinlong Wang, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04842">https://arxiv.org/abs/2410.04842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04842">https://arxiv.org/pdf/2410.04842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04842]] A Simple Image Segmentation Framework via In-Context Examples(https://arxiv.org/abs/2410.04842)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework. However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information. In order to address this issue, we present SINE, a simple image Segmentation framework utilizing in-context examples. Our approach leverages a Transformer encoder-decoder structure, where the encoder provides high-quality image representations, and the decoder is designed to yield multiple task-specific output masks to effectively eliminate task ambiguity. Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks. In addition, we have further perfected the current evaluation system for in-context image segmentation, aiming to facilitate a holistic appraisal of these models. Experiments on various segmentation tasks show the effectiveness of the proposed method.</li>
</ul>

<h3>Title: PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04844">https://arxiv.org/abs/2410.04844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04844">https://arxiv.org/pdf/2410.04844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04844]] PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing(https://arxiv.org/abs/2410.04844)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the field of image editing, three core challenges persist: controllability, background preservation, and efficiency. Inversion-based methods rely on time-consuming optimization to preserve the features of the initial images, which results in low efficiency due to the requirement for extensive network inference. Conversely, inversion-free methods lack theoretical support for background similarity, as they circumvent the issue of maintaining initial features to achieve efficiency. As a consequence, none of these methods can achieve both high efficiency and background consistency. To tackle the challenges and the aforementioned disadvantages, we introduce PostEdit, a method that incorporates a posterior scheme to govern the diffusion sampling process. Specifically, a corresponding measurement term related to both the initial features and Langevin dynamics is introduced to optimize the estimated image generated by the given target prompt. Extensive experimental results indicate that the proposed PostEdit achieves state-of-the-art editing performance while accurately preserving unedited regions. Furthermore, the method is both inversion- and training-free, necessitating approximately 1.5 seconds and 18 GB of GPU memory to generate high-quality results.</li>
</ul>

<h3>Title: TimeCNN: Refining Cross-Variable Interaction on Time Point for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ao Hu, Dongkai Wang, Yong Dai, Shiyi Qi, Liangjian Wen, Jun Wang, Zhi Chen, Xun Zhou, Zenglin Xu, Jiang Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04853">https://arxiv.org/abs/2410.04853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04853">https://arxiv.org/pdf/2410.04853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04853]] TimeCNN: Refining Cross-Variable Interaction on Time Point for Time Series Forecasting(https://arxiv.org/abs/2410.04853)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting is extensively applied across diverse domains. Transformer-based models demonstrate significant potential in modeling cross-time and cross-variable interaction. However, we notice that the cross-variable correlation of multivariate time series demonstrates multifaceted (positive and negative correlations) and dynamic progression over time, which is not well captured by existing Transformer-based models. To address this issue, we propose a TimeCNN model to refine cross-variable interactions to enhance time series forecasting. Its key innovation is timepoint-independent, where each time point has an independent convolution kernel, allowing each time point to have its independent model to capture relationships among variables. This approach effectively handles both positive and negative correlations and adapts to the evolving nature of variable relationships over time. Extensive experiments conducted on 12 real-world datasets demonstrate that TimeCNN consistently outperforms state-of-the-art models. Notably, our model achieves significant reductions in computational requirements (approximately 60.46%) and parameter count (about 57.50%), while delivering inference speeds 3 to 4 times faster than the benchmark iTransformer model</li>
</ul>

<h3>Title: Mastering Chinese Chess AI (Xiangqi) Without Search</h3>
<ul>
<li><strong>Authors: </strong>Yu Chen, Juntong Lin, Zhichao Shu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04865">https://arxiv.org/abs/2410.04865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04865">https://arxiv.org/pdf/2410.04865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04865]] Mastering Chinese Chess AI (Xiangqi) Without Search(https://arxiv.org/abs/2410.04865)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We have developed a high-performance Chinese Chess AI that operates without reliance on search algorithms. This AI has demonstrated the capability to compete at a level commensurate with the top 0.1\% of human players. By eliminating the search process typically associated with such systems, this AI achieves a Queries Per Second (QPS) rate that exceeds those of systems based on the Monte Carlo Tree Search (MCTS) algorithm by over a thousandfold and surpasses those based on the AlphaBeta pruning algorithm by more than a hundredfold. The AI training system consists of two parts: supervised learning and reinforcement learning. Supervised learning provides an initial human-like Chinese chess AI, while reinforcement learning, based on supervised learning, elevates the strength of the entire AI to a new level. Based on this training system, we carried out enough ablation experiments and discovered that 1. The same parameter amount of Transformer architecture has a higher performance than CNN on Chinese chess; 2. Possible moves of both sides as features can greatly improve the training process; 3. Selective opponent pool, compared to pure self-play training, results in a faster improvement curve and a higher strength limit. 4. Value Estimation with Cutoff(VECT) improves the original PPO algorithm training process and we will give the explanation.</li>
</ul>

<h3>Title: On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, Jianfei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04870">https://arxiv.org/abs/2410.04870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04870">https://arxiv.org/pdf/2410.04870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04870]] On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent(https://arxiv.org/abs/2410.04870)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem. However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam. Despite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind. In this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on a linearly separable noisy dataset. We identify four stages in the training dynamics, each exhibiting intriguing behaviors. Based on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset. We also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting. Additionally, we find that the poor generalization of SignGD is not solely due to data noise, suggesting that both SignGD and Adam requires high-quality data for real-world tasks. Finally, experiments on synthetic and real-world datasets empirically support our theoretical results.</li>
</ul>

<h3>Title: Leveraging Grammar Induction for Language Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Jushi Kai, Shengyuan Hou, Yusheng Huang, Zhouhan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04878">https://arxiv.org/abs/2410.04878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04878">https://arxiv.org/pdf/2410.04878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04878]] Leveraging Grammar Induction for Language Understanding and Generation(https://arxiv.org/abs/2410.04878)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Grammar induction has made significant progress in recent years. However, it is not clear how the application of induced grammar could enhance practical performance in downstream tasks. In this work, we introduce an unsupervised grammar induction method for language understanding and generation. We construct a grammar parser to induce constituency structures and dependency relations, which is simultaneously trained on downstream tasks without additional syntax annotations. The induced grammar features are subsequently incorporated into Transformer as a syntactic mask to guide self-attention. We evaluate and apply our method to multiple machine translation tasks and natural language understanding tasks. Our method demonstrates superior performance compared to the original Transformer and other models enhanced with external parsers. Experimental results indicate that our method is effective in both from-scratch and pre-trained scenarios. Additionally, our research highlights the contribution of explicitly modeling the grammatical structure of texts to neural network models.</li>
</ul>

<h3>Title: Improving the Sampling Strategy in KernelSHAP</h3>
<ul>
<li><strong>Authors: </strong>Lars Henry Berge Olsen, Martin Jullum</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04883">https://arxiv.org/abs/2410.04883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04883">https://arxiv.org/pdf/2410.04883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04883]] Improving the Sampling Strategy in KernelSHAP(https://arxiv.org/abs/2410.04883)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Shapley values are a popular model-agnostic explanation framework for explaining predictions made by complex machine learning models. The framework provides feature contribution scores that sum to the predicted response and represent each feature's importance. The computation of exact Shapley values is computationally expensive due to estimating an exponential amount of non-trivial conditional expectations. The KernelSHAP framework enables us to approximate the Shapley values using a sampled subset of weighted conditional expectations. We propose three main novel contributions: a stabilizing technique to reduce the variance of the weights in the current state-of-the-art strategy, a novel weighing scheme that corrects the Shapley kernel weights based on sampled subsets, and a straightforward strategy that includes the important subsets and integrates them with the corrected Shapley kernel weights. We compare these new approximation strategies against existing ones by evaluating their Shapley value accuracy as a function of the number of subsets. The results demonstrate that our sampling strategies significantly enhance the accuracy of the approximated Shapley value explanations, making them more reliable in practical applications. This work provides valuable insights and practical recommendations for researchers and practitioners seeking to implement Shapley value-based explainability of their models.</li>
</ul>

<h3>Title: Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models</h3>
<ul>
<li><strong>Authors: </strong>Dehong Kong, Siyuan Liang, Xiaopeng Zhu, Yuansheng Zhong, Wenqi Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04884">https://arxiv.org/abs/2410.04884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04884">https://arxiv.org/pdf/2410.04884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04884]] Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models(https://arxiv.org/abs/2410.04884)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Visual language pre-training (VLP) models have demonstrated significant success across various domains, yet they remain vulnerable to adversarial attacks. Addressing these adversarial vulnerabilities is crucial for enhancing security in multimodal learning. Traditionally, adversarial methods targeting VLP models involve simultaneously perturbing images and text. However, this approach faces notable challenges: first, adversarial perturbations often fail to translate effectively into real-world scenarios; second, direct modifications to the text are conspicuously visible. To overcome these limitations, we propose a novel strategy that exclusively employs image patches for attacks, thus preserving the integrity of the original text. Our method leverages prior knowledge from diffusion models to enhance the authenticity and naturalness of the perturbations. Moreover, to optimize patch placement and improve the efficacy of our attacks, we utilize the cross-attention mechanism, which encapsulates intermodal interactions by generating attention maps to guide strategic patch placements. Comprehensive experiments conducted in a white-box setting for image-to-text scenarios reveal that our proposed method significantly outperforms existing techniques, achieving a 100% attack success rate. Additionally, it demonstrates commendable performance in transfer tasks involving text-to-image configurations.</li>
</ul>

<h3>Title: D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and Shape Estimation</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Vasilikopoulos, Drosakis Drosakis, Antonis Argyros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04889">https://arxiv.org/abs/2410.04889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04889">https://arxiv.org/pdf/2410.04889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04889]] D-PoSE: Depth as an Intermediate Representation for 3D Human Pose and Shape Estimation(https://arxiv.org/abs/2410.04889)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present D-PoSE (Depth as an Intermediate Representation for 3D Human Pose and Shape Estimation), a one-stage method that estimates human pose and SMPL-X shape parameters from a single RGB image. Recent works use larger models with transformer backbones and decoders to improve the accuracy in human pose and shape (HPS) benchmarks. D-PoSE proposes a vision based approach that uses the estimated human depth-maps as an intermediate representation for HPS and leverages training with synthetic data and the ground-truth depth-maps provided with them for depth supervision during training. Although trained on synthetic datasets, D-PoSE achieves state-of-the-art performance on the real-world benchmark datasets, EMDB and 3DPW. Despite its simple lightweight design and the CNN backbone, it outperforms ViT-based models that have a number of parameters that is larger by almost an order of magnitude. D-PoSE code is available at: this https URL</li>
</ul>

<h3>Title: Low-Rank Continual Personalization of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Staniszewski, Katarzyna Zaleska, Kamil Deja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04891">https://arxiv.org/abs/2410.04891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04891">https://arxiv.org/pdf/2410.04891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04891]] Low-Rank Continual Personalization of Diffusion Models(https://arxiv.org/abs/2410.04891)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent personalization methods for diffusion models, such as Dreambooth, allow fine-tuning pre-trained models to generate new concepts. However, applying these techniques across multiple tasks in order to include, e.g., several new objects or styles, leads to mutual interference between their adapters. While recent studies attempt to mitigate this issue by combining trained adapters across tasks after fine-tuning, we adopt a more rigorous regime and investigate the personalization of large diffusion models under a continual learning scenario, where such interference leads to catastrophic forgetting of previous knowledge. To that end, we evaluate the naïve continual fine-tuning of customized models and compare this approach with three methods for consecutive adapters' training: sequentially merging new adapters, merging orthogonally initialized adapters, and updating only relevant parameters according to the task. In our experiments, we show that the proposed approaches mitigate forgetting when compared to the naïve approach.</li>
</ul>

<h3>Title: Defense-as-a-Service: Black-box Shielding against Backdoored Graph Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yang, Kai Zhou, Yuni Lai, Gaolei Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04916">https://arxiv.org/abs/2410.04916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04916">https://arxiv.org/pdf/2410.04916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04916]] Defense-as-a-Service: Black-box Shielding against Backdoored Graph Models(https://arxiv.org/abs/2410.04916)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, explainability</a></li>
<li><strong>Abstract: </strong>With the trend of large graph learning models, business owners tend to employ a model provided by a third party to deliver business services to users. However, these models might be backdoored, and malicious users can submit trigger-embedded inputs to manipulate the model predictions. Current graph backdoor defenses have several limitations: 1) depending on model-related details, 2) requiring additional model fine-tuning, and 3) relying upon extra explainability tools, all of which are infeasible under stringent privacy policies. To address those limitations, we propose GraphProt, which allows resource-constrained business owners to rely on third parties to avoid backdoor attacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and only relies on the input graph. The key insight is to leverage subgraph information for prediction, thereby mitigating backdoor effects induced by triggers. GraphProt comprises two components: clustering-based trigger elimination and robust subgraph ensemble. Specifically, we first propose feature-topology clustering that aims to remove most of the anomalous subgraphs (triggers). Moreover, we design subgraph sampling strategies based on feature-topology clustering to build a robust classifier via majority vote. Experimental results across three backdoor attacks and six benchmark datasets demonstrate that GraphProt significantly reduces the backdoor attack success rate while preserving the model accuracy on regular graph classification tasks.</li>
</ul>

<h3>Title: Intent Classification for Bank Chatbots through LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Bibiána Lajčinová, Patrik Valábek, Michal Spišiak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04925">https://arxiv.org/abs/2410.04925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04925">https://arxiv.org/pdf/2410.04925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04925]] Intent Classification for Bank Chatbots through LLM Fine-Tuning(https://arxiv.org/abs/2410.04925)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the application of large language models (LLMs) for intent classification within a chatbot with predetermined responses designed for banking industry websites. Specifically, the research examines the effectiveness of fine-tuning SlovakBERT compared to employing multilingual generative models, such as Llama 8b instruct and Gemma 7b instruct, in both their pre-trained and fine-tuned versions. The findings indicate that SlovakBERT outperforms the other models in terms of in-scope accuracy and out-of-scope false positive rate, establishing it as the benchmark for this application.</li>
</ul>

<h3>Title: PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion</h3>
<ul>
<li><strong>Authors: </strong>Sijie Wang, Qiyu Kang, Rui She, Kai Zhao, Yang Song, Wee Peng Tay</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04939">https://arxiv.org/abs/2410.04939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04939">https://arxiv.org/pdf/2410.04939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04939]] PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion(https://arxiv.org/abs/2410.04939)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Place recognition plays a crucial role in the fields of robotics and computer vision, finding applications in areas such as autonomous driving, mapping, and localization. Place recognition identifies a place using query sensor data and a known database. One of the main challenges is to develop a model that can deliver accurate results while being robust to environmental variations. We propose two multi-modal place recognition models, namely PRFusion and PRFusion++. PRFusion utilizes global fusion with manifold metric attention, enabling effective interaction between features without requiring camera-LiDAR extrinsic calibrations. In contrast, PRFusion++ assumes the availability of extrinsic calibrations and leverages pixel-point correspondences to enhance feature learning on local windows. Additionally, both models incorporate neural diffusion layers, which enable reliable operation even in challenging environments. We verify the state-of-the-art performance of both models on three large-scale benchmarks. Notably, they outperform existing models by a substantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore, we conduct ablation studies to validate the effectiveness of our proposed methods. The codes are available at: this https URL</li>
</ul>

<h3>Title: Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness</h3>
<ul>
<li><strong>Authors: </strong>Borja Carrillo Perez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04946">https://arxiv.org/abs/2410.04946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04946">https://arxiv.org/pdf/2410.04946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04946]] Real-time Ship Recognition and Georeferencing for the Improvement of Maritime Situational Awareness(https://arxiv.org/abs/2410.04946)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In an era where maritime infrastructures are crucial, advanced situational awareness solutions are increasingly important. The use of optical camera systems can allow real-time usage of maritime footage. This thesis presents an investigation into leveraging deep learning and computer vision to advance real-time ship recognition and georeferencing for the improvement of maritime situational awareness. A novel dataset, ShipSG, is introduced, containing 3,505 images and 11,625 ship masks with corresponding class and geographic position. After an exploration of state-of-the-art, a custom real-time segmentation architecture, ScatYOLOv8+CBAM, is designed for the NVIDIA Jetson AGX Xavier embedded system. This architecture adds the 2D scattering transform and attention mechanisms to YOLOv8, achieving an mAP of 75.46% and an 25.3 ms per frame, outperforming state-of-the-art methods by over 5%. To improve small and distant ship recognition in high-resolution images on embedded systems, an enhanced slicing mechanism is introduced, improving mAP by 8% to 11%. Additionally, a georeferencing method is proposed, achieving positioning errors of 18 m for ships up to 400 m away and 44 m for ships between 400 m and 1200 m. The findings are also applied in real-world scenarios, such as the detection of abnormal ship behaviour, camera integrity assessment and 3D reconstruction. The approach of this thesis outperforms existing methods and provides a framework for integrating recognized and georeferenced ships into real-time systems, enhancing operational effectiveness and decision-making for maritime stakeholders. This thesis contributes to the maritime computer vision field by establishing a benchmark for ship segmentation and georeferencing research, demonstrating the viability of deep-learning-based recognition and georeferencing methods for real-time maritime monitoring.</li>
</ul>

<h3>Title: Maximizing the practical achievability of quantum annealing attacks on factorization-based cryptography</h3>
<ul>
<li><strong>Authors: </strong>Olgierd Żołnierczyk</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04956">https://arxiv.org/abs/2410.04956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04956">https://arxiv.org/pdf/2410.04956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04956]] Maximizing the practical achievability of quantum annealing attacks on factorization-based cryptography(https://arxiv.org/abs/2410.04956)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This work focuses on quantum methods for cryptanalysis of schemes based on the integer factorization problem and the discrete logarithm problem. We demonstrate how to practically solve the largest instances of the factorization problem by improving an approach that combines quantum and classical computations, assuming the use of the best publicly available special-class quantum computer: the quantum annealer. We achieve new computational experiment results by solving the largest instance of the factorization problem ever announced as solved using quantum annealing, with a size of 29 bits. The core idea of the improved approach is to leverage known sub-exponential classical method to break the problem down into many smaller computations and perform the most critical ones on a quantum computer. This approach does not reduce the complexity class, but it assesses the pragmatic capabilities of an attacker. It also marks a step forward in the development of hybrid methods, which in practice may surpass classical methods in terms of efficiency sooner than purely quantum computations will.</li>
</ul>

<h3>Title: Failure-Proof Non-Contrastive Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Sansone, Tim Lebailly, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04959">https://arxiv.org/abs/2410.04959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04959">https://arxiv.org/pdf/2410.04959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04959]] Failure-Proof Non-Contrastive Self-Supervised Learning(https://arxiv.org/abs/2410.04959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We identify sufficient conditions to avoid known failure modes, including representation, dimensional, cluster and intracluster collapses, occurring in non-contrastive self-supervised learning. Based on these findings, we propose a principled design for the projector and loss function. We theoretically demonstrate that this design introduces an inductive bias that promotes learning representations that are both decorrelated and clustered without explicit enforcing these properties and leading to improved generalization. To the best of our knowledge, this is the first solution that achieves robust training with respect to these failure modes while guaranteeing enhanced generalization performance in downstream tasks. We validate our theoretical findings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100, and show that our solution, dubbed FALCON, outperforms existing feature decorrelation and cluster-based self-supervised learning methods in terms of generalization to clustering and linear classification tasks.</li>
</ul>

<h3>Title: On Efficient Variants of Segment Anything Model: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Xiaorui Sun, Jun Liu, Heng Tao Shen, Xiaofeng Zhu, Ping Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04960">https://arxiv.org/abs/2410.04960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04960">https://arxiv.org/pdf/2410.04960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04960]] On Efficient Variants of Segment Anything Model: A Survey(https://arxiv.org/abs/2410.04960)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) is a foundational model for image segmentation tasks, known for its strong generalization across diverse applications. However, its impressive performance comes with significant computational and resource demands, making it challenging to deploy in resource-limited environments such as mobile devices. To address this, a variety of SAM variants have been proposed to enhance efficiency without sacrificing accuracy. This survey provides the first comprehensive review of these efficient SAM variants. We begin by exploring the motivations driving this research. We then present core techniques used in SAM and model acceleration. This is followed by an in-depth analysis of various acceleration strategies, categorized by approach. Finally, we offer a unified and extensive evaluation of these methods, assessing their efficiency and accuracy on representative benchmarks, and providing a clear comparison of their overall performance.</li>
</ul>

<h3>Title: Revealing Directions for Text-guided 3D Face Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Yichao Yan, Sehngqi Liu, Yuhao Cheng, Weiming Zhao, Lincheng Li, Mengxiao Bi, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04965">https://arxiv.org/abs/2410.04965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04965">https://arxiv.org/pdf/2410.04965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04965]] Revealing Directions for Text-guided 3D Face Editing(https://arxiv.org/abs/2410.04965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D face editing is a significant task in multimedia, aimed at the manipulation of 3D face models across various control signals. The success of 3D-aware GAN provides expressive 3D models learned from 2D single-view images only, encouraging researchers to discover semantic editing directions in its latent space. However, previous methods face challenges in balancing quality, efficiency, and generalization. To solve the problem, we explore the possibility of introducing the strength of diffusion model into 3D-aware GANs. In this paper, we present Face Clan, a fast and text-general approach for generating and manipulating 3D faces based on arbitrary attribute descriptions. To achieve disentangled editing, we propose to diffuse on the latent space under a pair of opposite prompts to estimate the mask indicating the region of interest on latent codes. Based on the mask, we then apply denoising to the masked latent codes to reveal the editing direction. Our method offers a precisely controllable manipulation method, allowing users to intuitively customize regions of interest with the text description. Experiments demonstrate the effectiveness and generalization of our Face Clan for various pre-trained GANs. It offers an intuitive and wide application for text-guided face editing that contributes to the landscape of multimedia content creation.</li>
</ul>

<h3>Title: L-C4: Language-Based Video Colorization for Creative and Consistent Color</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chang, Shuchen Weng, Huan Ouyang, Yu Li, Si Li, Boxin Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04972">https://arxiv.org/abs/2410.04972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04972">https://arxiv.org/pdf/2410.04972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04972]] L-C4: Language-Based Video Colorization for Creative and Consistent Color(https://arxiv.org/abs/2410.04972)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Automatic video colorization is inherently an ill-posed problem because each monochrome frame has multiple optional color candidates. Previous exemplar-based video colorization methods restrict the user's imagination due to the elaborate retrieval process. Alternatively, conditional image colorization methods combined with post-processing algorithms still struggle to maintain temporal consistency. To address these issues, we present Language-based video Colorization for Creative and Consistent Colors (L-C4) to guide the colorization process using user-provided language descriptions. Our model is built upon a pre-trained cross-modality generative model, leveraging its comprehensive language understanding and robust color representation abilities. We introduce the cross-modality pre-fusion module to generate instance-aware text embeddings, enabling the application of creative colors. Additionally, we propose temporally deformable attention to prevent flickering or color shifts, and cross-clip fusion to maintain long-term color consistency. Extensive experimental results demonstrate that L-C4 outperforms relevant methods, achieving semantically accurate colors, unrestricted creative correspondence, and temporally robust consistency.</li>
</ul>

<h3>Title: On the Rigour of Scientific Writing: Criteria, Analysis, and Insights</h3>
<ul>
<li><strong>Authors: </strong>Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04981">https://arxiv.org/abs/2410.04981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04981">https://arxiv.org/pdf/2410.04981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04981]] On the Rigour of Scientific Writing: Criteria, Analysis, and Insights(https://arxiv.org/abs/2410.04981)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Rigour is crucial for scientific research as it ensures the reproducibility and validity of results and findings. Despite its importance, little work exists on modelling rigour computationally, and there is a lack of analysis on whether these criteria can effectively signal or measure the rigour of scientific papers in practice. In this paper, we introduce a bottom-up, data-driven framework to automatically identify and define rigour criteria and assess their relevance in scientific writing. Our framework includes rigour keyword extraction, detailed rigour definition generation, and salient criteria identification. Furthermore, our framework is domain-agnostic and can be tailored to the evaluation of scientific rigour for different areas, accommodating the distinct salient criteria across fields. We conducted comprehensive experiments based on datasets collected from two high impact venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the effectiveness of our framework in modelling rigour. In addition, we analyse linguistic patterns of rigour, revealing that framing certainty is crucial for enhancing the perception of scientific rigour, while suggestion certainty and probability uncertainty diminish it.</li>
</ul>

<h3>Title: RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection</h3>
<ul>
<li><strong>Authors: </strong>Pasquale De Marinis, Rino Vessio, Giovanna Castellano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04983">https://arxiv.org/abs/2410.04983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04983">https://arxiv.org/pdf/2410.04983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04983]] RoWeeder: Unsupervised Weed Mapping through Crop-Row Detection(https://arxiv.org/abs/2410.04983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Precision agriculture relies heavily on effective weed management to ensure robust crop yields. This study presents RoWeeder, an innovative framework for unsupervised weed mapping that combines crop-row detection with a noise-resilient deep learning model. By leveraging crop-row information to create a pseudo-ground truth, our method trains a lightweight deep learning model capable of distinguishing between crops and weeds, even in the presence of noisy data. Evaluated on the WeedMap dataset, RoWeeder achieves an F1 score of 75.3, outperforming several baselines. Comprehensive ablation studies further validated the model's performance. By integrating RoWeeder with drone technology, farmers can conduct real-time aerial surveys, enabling precise weed management across large fields. The code is available at: \url{this https URL}.</li>
</ul>

<h3>Title: Conditional Variational Autoencoders for Probabilistic Pose Regression</h3>
<ul>
<li><strong>Authors: </strong>Fereidoon Zangeneh, Leonard Bruns, Amit Dekel, Alessandro Pieropan, Patric Jensfelt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04989">https://arxiv.org/abs/2410.04989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04989">https://arxiv.org/pdf/2410.04989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04989]] Conditional Variational Autoencoders for Probabilistic Pose Regression(https://arxiv.org/abs/2410.04989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Robots rely on visual relocalization to estimate their pose from camera images when they lose track. One of the challenges in visual relocalization is repetitive structures in the operation environment of the robot. This calls for probabilistic methods that support multiple hypotheses for robot's pose. We propose such a probabilistic method to predict the posterior distribution of camera poses given an observed image. Our proposed training strategy results in a generative model of camera poses given an image, which can be used to draw samples from the pose posterior distribution. Our method is streamlined and well-founded in theory and outperforms existing methods on localization in presence of ambiguities.</li>
</ul>

<h3>Title: FRIDA: Free-Rider Detection using Privacy Attacks</h3>
<ul>
<li><strong>Authors: </strong>Pol G. Recasens, Ádám Horváth, Alberto Gutierrez-Torre, Jordi Torres, Josep Ll.Berral, Balázs Pejó</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05020">https://arxiv.org/abs/2410.05020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05020">https://arxiv.org/pdf/2410.05020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05020]] FRIDA: Free-Rider Detection using Privacy Attacks(https://arxiv.org/abs/2410.05020)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning is increasingly popular as it enables multiple parties with limited datasets and resources to train a high-performing machine learning model collaboratively. However, similarly to other collaborative systems, federated learning is vulnerable to free-riders -- participants who do not contribute to the training but still benefit from the shared model. Free-riders not only compromise the integrity of the learning process but also slow down the convergence of the global model, resulting in increased costs for the honest participants. To address this challenge, we propose FRIDA: free-rider detection using privacy attacks, a framework that leverages inference attacks to detect free-riders. Unlike traditional methods that only capture the implicit effects of free-riding, FRIDA directly infers details of the underlying training datasets, revealing characteristics that indicate free-rider behaviour. Through extensive experiments, we demonstrate that membership and property inference attacks are effective for this purpose. Our evaluation shows that FRIDA outperforms state-of-the-art methods, especially in non-IID settings.</li>
</ul>

<h3>Title: DEPT: Decoupled Embeddings for Pre-training Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas D. Lane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05021">https://arxiv.org/abs/2410.05021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05021">https://arxiv.org/pdf/2410.05021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05021]] DEPT: Decoupled Embeddings for Pre-training Language Models(https://arxiv.org/abs/2410.05021)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, transformer</a></li>
<li><strong>Abstract: </strong>Language Model pre-training benefits from a broader data mixture to enhance performance across domains and languages. However, training on such heterogeneous text corpora is complex, requiring extensive and cost-intensive efforts. Since these data sources vary in lexical, syntactic, and semantic aspects, they cause negative interference or the "curse of multilinguality". We propose a novel pre-training framework to alleviate this curse. Our method, DEPT, decouples the embedding layers from the transformer body while simultaneously training the latter in multiple contexts. DEPT enables the model to train without being bound to a shared global vocabulary. DEPT: (1) can train robustly and effectively under significant data heterogeneity, (2) reduces the parameter count of the token embeddings by up to 80% and the communication costs by 675x for billion-scale models (3) enhances model generalization and plasticity in adapting to new languages and domains, and (4) allows training with custom optimized vocabulary per data source. We prove DEPT's potential by performing the first vocabulary-agnostic federated multilingual pre-training of a 1.3 billion-parameter model across high and low-resource languages, reducing its parameter count by 409 million.</li>
</ul>

<h3>Title: Named Clinical Entity Recognition Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Wadood M Abdul, Marco AF Pimentel, Muhammad Umar Salman, Tathagata Raha, Clément Christophe, Praveen K Kanithi, Nasir Hayat, Ronnie Rajan, Shadab Khan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05046">https://arxiv.org/abs/2410.05046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05046">https://arxiv.org/pdf/2410.05046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05046]] Named Clinical Entity Recognition Benchmark(https://arxiv.org/abs/2410.05046)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support. The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations. By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.</li>
</ul>

<h3>Title: A test suite of prompt injection attacks for LLM-based machine translation</h3>
<ul>
<li><strong>Authors: </strong>Antonio Valerio Miceli-Barone, Zhifan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05047">https://arxiv.org/abs/2410.05047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05047">https://arxiv.org/pdf/2410.05047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05047]] A test suite of prompt injection attacks for LLM-based machine translation(https://arxiv.org/abs/2410.05047)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>LLM-based NLP systems typically work by embedding their input data into prompt templates which contain instructions and/or in-context examples, creating queries which are submitted to a LLM, and then parsing the LLM response in order to generate the system outputs. Prompt Injection Attacks (PIAs) are a type of subversion of these systems where a malicious user crafts special inputs which interfere with the prompt templates, causing the LLM to respond in ways unintended by the system designer. Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based machine translation. Specifically, the task is to translate questions from the TruthfulQA test suite, where an adversarial prompt is prepended to the questions, instructing the system to ignore the translation instruction and answer the questions instead. In this test suite, we extend this approach to all the language pairs of the WMT 2024 General Machine Translation task. Moreover, we include additional attack formats in addition to the one originally studied.</li>
</ul>

<h3>Title: HE-Drive: Human-Like End-to-End Driving with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junming Wang, Xingyu Zhang, Zebin Xing, Songen Gu, Xiaoyang Guo, Yang Hu, Ziying Song, Qian Zhang, Xiaoxiao Long, Wei Yin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05051">https://arxiv.org/abs/2410.05051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05051">https://arxiv.org/pdf/2410.05051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05051]] HE-Drive: Human-Like End-to-End Driving with Vision Language Models(https://arxiv.org/abs/2410.05051)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose HE-Drive: the first human-like-centric end-to-end autonomous driving system to generate trajectories that are both temporally consistent and comfortable. Recent studies have shown that imitation learning-based planners and learning-based trajectory scorers can effectively generate and select accuracy trajectories that closely mimic expert demonstrations. However, such trajectory planners and scorers face the dilemma of generating temporally inconsistent and uncomfortable trajectories. To solve the above problems, Our HE-Drive first extracts key 3D spatial representations through sparse perception, which then serves as conditional inputs for a Conditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion planner to generate temporal consistency multi-modal trajectories. A Vision-Language Models (VLMs)-guided trajectory scorer subsequently selects the most comfortable trajectory from these candidates to control the vehicle, ensuring human-like end-to-end driving. Experiments show that HE-Drive not only achieves state-of-the-art performance (i.e., reduces the average collision rate by 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the challenging nuScenes and OpenScene datasets but also provides the most comfortable driving experience on real-world this http URL more information, visit the project website: this https URL.</li>
</ul>

<h3>Title: Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Nishida, Kyosuke Nishida, Kuniko Saito</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05052">https://arxiv.org/abs/2410.05052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05052">https://arxiv.org/pdf/2410.05052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05052]] Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes(https://arxiv.org/abs/2410.05052)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Loss spikes, a phenomenon in which the loss value diverges suddenly, is a fundamental issue in the pre-training of large language models. This paper supposes that the non-uniformity of the norm of the parameters is one of the causes of loss spikes. Here, in training of neural networks, the scale of the gradients is required to be kept constant throughout the layers to avoid the vanishing and exploding gradients problem. However, to meet these requirements in the Transformer model, the norm of the model parameters must be non-uniform, and thus, parameters whose norm is smaller are more sensitive to the parameter update. To address this issue, we propose a novel technique, weight scaling as reparameterization (WeSaR). WeSaR introduces a gate parameter per parameter matrix and adjusts it to the value satisfying the requirements. Because of the gate parameter, WeSaR sets the norm of the original parameters uniformly, which results in stable training. Experimental results with the Transformer decoders consisting of 130 million, 1.3 billion, and 13 billion parameters showed that WeSaR stabilizes and accelerates training and that it outperformed compared methods including popular initialization methods.</li>
</ul>

<h3>Title: TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05076">https://arxiv.org/abs/2410.05076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05076">https://arxiv.org/pdf/2410.05076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05076]] TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention(https://arxiv.org/abs/2410.05076)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.</li>
</ul>

<h3>Title: ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Francesco Maria Molfese, Simone Conia, Riccardo Orlando, Roberto Navigli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05077">https://arxiv.org/abs/2410.05077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05077">https://arxiv.org/pdf/2410.05077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05077]] ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering(https://arxiv.org/abs/2410.05077)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) have shown strong reasoning capabilities in commonsense question answering benchmarks, but the process underlying their success remains largely opaque. As a consequence, recent approaches have equipped LLMs with mechanisms for knowledge retrieval, reasoning and introspection, not only to improve their capabilities but also to enhance the interpretability of their outputs. However, these methods require additional training, hand-crafted templates or human-written explanations. To address these issues, we introduce ZEBRA, a zero-shot question answering framework that combines retrieval, case-based reasoning and introspection and dispenses with the need for additional training of the LLM. Given an input question, ZEBRA retrieves relevant question-knowledge pairs from a knowledge base and generates new knowledge by reasoning over the relationships in these pairs. This generated knowledge is then used to answer the input question, improving the model's performance and interpretability. We evaluate our approach across 8 well-established commonsense reasoning benchmarks, demonstrating that ZEBRA consistently outperforms strong LLMs and previous knowledge integration approaches, achieving an average accuracy improvement of up to 4.5 points.</li>
</ul>

<h3>Title: Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</h3>
<ul>
<li><strong>Authors: </strong>David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05078">https://arxiv.org/abs/2410.05078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05078">https://arxiv.org/pdf/2410.05078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05078]] Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data(https://arxiv.org/abs/2410.05078)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foundation models have recently been shown to be strong data compressors. However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms. Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression. In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible. To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when factoring in parameter count. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak.</li>
</ul>

<h3>Title: Explanation sensitivity to the randomness of large language models: the case of journalistic text classification</h3>
<ul>
<li><strong>Authors: </strong>Jeremie Bogaert, Marie-Catherine de Marneffe, Antonin Descampe, Louis Escouflaire, Cedrick Fairon, Francois-Xavier Standaert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05085">https://arxiv.org/abs/2410.05085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05085">https://arxiv.org/pdf/2410.05085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05085]] Explanation sensitivity to the randomness of large language models: the case of journalistic text classification(https://arxiv.org/abs/2410.05085)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) perform very well in several natural language processing tasks but raise explainability challenges. In this paper, we examine the effect of random elements in the training of LLMs on the explainability of their predictions. We do so on a task of opinionated journalistic text classification in French. Using a fine-tuned CamemBERT model and an explanation method based on relevance propagation, we find that training with different random seeds produces models with similar accuracy but variable explanations. We therefore claim that characterizing the explanations' statistical distribution is needed for the explainability of LLMs. We then explore a simpler model based on textual features which offers stable explanations but is less accurate. Hence, this simpler model corresponds to a different tradeoff between accuracy and explainability. We show that it can be improved by inserting features derived from CamemBERT's explanations. We finally discuss new research directions suggested by our results, in particular regarding the origin of the sensitivity observed in the training randomness.</li>
</ul>

<h3>Title: DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects</h3>
<ul>
<li><strong>Authors: </strong>Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05097">https://arxiv.org/abs/2410.05097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05097">https://arxiv.org/pdf/2410.05097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05097]] DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects(https://arxiv.org/abs/2410.05097)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) enables to generate new images of a scene or convert a set of 2D images into a comprehensive 3D model. In the context of Space Domain Awareness, since space is becoming increasingly congested, NVS can accurately map space objects and debris, improving the safety and efficiency of space operations. Similarly, in Rendezvous and Proximity Operations missions, 3D models can provide details about a target object's shape, size, and orientation, allowing for better planning and prediction of the target's behavior. In this work, we explore the generalization abilities of these reconstruction techniques, aiming to avoid the necessity of retraining for each new scene, by presenting a novel approach to 3D spacecraft reconstruction from single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art single-view reconstruction model, on a high-quality dataset of 190 high-quality spacecraft models and integrating it into the DreamGaussian framework. We demonstrate consistent improvements in reconstruction quality across multiple metrics, including Contrastive Language-Image Pretraining (CLIP) score (+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS) (+0.16%) on a test set of 30 previously unseen spacecraft images. Our method addresses the lack of domain-specific 3D reconstruction tools in the space industry by leveraging state-of-the-art diffusion models and 3D Gaussian splatting techniques. This approach maintains the efficiency of the DreamGaussian framework while enhancing the accuracy and detail of spacecraft reconstructions. The code for this work can be accessed on GitHub (this https URL).</li>
</ul>

<h3>Title: Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances</h3>
<ul>
<li><strong>Authors: </strong>Alina Wróblewska</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05099">https://arxiv.org/abs/2410.05099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05099">https://arxiv.org/pdf/2410.05099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05099]] Investigating large language models for their competence in extracting grammatically sound sentences from transcribed noisy utterances(https://arxiv.org/abs/2410.05099)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Selectively processing noisy utterances while effectively disregarding speech-specific elements poses no considerable challenge for humans, as they exhibit remarkable cognitive abilities to separate semantically significant content from speech-specific noise (i.e. filled pauses, disfluencies, and restarts). These abilities may be driven by mechanisms based on acquired grammatical rules that compose abstract syntactic-semantic structures within utterances. Segments without syntactic and semantic significance are consistently disregarded in these structures. The structures, in tandem with lexis, likely underpin language comprehension and thus facilitate effective communication. In our study, grounded in linguistically motivated experiments, we investigate whether large language models (LLMs) can effectively perform analogical speech comprehension tasks. In particular, we examine the ability of LLMs to extract well-structured utterances from transcriptions of noisy dialogues. We conduct two evaluation experiments in the Polish language scenario, using a~dataset presumably unfamiliar to LLMs to mitigate the risk of data contamination. Our results show that not all extracted utterances are correctly structured, indicating that either LLMs do not fully acquire syntactic-semantic rules or they acquire them but cannot apply them effectively. We conclude that the ability of LLMs to comprehend noisy utterances is still relatively superficial compared to human proficiency in processing them.</li>
</ul>

<h3>Title: IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Yan He, Bing Tu, Puzhao Jiang, Bo Liu, Jun Li, Antonio Plaza</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05100">https://arxiv.org/abs/2410.05100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05100">https://arxiv.org/pdf/2410.05100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05100]] IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral Image Classification(https://arxiv.org/abs/2410.05100)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Hyperspectral image (HSI) classification has garnered substantial attention in remote sensing fields. Recent Mamba architectures built upon the Selective State Space Models (S6) have demonstrated enormous potential in long-range sequence modeling. However, the high dimensionality of hyperspectral data and information redundancy pose challenges to the application of Mamba in HSI classification, suffering from suboptimal performance and computational efficiency. In light of this, this paper investigates a lightweight Interval Group Spatial-Spectral Mamba framework (IGroupSS-Mamba) for HSI classification, which allows for multi-directional and multi-scale global spatial-spectral information extraction in a grouping and hierarchical manner. Technically, an Interval Group S6 Mechanism (IGSM) is developed as the core component, which partitions high-dimensional features into multiple non-overlapping groups at intervals, and then integrates a unidirectional S6 for each group with a specific scanning direction to achieve non-redundant sequence modeling. Compared to conventional applying multi-directional scanning to all bands, this grouping strategy leverages the complementary strengths of different scanning directions while decreasing computational costs. To adequately capture the spatial-spectral contextual information, an Interval Group Spatial-Spectral Block (IGSSB) is introduced, in which two IGSM-based spatial and spectral operators are cascaded to characterize the global spatial-spectral relationship along the spatial and spectral dimensions, respectively. IGroupSS-Mamba is constructed as a hierarchical structure stacked by multiple IGSSB blocks, integrating a pixel aggregation-based downsampling strategy for multiscale spatial-spectral semantic learning from shallow to deep stages. Extensive experiments demonstrate that IGroupSS-Mamba outperforms the state-of-the-art methods.</li>
</ul>

<h3>Title: AI-Enhanced Ethical Hacking: A Linux-Focused Experiment</h3>
<ul>
<li><strong>Authors: </strong>Haitham S. Al-Sinani, Chris J. Mitchell</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05105">https://arxiv.org/abs/2410.05105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05105">https://arxiv.org/pdf/2410.05105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05105]] AI-Enhanced Ethical Hacking: A Linux-Focused Experiment(https://arxiv.org/abs/2410.05105)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative</a></li>
<li><strong>Abstract: </strong>This technical report investigates the integration of generative AI (GenAI), specifically ChatGPT, into the practice of ethical hacking through a comprehensive experimental study and conceptual analysis. Conducted in a controlled virtual environment, the study evaluates GenAI's effectiveness across the key stages of penetration testing on Linux-based target machines operating within a virtual local area network (LAN), including reconnaissance, scanning and enumeration, gaining access, maintaining access, and covering tracks. The findings confirm that GenAI can significantly enhance and streamline the ethical hacking process while underscoring the importance of balanced human-AI collaboration rather than the complete replacement of human input. The report also critically examines potential risks such as misuse, data biases, hallucination, and over-reliance on AI. This research contributes to the ongoing discussion on the ethical use of AI in cybersecurity and highlights the need for continued innovation to strengthen security defences.</li>
</ul>

<h3>Title: Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization</h3>
<ul>
<li><strong>Authors: </strong>Rohan Reddy Mekala, Frederik Pahde, Simon Baur, Sneha Chandrashekar, Madeline Diep, Markus Wenzel, Eric L. Wisotzky, Galip Ümit Yolcu, Sebastian Lapuschkin, Jackie Ma, Peter Eisert, Mikael Lindvall, Adam Porter, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05114">https://arxiv.org/abs/2410.05114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05114">https://arxiv.org/pdf/2410.05114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05114]] Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization(https://arxiv.org/abs/2410.05114)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative</a></li>
<li><strong>Abstract: </strong>In the realm of dermatological diagnoses, where the analysis of dermatoscopic and microscopic skin lesion images is pivotal for the accurate and early detection of various medical conditions, the costs associated with creating diverse and high-quality annotated datasets have hampered the accuracy and generalizability of machine learning models. We propose an innovative unsupervised augmentation solution that harnesses Generative Adversarial Network (GAN) based models and associated techniques over their latent space to generate controlled semiautomatically-discovered semantic variations in dermatoscopic images. We created synthetic images to incorporate the semantic variations and augmented the training data with these images. With this approach, we were able to increase the performance of machine learning models and set a new benchmark amongst non-ensemble based models in skin lesion classification on the HAM10000 dataset; and used the observed analytics and generated models for detailed studies on model explainability, affirming the effectiveness of our solution.</li>
</ul>

<h3>Title: Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05116">https://arxiv.org/abs/2410.05116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05116">https://arxiv.org/pdf/2410.05116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05116]] Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning(https://arxiv.org/abs/2410.05116)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale datasets, limiting their applicability to scenarios where collecting such data is costly or difficult. To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) Feedback-Aligned Representation Learning, an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) Feedback-Guided Image Generation, which involves generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent. We demonstrate that HERO is 4x more efficient in online feedback for body part anomaly correction compared to the best existing method. Additionally, experiments show that HERO can effectively handle tasks like reasoning, counting, personalization, and reducing NSFW content with only 0.5K online feedback.</li>
</ul>

<h3>Title: LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05136">https://arxiv.org/abs/2410.05136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05136">https://arxiv.org/pdf/2410.05136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05136]] LOTOS: Layer-wise Orthogonalization for Training Robust Ensembles(https://arxiv.org/abs/2410.05136)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Transferability of adversarial examples is a well-known property that endangers all classification models, even those that are only accessible through black-box queries. Prior work has shown that an ensemble of models is more resilient to transferability: the probability that an adversarial example is effective against most models of the ensemble is low. Thus, most ongoing research focuses on improving ensemble diversity. Another line of prior work has shown that Lipschitz continuity of the models can make models more robust since it limits how a model's output changes with small input perturbations. In this paper, we study the effect of Lipschitz continuity on transferability rates. We show that although a lower Lipschitz constant increases the robustness of a single model, it is not as beneficial in training robust ensembles as it increases the transferability rate of adversarial examples across models in the ensemble. Therefore, we introduce LOTOS, a new training paradigm for ensembles, which counteracts this adverse effect. It does so by promoting orthogonality among the top-$k$ sub-spaces of the transformations of the corresponding affine layers of any pair of models in the ensemble. We theoretically show that $k$ does not need to be large for convolutional layers, which makes the computational overhead negligible. Through various experiments, we show LOTOS increases the robust accuracy of ensembles of ResNet-18 models by $6$ percentage points (p.p) against black-box attacks on CIFAR-10. It is also capable of combining with the robustness of prior state-of-the-art methods for training robust ensembles to enhance their robust accuracy by $10.7$ p.p.</li>
</ul>

<h3>Title: Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yifan Yang, Hao Ban, Minhui Huang, Shiqian Ma, Kaiyi Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05140">https://arxiv.org/abs/2410.05140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05140">https://arxiv.org/pdf/2410.05140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05140]] Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis(https://arxiv.org/abs/2410.05140)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Bilevel optimization has recently attracted considerable attention due to its abundant applications in machine learning problems. However, existing methods rely on prior knowledge of problem parameters to determine stepsizes, resulting in significant effort in tuning stepsizes when these parameters are unknown. In this paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO. D-TFBO employs a double-loop structure with stepsizes adaptively adjusted by the "inverse of cumulative gradient norms" strategy. S-TFBO features a simpler fully single-loop structure that updates three variables simultaneously with a theory-motivated joint design of adaptive stepsizes for all variables. We provide a comprehensive convergence analysis for both algorithms and show that D-TFBO and S-TFBO respectively require $O(\frac{1}{\epsilon})$ and $O(\frac{1}{\epsilon}\log^4(\frac{1}{\epsilon}))$ iterations to find an $\epsilon$-accurate stationary point, (nearly) matching their well-tuned counterparts using the information of problem parameters. Experiments on various problems show that our methods achieve performance comparable to existing well-tuned approaches, while being more robust to the selection of initial stepsizes. To the best of our knowledge, our methods are the first to completely eliminate the need for stepsize tuning, while achieving theoretical guarantees.</li>
</ul>

<h3>Title: Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information</h3>
<ul>
<li><strong>Authors: </strong>Timofey Efimov, Harry Dong, Megna Shah, Jeff Simmons, Sean Donegan, Yuejie Chi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05143">https://arxiv.org/abs/2410.05143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05143">https://arxiv.org/pdf/2410.05143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05143]] Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information(https://arxiv.org/abs/2410.05143)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have found phenomenal success as expressive priors for solving inverse problems, but their extension beyond natural images to more structured scientific domains remains limited. Motivated by applications in materials science, we aim to reduce the number of measurements required from an expensive imaging modality of interest, by leveraging side information from an auxiliary modality that is much cheaper to obtain. To deal with the non-differentiable and black-box nature of the forward model, we propose a framework to train a multimodal diffusion model over the joint modalities, turning inverse problems with black-box forward models into simple linear inpainting problems. Numerically, we demonstrate the feasibility of training diffusion models over materials imagery data, and show that our approach achieves superior image reconstruction by leveraging the available side information, requiring significantly less amount of data from the expensive microscopy modality.</li>
</ul>

<h3>Title: MIBench: A Comprehensive Benchmark for Model Inversion Attack and Defense</h3>
<ul>
<li><strong>Authors: </strong>Yixiang Qiu, Hongyao Yu, Hao Fang, Wenbo Yu, Bin Chen, Xuan Wang, Shu-Tao Xia, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05159">https://arxiv.org/abs/2410.05159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05159">https://arxiv.org/pdf/2410.05159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05159]] MIBench: A Comprehensive Benchmark for Model Inversion Attack and Defense(https://arxiv.org/abs/2410.05159)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, fair</a></li>
<li><strong>Abstract: </strong>Model Inversion (MI) attacks aim at leveraging the output information of target models to reconstruct privacy-sensitive training data, raising widespread concerns on privacy threats of Deep Neural Networks (DNNs). Unfortunately, in tandem with the rapid evolution of MI attacks, the lack of a comprehensive, aligned, and reliable benchmark has emerged as a formidable challenge. This deficiency leads to inadequate comparisons between different attack methods and inconsistent experimental setups. In this paper, we introduce the first practical benchmark for model inversion attacks and defenses to address this critical gap, which is named \textit{MIBench}. This benchmark serves as an extensible and reproducible modular-based toolbox and currently integrates a total of 16 state-of-the-art attack and defense methods. Moreover, we furnish a suite of assessment tools encompassing 9 commonly used evaluation protocols to facilitate standardized and fair evaluation and analysis. Capitalizing on this foundation, we conduct extensive experiments from multiple perspectives to holistically compare and analyze the performance of various methods across different scenarios, which overcomes the misalignment issues and discrepancy prevalent in previous works. Based on the collected attack methods and defense strategies, we analyze the impact of target resolution, defense robustness, model predictive power, model architectures, transferability and loss function. Our hope is that this \textit{MIBench} could provide a unified, practical and extensible toolbox and is widely utilized by researchers in the field to rigorously test and compare their novel methods, ensuring equitable evaluations and thereby propelling further advancements in the future development.</li>
</ul>

<h3>Title: Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mehrdad Farahani, Richard Johansson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05162">https://arxiv.org/abs/2410.05162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05162">https://arxiv.org/pdf/2410.05162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05162]] Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models(https://arxiv.org/abs/2410.05162)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in \emph{how} the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of \emph{whether the context is relevant}, and second, how the encoder computes output representations to support copying when relevant.</li>
</ul>

<h3>Title: A Simulation-Free Deep Learning Approach to Stochastic Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Mengjian Hua, Matthieu Laurière, Eric Vanden-Eijnden</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05163">https://arxiv.org/abs/2410.05163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05163">https://arxiv.org/pdf/2410.05163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05163]] A Simulation-Free Deep Learning Approach to Stochastic Optimal Control(https://arxiv.org/abs/2410.05163)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a simulation-free algorithm for the solution of generic problems in stochastic optimal control (SOC). Unlike existing methods, our approach does not require the solution of an adjoint problem, but rather leverages Girsanov theorem to directly calculate the gradient of the SOC objective on-policy. This allows us to speed up the optimization of control policies parameterized by neural networks since it completely avoids the expensive back-propagation step through stochastic differential equations (SDEs) used in the Neural SDE framework. In particular, it enables us to solve SOC problems in high dimension and on long time horizons. We demonstrate the efficiency of our approach in various domains of applications, including standard stochastic optimal control problems, sampling from unnormalized distributions via construction of a Schrödinger-Föllmer process, and fine-tuning of pre-trained diffusion models. In all cases our method is shown to outperform the existing methods in both the computing time and memory efficiency.</li>
</ul>

<h3>Title: ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05168">https://arxiv.org/abs/2410.05168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05168">https://arxiv.org/pdf/2410.05168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05168]] ReasoningRank: Teaching Student Models to Rank through Reasoning-Based Knowledge Distillation(https://arxiv.org/abs/2410.05168)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reranking documents based on their relevance to a given query is critical in information retrieval. Traditional reranking methods often focus on improving the initial rankings but lack transparency, failing to explain why one document is ranked higher. In this paper, we introduce ReasoningRank, a novel reranking approach that enhances clarity by generating two types of reasoning: explicit reasoning, which explains how a document addresses the query, and comparison reasoning, which justifies the relevance of one document over another. We leverage large language models (LLMs) as teacher models to generate these explanations and distill this knowledge into smaller, more resource-efficient student models. While the student models may not outperform LLMs in speed, they significantly reduce the computational burden by requiring fewer resources, making them more suitable for large-scale or resource-constrained settings. These student models are trained to both generate meaningful reasoning and rerank documents, achieving competitive performance across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate that ReasoningRank improves reranking accuracy and provides valuable insights into the decision-making process, offering a structured and interpretable solution for reranking tasks.</li>
</ul>

<h3>Title: Enhancing Equity in Large Language Models for Medical Applications</h3>
<ul>
<li><strong>Authors: </strong>Yuelyu Ji, Wenhe Ma, Sonish Sivarajkumar, Hang Zhang, Eugene Mathew Sadhu, Zhuochun Li, Xizhi Wu, Shyam Visweswaran, Yanshan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05180">https://arxiv.org/abs/2410.05180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05180">https://arxiv.org/pdf/2410.05180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05180]] Enhancing Equity in Large Language Models for Medical Applications(https://arxiv.org/abs/2410.05180)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements have highlighted the potential of large language models (LLMs) in medical applications, notably in automating Clinical Trial Matching for translational research and providing medical question-answering for clinical decision support. However, our study reveals significant inequities in the use of LLMs, particularly for individuals from specific racial, gender, and underrepresented groups influenced by social determinants of health. These disparities could worsen existing health inequities if LLMs are broadly adopted in healthcare. To address this, we propose and evaluate a novel framework, EquityGuard, designed to detect and mitigate biases in LLM-based medical applications. EquityGuard incorporates a Bias Detection Mechanism capable of identifying and correcting unfair predictions, thus enhancing outcomes and promoting equity across diverse population groups.</li>
</ul>

<h3>Title: MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain</h3>
<ul>
<li><strong>Authors: </strong>Timothy Chase Jr, Karthik Dantu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05182">https://arxiv.org/abs/2410.05182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05182">https://arxiv.org/pdf/2410.05182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05182]] MARs: Multi-view Attention Regularizations for Patch-based Feature Recognition of Space Terrain(https://arxiv.org/abs/2410.05182)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The visual detection and tracking of surface terrain is required for spacecraft to safely land on or navigate within close proximity to celestial objects. Current approaches rely on template matching with pre-gathered patch-based features, which are expensive to obtain and a limiting factor in perceptual capability. While recent literature has focused on in-situ detection methods to enhance navigation and operational autonomy, robust description is still needed. In this work, we explore metric learning as the lightweight feature description mechanism and find that current solutions fail to address inter-class similarity and multi-view observational geometry. We attribute this to the view-unaware attention mechanism and introduce Multi-view Attention Regularizations (MARs) to constrain the channel and spatial attention across multiple feature views, regularizing the what and where of attention focus. We thoroughly analyze many modern metric learning losses with and without MARs and demonstrate improved terrain-feature recognition performance by upwards of 85%. We additionally introduce the Luna-1 dataset, consisting of Moon crater landmarks and reference navigation frames from NASA mission data to support future research in this difficult task. Luna-1 and source code are publicly available at this https URL.</li>
</ul>

<h3>Title: RevisEval: Improving LLM-as-a-Judge via Response-Adapted References</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05193">https://arxiv.org/abs/2410.05193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05193">https://arxiv.org/pdf/2410.05193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05193]] RevisEval: Improving LLM-as-a-Judge via Response-Adapted References(https://arxiv.org/abs/2410.05193)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.</li>
</ul>

<h3>Title: Organizing Unstructured Image Collections using Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Liu, Zhun Zhong, Jun Li, Gianni Franchi, Subhankar Roy, Elisa Ricci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05217">https://arxiv.org/abs/2410.05217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05217">https://arxiv.org/pdf/2410.05217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05217]] Organizing Unstructured Image Collections using Natural Language(https://arxiv.org/abs/2410.05217)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Organizing unstructured visual data into semantic clusters is a key challenge in computer vision. Traditional deep clustering (DC) approaches focus on a single partition of data, while multiple clustering (MC) methods address this limitation by uncovering distinct clustering solutions. The rise of large language models (LLMs) and multimodal LLMs (MLLMs) has enhanced MC by allowing users to define clustering criteria in natural language. However, manually specifying criteria for large datasets is impractical. In this work, we introduce the task Semantic Multiple Clustering (SMC) that aims to automatically discover clustering criteria from large image collections, uncovering interpretable substructures without requiring human input. Our framework, Text Driven Semantic Multiple Clustering (TeDeSC), uses text as a proxy to concurrently reason over large image collections, discover partitioning criteria, expressed in natural language, and reveal semantic substructures. To evaluate TeDeSC, we introduce the COCO-4c and Food-4c benchmarks, each containing four grouping criteria and ground-truth annotations. We apply TeDeSC to various applications, such as discovering biases and analyzing social media image popularity, demonstrating its utility as a tool for automatically organizing image collections and revealing novel insights.</li>
</ul>

<h3>Title: Density estimation with LLMs: a geometric investigation of in-context learning trajectories</h3>
<ul>
<li><strong>Authors: </strong>Toni J.B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05218">https://arxiv.org/abs/2410.05218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05218">https://arxiv.org/pdf/2410.05218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05218]] Density estimation with LLMs: a geometric investigation of in-context learning trajectories(https://arxiv.org/abs/2410.05218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable emergent abilities to perform in-context learning across various tasks, including time series forecasting. This work investigates LLMs' ability to estimate probability density functions (PDFs) from data observed in-context; such density estimation (DE) is a fundamental task underlying many probabilistic modeling problems. We leverage the Intensive Principal Component Analysis (InPCA) to visualize and analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is that these LLMs all follow similar learning trajectories in a low-dimensional InPCA space, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). We interpret the LLaMA in-context DE process as a KDE with an adaptive kernel width and shape. This custom kernel model captures a significant portion of LLaMA's behavior despite having only two parameters. We further speculate on why LLaMA's kernel width and shape differs from classical algorithms, providing insights into the mechanism of in-context probabilistic reasoning in LLMs.</li>
</ul>

<h3>Title: Precise Model Benchmarking with Only a Few Observations</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05222">https://arxiv.org/abs/2410.05222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05222">https://arxiv.org/pdf/2410.05222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05222]] Precise Model Benchmarking with Only a Few Observations(https://arxiv.org/abs/2410.05222)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How can we precisely estimate a large language model's (LLM) accuracy on questions belonging to a specific topic within a larger question-answering dataset? The standard direct estimator, which averages the model's accuracy on the questions in each subgroup, may exhibit high variance for subgroups (topics) with small sample sizes. Synthetic regression modeling, which leverages the model's accuracy on questions about other topics, may yield biased estimates that are too unreliable for large subgroups. We prescribe a simple yet effective solution: an empirical Bayes (EB) estimator that balances direct and regression estimates for each subgroup separately, improving the precision of subgroup-level estimates of model performance. Our experiments on multiple datasets show that this approach consistently provides more precise estimates of the LLM performance compared to the direct and regression approaches, achieving substantial reductions in the mean squared error. Confidence intervals for EB estimates also have near-nominal coverage and are narrower compared to those for the direct estimator. Additional experiments on tabular and vision data validate the benefits of this EB approach.</li>
</ul>

<h3>Title: Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates</h3>
<ul>
<li><strong>Authors: </strong>Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05224">https://arxiv.org/abs/2410.05224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05224">https://arxiv.org/pdf/2410.05224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05224]] Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates(https://arxiv.org/abs/2410.05224)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on instruction datasets is a common way to improve their generative capabilities. However, instruction datasets can be expensive and time-consuming to manually curate, and while LLM-generated data is less labor-intensive, it may violate user privacy agreements or terms of service of LLM providers. Therefore, we seek a way of constructing instruction datasets with samples that are not generated by humans or LLMs but still improve LLM generative capabilities. In this work, we introduce Cookbook, a framework that programmatically generates training data consisting of simple patterns over random tokens, resulting in a scalable, cost-effective approach that avoids legal and privacy issues. First, Cookbook uses a template -- a data generating Python function -- to produce training data that encourages the model to learn an explicit pattern-based rule that corresponds to a desired task. We find that fine-tuning on Cookbook-generated data is able to improve performance on its corresponding task by up to 52.7 accuracy points. Second, since instruction datasets improve performance on multiple downstream tasks simultaneously, Cookbook algorithmically learns how to mix data from various templates to optimize performance on multiple tasks. On the standard multi-task GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated dataset attains the best accuracy on average compared to other 7B parameter instruction-tuned models and is the best performing model on 3 out of 8 tasks. Finally, we analyze when and why Cookbook improves performance and present a metric that allows us to verify that the improvement is largely explained by the model's generations adhering better to template rules.</li>
</ul>

<h3>Title: GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05229">https://arxiv.org/abs/2410.05229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05229">https://arxiv.org/pdf/2410.05229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05229]] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models(https://arxiv.org/abs/2410.05229)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of this http URL findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.</li>
</ul>

<h3>Title: DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Yongtai Zhuo, Yiqing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05234">https://arxiv.org/abs/2410.05234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05234">https://arxiv.org/pdf/2410.05234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05234]] DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration(https://arxiv.org/abs/2410.05234)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Deformable image registration aims to precisely align medical images from different modalities or times. Traditional deep learning methods, while effective, often lack interpretability, real-time observability and adjustment capacity during registration inference. Denoising diffusion models present an alternative by reformulating registration as iterative image denoising. However, existing diffusion registration approaches do not fully harness capabilities, neglecting the critical sampling phase that enables continuous observability during the inference. Hence, we introduce DiffuseReg, an innovative diffusion-based method that denoises deformation fields instead of images for improved transparency. We also propose a novel denoising network upon Swin Transformer, which better integrates moving and fixed images with diffusion time step throughout the denoising process. Furthermore, we enhance control over the denoising registration process with a novel similarity consistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg outperforms existing diffusion registration methods by 1.32 in Dice score. The sampling process in DiffuseReg enables real-time output observability and adjustment unmatched by previous deep models.</li>
</ul>

<h3>Title: CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures</h3>
<ul>
<li><strong>Authors: </strong>katerina Sviridova, Anar Yeginbergen, Ainara Estarrona, Elena Cabrio, Serena Villata, Rodrigo Agerri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05235">https://arxiv.org/abs/2410.05235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05235">https://arxiv.org/pdf/2410.05235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05235]] CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures(https://arxiv.org/abs/2410.05235)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Explaining Artificial Intelligence (AI) decisions is a major challenge nowadays in AI, in particular when applied to sensitive scenarios like medicine and law. However, the need to explain the rationale behind decisions is a main issue also for human-based deliberation as it is important to justify \textit{why} a certain decision has been taken. Resident medical doctors for instance are required not only to provide a (possibly correct) diagnosis, but also to explain how they reached a certain conclusion. Developing new tools to aid residents to train their explanation skills is therefore a central objective of AI in education. In this paper, we follow this direction, and we present, to the best of our knowledge, the first multilingual dataset for Medical Question Answering where correct and incorrect diagnoses for a clinical case are enriched with a natural language explanation written by doctors. These explanations have been manually annotated with argument components (i.e., premise, claim) and argument relations (i.e., attack, support), resulting in the Multilingual CasiMedicos-Arg dataset which consists of 558 clinical cases in four languages (English, Spanish, French, Italian) with explanations, where we annotated 5021 claims, 2313 premises, 2431 support relations, and 1106 attack relations. We conclude by showing how competitive baselines perform over this challenging dataset for the argument mining task.</li>
</ul>

<h3>Title: TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models</h3>
<ul>
<li><strong>Authors: </strong>Rabin Adhikari, Safal Thapaliya, Manish Dhakal, Bishesh Khanal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05239">https://arxiv.org/abs/2410.05239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05239">https://arxiv.org/pdf/2410.05239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05239]] TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models(https://arxiv.org/abs/2410.05239)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have shown impressive performance in vision tasks, but adapting them to new domains often requires expensive fine-tuning. Prompt tuning techniques, including textual, visual, and multimodal prompting, offer efficient alternatives by leveraging learnable prompts. However, their application to Vision-Language Segmentation Models (VLSMs) and evaluation under significant domain shifts remain unexplored. This work presents an open-source benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal prompt tuning techniques into VLSMs, making prompt tuning usable for downstream segmentation datasets with any number of classes. TuneVLSeg includes $6$ prompt tuning strategies on various prompt depths used in $2$ VLSMs totaling of $8$ different combinations. We test various prompt tuning on $8$ diverse medical datasets, including $3$ radiology datasets (breast tumor, echocardiograph, chest X-ray pathologies) and $5$ non-radiology datasets (polyp, ulcer, skin cancer), and two natural domain segmentation datasets. Our study found that textual prompt tuning struggles under significant domain shifts, from natural-domain images to medical data. Furthermore, visual prompt tuning, with fewer hyperparameters than multimodal prompt tuning, often achieves performance competitive to multimodal approaches, making it a valuable first attempt. Our work advances the understanding and applicability of different prompt-tuning techniques for robust domain-specific segmentation. The source code is available at this https URL.</li>
</ul>

<h3>Title: SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Xiao, Shujian Zhang, Wenxuan Zhou, Marzyeh Ghassemi, Sanqiang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05248">https://arxiv.org/abs/2410.05248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05248">https://arxiv.org/pdf/2410.05248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05248]] SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe(https://arxiv.org/abs/2410.05248)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications.</li>
</ul>

<h3>Title: Causal Micro-Narratives</h3>
<ul>
<li><strong>Authors: </strong>Mourad Heddaya, Qingcheng Zeng, Chenhao Tan, Rob Voigt, Alexander Zentefis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05252">https://arxiv.org/abs/2410.05252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05252">https://arxiv.org/pdf/2410.05252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05252]] Causal Micro-Narratives(https://arxiv.org/abs/2410.05252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model--a fine-tuned Llama 3.1 8B--achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research.</li>
</ul>

<h3>Title: GLEE: A Unified Framework and Benchmark for Language-based Economic Environments</h3>
<ul>
<li><strong>Authors: </strong>Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, Moshe Tennenholtz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.GT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05254">https://arxiv.org/abs/2410.05254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05254">https://arxiv.org/pdf/2410.05254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05254]] GLEE: A Unified Framework and Benchmark for Language-based Economic Environments(https://arxiv.org/abs/2410.05254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. While the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents.</li>
</ul>

<h3>Title: SePPO: Semi-Policy Preference Optimization for Diffusion Alignment</h3>
<ul>
<li><strong>Authors: </strong>Daoan Zhang, Guangchen Lan, Dong-Jun Han, Wenlin Yao, Xiaoman Pan, Hongming Zhang, Mingxiao Li, Pengcheng Chen, Yu Dong, Christopher Brinton, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05255">https://arxiv.org/abs/2410.05255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05255">https://arxiv.org/pdf/2410.05255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05255]] SePPO: Semi-Policy Preference Optimization for Diffusion Alignment(https://arxiv.org/abs/2410.05255)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace "losing images" in preference pairs. This approach allows us to optimize using only off-policy "winning images." Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in this https URL.</li>
</ul>

<h3>Title: Differential Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05258">https://arxiv.org/abs/2410.05258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05258">https://arxiv.org/pdf/2410.05258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05258]] Differential Transformer(https://arxiv.org/abs/2410.05258)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.</li>
</ul>

<h3>Title: GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yukang Cao, Masoud Hadi, Liang Pan, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05259">https://arxiv.org/abs/2410.05259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05259">https://arxiv.org/pdf/2410.05259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05259]] GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting(https://arxiv.org/abs/2410.05259)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based 2D virtual try-on (VTON) techniques have recently demonstrated strong performance, while the development of 3D VTON has largely lagged behind. Despite recent advances in text-guided 3D scene editing, integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains challenging. The reasons are twofold. First, text prompts cannot provide sufficient details in describing clothing. Second, 2D VTON results generated from different viewpoints of the same 3D scene lack coherence and spatial relationships, hence frequently leading to appearance inconsistencies and geometric distortions. To resolve these problems, we introduce an image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained knowledge from 2D VTON models to 3D while improving cross-view consistency. (1) Specifically, we propose a personalized diffusion model that utilizes low-rank adaptation (LoRA) fine-tuning to incorporate personalized information into pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a reference-driven image editing approach that enables the simultaneous editing of multi-view images while ensuring consistency. (2) Furthermore, we propose a persona-aware 3DGS editing framework to facilitate effective editing while maintaining consistent cross-view appearance and high-quality 3D geometry. (3) Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which facilitates comprehensive qualitative and quantitative 3D VTON evaluations. Through extensive experiments and comparative analyses with existing methods, the proposed \OM has demonstrated superior fidelity and advanced editing capabilities, affirming its effectiveness for 3D VTON.</li>
</ul>

<h3>Title: DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</h3>
<ul>
<li><strong>Authors: </strong>Kaifeng Zhao, Gen Li, Siyu Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05260">https://arxiv.org/abs/2410.05260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05260">https://arxiv.org/pdf/2410.05260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05260]] DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control(https://arxiv.org/abs/2410.05260)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose DART, a Diffusion-based Autoregressive motion primitive model for Real-time Text-driven motion control. Our model, DART, effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model's versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Video results are available on the project page: this https URL.</li>
</ul>

<h3>Title: TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles</h3>
<ul>
<li><strong>Authors: </strong>Qingchen Yu, Shichao Song, Ke Fang, Yunfeng Shi, Zifan Zheng, Hanyu Wang, Simin Niu, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05262">https://arxiv.org/abs/2410.05262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05262">https://arxiv.org/pdf/2410.05262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05262]] TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles(https://arxiv.org/abs/2410.05262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model's logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as "the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides reasoning benefits but also incurs noise costs."</li>
</ul>

<h3>Title: PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05265">https://arxiv.org/abs/2410.05265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05265">https://arxiv.org/pdf/2410.05265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05265]] PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs(https://arxiv.org/abs/2410.05265)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Andrew F. Luo, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05266">https://arxiv.org/abs/2410.05266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05266">https://arxiv.org/pdf/2410.05266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05266]] Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers(https://arxiv.org/abs/2410.05266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Advances in large-scale artificial neural networks have facilitated novel insights into the functional topology of the brain. Here, we leverage this approach to study how semantic categories are organized in the human visual cortex. To overcome the challenge presented by the co-occurrence of multiple categories in natural images, we introduce BrainSAIL (Semantic Attribution and Image Localization), a method for isolating specific neurally-activating visual concepts in images. BrainSAIL exploits semantically consistent, dense spatial features from pre-trained vision models, building upon their demonstrated ability to robustly predict neural activity. This method derives clean, spatially dense embeddings without requiring any additional training, and employs a novel denoising process that leverages the semantic consistency of images under random augmentations. By unifying the space of whole-image embeddings and dense visual features and then applying voxel-wise encoding models to these features, we enable the identification of specific subregions of each image which drive selectivity patterns in different areas of the higher visual cortex. We validate BrainSAIL on cortical regions with known category selectivity, demonstrating its ability to accurately localize and disentangle selectivity to diverse visual concepts. Next, we demonstrate BrainSAIL's ability to characterize high-level visual selectivity to scene properties and low-level visual features such as depth, luminance, and saturation, providing insights into the encoding of complex visual information. Finally, we use BrainSAIL to directly compare the feature selectivity of different brain encoding models across different regions of interest in visual cortex. Our innovative method paves the way for significant advances in mapping and decomposing high-level visual representations in the human brain.</li>
</ul>

<h3>Title: Grounding Partially-Defined Events in Multimodal Data</h3>
<ul>
<li><strong>Authors: </strong>Kate Sanders, Reno Kriz, David Etter, Hannah Recknor, Alexander Martin, Cameron Carpenter, Jingyang Lin, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05267">https://arxiv.org/abs/2410.05267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05267">https://arxiv.org/pdf/2410.05267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05267]] Grounding Partially-Defined Events in Multimodal Data(https://arxiv.org/abs/2410.05267)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>How are we able to learn about complex current events just from short snippets of video? While natural language enables straightforward ways to represent under-specified, partially observable events, visual data does not facilitate analogous methods and, consequently, introduces unique challenges in event understanding. With the growing prevalence of vision-capable AI agents, these systems must be able to model events from collections of unstructured video data. To tackle robust event modeling in multimodal settings, we introduce a multimodal formulation for partially-defined events and cast the extraction of these events as a three-stage span retrieval task. We propose a corresponding benchmark for this task, MultiVENT-G, that consists of 14.5 hours of densely annotated current event videos and 1,168 text documents, containing 22.8K labeled event-centric entities. We propose a collection of LLM-driven approaches to the task of multimodal event analysis, and evaluate them on MultiVENT-G. Results illustrate the challenges that abstract event understanding poses and demonstrates promise in event-centric video-language systems.</li>
</ul>

<h3>Title: Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Wang, Ninareh Mehrabi, Palash Goyal, Rahul Gupta, Kai-Wei Chang, Aram Galstyan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05269">https://arxiv.org/abs/2410.05269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05269">https://arxiv.org/pdf/2410.05269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05269]] Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models(https://arxiv.org/abs/2410.05269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
