<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A study on the use of perceptual hashing to detect manipulation of embedded messages in images. (arXiv:2303.00092v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00092">http://arxiv.org/abs/2303.00092</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00092] A study on the use of perceptual hashing to detect manipulation of embedded messages in images](http://arxiv.org/abs/2303.00092) #secure</code></li>
<li>Summary: <p>Typically, metadata of images are stored in a specific data segment of the
image file. However, to securely detect changes, data can also be embedded
within images. This follows the goal to invisibly and robustly embed as much
information as possible to, ideally, even survive compression.
</p></li>
</ul>

<p>This work searches for embedding principles which allow to distinguish
between unintended changes by lossy image compression and malicious
manipulation of the embedded message based on the change of its perceptual or
robust hash. Different embedding and compression algorithms are compared.
</p>
<p>The study shows that embedding a message via integer wavelet transform and
compression with Karhunen-Loeve-transform yields the best results. However, it
was not possible to distinguish between manipulation and compression in all
cases.
</p>

<h3>Title: Access-based Lightweight Physical Layer Authentication for the Internet of Things Devices. (arXiv:2303.00307v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00307">http://arxiv.org/abs/2303.00307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00307] Access-based Lightweight Physical Layer Authentication for the Internet of Things Devices](http://arxiv.org/abs/2303.00307) #secure</code></li>
<li>Summary: <p>Physical-layer authentication is a popular alternative to the conventional
key-based authentication for internet of things (IoT) devices due to their
limited computational capacity and battery power. However, this approach has
limitations due to poor robustness under channel fluctuations, reconciliation
overhead, and no clear safeguard distance to ensure the secrecy of the
generated authentication keys. In this regard, we propose a novel, secure, and
lightweight continuous authentication scheme for IoT device authentication. Our
scheme utilizes the inherent properties of the IoT devices transmission model
as its source for seed generation and device authentication. Specifically, our
proposed scheme provides continuous authentication by checking the access time
slots and spreading sequences of the IoT devices instead of repeatedly
generating and verifying shared keys. Due to this, access to a coherent key is
not required in our proposed scheme, resulting in the concealment of the seed
information from attackers. Our proposed authentication scheme for IoT devices
demonstrates improved performance compared to the benchmark schemes relying on
physical-channel. Our empirical results find a near threefold decrease in
misdetection rate of illegitimate devices and close to zero false alarm rate in
various system settings with varied numbers of active devices up to 200 and
signal-to-noise ratio from 0 dB to 30 dB. Our proposed authentication scheme
also has a lower computational complexity of at least half the computational
cost of the benchmark schemes based on support vector machine and binary
hypothesis testing in our studies. This further corroborates the practicality
of our scheme for IoT deployments.
</p></li>
</ul>

<h3>Title: SMPC Task Decomposition: A Theory for Accelerating Secure Multi-party Computation Task. (arXiv:2303.00343v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00343">http://arxiv.org/abs/2303.00343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00343] SMPC Task Decomposition: A Theory for Accelerating Secure Multi-party Computation Task](http://arxiv.org/abs/2303.00343) #secure</code></li>
<li>Summary: <p>Today, we are in the era of big data, and data are becoming more and more
important, especially private data. Secure Multi-party Computation (SMPC)
technology enables parties to perform computing tasks without revealing
original data. However, the underlying implementation of SMPC is too heavy,
such as garbled circuit (GC) and oblivious transfer(OT). Every time a piece of
data is added, the resources consumed by GC and OT will increase a lot.
Therefore, it is unacceptable to process large-scale data in a single SMPC
task.
</p></li>
</ul>

<p>In this work, we propose a novel theory called SMPC Task Decomposition
(SMPCTD), which can securely decompose a single SMPC task into multiple SMPC
sub-tasks and multiple local tasks without leaking the original data. After
decomposition, the computing time, memory and communication consumption drop
sharply. We then decompose three machine learning (ML) SMPC tasks using our
theory and implement them based on a hybrid protocol framework called ABY.
Furthermore, we use incremental computation technique to expand the amount of
data involved in these three SMPC tasks. The experimental results show that
after decomposing these three SMPC tasks, the time, memory and communication
consumption are not only greatly reduced, but also stabilized within a certain
range.
</p>

<h3>Title: Self-Sovereign Identity for Trust and Interoperability in the Metaverse. (arXiv:2303.00422v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00422">http://arxiv.org/abs/2303.00422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00422] Self-Sovereign Identity for Trust and Interoperability in the Metaverse](http://arxiv.org/abs/2303.00422) #secure</code></li>
<li>Summary: <p>With the advancement in computing power and speed, the Internet is being
transformed from screen-based information to immersive and extremely low
latency communication environments in web 3.0 and the Metaverse. With the
emergence of the Metaverse technology, more stringent demands are required in
terms of connectivity such as secure access and data privacy. Future
technologies such as 6G, Blockchain, and Artificial Intelligence (AI) can
mitigate some of these challenges. The Metaverse is now on the verge where
security and privacy concerns are crucial for the successful adaptation of such
disruptive technology. The Metaverse and web 3.0 are to be decentralized,
anonymous, and interoperable. Metaverse is the virtual world of Digital Twins
and non-fungible tokens (NFTs). The control and possession of users' data on
centralized servers are the cause of numerous security and privacy concerns.
This paper proposes a solution for the security and interoperability challenges
using Self-Sovereign Identity (SSI) integrated with blockchain. The philosophy
of Self-Sovereign Identity, where the users are the only holders and owners of
their identity, comes in handy to solve the questions of decentralization,
trust, and interoperability in the Metaverse. This work also discusses the
vision of a single, open standard, trustworthy, and interoperable Metaverse
with initial design and implementation of SSI concepts.
</p></li>
</ul>

<h3>Title: Dishing Out DoS: How to Disable and Secure the Starlink User Terminal. (arXiv:2303.00582v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00582">http://arxiv.org/abs/2303.00582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00582] Dishing Out DoS: How to Disable and Secure the Starlink User Terminal](http://arxiv.org/abs/2303.00582) #secure</code></li>
<li>Summary: <p>Satellite user terminals are a promising target for adversaries seeking to
target satellite communication networks. Despite this, many protections
commonly found in terrestrial routers are not present in some user terminals.
</p></li>
</ul>

<p>As a case study we audit the attack surface presented by the Starlink
router's admin interface, using fuzzing to uncover a denial of service attack
on the Starlink user terminal. We explore the attack's impact, particularly in
the cases of drive-by attackers, and attackers that are able to maintain a
continuous presence on the network. Finally, we discuss wider implications,
looking at lessons learned in terrestrial router security, and how to properly
implement them in this new context.
</p>

<h2>security</h2>
<h3>Title: A Practical Upper Bound for the Worst-Case Attribution Deviations. (arXiv:2303.00340v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00340">http://arxiv.org/abs/2303.00340</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00340] A Practical Upper Bound for the Worst-Case Attribution Deviations](http://arxiv.org/abs/2303.00340) #security</code></li>
<li>Summary: <p>Model attribution is a critical component of deep neural networks (DNNs) for
its interpretability to complex models. Recent studies bring up attention to
the security of attribution methods as they are vulnerable to attribution
attacks that generate similar images with dramatically different attributions.
Existing works have been investigating empirically improving the robustness of
DNNs against those attacks; however, none of them explicitly quantifies the
actual deviations of attributions. In this work, for the first time, a
constrained optimization problem is formulated to derive an upper bound that
measures the largest dissimilarity of attributions after the samples are
perturbed by any noises within a certain region while the classification
results remain the same. Based on the formulation, different practical
approaches are introduced to bound the attributions above using Euclidean
distance and cosine similarity under both $\ell_2$ and $\ell_\infty$-norm
perturbations constraints. The bounds developed by our theoretical study are
validated on various datasets and two different types of attacks (PGD attack
and IFIA attribution attack). Over 10 million attacks in the experiments
indicate that the proposed upper bounds effectively quantify the robustness of
models based on the worst-case attribution dissimilarities.
</p></li>
</ul>

<h3>Title: The propagation game: on simulatability, correlation matrices, and probing security. (arXiv:2303.00580v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00580">http://arxiv.org/abs/2303.00580</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00580] The propagation game: on simulatability, correlation matrices, and probing security](http://arxiv.org/abs/2303.00580) #security</code></li>
<li>Summary: <p>This work is intended for researchers in the field of side-channel attacks,
countermeasure analysis, and probing security. It reports on a formalization of
simulatability in terms of categorical properties, which we think will provide
a useful tool in the practitioner toolbox. The formalization allowed us to
revisit some existing definitions (such as probe isolating non-interference) in
a simpler way that corresponds to the propagation of \textit{erase morphisms}
in the diagrammatic language of \prop{} categories. From a theoretical
perspective, we shed light into probabilistic definitions of simulatability and
matrix-based spectral approaches. This could mean, in practice, that
potentially better tools can be built. Readers will find a different, and
perhaps less contrived, definition of simulatability, which could enable new
forms of reasoning. This work does not cover any practical implementation of
the proposed tools, which is left for future work.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Neural Auctions Compromise Bidder Information. (arXiv:2303.00116v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00116">http://arxiv.org/abs/2303.00116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00116] Neural Auctions Compromise Bidder Information](http://arxiv.org/abs/2303.00116) #privacy</code></li>
<li>Summary: <p>Single-shot auctions are commonly used as a means to sell goods, for example
when selling ad space or allocating radio frequencies, however devising
mechanisms for auctions with multiple bidders and multiple items can be
complicated. It has been shown that neural networks can be used to approximate
optimal mechanisms while satisfying the constraints that an auction be
strategyproof and individually rational. We show that despite such auctions
maximizing revenue, they do so at the cost of revealing private bidder
information. While randomness is often used to build in privacy, in this
context it comes with complications if done without care. Specifically, it can
violate rationality and feasibility constraints, fundamentally change the
incentive structure of the mechanism, and/or harm top-level metrics such as
revenue and social welfare. We propose a method that employs stochasticity to
improve privacy while meeting the requirements for auction mechanisms with only
a modest sacrifice in revenue. We analyze the cost to the auction house that
comes with introducing varying degrees of privacy in common auction settings.
Our results show that despite current neural auctions' ability to approximate
optimal mechanisms, the resulting vulnerability that comes with relying on
neural networks must be accounted for.
</p></li>
</ul>

<h3>Title: Two Views of Constrained Differential Privacy: Belief Revision and Update. (arXiv:2303.00228v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00228">http://arxiv.org/abs/2303.00228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00228] Two Views of Constrained Differential Privacy: Belief Revision and Update](http://arxiv.org/abs/2303.00228) #privacy</code></li>
<li>Summary: <p>In this paper, we provide two views of constrained differential private (DP)
mechanisms. The first one is as belief revision. A constrained DP mechanism is
obtained by standard probabilistic conditioning, and hence can be naturally
implemented by Monte Carlo algorithms. The other is as belief update. A
constrained DP is defined according to l2-distance minimization postprocessing
or projection and hence can be naturally implemented by optimization
algorithms. The main advantage of these two perspectives is that we can make
full use of the machinery of belief revision and update to show basic
properties for constrained differential privacy especially some important new
composition properties. Within the framework established in this paper,
constrained DP algorithms in the literature can be classified either as belief
revision or belief update. At the end of the paper, we demonstrate their
differences especially in utility in a couple of scenarios.
</p></li>
</ul>

<h3>Title: FedScore: A privacy-preserving framework for federated scoring system development. (arXiv:2303.00282v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00282">http://arxiv.org/abs/2303.00282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00282] FedScore: A privacy-preserving framework for federated scoring system development](http://arxiv.org/abs/2303.00282) #privacy</code></li>
<li>Summary: <p>We propose FedScore, a privacy-preserving federated learning framework for
scoring system generation across multiple sites to facilitate
cross-institutional collaborations. The FedScore framework includes five
modules: federated variable ranking, federated variable transformation,
federated score derivation, federated model selection and federated model
evaluation. To illustrate usage and assess FedScore's performance, we built a
hypothetical global scoring system for mortality prediction within 30 days
after a visit to an emergency department using 10 simulated sites divided from
a tertiary hospital in Singapore. We employed a pre-existing score generator to
construct 10 local scoring systems independently at each site and we also
developed a scoring system using centralized data for comparison. We compared
the acquired FedScore model's performance with that of other scoring models
using the receiver operating characteristic (ROC) analysis. The FedScore model
achieved an average area under the curve (AUC) value of 0.763 across all sites,
with a standard deviation (SD) of 0.020. We also calculated the average AUC
values and SDs for each local model, and the FedScore model showed promising
accuracy and stability with a high average AUC value which was closest to the
one of the pooled model and SD which was lower than that of most local models.
This study demonstrates that FedScore is a privacy-preserving scoring system
generator with potentially good generalizability.
</p></li>
</ul>

<h3>Title: Towards a Privacy-Preserving Dispute Resolution Protocol on Ethereum. (arXiv:2303.00533v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00533">http://arxiv.org/abs/2303.00533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00533] Towards a Privacy-Preserving Dispute Resolution Protocol on Ethereum](http://arxiv.org/abs/2303.00533) #privacy</code></li>
<li>Summary: <p>We present a new dispute resolution protocol that can be built on the
Ethereum blockchain. Unlike existing applications like Kleros, privacy is
ensured by design through the use of the zero-knowledge protocols Semaphore and
MACI (Minimal Anti-Collusion Infrastructure), which provide, among other
things, resistance to Sybil-like attacks and corruption. Conflicts can only be
resolved by people with expertise in the field, and dispute resolution is
guaranteed despite the users having the final say. Moreover, the proposed model
does not use a native token on the platform, but aims to reward stakeholders
through a social incentive mechanism based on soulbound tokens, introduced by
Weyl, Ohlhaver, and Buterin in 2022. Users with these tokens will be considered
trustworthy and will have the ability to govern the platform. As far as we
know, this is one of the first blockchain projects that seeks to introduce
social governance rather than one based on economic incentives.
</p></li>
</ul>

<h3>Title: How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. (arXiv:2303.00654v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00654">http://arxiv.org/abs/2303.00654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00654] How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy](http://arxiv.org/abs/2303.00654) #privacy</code></li>
<li>Summary: <p>ML models are ubiquitous in real world applications and are a constant focus
of research. At the same time, the community has started to realize the
importance of protecting the privacy of ML training data.
</p></li>
</ul>

<p>Differential Privacy (DP) has become a gold standard for making formal
statements about data anonymization. However, while some adoption of DP has
happened in industry, attempts to apply DP to real world complex ML models are
still few and far between. The adoption of DP is hindered by limited practical
guidance of what DP protection entails, what privacy guarantees to aim for, and
the difficulty of achieving good privacy-utility-computation trade-offs for ML
models. Tricks for tuning and maximizing performance are scattered among papers
or stored in the heads of practitioners. Furthermore, the literature seems to
present conflicting evidence on how and whether to apply architectural
adjustments and which components are ``safe'' to use with DP.
</p>
<p>This work is a self-contained guide that gives an in-depth overview of the
field of DP ML and presents information about achieving the best possible DP ML
model with rigorous privacy guarantees. Our target audience is both researchers
and practitioners. Researchers interested in DP for ML will benefit from a
clear overview of current advances and areas for improvement. We include
theory-focused sections that highlight important topics such as privacy
accounting and its assumptions, and convergence. For a practitioner, we provide
a background in DP theory and a clear step-by-step guide for choosing an
appropriate privacy definition and approach, implementing DP training,
potentially updating the model architecture, and tuning hyperparameters. For
both researchers and practitioners, consistently and fully reporting privacy
guarantees is critical, and so we propose a set of specific best practices for
stating guarantees.
</p>

<h3>Title: What Are the Chances? Explaining the Epsilon Parameter in Differential Privacy. (arXiv:2303.00738v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00738">http://arxiv.org/abs/2303.00738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00738] What Are the Chances? Explaining the Epsilon Parameter in Differential Privacy](http://arxiv.org/abs/2303.00738) #privacy</code></li>
<li>Summary: <p>Differential privacy (DP) is a mathematical privacy notion increasingly
deployed across government and industry. With DP, privacy protections are
probabilistic: they are bounded by the privacy budget parameter, $\epsilon$.
Prior work in health and computational science finds that people struggle to
reason about probabilistic risks. Yet, communicating the implications of
$\epsilon$ to people contributing their data is vital to avoiding privacy
theater -- presenting meaningless privacy protection as meaningful -- and
empowering more informed data-sharing decisions. Drawing on best practices in
risk communication and usability, we develop three methods to convey
probabilistic DP guarantees to end users: two that communicate odds and one
offering concrete examples of DP outputs.
</p></li>
</ul>

<p>We quantitatively evaluate these explanation methods in a vignette survey
study ($n=963$) via three metrics: objective risk comprehension, subjective
privacy understanding of DP guarantees, and self-efficacy. We find that
odds-based explanation methods are more effective than (1) output-based methods
and (2) state-of-the-art approaches that gloss over information about
$\epsilon$. Further, when offered information about $\epsilon$, respondents are
more willing to share their data than when presented with a state-of-the-art DP
explanation; this willingness to share is sensitive to $\epsilon$ values: as
privacy protections weaken, respondents are less likely to share data.
</p>

<h3>Title: DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction. (arXiv:2303.00171v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00171">http://arxiv.org/abs/2303.00171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00171] DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction](http://arxiv.org/abs/2303.00171) #privacy</code></li>
<li>Summary: <p>Personal Digital Assistants (PDAs) - such as Siri, Alexa and Google
Assistant, to name a few - play an increasingly important role to access
information and complete tasks spanning multiple domains, and by diverse groups
of users. A text-to-speech (TTS) module allows PDAs to interact in a natural,
human-like manner, and play a vital role when the interaction involves people
with visual impairments or other disabilities. To cater to the needs of a
diverse set of users, inclusive TTS is important to recognize and pronounce
correctly text in different languages and dialects. Despite great progress in
speech synthesis, the pronunciation accuracy of named entities in a
multi-lingual setting still has a large room for improvement. Existing
approaches to correct named entity (NE) mispronunciations, like retraining
Grapheme-to-Phoneme (G2P) models, or maintaining a TTS pronunciation
dictionary, require expensive annotation of the ground truth pronunciation,
which is also time consuming. In this work, we present a highly-precise,
PDA-compatible pronunciation learning framework for the task of TTS
mispronunciation detection and correction. In addition, we also propose a novel
mispronunciation detection model called DTW-SiameseNet, which employs metric
learning with a Siamese architecture for Dynamic Time Warping (DTW) with
triplet loss. We demonstrate that a locale-agnostic, privacy-preserving
solution to the problem of TTS mispronunciation detection is feasible. We
evaluate our approach on a real-world dataset, and a corpus of NE
pronunciations of an anonymized audio dataset of person names recorded by
participants from 10 different locales. Human evaluation shows our proposed
approach improves pronunciation accuracy on average by ~6% compared to strong
phoneme-based and audio-based baselines.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: DOLOS: A Novel Architecture for Moving Target Defense. (arXiv:2303.00387v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00387">http://arxiv.org/abs/2303.00387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00387] DOLOS: A Novel Architecture for Moving Target Defense](http://arxiv.org/abs/2303.00387) #defense</code></li>
<li>Summary: <p>Moving Target Defense and Cyber Deception emerged in recent years as two key
proactive cyber defense approaches, contrasting with the static nature of the
traditional reactive cyber defense. The key insight behind these approaches is
to impose an asymmetric disadvantage for the attacker by using deception and
randomization techniques to create a dynamic attack surface. Moving Target
Defense typically relies on system randomization and diversification, while
Cyber Deception is based on decoy nodes and fake systems to deceive attackers.
However, current Moving Target Defense techniques are complex to manage and can
introduce high overheads, while Cyber Deception nodes are easily recognized and
avoided by adversaries.
</p></li>
</ul>

<p>This paper presents DOLOS, a novel architecture that unifies Cyber Deception
and Moving Target Defense approaches. DOLOS is motivated by the insight that
deceptive techniques are much more powerful when integrated into production
systems rather than deployed alongside them. DOLOS combines typical Moving
Target Defense techniques, such as randomization, diversity, and redundancy,
with cyber deception and seamlessly integrates them into production systems
through multiple layers of isolation. We extensively evaluate DOLOS against a
wide range of attackers, ranging from automated malware to professional
penetration testers, and show that DOLOS is highly effective in slowing down
attacks and protecting the integrity of production systems. We also provide
valuable insights and considerations for the future development of MTD
techniques based on our findings.
</p>

<h2>attack</h2>
<h3>Title: Feature Extraction Matters More: Universal Deepfake Disruption through Attacking Ensemble Feature Extractors. (arXiv:2303.00200v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00200">http://arxiv.org/abs/2303.00200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00200] Feature Extraction Matters More: Universal Deepfake Disruption through Attacking Ensemble Feature Extractors](http://arxiv.org/abs/2303.00200) #attack</code></li>
<li>Summary: <p>Adversarial example is a rising way of protecting facial privacy security
from deepfake modification. To prevent massive facial images from being
illegally modified by various deepfake models, it is essential to design a
universal deepfake disruptor. However, existing works treat deepfake disruption
as an End-to-End process, ignoring the functional difference between feature
extraction and image reconstruction, which makes it difficult to generate a
cross-model universal disruptor. In this work, we propose a novel
Feature-Output ensemble UNiversal Disruptor (FOUND) against deepfake networks,
which explores a new opinion that considers attacking feature extractors as the
more critical and general task in deepfake disruption. We conduct an effective
two-stage disruption process. We first disrupt multi-model feature extractors
through multi-feature aggregation and individual-feature maintenance, and then
develop a gradient-ensemble algorithm to enhance the disruption effect by
simplifying the complex optimization problem of disrupting multiple End-to-End
models. Extensive experiments demonstrate that FOUND can significantly boost
the disruption effect against ensemble deepfake benchmark models. Besides, our
method can fast obtain a cross-attribute, cross-image, and cross-model
universal deepfake disruptor with only a few training images, surpassing
state-of-the-art universal disruptors in both success rate and efficiency.
</p></li>
</ul>

<h3>Title: To Make Yourself Invisible with Adversarial Semantic Contours. (arXiv:2303.00284v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00284">http://arxiv.org/abs/2303.00284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00284] To Make Yourself Invisible with Adversarial Semantic Contours](http://arxiv.org/abs/2303.00284) #attack</code></li>
<li>Summary: <p>Modern object detectors are vulnerable to adversarial examples, which may
bring risks to real-world applications. The sparse attack is an important task
which, compared with the popular adversarial perturbation on the whole image,
needs to select the potential pixels that is generally regularized by an
$\ell_0$-norm constraint, and simultaneously optimize the corresponding
texture. The non-differentiability of $\ell_0$ norm brings challenges and many
works on attacking object detection adopted manually-designed patterns to
address them, which are meaningless and independent of objects, and therefore
lead to relatively poor attack performance.
</p></li>
</ul>

<p>In this paper, we propose Adversarial Semantic Contour (ASC), an MAP estimate
of a Bayesian formulation of sparse attack with a deceived prior of object
contour. The object contour prior effectively reduces the search space of pixel
selection and improves the attack by introducing more semantic bias. Extensive
experiments demonstrate that ASC can corrupt the prediction of 9 modern
detectors with different architectures (\e.g., one-stage, two-stage and
Transformer) by modifying fewer than 5\% of the pixels of the object area in
COCO in white-box scenario and around 10\% of those in black-box scenario. We
further extend the attack to datasets for autonomous driving systems to verify
the effectiveness. We conclude with cautions about contour being the common
weakness of object detectors with various architecture and the care needed in
applying them in safety-sensitive scenarios.
</p>

<h3>Title: Competence-Based Analysis of Language Models. (arXiv:2303.00333v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00333">http://arxiv.org/abs/2303.00333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00333] Competence-Based Analysis of Language Models](http://arxiv.org/abs/2303.00333) #attack</code></li>
<li>Summary: <p>Despite the recent success of large pretrained language models (LMs) on a
variety of prompting tasks, these models can be alarmingly brittle to small
changes in inputs or application contexts. To better understand such behavior
and motivate the design of more robust LMs, we propose a general experimental
framework, CALM (Competence-based Analysis of Language Models), where targeted
causal interventions are utilized to damage an LM's internal representation of
various linguistic properties in order to evaluate its use of each
representation in performing a given task. We implement these interventions as
gradient-based adversarial attacks, which (in contrast to prior causal probing
methodologies) are able to target arbitrarily-encoded representations of
relational properties, and carry out a case study of this approach to analyze
how BERT-like LMs use representations of several relational properties in
performing associated relation prompting tasks. We find that, while the
representations LMs leverage in performing each task are highly entangled, they
may be meaningfully interpreted in terms of the tasks where they are most
utilized; and more broadly, that CALM enables an expanded scope of inquiry in
LM analysis that may be useful in predicting and explaining weaknesses of
existing LMs.
</p></li>
</ul>

<h3>Title: Optimization and Amplification of Cache Side Channel Signals. (arXiv:2303.00122v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00122">http://arxiv.org/abs/2303.00122</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00122] Optimization and Amplification of Cache Side Channel Signals](http://arxiv.org/abs/2303.00122) #attack</code></li>
<li>Summary: <p>In cache-based side channel attacks, an attacker infers information about the
victim based on the presence, or lack thereof, of one or more cachelines.
Determining a cacheline's presence, which we refer to as "reading the signal",
typically requires testing the access time of the line using a suitably high
precision timer. In this paper we introduce novel gadgets which leverage CPU
speculation to enable modification of these signals, before they are read, for
a variety of purposes. First, these gadgets enable an attacker to optimize
cache-based side channel attacks by evaluating arbitrary logic functions on
cacheline signals prior to their measurement. Second, we demonstrate
amplification techniques that enable an attacker to read a signal even if no
high precision timer is available. Combined, these techniques can be used to
improve existing side channel attacks even if timer access is limited. We
evaluate the effectiveness of these techniques on a modern x86 CPU and
demonstrate that when properly tuned, cache side channel signals can be
reliably modified with near 100% accuracy and are able to be read with a timer
as coarse as 100ms or more.
</p></li>
</ul>

<h3>Title: Addressing DAO Insider Attacks in IPv6-Based Low-Power and Lossy Networks. (arXiv:2303.00260v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00260">http://arxiv.org/abs/2303.00260</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00260] Addressing DAO Insider Attacks in IPv6-Based Low-Power and Lossy Networks](http://arxiv.org/abs/2303.00260) #attack</code></li>
<li>Summary: <p>Low-Power and Lossy Networks (LLNs) run on resource-constrained devices and
play a key role in many Industrial Internet of Things and Cyber-Physical
Systems based applications. But, achieving an energy-efficient routing in LLNs
is a major challenge nowadays. This challenge is addressed by Routing Protocol
for Low-power Lossy Networks (RPL), which is specified in RFC 6550 as a
"Proposed Standard" at present. In RPL, a client node uses Destination
Advertisement Object (DAO) control messages to pass on the destination
information towards the root node. An attacker may exploit the DAO sending
mechanism of RPL to perform a DAO Insider attack in LLNs. In this paper, it is
shown that an aggressive attacker can drastically degrade the network
performance. To address DAO Insider attack, a lightweight defense solution is
proposed. The proposed solution uses an early blacklisting strategy to
significantly mitigate the attack and restore RPL performance. The proposed
solution is implemented and tested on Cooja Simulator.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Neural inverse procedural modeling of knitting yarns from images. (arXiv:2303.00154v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00154">http://arxiv.org/abs/2303.00154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00154] Neural inverse procedural modeling of knitting yarns from images](http://arxiv.org/abs/2303.00154) #robust</code></li>
<li>Summary: <p>We investigate the capabilities of neural inverse procedural modeling to
infer high-quality procedural yarn models with fiber-level details from single
images of depicted yarn samples. While directly inferring all parameters of the
underlying yarn model based on a single neural network may seem an intuitive
choice, we show that the complexity of yarn structures in terms of twisting and
migration characteristics of the involved fibers can be better encountered in
terms of ensembles of networks that focus on individual characteristics. We
analyze the effect of different loss functions including a parameter loss to
penalize the deviation of inferred parameters to ground truth annotations, a
reconstruction loss to enforce similar statistics of the image generated for
the estimated parameters in comparison to training images as well as an
additional regularization term to explicitly penalize deviations between latent
codes of synthetic images and the average latent code of real images in the
latent space of the encoder. We demonstrate that the combination of a carefully
designed parametric, procedural yarn model with respective network ensembles as
well as loss functions even allows robust parameter inference when solely
trained on synthetic data. Since our approach relies on the availability of a
yarn database with parameter annotations and we are not aware of such a
respectively available dataset, we additionally provide, to the best of our
knowledge, the first dataset of yarn images with annotations regarding the
respective yarn parameters. For this purpose, we use a novel yarn generator
that improves the realism of the produced results over previous approaches.
</p></li>
</ul>

<h3>Title: Self-Supervised Convolutional Visual Prompts. (arXiv:2303.00198v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00198">http://arxiv.org/abs/2303.00198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00198] Self-Supervised Convolutional Visual Prompts](http://arxiv.org/abs/2303.00198) #robust</code></li>
<li>Summary: <p>Machine learning models often fail on out-of-distribution (OOD) samples.
Visual prompts emerge as a light-weight adaptation method in input space for
large-scale vision models. Existing vision prompts optimize a high-dimensional
additive vector and require labeled data on training. However, we find this
paradigm fails on test-time adaptation when labeled data is unavailable, where
the high-dimensional visual prompt overfits to the self-supervised objective.
We present convolutional visual prompts for test-time adaptation without
labels. Our convolutional prompt is structured and requires fewer trainable
parameters (less than 1 % parameters of standard visual prompts). Extensive
experiments on a wide variety of OOD recognition tasks show that our approach
is effective, improving robustness by up to 5.87 % over a number of large-scale
model architectures.
</p></li>
</ul>

<h3>Title: Single Image Backdoor Inversion via Robust Smoothed Classifiers. (arXiv:2303.00215v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00215">http://arxiv.org/abs/2303.00215</a></li>
<li>Code URL: <a href="https://github.com/locuslab/smoothinv">https://github.com/locuslab/smoothinv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00215] Single Image Backdoor Inversion via Robust Smoothed Classifiers](http://arxiv.org/abs/2303.00215) #robust</code></li>
<li>Summary: <p>Backdoor inversion, the process of finding a backdoor trigger inserted into a
machine learning model, has become the pillar of many backdoor detection and
defense methods. Previous works on backdoor inversion often recover the
backdoor through an optimization process to flip a support set of clean images
into the target class. However, it is rarely studied and understood how large
this support set should be to recover a successful backdoor. In this work, we
show that one can reliably recover the backdoor trigger with as few as a single
image. Specifically, we propose the SmoothInv method, which first constructs a
robust smoothed version of the backdoored classifier and then performs guided
image synthesis towards the target class to reveal the backdoor pattern.
SmoothInv requires neither an explicit modeling of the backdoor via a mask
variable, nor any complex regularization schemes, which has become the standard
practice in backdoor inversion methods. We perform both quantitaive and
qualitative study on backdoored classifiers from previous published backdoor
attacks. We demonstrate that compared to existing methods, SmoothInv is able to
recover successful backdoors from single images, while maintaining high
fidelity to the original backdoor. We also show how we identify the target
backdoored class from the backdoored classifier. Last, we propose and analyze
two countermeasures to our approach and show that SmoothInv remains robust in
the face of an adaptive attacker. Our code is available at
https://github.com/locuslab/smoothinv .
</p></li>
</ul>

<h3>Title: Renderable Neural Radiance Map for Visual Navigation. (arXiv:2303.00304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00304">http://arxiv.org/abs/2303.00304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00304] Renderable Neural Radiance Map for Visual Navigation](http://arxiv.org/abs/2303.00304) #robust</code></li>
<li>Summary: <p>We propose a novel type of map for visual navigation, a renderable neural
radiance map (RNR-Map), which is designed to contain the overall visual
information of a 3D environment. The RNR-Map has a grid form and consists of
latent codes at each pixel. These latent codes are embedded from image
observations, and can be converted to the neural radiance field which enables
image rendering given a camera pose. The recorded latent codes implicitly
contain visual information about the environment, which makes the RNR-Map
visually descriptive. This visual information in RNR-Map can be a useful
guideline for visual localization and navigation. We develop localization and
navigation frameworks that can effectively utilize the RNR-Map. We evaluate the
proposed frameworks on camera tracking, visual localization, and image-goal
navigation. Experimental results show that the RNR-Map-based localization
framework can find the target location based on a single query image with fast
speed and competitive accuracy compared to other baselines. Also, this
localization framework is robust to environmental changes, and even finds the
most visually similar places when a query image from a different environment is
given. The proposed navigation framework outperforms the existing image-goal
navigation methods in difficult scenarios, under odometry and actuation noises.
The navigation framework shows 65.7% success rate in curved scenarios of the
NRNS dataset, which is an improvement of 18.6% over the current
state-of-the-art.
</p></li>
</ul>

<h3>Title: RIFT2: Speeding-up RIFT with A New Rotation-Invariance Technique. (arXiv:2303.00319v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00319">http://arxiv.org/abs/2303.00319</a></li>
<li>Code URL: <a href="https://github.com/ljy-rs/rift2-multimodal-matching-rotation">https://github.com/ljy-rs/rift2-multimodal-matching-rotation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00319] RIFT2: Speeding-up RIFT with A New Rotation-Invariance Technique](http://arxiv.org/abs/2303.00319) #robust</code></li>
<li>Summary: <p>Multimodal image matching is an important prerequisite for multisource image
information fusion. Compared with the traditional matching problem, multimodal
feature matching is more challenging due to the severe nonlinear radiation
distortion (NRD). Radiation-variation insensitive feature transform
(RIFT)~\cite{li2019rift} has shown very good robustness to NRD and become a
baseline method in multimodal feature matching. However, the high computational
cost for rotation invariance largely limits its usage in practice. In this
paper, we propose an improved RIFT method, called RIFT2. We develop a new
rotation invariance technique based on dominant index value, which avoids the
construction process of convolution sequence ring. Hence, it can speed up the
running time and reduce the memory consumption of the original RIFT by almost 3
times in theory. Extensive experiments show that RIFT2 achieves similar
matching performance to RIFT while being much faster and having less memory
consumption. The source code will be made publicly available in
\url{https://github.com/LJY-RS/RIFT2-multimodal-matching-rotation}
</p></li>
</ul>

<h3>Title: Empowering Networks With Scale and Rotation Equivariance Using A Similarity Convolution. (arXiv:2303.00326v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00326">http://arxiv.org/abs/2303.00326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00326] Empowering Networks With Scale and Rotation Equivariance Using A Similarity Convolution](http://arxiv.org/abs/2303.00326) #robust</code></li>
<li>Summary: <p>The translational equivariant nature of Convolutional Neural Networks (CNNs)
is a reason for its great success in computer vision. However, networks do not
enjoy more general equivariance properties such as rotation or scaling,
ultimately limiting their generalization performance. To address this
limitation, we devise a method that endows CNNs with simultaneous equivariance
with respect to translation, rotation, and scaling. Our approach defines a
convolution-like operation and ensures equivariance based on our proposed
scalable Fourier-Argand representation. The method maintains similar efficiency
as a traditional network and hardly introduces any additional learnable
parameters, since it does not face the computational issue that often occurs in
group-convolution operators. We validate the efficacy of our approach in the
image classification task, demonstrating its robustness and the generalization
ability to both scaled and rotated inputs.
</p></li>
</ul>

<h3>Title: ORCHNet: A Robust Global Feature Aggregation approach for 3D LiDAR-based Place recognition in Orchards. (arXiv:2303.00477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00477">http://arxiv.org/abs/2303.00477</a></li>
<li>Code URL: <a href="https://github.com/cybonic/orchnet">https://github.com/cybonic/orchnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00477] ORCHNet: A Robust Global Feature Aggregation approach for 3D LiDAR-based Place recognition in Orchards](http://arxiv.org/abs/2303.00477) #robust</code></li>
<li>Summary: <p>Robust and reliable place recognition and loop closure detection in
agricultural environments is still an open problem. In particular, orchards are
a difficult case study due to structural similarity across the entire field. In
this work, we address the place recognition problem in orchards resorting to 3D
LiDAR data, which is considered a key modality for robustness. Hence, we
propose ORCHNet, a deep-learning-based approach that maps 3D-LiDAR scans to
global descriptors. Specifically, this work proposes a new global feature
aggregation approach, which fuses multiple aggregation methods into a robust
global descriptor. ORCHNet is evaluated on real-world data collected in
orchards, comprising data from the summer and autumn seasons. To assess the
robustness, We compare ORCHNet with state-of-the-art aggregation approaches on
data from the same season and across seasons. Moreover, we additionally
evaluate the proposed approach as part of a localization framework, where
ORCHNet is used as a loop closure detector. The empirical results indicate
that, on the place recognition task, ORCHNet outperforms the remaining
approaches, and is also more robust across seasons. As for the localization,
the edge cases where the path goes through the trees are solved when
integrating ORCHNet as a loop detector, showing the potential applicability of
the proposed approach in this task. The code and dataset will be publicly
available at:\url{https://github.com/Cybonic/ORCHNet.git}
</p></li>
</ul>

<h3>Title: Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis. (arXiv:2303.00703v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00703">http://arxiv.org/abs/2303.00703</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00703] Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis](http://arxiv.org/abs/2303.00703) #robust</code></li>
<li>Summary: <p>Performances on standard 3D point cloud benchmarks have plateaued, resulting
in oversized models and complex network design to make a fractional
improvement. We present an alternative to enhance existing deep neural networks
without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter
(SN-Adapter). Building on any trained 3D network, we utilize its learned
encoding capability to extract features of the training dataset and summarize
them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter
retrieves k nearest neighbors (k-NN) from the pre-constructed spatial
prototypes and linearly interpolates the k-NN prediction with that of the
original 3D network. By providing complementary characteristics, the proposed
SN-Adapter serves as a plug-and-play module to economically improve performance
in a non-parametric manner. More importantly, our SN-Adapter can be effectively
generalized to various 3D tasks, including shape classification, part
segmentation, and 3D object detection, demonstrating its superiority and
robustness. We hope our approach could show a new perspective for point cloud
analysis and facilitate future research.
</p></li>
</ul>

<h3>Title: A Complementarity-Based Switch-Fuse System for Improved Visual Place Recognition. (arXiv:2303.00714v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00714">http://arxiv.org/abs/2303.00714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00714] A Complementarity-Based Switch-Fuse System for Improved Visual Place Recognition](http://arxiv.org/abs/2303.00714) #robust</code></li>
<li>Summary: <p>Recently several fusion and switching based approaches have been presented to
solve the problem of Visual Place Recognition. In spite of these systems
demonstrating significant boost in VPR performance they each have their own set
of limitations. The multi-process fusion systems usually involve employing
brute force and running all available VPR techniques simultaneously while the
switching method attempts to negate this practise by only selecting the best
suited VPR technique for given query image. But switching does fail at times
when no available suitable technique can be identified. An innovative solution
would be an amalgamation of the two otherwise discrete approaches to combine
their competitive advantages while negating their shortcomings. The proposed,
Switch-Fuse system, is an interesting way to combine both the robustness of
switching VPR techniques based on complementarity and the force of fusing the
carefully selected techniques to significantly improve performance. Our system
holds a structure superior to the basic fusion methods as instead of simply
fusing all or any random techniques, it is structured to first select the best
possible VPR techniques for fusion, according to the query image. The system
combines two significant processes, switching and fusing VPR techniques, which
together as a hybrid model substantially improve performance on all major VPR
data sets illustrated using PR curves.
</p></li>
</ul>

<h3>Title: S-NeRF: Neural Radiance Fields for Street Views. (arXiv:2303.00749v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00749">http://arxiv.org/abs/2303.00749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00749] S-NeRF: Neural Radiance Fields for Street Views](http://arxiv.org/abs/2303.00749) #robust</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and
scenes, given the object-centric camera views with large overlaps. However, we
conjugate that this paradigm does not fit the nature of the street views that
are collected by many self-driving cars from the large-scale unbounded scenes.
Also, the onboard cameras perceive scenes without much overlapping. Thus,
existing NeRFs often produce blurs, 'floaters' and other artifacts on
street-view synthesis. In this paper, we propose a new street-view NeRF
(S-NeRF) that considers novel view synthesis of both the large-scale background
scenes and the foreground moving vehicles jointly. Specifically, we improve the
scene parameterization function and the camera poses for learning better neural
representations from street views. We also use the the noisy and sparse LiDAR
points to boost the training and learn a robust geometry and reprojection based
confidence to address the depth outliers. Moreover, we extend our S-NeRF for
reconstructing moving vehicles that is impracticable for conventional NeRFs.
Thorough experiments on the large-scale driving datasets (e.g., nuScenes and
Waymo) demonstrate that our method beats the state-of-the-art rivals by
reducing 7% to 40% of the mean-squared error in the street-view synthesis and a
45% PSNR gain for the moving vehicles rendering.
</p></li>
</ul>

<h3>Title: How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks. (arXiv:2303.00293v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00293">http://arxiv.org/abs/2303.00293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00293] How Robust is GPT-3](http://arxiv.org/abs/2303.00293) #robust</code></li>
<li>Summary: <p>The GPT-3.5 models have demonstrated impressive performance in various
Natural Language Processing (NLP) tasks, showcasing their strong understanding
and reasoning capabilities. However, their robustness and abilities to handle
various complexities of the open world have yet to be explored, which is
especially crucial in assessing the stability of models and is a key aspect of
trustworthy AI. In this study, we perform a comprehensive experimental analysis
of GPT-3.5, exploring its robustness using 21 datasets (about 116K test
samples) with 66 text transformations from TextFlint that cover 9 popular
Natural Language Understanding (NLU) tasks. Our findings indicate that while
GPT-3.5 outperforms existing fine-tuned models on some tasks, it still
encounters significant robustness degradation, such as its average performance
dropping by up to 35.74\% and 43.59\% in natural language inference and
sentiment analysis tasks, respectively. We also show that GPT-3.5 faces some
specific robustness challenges, including robustness instability, prompt
sensitivity, and number sensitivity. These insights are valuable for
understanding its limitations and guiding future research in addressing these
challenges to enhance GPT-3.5's overall performance and generalization
abilities.
</p></li>
</ul>

<h3>Title: N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00456">http://arxiv.org/abs/2303.00456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00456] N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space](http://arxiv.org/abs/2303.00456) #robust</code></li>
<li>Summary: <p>Error correction models form an important part of Automatic Speech
Recognition (ASR) post-processing to improve the readability and quality of
transcriptions. Most prior works use the 1-best ASR hypothesis as input and
therefore can only perform correction by leveraging the context within one
sentence. In this work, we propose a novel N-best T5 model for this task, which
is fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By
transferring knowledge from the pre-trained language model and obtaining richer
information from the ASR decoding space, the proposed approach outperforms a
strong Conformer-Transducer baseline. Another issue with standard error
correction is that the generation process is not well-guided. To address this a
constrained decoding process, either based on the N-best list or an ASR
lattice, is used which allows additional information to be propagated.
</p></li>
</ul>

<h3>Title: MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation. (arXiv:2303.00628v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00628">http://arxiv.org/abs/2303.00628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00628] MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation](http://arxiv.org/abs/2303.00628) #robust</code></li>
<li>Summary: <p>We introduce MuAViC, a multilingual audio-visual corpus for robust speech
recognition and robust speech-to-text translation providing 1200 hours of
audio-visual speech in 9 languages. It is fully transcribed and covers 6
English-to-X translation as well as 6 X-to-English translation directions. To
the best of our knowledge, this is the first open benchmark for audio-visual
speech-to-text translation and the largest open benchmark for multilingual
audio-visual speech recognition. Our baseline results show that MuAViC is
effective for building noise-robust speech recognition and translation models.
We make the corpus available at https://github.com/facebookresearch/muavic.
</p></li>
</ul>

<h3>Title: Robustness of edited neural networks. (arXiv:2303.00046v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00046">http://arxiv.org/abs/2303.00046</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00046] Robustness of edited neural networks](http://arxiv.org/abs/2303.00046) #robust</code></li>
<li>Summary: <p>Successful deployment in uncertain, real-world environments requires that
deep learning models can be efficiently and reliably modified in order to adapt
to unexpected issues. However, the current trend toward ever-larger models
makes standard retraining procedures an ever-more expensive burden. For this
reason, there is growing interest in model editing, which enables
computationally inexpensive, interpretable, post-hoc model modifications. While
many model editing techniques are promising, research on the properties of
edited models is largely limited to evaluation of validation accuracy. The
robustness of edited models is an important and yet mostly unexplored topic. In
this paper, we employ recently developed techniques from the field of deep
learning robustness to investigate both how model editing affects the general
robustness of a model, as well as the robustness of the specific behavior
targeted by the edit. We find that edits tend to reduce general robustness, but
that the degree of degradation depends on the editing algorithm chosen. In
particular, robustness is best preserved by more constrained techniques that
modify less of the model. Motivated by these observations, we introduce two new
model editing algorithms, direct low-rank model editing and 1-layer
interpolation (1-LI), which each exhibit strong generalization performance.
</p></li>
</ul>

<h3>Title: Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks. (arXiv:2303.00196v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00196">http://arxiv.org/abs/2303.00196</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00196] Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks](http://arxiv.org/abs/2303.00196) #robust</code></li>
<li>Summary: <p>Achieving efficient and robust multi-channel data learning is a challenging
task in data science. By exploiting low-rankness in the transformed domain,
i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has
achieved extensive success in multi-channel data representation and has
recently been extended to function representation such as Neural Networks with
t-product layers (t-NNs). However, it still remains unclear how t-SVD
theoretically affects the learning behavior of t-NNs. This paper is the first
to answer this question by deriving the upper bounds of the generalization
error of both standard and adversarially trained t-NNs. It reveals that the
t-NNs compressed by exact transformed low-rank parameterization can achieve a
sharper adversarial generalization bound. In practice, although t-NNs rarely
have exactly transformed low-rank weights, our analysis further shows that by
adversarial training with gradient flow (GF), the over-parameterized t-NNs with
ReLU activations are trained with implicit regularization towards transformed
low-rank parameterization under certain conditions. We also establish
adversarial generalization bounds for t-NNs with approximately transformed
low-rank weights. Our analysis indicates that the transformed low-rank
parameterization can promisingly enhance robust generalization for t-NNs.
</p></li>
</ul>

<h3>Title: Combating Exacerbated Heterogeneity for Robust Models in Federated Learning. (arXiv:2303.00250v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00250">http://arxiv.org/abs/2303.00250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00250] Combating Exacerbated Heterogeneity for Robust Models in Federated Learning](http://arxiv.org/abs/2303.00250) #robust</code></li>
<li>Summary: <p>Privacy and security concerns in real-world applications have led to the
development of adversarially robust federated models. However, the
straightforward combination between adversarial training and federated learning
in one framework can lead to the undesired robustness deterioration. We
discover that the attribution behind this phenomenon is that the generated
adversarial data could exacerbate the data heterogeneity among local clients,
making the wrapped federated learning perform poorly. To deal with this
problem, we propose a novel framework called Slack Federated Adversarial
Training (SFAT), assigning the client-wise slack during aggregation to combat
the intensified heterogeneity. Theoretically, we analyze the convergence of the
proposed method to properly relax the objective when combining federated
learning and adversarial training. Experimentally, we verify the rationality
and effectiveness of SFAT on various benchmarked and real-world datasets with
different adversarial training and federated optimization methods. The code is
publicly available at https://github.com/ZFancy/SFAT.
</p></li>
</ul>

<h3>Title: Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization. (arXiv:2303.00442v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00442">http://arxiv.org/abs/2303.00442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00442] Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization](http://arxiv.org/abs/2303.00442) #robust</code></li>
<li>Summary: <p>Many existing group fairness-aware training methods aim to achieve the group
fairness by either re-weighting underrepresented groups based on certain rules
or using weakly approximated surrogates for the fairness metrics in the
objective as regularization terms. Although each of the learning schemes has
its own strength in terms of applicability or performance, respectively, it is
difficult for any method in the either category to be considered as a gold
standard since their successful performances are typically limited to specific
cases. To that end, we propose a principled method, dubbed as \ours, which
unifies the two learning schemes by incorporating a well-justified group
fairness metric into the training objective using a class wise distributionally
robust optimization (DRO) framework. We then develop an iterative optimization
algorithm that minimizes the resulting objective by automatically producing the
correct re-weights for each group. Our experiments show that FairDRO is
scalable and easily adaptable to diverse applications, and consistently
achieves the state-of-the-art performance on several benchmark datasets in
terms of the accuracy-fairness trade-off, compared to recent strong baselines.
</p></li>
</ul>

<h3>Title: Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science. (arXiv:2303.00652v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00652">http://arxiv.org/abs/2303.00652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00652] Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science](http://arxiv.org/abs/2303.00652) #robust</code></li>
<li>Summary: <p>Explainable artificial intelligence (XAI) methods shed light on the
predictions of deep neural networks (DNNs). Several different approaches exist
and have partly already been successfully applied in climate science. However,
the often missing ground truth explanations complicate their evaluation and
validation, subsequently compounding the choice of the XAI method. Therefore,
in this work, we introduce XAI evaluation in the context of climate research
and assess different desired explanation properties, namely, robustness,
faithfulness, randomization, complexity, and localization. To this end we build
upon previous work and train a multi-layer perceptron (MLP) and a convolutional
neural network (CNN) to predict the decade based on annual-mean temperature
maps. Next, multiple local XAI methods are applied and their performance is
quantified for each evaluation property and compared against a baseline test.
Independent of the network type, we find that the XAI methods Integrated
Gradients, Layer-wise relevance propagation, and InputGradients exhibit
considerable robustness, faithfulness, and complexity while sacrificing
randomization. The opposite is true for Gradient, SmoothGrad, NoiseGrad, and
FusionGrad. Notably, explanations using input perturbations, such as SmoothGrad
and Integrated Gradients, do not improve robustness and faithfulness, contrary
to previous claims. Overall, our experiments offer a comprehensive overview of
different properties of explanation methods in the climate science context and
supports users in the selection of a suitable XAI method.
</p></li>
</ul>

<h3>Title: Cross-Modal Entity Matching for Visually Rich Documents. (arXiv:2303.00720v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00720">http://arxiv.org/abs/2303.00720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00720] Cross-Modal Entity Matching for Visually Rich Documents](http://arxiv.org/abs/2303.00720) #robust</code></li>
<li>Summary: <p>Visually rich documents (VRD) are physical/digital documents that utilize
visual cues to augment their semantics. The information contained in these
documents are often incomplete. Existing works that enable automated querying
on VRDs do not take this aspect into account. Consequently, they support a
limited set of queries. In this paper, we describe Juno -- a multimodal
framework that identifies a set of tuples from a relational database to augment
an incomplete VRD with supplementary information. Our main contribution in this
is an end-to-end-trainable neural network with bi-directional attention that
executes this cross-modal entity matching task without any prior knowledge
about the document type or the underlying database-schema. Exhaustive
experiments on two heteroegeneous datasets show that Juno outperforms
state-of-the-art baselines by more than 6% in F1-score, while reducing the
amount of human-effort in its workflow by more than 80%. To the best of our
knowledge, ours is the first work that investigates the incompleteness of VRDs
and proposes a robust framework to address it in a seamless way.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment. (arXiv:2303.00491v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00491">http://arxiv.org/abs/2303.00491</a></li>
<li>Code URL: <a href="https://github.com/datasciencegrimmer/syn-yawpitch">https://github.com/datasciencegrimmer/syn-yawpitch</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00491] Pose Impact Estimation on Face Recognition using 3D-Aware Synthetic Data with Application to Quality Assessment](http://arxiv.org/abs/2303.00491) #biometric</code></li>
<li>Summary: <p>Evaluating the quality of facial images is essential for operating face
recognition systems with sufficient accuracy. The recent advances in face
quality standardisation (ISO/IEC WD 29794-5) recommend the usage of component
quality measures for breaking down face quality into its individual factors,
hence providing valuable feedback for operators to re-capture low-quality
images. In light of recent advances in 3D-aware generative adversarial
networks, we propose a novel dataset, "Syn-YawPitch", comprising 1,000
identities with varying yaw-pitch angle combinations. Utilizing this dataset,
we demonstrate that pitch angles beyond 30 degrees have a significant impact on
the biometric performance of current face recognition systems. Furthermore, we
propose a lightweight and efficient pose quality predictor that adheres to the
standards of ISO/IEC WD 29794-5 and is freely available for use at
https://github.com/datasciencegrimmer/Syn-YawPitch/.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: DMSA: Dynamic Multi-scale Unsupervised Semantic Segmentation Based on Adaptive Affinity. (arXiv:2303.00199v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00199">http://arxiv.org/abs/2303.00199</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00199] DMSA: Dynamic Multi-scale Unsupervised Semantic Segmentation Based on Adaptive Affinity](http://arxiv.org/abs/2303.00199) #extraction</code></li>
<li>Summary: <p>The proposed method in this paper proposes an end-to-end unsupervised
semantic segmentation architecture DMSA based on four loss functions. The
framework uses Atrous Spatial Pyramid Pooling (ASPP) module to enhance feature
extraction. At the same time, a dynamic dilation strategy is designed to better
capture multi-scale context information. Secondly, a Pixel-Adaptive Refinement
(PAR) module is introduced, which can adaptively refine the initial pseudo
labels after feature fusion to obtain high quality pseudo labels. Experiments
show that the proposed DSMA framework is superior to the existing methods on
the saliency dataset. On the COCO 80 dataset, the MIoU is improved by 2.0, and
the accuracy is improved by 5.39. On the Pascal VOC 2012 Augmented dataset, the
MIoU is improved by 4.9, and the accuracy is improved by 3.4. In addition, the
convergence speed of the model is also greatly improved after the introduction
of the PAR module.
</p></li>
</ul>

<h3>Title: StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training. (arXiv:2303.00289v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00289">http://arxiv.org/abs/2303.00289</a></li>
<li>Code URL: <a href="https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT/v2">https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT/v2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00289] StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training](http://arxiv.org/abs/2303.00289) #extraction</code></li>
<li>Summary: <p>In this paper, we present StrucTexTv2, an effective document image
pre-training framework, by performing masked visual-textual prediction. It
consists of two self-supervised pre-training tasks: masked image modeling and
masked language modeling, based on text region-level image masking. The
proposed method randomly masks some image regions according to the bounding box
coordinates of text words. The objectives of our pre-training tasks are
reconstructing the pixels of masked image regions and the corresponding masked
tokens simultaneously. Hence the pre-trained encoder can capture more textual
semantics in comparison to the masked image modeling that usually predicts the
masked image patches. Compared to the masked multi-modal modeling methods for
document image understanding that rely on both the image and text modalities,
StrucTexTv2 models image-only input and potentially deals with more application
scenarios free from OCR pre-processing. Extensive experiments on mainstream
benchmarks of document image understanding demonstrate the effectiveness of
StrucTexTv2. It achieves competitive or even new state-of-the-art performance
in various downstream tasks such as image classification, layout analysis,
table structure recognition, document OCR, and information extraction under the
end-to-end scenario.
</p></li>
</ul>

<h3>Title: BiSVP: Building Footprint Extraction via Bidirectional Serialized Vertex Prediction. (arXiv:2303.00300v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00300">http://arxiv.org/abs/2303.00300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00300] BiSVP: Building Footprint Extraction via Bidirectional Serialized Vertex Prediction](http://arxiv.org/abs/2303.00300) #extraction</code></li>
<li>Summary: <p>Extracting building footprints from remote sensing images has been attracting
extensive attention recently. Dominant approaches address this challenging
problem by generating vectorized building masks with cumbersome refinement
stages, which limits the application of such methods. In this paper, we
introduce a new refinement-free and end-to-end building footprint extraction
method, which is conceptually intuitive, simple, and effective. Our method,
termed as BiSVP, represents a building instance with ordered vertices and
formulates the building footprint extraction as predicting the serialized
vertices directly in a bidirectional fashion. Moreover, we propose a
cross-scale feature fusion (CSFF) module to facilitate high resolution and rich
semantic feature learning, which is essential for the dense building vertex
prediction task. Without bells and whistles, our BiSVP outperforms
state-of-the-art methods by considerable margins on three building instance
segmentation benchmarks, clearly demonstrating its superiority. The code and
datasets will be made public available.
</p></li>
</ul>

<h3>Title: Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation. (arXiv:2303.00440v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00440">http://arxiv.org/abs/2303.00440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00440] Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation](http://arxiv.org/abs/2303.00440) #extraction</code></li>
<li>Summary: <p>Effectively extracting inter-frame motion and appearance information is
important for video frame interpolation (VFI). Previous works either extract
both types of information in a mixed way or elaborate separate modules for each
type of information, which lead to representation ambiguity and low efficiency.
In this paper, we propose a novel module to explicitly extract motion and
appearance information via a unifying operation. Specifically, we rethink the
information process in inter-frame attention and reuse its attention map for
both appearance feature enhancement and motion information extraction.
Furthermore, for efficient VFI, our proposed module could be seamlessly
integrated into a hybrid CNN and Transformer architecture. This hybrid pipeline
can alleviate the computational complexity of inter-frame attention as well as
preserve detailed low-level structure information. Experimental results
demonstrate that, for both fixed- and arbitrary-timestep interpolation, our
method achieves state-of-the-art performance on various datasets. Meanwhile,
our approach enjoys a lighter computation overhead over models with close
performance. The source code and models are available at
https://github.com/MCG-NJU/EMA-VFI.
</p></li>
</ul>

<h3>Title: Parameter Optimization of LLC-Converter with multiple operation points using Reinforcement Learning. (arXiv:2303.00004v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00004">http://arxiv.org/abs/2303.00004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00004] Parameter Optimization of LLC-Converter with multiple operation points using Reinforcement Learning](http://arxiv.org/abs/2303.00004) #extraction</code></li>
<li>Summary: <p>The optimization of electrical circuits is a difficult and time-consuming
process performed by experts, but also increasingly by sophisticated
algorithms. In this paper, a reinforcement learning (RL) approach is adapted to
optimize a LLC converter at multiple operation points corresponding to
different output powers at high converter efficiency at different switching
frequencies. During a training period, the RL agent learns a problem specific
optimization policy enabling optimizations for any objective and boundary
condition within a pre-defined range. The results show, that the trained RL
agent is able to solve new optimization problems based on LLC converter
simulations using Fundamental Harmonic Approximation (FHA) within 50 tuning
steps for two operation points with power efficiencies greater than 90%.
Therefore, this AI technique provides the potential to augment expert-driven
design processes with data-driven strategy extraction in the field of power
electronics and beyond.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Mitigating Backdoors in Federated Learning with FLD. (arXiv:2303.00302v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00302">http://arxiv.org/abs/2303.00302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00302] Mitigating Backdoors in Federated Learning with FLD](http://arxiv.org/abs/2303.00302) #federate</code></li>
<li>Summary: <p>Federated learning allows clients to collaboratively train a global model
without uploading raw data for privacy preservation. This feature, i.e., the
inability to review participants' datasets, has recently been found responsible
for federated learning's vulnerability in the face of backdoor attacks.
Existing defense methods fall short from two perspectives: 1) they consider
only very specific and limited attacker models and unable to cope with advanced
backdoor attacks, such as distributed backdoor attacks, which break down the
global trigger into multiple distributed triggers. 2) they conduct detection
based on model granularity thus the performance gets impacted by the model
dimension. To address these challenges, we propose Federated Layer Detection
(FLD), a novel model filtering approach for effectively defending against
backdoor attacks. FLD examines the models based on layer granularity to capture
the complete model details and effectively detect potential backdoor models
regardless of model dimension. We provide theoretical analysis and proof for
the convergence of FLD. Extensive experiments demonstrate that FLD effectively
mitigates state-of-the-art backdoor attacks with negligible impact on the
accuracy of the primary task.
</p></li>
</ul>

<h3>Title: Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices. (arXiv:2303.00492v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00492">http://arxiv.org/abs/2303.00492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00492] Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices](http://arxiv.org/abs/2303.00492) #federate</code></li>
<li>Summary: <p>Graph neural networks (GNN) have been widely deployed in real-world networked
applications and systems due to their capability to handle graph-structured
data. However, the growing awareness of data privacy severely challenges the
traditional centralized model training paradigm, where a server holds all the
graph information. Federated learning is an emerging collaborative computing
paradigm that allows model training without data centralization. Existing
federated GNN studies mainly focus on systems where clients hold distinctive
graphs or sub-graphs. The practical node-level federated situation, where each
client is only aware of its direct neighbors, has yet to be studied. In this
paper, we propose the first federated GNN framework called Lumos that supports
supervised and unsupervised learning with feature and degree protection on
node-level federated graphs. We first design a tree constructor to improve the
representation capability given the limited structural information. We further
present a Monte Carlo Markov Chain-based algorithm to mitigate the workload
imbalance caused by degree heterogeneity with theoretically-guaranteed
performance. Based on the constructed tree for each client, a decentralized
tree-based GNN trainer is proposed to support versatile training. Extensive
experiments demonstrate that Lumos outperforms the baseline with significantly
higher accuracy and greatly reduced communication cost and training time.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals. (arXiv:2303.00500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00500">http://arxiv.org/abs/2303.00500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00500] Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals](http://arxiv.org/abs/2303.00500) #interpretability</code></li>
<li>Summary: <p>Interpretability is essential for machine learning algorithms in high-stakes
application fields such as medical image analysis. However, high-performing
black-box neural networks do not provide explanations for their predictions,
which can lead to mistrust and suboptimal human-ML collaboration. Post-hoc
explanation techniques, which are widely used in practice, have been shown to
suffer from severe conceptual problems. Furthermore, as we show in this paper,
current explanation techniques do not perform adequately in the multi-label
scenario, in which multiple medical findings may co-occur in a single image. We
propose Attri-Net, an inherently interpretable model for multi-label
classification. Attri-Net is a powerful classifier that provides transparent,
trustworthy, and human-understandable explanations. The model first generates
class-specific attribution maps based on counterfactuals to identify which
image regions correspond to certain medical findings. Then a simple logistic
regression classifier is used to make predictions based solely on these
attribution maps. We compare Attri-Net to five post-hoc explanation techniques
and one inherently interpretable classifier on three chest X-ray datasets. We
find that Attri-Net produces high-quality multi-label explanations consistent
with clinical knowledge and has comparable classification performance to
state-of-the-art classification models.
</p></li>
</ul>

<h3>Title: Neural Nonnegative Matrix Factorization for Hierarchical Multilayer Topic Modeling. (arXiv:2303.00058v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00058">http://arxiv.org/abs/2303.00058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00058] Neural Nonnegative Matrix Factorization for Hierarchical Multilayer Topic Modeling](http://arxiv.org/abs/2303.00058) #interpretability</code></li>
<li>Summary: <p>We introduce a new method based on nonnegative matrix factorization, Neural
NMF, for detecting latent hierarchical structure in data. Datasets with
hierarchical structure arise in a wide variety of fields, such as document
classification, image processing, and bioinformatics. Neural NMF recursively
applies NMF in layers to discover overarching topics encompassing the
lower-level features. We derive a backpropagation optimization scheme that
allows us to frame hierarchical NMF as a neural network. We test Neural NMF on
a synthetic hierarchical dataset, the 20 Newsgroups dataset, and the MyLymeData
symptoms dataset. Numerical results demonstrate that Neural NMF outperforms
other hierarchical NMF methods on these data sets and offers better learned
hierarchical structure and interpretability of topics.
</p></li>
</ul>

<h3>Title: Interpretable Transformer for Water Level Forecasting. (arXiv:2303.00515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00515">http://arxiv.org/abs/2303.00515</a></li>
<li>Code URL: <a href="https://github.com/chulhongsung/wltran">https://github.com/chulhongsung/wltran</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00515] Interpretable Transformer for Water Level Forecasting](http://arxiv.org/abs/2303.00515) #interpretability</code></li>
<li>Summary: <p>Forecasting the water level of the Han river is important to control the
traffic and avoid natural disasters. There are many variables related to the
Han river and they are intricately connected. In this work, we propose a novel
transformer that exploits the causal relationship based on the prior knowledge
among the variables and forecasts the four bridges of the Han river: Cheongdam,
Jamsu, Hangang, and Haengju. Our proposed model considers both spatial and
temporal causation by formalizing the causal structure as a multilayer network
and using masking methods. Due to this approach, we can have the
interpretability that consistent with prior knowledge. Additionally, we propose
a novel recalibration method and loss function for high accuracy of extreme
risk in time series. In real data analysis, we use the Han river dataset from
2016 to 2021, and compare the proposed model with deep learning models.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Diffusion Probabilistic Fields. (arXiv:2303.00165v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00165">http://arxiv.org/abs/2303.00165</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00165] Diffusion Probabilistic Fields](http://arxiv.org/abs/2303.00165) #diffusion</code></li>
<li>Summary: <p>Diffusion probabilistic models have quickly become a major approach for
generative modeling of images, 3D geometry, video and other domains. However,
to adapt diffusion generative modeling to these domains the denoising network
needs to be carefully designed for each domain independently, oftentimes under
the assumption that data lives in a Euclidean grid. In this paper we introduce
Diffusion Probabilistic Fields (DPF), a diffusion model that can learn
distributions over continuous functions defined over metric spaces, commonly
known as fields. We extend the formulation of diffusion probabilistic models to
deal with this field parametrization in an explicit way, enabling us to define
an end-to-end learning algorithm that side-steps the requirement of
representing fields with latent vectors as in previous approaches (Dupont et
al., 2022a; Du et al., 2021). We empirically show that, while using the same
denoising network, DPF effectively deals with different modalities like 2D
images and 3D geometry, in addition to modeling distributions over fields
defined on non-Euclidean metric spaces.
</p></li>
</ul>

<h3>Title: Collage Diffusion. (arXiv:2303.00262v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00262">http://arxiv.org/abs/2303.00262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00262] Collage Diffusion](http://arxiv.org/abs/2303.00262) #diffusion</code></li>
<li>Summary: <p>Text-conditional diffusion models generate high-quality, diverse images.
However, text is often an ambiguous specification for a desired target image,
creating the need for additional user-friendly controls for diffusion-based
image generation. We focus on having precise control over image output for
scenes with several objects. Users control image generation by defining a
collage: a text prompt paired with an ordered sequence of layers, where each
layer is an RGBA image and a corresponding text prompt. We introduce Collage
Diffusion, a collage-conditional diffusion algorithm that allows users to
control both the spatial arrangement and visual attributes of objects in the
scene, and also enables users to edit individual components of generated
images. To ensure that different parts of the input text correspond to the
various locations specified in the input collage layers, Collage Diffusion
modifies text-image cross-attention with the layers' alpha masks. To maintain
characteristics of individual collage layers that are not specified in text,
Collage Diffusion learns specialized text representations per layer. Collage
input also enables layer-based controls that provide fine-grained control over
the final output: users can control image harmonization on a layer-by-layer
basis, and they can edit individual objects in generated images while keeping
other objects fixed. Collage-conditional image generation requires harmonizing
the input collage to make objects fit together--the key challenge involves
minimizing changes in the positions and key visual attributes of objects in the
input collage while allowing other attributes of the collage to change in the
harmonization process. By leveraging the rich information present in layer
input, Collage Diffusion generates globally harmonized images that maintain
desired object locations and visual characteristics better than prior
approaches.
</p></li>
</ul>

<h3>Title: Unlimited-Size Diffusion Restoration. (arXiv:2303.00354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00354">http://arxiv.org/abs/2303.00354</a></li>
<li>Code URL: <a href="https://github.com/wyhuai/ddnm">https://github.com/wyhuai/ddnm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00354] Unlimited-Size Diffusion Restoration](http://arxiv.org/abs/2303.00354) #diffusion</code></li>
<li>Summary: <p>Recently, using diffusion models for zero-shot image restoration (IR) has
become a new hot paradigm. This type of method only needs to use the
pre-trained off-the-shelf diffusion models, without any finetuning, and can
directly handle various IR tasks. The upper limit of the restoration
performance depends on the pre-trained diffusion models, which are in rapid
evolution. However, current methods only discuss how to deal with fixed-size
images, but dealing with images of arbitrary sizes is very important for
practical applications. This paper focuses on how to use those diffusion-based
zero-shot IR methods to deal with any size while maintaining the excellent
characteristics of zero-shot. A simple way to solve arbitrary size is to divide
it into fixed-size patches and solve each patch independently. But this may
yield significant artifacts since it neither considers the global semantics of
all patches nor the local information of adjacent patches. Inspired by the
Range-Null space Decomposition, we propose the Mask-Shift Restoration to
address local incoherence and propose the Hierarchical Restoration to alleviate
out-of-domain issues. Our simple, parameter-free approaches can be used not
only for image restoration but also for image generation of unlimited sizes,
with the potential to be a general tool for diffusion models. Code:
https://github.com/wyhuai/DDNM/tree/main/hq_demo
</p></li>
</ul>

<h3>Title: Level Up the Deepfake Detection: a Method to Effectively Discriminate Images Generated by GAN Architectures and Diffusion Models. (arXiv:2303.00608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00608">http://arxiv.org/abs/2303.00608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00608] Level Up the Deepfake Detection: a Method to Effectively Discriminate Images Generated by GAN Architectures and Diffusion Models](http://arxiv.org/abs/2303.00608) #diffusion</code></li>
<li>Summary: <p>The image deepfake detection task has been greatly addressed by the
scientific community to discriminate real images from those generated by
Artificial Intelligence (AI) models: a binary classification task. In this
work, the deepfake detection and recognition task was investigated by
collecting a dedicated dataset of pristine images and fake ones generated by 9
different Generative Adversarial Network (GAN) architectures and by 4
additional Diffusion Models (DM). A hierarchical multi-level approach was then
introduced to solve three different deepfake detection and recognition tasks:
(i) Real Vs AI generated; (ii) GANs Vs DMs; (iii) AI specific architecture
recognition. Experimental results demonstrated, in each case, more than 97%
classification accuracy, outperforming state-of-the-art methods.
</p></li>
</ul>

<h3>Title: StraIT: Non-autoregressive Generation with Stratified Image Transformer. (arXiv:2303.00750v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.00750">http://arxiv.org/abs/2303.00750</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.00750] StraIT: Non-autoregressive Generation with Stratified Image Transformer](http://arxiv.org/abs/2303.00750) #diffusion</code></li>
<li>Summary: <p>We propose Stratified Image Transformer(StraIT), a pure
non-autoregressive(NAR) generative model that demonstrates superiority in
high-quality image synthesis over existing autoregressive(AR) and diffusion
models(DMs). In contrast to the under-exploitation of visual characteristics in
existing vision tokenizer, we leverage the hierarchical nature of images to
encode visual tokens into stratified levels with emergent properties. Through
the proposed image stratification that obtains an interlinked token pair, we
alleviate the modeling difficulty and lift the generative power of NAR models.
Our experiments demonstrate that StraIT significantly improves NAR generation
and out-performs existing DMs and AR methods while being order-of-magnitude
faster, achieving FID scores of 3.96 at 256*256 resolution on ImageNet without
leveraging any guidance in sampling or auxiliary image classifiers. When
equipped with classifier-free guidance, our method achieves an FID of 3.36 and
IS of 259.3. In addition, we illustrate the decoupled modeling process of
StraIT generation, showing its compelling properties on applications including
domain transfer.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
