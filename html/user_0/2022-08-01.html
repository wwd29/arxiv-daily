<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Training a universal instance segmentation network for live cell images of various cell types and imaging modalities. (arXiv:2207.14347v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14347">http://arxiv.org/abs/2207.14347</a></li>
<li>Code URL: <a href="https://github.com/westgate458/xb-net">https://github.com/westgate458/xb-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14347] Training a universal instance segmentation network for live cell images of various cell types and imaging modalities](http://arxiv.org/abs/2207.14347)</code></li>
<li>Summary: <p>We share our recent findings in an attempt to train a universal segmentation
network for various cell types and imaging modalities. Our method was built on
the generalized U-Net architecture, which allows the evaluation of each
component individually. We modified the traditional binary training targets to
include three classes for direct instance segmentation. Detailed experiments
were performed regarding training schemes, training settings, network
backbones, and individual modules on the segmentation performance. Our proposed
training scheme draws minibatches in turn from each dataset, and the gradients
are accumulated before an optimization step. We found that the key to training
a universal network is all-time supervision on all datasets, and it is
necessary to sample each dataset in an unbiased way. Our experiments also
suggest that there might exist common features to define cell boundaries across
cell types and imaging modalities, which could allow application of trained
models to totally unseen datasets. A few training tricks can further boost the
segmentation performance, including uneven class weights in the cross-entropy
loss function, well-designed learning rate scheduler, larger image crops for
contextual information, and additional loss terms for unbalanced classes. We
also found that segmentation performance can benefit from group normalization
layer and Atrous Spatial Pyramid Pooling module, thanks to their more reliable
statistics estimation and improved semantic understanding, respectively. We
participated in the 6th Cell Tracking Challenge (CTC) held at IEEE
International Symposium on Biomedical Imaging (ISBI) 2021 using one of the
developed variants. Our method was evaluated as the best runner up during the
initial submission for the primary track, and also secured the 3rd place in an
additional round of competition in preparation for the summary publication.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Effectiveness of Transformer Models on IoT Security Detection in StackOverflow Discussions. (arXiv:2207.14542v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14542">http://arxiv.org/abs/2207.14542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14542] Effectiveness of Transformer Models on IoT Security Detection in StackOverflow Discussions](http://arxiv.org/abs/2207.14542)</code></li>
<li>Summary: <p>The Internet of Things (IoT) is an emerging concept that directly links to
the billions of physical items, or "things", that are connected to the Internet
and are all gathering and exchanging information between devices and systems.
However, IoT devices were not built with security in mind, which might lead to
security vulnerabilities in a multi-device system. Traditionally, we
investigated IoT issues by polling IoT developers and specialists. This
technique, however, is not scalable since surveying all IoT developers is not
feasible. Another way to look into IoT issues is to look at IoT developer
discussions on major online development forums like Stack Overflow (SO).
However, finding discussions that are relevant to IoT issues is challenging
since they are frequently not categorized with IoT-related terms. In this
paper, we present the "IoT Security Dataset", a domain-specific dataset of 7147
samples focused solely on IoT security discussions. As there are no automated
tools to label these samples, we manually labeled them. We further employed
multiple transformer models to automatically detect security discussions.
Through rigorous investigations, we found that IoT security discussions are
different and more complex than traditional security discussions. We
demonstrated a considerable performance loss (up to 44%) of transformer models
on cross-domain datasets when we transferred knowledge from a general-purpose
dataset "Opiner", supporting our claim. Thus, we built a domain-specific IoT
security detector with an F1-Score of 0.69. We have made the dataset public in
the hope that developers would learn more about the security discussion and
vendors would enhance their concerns about product security.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Content-Aware Differential Privacy with Conditional Invertible Neural Networks. (arXiv:2207.14625v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14625">http://arxiv.org/abs/2207.14625</a></li>
<li>Code URL: <a href="https://github.com/cardio-ai/cadp">https://github.com/cardio-ai/cadp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14625] Content-Aware Differential Privacy with Conditional Invertible Neural Networks](http://arxiv.org/abs/2207.14625)</code></li>
<li>Summary: <p>Differential privacy (DP) has arisen as the gold standard in protecting an
individual's privacy in datasets by adding calibrated noise to each data
sample. While the application to categorical data is straightforward, its
usability in the context of images has been limited. Contrary to categorical
data the meaning of an image is inherent in the spatial correlation of
neighboring pixels making the simple application of noise infeasible.
Invertible Neural Networks (INN) have shown excellent generative performance
while still providing the ability to quantify the exact likelihood. Their
principle is based on transforming a complicated distribution into a simple one
e.g. an image into a spherical Gaussian. We hypothesize that adding noise to
the latent space of an INN can enable differentially private image
modification. Manipulation of the latent space leads to a modified image while
preserving important details. Further, by conditioning the INN on meta-data
provided with the dataset we aim at leaving dimensions important for downstream
tasks like classification untouched while altering other parts that potentially
contain identifying information. We term our method content-aware differential
privacy (CADP). We conduct experiments on publicly available benchmarking
datasets as well as dedicated medical ones. In addition, we show the
generalizability of our method to categorical data. The source code is publicly
available at https://github.com/Cardio-AI/CADP.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h2>robust</h2>
<h3>Title: Inverse Reinforcement Learning from Diverse Third-Person Videos via Graph Abstraction. (arXiv:2207.14299v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14299">http://arxiv.org/abs/2207.14299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14299] Inverse Reinforcement Learning from Diverse Third-Person Videos via Graph Abstraction](http://arxiv.org/abs/2207.14299)</code></li>
<li>Summary: <p>Research on Inverse Reinforcement Learning (IRL) from third-person videos has
shown encouraging results on removing the need for manual reward design for
robotic tasks. However, most prior works are still limited by training from a
relatively restricted domain of videos. In this paper, we argue that the true
potential of third-person IRL lies in increasing the diversity of videos for
better scaling. To learn a reward function from diverse videos, we propose to
perform graph abstraction on the videos followed by temporal matching in the
graph space to measure the task progress. Our insight is that a task can be
described by entity interactions that form a graph, and this graph abstraction
can help remove irrelevant information such as textures, resulting in more
robust reward functions. We evaluate our approach, GraphIRL, on
cross-embodiment learning in X-MAGICAL and learning from human demonstrations
for real-robot manipulation. We show significant improvements in robustness to
diverse video demonstrations over previous approaches, and even achieve better
results than manual reward design on a real robot pushing task. Videos are
available at https://sateeshkumar21.github.io/GraphIRL .
</p></li>
</ul>

<h3>Title: Pro-tuning: Unified Prompt Tuning for Vision Tasks. (arXiv:2207.14381v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14381">http://arxiv.org/abs/2207.14381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14381] Pro-tuning: Unified Prompt Tuning for Vision Tasks](http://arxiv.org/abs/2207.14381)</code></li>
<li>Summary: <p>In computer vision, fine-tuning is the de-facto approach to leverage
pre-trained vision models to perform downstream tasks. However, deploying it in
practice is quite challenging, due to adopting parameter inefficient global
update and heavily relying on high-quality downstream data. Recently,
prompt-based learning, which adds a task-relevant prompt to adapt the
downstream tasks to pre-trained models, has drastically boosted the performance
of many natural language downstream tasks. In this work, we extend this notable
transfer ability benefited from prompt into vision models as an alternative to
fine-tuning. To this end, we propose parameter-efficient Prompt tuning
(Pro-tuning) to adapt frozen vision models to various downstream vision tasks.
The key to Pro-tuning is prompt-based tuning, i.e., learning task-specific
vision prompts for downstream input images with the pre-trained model frozen.
By only training a few additional parameters, it can work on diverse CNN-based
and Transformer-based architectures. Extensive experiments evidence that
Pro-tuning outperforms fine-tuning in a broad range of vision tasks and
scenarios, including image classification (generic objects, class imbalance,
image corruption, adversarial robustness, and out-of-distribution
generalization), and dense prediction tasks such as object detection and
semantic segmentation.
</p></li>
</ul>

<h3>Title: Neural Density-Distance Fields. (arXiv:2207.14455v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14455">http://arxiv.org/abs/2207.14455</a></li>
<li>Code URL: <a href="https://github.com/ueda0319/neddf">https://github.com/ueda0319/neddf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14455] Neural Density-Distance Fields](http://arxiv.org/abs/2207.14455)</code></li>
<li>Summary: <p>The success of neural fields for 3D vision tasks is now indisputable.
Following this trend, several methods aiming for visual localization (e.g.,
SLAM) have been proposed to estimate distance or density fields using neural
fields. However, it is difficult to achieve high localization performance by
only density fields-based methods such as Neural Radiance Field (NeRF) since
they do not provide density gradient in most empty regions. On the other hand,
distance field-based methods such as Neural Implicit Surface (NeuS) have
limitations in objects' surface shapes. This paper proposes Neural
Density-Distance Field (NeDDF), a novel 3D representation that reciprocally
constrains the distance and density fields. We extend distance field
formulation to shapes with no explicit boundary surface, such as fur or smoke,
which enable explicit conversion from distance field to density field.
Consistent distance and density fields realized by explicit conversion enable
both robustness to initial values and high-quality registration. Furthermore,
the consistency between fields allows fast convergence from sparse point
clouds. Experiments show that NeDDF can achieve high localization performance
while providing comparable results to NeRF on novel view synthesis. The code is
available at https://github.com/ueda0319/neddf.
</p></li>
</ul>

<h3>Title: Towards Domain-agnostic Depth Completion. (arXiv:2207.14466v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14466">http://arxiv.org/abs/2207.14466</a></li>
<li>Code URL: <a href="https://github.com/yvanyin/filldepth">https://github.com/yvanyin/filldepth</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14466] Towards Domain-agnostic Depth Completion](http://arxiv.org/abs/2207.14466)</code></li>
<li>Summary: <p>Existing depth completion methods are often targeted at a specific sparse
depth type, and generalize poorly across task domains. We present a method to
complete sparse/semi-dense, noisy, and potentially low-resolution depth maps
obtained by various range sensors, including those in modern mobile phones, or
by multi-view reconstruction algorithms. Our method leverages a data driven
prior in the form of a single image depth prediction network trained on
large-scale datasets, the output of which is used as an input to our model. We
propose an effective training scheme where we simulate various sparsity
patterns in typical task domains. In addition, we design two new benchmarks to
evaluate the generalizability and the robustness of depth completion methods.
Our simple method shows superior cross-domain generalization ability against
state-of-the-art depth completion methods, introducing a practical solution to
high quality depth capture on a mobile device. Code is available at:
https://github.com/YvanYin/FillDepth.
</p></li>
</ul>

<h3>Title: Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings. (arXiv:2207.14539v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14539">http://arxiv.org/abs/2207.14539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14539] Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings](http://arxiv.org/abs/2207.14539)</code></li>
<li>Summary: <p>Pre-training trajectory embeddings is a fundamental and critical procedure in
spatial-temporal trajectory mining, and is beneficial for a wide range of
downstream tasks. The key for generating effective trajectory embeddings is to
extract high-level travel semantics from trajectories, including movement
patterns and travel purposes, with consideration of the trajectories' long-term
spatial-temporal correlations. Despite the existing efforts, there are still
major challenges in pre-training trajectory embeddings. First, commonly used
generative pretext tasks are not suitable for extracting high-level semantics
from trajectories. Second, existing data augmentation methods fit badly on
trajectory datasets. Third, current encoder designs fail to fully incorporate
long-term spatial-temporal correlations hidden in trajectories. To tackle these
challenges, we propose a novel Contrastive Spatial-Temporal Trajectory
Embedding (CSTTE) model for learning comprehensive trajectory embeddings. CSTTE
adopts the contrastive learning framework so that its pretext task is robust to
noise. A specially designed data augmentation method for trajectories is
coupled with the contrastive pretext task to preserve the high-level travel
semantics. We also build an efficient spatial-temporal trajectory encoder to
efficiently and comprehensively model the long-term spatial-temporal
correlations in trajectories. Extensive experiments on two downstream tasks and
three real-world datasets prove the superiority of our model compared with the
existing trajectory embedding methods.
</p></li>
</ul>

<h3>Title: Prompting for Multi-Modal Tracking. (arXiv:2207.14571v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14571">http://arxiv.org/abs/2207.14571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14571] Prompting for Multi-Modal Tracking](http://arxiv.org/abs/2207.14571)</code></li>
<li>Summary: <p>Multi-modal tracking gains attention due to its ability to be more accurate
and robust in complex scenarios compared to traditional RGB-based tracking. Its
key lies in how to fuse multi-modal data and reduce the gap between modalities.
However, multi-modal tracking still severely suffers from data deficiency, thus
resulting in the insufficient learning of fusion modules. Instead of building
such a fusion module, in this paper, we provide a new perspective on
multi-modal tracking by attaching importance to the multi-modal visual prompts.
We design a novel multi-modal prompt tracker (ProTrack), which can transfer the
multi-modal inputs to a single modality by the prompt paradigm. By best
employing the tracking ability of pre-trained RGB trackers learning at scale,
our ProTrack can achieve high-performance multi-modal tracking by only altering
the inputs, even without any extra training on multi-modal data. Extensive
experiments on 5 benchmark datasets demonstrate the effectiveness of the
proposed ProTrack.
</p></li>
</ul>

<h3>Title: High Dynamic Range and Super-Resolution from Raw Image Bursts. (arXiv:2207.14671v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14671">http://arxiv.org/abs/2207.14671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14671] High Dynamic Range and Super-Resolution from Raw Image Bursts](http://arxiv.org/abs/2207.14671)</code></li>
<li>Summary: <p>Photographs captured by smartphones and mid-range cameras have limited
spatial resolution and dynamic range, with noisy response in underexposed
regions and color artefacts in saturated areas. This paper introduces the first
approach (to the best of our knowledge) to the reconstruction of
high-resolution, high-dynamic range color images from raw photographic bursts
captured by a handheld camera with exposure bracketing. This method uses a
physically-accurate model of image formation to combine an iterative
optimization algorithm for solving the corresponding inverse problem with a
learned image representation for robust alignment and a learned natural image
prior. The proposed algorithm is fast, with low memory requirements compared to
state-of-the-art learning-based approaches to image restoration, and features
that are learned end to end from synthetic yet realistic data. Extensive
experiments demonstrate its excellent performance with super-resolution factors
of up to $\times 4$ on real photographs taken in the wild with hand-held
cameras, and high robustness to low-light conditions, noise, camera shake, and
moderate object motion.
</p></li>
</ul>

<h3>Title: PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition. (arXiv:2207.14807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14807">http://arxiv.org/abs/2207.14807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14807] PageNet: Towards End-to-End Weakly Supervised Page-Level Handwritten Chinese Text Recognition](http://arxiv.org/abs/2207.14807)</code></li>
<li>Summary: <p>Handwritten Chinese text recognition (HCTR) has been an active research topic
for decades. However, most previous studies solely focus on the recognition of
cropped text line images, ignoring the error caused by text line detection in
real-world applications. Although some approaches aimed at page-level text
recognition have been proposed in recent years, they either are limited to
simple layouts or require very detailed annotations including expensive
line-level and even character-level bounding boxes. To this end, we propose
PageNet for end-to-end weakly supervised page-level HCTR. PageNet detects and
recognizes characters and predicts the reading order between them, which is
more robust and flexible when dealing with complex layouts including
multi-directional and curved text lines. Utilizing the proposed weakly
supervised learning framework, PageNet requires only transcripts to be
annotated for real data; however, it can still output detection and recognition
results at both the character and line levels, avoiding the labor and cost of
labeling bounding boxes of characters and text lines. Extensive experiments
conducted on five datasets demonstrate the superiority of PageNet over existing
weakly supervised and fully supervised page-level methods. These experimental
results may spark further research beyond the realms of existing methods based
on connectionist temporal classification or attention. The source code is
available at https://github.com/shannanyinxiang/PageNet.
</p></li>
</ul>

<h3>Title: Domain Specific Wav2vec 2.0 Fine-tuning For The SE&amp;R 2022 Challenge. (arXiv:2207.14418v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14418">http://arxiv.org/abs/2207.14418</a></li>
<li>Code URL: <a href="https://github.com/alefiury/se-r_2022_challenge_wav2vec2">https://github.com/alefiury/se-r_2022_challenge_wav2vec2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14418] Domain Specific Wav2vec 2](http://arxiv.org/abs/2207.14418)</code></li>
<li>Summary: <p>This paper presents our efforts to build a robust ASR model for the shared
task Automatic Speech Recognition for spontaneous and prepared speech &amp; Speech
Emotion Recognition in Portuguese (SE&amp;R 2022). The goal of the challenge is to
advance the ASR research for the Portuguese language, considering prepared and
spontaneous speech in different dialects. Our method consist on fine-tuning an
ASR model in a domain-specific approach, applying gain normalization and
selective noise insertion. The proposed method improved over the strong
baseline provided on the test set in 3 of the 4 tracks available
</p></li>
</ul>

<h3>Title: Design Methodology for Deep Out-of-Distribution Detectors in Real-Time Cyber-Physical Systems. (arXiv:2207.14694v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14694">http://arxiv.org/abs/2207.14694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14694] Design Methodology for Deep Out-of-Distribution Detectors in Real-Time Cyber-Physical Systems](http://arxiv.org/abs/2207.14694)</code></li>
<li>Summary: <p>When machine learning (ML) models are supplied with data outside their
training distribution, they are more likely to make inaccurate predictions; in
a cyber-physical system (CPS), this could lead to catastrophic system failure.
To mitigate this risk, an out-of-distribution (OOD) detector can run in
parallel with an ML model and flag inputs that could lead to undesirable
outcomes. Although OOD detectors have been well studied in terms of accuracy,
there has been less focus on deployment to resource constrained CPSs. In this
study, a design methodology is proposed to tune deep OOD detectors to meet the
accuracy and response time requirements of embedded applications. The
methodology uses genetic algorithms to optimize the detector's preprocessing
pipeline and selects a quantization method that balances robustness and
response time. It also identifies several candidate task graphs under the Robot
Operating System (ROS) for deployment of the selected design. The methodology
is demonstrated on two variational autoencoder based OOD detectors from the
literature on two embedded platforms. Insights into the trade-offs that occur
during the design process are provided, and it is shown that this design
methodology can lead to a drastic reduction in response time in relation to an
unoptimized OOD detector while maintaining comparable accuracy.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion. (arXiv:2207.14782v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14782">http://arxiv.org/abs/2207.14782</a></li>
<li>Code URL: <a href="https://github.com/low5545/minimal-neural-atlas">https://github.com/low5545/minimal-neural-atlas</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14782] Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion](http://arxiv.org/abs/2207.14782)</code></li>
<li>Summary: <p>Explicit neural surface representations allow for exact and efficient
extraction of the encoded surface at arbitrary precision, as well as analytic
derivation of differential geometric properties such as surface normal and
curvature. Such desirable properties, which are absent in its implicit
counterpart, makes it ideal for various applications in computer vision,
graphics and robotics. However, SOTA works are limited in terms of the topology
it can effectively describe, distortion it introduces to reconstruct complex
surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a
novel atlas-based explicit neural surface representation. At its core is a
fully learnable parametric domain, given by an implicit probabilistic occupancy
field defined on an open square of the parametric space. In contrast, prior
works generally predefine the parametric domain. The added flexibility enables
charts to admit arbitrary topology and boundary. Thus, our representation can
learn a minimal atlas of 3 charts with distortion-minimal parameterization for
surfaces of arbitrary topology, including closed and open surfaces with
arbitrary connected components. Our experiments support the hypotheses and show
that our reconstructions are more accurate in terms of the overall geometry,
due to the separation of concerns on topology and geometry.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Towards Communication-efficient Vertical Federated Learning Training via Cache-enabled Local Updates. (arXiv:2207.14628v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14628">http://arxiv.org/abs/2207.14628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14628] Towards Communication-efficient Vertical Federated Learning Training via Cache-enabled Local Updates](http://arxiv.org/abs/2207.14628)</code></li>
<li>Summary: <p>Vertical federated learning (VFL) is an emerging paradigm that allows
different parties (e.g., organizations or enterprises) to collaboratively build
machine learning models with privacy protection. In the training phase, VFL
only exchanges the intermediate statistics, i.e., forward activations and
backward derivatives, across parties to compute model gradients. Nevertheless,
due to its geo-distributed nature, VFL training usually suffers from the low
WAN bandwidth.
</p></li>
</ul>

<p>In this paper, we introduce CELU-VFL, a novel and efficient VFL training
framework that exploits the local update technique to reduce the cross-party
communication rounds. CELU-VFL caches the stale statistics and reuses them to
estimate model gradients without exchanging the ad hoc statistics. Significant
techniques are proposed to improve the convergence performance. First, to
handle the stochastic variance problem, we propose a uniform sampling strategy
to fairly choose the stale statistics for local updates. Second, to harness the
errors brought by the staleness, we devise an instance weighting mechanism that
measures the reliability of the estimated gradients. Theoretical analysis
proves that CELU-VFL achieves a similar sub-linear convergence rate as vanilla
VFL training but requires much fewer communication rounds. Empirical results on
both public and real-world workloads validate that CELU-VFL can be up to six
times faster than the existing works.
</p>

<h2>fair</h2>
<h3>Title: Multiple Attribute Fairness: Application to Fraud Detection. (arXiv:2207.14355v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2207.14355">http://arxiv.org/abs/2207.14355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2207.14355] Multiple Attribute Fairness: Application to Fraud Detection](http://arxiv.org/abs/2207.14355)</code></li>
<li>Summary: <p>We propose a fairness measure relaxing the equality conditions in the popular
equal odds fairness regime for classification. We design an iterative,
model-agnostic, grid-based heuristic that calibrates the outcomes per sensitive
attribute value to conform to the measure. The heuristic is designed to handle
high arity attribute values and performs a per attribute sanitization of
outcomes across different protected attribute values. We also extend our
heuristic for multiple attributes. Highlighting our motivating application,
fraud detection, we show that the proposed heuristic is able to achieve
fairness across multiple values of a single protected attribute, multiple
protected attributes. When compared to current fairness techniques, that focus
on two groups, we achieve comparable performance across several public data
sets.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
