<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Efficient Vertical Federated Learning with Secure Aggregation. (arXiv:2305.11236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11236">http://arxiv.org/abs/2305.11236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11236] Efficient Vertical Federated Learning with Secure Aggregation](http://arxiv.org/abs/2305.11236) #secure</code></li>
<li>Summary: <p>The majority of work in privacy-preserving federated learning (FL) has been
focusing on horizontally partitioned datasets where clients share the same sets
of features and can train complete models independently. However, in many
interesting problems, such as financial fraud detection and disease detection,
individual data points are scattered across different clients/organizations in
vertical federated learning. Solutions for this type of FL require the exchange
of gradients between participants and rarely consider privacy and security
concerns, posing a potential risk of privacy leakage. In this work, we present
a novel design for training vertical FL securely and efficiently using
state-of-the-art security modules for secure aggregation. We demonstrate
empirically that our method does not impact training performance whilst
obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).
</p></li>
</ul>

<h3>Title: Must the Communication Graph of MPC Protocols be an Expander?. (arXiv:2305.11428v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11428">http://arxiv.org/abs/2305.11428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11428] Must the Communication Graph of MPC Protocols be an Expander?](http://arxiv.org/abs/2305.11428) #secure</code></li>
<li>Summary: <p>Secure multiparty computation (MPC) on incomplete communication networks has
been studied within two primary models: (1) Where a partial network is fixed a
priori, and thus corruptions can occur dependent on its structure, and (2)
Where edges in the communication graph are determined dynamically as part of
the protocol. Whereas a rich literature has succeeded in mapping out the
feasibility and limitations of graph structures supporting secure computation
in the fixed-graph model (including strong classical lower bounds), these
bounds do not apply in the latter dynamic-graph setting, which has recently
seen exciting new results, but remains relatively unexplored.
</p></li>
</ul>

<p>In this work, we initiate a similar foundational study of MPC within the
dynamic-graph model. As a first step, we investigate the property of graph
expansion. All existing protocols (implicitly or explicitly) yield
communication graphs which are expanders, but it is not clear whether this is
inherent. Our results consist of two types (for constant fraction of
corruptions):
</p>
<p>* Upper bounds: We demonstrate secure protocols whose induced communication
graphs are not expander graphs, within a wide range of settings (computational,
information theoretic, with low locality, even with low locality and adaptive
security), each assuming some form of input-independent setup.
</p>
<p>* Lower bounds: In the plain model (no setup) with adaptive corruptions, we
demonstrate that for certain functionalities, no protocol can maintain a
non-expanding communication graph against all adversarial strategies. Our lower
bound relies only on protocol correctness (not privacy), and requires a
surprisingly delicate argument.
</p>
<p>More generally, we provide a formal framework for analyzing the evolving
communication graph of MPC protocols, giving a starting point for studying the
relation between secure computation and further, more general graph properties.
</p>

<h2>security</h2>
<h3>Title: Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping. (arXiv:2305.11423v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11423">http://arxiv.org/abs/2305.11423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11423] Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping](http://arxiv.org/abs/2305.11423) #security</code></li>
<li>Summary: <p>Homomorphic encryption (HE) enables computations on encrypted data by
concealing information under noise for security. However, the process of
bootstrapping, which resets the noise level in the ciphertext, is
computationally expensive and requires a large bootstrapping key. The TFHE
scheme offers a faster and programmable bootstrapping algorithm called PBS,
crucial for security-focused applications like machine learning. Nevertheless,
the current TFHE scheme lacks support for ciphertext packing, resulting in low
throughput. This work thoroughly analyzes TFHE bootstrapping, identifies the
bottleneck in GPUs caused by the blind rotation fragmentation problem, and
proposes a hardware TFHE accelerator called Strix. Strix introduces a two-level
batching approach to enhance the batch size in PBS, utilizes a specialized
microarchitecture for efficient streaming data processing, and incorporates a
fully-pipelined FFT microarchitecture to improve performance. It achieves
significantly higher throughput than state-of-the-art implementations on both
CPUs and GPUs, outperforming existing TFHE accelerators by a factor of 7.4.
</p></li>
</ul>

<h3>Title: Chrowned by an Extension: Abusing the Chrome DevTools Protocol through the Debugger API. (arXiv:2305.11506v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11506">http://arxiv.org/abs/2305.11506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11506] Chrowned by an Extension: Abusing the Chrome DevTools Protocol through the Debugger API](http://arxiv.org/abs/2305.11506) #security</code></li>
<li>Summary: <p>The Chromium open-source project has become a fundamental piece of the Web as
we know it today, with multiple vendors offering browsers based on its
codebase. One of its most popular features is the possibility of altering or
enhancing the browser functionality through third-party programs known as
browser extensions. Extensions have access to a wide range of capabilities
through the use of APIs exposed by Chromium. The Debugger API -- arguably the
most powerful of such APIs -- allows extensions to use the Chrome DevTools
Protocol (CDP), a capability-rich tool for debugging and instrumenting the
browser. In this paper, we describe several vulnerabilities present in the
Debugger API and in the granting of capabilities to extensions that can be used
by an attacker to take control of the browser, escalate privileges, and break
context isolation. We demonstrate their impact by introducing six attacks that
allow an attacker to steal user information, monitor network traffic, modify
site permissions (\eg access to camera or microphone), bypass security
interstitials without user intervention, and change the browser settings. Our
attacks work in all major Chromium-based browsers as they are rooted at the
core of the Chromium project. We reported our findings to the Chromium
Development Team, who already fixed some of them and are currently working on
fixing the remaining ones. We conclude by discussing how questionable design
decisions, lack of public specifications, and an overpowered Debugger API have
contributed to enabling these attacks, and propose mitigations.
</p></li>
</ul>

<h3>Title: Lifting Network Protocol Implementation to Precise Format Specification with Security Applications. (arXiv:2305.11781v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11781">http://arxiv.org/abs/2305.11781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11781] Lifting Network Protocol Implementation to Precise Format Specification with Security Applications](http://arxiv.org/abs/2305.11781) #security</code></li>
<li>Summary: <p>Inferring protocol formats is critical for many security applications.
However, existing format-inference techniques often miss many formats, because
almost all of them are in a fashion of dynamic analysis and rely on a limited
number of network packets to drive their analysis. If a feature is not present
in the input packets, the feature will be missed in the resulting formats. We
develop a novel static program analysis for format inference. It is well-known
that static analysis does not rely on any input packets and can achieve high
coverage by scanning every piece of code. However, for efficiency and
precision, we have to address two challenges, namely path explosion and
disordered path constraints. To this end, our approach uses abstract
interpretation to produce a novel data structure called the abstract format
graph. It delimits precise but costly operations to only small regions, thus
ensuring precision and efficiency at the same time. Our inferred formats are of
high coverage and precisely specify both field boundaries and semantic
constraints among packet fields. Our evaluation shows that we can infer formats
for a protocol in one minute with >95% precision and recall, much better than
four baseline techniques. Our inferred formats can substantially enhance
existing protocol fuzzers, improving the coverage by 20% to 260% and
discovering 53 zero-days with 47 assigned CVEs. We also provide case studies of
adopting our inferred formats in other security applications including traffic
auditing and intrusion detection.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: TPMDP: Threshold Personalized Multi-party Differential Privacy via Optimal Gaussian Mechanism. (arXiv:2305.11192v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11192">http://arxiv.org/abs/2305.11192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11192] TPMDP: Threshold Personalized Multi-party Differential Privacy via Optimal Gaussian Mechanism](http://arxiv.org/abs/2305.11192) #privacy</code></li>
<li>Summary: <p>In modern distributed computing applications, such as federated learning and
AIoT systems, protecting privacy is crucial to prevent misbehaving parties from
colluding to steal others' private information. However, guaranteeing the
utility of computation outcomes while protecting all parties' privacy can be
challenging, particularly when the parties' privacy requirements are highly
heterogeneous. In this paper, we propose a novel privacy framework for
multi-party computation called Threshold Personalized Multi-party Differential
Privacy (TPMDP), which addresses a limited number of semi-honest colluding
adversaries. Our framework enables each party to have a personalized privacy
budget. We design a multi-party Gaussian mechanism that is easy to implement
and satisfies TPMDP, wherein each party perturbs the computation outcome in a
secure multi-party computation protocol using Gaussian noise. To optimize the
utility of the mechanism, we cast the utility loss minimization problem into a
linear programming (LP) problem. We exploit the specific structure of this LP
problem to compute the optimal solution after O(n) computations, where n is the
number of parties, while a generic solver may require exponentially many
computations. Extensive experiments demonstrate the benefits of our approach in
terms of low utility loss and high efficiency compared to existing private
mechanisms that do not consider personalized privacy requirements or collusion
thresholds.
</p></li>
</ul>

<h3>Title: Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11414">http://arxiv.org/abs/2305.11414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11414] Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models](http://arxiv.org/abs/2305.11414) #privacy</code></li>
<li>Summary: <p>Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated
remarkable success in a wide range of applications, driven by their ability to
leverage vast amounts of data for pre-training. However, optimizing FMs often
requires access to sensitive data, raising privacy concerns and limiting their
applicability in certain domains. In this paper, we introduce the concept of
Federated Foundation Models (FFMs), a novel approach that combines the benefits
of FMs and Federated Learning (FL) to enable privacy-preserving and
collaborative learning across multiple institutions. We discuss the potential
benefits and challenges of integrating FL into the lifespan of FMs, covering
pre-training, fine-tuning, and application. We further provide formal
definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and
federated prompt engineering, allowing for more personalized and context-aware
models while maintaining data privacy. Moreover, we explore the possibility of
continual/lifelong learning in FFMs, as increased computational power at the
edge unlocks the potential for optimizing FMs using newly generated private
data at edges. We present experiments and evaluations comparing the performance
of FFMs to traditional FMs on various downstream tasks, demonstrating the
effectiveness of our approach in preserving privacy, reducing overfitting, and
improving model generalizability. The proposed Federated Foundation Models
offer a flexible and scalable framework for training large language models in a
privacy-preserving manner, paving the way for future advancements in both FM
pre-training and federated learning.
</p></li>
</ul>

<h3>Title: A Path to Holistic Privacy in Stream Processing Systems. (arXiv:2305.11638v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11638">http://arxiv.org/abs/2305.11638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11638] A Path to Holistic Privacy in Stream Processing Systems](http://arxiv.org/abs/2305.11638) #privacy</code></li>
<li>Summary: <p>The massive streams of Internet of Things (IoT) data require a timely
analysis to retain data usefulness. Stream processing systems (SPSs) enable
this task, deriving knowledge from the IoT data in real-time. Such real-time
analytics benefits many applications but can also be used to violate user
privacy, as the IoT data collected from users or their vicinity is inherently
sensitive. In this paper, we present our systematic look into privacy issues
arising from the intersection of SPSs and IoT, identifying key research
challenges towards achieving holistic privacy protection in SPSs and proposing
the solutions.
</p></li>
</ul>

<h3>Title: Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence. (arXiv:2305.11420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11420">http://arxiv.org/abs/2305.11420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11420] Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence](http://arxiv.org/abs/2305.11420) #privacy</code></li>
<li>Summary: <p>Decentralized learning has recently been attracting increasing attention for
its applications in parallel computation and privacy preservation. Many recent
studies stated that the underlying network topology with a faster consensus
rate (a.k.a. spectral gap) leads to a better convergence rate and accuracy for
decentralized learning. However, a topology with a fast consensus rate, e.g.,
the exponential graph, generally has a large maximum degree, which incurs
significant communication costs. Thus, seeking topologies with both a fast
consensus rate and small maximum degree is important. In this study, we propose
a novel topology combining both a fast consensus rate and small maximum degree
called the Base-$(k + 1)$ Graph. Unlike the existing topologies, the Base-$(k +
1)$ Graph enables all nodes to reach the exact consensus after a finite number
of iterations for any number of nodes and maximum degree k. Thanks to this
favorable property, the Base-$(k + 1)$ Graph endows Decentralized SGD (DSGD)
with both a faster convergence rate and more communication efficiency than the
exponential graph. We conducted experiments with various topologies,
demonstrating that the Base-$(k + 1)$ Graph enables various decentralized
learning methods to achieve higher accuracy with better communication
efficiency than the existing topologies.
</p></li>
</ul>

<h3>Title: PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy. (arXiv:2305.11437v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11437">http://arxiv.org/abs/2305.11437</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11437] PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy](http://arxiv.org/abs/2305.11437) #privacy</code></li>
<li>Summary: <p>Federated Learning (FL) has emerged as an effective learning paradigm for
distributed computation owing to its strong potential in capturing underlying
data statistics while preserving data privacy. However, in cases of practical
data heterogeneity among FL clients, existing FL frameworks still exhibit
deficiency in capturing the overall feature properties of local client data
that exhibit disparate distributions. In response, generative adversarial
networks (GANs) have recently been exploited in FL to address data
heterogeneity since GANs can be integrated for data regeneration without
exposing original raw data. Despite some successes, existing GAN-related FL
frameworks often incur heavy communication cost and also elicit other privacy
concerns, which limit their applications in real scenarios. To this end, this
work proposes a novel FL framework that requires only partial GAN model
sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and
training mechanism to address heterogeneous data distributions across clients
and to strengthen privacy preservation at reduced communication cost,
especially over wireless networks. Our analysis demonstrates the convergence
and privacy benefits of the proposed PS-FEdGAN framework. Through experimental
results based on several well-known benchmark datasets, our proposed PS-FedGAN
shows great promise to tackle FL under non-IID client data distributions, while
securing data privacy and lowering communication overhead.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Towards Generalizable Data Protection With Transferable Unlearnable Examples. (arXiv:2305.11191v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11191">http://arxiv.org/abs/2305.11191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11191] Towards Generalizable Data Protection With Transferable Unlearnable Examples](http://arxiv.org/abs/2305.11191) #protect</code></li>
<li>Summary: <p>Artificial Intelligence (AI) is making a profound impact in almost every
domain. One of the crucial factors contributing to this success has been the
access to an abundance of high-quality data for constructing machine learning
models. Lately, as the role of data in artificial intelligence has been
significantly magnified, concerns have arisen regarding the secure utilization
of data, particularly in the context of unauthorized data usage. To mitigate
data exploitation, data unlearning have been introduced to render data
unexploitable. However, current unlearnable examples lack the generalization
required for wide applicability. In this paper, we present a novel,
generalizable data protection method by generating transferable unlearnable
examples. To the best of our knowledge, this is the first solution that
examines data privacy from the perspective of data distribution. Through
extensive experimentation, we substantiate the enhanced generalizable
protection capabilities of our proposed method.
</p></li>
</ul>

<h3>Title: Comparing Biases and the Impact of Multilingual Training across Multiple Languages. (arXiv:2305.11242v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11242">http://arxiv.org/abs/2305.11242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11242] Comparing Biases and the Impact of Multilingual Training across Multiple Languages](http://arxiv.org/abs/2305.11242) #protect</code></li>
<li>Summary: <p>Studies in bias and fairness in natural language processing have primarily
examined social biases within a single language and/or across few attributes
(e.g. gender, race). However, biases can manifest differently across various
languages for individual attributes. As a result, it is critical to examine
biases within each language and attribute. Of equal importance is to study how
these biases compare across languages and how the biases are affected when
training a model on multilingual data versus monolingual data. We present a
bias analysis across Italian, Chinese, English, Hebrew, and Spanish on the
downstream sentiment analysis task to observe whether specific demographics are
viewed more positively. We study bias similarities and differences across these
languages and investigate the impact of multilingual vs. monolingual training
data. We adapt existing sentiment bias templates in English to Italian,
Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality,
and gender. Our results reveal similarities in bias expression such as
favoritism of groups that are dominant in each language's culture (e.g.
majority religions and nationalities). Additionally, we find an increased
variation in predictions across protected groups, indicating bias
amplification, after multilingual finetuning in comparison to multilingual
pretraining.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Few-Shot Learning with Visual Distribution Calibration and Cross-Modal Distribution Alignment. (arXiv:2305.11439v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11439">http://arxiv.org/abs/2305.11439</a></li>
<li>Code URL: <a href="https://github.com/bhrqw/sada">https://github.com/bhrqw/sada</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11439] Few-Shot Learning with Visual Distribution Calibration and Cross-Modal Distribution Alignment](http://arxiv.org/abs/2305.11439) #attack</code></li>
<li>Summary: <p>Pre-trained vision-language models have inspired much research on few-shot
learning. However, with only a few training images, there exist two crucial
problems: (1) the visual feature distributions are easily distracted by
class-irrelevant information in images, and (2) the alignment between the
visual and language feature distributions is difficult. To deal with the
distraction problem, we propose a Selective Attack module, which consists of
trainable adapters that generate spatial attention maps of images to guide the
attacks on class-irrelevant image areas. By messing up these areas, the
critical features are captured and the visual distributions of image features
are calibrated. To better align the visual and language feature distributions
that describe the same object class, we propose a cross-modal distribution
alignment module, in which we introduce a vision-language prototype for each
class to align the distributions, and adopt the Earth Mover's Distance (EMD) to
optimize the prototypes. For efficient computation, the upper bound of EMD is
derived. In addition, we propose an augmentation strategy to increase the
diversity of the images and the text prompts, which can reduce overfitting to
the few-shot training images. Extensive experiments on 11 datasets demonstrate
that our method consistently outperforms prior arts in few-shot learning. The
implementation code will be available at https://github.com/bhrqw/SADA.
</p></li>
</ul>

<h3>Title: DAP: A Dynamic Adversarial Patch for Evading Person Detectors. (arXiv:2305.11618v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11618">http://arxiv.org/abs/2305.11618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11618] DAP: A Dynamic Adversarial Patch for Evading Person Detectors](http://arxiv.org/abs/2305.11618) #attack</code></li>
<li>Summary: <p>In this paper, we present a novel approach for generating naturalistic
adversarial patches without using GANs. Our proposed approach generates a
Dynamic Adversarial Patch (DAP) that looks naturalistic while maintaining high
attack efficiency and robustness in real-world scenarios. To achieve this, we
redefine the optimization problem by introducing a new objective function,
where a similarity metric is used to construct a similarity loss. This guides
the patch to follow predefined patterns while maximizing the victim model's
loss function. Our technique is based on directly modifying the pixel values in
the patch which gives higher flexibility and larger space to incorporate
multiple transformations compared to the GAN-based techniques. Furthermore,
most clothing-based physical attacks assume static objects and ignore the
possible transformations caused by non-rigid deformation due to changes in a
person's pose. To address this limitation, we incorporate a ``Creases
Transformation'' (CT) block, i.e., a preprocessing block following an
Expectation Over Transformation (EOT) block used to generate a large variation
of transformed patches incorporated in the training process to increase its
robustness to different possible real-world distortions (e.g., creases in the
clothing, rotation, re-scaling, random noise, brightness and contrast
variations, etc.). We demonstrate that the presence of different real-world
variations in clothing and object poses (i.e., above-mentioned distortions)
lead to a drop in the performance of state-of-the-art attacks. For instance,
these techniques can merely achieve 20\% in the physical world and 30.8\% in
the digital world while our attack provides superior success rate of up to 65\%
and 84.56\%, respectively when attacking the YOLOv3tiny detector deployed in
smart cameras at the edge.
</p></li>
</ul>

<h3>Title: Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation. (arXiv:2305.11596v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11596">http://arxiv.org/abs/2305.11596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11596] Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation](http://arxiv.org/abs/2305.11596) #attack</code></li>
<li>Summary: <p>Modern NLP models are often trained over large untrusted datasets, raising
the potential for a malicious adversary to compromise model behaviour. For
instance, backdoors can be implanted through crafting training instances with a
specific textual trigger and a target label. This paper posits that backdoor
poisoning attacks exhibit spurious correlation between simple text features and
classification labels, and accordingly, proposes methods for mitigating
spurious correlation as means of defence. Our empirical study reveals that the
malicious triggers are highly correlated to their target labels; therefore such
correlations are extremely distinguishable compared to those scores of benign
features, and can be used to filter out potentially problematic instances.
Compared with several existing defences, our defence method significantly
reduces attack success rates across backdoor attacks, and in the case of
insertion based attacks, our method provides a near-perfect defence.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Vanishing Activations: A Symptom of Deep Capsule Networks. (arXiv:2305.11178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11178">http://arxiv.org/abs/2305.11178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11178] Vanishing Activations: A Symptom of Deep Capsule Networks](http://arxiv.org/abs/2305.11178) #robust</code></li>
<li>Summary: <p>Capsule Networks, an extension to Neural Networks utilizing vector or matrix
representations instead of scalars, were initially developed to create a
dynamic parse tree where visual concepts evolve from parts to complete objects.
Early implementations of Capsule Networks achieved and maintain
state-of-the-art results on various datasets. However, recent studies have
revealed shortcomings in the original Capsule Network architecture, notably its
failure to construct a parse tree and its susceptibility to vanishing gradients
when deployed in deeper networks. This paper extends the investigation to a
range of leading Capsule Network architectures, demonstrating that these issues
are not confined to the original design. We argue that the majority of Capsule
Network research has produced architectures that, while modestly divergent from
the original Capsule Network, still retain a fundamentally similar structure.
We posit that this inherent design similarity might be impeding the scalability
of Capsule Networks. Our study contributes to the broader discussion on
improving the robustness and scalability of Capsule Networks.
</p></li>
</ul>

<h3>Title: Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning. (arXiv:2305.11347v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11347">http://arxiv.org/abs/2305.11347</a></li>
<li>Code URL: <a href="https://github.com/hendrycks/robustness">https://github.com/hendrycks/robustness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11347] Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning](http://arxiv.org/abs/2305.11347) #robust</code></li>
<li>Summary: <p>In overhead image segmentation tasks, including additional spectral bands
beyond the traditional RGB channels can improve model performance. However, it
is still unclear how incorporating this additional data impacts model
robustness to adversarial attacks and natural perturbations. For adversarial
robustness, the additional information could improve the model's ability to
distinguish malicious inputs, or simply provide new attack avenues and
vulnerabilities. For natural perturbations, the additional information could
better inform model decisions and weaken perturbation effects or have no
significant influence at all. In this work, we seek to characterize the
performance and robustness of a multispectral (RGB and near infrared) image
segmentation model subjected to adversarial attacks and natural perturbations.
While existing adversarial and natural robustness research has focused
primarily on digital perturbations, we prioritize on creating realistic
perturbations designed with physical world conditions in mind. For adversarial
robustness, we focus on data poisoning attacks whereas for natural robustness,
we focus on extending ImageNet-C common corruptions for fog and snow that
coherently and self-consistently perturbs the input data. Overall, we find both
RGB and multispectral models are vulnerable to data poisoning attacks
regardless of input or fusion architectures and that while physically
realizable natural perturbations still degrade model performance, the impact
differs based on fusion architecture and input data.
</p></li>
</ul>

<h3>Title: Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition. (arXiv:2305.11367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11367">http://arxiv.org/abs/2305.11367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11367] Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition](http://arxiv.org/abs/2305.11367) #robust</code></li>
<li>Summary: <p>With the emphasis on healthcare, early childhood education, and fitness,
non-invasive measurement and recognition methods have received more attention.
Pressure sensing has been extensively studied due to its advantages of simple
structure, easy access, visualization application, and harmlessness. This paper
introduces a smart pressure e-mat (SPeM) system based on a piezoresistive
material Velostat for human monitoring applications, including sleeping
postures, sports, and yoga recognition. After a subsystem scans e-mat readings
and processes the signal, it generates a pressure image stream. Deep neural
networks (DNNs) are used to fit and train the pressure image stream and
recognize the corresponding human behavior. Four sleeping postures and five
dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are
used as a preliminary validation of the proposed SPeM system. The SPeM system
achieves high accuracies on both applications, which demonstrates the high
accuracy and generalization ability of the models. Compared with other pressure
sensor-based systems, SPeM possesses more flexible applications and commercial
application prospects, with reliable, robust, and repeatable properties.
</p></li>
</ul>

<h3>Title: DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment. (arXiv:2305.11522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11522">http://arxiv.org/abs/2305.11522</a></li>
<li>Code URL: <a href="https://github.com/lhyfst/dsfnet">https://github.com/lhyfst/dsfnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11522] DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment](http://arxiv.org/abs/2305.11522) #robust</code></li>
<li>Summary: <p>Sensitivity to severe occlusion and large view angles limits the usage
scenarios of the existing monocular 3D dense face alignment methods. The
state-of-the-art 3DMM-based method, directly regresses the model's
coefficients, underutilizing the low-level 2D spatial and semantic information,
which can actually offer cues for face shape and orientation. In this work, we
demonstrate how modeling 3D facial geometry in image and model space jointly
can solve the occlusion and view angle problems. Instead of predicting the
whole face directly, we regress image space features in the visible facial
region by dense prediction first. Subsequently, we predict our model's
coefficients based on the regressed feature of the visible regions, leveraging
the prior knowledge of whole face geometry from the morphable models to
complete the invisible regions. We further propose a fusion network that
combines the advantages of both the image and model space predictions to
achieve high robustness and accuracy in unconstrained scenarios. Thanks to the
proposed fusion module, our method is robust not only to occlusion and large
pitch and roll view angles, which is the benefit of our image space approach,
but also to noise and large yaw angles, which is the benefit of our model space
method. Comprehensive evaluations demonstrate the superior performance of our
method compared with the state-of-the-art methods. On the 3D dense face
alignment task, we achieve 3.80% NME on the AFLW2000-3D dataset, which
outperforms the state-of-the-art method by 5.5%. Code is available at
https://github.com/lhyfst/DSFNet.
</p></li>
</ul>

<h3>Title: StereoVAE: A lightweight stereo matching system through embedded GPUs. (arXiv:2305.11566v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11566">http://arxiv.org/abs/2305.11566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11566] StereoVAE: A lightweight stereo matching system through embedded GPUs](http://arxiv.org/abs/2305.11566) #robust</code></li>
<li>Summary: <p>We present a lightweight system for stereo matching through embedded GPUs. It
breaks the trade-off between accuracy and processing speed in stereo matching,
enabling our embedded system to further improve the matching accuracy while
ensuring real-time processing. The main idea of our method is to construct a
tiny neural network based on variational auto-encoder (VAE) to upsample and
refinement a small size of coarse disparity map, which is first generated by a
traditional matching method. The proposed hybrid structure cannot only bring
the advantage of traditional methods in terms of computational complexity, but
also ensure the matching accuracy under the impact of neural network. Extensive
experiments on the KITTI 2015 benchmark demonstrate that our tiny system
exhibits high robustness in improving the accuracy of the coarse disparity maps
generated by different algorithms, while also running in real-time on embedded
GPUs.
</p></li>
</ul>

<h3>Title: Efficient and Deterministic Search Strategy Based on Residual Projections for Point Cloud Registration. (arXiv:2305.11716v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11716">http://arxiv.org/abs/2305.11716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11716] Efficient and Deterministic Search Strategy Based on Residual Projections for Point Cloud Registration](http://arxiv.org/abs/2305.11716) #robust</code></li>
<li>Summary: <p>Estimating the rigid transformation between two LiDAR scans through putative
3D correspondences is a typical point cloud registration paradigm. Current 3D
feature matching approaches commonly lead to numerous outlier correspondences,
making outlier-robust registration techniques indispensable. Many recent
studies have adopted the branch and bound (BnB) optimization framework to solve
the correspondence-based point cloud registration problem globally and
deterministically. Nonetheless, BnB-based methods are time-consuming to search
the entire 6-dimensional parameter space, since their computational complexity
is exponential to the dimension of the solution domain. In order to enhance
algorithm efficiency, existing works attempt to decouple the 6 degrees of
freedom (DOF) original problem into two 3-DOF sub-problems, thereby reducing
the dimension of the parameter space. In contrast, our proposed approach
introduces a novel pose decoupling strategy based on residual projections,
effectively decomposing the raw problem into three 2-DOF rotation search
sub-problems. Subsequently, we employ a novel BnB-based search method to solve
these sub-problems, achieving efficient and deterministic registration.
Furthermore, our method can be adapted to address the challenging problem of
simultaneous pose and correspondence registration (SPCR). Through extensive
experiments conducted on synthetic and real-world datasets, we demonstrate that
our proposed method outperforms state-of-the-art methods in terms of
efficiency, while simultaneously ensuring robustness.
</p></li>
</ul>

<h3>Title: RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. (arXiv:2305.11845v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11845">http://arxiv.org/abs/2305.11845</a></li>
<li>Code URL: <a href="https://github.com/thomas0809/rxnscribe">https://github.com/thomas0809/rxnscribe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11845] RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing](http://arxiv.org/abs/2305.11845) #robust</code></li>
<li>Summary: <p>Reaction diagram parsing is the task of extracting reaction schemes from a
diagram in the chemistry literature. The reaction diagrams can be arbitrarily
complex, thus robustly parsing them into structured data is an open challenge.
In this paper, we present RxnScribe, a machine learning model for parsing
reaction diagrams of varying styles. We formulate this structured prediction
task with a sequence generation approach, which condenses the traditional
pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378
diagrams and evaluate it with cross validation, achieving an 80.0% soft match
F1 score, with significant improvements over previous models. Our code and data
are publicly available at https://github.com/thomas0809/RxnScribe.
</p></li>
</ul>

<h3>Title: Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11334">http://arxiv.org/abs/2305.11334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11334] Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs](http://arxiv.org/abs/2305.11334) #robust</code></li>
<li>Summary: <p>We introduce two novel methods, Tree-Search and Self-contextualizing QA,
designed to enhance the performance of large language models (LLMs) in
question-answering tasks. Tree-Search is a sampling technique specifically
created to extract diverse information from an LLM for a given prompt.
Self-contextualizing QA leverages Tree-Search to enable the model to create its
own context using a wide range of information relevant to the prompt, evaluate
it explicitly and return a open book answer to the initial prompt . We
demonstrate that the quality of generated answers improves according to various
metrics, including accuracy, informativeness, coherence, and consistency, as
evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods
result in increased robustness and that performance is positively correlated
with tree size, benefiting both answer quality and robustness. Finally, we
discuss other promising applications of Tree-Search, highlighting its potential
to enhance a broad range of tasks beyond question-answering.
</p></li>
</ul>

<p>\noindent We also discuss several areas for future work, including refining
the Tree-Search and Self-Contextualizing QA methods, improving the coherence of
the generated context, and investigating the impact of bootstrapping on model
robustness
</p>

<h3>Title: Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals. (arXiv:2305.11349v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11349">http://arxiv.org/abs/2305.11349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11349] Unsupervised Domain-agnostic Fake News Detection using Multi-modal Weak Signals](http://arxiv.org/abs/2305.11349) #robust</code></li>
<li>Summary: <p>The emergence of social media as one of the main platforms for people to
access news has enabled the wide dissemination of fake news. This has motivated
numerous studies on automating fake news detection. Although there have been
limited attempts at unsupervised fake news detection, their performance suffers
due to not exploiting the knowledge from various modalities related to news
records and due to the presence of various latent biases in the existing news
datasets. To address these limitations, this work proposes an effective
framework for unsupervised fake news detection, which first embeds the
knowledge available in four modalities in news records and then proposes a
novel noise-robust self-supervised learning technique to identify the veracity
of news records from the multi-modal embeddings. Also, we propose a novel
technique to construct news datasets minimizing the latent biases in existing
news datasets. Following the proposed approach for dataset construction, we
produce a Large-scale Unlabelled News Dataset consisting 419,351 news articles
related to COVID-19, acronymed as LUND-COVID. We trained the proposed
unsupervised framework using LUND-COVID to exploit the potential of large
datasets, and evaluate it using a set of existing labelled datasets. Our
results show that the proposed unsupervised framework largely outperforms
existing unsupervised baselines for different tasks such as multi-modal fake
news detection, fake news early detection and few-shot fake news detection,
while yielding notable improvements for unseen domains during training.
</p></li>
</ul>

<h3>Title: Contextualized Word Vector-based Methods for Discovering Semantic Differences with No Training nor Word Alignment. (arXiv:2305.11516v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11516">http://arxiv.org/abs/2305.11516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11516] Contextualized Word Vector-based Methods for Discovering Semantic Differences with No Training nor Word Alignment](http://arxiv.org/abs/2305.11516) #robust</code></li>
<li>Summary: <p>In this paper, we propose methods for discovering semantic differences in
words appearing in two corpora based on the norms of contextualized word
vectors. The key idea is that the coverage of meanings is reflected in the norm
of its mean word vector. The proposed methods do not require the assumptions
concerning words and corpora for comparison that the previous methods do. All
they require are to compute the mean vector of contextualized word vectors and
its norm for each word type. Nevertheless, they are (i) robust for the skew in
corpus size; (ii) capable of detecting semantic differences in infrequent
words; and (iii) effective in pinpointing word instances that have a meaning
missing in one of the two corpora for comparison. We show these advantages for
native and non-native English corpora and also for historical corpora.
</p></li>
</ul>

<h3>Title: Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11792">http://arxiv.org/abs/2305.11792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11792] Chain-of-thought prompting for responding to in-depth dialogue questions with LLM](http://arxiv.org/abs/2305.11792) #robust</code></li>
<li>Summary: <p>The way and content in which users ask questions can provide insight into
their current status, including their personality, emotions, and psychology.
Instead of directly prompting the large language models (LLMs), we explore how
chain-of-thought prompting helps in this scenario to perform reasoning and
planning according to user status, aiming to provide a more personalized and
engaging experience for the user query. To this end, we first construct a
benchmark of 6 dialogue or question-answering datasets in both English and
Chinese, covering 3 different aspects of user status (\textit{including}
\textit{personality}, \textit{emotion}, and \textit{psychology}). Then we
prompt the LLMs to generate the response regarding the user status as
intermediate reasoning processing. We propose a novel demonstration selection
strategy using the semantic similarity of intermediate reasoning instead of
test queries. To evaluate the effectiveness and robustness of our approach, we
conduct extensive experiments with 7 LLMs under zero-shot and one-shot
settings. The experimental results show that our approach consistently
outperforms standard prompting in terms of both \textit{helpfulness} and
\textit{acceptness} across all datasets, regardless of the LLMs used. The code
and dataset can be found at
\url{https://github.com/ruleGreen/Dialogue_CoT.git}.
</p></li>
</ul>

<h3>Title: Prediction with Incomplete Data under Agnostic Mask Distribution Shift. (arXiv:2305.11197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11197">http://arxiv.org/abs/2305.11197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11197] Prediction with Incomplete Data under Agnostic Mask Distribution Shift](http://arxiv.org/abs/2305.11197) #robust</code></li>
<li>Summary: <p>Data with missing values is ubiquitous in many applications. Recent years
have witnessed increasing attention on prediction with only incomplete data
consisting of observed features and a mask that indicates the missing pattern.
Existing methods assume that the training and testing distributions are the
same, which may be violated in real-world scenarios. In this paper, we consider
prediction with incomplete data in the presence of distribution shift. We focus
on the case where the underlying joint distribution of complete features and
label is invariant, but the missing pattern, i.e., mask distribution may shift
agnostically between training and testing. To achieve generalization, we
leverage the observation that for each mask, there is an invariant optimal
predictor. To avoid the exponential explosion when learning them separately, we
approximate the optimal predictors jointly using a double parameterization
technique. This has the undesirable side effect of allowing the learned
predictors to rely on the intra-mask correlation and that between features and
mask. We perform decorrelation to minimize this effect. Combining the
techniques above, we propose a novel prediction method called StableMiss.
Extensive experiments on both synthetic and real-world datasets show that
StableMiss is robust and outperforms state-of-the-art methods under agnostic
mask distribution shift.
</p></li>
</ul>

<h3>Title: Bayesian Risk-Averse Q-Learning with Streaming Observations. (arXiv:2305.11300v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11300">http://arxiv.org/abs/2305.11300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11300] Bayesian Risk-Averse Q-Learning with Streaming Observations](http://arxiv.org/abs/2305.11300) #robust</code></li>
<li>Summary: <p>We consider a robust reinforcement learning problem, where a learning agent
learns from a simulated training environment. To account for the model
mis-specification between this training environment and the real environment
due to lack of data, we adopt a formulation of Bayesian risk MDP (BRMDP) with
infinite horizon, which uses Bayesian posterior to estimate the transition
model and impose a risk functional to account for the model uncertainty.
Observations from the real environment that is out of the agent's control
arrive periodically and are utilized by the agent to update the Bayesian
posterior to reduce model uncertainty. We theoretically demonstrate that BRMDP
balances the trade-off between robustness and conservativeness, and we further
develop a multi-stage Bayesian risk-averse Q-learning algorithm to solve BRMDP
with streaming observations from real environment. The proposed algorithm
learns a risk-averse yet optimal policy that depends on the availability of
real-world observations. We provide a theoretical guarantee of strong
convergence for the proposed algorithm.
</p></li>
</ul>

<h3>Title: pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting. (arXiv:2305.11304v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11304">http://arxiv.org/abs/2305.11304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11304] pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting](http://arxiv.org/abs/2305.11304) #robust</code></li>
<li>Summary: <p>Various probabilistic time series forecasting models have sprung up and shown
remarkably good performance. However, the choice of model highly relies on the
characteristics of the input time series and the fixed distribution that the
model is based on. Due to the fact that the probability distributions cannot be
averaged over different models straightforwardly, the current time series model
ensemble methods cannot be directly applied to improve the robustness and
accuracy of forecasting. To address this issue, we propose pTSE, a multi-model
distribution ensemble method for probabilistic forecasting based on Hidden
Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models
without requiring further information about each model. Besides, we provide a
complete theoretical analysis of pTSE to prove that the empirical distribution
of time series subject to an HMM will converge to the stationary distribution
almost surely. Experiments on benchmarks show the superiority of pTSE overall
member models and competitive ensemble methods.
</p></li>
</ul>

<h3>Title: BELLA: Black box model Explanations by Local Linear Approximations. (arXiv:2305.11311v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11311">http://arxiv.org/abs/2305.11311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11311] BELLA: Black box model Explanations by Local Linear Approximations](http://arxiv.org/abs/2305.11311) #robust</code></li>
<li>Summary: <p>In recent years, understanding the decision-making process of black-box
models has become not only a legal requirement but also an additional way to
assess their performance. However, the state of the art post-hoc interpretation
approaches rely on synthetic data generation. This introduces uncertainty and
can hurt the reliability of the interpretations. Furthermore, they tend to
produce explanations that apply to only very few data points. This makes the
explanations brittle and limited in scope. Finally, they provide scores that
have no direct verifiable meaning. In this paper, we present BELLA, a
deterministic model-agnostic post-hoc approach for explaining the individual
predictions of regression black-box models. BELLA provides explanations in the
form of a linear model trained in the feature space. Thus, its coefficients can
be used directly to compute the predicted value from the feature values.
Furthermore, BELLA maximizes the size of the neighborhood to which the linear
model applies, so that the explanations are accurate, simple, general, and
robust. BELLA can produce both factual and counterfactual explanations. Our
user study confirms the importance of the desiderata we optimize, and our
experiments show that BELLA outperforms the state-of-the-art approaches on
these desiderata.
</p></li>
</ul>

<h3>Title: GraphFC: Customs Fraud Detection with Label Scarcity. (arXiv:2305.11377v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11377">http://arxiv.org/abs/2305.11377</a></li>
<li>Code URL: <a href="https://github.com/k-s-b/gnn_wco">https://github.com/k-s-b/gnn_wco</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11377] GraphFC: Customs Fraud Detection with Label Scarcity](http://arxiv.org/abs/2305.11377) #robust</code></li>
<li>Summary: <p>Custom officials across the world encounter huge volumes of transactions.
With increased connectivity and globalization, the customs transactions
continue to grow every year. Associated with customs transactions is the
customs fraud - the intentional manipulation of goods declarations to avoid the
taxes and duties. With limited manpower, the custom offices can only undertake
manual inspection of a limited number of declarations. This necessitates the
need for automating the customs fraud detection by machine learning (ML)
techniques. Due the limited manual inspection for labeling the new-incoming
declarations, the ML approach should have robust performance subject to the
scarcity of labeled data. However, current approaches for customs fraud
detection are not well suited and designed for this real-world setting. In this
work, we propose $\textbf{GraphFC}$ ($\textbf{Graph}$ neural networks for
$\textbf{C}$ustoms $\textbf{F}$raud), a model-agnostic, domain-specific,
semi-supervised graph neural network based customs fraud detection algorithm
that has strong semi-supervised and inductive capabilities. With upto 252%
relative increase in recall over the present state-of-the-art, extensive
experimentation on real customs data from customs administrations of three
different countries demonstrate that GraphFC consistently outperforms various
baselines and the present state-of-art by a large margin.
</p></li>
</ul>

<h3>Title: A Novel Tensor Factorization-Based Method with Robustness to Inaccurate Rank Estimation. (arXiv:2305.11458v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11458">http://arxiv.org/abs/2305.11458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11458] A Novel Tensor Factorization-Based Method with Robustness to Inaccurate Rank Estimation](http://arxiv.org/abs/2305.11458) #robust</code></li>
<li>Summary: <p>This study aims to solve the over-reliance on the rank estimation strategy in
the standard tensor factorization-based tensor recovery and the problem of a
large computational cost in the standard t-SVD-based tensor recovery. To this
end, we proposes a new tensor norm with a dual low-rank constraint, which
utilizes the low-rank prior and rank information at the same time. In the
proposed tensor norm, a series of surrogate functions of the tensor tubal rank
can be used to achieve better performance in harness low-rankness within tensor
data. It is proven theoretically that the resulting tensor completion model can
effectively avoid performance degradation caused by inaccurate rank estimation.
Meanwhile, attributed to the proposed dual low-rank constraint, the t-SVD of a
smaller tensor instead of the original big one is computed by using a sample
trick. Based on this, the total cost at each iteration of the optimization
algorithm is reduced to $\mathcal{O}(n^3\log n +kn^3)$ from $\mathcal{O}(n^4)$
achieved with standard methods, where $k$ is the estimation of the true tensor
rank and far less than $n$. Our method was evaluated on synthetic and
real-world data, and it demonstrated superior performance and efficiency over
several existing state-of-the-art tensor completion methods.
</p></li>
</ul>

<h3>Title: Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11476">http://arxiv.org/abs/2305.11476</a></li>
<li>Code URL: <a href="https://github.com/jackory/rpbt">https://github.com/jackory/rpbt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11476] Learning Diverse Risk Preferences in Population-based Self-play](http://arxiv.org/abs/2305.11476) #robust</code></li>
<li>Summary: <p>Among the great successes of Reinforcement Learning (RL), self-play
algorithms play an essential role in solving competitive games. Current
self-play algorithms optimize the agent to maximize expected win-rates against
its current or historical copies, making it often stuck in the local optimum
and its strategy style simple and homogeneous. A possible solution is to
improve the diversity of policies, which helps the agent break the stalemate
and enhances its robustness when facing different opponents. However, enhancing
diversity in the self-play algorithms is not trivial. In this paper, we aim to
introduce diversity from the perspective that agents could have diverse risk
preferences in the face of uncertainty. Specifically, we design a novel
reinforcement learning algorithm called Risk-sensitive Proximal Policy
Optimization (RPPO), which smoothly interpolates between worst-case and
best-case policy learning and allows for policy learning with desired risk
preferences. Seamlessly integrating RPPO with population-based self-play,
agents in the population optimize dynamic risk-sensitive objectives with
experiences from playing against diverse opponents. Empirical results show that
our method achieves comparable or superior performance in competitive games and
that diverse modes of behaviors emerge. Our code is public online at
\url{https://github.com/Jackory/RPBT}.
</p></li>
</ul>

<h3>Title: Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank Approximation. (arXiv:2305.11495v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11495">http://arxiv.org/abs/2305.11495</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11495] Nonconvex Robust High-Order Tensor Completion Using Randomized Low-Rank Approximation](http://arxiv.org/abs/2305.11495) #robust</code></li>
<li>Summary: <p>Within the tensor singular value decomposition (T-SVD) framework, existing
robust low-rank tensor completion approaches have made great achievements in
various areas of science and engineering. Nevertheless, these methods involve
the T-SVD based low-rank approximation, which suffers from high computational
costs when dealing with large-scale tensor data. Moreover, most of them are
only applicable to third-order tensors. Against these issues, in this article,
two efficient low-rank tensor approximation approaches fusing randomized
techniques are first devised under the order-d (d >= 3) T-SVD framework. On
this basis, we then further investigate the robust high-order tensor completion
(RHTC) problem, in which a double nonconvex model along with its corresponding
fast optimization algorithms with convergence guarantees are developed. To the
best of our knowledge, this is the first study to incorporate the randomized
low-rank approximation into the RHTC problem. Empirical studies on large-scale
synthetic and real tensor data illustrate that the proposed method outperforms
other state-of-the-art approaches in terms of both computational efficiency and
estimated precision.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Multi-Focus Image Fusion Based on Spatial Frequency(SF) and Consistency Verification(CV) in DCT Domain. (arXiv:2305.11265v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11265">http://arxiv.org/abs/2305.11265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11265] Multi-Focus Image Fusion Based on Spatial Frequency(SF) and Consistency Verification(CV) in DCT Domain](http://arxiv.org/abs/2305.11265) #biometric</code></li>
<li>Summary: <p>Multi-focus is a technique of focusing on different aspects of a particular
object or scene. Wireless Visual Sensor Networks (WVSN) use multi-focus image
fusion, which combines two or more images to create a more accurate output
image that describes the scene better than any individual input image. WVSN has
various applications, including video surveillance, monitoring, and tracking.
Therefore, a high-level analysis of these networks can benefit Biometrics. This
paper introduces an algorithm that utilizes discrete cosine transform (DCT)
standards to fuse multi-focus images in WVSNs. The spatial frequency (SF) of
the corresponding blocks from the source images determines the fusion
criterion. The blocks with higher spatial frequencies make up the DCT
presentation of the fused image, and the Consistency Verification (CV)
procedure is used to enhance the output image quality. The proposed fusion
method was tested on multiple pairs of multi-focus images coded on JPEG
standard to evaluate the fusion performance, and the results indicate that it
improves the visual quality of the output image and outperforms other DCT-based
techniques.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Learning Sequence Descriptor based on Spatiotemporal Attention for Visual Place Recognition. (arXiv:2305.11467v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11467">http://arxiv.org/abs/2305.11467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11467] Learning Sequence Descriptor based on Spatiotemporal Attention for Visual Place Recognition](http://arxiv.org/abs/2305.11467) #extraction</code></li>
<li>Summary: <p>Sequence-based visual place recognition (sVPR) aims to match frame sequences
with frames stored in a reference map for localization. Existing methods
include sequence matching and sequence descriptor-based retrieval. The former
is based on the assumption of constant velocity, which is difficult to hold in
real scenarios and does not get rid of the intrinsic single frame descriptor
mismatch. The latter solves this problem by extracting a descriptor for the
whole sequence, but current sequence descriptors are only constructed by
feature aggregation of multi-frames, with no temporal information interaction.
In this paper, we propose a sequential descriptor extraction method to fuse
spatiotemporal information effectively and generate discriminative descriptors.
Specifically, similar features on the same frame focu on each other and learn
space structure, and the same local regions of different frames learn local
feature changes over time. And we use sliding windows to control the temporal
self-attention range and adpot relative position encoding to construct the
positional relationships between different features, which allows our
descriptor to capture the inherent dynamics in the frame sequence and local
feature motion.
</p></li>
</ul>

<h3>Title: Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling. (arXiv:2305.11719v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11719">http://arxiv.org/abs/2305.11719</a></li>
<li>Code URL: <a href="https://github.com/chocowu/mre-ise">https://github.com/chocowu/mre-ise</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11719] Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling](http://arxiv.org/abs/2305.11719) #extraction</code></li>
<li>Summary: <p>Existing research on multimodal relation extraction (MRE) faces two
co-existing challenges, internal-information over-utilization and
external-information under-exploitation. To combat that, we propose a novel
framework that simultaneously implements the idea of internal-information
screening and external-information exploiting. First, we represent the
fine-grained semantic structures of the input image and text with the visual
and textual scene graphs, which are further fused into a unified cross-modal
graph (CMG). Based on CMG, we perform structure refinement with the guidance of
the graph information bottleneck principle, actively denoising the
less-informative features. Next, we perform topic modeling over the input image
and text, incorporating latent multimodal topic features to enrich the
contexts. On the benchmark MRE dataset, our system outperforms the current best
model significantly. With further in-depth analyses, we reveal the great
potential of our method for the MRE task. Our codes are open at
https://github.com/ChocoWu/MRE-ISE.
</p></li>
</ul>

<h3>Title: Computational thematics: Comparing algorithms for clustering the genres of literary fiction. (arXiv:2305.11251v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11251">http://arxiv.org/abs/2305.11251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11251] Computational thematics: Comparing algorithms for clustering the genres of literary fiction](http://arxiv.org/abs/2305.11251) #extraction</code></li>
<li>Summary: <p>What are the best methods of capturing thematic similarity between literary
texts? Knowing the answer to this question would be useful for automatic
clustering of book genres, or any other thematic grouping. This paper compares
a variety of algorithms for unsupervised learning of thematic similarities
between texts, which we call "computational thematics". These algorithms belong
to three steps of analysis: text preprocessing, extraction of text features,
and measuring distances between the lists of features. Each of these steps
includes a variety of options. We test all the possible combinations of these
options: every combination of algorithms is given a task to cluster a corpus of
books belonging to four pre-tagged genres of fiction. This clustering is then
validated against the "ground truth" genre labels. Such comparison of
algorithms allows us to learn the best and the worst combinations for
computational thematic analysis. To illustrate the sharp difference between the
best and the worst methods, we then cluster 5000 random novels from the
HathiTrust corpus of fiction.
</p></li>
</ul>

<h3>Title: Recouple Event Field via Probabilistic Bias for Event Extraction. (arXiv:2305.11498v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11498">http://arxiv.org/abs/2305.11498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11498] Recouple Event Field via Probabilistic Bias for Event Extraction](http://arxiv.org/abs/2305.11498) #extraction</code></li>
<li>Summary: <p>Event Extraction (EE), aiming to identify and classify event triggers and
arguments from event mentions, has benefited from pre-trained language models
(PLMs). However, existing PLM-based methods ignore the information of
trigger/argument fields, which is crucial for understanding event schemas. To
this end, we propose a Probabilistic reCoupling model enhanced Event extraction
framework (ProCE). Specifically, we first model the syntactic-related event
fields as probabilistic biases, to clarify the event fields from ambiguous
entanglement. Furthermore, considering multiple occurrences of the same
triggers/arguments in EE, we explore probabilistic interaction strategies among
multiple fields of the same triggers/arguments, to recouple the corresponding
clarified distributions and capture more latent information fields. Experiments
on EE datasets demonstrate the effectiveness and generalization of our proposed
approach.
</p></li>
</ul>

<h3>Title: InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11527">http://arxiv.org/abs/2305.11527</a></li>
<li>Code URL: <a href="https://github.com/zjunlp/deepke">https://github.com/zjunlp/deepke</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11527] InstructIE: A Chinese Instruction-based Information Extraction Dataset](http://arxiv.org/abs/2305.11527) #extraction</code></li>
<li>Summary: <p>We introduce a new Information Extraction (IE) task dubbed Instruction-based
IE, which aims to ask the system to follow specific instructions or guidelines
to extract information. To facilitate research in this area, we construct a
dataset called InstructIE, consisting of 270,000 weakly supervised data from
Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We
further evaluate the performance of various baseline models on the InstructIE
dataset. The results reveal that although current models exhibit promising
performance, there is still room for improvement. Furthermore, we conduct a
comprehensive case study analysis, underlining the challenges inherent in the
Instruction-based IE task. Code and dataset are available at
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</p></li>
</ul>

<h3>Title: Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11759">http://arxiv.org/abs/2305.11759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11759] Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](http://arxiv.org/abs/2305.11759) #extraction</code></li>
<li>Summary: <p>Large Language Models (LLMs) are known to memorize significant portions of
their training data. Parts of this memorized content have been shown to be
extractable by simply querying the model, which poses a privacy risk. We
present a novel approach which uses prompt-tuning to control the extraction
rates of memorized content in LLMs. We present two prompt training strategies
to increase and decrease extraction rates, which correspond to an attack and a
defense, respectively. We demonstrate the effectiveness of our techniques by
using models from the GPT-Neo family on a public benchmark. For the 1.3B
parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in
extraction rate compared to our baseline. Our defense can be tuned to achieve
different privacy-utility trade-offs by a user-specified hyperparameter. We
achieve an extraction rate reduction of up to 97.7% relative to our baseline,
with a perplexity increase of 16.9%.
</p></li>
</ul>

<h3>Title: DMDD: A Large-Scale Dataset for Dataset Mentions Detection. (arXiv:2305.11779v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11779">http://arxiv.org/abs/2305.11779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11779] DMDD: A Large-Scale Dataset for Dataset Mentions Detection](http://arxiv.org/abs/2305.11779) #extraction</code></li>
<li>Summary: <p>The recognition of dataset names is a critical task for automatic information
extraction in scientific literature, enabling researchers to understand and
identify research opportunities. However, existing corpora for dataset mention
detection are limited in size and naming diversity. In this paper, we introduce
the Dataset Mentions Detection Dataset (DMDD), the largest publicly available
corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219
scientific articles with over 449,000 dataset mentions weakly annotated in the
format of in-text spans, and an evaluation set, which comprises of 450
scientific articles manually annotated for evaluation purposes. We use DMDD to
establish baseline performance for dataset mention detection and linking. By
analyzing the performance of various models on DMDD, we are able to identify
open problems in dataset mention detection. We invite the community to use our
dataset as a challenge to develop novel dataset mention detection models.
</p></li>
</ul>

<h3>Title: Enhancing Short-Term Wind Speed Forecasting using Graph Attention and Frequency-Enhanced Mechanisms. (arXiv:2305.11526v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11526">http://arxiv.org/abs/2305.11526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11526] Enhancing Short-Term Wind Speed Forecasting using Graph Attention and Frequency-Enhanced Mechanisms](http://arxiv.org/abs/2305.11526) #extraction</code></li>
<li>Summary: <p>The safe and stable operation of power systems is greatly challenged by the
high variability and randomness of wind power in large-scale
wind-power-integrated grids. Wind power forecasting is an effective solution to
tackle this issue, with wind speed forecasting being an essential aspect. In
this paper, a Graph-attentive Frequency-enhanced Spatial-Temporal Wind Speed
Forecasting model based on graph attention and frequency-enhanced mechanisms,
i.e., GFST-WSF, is proposed to improve the accuracy of short-term wind speed
forecasting. The GFST-WSF comprises a Transformer architecture for temporal
feature extraction and a Graph Attention Network (GAT) for spatial feature
extraction. The GAT is specifically designed to capture the complex spatial
dependencies among wind speed stations to effectively aggregate information
from neighboring nodes in the graph, thus enhancing the spatial representation
of the data. To model the time lag in wind speed correlation between adjacent
wind farms caused by geographical factors, a dynamic complex adjacency matrix
is formulated and utilized by the GAT. Benefiting from the effective
spatio-temporal feature extraction and the deep architecture of the
Transformer, the GFST-WSF outperforms other baselines in wind speed forecasting
for the 6-24 hours ahead forecast horizon in case studies.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods. (arXiv:2305.11386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11386">http://arxiv.org/abs/2305.11386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11386] Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods](http://arxiv.org/abs/2305.11386) #federate</code></li>
<li>Summary: <p>Developing AI tools that preserve fairness is of critical importance,
specifically in high-stakes applications such as those in healthcare. However,
health AI models' overall prediction performance is often prioritized over the
possible biases such models could have. In this study, we show one possible
approach to mitigate bias concerns by having healthcare institutions
collaborate through a federated learning paradigm (FL; which is a popular
choice in healthcare settings). While FL methods with an emphasis on fairness
have been previously proposed, their underlying model and local implementation
techniques, as well as their possible applications to the healthcare domain
remain widely underinvestigated. Therefore, we propose a comprehensive FL
approach with adversarial debiasing and a fair aggregation method, suitable to
various fairness metrics, in the healthcare domain where electronic health
records are used. Not only our approach explicitly mitigates bias as part of
the optimization process, but an FL-based paradigm would also implicitly help
with addressing data imbalance and increasing the data size, offering a
practical solution for healthcare applications. We empirically demonstrate our
method's superior performance on multiple experiments simulating large-scale
real-world scenarios and compare it to several baselines. Our method has
achieved promising fairness performance with the lowest impact on overall
discrimination performance (accuracy).
</p></li>
</ul>

<h3>Title: Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11584">http://arxiv.org/abs/2305.11584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11584] Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape](http://arxiv.org/abs/2305.11584) #federate</code></li>
<li>Summary: <p>In federated learning (FL), a cluster of local clients are chaired under the
coordination of the global server and cooperatively train one model with
privacy protection. Due to the multiple local updates and the isolated non-iid
dataset, clients are prone to overfit into their own optima, which extremely
deviates from the global objective and significantly undermines the
performance. Most previous works only focus on enhancing the consistency
between the local and global objectives to alleviate this prejudicial client
drifts from the perspective of the optimization view, whose performance would
be prominently deteriorated on the high heterogeneity. In this work, we propose
a novel and general algorithm {\ttfamily FedSMOO} by jointly considering the
optimization and generalization targets to efficiently improve the performance
in FL. Concretely, {\ttfamily FedSMOO} adopts a dynamic regularizer to
guarantee the local optima towards the global objective, which is meanwhile
revised by the global Sharpness Aware Minimization (SAM) optimizer to search
for the consistent flat minima. Our theoretical analysis indicates that
{\ttfamily FedSMOO} achieves fast $\mathcal{O}(1/T)$ convergence rate with low
generalization bound. Extensive numerical studies are conducted on the
real-world dataset to verify its peerless efficiency and excellent generality.
</p></li>
</ul>

<h3>Title: V2X-Boosted Federated Learning for Cooperative Intelligent Transportation Systems with Contextual Client Selection. (arXiv:2305.11654v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11654">http://arxiv.org/abs/2305.11654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11654] V2X-Boosted Federated Learning for Cooperative Intelligent Transportation Systems with Contextual Client Selection](http://arxiv.org/abs/2305.11654) #federate</code></li>
<li>Summary: <p>Machine learning (ML) has revolutionized transportation systems, enabling
autonomous driving and smart traffic services. Federated learning (FL)
overcomes privacy constraints by training ML models in distributed systems,
exchanging model parameters instead of raw data. However, the dynamic states of
connected vehicles affect the network connection quality and influence the FL
performance. To tackle this challenge, we propose a contextual client selection
pipeline that uses Vehicle-to-Everything (V2X) messages to select clients based
on the predicted communication latency. The pipeline includes: (i) fusing V2X
messages, (ii) predicting future traffic topology, (iii) pre-clustering clients
based on local data distribution similarity, and (iv) selecting clients with
minimal latency for future model aggregation. Experiments show that our
pipeline outperforms baselines on various datasets, particularly in non-iid
settings.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11348">http://arxiv.org/abs/2305.11348</a></li>
<li>Code URL: <a href="https://github.com/xiaoyuxin1002/bias_in_deid">https://github.com/xiaoyuxin1002/bias_in_deid</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11348] In the Name of Fairness: Assessing the Bias in Clinical Record De-identification](http://arxiv.org/abs/2305.11348) #fair</code></li>
<li>Summary: <p>Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</p></li>
</ul>

<h3>Title: Arukikata Travelogue Dataset. (arXiv:2305.11444v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11444">http://arxiv.org/abs/2305.11444</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11444] Arukikata Travelogue Dataset](http://arxiv.org/abs/2305.11444) #fair</code></li>
<li>Summary: <p>We have constructed Arukikata Travelogue Dataset and released it free of
charge for academic research. This dataset is a Japanese text dataset with a
total of over 31 million words, comprising 4,672 Japanese domestic travelogues
and 9,607 overseas travelogues. Before providing our dataset, there was a
scarcity of widely available travelogue data for research purposes, and each
researcher had to prepare their own data. This hinders the replication of
existing studies and fair comparative analysis of experimental results. Our
dataset enables any researchers to conduct investigation on the same data and
to ensure transparency and reproducibility in research. In this paper, we
describe the academic significance, characteristics, and prospects of our
dataset.
</p></li>
</ul>

<h3>Title: SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization. (arXiv:2305.11615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11615">http://arxiv.org/abs/2305.11615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11615] SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization](http://arxiv.org/abs/2305.11615) #fair</code></li>
<li>Summary: <p>Model substructure learning aims to find an invariant network substructure
that can have better out-of-distribution (OOD) generalization than the original
full structure. Existing works usually search the invariant substructure using
modular risk minimization (MRM) with fully exposed out-domain data, which may
bring about two drawbacks: 1) Unfairness, due to the dependence of the full
exposure of out-domain data; and 2) Sub-optimal OOD generalization, due to the
equally feature-untargeted pruning on the whole data distribution. Based on the
idea that in-distribution (ID) data with spurious features may have a lower
experience risk, in this paper, we propose a novel Spurious Feature-targeted
model Pruning framework, dubbed SFP, to automatically explore invariant
substructures without referring to the above drawbacks. Specifically, SFP
identifies spurious features within ID instances during training using our
theoretically verified task loss, upon which, SFP attenuates the corresponding
feature projections in model space to achieve the so-called spurious
feature-targeted pruning. This is typically done by removing network branches
with strong dependencies on identified spurious features, thus SFP can push the
model learning toward invariant features and pull that out of spurious features
and devise optimal OOD generalization. Moreover, we also conduct detailed
theoretical analysis to provide the rationality guarantee and a proof framework
for OOD structures via model sparsity, and for the first time, reveal how a
highly biased data distribution affects the model's OOD generalization.
Experiments on various OOD datasets show that SFP can significantly outperform
both structure-based and non-structure-based OOD generalization SOTAs, with
accuracy improvement up to 4.72% and 23.35%, respectively
</p></li>
</ul>

<h3>Title: On the Fairness Impacts of Private Ensembles Models. (arXiv:2305.11807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11807">http://arxiv.org/abs/2305.11807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11807] On the Fairness Impacts of Private Ensembles Models](http://arxiv.org/abs/2305.11807) #fair</code></li>
<li>Summary: <p>The Private Aggregation of Teacher Ensembles (PATE) is a machine learning
framework that enables the creation of private models through the combination
of multiple "teacher" models and a "student" model. The student model learns to
predict an output based on the voting of the teachers, and the resulting model
satisfies differential privacy. PATE has been shown to be effective in creating
private models in semi-supervised settings or when protecting data labels is a
priority. This paper explores whether the use of PATE can result in unfairness,
and demonstrates that it can lead to accuracy disparities among groups of
individuals. The paper also analyzes the algorithmic and data properties that
contribute to these disproportionate impacts, why these aspects are affecting
different groups disproportionately, and offers recommendations for mitigating
these effects
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding. (arXiv:2305.11497v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11497">http://arxiv.org/abs/2305.11497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11497] TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding](http://arxiv.org/abs/2305.11497) #interpretability</code></li>
<li>Summary: <p>Prompt tuning has achieved great success in transferring the knowledge from
large pretrained vision-language models into downstream tasks, and has
dominated the performance on visual grounding (VG). However, almost all
existing prompt tuning paradigms suffer from poor interpretability. In this
paper, we argue that their poor interpretability is attributed to the holistic
prompt generation and inference process. By "holistic", we mean that they
usually directly learn a set of vectors as the prompt (i.e., prompt
generation), and use the learned global prompt to augment the textual input for
the VG model (i.e., prompt inference). To this end, we propose a new prompt
construction paradigm with explicit explainable ability, named TreePrompt.
Specifically, we first deconstruct a complex sentence into a tree, that is
consistent with human reasoning. Then, following the syntax tree, we compose a
structured prompt in a bottom-up manner. Thanks to this step-by-step prompt
construction process, each intermediate prompt (i.e., tree node) permits us to
understand the reasoning process. Extensive ablations on various backbones and
benchmarks consistently demonstrate the effectiveness and interpretability of
our TreePrompt.
</p></li>
</ul>

<h3>Title: Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11543">http://arxiv.org/abs/2305.11543</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11543] Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling](http://arxiv.org/abs/2305.11543) #interpretability</code></li>
<li>Summary: <p>As the foundation of current natural language processing methods, pre-trained
language model has achieved excellent performance. However, the black-box
structure of the deep neural network in pre-trained language models seriously
limits the interpretability of the language modeling process. After revisiting
the coupled requirement of deep neural representation and semantics logic of
language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by
introducing the alignment processing between uninterpretable neural
representation and interpretable statistical logic. Moreover, a clustering
process is also designed to connect the word- and context-level semantics.
Specifically, an associative knowledge network (AKN), considered interpretable
statistical logic, is introduced in the alignment process for word-level
semantics. Furthermore, the context-relative distance is employed as the
semantic feature for the downstream classifier, which is greatly different from
the current uninterpretable semantic representations of pre-trained models. Our
experiments for performance evaluation and interpretable analysis are executed
on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a
novel evaluation strategy for the interpretability of machine learning models
is first proposed. According to the experimental results, our language model
can achieve better performance and highly credible interpretable ability
compared to related state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11564">http://arxiv.org/abs/2305.11564</a></li>
<li>Code URL: <a href="https://github.com/hannibal046/pluglm">https://github.com/hannibal046/pluglm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11564] Decouple knowledge from paramters for plug-and-play language modeling](http://arxiv.org/abs/2305.11564) #interpretability</code></li>
<li>Summary: <p>Pre-trained language models(PLM) have made impressive results in various NLP
tasks. It has been revealed that one of the key factors to their success is the
parameters of these models implicitly learn all kinds of knowledge during
pre-training. However, encoding knowledge implicitly in the model parameters
has two fundamental drawbacks. First, the knowledge is neither editable nor
scalable once the model is trained, which is especially problematic in that
knowledge is consistently evolving. Second, it lacks interpretability and
prevents humans from understanding which knowledge PLM requires for a certain
problem. In this paper, we introduce PlugLM, a pre-training model with
differentiable plug-in memory(DPM). The key intuition is to decouple the
knowledge storage from model parameters with an editable and scalable key-value
memory and leverage knowledge in an explainable manner by knowledge retrieval
in the DPM. To justify this design choice, we conduct evaluations in three
settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements
across four domains on average without any in-domain pre-training. (2)
knowledge update. PlugLM could absorb new knowledge in a training-free way
after pre-training is done. (3) in-task knowledge learning. PlugLM could be
further improved by incorporating training samples into DPM with knowledge
prompting.
</p></li>
</ul>

<h3>Title: Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models. (arXiv:2305.11475v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11475">http://arxiv.org/abs/2305.11475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11475] Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models](http://arxiv.org/abs/2305.11475) #interpretability</code></li>
<li>Summary: <p>Generalized Additive Models (GAMs) have recently experienced a resurgence in
popularity due to their interpretability, which arises from expressing the
target value as a sum of non-linear transformations of the features. Despite
the current enthusiasm for GAMs, their susceptibility to concurvity - i.e.,
(possibly non-linear) dependencies between the features - has hitherto been
largely overlooked. Here, we demonstrate how concurvity can severly impair the
interpretability of GAMs and propose a remedy: a conceptually simple, yet
effective regularizer which penalizes pairwise correlations of the non-linearly
transformed feature variables. This procedure is applicable to any
differentiable additive model, such as Neural Additive Models or NeuralProphet,
and enhances interpretability by eliminating ambiguities due to self-canceling
feature contributions. We validate the effectiveness of our regularizer in
experiments on synthetic as well as real-world datasets for time-series and
tabular data. Our experiments show that concurvity in GAMs can be reduced
without significantly compromising prediction quality, improving
interpretability and reducing variance in the feature importances.
</p></li>
</ul>

<h3>Title: Self-Reinforcement Attention Mechanism For Tabular Learning. (arXiv:2305.11684v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11684">http://arxiv.org/abs/2305.11684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11684] Self-Reinforcement Attention Mechanism For Tabular Learning](http://arxiv.org/abs/2305.11684) #interpretability</code></li>
<li>Summary: <p>Apart from the high accuracy of machine learning models, what interests many
researchers in real-life problems (e.g., fraud detection, credit scoring) is to
find hidden patterns in data; particularly when dealing with their challenging
imbalanced characteristics. Interpretability is also a key requirement that
needs to accompany the used machine learning model. In this concern, often,
intrinsically interpretable models are preferred to complex ones, which are in
most cases black-box models. Also, linear models are used in some high-risk
fields to handle tabular data, even if performance must be sacrificed. In this
paper, we introduce Self-Reinforcement Attention (SRA), a novel attention
mechanism that provides a relevance of features as a weight vector which is
used to learn an intelligible representation. This weight is then used to
reinforce or reduce some components of the raw input through element-wise
vector multiplication. Our results on synthetic and real-world imbalanced data
show that our proposed SRA block is effective in end-to-end combination with
baseline models.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11789">http://arxiv.org/abs/2305.11789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11789] Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach](http://arxiv.org/abs/2305.11789) #explainability</code></li>
<li>Summary: <p>Humans work together to solve common problems by having discussions,
explaining, and agreeing or disagreeing with each other. Similarly, if a system
can have discussions with humans when solving tasks, it can improve the
system's performance and reliability. In previous research on explainability,
it has only been possible for the system to make predictions and for humans to
ask questions about them rather than having a mutual exchange of opinions. This
research aims to create a dataset and computational framework for systems that
discuss and refine their predictions through dialogue. Through experiments, we
show that the proposed system can have beneficial discussions with humans
improving the accuracy by up to 25 points in the natural language inference
task.
</p></li>
</ul>

<h3>Title: The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics. (arXiv:2305.11806v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11806">http://arxiv.org/abs/2305.11806</a></li>
<li>Code URL: <a href="https://github.com/Unbabel/COMET">https://github.com/Unbabel/COMET</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11806] The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics](http://arxiv.org/abs/2305.11806) #explainability</code></li>
<li>Summary: <p>Neural metrics for machine translation evaluation, such as COMET, exhibit
significant improvements in their correlation with human judgments, as compared
to traditional metrics based on lexical overlap, such as BLEU. Yet, neural
metrics are, to a great extent, "black boxes" returning a single sentence-level
score without transparency about the decision-making process. In this work, we
develop and compare several neural explainability methods and demonstrate their
effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our
study reveals that these metrics leverage token-level information that can be
directly attributed to translation errors, as assessed through comparison of
token-level neural saliency maps with Multidimensional Quality Metrics (MQM)
annotations and with synthetically-generated critical translation errors. To
ease future research, we release our code at:
https://github.com/Unbabel/COMET/tree/explainable-metrics.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. (arXiv:2305.11281v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11281">http://arxiv.org/abs/2305.11281</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11281] SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models](http://arxiv.org/abs/2305.11281) #diffusion</code></li>
<li>Summary: <p>Object-centric learning aims to represent visual data with a set of object
entities (a.k.a. slots), providing structured representations that enable
systematic generalization. Leveraging advanced architectures like Transformers,
recent approaches have made significant progress in unsupervised object
discovery. In addition, slot-based representations hold great potential for
generative modeling, such as controllable image generation and object
manipulation in image editing. However, current slot-based methods often
produce blurry images and distorted objects, exhibiting poor generative
modeling capabilities. In this paper, we focus on improving slot-to-image
decoding, a crucial aspect for high-quality visual generation. We introduce
SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for
both image and video data. Thanks to the powerful modeling capacity of LDMs,
SlotDiffusion surpasses previous slot models in unsupervised object
segmentation and visual generation across six datasets. Furthermore, our
learned object features can be utilized by existing object-centric dynamics
models, improving video prediction quality and downstream temporal reasoning
tasks. Finally, we demonstrate the scalability of SlotDiffusion to
unconstrained real-world datasets such as PASCAL VOC and COCO, when integrated
with self-supervised pre-trained image encoders.
</p></li>
</ul>

<h3>Title: RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture. (arXiv:2305.11337v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11337">http://arxiv.org/abs/2305.11337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11337] RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture](http://arxiv.org/abs/2305.11337) #diffusion</code></li>
<li>Summary: <p>The techniques for 3D indoor scene capturing are widely used, but the meshes
produced leave much to be desired. In this paper, we propose "RoomDreamer",
which leverages powerful natural language to synthesize a new room with a
different style. Unlike existing image synthesis methods, our work addresses
the challenge of synthesizing both geometry and texture aligned to the input
scene structure and prompt simultaneously. The key insight is that a scene
should be treated as a whole, taking into account both scene texture and
geometry. The proposed framework consists of two significant components:
Geometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for
3D Scene guarantees the consistency of the scene style by applying the 2D prior
to the entire scene simultaneously. Mesh Optimization improves the geometry and
texture jointly and eliminates the artifacts in the scanned scene. To validate
the proposed method, real indoor scenes scanned with smartphones are used for
extensive experiments, through which the effectiveness of our method is
demonstrated.
</p></li>
</ul>

<h3>Title: Late-Constraint Diffusion Guidance for Controllable Image Synthesis. (arXiv:2305.11520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11520">http://arxiv.org/abs/2305.11520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11520] Late-Constraint Diffusion Guidance for Controllable Image Synthesis](http://arxiv.org/abs/2305.11520) #diffusion</code></li>
<li>Summary: <p>Diffusion models, either with or without text condition, have demonstrated
impressive capability in synthesizing photorealistic images given a few or even
no words. These models may not fully satisfy user need, as normal users or
artists intend to control the synthesized images with specific guidance, like
overall layout, color, structure, object shape, and so on. To adapt diffusion
models for controllable image synthesis, several methods have been proposed to
incorporate the required conditions as regularization upon the intermediate
features of the diffusion denoising network. These methods, known as
early-constraint ones in this paper, have difficulties in handling multiple
conditions with a single solution. They intend to train separate models for
each specific condition, which require much training cost and result in
non-generalizable solutions. To address these difficulties, we propose a new
approach namely late-constraint: we leave the diffusion networks unchanged, but
constrain its output to be aligned with the required conditions. Specifically,
we train a lightweight condition adapter to establish the correlation between
external conditions and internal representations of diffusion models. During
the iterative denoising process, the conditional guidance is sent into
corresponding condition adapter to manipulate the sampling process with the
established correlation. We further equip the introduced late-constraint
strategy with a timestep resampling method and an early stopping technique,
which boost the quality of synthesized image meanwhile complying with the
guidance. Our method outperforms the existing early-constraint methods and
generalizes better to unseen condition.
</p></li>
</ul>

<h3>Title: Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots. (arXiv:2305.11540v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11540">http://arxiv.org/abs/2305.11540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11540] Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots](http://arxiv.org/abs/2305.11540) #diffusion</code></li>
<li>Summary: <p>Diffusion models have made impressive progress in text-to-image synthesis.
However, training such large-scale models (e.g. Stable Diffusion), from scratch
requires high computational costs and massive high-quality text-image pairs,
which becomes unaffordable in other languages. To handle this challenge, we
propose IAP, a simple but effective method to transfer English Stable Diffusion
into Chinese. IAP optimizes only a separate Chinese text encoder with all other
parameters fixed to align Chinese semantics space to the English one in CLIP.
To achieve this, we innovatively treat images as pivots and minimize the
distance of attentive features produced from cross-attention between images and
each language respectively. In this way, IAP establishes connections of
Chinese, English and visual semantics in CLIP's embedding space efficiently,
advancing the quality of the generated image with direct Chinese prompts.
Experimental results show that our method outperforms several strong Chinese
diffusion models with only 5%~10% training data.
</p></li>
</ul>

<h3>Title: Brain Captioning: Decoding human brain activity into images and text. (arXiv:2305.11560v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11560">http://arxiv.org/abs/2305.11560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11560] Brain Captioning: Decoding human brain activity into images and text](http://arxiv.org/abs/2305.11560) #diffusion</code></li>
<li>Summary: <p>Every day, the human brain processes an immense volume of visual information,
relying on intricate neural mechanisms to perceive and interpret these stimuli.
Recent breakthroughs in functional magnetic resonance imaging (fMRI) have
enabled scientists to extract visual information from human brain activity
patterns. In this study, we present an innovative method for decoding brain
activity into meaningful images and captions, with a specific focus on brain
captioning due to its enhanced flexibility as compared to brain decoding into
images. Our approach takes advantage of cutting-edge image captioning models
and incorporates a unique image reconstruction pipeline that utilizes latent
diffusion models and depth estimation. We utilized the Natural Scenes Dataset,
a comprehensive fMRI dataset from eight subjects who viewed images from the
COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our
backbone for captioning and propose a new image reconstruction pipeline based
on latent diffusion models. The method involves training regularized linear
regression models between brain activity and extracted features. Additionally,
we incorporated depth maps from the ControlNet model to further guide the
reconstruction process. We evaluate our methods using quantitative metrics for
both generated captions and images. Our brain captioning approach outperforms
existing methods, while our image reconstruction pipeline generates plausible
images with improved spatial relationships. In conclusion, we demonstrate
significant progress in brain decoding, showcasing the enormous potential of
integrating vision and language to better understand human cognition. Our
approach provides a flexible platform for future research, with potential
applications in various fields, including neural art, style transfer, and
portable devices.
</p></li>
</ul>

<h3>Title: Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields. (arXiv:2305.11588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11588">http://arxiv.org/abs/2305.11588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11588] Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields](http://arxiv.org/abs/2305.11588) #diffusion</code></li>
<li>Summary: <p>Text-driven 3D scene generation is widely applicable to video gaming, film
industry, and metaverse applications that have a large demand for 3D scenes.
However, existing text-to-3D generation methods are limited to producing 3D
objects with simple geometries and dreamlike styles that lack realism. In this
work, we present Text2NeRF, which is able to generate a wide range of 3D scenes
with complicated geometric structures and high-fidelity textures purely from a
text prompt. To this end, we adopt NeRF as the 3D representation and leverage a
pre-trained text-to-image diffusion model to constrain the 3D reconstruction of
the NeRF to reflect the scene description. Specifically, we employ the
diffusion model to infer the text-related image as the content prior and use a
monocular depth estimation method to offer the geometric prior. Both content
and geometric priors are utilized to update the NeRF model. To guarantee
textured and geometric consistency between different views, we introduce a
progressive scene inpainting and updating strategy for novel view synthesis of
the scene. Our method requires no additional training data but only a natural
language description of the scene as the input. Extensive experiments
demonstrate that our Text2NeRF outperforms existing methods in producing
photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of
natural language prompts.
</p></li>
</ul>

<h3>Title: Few-shot 3D Shape Generation. (arXiv:2305.11664v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11664">http://arxiv.org/abs/2305.11664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11664] Few-shot 3D Shape Generation](http://arxiv.org/abs/2305.11664) #diffusion</code></li>
<li>Summary: <p>Realistic and diverse 3D shape generation is helpful for a wide variety of
applications such as virtual reality, gaming, and animation. Modern generative
models, such as GANs and diffusion models, learn from large-scale datasets and
generate new samples following similar data distributions. However, when
training data is limited, deep neural generative networks overfit and tend to
replicate training samples. Prior works focus on few-shot image generation to
produce high-quality and diverse results using a few target images.
Unfortunately, abundant 3D shape data is typically hard to obtain as well. In
this work, we make the first attempt to realize few-shot 3D shape generation by
adapting generative models pre-trained on large source domains to target
domains using limited data. To relieve overfitting and keep considerable
diversity, we propose to maintain the probability distributions of the pairwise
relative distances between adapted samples at feature-level and shape-level
during domain adaptation. Our approach only needs the silhouettes of few-shot
target samples as training data to learn target geometry distributions and
achieve generated shapes with diverse topology and textures. Moreover, we
introduce several metrics to evaluate the quality and diversity of few-shot 3D
shape generation. The effectiveness of our approach is demonstrated
qualitatively and quantitatively under a series of few-shot 3D shape adaptation
setups.
</p></li>
</ul>

<h3>Title: Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity. (arXiv:2305.11675v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11675">http://arxiv.org/abs/2305.11675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11675] Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity](http://arxiv.org/abs/2305.11675) #diffusion</code></li>
<li>Summary: <p>Reconstructing human vision from brain activities has been an appealing task
that helps to understand our cognitive process. Even though recent research has
seen great success in reconstructing static images from non-invasive brain
recordings, work on recovering continuous visual experiences in the form of
videos is limited. In this work, we propose Mind-Video that learns
spatiotemporal information from continuous fMRI data of the cerebral cortex
progressively through masked brain modeling, multimodal contrastive learning
with spatiotemporal attention, and co-training with an augmented Stable
Diffusion model that incorporates network temporal inflation. We show that
high-quality videos of arbitrary frame rates can be reconstructed with
Mind-Video using adversarial guidance. The recovered videos were evaluated with
various semantic and pixel-level metrics. We achieved an average accuracy of
85% in semantic classification tasks and 0.19 in structural similarity index
(SSIM), outperforming the previous state-of-the-art by 45%. We also show that
our model is biologically plausible and interpretable, reflecting established
physiological processes.
</p></li>
</ul>

<h3>Title: Any-to-Any Generation via Composable Diffusion. (arXiv:2305.11846v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11846">http://arxiv.org/abs/2305.11846</a></li>
<li>Code URL: <a href="https://github.com/microsoft/i-Code/tree/main/i-Code-V3">https://github.com/microsoft/i-Code/tree/main/i-Code-V3</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11846] Any-to-Any Generation via Composable Diffusion](http://arxiv.org/abs/2305.11846) #diffusion</code></li>
<li>Summary: <p>We present Composable Diffusion (CoDi), a novel generative model capable of
generating any combination of output modalities, such as language, image,
video, or audio, from any combination of input modalities. Unlike existing
generative AI systems, CoDi can generate multiple modalities in parallel and
its input is not limited to a subset of modalities like text or image. Despite
the absence of training datasets for many combinations of modalities, we
propose to align modalities in both the input and output space. This allows
CoDi to freely condition on any input combination and generate any group of
modalities, even if they are not present in the training data. CoDi employs a
novel composable generation strategy which involves building a shared
multimodal space by bridging alignment in the diffusion process, enabling the
synchronized generation of intertwined modalities, such as temporally aligned
video and audio. Highly customizable and flexible, CoDi achieves strong
joint-modality generation quality, and outperforms or is on par with the
unimodal state-of-the-art for single-modality synthesis. The project page with
demonstrations and code is at https://codi-gen.github.io
</p></li>
</ul>

<h3>Title: Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models. (arXiv:2305.11870v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11870">http://arxiv.org/abs/2305.11870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11870] Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models](http://arxiv.org/abs/2305.11870) #diffusion</code></li>
<li>Summary: <p>We propose a 3D generation pipeline that uses diffusion models to generate
realistic human digital avatars. Due to the wide variety of human identities,
poses, and stochastic details, the generation of 3D human meshes has been a
challenging problem. To address this, we decompose the problem into 2D normal
map generation and normal map-based 3D reconstruction. Specifically, we first
simultaneously generate realistic normal maps for the front and backside of a
clothed human, dubbed dual normal maps, using a pose-conditional diffusion
model. For 3D reconstruction, we ``carve'' the prior SMPL-X mesh to a detailed
3D mesh according to the normal maps through mesh optimization. To further
enhance the high-frequency details, we present a diffusion resampling scheme on
both body and facial regions, thus encouraging the generation of realistic
digital avatars. We also seamlessly incorporate a recent text-to-image
diffusion model to support text-based human identity control. Our method,
namely, Chupa, is capable of generating realistic 3D clothed humans with better
perceptual quality and identity variety.
</p></li>
</ul>

<h3>Title: DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion. (arXiv:2305.11517v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11517">http://arxiv.org/abs/2305.11517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11517] DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion](http://arxiv.org/abs/2305.11517) #diffusion</code></li>
<li>Summary: <p>Diffusion models have emerged as the new state-of-the-art family of deep
generative models, and their promising potentials for text generation have
recently attracted increasing attention. Existing studies mostly adopt a single
encoder architecture with partially noising processes for conditional text
generation, but its degree of flexibility for conditional modeling is limited.
In fact, the encoder-decoder architecture is naturally more flexible for its
detachable encoder and decoder modules, which is extensible to multilingual and
multimodal generation tasks for conditions and target texts. However, the
encoding process of conditional texts lacks the understanding of target texts.
To this end, a spiral interaction architecture for encoder-decoder text
diffusion (DiffuSIA) is proposed. Concretely, the conditional information from
encoder is designed to be captured by the diffusion decoder, while the target
information from decoder is designed to be captured by the conditional encoder.
These two types of information flow run through multilayer interaction spirally
for deep fusion and understanding. DiffuSIA is evaluated on four text
generation tasks, including paraphrase, text simplification, question
generation, and open-domain dialogue generation. Experimental results show that
DiffuSIA achieves competitive performance among previous methods on all four
tasks, demonstrating the effectiveness and generalization ability of the
proposed method.
</p></li>
</ul>

<h3>Title: Information-Ordered Bottlenecks for Adaptive Semantic Compression. (arXiv:2305.11213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11213">http://arxiv.org/abs/2305.11213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11213] Information-Ordered Bottlenecks for Adaptive Semantic Compression](http://arxiv.org/abs/2305.11213) #diffusion</code></li>
<li>Summary: <p>We present the information-ordered bottleneck (IOB), a neural layer designed
to adaptively compress data into latent variables ordered by likelihood
maximization. Without retraining, IOB nodes can be truncated at any bottleneck
width, capturing the most crucial information in the first latent variables.
Unifying several previous approaches, we show that IOBs achieve near-optimal
compression for a given encoding architecture and can assign ordering to latent
signals in a manner that is semantically meaningful. IOBs demonstrate a
remarkable ability to compress embeddings of image and text data, leveraging
the performance of SOTA architectures such as CNNs, transformers, and diffusion
models. Moreover, we introduce a novel theory for estimating global intrinsic
dimensionality with IOBs and show that they recover SOTA dimensionality
estimates for complex synthetic data. Furthermore, we showcase the utility of
these models for exploratory analysis through applications on heterogeneous
datasets, enabling computer-aided discovery of dataset complexity.
</p></li>
</ul>

<h3>Title: Incomplete Multi-view Clustering via Diffusion Completion. (arXiv:2305.11489v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11489">http://arxiv.org/abs/2305.11489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11489] Incomplete Multi-view Clustering via Diffusion Completion](http://arxiv.org/abs/2305.11489) #diffusion</code></li>
<li>Summary: <p>Incomplete multi-view clustering is a challenging and non-trivial task to
provide effective data analysis for large amounts of unlabeled data in the real
world. All incomplete multi-view clustering methods need to address the problem
of how to reduce the impact of missing views. To address this issue, we propose
diffusion completion to recover the missing views integrated into an incomplete
multi-view clustering framework. Based on the observable views information, the
diffusion model is used to recover the missing views, and then the consistency
information of the multi-view data is learned by contrastive learning to
improve the performance of multi-view clustering. To the best of our knowledge,
this may be the first work to incorporate diffusion models into an incomplete
multi-view clustering framework. Experimental results show that the proposed
method performs well in recovering the missing views while achieving superior
clustering performance compared to state-of-the-art methods.
</p></li>
</ul>

<h3>Title: The probability flow ODE is provably fast. (arXiv:2305.11798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11798">http://arxiv.org/abs/2305.11798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11798] The probability flow ODE is provably fast](http://arxiv.org/abs/2305.11798) #diffusion</code></li>
<li>Summary: <p>We provide the first polynomial-time convergence guarantees for the
probability flow ODE implementation (together with a corrector step) of
score-based generative modeling. Our analysis is carried out in the wake of
recent results obtaining such guarantees for the SDE-based implementation
(i.e., denoising diffusion probabilistic modeling or DDPM), but requires the
development of novel techniques for studying deterministic dynamics without
contractivity. Through the use of a specially chosen corrector step based on
the underdamped Langevin diffusion, we obtain better dimension dependence than
prior works on DDPM ($O(\sqrt{d})$ vs. $O(d)$, assuming smoothness of the data
distribution), highlighting potential advantages of the ODE framework.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Coordinated Transformer with Position \&amp; Sample-aware Central Loss for Anatomical Landmark Detection. (arXiv:2305.11338v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11338">http://arxiv.org/abs/2305.11338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11338] Coordinated Transformer with Position \&amp; Sample-aware Central Loss for Anatomical Landmark Detection](http://arxiv.org/abs/2305.11338) #transformer</code></li>
<li>Summary: <p>Heatmap-based anatomical landmark detection is still facing two unresolved
challenges: 1) inability to accurately evaluate the distribution of heatmap; 2)
inability to effectively exploit global spatial structure information. To
address the computational inability challenge, we propose a novel
position-aware and sample-aware central loss. Specifically, our central loss
can absorb position information, enabling accurate evaluation of the heatmap
distribution. More advanced is that our central loss is sample-aware, which can
adaptively distinguish easy and hard samples and make the model more focused on
hard samples while solving the challenge of extreme imbalance between landmarks
and non-landmarks. To address the challenge of ignoring structure information,
a Coordinated Transformer, called CoorTransformer, is proposed, which
establishes long-range dependencies under the guidance of landmark coordination
information, making the attention more focused on the sparse landmarks while
taking advantage of global spatial structure. Furthermore, CoorTransformer can
speed up convergence, effectively avoiding the defect that Transformers have
difficulty converging in sparse representation learning. Using the advanced
CoorTransformer and central loss, we propose a generalized detection model that
can handle various scenarios, inherently exploiting the underlying relationship
between landmarks and incorporating rich structural knowledge around the target
landmarks. We analyzed and evaluated CoorTransformer and central loss on three
challenging landmark detection tasks. The experimental results show that our
CoorTransformer outperforms state-of-the-art methods, and the central loss
significantly improves the performance of the model with p-values< 0.05.
</p></li>
</ul>

<h3>Title: Enhancing Transformer Backbone for Egocentric Video Action Segmentation. (arXiv:2305.11365v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11365">http://arxiv.org/abs/2305.11365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11365] Enhancing Transformer Backbone for Egocentric Video Action Segmentation](http://arxiv.org/abs/2305.11365) #transformer</code></li>
<li>Summary: <p>Egocentric temporal action segmentation in videos is a crucial task in
computer vision with applications in various fields such as mixed reality,
human behavior analysis, and robotics. Although recent research has utilized
advanced visual-language frameworks, transformers remain the backbone of action
segmentation models. Therefore, it is necessary to improve transformers to
enhance the robustness of action segmentation models. In this work, we propose
two novel ideas to enhance the state-of-the-art transformer for action
segmentation. First, we introduce a dual dilated attention mechanism to
adaptively capture hierarchical representations in both local-to-global and
global-to-local contexts. Second, we incorporate cross-connections between the
encoder and decoder blocks to prevent the loss of local context by the decoder.
Additionally, we utilize state-of-the-art visual-language representation
learning techniques to extract richer and more compact features for our
transformer. Our proposed approach outperforms other state-of-the-art methods
on the Georgia Tech Egocentric Activities (GTEA) and HOI4D Office Tools
datasets, and we validate our introduced components with ablation studies. The
source code and supplementary materials are publicly available on
https://www.sail-nu.com/dxformer.
</p></li>
</ul>

<h3>Title: Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding. (arXiv:2305.11392v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11392">http://arxiv.org/abs/2305.11392</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11392] Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding](http://arxiv.org/abs/2305.11392) #transformer</code></li>
<li>Summary: <p>Transformers achieve promising performance in document understanding because
of their high effectiveness and still suffer from quadratic computational
complexity dependency on the sequence length. General efficient transformers
are challenging to be directly adapted to model document. They are unable to
handle the layout representation in documents, e.g. word, line and paragraph,
on different granularity levels and seem hard to achieve a good trade-off
between efficiency and performance. To tackle the concerns, we propose
Fast-StrucTexT, an efficient multi-modal framework based on the StrucTexT
algorithm with an hourglass transformer architecture, for visual document
understanding. Specifically, we design a modality-guided dynamic token merging
block to make the model learn multi-granularity representation and prunes
redundant tokens. Additionally, we present a multi-modal interaction module
called Symmetry Cross Attention (SCA) to consider multi-modal fusion and
efficiently guide the token mergence. The SCA allows one modality input as
query to calculate cross attention with another modality in a dual phase.
Extensive experiments on FUNSD, SROIE, and CORD datasets demonstrate that our
model achieves the state-of-the-art performance and almost 1.9X faster
inference time than the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Efficient Mixed Transformer for Single Image Super-Resolution. (arXiv:2305.11403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11403">http://arxiv.org/abs/2305.11403</a></li>
<li>Code URL: <a href="https://github.com/fried-rice-lab/emt">https://github.com/fried-rice-lab/emt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11403] Efficient Mixed Transformer for Single Image Super-Resolution](http://arxiv.org/abs/2305.11403) #transformer</code></li>
<li>Summary: <p>Recently, Transformer-based methods have achieved impressive results in
single image super-resolution (SISR). However, the lack of locality mechanism
and high complexity limit their application in the field of super-resolution
(SR). To solve these problems, we propose a new method, Efficient Mixed
Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer
Block (MTB), consisting of multiple consecutive transformer layers, in some of
which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can
enhance the local knowledge aggregation with pixel shifting operations. At the
same time, no additional complexity is introduced as PM has no parameters and
floating-point operations. Moreover, we employ striped window for SA (SWSA) to
gain an efficient global dependency modelling by utilizing image anisotropy.
Experimental results show that EMT outperforms the existing methods on
benchmark dataset and achieved state-of-the-art performance. The Code is
available at https://github. com/Fried-Rice-Lab/EMT.git.
</p></li>
</ul>

<h3>Title: SurgMAE: Masked Autoencoders for Long Surgical Video Analysis. (arXiv:2305.11451v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11451">http://arxiv.org/abs/2305.11451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11451] SurgMAE: Masked Autoencoders for Long Surgical Video Analysis](http://arxiv.org/abs/2305.11451) #transformer</code></li>
<li>Summary: <p>There has been a growing interest in using deep learning models for
processing long surgical videos, in order to automatically detect
clinical/operational activities and extract metrics that can enable workflow
efficiency tools and applications. However, training such models require vast
amounts of labeled data which is costly and not scalable. Recently,
self-supervised learning has been explored in computer vision community to
reduce the burden of the annotation cost. Masked autoencoders (MAE) got the
attention in self-supervised paradigm for Vision Transformers (ViTs) by
predicting the randomly masked regions given the visible patches of an image or
a video clip, and have shown superior performance on benchmark datasets.
However, the application of MAE in surgical data remains unexplored. In this
paper, we first investigate whether MAE can learn transferrable representations
in surgical video domain. We propose SurgMAE, which is a novel architecture
with a masking strategy based on sampling high spatio-temporal tokens for MAE.
We provide an empirical study of SurgMAE on two large scale long surgical video
datasets, and find that our method outperforms several baselines in low data
regime. We conduct extensive ablation studies to show the efficacy of our
approach and also demonstrate it's superior performance on UCF-101 to prove
it's generalizability in non-surgical datasets as well.
</p></li>
</ul>

<h3>Title: RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11474">http://arxiv.org/abs/2305.11474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11474] RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration](http://arxiv.org/abs/2305.11474) #transformer</code></li>
<li>Summary: <p>Although many recent works have made advancements in the image restoration
(IR) field, they often suffer from an excessive number of parameters. Another
issue is that most Transformer-based IR methods focus only on either local or
global features, leading to limited receptive fields or deficient parameter
issues. To address these problems, we propose a lightweight IR network,
Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed
dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which
compute bi-dimensional (spatial and channel) self-attentions in parallel with
different numbers of multi-heads. The bi-dimensional attentions help each other
to complement their counterpart's drawbacks and are then mixed. Additionally,
we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that
compensates for pixel-level information losses and utilizes semantic
information while maintaining an efficient hierarchical structure. Furthermore,
we revisit and modify MobileNet V1 and V2 to attach efficient convolutions to
our proposed components. The experimental results demonstrate that RAMiT
achieves state-of-the-art performance on multiple lightweight IR tasks,
including super-resolution, color denoising, grayscale denoising, low-light
enhancement, and deraining. Codes will be available soon.
</p></li>
</ul>

<h3>Title: Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11692">http://arxiv.org/abs/2305.11692</a></li>
<li>Code URL: <a href="https://github.com/longbai1006/surgical-vqla">https://github.com/longbai1006/surgical-vqla</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11692] Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery](http://arxiv.org/abs/2305.11692) #transformer</code></li>
<li>Summary: <p>Despite the availability of computer-aided simulators and recorded videos of
surgical procedures, junior residents still heavily rely on experts to answer
their queries. However, expert surgeons are often overloaded with clinical and
academic workloads and limit their time in answering. For this purpose, we
develop a surgical question-answering system to facilitate robot-assisted
surgical scene and activity understanding from recorded videos. Most of the
existing VQA methods require an object detector and regions based feature
extractor to extract visual features and fuse them with the embedded text of
the question for answer generation. However, (1) surgical object detection
model is scarce due to smaller datasets and lack of bounding box annotation;
(2) current fusion strategy of heterogeneous modalities like text and image is
naive; (3) the localized answering is missing, which is crucial in complex
surgical scenarios. In this paper, we propose Visual Question
Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific
surgical area during the answer prediction. To deal with the fusion of the
heterogeneous modalities, we design gated vision-language embedding (GVLE) to
build input patches for the Language Vision Transformer (LViT) to predict the
answer. To get localization, we add the detection head in parallel with the
prediction head of the LViT. We also integrate GIoU loss to boost localization
performance by preserving the accuracy of the question-answering model. We
annotate two datasets of VQLA by utilizing publicly available surgical videos
from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that
Surgical-VQLA can better understand the surgical scene and localize the
specific area related to the question-answering. GVLE presents an efficient
language-vision embedding technique by showing superior performance over the
existing benchmarks.
</p></li>
</ul>

<h3>Title: Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization. (arXiv:2305.11718v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11718">http://arxiv.org/abs/2305.11718</a></li>
<li>Code URL: <a href="https://github.com/crossmodalgroup/dynamicvectorquantization">https://github.com/crossmodalgroup/dynamicvectorquantization</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11718] Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization](http://arxiv.org/abs/2305.11718) #transformer</code></li>
<li>Summary: <p>Existing vector quantization (VQ) based autoregressive models follow a
two-stage generation paradigm that first learns a codebook to encode images as
discrete codes, and then completes generation based on the learned codebook.
However, they encode fixed-size image regions into fixed-length codes and
ignore their naturally different information densities, which results in
insufficiency in important regions and redundancy in unimportant ones, and
finally degrades the generation quality and speed. Moreover, the fixed-length
coding leads to an unnatural raster-scan autoregressive generation. To address
the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization
VAE (DQ-VAE) which encodes image regions into variable-length codes based on
their information densities for an accurate and compact code representation.
(2) DQ-Transformer which thereby generates images autoregressively from
coarse-grained (smooth regions with fewer codes) to fine-grained (details
regions with more codes) by modeling the position and content of codes in each
granularity alternately, through a novel stacked-transformer architecture and
shared-content, non-shared position input layers designs. Comprehensive
experiments on various generation tasks validate our superiorities in both
effectiveness and efficiency. Code will be released at
https://github.com/CrossmodalGroup/DynamicVectorQuantization.
</p></li>
</ul>

<h3>Title: Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution. (arXiv:2305.11315v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11315">http://arxiv.org/abs/2305.11315</a></li>
<li>Code URL: <a href="https://github.com/clulab/geonorm">https://github.com/clulab/geonorm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11315] Improving Toponym Resolution with Better Candidate Generation, Transformer-based Reranking, and Two-Stage Resolution](http://arxiv.org/abs/2305.11315) #transformer</code></li>
<li>Summary: <p>Geocoding is the task of converting location mentions in text into structured
data that encodes the geospatial semantics. We propose a new architecture for
geocoding, GeoNorm. GeoNorm first uses information retrieval techniques to
generate a list of candidate entries from the geospatial ontology. Then it
reranks the candidate entries using a transformer-based neural network that
incorporates information from the ontology such as the entry's population. This
generate-and-rerank process is applied twice: first to resolve the less
ambiguous countries, states, and counties, and second to resolve the remaining
location mentions, using the identified countries, states, and counties as
context. Our proposed toponym resolution framework achieves state-of-the-art
performance on multiple datasets. Code and models are available at
\url{https://github.com/clulab/geonorm}.
</p></li>
</ul>

<h3>Title: Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions. (arXiv:2305.11460v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11460">http://arxiv.org/abs/2305.11460</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11460] Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions](http://arxiv.org/abs/2305.11460) #transformer</code></li>
<li>Summary: <p>Finding an agreement among diverse opinions is a challenging topic in
multiagent systems. Recently, large language models (LLMs) have shown great
potential in addressing this challenge due to their remarkable capabilities in
comprehending human opinions and generating human-like text. However, they
typically rely on extensive human-annotated data. In this paper, we propose
Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find
agreement using data generated by LLM itself. Specifically, our approach
employs the generative pre-trained transformer-3 (GPT-3) to generate multiple
opinions for each question in a question dataset and create several agreement
candidates among these opinions. Then, a bidirectional encoder representations
from transformers (BERT)-based model evaluates the agreement score of each
agreement candidate and selects the one with the highest agreement score. This
process yields a dataset of question-opinion-agreements, which we use to
fine-tune a pre-trained LLM for discovering agreements among diverse opinions.
Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework
achieves comparable performance to GPT-3 with only 1/25 of its parameters,
showcasing its ability to identify agreement among various opinions without the
need for human-annotated data.
</p></li>
</ul>

<h3>Title: Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11863">http://arxiv.org/abs/2305.11863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11863] Scaling laws for language encoding models in fMRI](http://arxiv.org/abs/2305.11863) #transformer</code></li>
<li>Summary: <p>Representations from transformer-based unidirectional language models are
known to be effective at predicting brain responses to natural language.
However, most studies comparing language models to brains have used GPT-2 or
similarly sized language models. Here we tested whether larger open-source
models such as those from the OPT and LLaMA families are better at predicting
brain responses recorded using fMRI. Mirroring scaling results from other
contexts, we found that brain prediction performance scales log-linearly with
model size from 125M to 30B parameter models, with ~15% increased encoding
performance as measured by correlation with a held-out test set across 3
subjects. Similar log-linear behavior was observed when scaling the size of the
fMRI training set. We also characterized scaling for acoustic encoding models
that use HuBERT, WavLM, and Whisper, and we found comparable improvements with
model size. A noise ceiling analysis of these large, high-performance encoding
models showed that performance is nearing the theoretical maximum for brain
areas such as the precuneus and higher auditory cortex. These results suggest
that increasing scale in both models and data will yield incredibly effective
models of language processing in the brain, enabling better scientific
understanding as well as applications such as decoding.
</p></li>
</ul>

<h3>Title: Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11424">http://arxiv.org/abs/2305.11424</a></li>
<li>Code URL: <a href="https://github.com/czczup/gptrans">https://github.com/czczup/gptrans</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11424] Graph Propagation Transformer for Graph Representation Learning](http://arxiv.org/abs/2305.11424) #transformer</code></li>
<li>Summary: <p>This paper presents a novel transformer architecture for graph representation
learning. The core insight of our method is to fully consider the information
propagation among nodes and edges in a graph when building the attention module
in the transformer blocks. Specifically, we propose a new attention mechanism
called Graph Propagation Attention (GPA). It explicitly passes the information
among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and
edge-to-node, which is essential for learning graph-structured data. On this
basis, we design an effective transformer architecture named Graph Propagation
Transformer (GPTrans) to further help learn graph data. We verify the
performance of GPTrans in a wide range of graph learning experiments on several
benchmark datasets. These results show that our method outperforms many
state-of-the-art transformer-based graph models with better performance. The
code will be released at https://github.com/czczup/GPTrans.
</p></li>
</ul>

<h3>Title: Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11854">http://arxiv.org/abs/2305.11854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11854] Multimodal Web Navigation with Instruction-Finetuned Foundation Models](http://arxiv.org/abs/2305.11854) #transformer</code></li>
<li>Summary: <p>The progress of autonomous web navigation has been hindered by the dependence
on billions of exploratory interactions via online reinforcement learning, and
domain-specific model designs that make it difficult to leverage generalization
from rich out-of-domain data. In this work, we study data-driven offline
training for web agents with vision-language foundation models. We propose an
instruction-following multimodal agent, WebGUM, that observes both webpage
screenshots and HTML pages and outputs web navigation actions, such as click
and type. WebGUM is trained by jointly finetuning an instruction-finetuned
language model and a vision transformer on a large corpus of demonstrations. We
empirically demonstrate this recipe improves the agent's ability of grounded
visual perception, HTML comprehension and multi-step reasoning, outperforming
prior works by a significant margin. On the MiniWoB benchmark, we improve over
the previous best offline methods by more than 31.9%, being close to reaching
online-finetuned SoTA. On the WebShop benchmark, our 3-billion-parameter model
achieves superior performance to the existing SoTA, PaLM-540B. We also collect
347K high-quality demonstrations using our trained models, 38 times larger than
prior work, and make them available to promote future research in this
direction.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: JoIN: Joint GANs Inversion for Intrinsic Image Decomposition. (arXiv:2305.11321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11321">http://arxiv.org/abs/2305.11321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11321] JoIN: Joint GANs Inversion for Intrinsic Image Decomposition](http://arxiv.org/abs/2305.11321) #generative</code></li>
<li>Summary: <p>In this work, we propose to solve ill-posed inverse imaging problems using a
bank of Generative Adversarial Networks (GAN) as a prior and apply our method
to the case of Intrinsic Image Decomposition for faces and materials. Our
method builds on the demonstrated success of GANs to capture complex image
distributions. At the core of our approach is the idea that the latent space of
a GAN is a well-suited optimization domain to solve inverse problems. Given an
input image, we propose to jointly inverse the latent codes of a set of GANs
and combine their outputs to reproduce the input. Contrary to most GAN
inversion methods which are limited to inverting only a single GAN, we
demonstrate that it is possible to maintain distribution priors while inverting
several GANs jointly. We show that our approach is modular, allowing various
forward imaging models, that it can successfully decompose both synthetic and
real images, and provides additional advantages such as leveraging properties
of GAN latent space for image relighting.
</p></li>
</ul>

<h3>Title: Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11351">http://arxiv.org/abs/2305.11351</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11351] Data Redaction from Conditional Generative Models](http://arxiv.org/abs/2305.11351) #generative</code></li>
<li>Summary: <p>Deep generative models are known to produce undesirable samples such as
harmful content. Traditional mitigation methods include re-training from
scratch, filtering, or editing; however, these are either computationally
expensive or can be circumvented by third parties. In this paper, we take a
different approach and study how to post-edit an already-trained conditional
generative model so that it redacts certain conditionals that will, with high
probability, lead to undesirable content. This is done by distilling the
conditioning network in the models, giving a solution that is effective,
efficient, controllable, and universal for a class of deep generative models.
We conduct experiments on redacting prompts in text-to-image models and
redacting voices in text-to-speech models. Our method is computationally light,
leads to better redaction quality and robustness than baseline methods while
still retaining high generation quality.
</p></li>
</ul>

<h3>Title: PointGPT: Auto-regressively Generative Pre-training from Point Clouds. (arXiv:2305.11487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11487">http://arxiv.org/abs/2305.11487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11487] PointGPT: Auto-regressively Generative Pre-training from Point Clouds](http://arxiv.org/abs/2305.11487) #generative</code></li>
<li>Summary: <p>Large language models (LLMs) based on the generative pre-training transformer
(GPT) have demonstrated remarkable effectiveness across a diverse range of
downstream tasks. Inspired by the advancements of the GPT, we present PointGPT,
a novel approach that extends the concept of GPT to point clouds, addressing
the challenges associated with disorder properties, low information density,
and task gaps. Specifically, a point cloud auto-regressive generation task is
proposed to pre-train transformer models. Our method partitions the input point
cloud into multiple point patches and arranges them in an ordered sequence
based on their spatial proximity. Then, an extractor-generator based
transformer decoder, with a dual masking strategy, learns latent
representations conditioned on the preceding point patches, aiming to predict
the next one in an auto-regressive manner. Our scalable approach allows for
learning high-capacity models that generalize well, achieving state-of-the-art
performance on various downstream tasks. In particular, our approach achieves
classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the
ScanObjectNN dataset, outperforming all other transformer models. Furthermore,
our method also attains new state-of-the-art accuracies on all four few-shot
learning benchmarks.
</p></li>
</ul>

<h3>Title: A Unified Prompt-Guided In-Context Inpainting Framework for Reference-based Image Manipulations. (arXiv:2305.11577v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11577">http://arxiv.org/abs/2305.11577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11577] A Unified Prompt-Guided In-Context Inpainting Framework for Reference-based Image Manipulations](http://arxiv.org/abs/2305.11577) #generative</code></li>
<li>Summary: <p>Recent advancements in Text-to-Image (T2I) generative models have yielded
impressive results in generating high-fidelity images based on consistent text
prompts. However, there is a growing interest in exploring the potential of
these models for more diverse reference-based image manipulation tasks that
require spatial understanding and visual context. Previous approaches have
achieved this by incorporating additional control modules or fine-tuning the
generative models specifically for each task until convergence. In this paper,
we propose a different perspective. We conjecture that current large-scale T2I
generative models already possess the capability to perform these tasks but are
not fully activated within the standard generation process. To unlock these
capabilities, we introduce a unified Prompt-Guided In-Context inpainting (PGIC)
framework, which leverages large-scale T2I models to re-formulate and solve
reference-guided image manipulations. In the PGIC framework, the reference and
masked target are stitched together as a new input for the generative models,
enabling the filling of masked regions as producing final results. Furthermore,
we demonstrate that the self-attention modules in T2I models are well-suited
for establishing spatial correlations and efficiently addressing challenging
reference-guided manipulations. These large T2I models can be effectively
driven by task-specific prompts with minimal training cost or even with frozen
backbones. We synthetically evaluate the effectiveness of the proposed PGIC
framework across various tasks, including reference-guided image inpainting,
faithful inpainting, outpainting, local super-resolution, and novel view
synthesis. Our results show that PGIC achieves significantly better performance
while requiring less computation compared to other fine-tuning based
approaches.
</p></li>
</ul>

<h3>Title: A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images. (arXiv:2305.11795v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11795">http://arxiv.org/abs/2305.11795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11795] A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images](http://arxiv.org/abs/2305.11795) #generative</code></li>
<li>Summary: <p>The highly realistic image quality achieved by current image generative
models has many academic and industrial applications. To limit the use of such
models to benign applications, though, it is necessary that tools to
conclusively detect whether an image has been generated synthetically or not
are developed. For this reason, several detectors have been developed providing
excellent performance in computer vision applications, however, they can not be
applied as they are to multispectral satellite images, and hence new models
must be trained. In general, two-class classifiers can achieve very good
detection accuracies, however they are not able to generalise to image domains
and generative models architectures different than those used during training.
For this reason, in this paper, we propose a one-class classifier based on
Vector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome the
limitations of two-class classifiers. First, we emphasize the generalization
problem that binary classifiers suffer from by training and testing an
EfficientNet-B4 architecture on multiple multispectral datasets. Then we show
that, since the VQ-VAE 2 based classifier is trained only on pristine images,
it is able to detect images belonging to different domains and generated by
architectures that have not been used during training. Last, we compare the two
classifiers head-to-head on the same generated datasets, highlighting the
superiori generalization capabilities of the VQ-VAE 2-based detector.
</p></li>
</ul>

<h3>Title: Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation. (arXiv:2305.11317v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11317">http://arxiv.org/abs/2305.11317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11317] Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation](http://arxiv.org/abs/2305.11317) #generative</code></li>
<li>Summary: <p>The field of text-to-image (T2I) generation has garnered significant
attention both within the research community and among everyday users. Despite
the advancements of T2I models, a common issue encountered by users is the need
for repetitive editing of input prompts in order to receive a satisfactory
image, which is time-consuming and labor-intensive. Given the demonstrated text
generation power of large-scale language models, such as GPT-k, we investigate
the potential of utilizing such models to improve the prompt editing process
for T2I generation. We conduct a series of experiments to compare the common
edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting
T2I, and examine factors that may influence this process. We found that GPT-k
models focus more on inserting modifiers while humans tend to replace words and
phrases, which includes changes to the subject matter. Experimental results
show that GPT-k are more effective in adjusting modifiers rather than
predicting spontaneous changes in the primary subject matters. Adopting the
edit suggested by GPT-k models may reduce the percentage of remaining edits by
20-30%.
</p></li>
</ul>

<h3>Title: Algorithmic failure as a humanities methodology: machine learning's mispredictions identify rich cases for qualitative analysis. (arXiv:2305.11663v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11663">http://arxiv.org/abs/2305.11663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11663] Algorithmic failure as a humanities methodology: machine learning's mispredictions identify rich cases for qualitative analysis](http://arxiv.org/abs/2305.11663) #generative</code></li>
<li>Summary: <p>This commentary tests a methodology proposed by Munk et al. (2022) for using
failed predictions in machine learning as a method to identify ambiguous and
rich cases for qualitative analysis. Using a dataset describing actions
performed by fictional characters interacting with machine vision technologies
in 500 artworks, movies, novels and videogames, I trained a simple machine
learning algorithm (using the kNN algorithm in R) to predict whether or not an
action was active or passive using only information about the fictional
characters. Predictable actions were generally unemotional and unambiguous
activities where machine vision technologies were treated as simple tools.
Unpredictable actions, that is, actions that the algorithm could not correctly
predict, were more ambivalent and emotionally loaded, with more complex power
relationships between characters and technologies. The results thus support
Munk et al.'s theory that failed predictions can be productively used to
identify rich cases for qualitative analysis. This test goes beyond simply
replicating Munk et al.'s results by demonstrating that the method can be
applied to a broader humanities domain, and that it does not require complex
neural networks but can also work with a simpler machine learning algorithm.
Further research is needed to develop an understanding of what kinds of data
the method is useful for and which kinds of machine learning are most
generative. To support this, the R code required to produce the results is
included so the test can be replicated. The code can also be reused or adapted
to test the method on other datasets.
</p></li>
</ul>

<h3>Title: Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11790">http://arxiv.org/abs/2305.11790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11790] Prompting with Pseudo-Code Instructions](http://arxiv.org/abs/2305.11790) #generative</code></li>
<li>Summary: <p>Prompting with natural language instructions has recently emerged as a
popular method of harnessing the capabilities of large language models. Given
the inherent ambiguity present in natural language, it is intuitive to consider
the possible advantages of prompting with less ambiguous prompt styles, such as
the use of pseudo-code.
</p></li>
</ul>

<p>In this paper we explore if prompting via pseudo-code instructions helps
improve the performance of pre-trained language models. We manually create a
dataset of pseudo-code prompts for 132 different tasks spanning classification,
QA and generative language tasks, sourced from the Super-NaturalInstructions
dataset. Using these prompts along with their counterparts in natural language,
we study their performance on two LLM families - BLOOM and CodeGen. Our
experiments show that using pseudo-code instructions leads to better results,
with an average increase (absolute) of 7-16 points in F1 scores for
classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE
scores across all tasks. We include detailed ablation studies which indicate
that code comments, docstrings, and the structural clues encoded in pseudo-code
all contribute towards the improvement in performance.
</p>
<p>To the best of our knowledge our work is the first to demonstrate how
pseudo-code prompts can be helpful in improving the performance of pre-trained
LMs.
</p>

<h3>Title: SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models. (arXiv:2305.11840v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11840">http://arxiv.org/abs/2305.11840</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11840] SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models](http://arxiv.org/abs/2305.11840) #generative</code></li>
<li>Summary: <p>Stereotype benchmark datasets are crucial to detect and mitigate social
stereotypes about groups of people in NLP models. However, existing datasets
are limited in size and coverage, and are largely restricted to stereotypes
prevalent in the Western society. This is especially problematic as language
technologies gain hold across the globe. To address this gap, we present
SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative
capabilities of large language models such as PaLM, and GPT-3, and leveraging a
globally diverse rater pool to validate the prevalence of those stereotypes in
society. SeeGULL is in English, and contains stereotypes about identity groups
spanning 178 countries across 8 different geo-political regions across 6
continents, as well as state-level identities within the US and India. We also
include fine-grained offensiveness scores for different stereotypes and
demonstrate their global disparities. Furthermore, we include comparative
annotations about the same groups by annotators living in the region vs. those
that are based in North America, and demonstrate that within-region stereotypes
about groups differ from those prevalent in North America. CONTENT WARNING:
This paper contains stereotype examples that may be offensive.
</p></li>
</ul>

<h3>Title: On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11283">http://arxiv.org/abs/2305.11283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11283] On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation](http://arxiv.org/abs/2305.11283) #generative</code></li>
<li>Summary: <p>In this paper, we study the statistical efficiency of Reinforcement Learning
in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function
approximation. We introduce a new concept called Mean-Field Model-Based Eluder
Dimension (MBED), which subsumes a rich family of Mean-Field RL problems.
Additionally, we propose algorithms based on Optimistic Maximal Likelihood
Estimation, which can return an $\epsilon$-optimal policy for MFC or an
$\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial
w.r.t. relevant parameters and independent of the number of states, actions and
the number of agents. Notably, our results only require a mild assumption of
Lipschitz continuity on transition dynamics and avoid strong structural
assumptions in previous work. Finally, in the tabular setting, given the access
to a generative model, we establish an exponential lower bound for MFC setting,
while providing a novel sample-efficient model elimination algorithm to
approximate equilibrium in MFG setting. Our results reveal a fundamental
separation between RL for single-agent, MFC, and MFG from the sample efficiency
perspective.
</p></li>
</ul>

<h3>Title: Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11400">http://arxiv.org/abs/2305.11400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11400] Few-Shot Continual Learning for Conditional Generative Adversarial Networks](http://arxiv.org/abs/2305.11400) #generative</code></li>
<li>Summary: <p>In few-shot continual learning for generative models, a target mode must be
learned with limited samples without adversely affecting the previously learned
modes. In this paper, we propose a new continual learning approach for
conditional generative adversarial networks (cGAN) based on a new mode-affinity
measure for generative modeling. Our measure is entirely based on the cGAN's
discriminator and can identify the existing modes that are most similar to the
target. Subsequently, we expand the continual learning model by including the
target mode using a weighted label derived from those of the closest modes. To
prevent catastrophic forgetting, we first generate labeled data samples using
the cGAN's generator, and then train the cGAN model for the target mode while
memory replaying with the generated data. Our experimental results demonstrate
the efficacy of our approach in improving the generation performance over the
baselines and the state-of-the-art approaches for various standard datasets
while utilizing fewer training samples.
</p></li>
</ul>

<h3>Title: Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11463">http://arxiv.org/abs/2305.11463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11463] Generative Sliced MMD Flows with Riesz Kernels](http://arxiv.org/abs/2305.11463) #generative</code></li>
<li>Summary: <p>Maximum mean discrepancy (MMD) flows suffer from high computational costs in
large scale computations. In this paper, we show that MMD flows with Riesz
kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which
allow for their efficient computation. First, the MMD of Riesz kernels
coincides with the MMD of their sliced version. As a consequence, the
computation of gradients of MMDs can be performed in the one-dimensional
setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce
the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical
measures with $M$ and $N$ support points. For the implementations we
approximate the gradient of the sliced MMD by using only a finite number $P$ of
slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where
$d$ is the data dimension. These results enable us to train generative models
by approximating MMD gradient flows by neural networks even for large scale
applications. We demonstrate the efficiency of our model by image generation on
MNIST, FashionMNIST and CIFAR10.
</p></li>
</ul>

<h3>Title: TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. (arXiv:2305.11567v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11567">http://arxiv.org/abs/2305.11567</a></li>
<li>Code URL: <a href="https://github.com/alexandervnikitin/tsgm">https://github.com/alexandervnikitin/tsgm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11567] TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series](http://arxiv.org/abs/2305.11567) #generative</code></li>
<li>Summary: <p>Temporally indexed data are essential in a wide range of fields and of
interest to machine learning researchers. Time series data, however, are often
scarce or highly sensitive, which precludes the sharing of data between
researchers and industrial organizations and the application of existing and
new data-intensive ML methods. A possible solution to this bottleneck is to
generate synthetic data. In this work, we introduce Time Series Generative
Modeling (TSGM), an open-source framework for the generative modeling of
synthetic time series. TSGM includes a broad repertoire of machine learning
methods: generative models, probabilistic, and simulator-based approaches. The
framework enables users to evaluate the quality of the produced data from
different angles: similarity, downstream effectiveness, predictive consistency,
diversity, and privacy. The framework is extensible, which allows researchers
to rapidly implement their own methods and compare them in a shareable
environment. TSGM was tested on open datasets and in production and proved to
be beneficial in both cases. Additionally to the library, the project allows
users to employ command line interfaces for synthetic data generation which
lowers the entry threshold for those without a programming background.
</p></li>
</ul>

<h3>Title: RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11699">http://arxiv.org/abs/2305.11699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11699] RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design](http://arxiv.org/abs/2305.11699) #generative</code></li>
<li>Summary: <p>Identifying molecules that exhibit some pre-specified properties is a
difficult problem to solve. In the last few years, deep generative models have
been used for molecule generation. Deep Graph Variational Autoencoders are
among the most powerful machine learning tools with which it is possible to
address this problem. However, existing methods struggle in capturing the true
data distribution and tend to be computationally expensive. In this work, we
propose RGCVAE, an efficient and effective Graph Variational Autoencoder based
on: (i) an encoding network exploiting a new powerful Relational Graph
Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to
several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE
shows state-of-the-art molecule generation performance while being
significantly faster to train.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11490">http://arxiv.org/abs/2305.11490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11490] LLM Itself Can Read and Generate CXR Images](http://arxiv.org/abs/2305.11490) #large language model</code></li>
<li>Summary: <p>Building on the recent remarkable development of large language models
(LLMs), active attempts are being made to extend the utility of LLMs to
multimodal tasks. There have been previous efforts to link language and visual
information, and attempts to add visual capabilities to LLMs are ongoing as
well. However, existing attempts use LLMs only as image decoders and no attempt
has been made to generate images in the same line as the natural language. By
adopting a VQ-GAN framework in which latent representations of images are
treated as a kind of text tokens, we present a novel method to fine-tune a
pre-trained LLM to read and generate images like text without any structural
changes, extra training objectives, or the need for training an ad-hoc network
while still preserving the of the instruction-following capability of the LLM.
We apply this framework to chest X-ray (CXR) image and report generation tasks
as it is a domain in which translation of complex information between visual
and language domains is important. The code will soon be made publicly
available.
</p></li>
</ul>

<h3>Title: Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11186">http://arxiv.org/abs/2305.11186</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11186] Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt](http://arxiv.org/abs/2305.11186) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs), armed with billions of parameters, exhibit
exceptional performance across a wide range of Natural Language Processing
(NLP) tasks. However, they present a significant computational challenge during
inference, especially when deploying on common hardware such as single GPUs. As
such, minimizing the latency of LLM inference by curtailing computational and
memory requirements, though achieved through compression, becomes critically
important. However, this process inevitably instigates a trade-off between
efficiency and accuracy, as compressed LLMs typically experience a reduction in
predictive precision. In this research, we introduce an innovative perspective:
to optimize this trade-off, compressed LLMs require a unique input format that
varies from that of the original models. Our findings indicate that the
generation quality in a compressed LLM can be markedly improved for specific
queries by selecting prompts with precision. Capitalizing on this insight, we
introduce a prompt learning paradigm that cultivates an additive prompt over a
compressed LLM to bolster their accuracy. Our empirical results imply that
through our strategic prompt utilization, compressed LLMs can match, and
occasionally even exceed, the accuracy of the original models. Moreover, we
demonstrated that these learned prompts have a certain degree of
transferability across various datasets, tasks, and compression levels. These
insights shine a light on new possibilities for enhancing the balance between
accuracy and efficiency in LLM inference. Specifically, they underscore the
importance of judicious input editing to a compressed large model, hinting at
potential advancements in scaling LLMs on common hardware.
</p></li>
</ul>

<h3>Title: LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11206">http://arxiv.org/abs/2305.11206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11206] LIMA: Less Is More for Alignment](http://arxiv.org/abs/2305.11206) #large language model</code></li>
<li>Summary: <p>Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.
</p></li>
</ul>

<h3>Title: Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11243">http://arxiv.org/abs/2305.11243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11243] Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses](http://arxiv.org/abs/2305.11243) #large language model</code></li>
<li>Summary: <p>Developmental psychologists have spent decades devising experiments to test
the intelligence and knowledge of infants and children, tracing the origin of
crucial concepts and capacities. Moreover, experimental techniques in
developmental psychology have been carefully designed to discriminate the
cognitive capacities that underlie particular behaviors. We propose that using
classical experiments from child development is a particularly effective way to
probe the computational abilities of AI models, in general, and LLMs in
particular. First, the methodological techniques of developmental psychology,
such as the use of novel stimuli to control for past experience or control
conditions to determine whether children are using simple associations, can be
equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs
in this way can tell us whether the information that is encoded in text is
sufficient to enable particular responses, or whether those responses depend on
other kinds of information, such as information from exploration of the
physical world. In this work we adapt classical developmental experiments to
evaluate the capabilities of LaMDA, a large language model from Google. We
propose a novel LLM Response Score (LRS) metric which can be used to evaluate
other language models, such as GPT. We find that LaMDA generates appropriate
responses that are similar to those of children in experiments involving social
understanding, perhaps providing evidence that knowledge of these domains is
discovered through language. On the other hand, LaMDA's responses in early
object and action understanding, theory of mind, and especially causal
reasoning tasks are very different from those of young children, perhaps
showing that these domains require more real-world, self-initiated exploration
and cannot simply be learned from patterns in language input.
</p></li>
</ul>

<h3>Title: Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11364">http://arxiv.org/abs/2305.11364</a></li>
<li>Code URL: <a href="https://github.com/pair-code/interpretability">https://github.com/pair-code/interpretability</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11364] Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models](http://arxiv.org/abs/2305.11364) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) can be used to generate smaller, more refined
datasets via few-shot prompting for benchmarking, fine-tuning or other use
cases. However, understanding and evaluating these datasets is difficult, and
the failure modes of LLM-generated data are still not well understood.
Specifically, the data can be repetitive in surprising ways, not only
semantically but also syntactically and lexically. We present LinguisticLens, a
novel inter-active visualization tool for making sense of and analyzing
syntactic diversity of LLM-generated datasets. LinguisticLens clusters text
along syntactic, lexical, and semantic axes. It supports hierarchical
visualization of a text dataset, allowing users to quickly scan for an overview
and inspect individual examples. The live demo is available at
shorturl.at/zHOUV.
</p></li>
</ul>

<h3>Title: Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11426">http://arxiv.org/abs/2305.11426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11426] Post Hoc Explanations of Language Models Can Improve Language Models](http://arxiv.org/abs/2305.11426) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
performing complex tasks. Moreover, recent research has shown that
incorporating human-annotated rationales (e.g., Chain-of- Thought prompting)
during in-context learning can significantly enhance the performance of these
models, particularly on tasks that require reasoning capabilities. However,
incorporating such rationales poses challenges in terms of scalability as this
requires a high degree of human involvement. In this work, we present a novel
framework, Amplifying Model Performance by Leveraging In-Context Learning with
Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges
by automating the process of rationale generation. To this end, we leverage
post hoc explanation methods which output attribution scores (explanations)
capturing the influence of each of the input features on model predictions.
More specifically, we construct automated natural language rationales that
embed insights from post hoc explanations to provide corrective signals to
LLMs. Extensive experimentation with real-world datasets demonstrates that our
framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%
over a wide range of tasks, including those where prior approaches which rely
on human-annotated rationales such as Chain-of-Thought prompting fall short.
Our work makes one of the first attempts at highlighting the potential of post
hoc explanations as valuable tools for enhancing the effectiveness of LLMs.
Furthermore, we conduct additional empirical analyses and ablation studies to
demonstrate the impact of each of the components of AMPLIFY, which, in turn,
lead to critical insights for refining in-context learning.
</p></li>
</ul>

<h3>Title: RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. (arXiv:2305.11499v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11499">http://arxiv.org/abs/2305.11499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11499] RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought](http://arxiv.org/abs/2305.11499) #large language model</code></li>
<li>Summary: <p>Large language Models (LLMs) have achieved promising performance on
arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting. However, LLMs face challenges in maintaining factual consistency
during reasoning, exhibiting tendencies to condition overlooking, question
misinterpretation, and condition hallucination over given problems. Existing
methods use coarse-grained feedback (e.g., whether the answer is correct) to
improve factual consistency. In this work, we propose RCoT (Reversing
Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by
automatically detecting and rectifying factual inconsistency in LLMs' generated
solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct
the problem based on generated solutions. Then fine-grained comparisons between
the original problem and the reconstructed problem expose the factual
inconsistency in the original solutions. To rectify the solution, RCoT
formulates detected factual inconsistency into fine-grained feedback to guide
LLMs in revising solutions. Experimental results demonstrate consistent
improvements of RCoT over standard CoT across seven arithmetic datasets.
Moreover, we find that manually written fine-grained feedback can dramatically
improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on
GSM8K), encouraging the community to further explore the fine-grained feedback
generation methods.
</p></li>
</ul>

<h3>Title: Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11541">http://arxiv.org/abs/2305.11541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11541] Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering](http://arxiv.org/abs/2305.11541) #large language model</code></li>
<li>Summary: <p>Large Language Model (LLM) has gained popularity and achieved remarkable
results in open-domain tasks, but its performance in real industrial
domain-specific scenarios is average since there is no specific knowledge in
it. This issue has attracted widespread attention, but there are few relevant
benchmarks available. In this paper, we provide a benchmark Question Answering
(QA) dataset named MSQA, which is about Microsoft products and IT technical
problems encountered by customers. This dataset contains industry
cloud-specific QA knowledge, which is not available for general LLM, so it is
well suited for evaluating methods aimed at improving domain-specific
capabilities of LLM. In addition, we propose a new model interaction paradigm
that can empower LLM to achieve better performance on domain-specific tasks
where it is not proficient. Extensive experiments demonstrate that the approach
following our model fusion framework outperforms the commonly used LLM with
retrieval methods.
</p></li>
</ul>

<h3>Title: ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11554">http://arxiv.org/abs/2305.11554</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11554] ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings](http://arxiv.org/abs/2305.11554) #large language model</code></li>
<li>Summary: <p>Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our
approach represents each $\underline{tool}$ as a to$\underline{ken}$
($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.
</p></li>
</ul>

<h3>Title: Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11595">http://arxiv.org/abs/2305.11595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11595] Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate](http://arxiv.org/abs/2305.11595) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated impressive zero-shot or
few-shot commonsense reasoning performance on various natural language
processing (NLP) tasks. However, despite their strong commonsense reasoning
abilities, LLMs still exhibit various kinds of inconsistency problems. While
previous researches mainly focused on the self-consistency within a single LLM,
we propose to explore the inter-consistency issue between two or more LLMs,
which is critical for diverse and precise decision-making processes. Since the
LLMs possess human-like intelligence after instruction tuning and reinforcement
learning with human feedback (RLHF), we design a formal debate framework to
delve into the inter-consistency problem among LLMs with three-stage debate:
fair debate, mismatched debate, and roundtable debate. Through extensive
experiments on 7 commonsense reasoning datasets, LLMs not only become more
inter-consistent by compromising and refuting but also achieve higher
performance and stronger interpretability. Furthermore, we find a much stronger
LLM would be dominant in mismatched debates, while it will be easily misled by
relatively weaker LLMs in a more complex debate scenario such as roundtable
debate.
</p></li>
</ul>

<h3>Title: LLM-Pruner: On the Structural Pruning of Large Language Models. (arXiv:2305.11627v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11627">http://arxiv.org/abs/2305.11627</a></li>
<li>Code URL: <a href="https://github.com/horseee/llm-pruner">https://github.com/horseee/llm-pruner</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11627] LLM-Pruner: On the Structural Pruning of Large Language Models](http://arxiv.org/abs/2305.11627) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
both the deployment, inference, and training stages. With LLM being a
general-purpose task solver, we explore its compression in a task-agnostic
manner, which aims to preserve the multi-task solving and language generation
ability of the original LLM. One challenge to achieving this is the enormous
size of the training corpus of LLM, which makes both data transfer and model
post-training over-burdensome. Thus, we tackle the compression of LLMs within
the bound of two constraints: being task-agnostic and minimizing the reliance
on the original training dataset. Our method, named LLM-Pruner, adopts
structural pruning that selectively removes non-critical coupled structures
based on gradient information, maximally preserving the majority of the LLM's
functionality. To this end, the performance of pruned models can be efficiently
recovered through tuning techniques, LoRA, in merely 3 hours, requiring only
50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,
and ChatGLM, and demonstrate that the compressed models still exhibit
satisfactory capabilities in zero-shot classification and generation. The code
is available at: https://github.com/horseee/LLM-Pruner
</p></li>
</ul>

<h3>Title: Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11662">http://arxiv.org/abs/2305.11662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11662] Evaluating task understanding through multilingual consistency: A ChatGPT case study](http://arxiv.org/abs/2305.11662) #large language model</code></li>
<li>Summary: <p>At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model's understanding and
simultaneously addressing the important topic of multilingualism. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.
</p></li>
</ul>

<h3>Title: CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11738">http://arxiv.org/abs/2305.11738</a></li>
<li>Code URL: <a href="https://github.com/microsoft/ProphetNet">https://github.com/microsoft/ProphetNet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11738] CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](http://arxiv.org/abs/2305.11738) #large language model</code></li>
<li>Summary: <p>Recent developments in large language models (LLMs) have been impressive.
However, these models sometimes show inconsistencies and problematic behavior,
such as hallucinating facts, generating flawed code, or creating offensive and
toxic content. Unlike these models, humans typically utilize external tools to
cross-check and refine their initial content, like using a search engine for
fact-checking, or a code interpreter for debugging. Inspired by this
observation, we introduce a framework called CRITIC that allows LLMs, which are
essentially "black boxes" to validate and progressively amend their own outputs
in a manner similar to human interaction with tools. More specifically,
starting with an initial output, CRITIC interacts with appropriate tools to
evaluate certain aspects of the text, and then revises the output based on the
feedback obtained during this validation process. Comprehensive evaluations
involving free-form question answering, mathematical program synthesis, and
toxicity reduction demonstrate that CRITIC consistently enhances the
performance of LLMs. Meanwhile, our research highlights the crucial importance
of external feedback in promoting the ongoing self-improvement of LLMs.
</p></li>
</ul>

<h3>Title: HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11747">http://arxiv.org/abs/2305.11747</a></li>
<li>Code URL: <a href="https://github.com/RUCAIBox/HaluEval">https://github.com/RUCAIBox/HaluEval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11747] HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](http://arxiv.org/abs/2305.11747) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs), such as ChatGPT, are prone to generate
hallucinations, \ie content that conflicts with the source or cannot be
verified by the factual knowledge. To understand what types of content and to
which extent LLMs are apt to hallucinate, we introduce the Hallucination
Evaluation for Large Language Models (HELMA) benchmark, a large collection of
generated and human-annotated hallucinated samples for evaluating the
performance of LLMs in recognizing and alleviating hallucination. To generate
these samples, we propose a ChatGPT-based two-step framework, \ie
sampling-then-filtering. Specifically, we first adopt two different sampling
methods to generate hallucinated samples based on instructions, and then use an
example-enhanced filtering method to select the best one. Furthermore, we also
hire some human labelers to annotate the hallucinations in ChatGPT responses.
The empirical results suggest that ChatGPT has some probabilities to generate
hallucinations and existing LLMs face great challenges in recognizing the
hallucinations in text. In addition, the performance can be improved by
providing external knowledge or adding reasoning steps. Our benchmark can be
accessed at https://github.com/RUCAIBox/HELMA.
</p></li>
</ul>

<h3>Title: Cross-Lingual Supervision improves Large Language Models Pre-training. (arXiv:2305.11778v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11778">http://arxiv.org/abs/2305.11778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11778] Cross-Lingual Supervision improves Large Language Models Pre-training](http://arxiv.org/abs/2305.11778) #large language model</code></li>
<li>Summary: <p>The recent rapid progress in pre-training Large Language Models has relied on
using self-supervised language modeling objectives like next token prediction
or span corruption. On the other hand, Machine Translation Systems are mostly
trained using cross-lingual supervision that requires aligned data between
source and target languages. We demonstrate that pre-training Large Language
Models on a mixture of a self-supervised Language Modeling objective and the
supervised Machine Translation objective, therefore including cross-lingual
parallel data during pre-training, yields models with better in-context
learning abilities. As pre-training is a very resource-intensive process and a
grid search on the best mixing ratio between the two objectives is
prohibitively expensive, we propose a simple yet effective strategy to learn it
during pre-training.
</p></li>
</ul>

<h3>Title: How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11853">http://arxiv.org/abs/2305.11853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11853] How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings](http://arxiv.org/abs/2305.11853) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) with in-context learning have demonstrated
remarkable capability in the text-to-SQL task. Previous research has prompted
LLMs with various demonstration-retrieval strategies and intermediate reasoning
steps to enhance the performance of LLMs. However, those works often employ
varied strategies when constructing the prompt text for text-to-SQL inputs,
such as databases and demonstration examples. This leads to a lack of
comparability in both the prompt constructions and their primary contributions.
Furthermore, selecting an effective prompt construction has emerged as a
persistent problem for future research. To address this limitation, we
comprehensively investigate the impact of prompt constructions across various
settings and provide insights for future work.
</p></li>
</ul>

<h3>Title: Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11862">http://arxiv.org/abs/2305.11862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11862] Reducing Sequence Length by Predicting Edit Operations with Large Language Models](http://arxiv.org/abs/2305.11862) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable performance in
various tasks and gained significant attention. LLMs are also used for local
sequence transduction tasks, including grammatical error correction (GEC) and
formality style transfer, where most tokens in a source text are kept
unchanged. However, it is inefficient to generate all target tokens because a
prediction error of a target token may cause a catastrophe in predicting
subsequent tokens and because the computational cost grows quadratically with
the target sequence length. This paper proposes to predict a set of edit
operations for the source text for local sequence transduction tasks.
Representing an edit operation with a span of the source text and changed
tokens, we can reduce the length of the target sequence and thus the
computational cost for inference. We apply instruction tuning for LLMs on the
supervision data of edit operations. Experiments show that the proposed method
achieves comparable performance to the baseline in four tasks, paraphrasing,
formality style transfer, GEC, and text simplification, despite reducing the
length of the target text by as small as 21\%. Furthermore, we report that the
instruction tuning with the proposed method achieved the state-of-the-art
performance in the four tasks.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems. (arXiv:2305.11419v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11419">http://arxiv.org/abs/2305.11419</a></li>
<li>Code URL: <a href="https://github.com/mmontielpz/jetseg">https://github.com/mmontielpz/jetseg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11419] JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems](http://arxiv.org/abs/2305.11419) #segmentation</code></li>
<li>Summary: <p>Real-time semantic segmentation is a challenging task that requires
high-accuracy models with low-inference times. Implementing these models on
embedded systems is limited by hardware capability and memory usage, which
produces bottlenecks. We propose an efficient model for real-time semantic
segmentation called JetSeg, consisting of an encoder called JetNet, and an
improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and
includes two main components: a new light-weight efficient block called
JetBlock, that reduces the number of parameters minimizing memory usage and
inference time without sacrificing accuracy; a new strategy that involves the
combination of asymmetric and non-asymmetric convolutions with
depthwise-dilated convolutions called JetConv, a channel shuffle operation,
light-weight activation functions, and a convenient number of group
convolutions for embedded systems, and an innovative loss function named
JetLoss, which integrates the Precision, Recall, and IoUB losses to improve
semantic segmentation and reduce computational complexity. Experiments
demonstrate that JetSeg is much faster on workstation devices and more suitable
for Low-Power GPU-Embedded Systems than existing state-of-the-art models for
real-time semantic segmentation. Our approach outperforms state-of-the-art
real-time encoder-decoder models by reducing 46.70M parameters and 5.14%
GFLOPs, which makes JetSeg up to 2x faster on the NVIDIA Titan RTX GPU and the
Jetson Xavier than other models. The JetSeg code is available at
https://github.com/mmontielpz/jetseg.
</p></li>
</ul>

<h3>Title: RGB-D And Thermal Sensor Fusion: A Systematic Literature Review. (arXiv:2305.11427v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11427">http://arxiv.org/abs/2305.11427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11427] RGB-D And Thermal Sensor Fusion: A Systematic Literature Review](http://arxiv.org/abs/2305.11427) #segmentation</code></li>
<li>Summary: <p>In the last decade, the computer vision field has seen significant progress
in multimodal data fusion and learning, where multiple sensors, including
depth, infrared, and visual, are used to capture the environment across diverse
spectral ranges. Despite these advancements, there has been no systematic and
comprehensive evaluation of fusing RGB-D and thermal modalities to date. While
autonomous driving using LiDAR, radar, RGB, and other sensors has garnered
substantial research interest, along with the fusion of RGB and depth
modalities, the integration of thermal cameras and, specifically, the fusion of
RGB-D and thermal data, has received comparatively less attention. This might
be partly due to the limited number of publicly available datasets for such
applications. This paper provides a comprehensive review of both,
state-of-the-art and traditional methods used in fusing RGB-D and thermal
camera data for various applications, such as site inspection, human tracking,
fault detection, and others. The reviewed literature has been categorised into
technical areas, such as 3D reconstruction, segmentation, object detection,
available datasets, and other related topics. Following a brief introduction
and an overview of the methodology, the study delves into calibration and
registration techniques, then examines thermal visualisation and 3D
reconstruction, before discussing the application of classic feature-based
techniques as well as modern deep learning approaches. The paper concludes with
a discourse on current limitations and potential future research directions. It
is hoped that this survey will serve as a valuable reference for researchers
looking to familiarise themselves with the latest advancements and contribute
to the RGB-DT research field.
</p></li>
</ul>

<h3>Title: Equivariant Multi-Modality Image Fusion. (arXiv:2305.11443v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11443">http://arxiv.org/abs/2305.11443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11443] Equivariant Multi-Modality Image Fusion](http://arxiv.org/abs/2305.11443) #segmentation</code></li>
<li>Summary: <p>Multi-modality image fusion is a technique used to combine information from
different sensors or modalities, allowing the fused image to retain
complementary features from each modality, such as functional highlights and
texture details. However, effectively training such fusion models is difficult
due to the lack of ground truth fusion data. To address this issue, we propose
the Equivariant Multi-Modality imAge fusion (EMMA) paradigm for end-to-end
self-supervised learning. Our approach is based on the prior knowledge that
natural images are equivariant to specific transformations. Thus, we introduce
a novel training framework that includes a fusion module and a learnable
pseudo-sensing module, which allow the network training to follow the
principles of physical sensing and imaging process, and meanwhile satisfy the
equivariant prior for natural images. Our extensive experiments demonstrate
that our method produces high-quality fusion results for both infrared-visible
and medical images, while facilitating downstream multi-modal segmentation and
detection tasks. The code will be released.
</p></li>
</ul>

<h3>Title: CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation. (arXiv:2305.11481v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11481">http://arxiv.org/abs/2305.11481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11481] CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation](http://arxiv.org/abs/2305.11481) #segmentation</code></li>
<li>Summary: <p>Referring image segmentation (RIS) is a fundamental vision-language task that
intends to segment a desired object from an image based on a given natural
language expression. Due to the essentially distinct data properties between
image and text, most of existing methods either introduce complex designs
towards fine-grained vision-language alignment or lack required dense
alignment, resulting in scalability issues or mis-segmentation problems such as
over- or under-segmentation. To achieve effective and efficient fine-grained
feature alignment in the RIS task, we explore the potential of masked
multimodal modeling coupled with self-distillation and propose a novel
cross-modality masked self-distillation framework named CM-MaskSD, in which our
method inherits the transferred knowledge of image-text semantic alignment from
CLIP model to realize fine-grained patch-word feature alignment for better
segmentation accuracy. Moreover, our CM-MaskSD framework can considerably boost
model performance in a nearly parameter-free manner, since it shares weights
between the main segmentation branch and the introduced masked
self-distillation branches, and solely introduces negligible parameters for
coordinating the multimodal features. Comprehensive experiments on three
benchmark datasets (i.e. RefCOCO, RefCOCO+, G-Ref) for the RIS task
convincingly demonstrate the superiority of our proposed framework over
previous state-of-the-art methods.
</p></li>
</ul>

<h3>Title: When SAM Meets Shadow Detection. (arXiv:2305.11513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11513">http://arxiv.org/abs/2305.11513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11513] When SAM Meets Shadow Detection](http://arxiv.org/abs/2305.11513) #segmentation</code></li>
<li>Summary: <p>As a promptable generic object segmentation model, segment anything model
(SAM) has recently attracted significant attention, and also demonstrates its
powerful performance. Nevertheless, it still meets its Waterloo when
encountering several tasks, e.g., medical image segmentation, camouflaged
object detection, etc. In this report, we try SAM on an unexplored popular
task: shadow detection. Specifically, four benchmarks were chosen and evaluated
with widely used metrics. The experimental results show that the performance
for shadow detection using SAM is not satisfactory, especially when comparing
with the elaborate models. Code is available at
https://github.com/LeipingJie/SAMSh.
</p></li>
</ul>

<h3>Title: MaGIC: Multi-modality Guided Image Completion. (arXiv:2305.11818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11818">http://arxiv.org/abs/2305.11818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11818] MaGIC: Multi-modality Guided Image Completion](http://arxiv.org/abs/2305.11818) #segmentation</code></li>
<li>Summary: <p>The vanilla image completion approaches are sensitive to the large missing
regions due to limited available reference information for plausible
generation. To mitigate this, existing methods incorporate the extra cue as a
guidance for image completion. Despite improvements, these approaches are often
restricted to employing a single modality (e.g., segmentation or sketch maps),
which lacks scalability in leveraging multi-modality for more plausible
completion. In this paper, we propose a novel, simple yet effective method for
Multi-modal Guided Image Completion, dubbed MaGIC, which not only supports a
wide range of single modality as the guidance (e.g., text, canny edge, sketch,
segmentation, reference image, depth, and pose), but also adapts to arbitrarily
customized combination of these modalities (i.e., arbitrary multi-modality) for
image completion. For building MaGIC, we first introduce a modality-specific
conditional U-Net (MCU-Net) that injects single-modal signal into a U-Net
denoiser for single-modal guided image completion. Then, we devise a consistent
modality blending (CMB) method to leverage modality signals encoded in multiple
learned MCU-Nets through gradient guidance in latent space. Our CMB is
training-free, and hence avoids the cumbersome joint re-training of different
modalities, which is the secret of MaGIC to achieve exceptional flexibility in
accommodating new modalities for completion. Experiments show the superiority
of MaGIC over state-of-arts and its generalization to various completion tasks
including in/out-painting and local editing. Our project with code and models
is available at yeates.github.io/MaGIC-Page/.
</p></li>
</ul>

<h3>Title: Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information. (arXiv:2305.11553v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11553">http://arxiv.org/abs/2305.11553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11553] Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information](http://arxiv.org/abs/2305.11553) #segmentation</code></li>
<li>Summary: <p>The abstracts of scientific papers consist of premises and conclusions.
Structured abstracts explicitly highlight the conclusion sentences, whereas
non-structured abstracts may have conclusion sentences at uncertain positions.
This implicit nature of conclusion positions makes the automatic segmentation
of scientific abstracts into premises and conclusions a challenging task. In
this work, we empirically explore using Normalized Mutual Information (NMI) for
abstract segmentation. We consider each abstract as a recurrent cycle of
sentences and place segmentation boundaries by greedily optimizing the NMI
score between premises and conclusions. On non-structured abstracts, our
proposed unsupervised approach GreedyCAS achieves the best performance across
all evaluation metrics; on structured abstracts, GreedyCAS outperforms all
baseline methods measured by $P_k$. The strong correlation of NMI to our
evaluation metrics reveals the effectiveness of NMI for abstract segmentation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
