<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Evaluating Digital Tools for Sustainable Agriculture using Causal Inference. (arXiv:2211.03195v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03195">http://arxiv.org/abs/2211.03195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03195] Evaluating Digital Tools for Sustainable Agriculture using Causal Inference](http://arxiv.org/abs/2211.03195)</code></li>
<li>Summary: <p>In contrast to the rapid digitalization of several industries, agriculture
suffers from low adoption of climate-smart farming tools. Even though AI-driven
digital agriculture can offer high-performing predictive functionalities, it
lacks tangible quantitative evidence on its benefits to the farmers. Field
experiments can derive such evidence, but are often costly and time consuming.
To this end, we propose an observational causal inference framework for the
empirical evaluation of the impact of digital tools on target farm performance
indicators. This way, we can increase farmers' trust by enhancing the
transparency of the digital agriculture market, and in turn accelerate the
adoption of technologies that aim to increase productivity and secure a
sustainable and resilient agriculture against a changing climate. As a case
study, we perform an empirical evaluation of a recommendation system for
optimal cotton sowing, which was used by a farmers' cooperative during the
growing season of 2021. We leverage agricultural knowledge to develop a causal
graph of the farm system, we use the back-door criterion to identify the impact
of recommendations on the yield and subsequently estimate it using several
methods on observational data. The results show that a field sown according to
our recommendations enjoyed a significant increase in yield (12% to 17%).
</p></li>
</ul>

<h2>security</h2>
<h3>Title: A Comparative Analysis of the Face Recognition Methods in Video Surveillance Scenarios. (arXiv:2211.02952v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02952">http://arxiv.org/abs/2211.02952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02952] A Comparative Analysis of the Face Recognition Methods in Video Surveillance Scenarios](http://arxiv.org/abs/2211.02952)</code></li>
<li>Summary: <p>Facial recognition is fundamental for a wide variety of security systems
operating in real-time applications. In video surveillance based face
recognition, face images are typically captured over multiple frames in
uncontrolled conditions; where head pose, illumination, shadowing, motion blur
and focus change over the sequence. We can generalize that the three
fundamental operations involved in the facial recognition tasks: face
detection, face alignment and face recognition. This study presents comparative
benchmark tables for the state-of-art face recognition methods by testing them
with same backbone architecture in order to focus only on the face recognition
solution instead of network architecture. For this purpose, we constructed a
video surveillance dataset of face IDs that has high age variance, intra-class
variance (face make-up, beard, etc.) with native surveillance facial imagery
data for evaluation. On the other hand, this work discovers the best
recognition methods for different conditions like non-masked faces, masked
faces, and faces with glasses.
</p></li>
</ul>

<h3>Title: MSMG-Net: Multi-scale Multi-grained Supervised Metworks for Multi-task Image Manipulation Detection and Localization. (arXiv:2211.03140v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03140">http://arxiv.org/abs/2211.03140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03140] MSMG-Net: Multi-scale Multi-grained Supervised Metworks for Multi-task Image Manipulation Detection and Localization](http://arxiv.org/abs/2211.03140)</code></li>
<li>Summary: <p>With the rapid advances of image editing techniques in recent years, image
manipulation detection has attracted considerable attention since the
increasing security risks posed by tampered images. To address these
challenges, a novel multi-scale multi-grained deep network (MSMG-Net) is
proposed to automatically identify manipulated regions. In our MSMG-Net, a
parallel multi-scale feature extraction structure is used to extract
multi-scale features. Then the multi-grained feature learning is utilized to
perceive object-level semantics relation of multi-scale features by introducing
the shunted self-attention. To fuse multi-scale multi-grained features, global
and local feature fusion block are designed for manipulated region segmentation
by a bottom-up approach and multi-level feature aggregation block is designed
for edge artifacts detection by a top-down approach. Thus, MSMG-Net can
effectively perceive the object-level semantics and encode the edge artifact.
Experimental results on five benchmark datasets justify the superior
performance of the proposed method, outperforming state-of-the-art manipulation
detection and localization methods. Extensive ablation experiments and feature
visualization demonstrate the multi-scale multi-grained learning can present
effective visual representations of manipulated regions. In addition, MSMG-Net
shows better robustness when various post-processing methods further manipulate
images.
</p></li>
</ul>

<h3>Title: DeepSec: Deciding Equivalence Properties for Security Protocols -- Improved theory and practice. (arXiv:2211.03225v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03225">http://arxiv.org/abs/2211.03225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03225] DeepSec: Deciding Equivalence Properties for Security Protocols -- Improved theory and practice](http://arxiv.org/abs/2211.03225)</code></li>
<li>Summary: <p>Automated verification has become an essential part in the security
evaluation of cryptographic protocols. In this context privacy-type properties
are often modelled by indistinguishability statements, expressed as behavioural
equivalences in a process calculus. In this paper we contribute both to the
theory and practice of this verification problem. We establish new complexity
results for static equivalence, trace equivalence and labelled bisimilarity and
provide a decision procedure for these equivalences in the case of a bounded
number of protocol sessions. Our procedure is the first to decide trace
equivalence and labelled bisimilarity exactly for a large variety of
cryptographic primitives -- those that can be represented by a subterm
convergent destructor rewrite system. We also implemented the procedure in a
new tool, DeepSec. We showed through extensive experiments that it is
significantly more efficient than other similar tools, while at the same time
raises the scope of the protocols that can be analysed.
</p></li>
</ul>

<h3>Title: Accurate and Reliable Methods for 5G UAV Jamming Identification With Calibrated Uncertainty. (arXiv:2211.02924v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02924">http://arxiv.org/abs/2211.02924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02924] Accurate and Reliable Methods for 5G UAV Jamming Identification With Calibrated Uncertainty](http://arxiv.org/abs/2211.02924)</code></li>
<li>Summary: <p>Only increasing accuracy without considering uncertainty may negatively
impact Deep Neural Network (DNN) decision-making and decrease its reliability.
This paper proposes five combined preprocessing and post-processing methods for
time-series binary classification problems that simultaneously increase the
accuracy and reliability of DNN outputs applied in a 5G UAV security dataset.
These techniques use DNN outputs as input parameters and process them in
different ways. Two methods use a well-known Machine Learning (ML) algorithm as
a complement, and the other three use only confidence values that the DNN
estimates. We compare seven different metrics, such as the Expected Calibration
Error (ECE), Maximum Calibration Error (MCE), Mean Confidence (MC), Mean
Accuracy (MA), Normalized Negative Log Likelihood (NLL), Brier Score Loss
(BSL), and Reliability Score (RS) and the tradeoffs between them to evaluate
the proposed hybrid algorithms. First, we show that the eXtreme Gradient
Boosting (XGB) classifier might not be reliable for binary classification under
the conditions this work presents. Second, we demonstrate that at least one of
the potential methods can achieve better results than the classification in the
DNN softmax layer. Finally, we show that the prospective methods may improve
accuracy and reliability with better uncertainty calibration based on the
assumption that the RS determines the difference between MC and MA metrics, and
this difference should be zero to increase reliability. For example, Method 3
presents the best RS of 0.65 even when compared to the XGB classifier, which
achieves RS of 7.22.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy-Preserving Models for Legal Natural Language Processing. (arXiv:2211.02956v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02956">http://arxiv.org/abs/2211.02956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02956] Privacy-Preserving Models for Legal Natural Language Processing](http://arxiv.org/abs/2211.02956)</code></li>
<li>Summary: <p>Pre-training large transformer models with in-domain data improves domain
adaptation and helps gain performance on the domain-specific downstream tasks.
However, sharing models pre-trained on potentially sensitive data is prone to
adversarial privacy attacks. In this paper, we asked to which extent we can
guarantee privacy of pre-training data and, at the same time, achieve better
downstream performance on legal tasks without the need of additional labeled
data. We extensively experiment with scalable self-supervised learning of
transformer models under the formal paradigm of differential privacy and show
that under specific training configurations we can improve downstream
performance without sacrifying privacy protection for the in-domain data. Our
main contribution is utilizing differential privacy for large-scale
pre-training of transformer language models in the legal NLP domain, which, to
the best of our knowledge, has not been addressed before.
</p></li>
</ul>

<h3>Title: Unlearning Nonlinear Graph Classifiers in the Limited Training Data Regime. (arXiv:2211.03216v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03216">http://arxiv.org/abs/2211.03216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03216] Unlearning Nonlinear Graph Classifiers in the Limited Training Data Regime](http://arxiv.org/abs/2211.03216)</code></li>
<li>Summary: <p>As the demand for user privacy grows, controlled data removal (machine
unlearning) is becoming an important feature of machine learning models for
data-sensitive Web applications such as social networks and recommender
systems. Nevertheless, at this point it is still largely unknown how to perform
efficient machine unlearning of graph neural networks (GNNs); this is
especially the case when the number of training samples is small, in which case
unlearning can seriously compromise the performance of the model. To address
this issue, we initiate the study of unlearning the Graph Scattering Transform
(GST), a mathematical framework that is efficient, provably stable under
feature or graph topology perturbations, and offers graph classification
performance comparable to that of GNNs. Our main contribution is the first
known nonlinear approximate graph unlearning method based on GSTs. Our second
contribution is a theoretical analysis of the computational complexity of the
proposed unlearning mechanism, which is hard to replicate for deep neural
networks. Our third contribution are extensive simulation results which show
that, compared to complete retraining of GNNs after each removal request, the
new GST-based approach offers, on average, a $10.38$x speed-up and leads to a
$2.6$% increase in test accuracy during unlearning of $90$ out of $100$
training graphs from the IMDB dataset ($10$% training ratio).
</p></li>
</ul>

<h3>Title: Wind Power Forecasting Considering Data Privacy Protection: A Federated Deep Reinforcement Learning Approach. (arXiv:2211.02674v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02674">http://arxiv.org/abs/2211.02674</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02674] Wind Power Forecasting Considering Data Privacy Protection: A Federated Deep Reinforcement Learning Approach](http://arxiv.org/abs/2211.02674)</code></li>
<li>Summary: <p>In a modern power system with an increasing proportion of renewable energy,
wind power prediction is crucial to the arrangement of power grid dispatching
plans due to the volatility of wind power. However, traditional centralized
forecasting methods raise concerns regarding data privacy-preserving and data
islands problem. To handle the data privacy and openness, we propose a
forecasting scheme that combines federated learning and deep reinforcement
learning (DRL) for ultra-short-term wind power forecasting, called federated
deep reinforcement learning (FedDRL). Firstly, this paper uses the deep
deterministic policy gradient (DDPG) algorithm as the basic forecasting model
to improve prediction accuracy. Secondly, we integrate the DDPG forecasting
model into the framework of federated learning. The designed FedDRL can obtain
an accurate prediction model in a decentralized way by sharing model parameters
instead of sharing private data which can avoid sensitive privacy issues. The
simulation results show that the proposed FedDRL outperforms the traditional
prediction methods in terms of forecasting accuracy. More importantly, while
ensuring the forecasting performance, FedDRL can effectively protect the data
privacy and relieve the communication pressure compared with the traditional
centralized forecasting method. In addition, a simulation with different
federated learning parameters is conducted to confirm the robustness of the
proposed scheme.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Textual Manifold-based Defense Against Natural Language Adversarial Examples. (arXiv:2211.02878v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02878">http://arxiv.org/abs/2211.02878</a></li>
<li>Code URL: <a href="https://github.com/dangne/tmd">https://github.com/dangne/tmd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02878] Textual Manifold-based Defense Against Natural Language Adversarial Examples](http://arxiv.org/abs/2211.02878)</code></li>
<li>Summary: <p>Recent studies on adversarial images have shown that they tend to leave the
underlying low-dimensional data manifold, making them significantly more
challenging for current models to make correct predictions. This so-called
off-manifold conjecture has inspired a novel line of defenses against
adversarial attacks on images. In this study, we find a similar phenomenon
occurs in the contextualized embedding space induced by pretrained language
models, in which adversarial texts tend to have their embeddings diverge from
the manifold of natural ones. Based on this finding, we propose Textual
Manifold-based Defense (TMD), a defense mechanism that projects text embeddings
onto an approximated embedding manifold before classification. It reduces the
complexity of potential adversarial examples, which ultimately enhances the
robustness of the protected model. Through extensive experiments, our method
consistently and significantly outperforms previous defenses under various
attack settings without trading off clean accuracy. To the best of our
knowledge, this is the first NLP defense that leverages the manifold structure
against adversarial attacks. Our code is available at
\url{https://github.com/dangne/tmd}.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Contrastive Weighted Learning for Near-Infrared Gaze Estimation. (arXiv:2211.03073v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03073">http://arxiv.org/abs/2211.03073</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03073] Contrastive Weighted Learning for Near-Infrared Gaze Estimation](http://arxiv.org/abs/2211.03073)</code></li>
<li>Summary: <p>Appearance-based gaze estimation has been very successful with the use of
deep learning. Many following works improved domain generalization for gaze
estimation. However, even though there has been much progress in domain
generalization for gaze estimation, most of the recent work have been focused
on cross-dataset performance -- accounting for different distributions in
illuminations, head pose, and lighting. Although improving gaze estimation in
different distributions of RGB images is important, near-infrared image based
gaze estimation is also critical for gaze estimation in dark settings. Also
there are inherent limitations relying solely on supervised learning for
regression tasks. This paper contributes to solving these problems and proposes
GazeCWL, a novel framework for gaze estimation with near-infrared images using
contrastive learning. This leverages adversarial attack techniques for data
augmentation and a novel contrastive loss function specifically for regression
tasks that effectively clusters the features of different samples in the latent
space. Our model outperforms previous domain generalization models in infrared
image based gaze estimation and outperforms the baseline by 45.6\% while
improving the state-of-the-art by 8.6\%, we demonstrate the efficacy of our
method.
</p></li>
</ul>

<h3>Title: Experience Report on the Challenges and Opportunities in Securing Smartphones Against Zero-Click Attacks. (arXiv:2211.03015v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03015">http://arxiv.org/abs/2211.03015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03015] Experience Report on the Challenges and Opportunities in Securing Smartphones Against Zero-Click Attacks](http://arxiv.org/abs/2211.03015)</code></li>
<li>Summary: <p>Zero-click attacks require no user interaction and typically exploit zero-day
(i.e., unpatched) vulnerabilities in instant chat applications (such as
WhatsApp and iMessage) to gain root access to the victim's smartphone and
exfiltrate sensitive data. In this paper, we report our experiences in
attempting to secure smartphones against zero-click attacks. We approached the
problem by first enumerating several properties we believed were necessary to
prevent zero-click attacks against smartphones. Then, we created a security
design that satisfies all the identified properties, and attempted to build it
using off-the-shelf components. Our key idea was to shift the attack surface
from the user's smartphone to a sandboxed virtual smartphone ecosystem where
each chat application runs in isolation. Our performance and usability
evaluations of the system we built highlighted several shortcomings and the
fundamental challenges in securing modern smartphones against zero-click
attacks. In this experience report, we discuss the lessons we learned, and
share insights on the missing components necessary to achieve foolproof
security against zero-click attacks for modern mobile devices.
</p></li>
</ul>

<h3>Title: Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain. (arXiv:2211.03027v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03027">http://arxiv.org/abs/2211.03027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03027] Exposing Surveillance Detection Routes via Reinforcement Learning, Attack Graphs, and Cyber Terrain](http://arxiv.org/abs/2211.03027)</code></li>
<li>Summary: <p>Reinforcement learning (RL) operating on attack graphs leveraging cyber
terrain principles are used to develop reward and state associated with
determination of surveillance detection routes (SDR). This work extends
previous efforts on developing RL methods for path analysis within enterprise
networks. This work focuses on building SDR where the routes focus on exploring
the network services while trying to evade risk. RL is utilized to support the
development of these routes by building a reward mechanism that would help in
realization of these paths. The RL algorithm is modified to have a novel
warm-up phase which decides in the initial exploration which areas of the
network are safe to explore based on the rewards and penalty scale factor.
</p></li>
</ul>

<h3>Title: Going In Style: Audio Backdoors Through Stylistic Transformations. (arXiv:2211.03117v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03117">http://arxiv.org/abs/2211.03117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03117] Going In Style: Audio Backdoors Through Stylistic Transformations](http://arxiv.org/abs/2211.03117)</code></li>
<li>Summary: <p>A backdoor attack places triggers in victims' deep learning models to enable
a targeted misclassification at testing time. In general, triggers are fixed
artifacts attached to samples, making backdoor attacks easy to spot. Only
recently, a new trigger generation harder to detect has been proposed: the
stylistic triggers that apply stylistic transformations to the input samples
(e.g., a specific writing style).
</p></li>
</ul>

<p>Currently, stylistic backdoor literature lacks a proper formalization of the
attack, which is established in this paper. Moreover, most studies of stylistic
triggers focus on text and images, while there is no understanding of whether
they can work in sound. This work fills this gap. We propose JingleBack, the
first stylistic backdoor attack based on audio transformations such as chorus
and gain. Using 444 models in a speech classification task, we confirm the
feasibility of stylistic triggers in audio, achieving 96% attack success.
</p>

<h3>Title: Detection Of Insider Attacks In Block Chain Network Using The Trusted Two Way Intrusion Detection System. (arXiv:2211.03138v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03138">http://arxiv.org/abs/2211.03138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03138] Detection Of Insider Attacks In Block Chain Network Using The Trusted Two Way Intrusion Detection System](http://arxiv.org/abs/2211.03138)</code></li>
<li>Summary: <p>For data privacy, system reliability, and security, Blockchain technologies
have become more popular in recent years. Despite its usefulness, the
blockchain is vulnerable to cyber assaults; for example, in January 2019 a 51%
attack on Ethereum Classic successfully exposed flaws in the platform's
security. From a statistical point of view, attacks represent a highly unusual
occurrence that deviates significantly from the norm. Blockchain attack
detection may benefit from Deep Learning, a field of study whose aim is to
discover insights, patterns, and anomalies within massive data repositories. In
this work, we define an trusted two way intrusion detection system based on a
Hierarchical weighed fuzzy algorithm and self-organized stacked network (SOSN)
deep learning model, that is trained exploiting aggregate information extracted
by monitoring blockchain activities. Here initially the smart contract handles
the node authentication. The purpose of authenticating the node is to ensure
that only specific nodes can submit and retrieve the information. We implement
Hierarchical weighed fuzzy algorithm to evaluate the trust ability of the
transaction nodes. Then the transaction verification step ensures that all
malicious transactions or activities on the submitted transaction by
self-organized stacked network deep learning model. The whole experimentation
was carried out under matlab environment. Extensive experimental results
confirm that our suggested detection method has better performance over
important indicators such as Precision, Recall, F-Score, overhead.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: MONAI: An open-source framework for deep learning in healthcare. (arXiv:2211.02701v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02701">http://arxiv.org/abs/2211.02701</a></li>
<li>Code URL: <a href="https://github.com/Project-MONAI/MONAI">https://github.com/Project-MONAI/MONAI</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02701] MONAI: An open-source framework for deep learning in healthcare](http://arxiv.org/abs/2211.02701)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) is having a tremendous impact across most areas
of science. Applications of AI in healthcare have the potential to improve our
ability to detect, diagnose, prognose, and intervene on human disease. For AI
models to be used clinically, they need to be made safe, reproducible and
robust, and the underlying software framework must be aware of the
particularities (e.g. geometry, physiology, physics) of medical data being
processed. This work introduces MONAI, a freely available, community-supported,
and consortium-led PyTorch-based framework for deep learning in healthcare.
MONAI extends PyTorch to support medical data, with a particular focus on
imaging, and provide purpose-specific AI model architectures, transformations
and utilities that streamline the development and deployment of medical AI
models. MONAI follows best practices for software-development, providing an
easy-to-use, robust, well-documented, and well-tested software framework. MONAI
preserves the simple, additive, and compositional approach of its underlying
PyTorch libraries. MONAI is being used by and receiving contributions from
research, clinical and industrial teams from around the world, who are pursuing
applications spanning nearly every aspect of healthcare.
</p></li>
</ul>

<h3>Title: Local Manifold Augmentation for Multiview Semantic Consistency. (arXiv:2211.02798v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02798">http://arxiv.org/abs/2211.02798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02798] Local Manifold Augmentation for Multiview Semantic Consistency](http://arxiv.org/abs/2211.02798)</code></li>
<li>Summary: <p>Multiview self-supervised representation learning roots in exploring semantic
consistency across data of complex intra-class variation. Such variation is not
directly accessible and therefore simulated by data augmentations. However,
commonly adopted augmentations are handcrafted and limited to simple
geometrical and color changes, which are unable to cover the abundant
intra-class variation. In this paper, we propose to extract the underlying data
variation from datasets and construct a novel augmentation operator, named
local manifold augmentation (LMA). LMA is achieved by training an
instance-conditioned generator to fit the distribution on the local manifold of
data and sampling multiview data using it. LMA shows the ability to create an
infinite number of data views, preserve semantics, and simulate complicated
variations in object pose, viewpoint, lighting condition, background etc.
Experiments show that with LMA integrated, self-supervised learning methods
such as MoCov2 and SimSiam gain consistent improvement on prevalent benchmarks
including CIFAR10, CIFAR100, STL10, ImageNet100, and ImageNet. Furthermore, LMA
leads to representations that obtain more significant invariance to the
viewpoint, object pose, and illumination changes and stronger robustness to
various real distribution shifts reflected by ImageNet-V2, ImageNet-R, ImageNet
Sketch etc.
</p></li>
</ul>

<h3>Title: KSS-ICP: Point Cloud Registration based on Kendall Shape Space. (arXiv:2211.02807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02807">http://arxiv.org/abs/2211.02807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02807] KSS-ICP: Point Cloud Registration based on Kendall Shape Space](http://arxiv.org/abs/2211.02807)</code></li>
<li>Summary: <p>Point cloud registration is a popular topic which has been widely used in 3D
model reconstruction, location, and retrieval. In this paper, we propose a new
registration method, KSS-ICP, to address the rigid registration task in Kendall
shape space (KSS) with Iterative Closest Point (ICP). The KSS is a quotient
space that removes influences of translations, scales, and rotations for shape
feature-based analysis. Such influences can be concluded as the similarity
transformations that do not change the shape feature. The point cloud
representation in KSS is invariant to similarity transformations. We utilize
such property to design the KSS-ICP for point cloud registration. To tackle the
difficulty to achieve the KSS representation in general, the proposed KSS-ICP
formulates a practical solution that does not require complex feature analysis,
data training, and optimization. With a simple implementation, KSS-ICP achieves
more accurate registration from point clouds. It is robust to similarity
transformation, non-uniform density, noise, and defective parts. Experiments
show that KSS-ICP has better performance than the state of the art.
</p></li>
</ul>

<h3>Title: A Robust and Low Complexity Deep Learning Model for Remote Sensing Image Classification. (arXiv:2211.02820v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02820">http://arxiv.org/abs/2211.02820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02820] A Robust and Low Complexity Deep Learning Model for Remote Sensing Image Classification](http://arxiv.org/abs/2211.02820)</code></li>
<li>Summary: <p>In this paper, we present a robust and low complexity deep learning model for
Remote Sensing Image Classification (RSIC), the task of identifying the scene
of a remote sensing image. In particular, we firstly evaluate different low
complexity and benchmark deep neural networks: MobileNetV1, MobileNetV2,
NASNetMobile, and EfficientNetB0, which present the number of trainable
parameters lower than 5 Million (M). After indicating best network
architecture, we further improve the network performance by applying attention
schemes to multiple feature maps extracted from middle layers of the network.
To deal with the issue of increasing the model footprint as using attention
schemes, we apply the quantization technique to satisfies the number trainable
parameter of the model lower than 5 M. By conducting extensive experiments on
the benchmark datasets NWPU-RESISC45, we achieve a robust and low-complexity
model, which is very competitive to the state-of-the-art systems and potential
for real-life applications on edge devices.
</p></li>
</ul>

<h3>Title: Robust Reflection Removal with Flash-only Cues in the Wild. (arXiv:2211.02914v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02914">http://arxiv.org/abs/2211.02914</a></li>
<li>Code URL: <a href="https://github.com/ChenyangLEI/flash-reflection-removal">https://github.com/ChenyangLEI/flash-reflection-removal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02914] Robust Reflection Removal with Flash-only Cues in the Wild](http://arxiv.org/abs/2211.02914)</code></li>
<li>Summary: <p>We propose a simple yet effective reflection-free cue for robust reflection
removal from a pair of flash and ambient (no-flash) images. The reflection-free
cue exploits a flash-only image obtained by subtracting the ambient image from
the corresponding flash image in raw data space. The flash-only image is
equivalent to an image taken in a dark environment with only a flash on. This
flash-only image is visually reflection-free and thus can provide robust cues
to infer the reflection in the ambient image. Since the flash-only image
usually has artifacts, we further propose a dedicated model that not only
utilizes the reflection-free cue but also avoids introducing artifacts, which
helps accurately estimate reflection and transmission. Our experiments on
real-world images with various types of reflection demonstrate the
effectiveness of our model with reflection-free flash-only cues: our model
outperforms state-of-the-art reflection removal approaches by more than 5.23dB
in PSNR. We extend our approach to handheld photography to address the
misalignment between the flash and no-flash pair. With misaligned training data
and the alignment module, our aligned model outperforms our previous version by
more than 3.19dB in PSNR on a misaligned dataset. We also study using linear
RGB images as training data. Our source code and dataset are publicly available
at https://github.com/ChenyangLEI/flash-reflection-removal.
</p></li>
</ul>

<h3>Title: Prototypical quadruplet for few-shot class incremental learning. (arXiv:2211.02947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02947">http://arxiv.org/abs/2211.02947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02947] Prototypical quadruplet for few-shot class incremental learning](http://arxiv.org/abs/2211.02947)</code></li>
<li>Summary: <p>Many modern computer vision algorithms suffer from two major bottlenecks:
scarcity of data and learning new tasks incrementally. While training the model
with new batches of data the model looses it's ability to classify the previous
data judiciously which is termed as catastrophic forgetting. Conventional
methods have tried to mitigate catastrophic forgetting of the previously
learned data while the training at the current session has been compromised.
The state-of-the-art generative replay based approaches use complicated
structures such as generative adversarial network (GAN) to deal with
catastrophic forgetting. Additionally, training a GAN with few samples may lead
to instability. In this work, we present a novel method to deal with these two
major hurdles. Our method identifies a better embedding space with an improved
contrasting loss to make classification more robust. Moreover, our approach is
able to retain previously acquired knowledge in the embedding space even when
trained with new classes. We update previous session class prototypes while
training in such a way that it is able to represent the true class mean. This
is of prime importance as our classification rule is based on the nearest class
mean classification strategy. We have demonstrated our results by showing that
the embedding space remains intact after training the model with new classes.
We showed that our method preformed better than the existing state-of-the-art
algorithms in terms of accuracy across different sessions.
</p></li>
</ul>

<h3>Title: Bringing Online Egocentric Action Recognition into the wild. (arXiv:2211.03004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03004">http://arxiv.org/abs/2211.03004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03004] Bringing Online Egocentric Action Recognition into the wild](http://arxiv.org/abs/2211.03004)</code></li>
<li>Summary: <p>To enable a safe and effective human-robot cooperation, it is crucial to
develop models for the identification of human activities. Egocentric vision
seems to be a viable solution to solve this problem, and therefore many works
provide deep learning solutions to infer human actions from first person
videos. However, although very promising, most of these do not consider the
major challenges that comes with a realistic deployment, such as the
portability of the model, the need for real-time inference, and the robustness
with respect to the novel domains (i.e., new spaces, users, tasks). With this
paper, we set the boundaries that egocentric vision models should consider for
realistic applications, defining a novel setting of egocentric action
recognition in the wild, which encourages researchers to develop novel,
applications-aware solutions. We also present a new model-agnostic technique
that enables the rapid repurposing of existing architectures in this new
context, demonstrating the feasibility to deploy a model on a tiny device
(Jetson Nano) and to perform the task directly on the edge with very low energy
consumption (2.4W on average at 50 fps).
</p></li>
</ul>

<h3>Title: A Geometrically Constrained Point Matching based on View-invariant Cross-ratios, and Homography. (arXiv:2211.03007v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03007">http://arxiv.org/abs/2211.03007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03007] A Geometrically Constrained Point Matching based on View-invariant Cross-ratios, and Homography](http://arxiv.org/abs/2211.03007)</code></li>
<li>Summary: <p>In computer vision, finding point correspondence among images plays an
important role in many applications, such as image stitching, image retrieval,
visual localization, etc. Most of the research worksfocus on the matching of
local feature before a sampling method is employed, such as RANSAC, to verify
initial matching results via repeated fitting of certain global transformation
among the images. However, incorrect matches may still exist, while careful
examination of such problems is often skipped. Accordingly, a geometrically
constrained algorithm is proposed in this work to verify the correctness of
initially matched SIFT keypoints based on view-invariant cross-ratios (CRs). By
randomly forming pentagons from these keypoints and matching their shape and
location among images with CRs, robust planar region estimation can be achieved
efficiently for the above verification, while correct and incorrect matches of
keypoints can be examined easily with respect to those shape and location
matched pentagons. Experimental results show that satisfactory results can be
obtained for various scenes with single as well as multiple planar regions.
</p></li>
</ul>

<h3>Title: Learning Dual-Fused Modality-Aware Representations for RGBD Tracking. (arXiv:2211.03055v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03055">http://arxiv.org/abs/2211.03055</a></li>
<li>Code URL: <a href="https://github.com/shanggaog/dmtracker">https://github.com/shanggaog/dmtracker</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03055] Learning Dual-Fused Modality-Aware Representations for RGBD Tracking](http://arxiv.org/abs/2211.03055)</code></li>
<li>Summary: <p>With the development of depth sensors in recent years, RGBD object tracking
has received significant attention. Compared with the traditional RGB object
tracking, the addition of the depth modality can effectively solve the target
and background interference. However, some existing RGBD trackers use the two
modalities separately and thus some particularly useful shared information
between them is ignored. On the other hand, some methods attempt to fuse the
two modalities by treating them equally, resulting in the missing of
modality-specific features. To tackle these limitations, we propose a novel
Dual-fused Modality-aware Tracker (termed DMTracker) which aims to learn
informative and discriminative representations of the target objects for robust
RGBD tracking. The first fusion module focuses on extracting the shared
information between modalities based on cross-modal attention. The second aims
at integrating the RGB-specific and depth-specific information to enhance the
fused features. By fusing both the modality-shared and modality-specific
information in a modality-aware scheme, our DMTracker can learn discriminative
representations in complex tracking scenes. Experiments show that our proposed
tracker achieves very promising results on challenging RGBD benchmarks. Code is
available at \url{https://github.com/ShangGaoG/DMTracker}.
</p></li>
</ul>

<h3>Title: Measurement-Consistent Networks via a Deep Implicit Layer for Solving Inverse Problems. (arXiv:2211.03177v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03177">http://arxiv.org/abs/2211.03177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03177] Measurement-Consistent Networks via a Deep Implicit Layer for Solving Inverse Problems](http://arxiv.org/abs/2211.03177)</code></li>
<li>Summary: <p>End-to-end deep neural networks (DNNs) have become state-of-the-art (SOTA)
for solving inverse problems. Despite their outstanding performance, during
deployment, such networks are sensitive to minor variations in the training
pipeline and often fail to reconstruct small but important details, a feature
critical in medical imaging, astronomy, or defence. Such instabilities in DNNs
can be explained by the fact that they ignore the forward measurement model
during deployment, and thus fail to enforce consistency between their output
and the input measurements. To overcome this, we propose a framework that
transforms any DNN for inverse problems into a measurement-consistent one. This
is done by appending to it an implicit layer (or deep equilibrium network)
designed to solve a model-based optimization problem. The implicit layer
consists of a shallow learnable network that can be integrated into the
end-to-end training. Experiments on single-image super-resolution show that the
proposed framework leads to significant improvements in reconstruction quality
and robustness over the SOTA DNNs.
</p></li>
</ul>

<h3>Title: A Deep-Unfolded Spatiotemporal RPCA Network For L+S Decomposition. (arXiv:2211.03184v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03184">http://arxiv.org/abs/2211.03184</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03184] A Deep-Unfolded Spatiotemporal RPCA Network For L+S Decomposition](http://arxiv.org/abs/2211.03184)</code></li>
<li>Summary: <p>Low-rank and sparse decomposition based methods find their use in many
applications involving background modeling such as clutter suppression and
object tracking. While Robust Principal Component Analysis (RPCA) has achieved
great success in performing this task, it can take hundreds of iterations to
converge and its performance decreases in the presence of different phenomena
such as occlusion, jitter and fast motion. The recently proposed deep unfolded
networks, on the other hand, have demonstrated better accuracy and improved
convergence over both their iterative equivalents as well as over other neural
network architectures. In this work, we propose a novel deep unfolded
spatiotemporal RPCA (DUST-RPCA) network, which explicitly takes advantage of
the spatial and temporal continuity in the low-rank component. Our experimental
results on the moving MNIST dataset indicate that DUST-RPCA gives better
accuracy when compared with the existing state of the art deep unfolded RPCA
networks.
</p></li>
</ul>

<h3>Title: Momentum-based Weight Interpolation of Strong Zero-Shot Models for Continual Learning. (arXiv:2211.03186v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03186">http://arxiv.org/abs/2211.03186</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03186] Momentum-based Weight Interpolation of Strong Zero-Shot Models for Continual Learning](http://arxiv.org/abs/2211.03186)</code></li>
<li>Summary: <p>Large pre-trained, zero-shot capable models have shown considerable success
both for standard transfer and adaptation tasks, with particular robustness
towards distribution shifts. In addition, subsequent fine-tuning can
considerably improve performance on a selected downstream task. However,
through naive fine-tuning, these zero-shot models lose their generalizability
and robustness towards distribution shifts. This is a particular problem for
tasks such as Continual Learning (CL), where continuous adaptation has to be
performed as new task distributions are introduced sequentially. In this work,
we showcase that where fine-tuning falls short to adapt such zero-shot capable
models, simple momentum-based weight interpolation can provide consistent
improvements for CL tasks in both memory-free and memory-based settings. In
particular, we find improvements of over $+4\%$ on standard CL benchmarks,
while reducing the error to the upper limit of jointly training on all tasks at
once in parts by more than half, allowing the continual learner to inch closer
to the joint training limits.
</p></li>
</ul>

<h3>Title: Intriguing Properties of Compression on Multilingual Models. (arXiv:2211.02738v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02738">http://arxiv.org/abs/2211.02738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02738] Intriguing Properties of Compression on Multilingual Models](http://arxiv.org/abs/2211.02738)</code></li>
<li>Summary: <p>Multilingual models are often particularly dependent on scaling to generalize
to a growing number of languages. Compression techniques are widely relied upon
to reconcile the growth in model size with real world resource constraints, but
compression can have a disparate effect on model performance for low-resource
languages. It is thus crucial to understand the trade-offs between scale,
multilingualism, and compression. In this work, we propose an experimental
framework to characterize the impact of sparsifying multilingual pre-trained
language models during fine-tuning. Applying this framework to mBERT named
entity recognition models across 40 languages, we find that compression confers
several intriguing and previously unknown generalization properties. In
contrast to prior findings, we find that compression may improve model
robustness over dense models. We additionally observe that under certain
sparsification regimes compression may aid, rather than disproportionately
impact the performance of low-resource languages.
</p></li>
</ul>

<h3>Title: Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference. (arXiv:2211.02971v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02971">http://arxiv.org/abs/2211.02971</a></li>
<li>Code URL: <a href="https://github.com/msadat3/ssl_for_nli">https://github.com/msadat3/ssl_for_nli</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02971] Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference](http://arxiv.org/abs/2211.02971)</code></li>
<li>Summary: <p>Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) aims
at predicting the relation between a pair of sentences (premise and hypothesis)
as entailment, contradiction or semantic independence. Although deep learning
models have shown promising performance for NLI in recent years, they rely on
large scale expensive human-annotated datasets. Semi-supervised learning (SSL)
is a popular technique for reducing the reliance on human annotation by
leveraging unlabeled data for training. However, despite its substantial
success on single sentence classification tasks where the challenge in making
use of unlabeled data is to assign "good enough" pseudo-labels, for NLI tasks,
the nature of unlabeled data is more complex: one of the sentences in the pair
(usually the hypothesis) along with the class label are missing from the data
and require human annotations, which makes SSL for NLI more challenging. In
this paper, we propose a novel way to incorporate unlabeled data in SSL for NLI
where we use a conditional language model, BART to generate the hypotheses for
the unlabeled sentences (used as premises). Our experiments show that our SSL
framework successfully exploits unlabeled data and substantially improves the
performance of four NLI datasets in low-resource settings. We release our code
at: https://github.com/msadat3/SSL_for_NLI.
</p></li>
</ul>

<h3>Title: Robust Lottery Tickets for Pre-trained Language Models. (arXiv:2211.03013v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03013">http://arxiv.org/abs/2211.03013</a></li>
<li>Code URL: <a href="https://github.com/ruizheng20/robust_ticket">https://github.com/ruizheng20/robust_ticket</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03013] Robust Lottery Tickets for Pre-trained Language Models](http://arxiv.org/abs/2211.03013)</code></li>
<li>Summary: <p>Recent works on Lottery Ticket Hypothesis have shown that pre-trained
language models (PLMs) contain smaller matching subnetworks(winning tickets)
which are capable of reaching accuracy comparable to the original models.
However, these tickets are proved to be notrobust to adversarial examples, and
even worse than their PLM counterparts. To address this problem, we propose a
novel method based on learning binary weight masks to identify robust tickets
hidden in the original PLMs. Since the loss is not differentiable for the
binary mask, we assign the hard concrete distribution to the masks and
encourage their sparsity using a smoothing approximation of L0
regularization.Furthermore, we design an adversarial loss objective to guide
the search for robust tickets and ensure that the tickets perform well bothin
accuracy and robustness. Experimental results show the significant improvement
of the proposed method over previous work on adversarial robustness evaluation.
</p></li>
</ul>

<h3>Title: Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03046">http://arxiv.org/abs/2211.03046</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03046] Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust](http://arxiv.org/abs/2211.03046)</code></li>
<li>Summary: <p>Legal judgment Prediction (LJP), aiming to predict a judgment based on fact
descriptions, serves as legal assistance to mitigate the great work burden of
limited legal practitioners. Most existing methods apply various large-scale
pre-trained language models (PLMs) finetuned in LJP tasks to obtain consistent
improvements. However, we discover the fact that the state-of-the-art (SOTA)
model makes judgment predictions according to wrong (or non-casual)
information, which not only weakens the model's generalization capability but
also results in severe social problems like discrimination. Here, we analyze
the causal mechanism misleading the LJP model to learn the spurious
correlations, and then propose a framework to guide the model to learn the
underlying causality knowledge in the legal texts. Specifically, we first
perform open information extraction (OIE) to refine the text having a high
proportion of causal information, according to which we generate a new set of
data. Then, we design a model learning the weights of the refined data and the
raw data for LJP model training. The extensive experimental results show that
our model is more generalizable and robust than the baselines and achieves a
new SOTA performance on two commonly used legal-specific datasets.
</p></li>
</ul>

<h3>Title: An Adversarial Robustness Perspective on the Topology of Neural Networks. (arXiv:2211.02675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02675">http://arxiv.org/abs/2211.02675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02675] An Adversarial Robustness Perspective on the Topology of Neural Networks](http://arxiv.org/abs/2211.02675)</code></li>
<li>Summary: <p>In this paper, we investigate the impact of neural networks (NNs) topology on
adversarial robustness. Specifically, we study the graph produced when an input
traverses all the layers of a NN, and show that such graphs are different for
clean and adversarial inputs. We find that graphs from clean inputs are more
centralized around highway edges, whereas those from adversaries are more
diffuse, leveraging under-optimized edges. Through experiments on a variety of
datasets and architectures, we show that these under-optimized edges are a
source of adversarial vulnerability and that they can be used to detect
adversarial inputs.
</p></li>
</ul>

<h3>Title: Predicting Treatment Adherence of Tuberculosis Patients at Scale. (arXiv:2211.02943v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02943">http://arxiv.org/abs/2211.02943</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02943] Predicting Treatment Adherence of Tuberculosis Patients at Scale](http://arxiv.org/abs/2211.02943)</code></li>
<li>Summary: <p>Tuberculosis (TB), an infectious bacterial disease, is a significant cause of
death, especially in low-income countries, with an estimated ten million new
cases reported globally in $2020$. While TB is treatable, non-adherence to the
medication regimen is a significant cause of morbidity and mortality. Thus,
proactively identifying patients at risk of dropping off their medication
regimen enables corrective measures to mitigate adverse outcomes. Using a proxy
measure of extreme non-adherence and a dataset of nearly $700,000$ patients
from four states in India, we formulate and solve the machine learning (ML)
problem of early prediction of non-adherence based on a custom rank-based
metric. We train ML models and evaluate against baselines, achieving a $\sim
100\%$ lift over rule-based baselines and $\sim 214\%$ over a random
classifier, taking into account country-wide large-scale future deployment. We
deal with various issues in the process, including data quality,
high-cardinality categorical data, low target prevalence, distribution shift,
variation across cohorts, algorithmic fairness, and the need for robustness and
explainability. Our findings indicate that risk stratification of non-adherent
patients is a viable, deployable-at-scale ML solution.
</p></li>
</ul>

<h3>Title: NLP Inspired Training Mechanics For Modeling Transient Dynamics. (arXiv:2211.02716v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02716">http://arxiv.org/abs/2211.02716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02716] NLP Inspired Training Mechanics For Modeling Transient Dynamics](http://arxiv.org/abs/2211.02716)</code></li>
<li>Summary: <p>In recent years, Machine learning (ML) techniques developed for Natural
Language Processing (NLP) have permeated into developing better computer vision
algorithms. In this work, we use such NLP-inspired techniques to improve the
accuracy, robustness and generalizability of ML models for simulating transient
dynamics. We introduce teacher forcing and curriculum learning based training
mechanics to model vortical flows and show an enhancement in accuracy for ML
models, such as FNO and UNet by more than 50%.
</p></li>
</ul>

<h3>Title: Clustering above Exponential Families with Tempered Exponential Measures. (arXiv:2211.02765v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02765">http://arxiv.org/abs/2211.02765</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02765] Clustering above Exponential Families with Tempered Exponential Measures](http://arxiv.org/abs/2211.02765)</code></li>
<li>Summary: <p>The link with exponential families has allowed $k$-means clustering to be
generalized to a wide variety of data generating distributions in exponential
families and clustering distortions among Bregman divergences. Getting the
framework to work above exponential families is important to lift roadblocks
like the lack of robustness of some population minimizers carved in their
axiomatization. Current generalisations of exponential families like
$q$-exponential families or even deformed exponential families fail at
achieving the goal. In this paper, we provide a new attempt at getting the
complete framework, grounded in a new generalisation of exponential families
that we introduce, tempered exponential measures (TEM). TEMs keep the maximum
entropy axiomatization framework of $q$-exponential families, but instead of
normalizing the measure, normalize a dual called a co-distribution. Numerous
interesting properties arise for clustering such as improved and controllable
robustness for population minimizers, that keep a simple analytic form.
</p></li>
</ul>

<h3>Title: Pitfalls of Climate Network Construction: A Statistical Perspective. (arXiv:2211.02888v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02888">http://arxiv.org/abs/2211.02888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02888] Pitfalls of Climate Network Construction: A Statistical Perspective](http://arxiv.org/abs/2211.02888)</code></li>
<li>Summary: <p>Network-based analyses of dynamical systems have become increasingly popular
in climate science. Here we address network construction from a statistical
perspective and highlight the often ignored fact that the calculated
correlation values are only empirical estimates. To measure spurious behaviour
as deviation from a ground truth network, we simulate time-dependent isotropic
random fields on the sphere and apply common network construction techniques.
We find several ways in which the uncertainty stemming from the estimation
procedure has major impact on network characteristics. When the data has
locally coherent correlation structure, spurious link bundle teleconnections
and spurious high-degree clusters have to be expected. Anisotropic estimation
variance can also induce severe biases into empirical networks. We validate our
findings with ERA5 reanalysis data. Moreover we explain why commonly applied
resampling procedures are inappropriate for significance evaluation and propose
a statistically more meaningful ensemble construction framework. By
communicating which difficulties arise in estimation from scarce data and by
presenting which design decisions increase robustness, we hope to contribute to
more reliable climate network construction in the future.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Evaluating Novel Mask-RCNN Architectures for Ear Mask Segmentation. (arXiv:2211.02799v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02799">http://arxiv.org/abs/2211.02799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02799] Evaluating Novel Mask-RCNN Architectures for Ear Mask Segmentation](http://arxiv.org/abs/2211.02799)</code></li>
<li>Summary: <p>The human ear is generally universal, collectible, distinct, and permanent.
Ear-based biometric recognition is a niche and recent approach that is being
explored. For any ear-based biometric algorithm to perform well, ear detection
and segmentation need to be accurately performed. While significant work has
been done in existing literature for bounding boxes, a lack of approaches
output a segmentation mask for ears. This paper trains and compares three newer
models to the state-of-the-art MaskRCNN (ResNet 101 +FPN) model across four
different datasets. The Average Precision (AP) scores reported show that the
newer models outperform the state-of-the-art but no one model performs the best
over multiple datasets.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Improved Kidney Stone Recognition Through Attention and Multi-View Feature Fusion Strategies. (arXiv:2211.02967v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02967">http://arxiv.org/abs/2211.02967</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02967] Improved Kidney Stone Recognition Through Attention and Multi-View Feature Fusion Strategies](http://arxiv.org/abs/2211.02967)</code></li>
<li>Summary: <p>This contribution presents a deep learning method for the extraction and
fusion of information relating to kidney stone fragments acquired from
different viewpoints of the endoscope. Surface and section fragment images are
jointly used during the training of the classifier to improve the
discrimination power of the features by adding attention layers at the end of
each convolutional block. This approach is specifically designed to mimic the
morpho-constitutional analysis performed in ex-vivo by biologists to visually
identify kidney stones by inspecting both views. The addition of attention
mechanisms to the backbone improved the results of single view extraction
backbones by 4% on average. Moreover, in comparison to the state-of-the-art,
the fusion of the deep features improved the overall results up to 11% in terms
of kidney stone classification accuracy.
</p></li>
</ul>

<h3>Title: Semantic Metadata Extraction from Dense Video Captioning. (arXiv:2211.02982v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02982">http://arxiv.org/abs/2211.02982</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02982] Semantic Metadata Extraction from Dense Video Captioning](http://arxiv.org/abs/2211.02982)</code></li>
<li>Summary: <p>Annotation of multimedia data by humans is time-consuming and costly, while
reliable automatic generation of semantic metadata is a major challenge. We
propose a framework to extract semantic metadata from automatically generated
video captions. As metadata, we consider entities, the entities' properties,
relations between entities, and the video category. We employ two
state-of-the-art dense video captioning models with masked transformer (MT) and
parallel decoding (PVDC) to generate captions for videos of the ActivityNet
Captions dataset. Our experiments show that it is possible to extract entities,
their properties, relations between entities, and the video category from the
generated captions. We observe that the quality of the extracted information is
mainly influenced by the quality of the event localization in the video as well
as the performance of the event caption generation.
</p></li>
</ul>

<h3>Title: 1Cademy @ Causal News Corpus 2022: Leveraging Self-Training in Causality Classification of Socio-Political Event Data. (arXiv:2211.02729v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02729">http://arxiv.org/abs/2211.02729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02729] 1Cademy @ Causal News Corpus 2022: Leveraging Self-Training in Causality Classification of Socio-Political Event Data](http://arxiv.org/abs/2211.02729)</code></li>
<li>Summary: <p>This paper details our participation in the Challenges and Applications of
Automated Extraction of Socio-political Events from Text (CASE) workshop @
EMNLP 2022, where we take part in Subtask 1 of Shared Task 3. We approach the
given task of event causality detection by proposing a self-training pipeline
that follows a teacher-student classifier method. More specifically, we
initially train a teacher model on the true, original task data, and use that
teacher model to self-label data to be used in the training of a separate
student model for the final task prediction. We test how restricting the number
of positive or negative self-labeled examples in the self-training process
affects classification performance. Our final results show that using
self-training produces a comprehensive performance improvement across all
models and self-labeled training sets tested within the task of event causality
sequence classification. On top of that, we find that self-training performance
did not diminish even when restricting either positive/negative examples used
in training. Our code is be publicly available at
https://github.com/Gzhang-umich/1CademyTeamOfCASE.
</p></li>
</ul>

<h3>Title: BEKG: A Built Environment Knowledge Graph. (arXiv:2211.02864v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02864">http://arxiv.org/abs/2211.02864</a></li>
<li>Code URL: <a href="https://github.com/hkust-knowcomp/bekg">https://github.com/hkust-knowcomp/bekg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02864] BEKG: A Built Environment Knowledge Graph](http://arxiv.org/abs/2211.02864)</code></li>
<li>Summary: <p>Practices in the built environment have become more digitalized with the
rapid development of modern design and construction technologies. However, the
requirement of practitioners or scholars to gather complicated professional
knowledge in the built environment has not been satisfied yet. In this paper,
more than 80,000 paper abstracts in the built environment field were obtained
to build a knowledge graph, a knowledge base storing entities and their
connective relations in a graph-structured data model. To ensure the retrieval
accuracy of the entities and relations in the knowledge graph, two
well-annotated datasets have been created, containing 2,000 instances and 1,450
instances each in 29 relations for the named entity recognition task and
relation extraction task respectively. These two tasks were solved by two
BERT-based models trained on the proposed dataset. Both models attained an
accuracy above 85% on these two tasks. More than 200,000 high-quality relations
and entities were obtained using these models to extract all abstract data.
Finally, this knowledge graph is presented as a self-developed visualization
system to reveal relations between various entities in the domain. Both the
source code and the annotated dataset can be found here:
https://github.com/HKUST-KnowComp/BEKG.
</p></li>
</ul>

<h3>Title: Coarse-to-fine Knowledge Graph Domain Adaptation based on Distantly-supervised Iterative Training. (arXiv:2211.02849v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02849">http://arxiv.org/abs/2211.02849</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02849] Coarse-to-fine Knowledge Graph Domain Adaptation based on Distantly-supervised Iterative Training](http://arxiv.org/abs/2211.02849)</code></li>
<li>Summary: <p>Modern supervised learning neural network models require a large amount of
manually labeled data, which makes the construction of domain-specific
knowledge graphs time-consuming and labor-intensive. In parallel, although
there has been much research on named entity recognition and relation
extraction based on distantly supervised learning, constructing a
domain-specific knowledge graph from large collections of textual data without
manual annotations is still an urgent problem to be solved. In response, we
propose an integrated framework for adapting and re-learning knowledge graphs
from one coarse domain (biomedical) to a finer-define domain (oncology). In
this framework, we apply distant-supervision on cross-domain knowledge graph
adaptation. Consequently, no manual data annotation is required to train the
model. We introduce a novel iterative training strategy to facilitate the
discovery of domain-specific named entities and triples. Experimental results
indicate that the proposed framework can perform domain adaptation and
construction of knowledge graph efficiently.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: ON-DEMAND-FL: A Dynamic and Efficient Multi-Criteria Federated Learning Client Deployment Scheme. (arXiv:2211.02906v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02906">http://arxiv.org/abs/2211.02906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02906] ON-DEMAND-FL: A Dynamic and Efficient Multi-Criteria Federated Learning Client Deployment Scheme](http://arxiv.org/abs/2211.02906)</code></li>
<li>Summary: <p>In this paper, we increase the availability and integration of devices in the
learning process to enhance the convergence of federated learning (FL) models.
To address the issue of having all the data in one location, federated
learning, which maintains the ability to learn over decentralized data sets,
combines privacy and technology. Until the model converges, the server combines
the updated weights obtained from each dataset over a number of rounds. The
majority of the literature suggested client selection techniques to accelerate
convergence and boost accuracy. However, none of the existing proposals have
focused on the flexibility to deploy and select clients as needed, wherever and
whenever that may be. Due to the extremely dynamic surroundings, some devices
are actually not available to serve as clients in FL, which affects the
availability of data for learning and the applicability of the existing
solution for client selection. In this paper, we address the aforementioned
limitations by introducing an On-Demand-FL, a client deployment approach for
FL, offering more volume and heterogeneity of data in the learning process. We
make use of the containerization technology such as Docker to build efficient
environments using IoT and mobile devices serving as volunteers. Furthermore,
Kubernetes is used for orchestration. The Genetic algorithm (GA) is used to
solve the multi-objective optimization problem due to its evolutionary
strategy. The performed experiments using the Mobile Data Challenge (MDC)
dataset and the Localfed framework illustrate the relevance of the proposed
approach and the efficiency of the on-the-fly deployment of clients whenever
and wherever needed with less discarded rounds and more available data.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models. (arXiv:2211.02882v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02882">http://arxiv.org/abs/2211.02882</a></li>
<li>Code URL: <a href="https://github.com/bernard-yang/herb">https://github.com/bernard-yang/herb</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02882] HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models](http://arxiv.org/abs/2211.02882)</code></li>
<li>Summary: <p>Fairness has become a trending topic in natural language processing (NLP),
which addresses biases targeting certain social groups such as genders and
religions. However, regional bias in language models (LMs), a long-standing
global discrimination problem, still remains unexplored. This paper bridges the
gap by analysing the regional bias learned by the pre-trained language models
that are broadly used in NLP tasks. In addition to verifying the existence of
regional bias in LMs, we find that the biases on regional groups can be
strongly influenced by the geographical clustering of the groups. We
accordingly propose a HiErarchical Regional Bias evaluation method (HERB)
utilising the information from the sub-region clusters to quantify the bias in
pre-trained LMs. Experiments show that our hierarchical metric can effectively
evaluate the regional bias with respect to comprehensive topics and measure the
potential regional bias that can be propagated to downstream tasks. Our codes
are available at https://github.com/Bernard-Yang/HERB.
</p></li>
</ul>

<h3>Title: GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization. (arXiv:2211.02733v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02733">http://arxiv.org/abs/2211.02733</a></li>
<li>Code URL: <a href="https://github.com/uw-exp/globem">https://github.com/uw-exp/globem</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02733] GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization](http://arxiv.org/abs/2211.02733)</code></li>
<li>Summary: <p>Recent research has demonstrated the capability of behavior signals captured
by smartphones and wearables for longitudinal behavior modeling. However, there
is a lack of a comprehensive public dataset that serves as an open testbed for
fair comparison among algorithms. Moreover, prior studies mainly evaluate
algorithms using data from a single population within a short period, without
measuring the cross-dataset generalizability of these algorithms. We present
the first multi-year passive sensing datasets, containing over 700 user-years
and 497 unique users' data collected from mobile and wearable sensors, together
with a wide range of well-being metrics. Our datasets can support multiple
cross-dataset evaluations of behavior modeling algorithms' generalizability
across different users and years. As a starting point, we provide the benchmark
results of 18 algorithms on the task of depression detection. Our results
indicate that both prior depression detection algorithms and domain
generalization techniques show potential but need further research to achieve
adequate cross-dataset generalizability. We envision our multi-year datasets
can support the ML community in developing generalizable longitudinal behavior
modeling algorithms.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Mixture-Net: Low-Rank Deep Image Prior Inspired by Mixture Models for Spectral Image Recovery. (arXiv:2211.02973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02973">http://arxiv.org/abs/2211.02973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02973] Mixture-Net: Low-Rank Deep Image Prior Inspired by Mixture Models for Spectral Image Recovery](http://arxiv.org/abs/2211.02973)</code></li>
<li>Summary: <p>This paper proposes a non-data-driven deep neural network for spectral image
recovery problems such as denoising, single hyperspectral image
super-resolution, and compressive spectral imaging reconstruction. Unlike
previous methods, the proposed approach, dubbed Mixture-Net, implicitly learns
the prior information through the network. Mixture-Net consists of a deep
generative model whose layers are inspired by the linear and non-linear
low-rank mixture models, where the recovered image is composed of a weighted
sum between the linear and non-linear decomposition. Mixture-Net also provides
a low-rank decomposition interpreted as the spectral image abundances and
endmembers, helpful in achieving remote sensing tasks without running
additional routines. The experiments show the MixtureNet effectiveness
outperforming state-of-the-art methods in recovery quality with the advantage
of architecture interpretability.
</p></li>
</ul>

<h3>Title: ProtoX: Explaining a Reinforcement Learning Agent via Prototyping. (arXiv:2211.03162v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.03162">http://arxiv.org/abs/2211.03162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.03162] ProtoX: Explaining a Reinforcement Learning Agent via Prototyping](http://arxiv.org/abs/2211.03162)</code></li>
<li>Summary: <p>While deep reinforcement learning has proven to be successful in solving
control tasks, the "black-box" nature of an agent has received increasing
concerns. We propose a prototype-based post-hoc policy explainer, ProtoX, that
explains a blackbox agent by prototyping the agent's behaviors into scenarios,
each represented by a prototypical state. When learning prototypes, ProtoX
considers both visual similarity and scenario similarity. The latter is unique
to the reinforcement learning context, since it explains why the same action is
taken in visually different states. To teach ProtoX about visual similarity, we
pre-train an encoder using contrastive learning via self-supervised learning to
recognize states as similar if they occur close together in time and receive
the same action from the black-box agent. We then add an isometry layer to
allow ProtoX to adapt scenario similarity to the downstream task. ProtoX is
trained via imitation learning using behavior cloning, and thus requires no
access to the environment or agent. In addition to explanation fidelity, we
design different prototype shaping terms in the objective function to encourage
better interpretability. We conduct various experiments to test ProtoX. Results
show that ProtoX achieved high fidelity to the original black-box agent while
providing meaningful and understandable explanations.
</p></li>
</ul>

<h3>Title: Quantitative Assessment of Drought Impacts Using XGBoost based on the Drought Impact Reporter. (arXiv:2211.02768v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.02768">http://arxiv.org/abs/2211.02768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.02768] Quantitative Assessment of Drought Impacts Using XGBoost based on the Drought Impact Reporter](http://arxiv.org/abs/2211.02768)</code></li>
<li>Summary: <p>Under climate change, the increasing frequency, intensity, and spatial extent
of drought events lead to higher socio-economic costs. However, the
relationships between the hydro-meteorological indicators and drought impacts
are not identified well yet because of the complexity and data scarcity. In
this paper, we proposed a framework based on the extreme gradient model
(XGBoost) for Texas to predict multi-category drought impacts and connected a
typical drought indicator, Standardized Precipitation Index (SPI), to the
text-based impacts from the Drought Impact Reporter (DIR). The preliminary
results of this study showed an outstanding performance of the well-trained
models to assess drought impacts on agriculture, fire, society &amp; public health,
plants &amp; wildlife, as well as relief, response &amp; restrictions in Texas. It also
provided a possibility to appraise drought impacts using hydro-meteorological
indicators with the proposed framework in the United States, which could help
drought risk management by giving additional information and improving the
updating frequency of drought impacts. Our interpretation results using the
Shapley additive explanation (SHAP) interpretability technique revealed that
the rules guiding the predictions of XGBoost comply with domain expertise
knowledge around the role that SPI indicators play around drought impacts.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
