<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Segment and Track Anything. (arXiv:2305.06558v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06558">http://arxiv.org/abs/2305.06558</a></li>
<li>Code URL: <a href="https://github.com/z-x-yang/segment-and-track-anything">https://github.com/z-x-yang/segment-and-track-anything</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06558] Segment and Track Anything](http://arxiv.org/abs/2305.06558) #secure</code></li>
<li>Summary: <p>This report presents a framework called Segment And Track Anything (SAMTrack)
that allows users to precisely and effectively segment and track any object in
a video. Additionally, SAM-Track employs multimodal interaction methods that
enable users to select multiple objects in videos for tracking, corresponding
to their specific requirements. These interaction methods comprise click,
stroke, and text, each possessing unique benefits and capable of being employed
in combination. As a result, SAM-Track can be used across an array of fields,
ranging from drone technology, autonomous driving, medical imaging, augmented
reality, to biological analysis. SAM-Track amalgamates Segment Anything Model
(SAM), an interactive key-frame segmentation model, with our proposed AOT-based
tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022
challenge, to facilitate object tracking in video. In addition, SAM-Track
incorporates Grounding-DINO, which enables the framework to support text-based
interaction. We have demonstrated the remarkable capabilities of SAM-Track on
DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in
diverse applications. The project page is available at:
https://github.com/z-x-yang/Segment-and-Track-Anything.
</p></li>
</ul>

<h3>Title: Universally Composable Simultaneous Broadcast against a Dishonest Majority and Applications. (arXiv:2305.06468v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06468">http://arxiv.org/abs/2305.06468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06468] Universally Composable Simultaneous Broadcast against a Dishonest Majority and Applications](http://arxiv.org/abs/2305.06468) #secure</code></li>
<li>Summary: <p>Simultaneous broadcast (SBC) protocols [Chor et al., FOCS 1985] constitute a
special class of broadcast channels which have proved extremely useful in the
design of various distributed computing constructions (e.g., multiparty
computation, coin flipping, e-voting, fair bidding). As with any communication
channel, it is crucial that SBC security is composable, i.e., it is preserved
under concurrent protocol executions. The work of [Hevia, SCN 2006] proposes a
formal treatment of SBC in the Universal Composability (UC) framework [Canetti,
FOCS 2001] and a construction secure assuming an honest majority. In this work,
we provide a comprehensive revision of SBC in the UC setting and improve the
results of [Hevia, SCN 2006]. In particular, we present a new SBC functionality
that captures both simultaneity and liveness by considering a broadcast period
such that (i) within this period all messages are broadcast independently and
(ii) after the period ends, the session is terminated without requiring
participation of all parties. Next, we employ time-lock encryption (TLE) over a
standard broadcast channel to devise an SBC protocol that realizes our
functionality against any adaptive adversary corrupting up to all-but-one
parties. In our study, we capture synchronicity via a global clock [Katz et
al., TCC 2013], thus lifting the restrictions of the original synchronous
communication setting used in [Hevia, SCN 2006]. As a building block of
independent interest, we prove the first TLE protocol that is adaptively secure
in the UC setting, strengthening the main result of [Arapinis et al., ASIACRYPT
2021]. Finally, we formally exhibit the power of our SBC construction in the
design of UC-secure applications by presenting two interesting use cases: (i)
distributed generation of uniform random strings, and (ii) decentralized
electronic voting systems, without the presence of a special trusted party.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Exploiting Fine-Grained DCT Representations for Hiding Image-Level Messages within JPEG Images. (arXiv:2305.06582v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06582">http://arxiv.org/abs/2305.06582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06582] Exploiting Fine-Grained DCT Representations for Hiding Image-Level Messages within JPEG Images](http://arxiv.org/abs/2305.06582) #security</code></li>
<li>Summary: <p>Unlike hiding bit-level messages, hiding image-level messages is more
challenging, which requires large capacity, high imperceptibility, and high
security. Although recent advances in hiding image-level messages have been
remarkable, existing schemes are limited to lossless spatial images as covers
and cannot be directly applied to JPEG images, the ubiquitous lossy format
images in daily life. The difficulties of migration are caused by the lack of
targeted design and the loss of details due to lossy decompression and
re-compression. Considering that taking DCT densely on $8\times8$ image patches
is the core of the JPEG compression standard, we design a novel model called
\textsf{EFDR}, which can comprehensively \underline{E}xploit
\underline{F}ine-grained \underline{D}CT \underline{R}epresentations and embed
the secret image into quantized DCT coefficients to avoid the lossy process.
Specifically, we transform the JPEG cover image and hidden secret image into
fine-grained DCT representations that compact the frequency and are associated
with the inter-block and intra-block correlations. Subsequently, the
fine-grained DCT representations are further enhanced by a sub-band features
enhancement module. Afterward, a transformer-based invertibility module is
designed to fuse enhanced sub-band features. Such a design enables a
fine-grained self-attention on each sub-band and captures long-range
dependencies while maintaining excellent reversibility for hiding and recovery.
To our best knowledge, this is the first attempt to embed a color image of
equal size in a color JPEG image. Extensive experiments demonstrate the
effectiveness of our \textsf{EFDR} with superior performance.
</p></li>
</ul>

<h3>Title: Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network. (arXiv:2305.06842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06842">http://arxiv.org/abs/2305.06842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06842] Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network](http://arxiv.org/abs/2305.06842) #security</code></li>
<li>Summary: <p>Human communication is the vocal and non verbal signal to communicate with
others. Human expression is a significant biometric object in picture and
record databases of surveillance systems. Face appreciation has a serious role
in biometric methods and is good-looking for plentiful applications, including
visual scrutiny and security. Facial expressions are a form of nonverbal
communication; recognizing them helps improve the human machine interaction.
This paper proposes an idea for face and enlightenment invariant credit of
facial expressions by the images. In order on, the person's face can be
computed. Face expression is used in CNN classifier to categorize the acquired
picture into different emotion categories. It is a deep, feed-forward
artificial neural network. Outcome surpasses human presentation and shows poses
alternate performance. Varying lighting conditions can influence the fitting
process and reduce recognition precision. Results illustrate that dependable
facial appearance credited with changing lighting conditions for separating
reasonable facial terminology display emotions is an efficient representation
of clean and assorted moving expressions. This process can also manage the
proportions of dissimilar basic affecting expressions of those mixed jointly to
produce sensible emotional facial expressions. Our system contains a
pre-defined data set, which was residential by a statistics scientist and
includes all pure and varied expressions. On average, a data set has achieved
92.4% exact validation of the expressions synthesized by our technique. These
facial expressions are compared through the pre-defined data-position inside
our system. If it recognizes the person in an abnormal condition, an alert will
be passed to the nearby hospital/doctor seeing that a message.
</p></li>
</ul>

<h3>Title: HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning. (arXiv:2305.06430v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06430">http://arxiv.org/abs/2305.06430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06430] HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning](http://arxiv.org/abs/2305.06430) #security</code></li>
<li>Summary: <p>As IoT devices are becoming widely deployed, there exist many threats to
IoT-based systems due to their inherent vulnerabilities. One effective approach
to improving IoT security is to deploy IoT honeypot systems, which can collect
attack information and reveal the methods and strategies used by attackers.
However, building high-interaction IoT honeypots is challenging due to the
heterogeneity of IoT devices. Vulnerabilities in IoT devices typically depend
on specific device types or firmware versions, which encourages attackers to
perform pre-attack checks to gather device information before launching
attacks. Moreover, conventional honeypots are easily detected because their
replying logic differs from that of the IoT devices they try to mimic. To
address these problems, we develop an adaptive high-interaction honeypot for
IoT devices, called HoneyIoT. We first build a real device based attack trace
collection system to learn how attackers interact with IoT devices. We then
model the attack behavior through markov decision process and leverage
reinforcement learning techniques to learn the best responses to engage
attackers based on the attack trace. We also use differential analysis
techniques to mutate response values in some fields to generate high-fidelity
responses. HoneyIoT has been deployed on the public Internet. Experimental
results show that HoneyIoT can effectively bypass the pre-attack checks and
mislead the attackers into uploading malware. Furthermore, HoneyIoT is covert
against widely used reconnaissance and honeypot detection tools.
</p></li>
</ul>

<h3>Title: Assault and Battery: Evaluating the Security of Power Conversion Systems Against Electromagnetic Injection Attacks. (arXiv:2305.06901v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06901">http://arxiv.org/abs/2305.06901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06901] Assault and Battery: Evaluating the Security of Power Conversion Systems Against Electromagnetic Injection Attacks](http://arxiv.org/abs/2305.06901) #security</code></li>
<li>Summary: <p>Many modern devices, including critical infrastructures, depend on the
reliable operation of electrical power conversion systems. The small size and
versatility of switched-mode power converters has resulted in their widespread
adoption. Whereas transformer-based systems passively convert voltage,
switched-mode converters feature an actively regulated feedback loop, which
relies on accurate sensor measurements. Previous academic work has shown that
many types of sensors are vulnerable to Intentional Electromagnetic
Interference (IEMI) attacks, and it has been postulated that power converters,
too, are affected.
</p></li>
</ul>

<p>In this paper, we present the first detailed study on switched-mode power
converters by targeting their voltage and current sensors through IEMI attacks.
We present a theoretical framework for evaluating IEMI attacks against
feedback-based power supplies in the general case. We experimentally validate
our theoretical predictions by analyzing multiple AC-DC and DC-DC converters,
automotive grade current sensors, and dedicated battery chargers, and
demonstrate the systematic vulnerability of all examined categories under
real-world conditions. Finally, we demonstrate that sensor attacks on power
converters can cause permanent damage to Li-Ion batteries during the charging
process.
</p>

<h3>Title: REMaQE -- Reverse Engineering Math Equations from Executables. (arXiv:2305.06902v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06902">http://arxiv.org/abs/2305.06902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06902] REMaQE -- Reverse Engineering Math Equations from Executables](http://arxiv.org/abs/2305.06902) #security</code></li>
<li>Summary: <p>Cybersecurity attacks against industrial control systems and cyber-physical
systems can cause catastrophic real-world damage by infecting device binaries
with malware. Mitigating such attacks can benefit from reverse engineering
tools that recover sufficient semantic knowledge in terms of mathematical
operations in the code. Conventional reverse engineering tools can decompile
binaries to low-level code, but offer little semantic insight. This paper
proposes REMaQE, an automated framework for reverse engineering of math
equations from binary executables. REMaQE uses symbolic execution for dynamic
analysis of the binary to extract the relevant semantic knowledge of the
implemented algorithms. REMaQE provides an automatic parameter analysis pass
which also leverages symbolic execution to identify input, output, and constant
parameters of the implemented math equations. REMaQE automatically handles
parameters accessed via registers, the stack, global memory, or pointers, and
supports reverse engineering of object-oriented implementations such as C++
classes. REMaQE uses an algebraic simplification method which allows it to
scale to complex conditional equations with ease. These features make REMaQE
stand out over existing reverse engineering approaches for math equations. On a
dataset of randomly generated math equations compiled to binaries from C and
Simulink implementations, REMaQE accurately recovers a semantically matching
equation for 97.53% of the models. For complex equations with more operations,
accuracy stays consistently over 94%. REMaQE executes in 0.25 seconds on
average and in 1.3 seconds for more complex equations. This real-time execution
speed enables a smooth integration in an interactive mathematics-oriented
reverse engineering workflow.
</p></li>
</ul>

<h3>Title: Watch This Space: Securing Satellite Communication through Resilient Transmitter Fingerprinting. (arXiv:2305.06947v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06947">http://arxiv.org/abs/2305.06947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06947] Watch This Space: Securing Satellite Communication through Resilient Transmitter Fingerprinting](http://arxiv.org/abs/2305.06947) #security</code></li>
<li>Summary: <p>Due to an increase in the availability of cheap off-the-shelf radio hardware,
spoofing and replay attacks on satellite ground systems have become more
accessible than ever. This is particularly a problem for legacy systems, many
of which do not offer cryptographic security and cannot be patched to support
novel security measures.
</p></li>
</ul>

<p>In this paper we explore radio transmitter fingerprinting in satellite
systems. We introduce the SatIQ system, proposing novel techniques for
authenticating transmissions using characteristics of transmitter hardware
expressed as impairments on the downlinked signal. We look in particular at
high sample rate fingerprinting, making fingerprints difficult to forge without
similarly high sample rate transmitting hardware, thus raising the budget for
attacks. We also examine the difficulty of this approach with high levels of
atmospheric noise and multipath scattering, and analyze potential solutions to
this problem.
</p>
<p>We focus on the Iridium satellite constellation, for which we collected
1010464 messages at a sample rate of 25 MS/s. We use this data to train a
fingerprinting model consisting of an autoencoder combined with a Siamese
neural network, enabling the model to learn an efficient encoding of message
headers that preserves identifying information.
</p>
<p>We demonstrate the system's robustness under attack by replaying messages
using a Software-Defined Radio, achieving an Equal Error Rate of 0.120, and ROC
AUC of 0.946. Finally, we analyze its stability over time by introducing a time
gap between training and testing data, and its extensibility by introducing new
transmitters which have not been seen before. We conclude that our techniques
are useful for building systems that are stable over time, can be used
immediately with new transmitters without retraining, and provide robustness
against spoofing and replay by raising the required budget for attacks.
</p>

<h3>Title: Specification and Verification of Side-channel Security for Open-source Processors via Leakage Contracts. (arXiv:2305.06979v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06979">http://arxiv.org/abs/2305.06979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06979] Specification and Verification of Side-channel Security for Open-source Processors via Leakage Contracts](http://arxiv.org/abs/2305.06979) #security</code></li>
<li>Summary: <p>Leakage contracts have recently been proposed as a new security abstraction
at the Instruction Set Architecture (ISA) level. Such contracts aim to
faithfully capture the information processors may leak through side effects of
their microarchitectural implementations. However, so far, we lack a
verification methodology to check that a processor actually satisfies a given
leakage contract. In this paper, we address this problem by developing LeaVe,
the first tool for verifying register-transfer-level (RTL) processor designs
against ISA-level leakage contracts. To this end, we introduce a decoupling
theorem that separates security and functional correctness concerns when
verifying contract satisfaction. LeaVe leverages this decoupling to make
verification of contract satisfaction practical. To scale to realistic
processor designs LeaVe further employs inductive reasoning on relational
abstractions. Using LeaVe, we precisely characterize the side-channel security
guarantees provided by three open-source RISC-V processors, thereby obtaining
the first contract satisfaction proofs for RTL processor designs.
</p></li>
</ul>

<h3>Title: Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06360">http://arxiv.org/abs/2305.06360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06360] Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy](http://arxiv.org/abs/2305.06360) #security</code></li>
<li>Summary: <p>Machine unlearning (MU) is a field that is gaining increasing attention due
to the need to remove or modify predictions made by machine learning (ML)
models. While training models have become more efficient and accurate, the
importance of unlearning previously learned information has become increasingly
significant in fields such as privacy, security, and fairness. This paper
presents a comprehensive survey of MU, covering current state-of-the-art
techniques and approaches, including data deletion, perturbation, and model
updates. In addition, commonly used metrics and datasets are also presented.
The paper also highlights the challenges that need to be addressed, including
attack sophistication, standardization, transferability, interpretability,
training data, and resource constraints. The contributions of this paper
include discussions about the potential benefits of MU and its future
directions in Natural Language Processing, Computer vision, and Recommender
Systems. Additionally, the paper emphasizes the need for researchers and
practitioners to continue exploring and refining unlearning techniques to
ensure that ML models can adapt to changing circumstances while maintaining
user trust. The importance of unlearning is further highlighted in making
Artificial Intelligence (AI) more trustworthy and transparent, especially with
the increasing importance of AI in various domains that involve large amounts
of personal user data
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06626">http://arxiv.org/abs/2305.06626</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06626] When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks](http://arxiv.org/abs/2305.06626) #privacy</code></li>
<li>Summary: <p>Though majority vote among annotators is typically used for ground truth
labels in natural language processing, annotator disagreement in tasks such as
hate speech detection may reflect differences among group opinions, not noise.
Thus, a crucial problem in hate speech detection is whether a statement is
offensive to the demographic group that it targets, which may constitute a
small fraction of the annotator pool. We construct a model that predicts
individual annotator ratings on potentially offensive text and combines this
information with the predicted target group of the text to model the opinions
of target group members. We show gains across a range of metrics, including
raising performance over the baseline by 22% at predicting individual
annotators' ratings and 33% at predicting variance among annotators, which
provides a method of measuring model uncertainty downstream. We find that
annotators' ratings can be predicted using their demographic information and
opinions on online content, without the need to track identifying annotator IDs
that link each annotator to their ratings. We also find that use of
non-invasive survey questions on annotators' online experiences helps to
maximize privacy and minimize unnecessary collection of demographic information
when predicting annotators' opinions.
</p></li>
</ul>

<h3>Title: Speranza: Usable, privacy-friendly software signing. (arXiv:2305.06463v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06463">http://arxiv.org/abs/2305.06463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06463] Speranza: Usable, privacy-friendly software signing](http://arxiv.org/abs/2305.06463) #privacy</code></li>
<li>Summary: <p>Software repositories, used for wide-scale open software distribution, are a
significant vector for security attacks. Much of this malicious behavior can be
traced to a lack of strong authentication for software. To mitigate this
problem, digital signatures provide confidence in the authenticity and
authorization for signers of the software, but introduce privacy problems by
exposing maintainers' personally identifiable information. The contribution of
this project, Speranza, is to allow for verification of authenticity for
software packages in a repository while providing anonymity to signers through
the use of zero-knowledge identity co-commitments.
</p></li>
</ul>

<p>In Speranza, a signer uses an automated certificate authority (CA) to create
a private identity-backed signature and proof of authorization. Verifiers check
that a signer was authorized to publish a package, without learning the
signer's identity. The package repository keeps a private mapping from package
names to the identities of authorized signers, but publishes only commitments
to identities in a public map. When issuing certificates, the CA issues the
certificate to a distinct commitment to the same identity. The signer then
creates a zero-knowledge proof of a commitment that these are identity
co-commitments.
</p>
<p>We implemented a proof-of-concept, finding that costs to maintainers
(signing) and end users (verifying) are small, even for a repository with
millions of packages: 404 us and 372 us, respectively. End users must learn the
authorization policy in order to verify packages. In a naive approach, they
must download the policy for every package in advance (possibly 100 MiB total,
or more); we use techniques inspired by recent key transparency systems to
reduce this to 2 KiB. Server costs in this system are negligible. Our
evaluation finds that Speranza is practical on the scale of the largest
software repositories.
</p>

<h3>Title: Securing Distributed SGD against Gradient Leakage Threats. (arXiv:2305.06473v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06473">http://arxiv.org/abs/2305.06473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06473] Securing Distributed SGD against Gradient Leakage Threats](http://arxiv.org/abs/2305.06473) #privacy</code></li>
<li>Summary: <p>This paper presents a holistic approach to gradient leakage resilient
distributed Stochastic Gradient Descent (SGD). First, we analyze two types of
strategies for privacy-enhanced federated learning: (i) gradient pruning with
random selection or low-rank filtering and (ii) gradient perturbation with
additive random noise or differential privacy noise. We analyze the inherent
limitations of these approaches and their underlying impact on privacy
guarantee, model accuracy, and attack resilience. Next, we present a gradient
leakage resilient approach to securing distributed SGD in federated learning,
with differential privacy controlled noise as the tool. Unlike conventional
methods with the per-client federated noise injection and fixed noise parameter
strategy, our approach keeps track of the trend of per-example gradient
updates. It makes adaptive noise injection closely aligned throughout the
federated model training. Finally, we provide an empirical privacy analysis on
the privacy guarantee, model utility, and attack resilience of the proposed
approach. Extensive evaluation using five benchmark datasets demonstrates that
our gradient leakage resilient approach can outperform the state-of-the-art
methods with competitive accuracy performance, strong differential privacy
guarantee, and high resilience against gradient leakage attacks. The code
associated with this paper can be found:
https://github.com/git-disl/Fed-alphaCDP.
</p></li>
</ul>

<h3>Title: MISO: Legacy-compatible Privacy-preserving Single Sign-on using Trusted Execution Environments. (arXiv:2305.06833v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06833">http://arxiv.org/abs/2305.06833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06833] MISO: Legacy-compatible Privacy-preserving Single Sign-on using Trusted Execution Environments](http://arxiv.org/abs/2305.06833) #privacy</code></li>
<li>Summary: <p>Single sign-on (SSO) allows users to authenticate to third-party applications
through a central identity provider. Despite their wide adoption, deployed SSO
systems suffer from privacy problems such as user tracking by the identity
provider. While numerous solutions have been proposed by academic papers, none
were adopted because they require modifying identity providers, a significant
adoption barrier in practice. Solutions do get deployed, however, fail to
eliminate major privacy issues. Leveraging Trusted Execution Environments
(TEEs), we propose MISO, the first privacy-preserving SSO system that is
completely compatible with existing identity providers (such as Google and
Facebook). This means MISO can be easily integrated into existing SSO ecosystem
today and benefit end users. MISO also enables new functionality that standard
SSO cannot offer: MISO allows users to leverage multiple identity providers in
a single SSO workflow, potentially in a threshold fashion, to better protect
user accounts. We fully implemented MISO based on Intel SGX. Our evaluation
shows that MISO can handle high user concurrency with practical performance.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06424">http://arxiv.org/abs/2305.06424</a></li>
<li>Code URL: <a href="https://github.com/hongwang600/flair">https://github.com/hongwang600/flair</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06424] Bot or Human? Detecting ChatGPT Imposters with A Single Question](http://arxiv.org/abs/2305.06424) #protect</code></li>
<li>Summary: <p>Large language models like ChatGPT have recently demonstrated impressive
capabilities in natural language understanding and generation, enabling various
applications including translation, essay writing, and chit-chatting. However,
there is a concern that they can be misused for malicious purposes, such as
fraud or denial-of-service attacks. Therefore, it is crucial to develop methods
for detecting whether the party involved in a conversation is a bot or a human.
In this paper, we propose a framework named FLAIR, Finding Large language model
Authenticity via a single Inquiry and Response, to detect conversational bots
in an online manner. Specifically, we target a single question scenario that
can effectively differentiate human users from bots. The questions are divided
into two categories: those that are easy for humans but difficult for bots
(e.g., counting, substitution, positioning, noise filtering, and ASCII art),
and those that are easy for bots but difficult for humans (e.g., memorization
and computation). Our approach shows different strengths of these questions in
their effectiveness, providing a new way for online service providers to
protect themselves against nefarious activities and ensure that they are
serving real users. We open-sourced our dataset on
https://github.com/hongwang600/FLAIR and welcome contributions from the
community to enrich such detection datasets.
</p></li>
</ul>

<h3>Title: Simplification of General Mixed Boolean-Arithmetic Expressions: GAMBA. (arXiv:2305.06763v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06763">http://arxiv.org/abs/2305.06763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06763] Simplification of General Mixed Boolean-Arithmetic Expressions: GAMBA](http://arxiv.org/abs/2305.06763) #protect</code></li>
<li>Summary: <p>Malware code often resorts to various self-protection techniques to
complicate analysis. One such technique is applying Mixed-Boolean Arithmetic
(MBA) expressions as a way to create opaque predicates and diversify and
obfuscate the data flow.
</p></li>
</ul>

<p>In this work we aim to provide tools for the simplification of nonlinear MBA
expressions in a very practical context to compete in the arms race between the
generation of hard, diverse MBAs and their analysis. The proposed algorithm
GAMBA employs algebraic rewriting at its core and extends SiMBA. It achieves
efficient deobfuscation of MBA expressions from the most widely tested public
datasets and simplifies expressions to their ground truths in most cases,
surpassing peer tools.
</p>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Inter-frame Accelerate Attack against Video Interpolation Models. (arXiv:2305.06540v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06540">http://arxiv.org/abs/2305.06540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06540] Inter-frame Accelerate Attack against Video Interpolation Models](http://arxiv.org/abs/2305.06540) #attack</code></li>
<li>Summary: <p>Deep learning based video frame interpolation (VIF) method, aiming to
synthesis the intermediate frames to enhance video quality, have been highly
developed in the past few years. This paper investigates the adversarial
robustness of VIF models. We apply adversarial attacks to VIF models and find
that the VIF models are very vulnerable to adversarial examples. To improve
attack efficiency, we suggest to make full use of the property of video frame
interpolation task. The intuition is that the gap between adjacent frames would
be small, leading to the corresponding adversarial perturbations being similar
as well. Then we propose a novel attack method named Inter-frame Accelerate
Attack (IAA) that initializes the perturbation as the perturbation for the
previous adjacent frame and reduces the number of attack iterations. It is
shown that our method can improve attack efficiency greatly while achieving
comparable attack performance with traditional methods. Besides, we also extend
our method to video recognition models which are higher level vision tasks and
achieves great attack efficiency.
</p></li>
</ul>

<h3>Title: Distracting Downpour: Adversarial Weather Attacks for Motion Estimation. (arXiv:2305.06716v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06716">http://arxiv.org/abs/2305.06716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06716] Distracting Downpour: Adversarial Weather Attacks for Motion Estimation](http://arxiv.org/abs/2305.06716) #attack</code></li>
<li>Summary: <p>Current adversarial attacks on motion estimation, or optical flow, optimize
small per-pixel perturbations, which are unlikely to appear in the real world.
In contrast, adverse weather conditions constitute a much more realistic threat
scenario. Hence, in this work, we present a novel attack on motion estimation
that exploits adversarially optimized particles to mimic weather effects like
snowflakes, rain streaks or fog clouds. At the core of our attack framework is
a differentiable particle rendering system that integrates particles (i)
consistently over multiple time steps (ii) into the 3D space (iii) with a
photo-realistic appearance. Through optimization, we obtain adversarial weather
that significantly impacts the motion estimation. Surprisingly, methods that
previously showed good robustness towards small per-pixel perturbations are
particularly vulnerable to adversarial weather. At the same time, augmenting
the training with non-optimized weather increases a method's robustness towards
weather effects and improves generalizability at almost no additional cost.
</p></li>
</ul>

<h3>Title: Prevention of shoulder-surfing attacks using shifting condition using digraph substitution rules. (arXiv:2305.06549v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06549">http://arxiv.org/abs/2305.06549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06549] Prevention of shoulder-surfing attacks using shifting condition using digraph substitution rules](http://arxiv.org/abs/2305.06549) #attack</code></li>
<li>Summary: <p>Graphical passwords are implemented as an alternative scheme to replace
alphanumeric passwords to help users to memorize their password. However, most
of the graphical password systems are vulnerable to shoulder-surfing attack due
to the usage of the visual interface. In this research, a method that uses
shifting condition with digraph substitution rules is proposed to address
shoulder-surfing attack problem. The proposed algorithm uses both password
images and decoy images throughout the user authentication procedure to confuse
adversaries from obtaining the password images via direct observation or
watching from a recorded session. The pass-images generated by this suggested
algorithm are random and can only be generated if the algorithm is fully
understood. As a result, adversaries will have no clue to obtain the right
password images to log in. A user study was undertaken to assess the proposed
method's effectiveness to avoid shoulder-surfing attacks. The results of the
user study indicate that the proposed approach can withstand shoulder-surfing
attacks (both direct observation and video recording method).The proposed
method was tested and the results showed that it is able to resist
shoulder-surfing and frequency of occurrence analysis attacks. Moreover, the
experience gained in this research can be pervaded the gap on the realm of
knowledge of the graphical password.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks. (arXiv:2305.06382v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06382">http://arxiv.org/abs/2305.06382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06382] HyperE2VID: Improving Event-Based Video Reconstruction via Hypernetworks](http://arxiv.org/abs/2305.06382) #robust</code></li>
<li>Summary: <p>Event-based cameras are becoming increasingly popular for their ability to
capture high-speed motion with low latency and high dynamic range. However,
generating videos from events remains challenging due to the highly sparse and
varying nature of event data. To address this, in this study, we propose
HyperE2VID, a dynamic neural network architecture for event-based video
reconstruction. Our approach uses hypernetworks and dynamic convolutions to
generate per-pixel adaptive filters guided by a context fusion module that
combines information from event voxel grids and previously reconstructed
intensity images. We also employ a curriculum learning strategy to train the
network more robustly. Experimental results demonstrate that HyperE2VID
achieves better reconstruction quality with fewer parameters and faster
inference time than the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: An Empirical Study on the Robustness of the Segment Anything Model (SAM). (arXiv:2305.06422v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06422">http://arxiv.org/abs/2305.06422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06422] An Empirical Study on the Robustness of the Segment Anything Model (SAM)](http://arxiv.org/abs/2305.06422) #robust</code></li>
<li>Summary: <p>The Segment Anything Model (SAM) is a foundation model for general image
segmentation. Although it exhibits impressive performance predominantly on
natural images, understanding its robustness against various image
perturbations and domains is critical for real-world applications where such
challenges frequently arise. In this study we conduct a comprehensive
robustness investigation of SAM under diverse real-world conditions. Our
experiments encompass a wide range of image perturbations. Our experimental
results demonstrate that SAM's performance generally declines under perturbed
images, with varying degrees of vulnerability across different perturbations.
By customizing prompting techniques and leveraging domain knowledge based on
the unique characteristics of each dataset, the model's resilience to these
perturbations can be enhanced, addressing dataset-specific challenges. This
work sheds light on the limitations and strengths of SAM in real-world
applications, promoting the development of more robust and versatile image
segmentation solutions.
</p></li>
</ul>

<h3>Title: Can SAM Boost Video Super-Resolution?. (arXiv:2305.06524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06524">http://arxiv.org/abs/2305.06524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06524] Can SAM Boost Video Super-Resolution?](http://arxiv.org/abs/2305.06524) #robust</code></li>
<li>Summary: <p>The primary challenge in video super-resolution (VSR) is to handle large
motions in the input frames, which makes it difficult to accurately aggregate
information from multiple frames. Existing works either adopt deformable
convolutions or estimate optical flow as a prior to establish correspondences
between frames for the effective alignment and fusion. However, they fail to
take into account the valuable semantic information that can greatly enhance
it; and flow-based methods heavily rely on the accuracy of a flow estimate
model, which may not provide precise flows given two low-resolution frames.
</p></li>
</ul>

<p>In this paper, we investigate a more robust and semantic-aware prior for
enhanced VSR by utilizing the Segment Anything Model (SAM), a powerful
foundational model that is less susceptible to image degradation. To use the
SAM-based prior, we propose a simple yet effective module -- SAM-guidEd
refinEment Module (SEEM), which can enhance both alignment and fusion
procedures by the utilization of semantic information. This light-weight
plug-in module is specifically designed to not only leverage the attention
mechanism for the generation of semantic-aware feature but also be easily and
seamlessly integrated into existing methods. Concretely, we apply our SEEM to
two representative methods, EDVR and BasicVSR, resulting in consistently
improved performance with minimal implementation effort, on three widely used
VSR datasets: Vimeo-90K, REDS and Vid4. More importantly, we found that the
proposed SEEM can advance the existing methods in an efficient tuning manner,
providing increased flexibility in adjusting the balance between performance
and the number of training parameters. Code will be open-source soon.
</p>

<h3>Title: WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents. (arXiv:2305.06553v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06553">http://arxiv.org/abs/2305.06553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06553] WeLayout: WeChat Layout Analysis System for the ICDAR 2023 Competition on Robust Layout Segmentation in Corporate Documents](http://arxiv.org/abs/2305.06553) #robust</code></li>
<li>Summary: <p>In this paper, we introduce WeLayout, a novel system for segmenting the
layout of corporate documents, which stands for WeChat Layout Analysis System.
Our approach utilizes a sophisticated ensemble of DINO and YOLO models,
specifically developed for the ICDAR 2023 Competition on Robust Layout
Segmentation. Our method significantly surpasses the baseline, securing a top
position on the leaderboard with a mAP of 70.0. To achieve this performance, we
concentrated on enhancing various aspects of the task, such as dataset
augmentation, model architecture, bounding box refinement, and model ensemble
techniques. Additionally, we trained the data separately for each document
category to ensure a higher mean submission score. We also developed an
algorithm for cell matching to further improve our performance. To identify the
optimal weights and IoU thresholds for our model ensemble, we employed a
Bayesian optimization algorithm called the Tree-Structured Parzen Estimator.
Our approach effectively demonstrates the benefits of combining query-based and
anchor-free models for achieving robust layout segmentation in corporate
documents.
</p></li>
</ul>

<h3>Title: Hyperbolic Deep Learning in Computer Vision: A Survey. (arXiv:2305.06611v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06611">http://arxiv.org/abs/2305.06611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06611] Hyperbolic Deep Learning in Computer Vision: A Survey](http://arxiv.org/abs/2305.06611) #robust</code></li>
<li>Summary: <p>Deep representation learning is a ubiquitous part of modern computer vision.
While Euclidean space has been the de facto standard manifold for learning
visual representations, hyperbolic space has recently gained rapid traction for
learning in computer vision. Specifically, hyperbolic learning has shown a
strong potential to embed hierarchical structures, learn from limited samples,
quantify uncertainty, add robustness, limit error severity, and more. In this
paper, we provide a categorization and in-depth overview of current literature
on hyperbolic learning for computer vision. We research both supervised and
unsupervised literature and identify three main research themes in each
direction. We outline how hyperbolic learning is performed in all themes and
discuss the main research problems that benefit from current advances in
hyperbolic learning for computer vision. Moreover, we provide a high-level
intuition behind hyperbolic geometry and outline open research questions to
further advance research in this direction.
</p></li>
</ul>

<h3>Title: DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles. (arXiv:2305.06820v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06820">http://arxiv.org/abs/2305.06820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06820] DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles](http://arxiv.org/abs/2305.06820) #robust</code></li>
<li>Summary: <p>Autonomous vehicles demand high accuracy and robustness of perception
algorithms. To develop efficient and scalable perception algorithms, the
maximum information should be extracted from the available sensor data. In this
work, we present our concept for an end-to-end perception architecture, named
DeepSTEP. The deep learning-based architecture processes raw sensor data from
the camera, LiDAR, and RaDAR, and combines the extracted data in a deep fusion
network. The output of this deep fusion network is a shared feature space,
which is used by perception head networks to fulfill several perception tasks,
such as object detection or local mapping. DeepSTEP incorporates multiple ideas
to advance state of the art: First, combining detection and localization into a
single pipeline allows for efficient processing to reduce computational
overhead and further improves overall performance. Second, the architecture
leverages the temporal domain by using a self-attention mechanism that focuses
on the most important features. We believe that our concept of DeepSTEP will
advance the development of end-to-end perception systems. The network will be
deployed on our research vehicle, which will be used as a platform for data
collection, real-world testing, and validation. In conclusion, DeepSTEP
represents a significant advancement in the field of perception for autonomous
vehicles. The architecture's end-to-end design, time-aware attention mechanism,
and integration of multiple perception tasks make it a promising solution for
real-world deployment. This research is a work in progress and presents the
first concept of establishing a novel perception pipeline.
</p></li>
</ul>

<h3>Title: Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. (arXiv:2305.06522v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06522">http://arxiv.org/abs/2305.06522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06522] Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications](http://arxiv.org/abs/2305.06522) #robust</code></li>
<li>Summary: <p>Large-scale pre-trained language models have shown outstanding performance in
a variety of NLP tasks. However, they are also known to be significantly
brittle against specifically crafted adversarial examples, leading to
increasing interest in probing the adversarial robustness of NLP systems. We
introduce RSMI, a novel two-stage framework that combines randomized smoothing
(RS) with masked inference (MI) to improve the adversarial robustness of NLP
systems. RS transforms a classifier into a smoothed classifier to obtain robust
representations, whereas MI forces a model to exploit the surrounding context
of a masked token in an input sequence. RSMI improves adversarial robustness by
2 to 3 times over existing state-of-the-art methods on benchmark datasets. We
also perform in-depth qualitative analysis to validate the effectiveness of the
different stages of RSMI and probe the impact of its components through
extensive ablations. By empirically proving the stability of RSMI, we put it
forward as a practical method to robustly train large-scale NLP models. Our
code and datasets are available at https://github.com/Han8931/rsmi_nlp
</p></li>
</ul>

<h3>Title: SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06586">http://arxiv.org/abs/2305.06586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06586] SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)](http://arxiv.org/abs/2305.06586) #robust</code></li>
<li>Summary: <p>We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual
Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task
focused on methods to identify complex fine-grained named entities (like
WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and
multilingual scenarios, as well as noisy settings. The task used the MultiCoNER
V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,
Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and
Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It
attracted 842 submissions from 47 teams, and 34 teams submitted system papers.
Results showed that complex entity types such as media titles and product names
were the most challenging. Methods fusing external knowledge into transformer
models achieved the best performance, and the largest gains were on the
Creative Work and Group classes, which are still challenging even with external
knowledge. Some fine-grained classes proved to be more challenging than others,
such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data
has a significant impact on model performance, with an average drop of 10% on
the noisy subset. The task highlights the need for future research on improving
NER robustness on noisy data containing complex entities.
</p></li>
</ul>

<h3>Title: THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment. (arXiv:2305.06817v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06817">http://arxiv.org/abs/2305.06817</a></li>
<li>Code URL: <a href="https://github.com/cshaitao/thuir-coliee2023">https://github.com/cshaitao/thuir-coliee2023</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06817] THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment](http://arxiv.org/abs/2305.06817) #robust</code></li>
<li>Summary: <p>This paper describes the approach of the THUIR team at the COLIEE 2023 Legal
Case Entailment task. This task requires the participant to identify a specific
paragraph from a given supporting case that entails the decision for the query
case. We try traditional lexical matching methods and pre-trained language
models with different sizes. Furthermore, learning-to-rank methods are employed
to further improve performance. However, learning-to-rank is not very robust on
this task. which suggests that answer passages cannot simply be determined with
information retrieval techniques. Experimental results show that more
parameters and legal knowledge contribute to the legal case entailment task.
Finally, we get the third place in COLIEE 2023. The implementation of our
method can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.
</p></li>
</ul>

<h3>Title: Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06841">http://arxiv.org/abs/2305.06841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06841] Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models](http://arxiv.org/abs/2305.06841) #robust</code></li>
<li>Summary: <p>While the Large Language Models (LLMs) dominate a majority of language
understanding tasks, previous work shows that some of these results are
supported by modelling spurious correlations of training datasets. Authors
commonly assess model robustness by evaluating their models on
out-of-distribution (OOD) datasets of the same task, but these datasets might
share the bias of the training dataset.
</p></li>
</ul>

<p>We propose a simple method for measuring a scale of models' reliance on any
identified spurious feature and assess the robustness towards a large set of
known and newly found prediction biases for various pre-trained models and
debiasing methods in Question Answering (QA). We find that the reported OOD
gains of debiasing methods can not be explained by mitigated reliance on biased
features, suggesting that biases are shared among QA datasets. We further
evidence this by measuring that performance of OOD models depends on bias
features comparably to the ID model, motivating future work to refine the
reports of LLMs' robustness to a level of known spurious features.
</p>

<h3>Title: Matrix tri-factorization over the tropical semiring. (arXiv:2305.06624v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06624">http://arxiv.org/abs/2305.06624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06624] Matrix tri-factorization over the tropical semiring](http://arxiv.org/abs/2305.06624) #robust</code></li>
<li>Summary: <p>Tropical semiring has proven successful in several research areas, including
optimal control, bioinformatics, discrete event systems, or solving a decision
problem. In previous studies, a matrix two-factorization algorithm based on the
tropical semiring has been applied to investigate bipartite and tripartite
networks. Tri-factorization algorithms based on standard linear algebra are
used for solving tasks such as data fusion, co-clustering, matrix completion,
community detection, and more. However, there is currently no tropical matrix
tri-factorization approach, which would allow for the analysis of multipartite
networks with a high number of parts. To address this, we propose the
triFastSTMF algorithm, which performs tri-factorization over the tropical
semiring. We apply it to analyze a four-partition network structure and recover
the edge lengths of the network. We show that triFastSTMF performs similarly to
Fast-NMTF in terms of approximation and prediction performance when fitted on
the whole network. When trained on a specific subnetwork and used to predict
the whole network, triFastSTMF outperforms Fast-NMTF by several orders of
magnitude smaller error. The robustness of triFastSTMF is due to tropical
operations, which are less prone to predict large values compared to standard
operations.
</p></li>
</ul>

<h3>Title: On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06657">http://arxiv.org/abs/2305.06657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06657] On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm](http://arxiv.org/abs/2305.06657) #robust</code></li>
<li>Summary: <p>Robust reinforcement learning (RL) aims at learning a policy that optimizes
the worst-case performance over an uncertainty set. Given nominal Markov
decision process (N-MDP) that generates samples for training, the set contains
MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a
new uncertainty set containing more realistic MDPs in practice than the
existing sets. Using this uncertainty set, we present a robust RL, named
ARQ-Learning, for tabular cases. Also, we characterize the finite-time error
bounds and prove that it converges as fast as Q-Learning and robust Q-Learning
(i.e., the state-of-the-art robust RL method) while providing better robustness
for real applications. We propose {\em pessimistic agent} that efficiently
tackles the key bottleneck for the extension of ARQ-Learning into large or
continuous state spaces. Using this technique, we first propose PRQ-Learning.
To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG,
respectively. We emphasize that our technique can be easily combined with the
other popular model-free methods. Via experiments, we demonstrate the
superiority of the proposed methods in various RL applications with model
uncertainties.
</p></li>
</ul>

<h3>Title: Towards Theoretical Understanding of Data-Driven Policy Refinement. (arXiv:2305.06796v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06796">http://arxiv.org/abs/2305.06796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06796] Towards Theoretical Understanding of Data-Driven Policy Refinement](http://arxiv.org/abs/2305.06796) #robust</code></li>
<li>Summary: <p>This paper presents an approach for data-driven policy refinement in
reinforcement learning, specifically designed for safety-critical applications.
Our methodology leverages the strengths of data-driven optimization and
reinforcement learning to enhance policy safety and optimality through
iterative refinement. Our principal contribution lies in the mathematical
formulation of this data-driven policy refinement concept. This framework
systematically improves reinforcement learning policies by learning from
counterexamples surfaced during data-driven verification. Furthermore, we
present a series of theorems elucidating key theoretical properties of our
approach, including convergence, robustness bounds, generalization error, and
resilience to model mismatch. These results not only validate the effectiveness
of our methodology but also contribute to a deeper understanding of its
behavior in different environments and scenarios.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species. (arXiv:2305.06695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06695">http://arxiv.org/abs/2305.06695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06695] Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species](http://arxiv.org/abs/2305.06695) #biometric</code></li>
<li>Summary: <p>Visual as well as genetic biometrics are routinely employed to identify
species and individuals in biological applications. However, no attempts have
been made in this domain to computationally enhance visual classification of
rare classes with little image data via genetics. In this paper, we thus
propose aligned visual-genetic inference spaces with the aim to implicitly
encode cross-domain associations for improved performance. We demonstrate for
the first time that such alignment can be achieved via deep embedding models
and that the approach is directly applicable to boosting long-tailed
recognition (LTR) particularly for rare species. We experimentally demonstrate
the efficacy of the concept via application to microscopic imagery of 30k+
planktic foraminifer shells across 32 species when used together with
independent genetic data samples. Most importantly for practitioners, we show
that visual-genetic alignment can significantly benefit visual-only recognition
of the rarest species. Technically, we pre-train a visual ResNet50 deep
learning model using triplet loss formulations to create an initial embedding
space. We re-structure this space based on genetic anchors embedded via a
Sequence Graph Transform (SGT) and linked to visual data by cross-domain cosine
alignment. We show that an LTR approach improves the state-of-the-art across
all benchmarks and that adding our visual-genetic alignment improves per-class
and particularly rare tail class benchmarks significantly further. We conclude
that visual-genetic alignment can be a highly effective tool for complementing
visual biological data containing rare classes. The concept proposed may serve
as an important future tool for integrating genetics and imageomics towards a
more complete scientific representation of taxonomic spaces and life itself.
Code, weights, and data splits are published for full reproducibility.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Towards L-System Captioning for Tree Reconstruction. (arXiv:2305.06483v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06483">http://arxiv.org/abs/2305.06483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06483] Towards L-System Captioning for Tree Reconstruction](http://arxiv.org/abs/2305.06483) #extraction</code></li>
<li>Summary: <p>This work proposes a novel concept for tree and plant reconstruction by
directly inferring a Lindenmayer-System (L-System) word representation from
image data in an image captioning approach. We train a model end-to-end which
is able to translate given images into L-System words as a description of the
displayed tree. To prove this concept, we demonstrate the applicability on 2D
tree topologies. Transferred to real image data, this novel idea could lead to
more efficient, accurate and semantically meaningful tree and plant
reconstruction without using error-prone point cloud extraction, and other
processes usually utilized in tree reconstruction. Furthermore, this approach
bypasses the need for a predefined L-System grammar and enables
species-specific L-System inference without biological knowledge.
</p></li>
</ul>

<h3>Title: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. (arXiv:2305.06500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06500">http://arxiv.org/abs/2305.06500</a></li>
<li>Code URL: <a href="https://github.com/salesforce/lavis">https://github.com/salesforce/lavis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06500] InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](http://arxiv.org/abs/2305.06500) #extraction</code></li>
<li>Summary: <p>General-purpose language models that can solve various language-domain tasks
have emerged driven by the pre-training and instruction-tuning pipeline.
However, building general-purpose vision-language models is challenging due to
the increased task discrepancy introduced by the additional visual input.
Although vision-language pre-training has been widely studied, vision-language
instruction tuning remains relatively less explored. In this paper, we conduct
a systematic and comprehensive study on vision-language instruction tuning
based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly
available datasets, transform them into instruction tuning format and
categorize them into two clusters for held-in instruction tuning and held-out
zero-shot evaluation. Additionally, we introduce instruction-aware visual
feature extraction, a crucial method that enables the model to extract
informative features tailored to the given instruction. The resulting
InstructBLIP models achieve state-of-the-art zero-shot performance across all
13 held-out datasets, substantially outperforming BLIP-2 and the larger
Flamingo. Our models also lead to state-of-the-art performance when finetuned
on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG).
Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over
concurrent multimodal models. All InstructBLIP models have been open-sourced at
https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.
</p></li>
</ul>

<h3>Title: Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction. (arXiv:2305.06616v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06616">http://arxiv.org/abs/2305.06616</a></li>
<li>Code URL: <a href="https://github.com/nju-websoft/sckd">https://github.com/nju-websoft/sckd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06616] Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction](http://arxiv.org/abs/2305.06616) #extraction</code></li>
<li>Summary: <p>Continual few-shot relation extraction (RE) aims to continuously train a
model for new relations with few labeled training data, of which the major
challenges are the catastrophic forgetting of old relations and the overfitting
caused by data sparsity. In this paper, we propose a new model, namely SCKD, to
accomplish the continual few-shot RE task. Specifically, we design serial
knowledge distillation to preserve the prior knowledge from previous models and
conduct contrastive learning with pseudo samples to keep the representations of
samples in different relations sufficiently distinguishable. Our experiments on
two benchmark datasets validate the effectiveness of SCKD for continual
few-shot RE and its superiority in knowledge transfer and memory utilization
over state-of-the-art models.
</p></li>
</ul>

<h3>Title: Improving Continual Relation Extraction by Distinguishing Analogous Semantics. (arXiv:2305.06620v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06620">http://arxiv.org/abs/2305.06620</a></li>
<li>Code URL: <a href="https://github.com/nju-websoft/cear">https://github.com/nju-websoft/cear</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06620] Improving Continual Relation Extraction by Distinguishing Analogous Semantics](http://arxiv.org/abs/2305.06620) #extraction</code></li>
<li>Summary: <p>Continual relation extraction (RE) aims to learn constantly emerging
relations while avoiding forgetting the learned relations. Existing works store
a small number of typical samples to re-train the model for alleviating
forgetting. However, repeatedly replaying these samples may cause the
overfitting problem. We conduct an empirical study on existing works and
observe that their performance is severely affected by analogous relations. To
address this issue, we propose a novel continual extraction model for analogous
relations. Specifically, we design memory-insensitive relation prototypes and
memory augmentation to overcome the overfitting problem. We also introduce
integrated training and focal knowledge distillation to enhance the performance
on analogous relations. Experimental results show the superiority of our model
and demonstrate its effectiveness in distinguishing analogous relations and
overcoming overfitting.
</p></li>
</ul>

<h3>Title: A fast topological approach for predicting anomalies in time-varying graphs. (arXiv:2305.06523v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06523">http://arxiv.org/abs/2305.06523</a></li>
<li>Code URL: <a href="https://github.com/tdavecs/vab">https://github.com/tdavecs/vab</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06523] A fast topological approach for predicting anomalies in time-varying graphs](http://arxiv.org/abs/2305.06523) #extraction</code></li>
<li>Summary: <p>Large time-varying graphs are increasingly common in financial, social and
biological settings. Feature extraction that efficiently encodes the complex
structure of sparse, multi-layered, dynamic graphs presents computational and
methodological challenges. In the past decade, a persistence diagram (PD) from
topological data analysis (TDA) has become a popular descriptor of shape of
data with a well-defined distance between points. However, applications of TDA
to graphs, where there is no intrinsic concept of distance between the nodes,
remain largely unexplored. This paper addresses this gap in the literature by
introducing a computationally efficient framework to extract shape information
from graph data. Our framework has two main steps: first, we compute a PD using
the so-called lower-star filtration which utilizes quantitative node
attributes, and then vectorize it by averaging the associated Betti function
over successive scale values on a one-dimensional grid. Our approach avoids
embedding a graph into a metric space and has stability properties against
input noise. In simulation studies, we show that the proposed vector summary
leads to improved change point detection rate in time-varying graphs. In a real
data application, our approach provides up to 22% gain in anomalous price
prediction for the Ethereum cryptocurrency transaction networks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06784">http://arxiv.org/abs/2305.06784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06784] Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning](http://arxiv.org/abs/2305.06784) #federate</code></li>
<li>Summary: <p>Auction-based Federated Learning (AFL) has attracted extensive research
interest due to its ability to motivate data owners to join FL through economic
means. Existing works assume that only one data consumer and multiple data
owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data
owners bid to join the data consumer for FL. However, this assumption is not
realistic in practical AFL marketplaces in which multiple data consumers can
compete to attract data owners to join their respective FL tasks. In this
paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing
bidding strategy for data consumers in federated learning (Fed-Bidder). It
enables multiple FL data consumers to compete for data owners via AFL
effectively and efficiently by providing with utility estimation capabilities
which can accommodate diverse forms of winning functions, each reflecting
different market dynamics. Extensive experiments based on six commonly adopted
benchmark datasets show that Fed-Bidder is significantly more advantageous
compared to four state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: Multi-Tier Client Selection for Mobile Federated Learning Networks. (arXiv:2305.06865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06865">http://arxiv.org/abs/2305.06865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06865] Multi-Tier Client Selection for Mobile Federated Learning Networks](http://arxiv.org/abs/2305.06865) #federate</code></li>
<li>Summary: <p>Federated learning (FL), which addresses data privacy issues by training
models on resource-constrained mobile devices in a distributed manner, has
attracted significant research attention. However, the problem of optimizing FL
client selection in mobile federated learning networks (MFLNs), where devices
move in and out of each others' coverage and no FL server knows all the data
owners, remains open. To bridge this gap, we propose a first-of-its-kind
\underline{Soc}ially-aware \underline{Fed}erated \underline{C}lient
\underline{S}election (SocFedCS) approach to minimize costs and train
high-quality FL models. SocFedCS enriches the candidate FL client pool by
enabling data owners to propagate FL task information through their local
networks of trust, even as devices are moving into and out of each others'
coverage. Based on Lyapunov optimization, we first transform this time-coupled
problem into a step-by-step optimization problem. Then, we design a method
based on alternating minimization and self-adaptive global best harmony search
to solve this mixed-integer optimization problem. Extensive experiments
comparing SocFedCS against five state-of-the-art approaches based on four
real-world multimedia datasets demonstrate that it achieves 2.06\% higher test
accuracy and 12.24\% lower cost on average than the best-performing baseline.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: SMATCH++: Standardized and Extended Evaluation of Semantic Graphs. (arXiv:2305.06993v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06993">http://arxiv.org/abs/2305.06993</a></li>
<li>Code URL: <a href="https://github.com/flipz357/smatchpp">https://github.com/flipz357/smatchpp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06993] SMATCH++: Standardized and Extended Evaluation of Semantic Graphs](http://arxiv.org/abs/2305.06993) #fair</code></li>
<li>Summary: <p>The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
</p></li>
</ul>

<p>For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp
</p>

<h3>Title: A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges. (arXiv:2305.06969v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06969">http://arxiv.org/abs/2305.06969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06969] A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges](http://arxiv.org/abs/2305.06969) #fair</code></li>
<li>Summary: <p>The widespread adoption of Machine Learning systems, especially in more
decision-critical applications such as criminal sentencing and bank loans, has
led to increased concerns about fairness implications. Algorithms and metrics
have been developed to mitigate and measure these discriminations. More
recently, works have identified a more challenging form of bias called
intersectional bias, which encompasses multiple sensitive attributes, such as
race and gender, together. In this survey, we review the state-of-the-art in
intersectional fairness. We present a taxonomy for intersectional notions of
fairness and mitigation. Finally, we identify the key challenges and provide
researchers with guidelines for future directions.
</p></li>
</ul>

<h3>Title: A statistical approach to detect sensitive features in a group fairness setting. (arXiv:2305.06994v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06994">http://arxiv.org/abs/2305.06994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06994] A statistical approach to detect sensitive features in a group fairness setting](http://arxiv.org/abs/2305.06994) #fair</code></li>
<li>Summary: <p>The use of machine learning models in decision support systems with high
societal impact raised concerns about unfair (disparate) results for different
groups of people. When evaluating such unfair decisions, one generally relies
on predefined groups that are determined by a set of features that are
considered sensitive. However, such an approach is subjective and does not
guarantee that these features are the only ones to be considered as sensitive
nor that they entail unfair (disparate) outcomes.
</p></li>
</ul>

<p>In this paper, we propose a preprocessing step to address the task of
automatically recognizing sensitive features that does not require a trained
model to verify unfair results. Our proposal is based on the Hilber-Schmidt
independence criterion, which measures the statistical dependence of variable
distributions. We hypothesize that if the dependence between the label vector
and a candidate is high for a sensitive feature, then the information provided
by this feature will entail disparate performance measures between groups. Our
empirical results attest our hypothesis and show that several features
considered as sensitive in the literature do not necessarily entail disparate
(unfair) results.
</p>

<h2>interpretability</h2>
<h3>Title: COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06754">http://arxiv.org/abs/2305.06754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06754] COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks](http://arxiv.org/abs/2305.06754) #interpretability</code></li>
<li>Summary: <p>Transformer architectures are complex and their use in NLP, while it has
engendered many successes, makes their interpretability or explainability
challenging. Recent debates have shown that attention maps and attribution
methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this
paper, we present some of their limitations and introduce COCKATIEL, which
successfully addresses some of them. COCKATIEL is a novel, post-hoc,
concept-based, model-agnostic XAI technique that generates meaningful
explanations from the last layer of a neural net model trained on an NLP
classification task by using Non-Negative Matrix Factorization (NMF) to
discover the concepts the model leverages to make predictions and by exploiting
a Sensitivity Analysis to estimate accurately the importance of each of these
concepts for the model. It does so without compromising the accuracy of the
underlying model or requiring a new one to be trained. We conduct experiments
in single and multi-aspect sentiment analysis tasks and we show COCKATIEL's
superior ability to discover concepts that align with humans' on Transformer
models without any supervision, we objectively verify the faithfulness of its
explanations through fidelity metrics, and we showcase its ability to provide
meaningful explanations in two different datasets.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: ReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network. (arXiv:2305.06786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06786">http://arxiv.org/abs/2305.06786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06786] ReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network](http://arxiv.org/abs/2305.06786) #watermark</code></li>
<li>Summary: <p>Watermarking is one of the most important copyright protection tools for
digital media. The most challenging type of watermarking is the imperceptible
one, which embeds identifying information in the data while retaining the
latter's original quality. To fulfill its purpose, watermarks need to withstand
various distortions whose goal is to damage their integrity. In this study, we
investigate a novel deep learning-based architecture for embedding
imperceptible watermarks. The key insight guiding our architecture design is
the need to correlate the dimensions of our watermarks with the sizes of
receptive fields (RF) of modules of our architecture. This adaptation makes our
watermarks more robust, while also enabling us to generate them in a way that
better maintains image quality. Extensive evaluations on a wide variety of
distortions show that the proposed method is robust against most common
distortions on watermarks including collusive distortion.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Analyzing Bias in Diffusion-based Face Generation Models. (arXiv:2305.06402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06402">http://arxiv.org/abs/2305.06402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06402] Analyzing Bias in Diffusion-based Face Generation Models](http://arxiv.org/abs/2305.06402) #diffusion</code></li>
<li>Summary: <p>Diffusion models are becoming increasingly popular in synthetic data
generation and image editing applications. However, these models can amplify
existing biases and propagate them to downstream applications. Therefore, it is
crucial to understand the sources of bias in their outputs. In this paper, we
investigate the presence of bias in diffusion-based face generation models with
respect to attributes such as gender, race, and age. Moreover, we examine how
dataset size affects the attribute composition and perceptual quality of both
diffusion and Generative Adversarial Network (GAN) based face generation models
across various attribute classes. Our findings suggest that diffusion models
tend to worsen distribution bias in the training data for various attributes,
which is heavily influenced by the size of the dataset. Conversely, GAN models
trained on balanced datasets with a larger number of samples show less bias
across different attributes.
</p></li>
</ul>

<h3>Title: Undercover Deepfakes: Detecting Fake Segments in Videos. (arXiv:2305.06564v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06564">http://arxiv.org/abs/2305.06564</a></li>
<li>Code URL: <a href="https://github.com/sanjaysaha1311/temporal-deepfake-segmentation">https://github.com/sanjaysaha1311/temporal-deepfake-segmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06564] Undercover Deepfakes: Detecting Fake Segments in Videos](http://arxiv.org/abs/2305.06564) #diffusion</code></li>
<li>Summary: <p>The recent renaissance in generative models, driven primarily by the advent
of diffusion models and iterative improvement in GAN methods, has enabled many
creative applications. However, each advancement is also accompanied by a rise
in the potential for misuse. In the arena of deepfake generation this is a key
societal issue. In particular, the ability to modify segments of videos using
such generative techniques creates a new paradigm of deepfakes which are mostly
real videos altered slightly to distort the truth. Current deepfake detection
methods in the academic literature are not evaluated on this paradigm. In this
paper, we present a deepfake detection method able to address this issue by
performing both frame and video level deepfake prediction. To facilitate
testing our method we create a new benchmark dataset where videos have both
real and fake frame sequences. Our method utilizes the Vision Transformer,
Scaling and Shifting pretraining and Timeseries Transformer to temporally
segment videos to help facilitate the interpretation of possible deepfakes.
Extensive experiments on a variety of deepfake generation methods show
excellent results on temporal segmentation and classical video level
predictions as well. In particular, the paradigm we introduce will form a
powerful tool for the moderation of deepfakes, where human oversight can be
better targeted to the parts of videos suspected of being deepfakes. All
experiments can be reproduced at:
https://github.com/sanjaysaha1311/temporal-deepfake-segmentation.
</p></li>
</ul>

<h3>Title: Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06710">http://arxiv.org/abs/2305.06710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06710] Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator](http://arxiv.org/abs/2305.06710) #diffusion</code></li>
<li>Summary: <p>Classifier-free guidance is an effective sampling technique in diffusion
models that has been widely adopted. The main idea is to extrapolate the model
in the direction of text guidance and away from null-text guidance. In this
paper, we demonstrate that null-text guidance in diffusion models is secretly a
cartoon-style creator, i.e., the generated images can be efficiently
transformed into cartoons by simply perturbing the null-text guidance.
Specifically, we proposed two disturbance methods, i.e., Rollback disturbance
(Back-D) and Image disturbance (Image-D), to construct misalignment between the
noisy images used for predicting null-text guidance and text guidance
(subsequently referred to as \textbf{null-text noisy image} and \textbf{text
noisy image} respectively) in the sampling process. Back-D achieves
cartoonization by altering the noise level of null-text noisy image via
replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces
high-fidelity, diverse cartoons by defining $x_t$ as a clean input image, which
further improves the incorporation of finer image details. Through
comprehensive experiments, we delved into the principle of noise disturbing for
null-text and uncovered that the efficacy of disturbance depends on the
correlation between the null-text noisy image and the source image. Moreover,
our proposed techniques, which can generate cartoon images and cartoonize
specific ones, are training-free and easily integrated as a plug-and-play
component in any classifier-free guided diffusion model. Project page is
available at \url{https://nulltextforcartoon.github.io/}.
</p></li>
</ul>

<h3>Title: Exploiting Diffusion Prior for Real-World Image Super-Resolution. (arXiv:2305.07015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07015">http://arxiv.org/abs/2305.07015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07015] Exploiting Diffusion Prior for Real-World Image Super-Resolution](http://arxiv.org/abs/2305.07015) #diffusion</code></li>
<li>Summary: <p>We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
introduce a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Patch-wise Mixed-Precision Quantization of Vision Transformer. (arXiv:2305.06559v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06559">http://arxiv.org/abs/2305.06559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06559] Patch-wise Mixed-Precision Quantization of Vision Transformer](http://arxiv.org/abs/2305.06559) #transformer</code></li>
<li>Summary: <p>As emerging hardware begins to support mixed bit-width arithmetic
computation, mixed-precision quantization is widely used to reduce the
complexity of neural networks. However, Vision Transformers (ViTs) require
complex self-attention computation to guarantee the learning of powerful
feature representations, which makes mixed-precision quantization of ViTs still
challenging. In this paper, we propose a novel patch-wise mixed-precision
quantization (PMQ) for efficient inference of ViTs. Specifically, we design a
lightweight global metric, which is faster than existing methods, to measure
the sensitivity of each component in ViTs to quantization errors. Moreover, we
also introduce a pareto frontier approach to automatically allocate the optimal
bit-precision according to the sensitivity. To further reduce the computational
complexity of self-attention in inference stage, we propose a patch-wise module
to reallocate bit-width of patches in each layer. Extensive experiments on the
ImageNet dataset shows that our method greatly reduces the search cost and
facilitates the application of mixed-precision quantization to ViTs.
</p></li>
</ul>

<h3>Title: PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer. (arXiv:2305.06621v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06621">http://arxiv.org/abs/2305.06621</a></li>
<li>Code URL: <a href="https://github.com/nightmare-n/pvt-ssd">https://github.com/nightmare-n/pvt-ssd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06621] PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer](http://arxiv.org/abs/2305.06621) #transformer</code></li>
<li>Summary: <p>Recent Transformer-based 3D object detectors learn point cloud features
either from point- or voxel-based representations. However, the former requires
time-consuming sampling while the latter introduces quantization errors. In
this paper, we present a novel Point-Voxel Transformer for single-stage 3D
detection (PVT-SSD) that takes advantage of these two representations.
Specifically, we first use voxel-based sparse convolutions for efficient
feature encoding. Then, we propose a Point-Voxel Transformer (PVT) module that
obtains long-range contexts in a cheap manner from voxels while attaining
accurate positions from points. The key to associating the two different
representations is our introduced input-dependent Query Initialization module,
which could efficiently generate reference points and content queries. Then,
PVT adaptively fuses long-range contextual and local geometric information
around reference points into content queries. Further, to quickly find the
neighboring points of reference points, we design the Virtual Range Image
module, which generalizes the native range image to multi-sensor and
multi-frame. The experiments on several autonomous driving benchmarks verify
the effectiveness and efficiency of the proposed method. Code will be available
at https://github.com/Nightmare-n/PVT-SSD.
</p></li>
</ul>

<h3>Title: Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers. (arXiv:2305.06963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06963">http://arxiv.org/abs/2305.06963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06963] Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers](http://arxiv.org/abs/2305.06963) #transformer</code></li>
<li>Summary: <p>Whole-Slide Imaging allows for the capturing and digitization of
high-resolution images of histological specimen. An automated analysis of such
images using deep learning models is therefore of high demand. The transformer
architecture has been proposed as a possible candidate for effectively
leveraging the high-resolution information. Here, the whole-slide image is
partitioned into smaller image patches and feature tokens are extracted from
these image patches. However, while the conventional transformer allows for a
simultaneous processing of a large set of input tokens, the computational
demand scales quadratically with the number of input tokens and thus
quadratically with the number of image patches. To address this problem we
propose a novel cascaded cross-attention network (CCAN) based on the
cross-attention mechanism that scales linearly with the number of extracted
patches. Our experiments demonstrate that this architecture is at least on-par
with and even outperforms other attention-based state-of-the-art methods on two
public datasets: On the use-case of lung cancer (TCGA NSCLC) our model reaches
a mean area under the receiver operating characteristic (AUC) of 0.970 $\pm$
0.008 and on renal cancer (TCGA RCC) reaches a mean AUC of 0.985 $\pm$ 0.004.
Furthermore, we show that our proposed model is efficient in low-data regimes,
making it a promising approach for analyzing whole-slide images in
resource-limited settings. To foster research in this direction, we make our
code publicly available on GitHub: XXX.
</p></li>
</ul>

<h3>Title: Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07011">http://arxiv.org/abs/2305.07011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07011] Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers](http://arxiv.org/abs/2305.07011) #transformer</code></li>
<li>Summary: <p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a
contrastive image-text pretraining recipe to bridge the gap between image-level
pretraining and open-vocabulary object detection. At the pretraining phase, we
propose to randomly crop and resize regions of positional embeddings instead of
using the whole image positional embeddings. This better matches the use of
positional embeddings at region-level in the detection finetuning phase. In
addition, we replace the common softmax cross entropy loss in contrastive
learning with focal loss to better learn the informative yet difficult
examples. Finally, we leverage recent advances in novel object proposals to
improve open-vocabulary detection finetuning. We evaluate our full model on the
LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.
RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best
existing approach by +5.8 points in addition to competitive zero-shot transfer
detection. Surprisingly, RO-ViT improves the image-level representation as well
and achieves the state of the art on 9 out of 12 metrics on COCO and Flickr
image-text retrieval benchmarks, outperforming competitive approaches with
larger models.
</p></li>
</ul>

<h3>Title: SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views. (arXiv:2305.07024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07024">http://arxiv.org/abs/2305.07024</a></li>
<li>Code URL: <a href="https://github.com/xt4d/sparsegnv">https://github.com/xt4d/sparsegnv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07024] SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views](http://arxiv.org/abs/2305.07024) #transformer</code></li>
<li>Summary: <p>We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.
</p></li>
</ul>

<h3>Title: EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention. (arXiv:2305.07027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07027">http://arxiv.org/abs/2305.07027</a></li>
<li>Code URL: <a href="https://github.com/microsoft/cream">https://github.com/microsoft/cream</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07027] EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](http://arxiv.org/abs/2305.07027) #transformer</code></li>
<li>Summary: <p>Vision transformers have shown great success due to their high model
capabilities. However, their remarkable performance is accompanied by heavy
computation costs, which makes them unsuitable for real-time applications. In
this paper, we propose a family of high-speed vision transformers named
EfficientViT. We find that the speed of existing transformer models is commonly
bounded by memory inefficient operations, especially the tensor reshaping and
element-wise functions in MHSA. Therefore, we design a new building block with
a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN
layers, which improves memory efficiency while enhancing channel communication.
Moreover, we discover that the attention maps share high similarities across
heads, leading to computational redundancy. To address this, we present a
cascaded group attention module feeding attention heads with different splits
of the full feature, which not only saves computation cost but also improves
attention diversity. Comprehensive experiments demonstrate EfficientViT
outperforms existing efficient models, striking a good trade-off between speed
and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by
1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia
V100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient
model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while
running 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX
format. Code and models are available at
https://github.com/microsoft/Cream/tree/main/EfficientViT.
</p></li>
</ul>

<h3>Title: A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06416">http://arxiv.org/abs/2305.06416</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06416] A Method to Automate the Discharge Summary Hospital Course for Neurology Patients](http://arxiv.org/abs/2305.06416) #transformer</code></li>
<li>Summary: <p>Generation of automated clinical notes have been posited as a strategy to
mitigate physician burnout. In particular, an automated narrative summary of a
patient's hospital stay could supplement the hospital course section of the
discharge summary that inpatient physicians document in electronic health
record (EHR) systems. In the current study, we developed and evaluated an
automated method for summarizing the hospital course section using
encoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and
BART models and optimized for factuality through constraining beam search,
which we trained and tested using EHR data from patients admitted to the
neurology unit of an academic medical center. The approach demonstrated good
ROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified
physicians rated 62% of the automated summaries as meeting the standard of
care, which suggests the method may be useful clinically. To our knowledge,
this study is among the first to demonstrate an automated method for generating
a discharge summary hospital course that approaches a quality level of what a
physician would write.
</p></li>
</ul>

<h3>Title: Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06721">http://arxiv.org/abs/2305.06721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06721] Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*](http://arxiv.org/abs/2305.06721) #transformer</code></li>
<li>Summary: <p>To advance the neural encoding of Portuguese (PT), and a fortiori the
technological preparation of this language for the digital age, we developed a
Transformer-based foundation model that sets a new state of the art in this
respect for two of its variants, namely European Portuguese from Portugal
(PT-PT) and American Portuguese from Brazil (PT-BR).
</p></li>
</ul>

<p>To develop this encoder, which we named Albertina PT-*, a strong model was
used as a starting point, DeBERTa, and its pre-training was done over data sets
of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC
corpus for PT-BR. The performance of Albertina and competing models was
assessed by evaluating them on prominent downstream language processing tasks
adapted for Portuguese.
</p>
<p>Both Albertina PT-PT and PT-BR versions are distributed free of charge and
under the most permissive license possible and can be run on consumer-grade
hardware, thus seeking to contribute to the advancement of research and
innovation in language technology for Portuguese.
</p>

<h3>Title: Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning. (arXiv:2305.06801v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06801">http://arxiv.org/abs/2305.06801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06801] Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning](http://arxiv.org/abs/2305.06801) #transformer</code></li>
<li>Summary: <p>This paper shines a light on the potential of definition-based semantic
models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs)
in clinical terminology. Our study focuses on biomedical entities defined in
the UMLS ontology and aims to help prioritize the translation efforts of these
entities. In particular, we develop an effective tool for scoring the
idiomaticity of biomedical MWEs based on the degree of similarity between the
semantic representations of those MWEs and a weighted average of the
representation of their constituents. We achieve this using a biomedical
language model trained to produce similar representations for entity names and
their definitions, called BioLORD. The importance of this definition-based
approach is highlighted by comparing the BioLORD model to two other
state-of-the-art biomedical language models based on Transformer: SapBERT and
CODER. Our results show that the BioLORD model has a strong ability to identify
idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity
estimation helps ontology translators to focus on more challenging MWEs.
</p></li>
</ul>

<h3>Title: IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining. (arXiv:2305.06892v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06892">http://arxiv.org/abs/2305.06892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06892] IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining](http://arxiv.org/abs/2305.06892) #transformer</code></li>
<li>Summary: <p>This paper describes our system on SemEval-2023 Task 10: Explainable
Detection of Online Sexism (EDOS). This work aims to design an automatic system
for detecting and classifying sexist content in online spaces. We propose a set
of transformer-based pre-trained models with task-adaptive pretraining and
ensemble learning. The main contributions of our system include analyzing the
performance of different transformer-based pre-trained models and combining
these models, as well as providing an efficient method using large amounts of
unlabeled data for model adaptive pretraining. We have also explored several
other strategies. On the test dataset, our system achieves F1-scores of 83%,
64%, and 47% on subtasks A, B, and C, respectively.
</p></li>
</ul>

<h3>Title: A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07016">http://arxiv.org/abs/2305.07016</a></li>
<li>Code URL: <a href="https://github.com/ogaloglu/pre-training-multilingual-document-encoders">https://github.com/ogaloglu/pre-training-multilingual-document-encoders</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07016] A General-Purpose Multilingual Document Encoder](http://arxiv.org/abs/2305.07016) #transformer</code></li>
<li>Summary: <p>Massively multilingual pretrained transformers (MMTs) have tremendously
pushed the state of the art on multilingual NLP and cross-lingual transfer of
NLP models in particular. While a large body of work leveraged MMTs to mine
parallel data and induce bilingual document embeddings, much less effort has
been devoted to training general-purpose (massively) multilingual document
encoder that can be used for both supervised and unsupervised document-level
tasks. In this work, we pretrain a massively multilingual document encoder as a
hierarchical transformer model (HMDE) in which a shallow document transformer
contextualizes sentence representations produced by a state-of-the-art
pretrained multilingual sentence encoder. We leverage Wikipedia as a readily
available source of comparable documents for creating training data, and train
HMDE by means of a cross-lingual contrastive objective, further exploiting the
category hierarchy of Wikipedia for creation of difficult negatives. We
evaluate the effectiveness of HMDE in two arguably most common and prominent
cross-lingual document-level tasks: (1) cross-lingual transfer for topical
document classification and (2) cross-lingual document retrieval. HMDE is
significantly more effective than (i) aggregations of segment-based
representations and (ii) multilingual Longformer. Crucially, owing to its
massively multilingual lower transformer, HMDE successfully generalizes to
languages unseen in document-level pretraining. We publicly release our code
and models at
https://github.com/ogaloglu/pre-training-multilingual-document-encoders .
</p></li>
</ul>

<h3>Title: Dynamic Graph Representation Learning for Depression Screening with Transformer. (arXiv:2305.06447v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06447">http://arxiv.org/abs/2305.06447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06447] Dynamic Graph Representation Learning for Depression Screening with Transformer](http://arxiv.org/abs/2305.06447) #transformer</code></li>
<li>Summary: <p>Early detection of mental disorder is crucial as it enables prompt
intervention and treatment, which can greatly improve outcomes for individuals
suffering from debilitating mental affliction. The recent proliferation of
mental health discussions on social media platforms presents research
opportunities to investigate mental health and potentially detect instances of
mental illness. However, existing depression detection methods are constrained
due to two major limitations: (1) the reliance on feature engineering and (2)
the lack of consideration for time-varying factors. Specifically, these methods
require extensive feature engineering and domain knowledge, which heavily rely
on the amount, quality, and type of user-generated content. Moreover, these
methods ignore the important impact of time-varying factors on depression
detection, such as the dynamics of linguistic patterns and interpersonal
interactive behaviors over time on social media (e.g., replies, mentions, and
quote-tweets). To tackle these limitations, we propose an early depression
detection framework, ContrastEgo treats each user as a dynamic time-evolving
attributed graph (ego-network) and leverages supervised contrastive learning to
maximize the agreement of users' representations at different scales while
minimizing the agreement of users' representations to differentiate between
depressed and control groups. ContrastEgo embraces four modules, (1)
constructing users' heterogeneous interactive graphs, (2) extracting the
representations of users' interaction snapshots using graph neural networks,
(3) modeling the sequences of snapshots using attention mechanism, and (4)
depression detection using contrastive learning. Extensive experiments on
Twitter data demonstrate that ContrastEgo significantly outperforms the
state-of-the-art methods in terms of all the effectiveness metrics in various
experimental settings.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Text-To-Concept (and Back) via Cross-Model Alignment. (arXiv:2305.06386v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06386">http://arxiv.org/abs/2305.06386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06386] Text-To-Concept (and Back) via Cross-Model Alignment](http://arxiv.org/abs/2305.06386) #generative</code></li>
<li>Summary: <p>We observe that the mapping between an image's representation in one model to
its representation in another can be learned surprisingly well with just a
linear layer, even across diverse models. Building on this observation, we
propose $\textit{text-to-concept}$, where features from a fixed pretrained
model are aligned linearly to the CLIP space, so that text embeddings from
CLIP's text encoder become directly comparable to the aligned features. With
text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly
strong zero-shot classifiers for free, with accuracy at times even surpassing
that of CLIP, despite being much smaller models and trained on a small fraction
of the data compared to CLIP. We show other immediate use-cases of
text-to-concept, like building concept bottleneck models with no concept
supervision, diagnosing distribution shifts in terms of human concepts, and
retrieving images satisfying a set of text-based constraints. Lastly, we
demonstrate the feasibility of $\textit{concept-to-text}$, where vectors in a
model's feature space are decoded by first aligning to the CLIP before being
fed to a GPT-based generative model. Our work suggests existing deep models,
with presumably diverse architectures and training, represent input samples
relatively similarly, and a two-way communication across model representation
spaces and to humans (through language) is viable.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM. (arXiv:2305.06404v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06404">http://arxiv.org/abs/2305.06404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06404] LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM](http://arxiv.org/abs/2305.06404) #large language model</code></li>
<li>Summary: <p>Text embeddings are useful features for several NLP applications, such as
sentence similarity, text clustering, and semantic search. In this paper, we
present a Low-rank Adaptation with a Contrastive objective on top of 8-bit
Siamese-BLOOM, a multilingual large language model optimized to produce
semantically meaningful word embeddings. The innovation is threefold. First, we
cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable
adapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification.
Third, we apply a Siamese architecture on BLOOM model with a contrastive
objective to ease the multi-lingual labeled data scarcity. The experiment
results show the quality of learned embeddings from LACoS-BLOOM is proportional
to the number of model parameters and the amount of unlabeled training data.
With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1
billion parameters end-to-end on a single GPU machine with 32GB memory.
Compared to previous solution Sentence-BERT, we achieve significant improvement
on both English and multi-lingual STS tasks.
</p></li>
</ul>

<h3>Title: How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06530">http://arxiv.org/abs/2305.06530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06530] How Good are Commercial Large Language Models on African Languages?](http://arxiv.org/abs/2305.06530) #large language model</code></li>
<li>Summary: <p>Recent advancements in Natural Language Processing (NLP) has led to the
proliferation of large pretrained language models. These models have been shown
to yield good performance, using in-context learning, even on unseen tasks and
languages. They have also been exposed as commercial APIs as a form of
language-model-as-a-service, with great adoption. However, their performance on
African languages is largely unknown. We present a preliminary analysis of
commercial large language models on two tasks (machine translation and text
classification) across eight African languages, spanning different language
families and geographical areas. Our results suggest that commercial language
models produce below-par performance on African languages. We also find that
they perform better on text classification than machine translation. In
general, our findings present a call-to-action to ensure African languages are
well represented in commercial large language models, given their growing
popularity.
</p></li>
</ul>

<h3>Title: Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06575">http://arxiv.org/abs/2305.06575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06575] Chain-of-Dictionary Prompting Elicits Translation in Large Language Models](http://arxiv.org/abs/2305.06575) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have shown surprisingly good performance in
multilingual neural machine translation (MNMT) even when trained without
parallel data. Yet, despite the fact that the amount of training data is
gigantic, they still struggle with translating rare words, particularly for
low-resource languages. Even worse, it is usually unrealistic to retrieve
relevant demonstrations for in-context learning with low-resource languages on
LLMs, which restricts the practical use of LLMs for translation -- how should
we mitigate this problem? To this end, we present a novel method, CoD, which
augments LLMs with prior knowledge with the chains of multilingual dictionaries
for a subset of input words to elicit translation abilities for LLMs. Extensive
experiments indicate that augmenting ChatGPT with CoD elicits large gains by up
to 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in
Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the
importance of chaining the multilingual dictionaries, as well as the
superiority of CoD to few-shot demonstration for low-resource languages.
</p></li>
</ul>

<h3>Title: INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06677">http://arxiv.org/abs/2305.06677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06677] INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models](http://arxiv.org/abs/2305.06677) #large language model</code></li>
<li>Summary: <p>A salient characteristic of large pre-trained language models (PTLMs) is a
remarkable improvement in their generalization capability and emergence of new
capabilities with increasing model capacity and pre-training dataset size.
Consequently, we are witnessing the development of enormous models pushing the
state-of-the-art. It is, however, imperative to realize that this inevitably
leads to prohibitively long training times, extortionate computing costs, and a
detrimental environmental impact. Significant efforts are underway to make PTLM
training more efficient through innovations in model architectures, training
pipelines, and loss function design, with scant attention being paid to
optimizing the utility of training data. The key question that we ask is
whether it is possible to train PTLMs by employing only highly informative
subsets of the training data while maintaining downstream performance? Building
upon the recent progress in informative data subset selection, we show how we
can employ submodular optimization to select highly representative subsets of
the training corpora. Our results demonstrate that the proposed framework can
be applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using
only a fraction of data while retaining up to $\sim99\%$ of the performance of
the fully-trained models.
</p></li>
</ul>

<h3>Title: Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06983">http://arxiv.org/abs/2305.06983</a></li>
<li>Code URL: <a href="https://github.com/jzbjyb/flare">https://github.com/jzbjyb/flare</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06983] Active Retrieval Augmented Generation](http://arxiv.org/abs/2305.06983) #large language model</code></li>
<li>Summary: <p>Despite the remarkable ability of large language models (LMs) to comprehend
and generate language, they have a tendency to hallucinate and create factually
inaccurate output. Augmenting LMs by retrieving information from external
knowledge resources is one promising solution. Most existing
retrieval-augmented LMs employ a retrieve-and-generate setup that only
retrieves information once based on the input. This is limiting, however, in
more general scenarios involving generation of long texts, where continually
gathering information throughout the generation process is essential. There
have been some past efforts to retrieve information multiple times while
generating outputs, which mostly retrieve documents at fixed intervals using
the previous context as queries. In this work, we provide a generalized view of
active retrieval augmented generation, methods that actively decide when and
what to retrieve across the course of the generation. We propose
Forward-Looking Active REtrieval augmented generation (FLARE), a generic
retrieval-augmented generation method which iteratively uses a prediction of
the upcoming sentence to anticipate future content, which is then utilized as a
query to retrieve relevant documents to regenerate the sentence if it contains
low-confidence tokens. We test FLARE along with baselines comprehensively over
4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves
superior or competitive performance on all tasks, demonstrating the
effectiveness of our method. Code and datasets are available at
https://github.com/jzbjyb/FLARE.
</p></li>
</ul>

<h3>Title: Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06984">http://arxiv.org/abs/2305.06984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06984] Evaluating Open-Domain Question Answering in the Era of Large Language Models](http://arxiv.org/abs/2305.06984) #large language model</code></li>
<li>Summary: <p>Lexical matching remains the de facto evaluation method for open-domain
question answering (QA). Unfortunately, lexical matching fails completely when
a plausible candidate answer does not appear in the list of gold answers, which
is increasingly the case as we shift from extractive to generative models. The
recent success of large language models (LLMs) for QA aggravates lexical
matching failures since candidate answers become longer, thereby making
matching with the gold answers even more challenging. Without accurate
evaluation, the true progress in open-domain QA remains unknown. In this paper,
we conduct a thorough analysis of various open-domain QA models, including
LLMs, by manually evaluating their answers on a subset of NQ-open, a popular
benchmark. Our assessments reveal that while the true performance of all models
is significantly underestimated, the performance of the InstructGPT (zero-shot)
LLM increases by nearly +60%, making it on par with existing top models, and
the InstructGPT (few-shot) model actually achieves a new state-of-the-art on
NQ-open. We also find that more than 50% of lexical matching failures are
attributed to semantically equivalent answers. We further demonstrate that
regex matching ranks QA models consistent with human judgments, although still
suffering from unnecessary strictness. Finally, we demonstrate that automated
evaluation models are a reasonable surrogate for lexical matching in some
circumstances, but not for long-form answers generated by LLMs. The automated
models struggle in detecting hallucinations in LLM answers and are thus unable
to evaluate LLMs. At this time, there appears to be no substitute for human
evaluation.
</p></li>
</ul>

<h3>Title: Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07004">http://arxiv.org/abs/2305.07004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07004] Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting](http://arxiv.org/abs/2305.07004) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) demonstrate impressive multilingual capability,
but their performance varies substantially across different languages. In this
work, we introduce a simple yet effective method, called cross-lingual-thought
prompting (XLT), to systematically improve the multilingual capability of LLMs.
Specifically, XLT is a generic template prompt that stimulates cross-lingual
and logical reasoning skills to enhance task performance across languages. We
conduct comprehensive evaluations on 7 typical benchmarks related to reasoning,
understanding, and generation tasks, covering both high-resource and
low-resource languages. Experimental results show that XLT not only remarkably
enhances the performance of various multilingual tasks but also significantly
reduces the gap between the average performance and the best performance of
each task in different languages. Notably, XLT brings over 10 points of average
improvement in arithmetic reasoning and open-domain question-answering tasks.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Convolutional Neural Networks Rarely Learn Shape for Semantic Segmentation. (arXiv:2305.06568v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06568">http://arxiv.org/abs/2305.06568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06568] Convolutional Neural Networks Rarely Learn Shape for Semantic Segmentation](http://arxiv.org/abs/2305.06568) #segmentation</code></li>
<li>Summary: <p>Shape learning, or the ability to leverage shape information, could be a
desirable property of convolutional neural networks (CNNs) when target objects
have specific shapes. While some research on the topic is emerging, there is no
systematic study to conclusively determine whether and under what circumstances
CNNs learn shape. Here, we present such a study in the context of segmentation
networks where shapes are particularly important. We define shape and propose a
new behavioral metric to measure the extent to which a CNN utilizes shape
information. We then execute a set of experiments with synthetic and real-world
data to progressively uncover under which circumstances CNNs learn shape and
what can be done to encourage such behavior. We conclude that (i) CNNs do not
learn shape in typical settings but rather rely on other features available to
identify the objects of interest, (ii) CNNs can learn shape, but only if the
shape is the only feature available to identify the object, (iii) sufficiently
large receptive field size relative to the size of target objects is necessary
for shape learning; (iv) a limited set of augmentations can encourage shape
learning; (v) learning shape is indeed useful in the presence of
out-of-distribution data.
</p></li>
</ul>

<h3>Title: Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond. (arXiv:2305.06720v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06720">http://arxiv.org/abs/2305.06720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06720] Bi-level Dynamic Learning for Jointly Multi-modality Image Fusion and Beyond](http://arxiv.org/abs/2305.06720) #segmentation</code></li>
<li>Summary: <p>Recently, multi-modality scene perception tasks, e.g., image fusion and scene
understanding, have attracted widespread attention for intelligent vision
systems. However, early efforts always consider boosting a single task
unilaterally and neglecting others, seldom investigating their underlying
connections for joint promotion. To overcome these limitations, we establish
the hierarchical dual tasks-driven deep model to bridge these tasks.
Concretely, we firstly construct an image fusion module to fuse complementary
characteristics and cascade dual task-related modules, including a
discriminator for visual effects and a semantic network for feature
measurement. We provide a bi-level perspective to formulate image fusion and
follow-up downstream tasks. To incorporate distinct task-related responses for
image fusion, we consider image fusion as a primary goal and dual modules as
learnable constraints. Furthermore, we develop an efficient first-order
approximation to compute corresponding gradients and present dynamic weighted
aggregation to balance the gradients for fusion learning. Extensive experiments
demonstrate the superiority of our method, which not only produces visually
pleasant fused results but also realizes significant promotion for detection
and segmentation than the state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: Towards a Better Understanding of the Computer Vision Research Community in Africa. (arXiv:2305.06773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06773">http://arxiv.org/abs/2305.06773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06773] Towards a Better Understanding of the Computer Vision Research Community in Africa](http://arxiv.org/abs/2305.06773) #segmentation</code></li>
<li>Summary: <p>Computer vision is a broad field of study that encompasses different tasks
(e.g., object detection, semantic segmentation, 3D reconstruction). Although
computer vision is relevant to the African communities in various applications,
yet computer vision research is under-explored in the continent and constructs
only 0.06% of top-tier publications in the last 10 years. In this paper, our
goal is to have a better understanding of the computer vision research
conducted in Africa and provide pointers on whether there is equity in research
or not. We do this through an empirical analysis of the African computer vision
publications that are Scopus indexed. We first study the opportunities
available for African institutions to publish in top-tier computer vision
venues. We show that African publishing trends in top-tier venues over the
years do not exhibit consistent growth. We also devise a novel way to retrieve
African authors through their affiliation history to have a better
understanding of their contributions in top-tier venues. Moreover, we study all
computer vision publications beyond top-tier venues in different African
regions to find that mainly Northern and Southern Africa are publishing in
computer vision with more than 85% of African publications. Finally, we present
the most recurring keywords in computer vision publications. In summary, our
analysis reveals that African researchers are key contributors to African
research, yet there exists multiple barriers to publish in top-tier venues and
the current trend of topics published in the continent might not necessarily
reflect the communities' needs. This work is part of a community based effort
that is focused on improving computer vision research in Africa.
</p></li>
</ul>

<h3>Title: Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation. (arXiv:2305.06912v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06912">http://arxiv.org/abs/2305.06912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06912] Meta-Learners for Few-Shot Weakly-Supervised Medical Image Segmentation](http://arxiv.org/abs/2305.06912) #segmentation</code></li>
<li>Summary: <p>Most uses of Meta-Learning in visual recognition are very often applied to
image classification, with a relative lack of works in other tasks {such} as
segmentation and detection. We propose a generic Meta-Learning framework for
few-shot weakly-supervised segmentation in medical imaging domains. We conduct
a comparative analysis of meta-learners from distinct paradigms adapted to
few-shot image segmentation in different sparsely annotated radiological tasks.
The imaging modalities include 2D chest, mammographic and dental X-rays, as
well as 2D slices of volumetric tomography and resonance images. Our
experiments consider a total of 9 meta-learners, 4 backbones and multiple
target organ segmentation tasks. We explore small-data scenarios in radiology
with varying weak annotation styles and densities. Our analysis shows that
metric-based meta-learning approaches achieve better segmentation results in
tasks with smaller domain shifts in comparison to the meta-training datasets,
while some gradient- and fusion-based meta-learners are more generalizable to
larger domain shifts.
</p></li>
</ul>

<h3>Title: FreePoint: Unsupervised Point Cloud Instance Segmentation. (arXiv:2305.06973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06973">http://arxiv.org/abs/2305.06973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06973] FreePoint: Unsupervised Point Cloud Instance Segmentation](http://arxiv.org/abs/2305.06973) #segmentation</code></li>
<li>Summary: <p>Instance segmentation of point clouds is a crucial task in 3D field with
numerous applications that involve localizing and segmenting objects in a
scene. However, achieving satisfactory results requires a large number of
manual annotations, which is a time-consuming and expensive process. To
alleviate dependency on annotations, we propose a method, called FreePoint, for
underexplored unsupervised class-agnostic instance segmentation on point
clouds. In detail, we represent the point features by combining coordinates,
colors, normals, and self-supervised deep features. Based on the point
features, we perform a multicut algorithm to segment point clouds into coarse
instance masks as pseudo labels, which are used to train a point cloud instance
segmentation model. To alleviate the inaccuracy of coarse masks during
training, we propose a weakly-supervised training strategy and corresponding
loss. Our work can also serve as an unsupervised pre-training pretext for
supervised semantic instance segmentation with limited annotations. For
class-agnostic instance segmentation on point clouds, FreePoint largely fills
the gap with its fully-supervised counterpart based on the state-of-the-art
instance segmentation model Mask3D and even surpasses some previous
fully-supervised methods. When serving as a pretext task and fine-tuning on
S3DIS, FreePoint outperforms training from scratch by 5.8% AP with only 10%
mask annotations.
</p></li>
</ul>

<h3>Title: Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation. (arXiv:2305.06978v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06978">http://arxiv.org/abs/2305.06978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06978] Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation](http://arxiv.org/abs/2305.06978) #segmentation</code></li>
<li>Summary: <p>Domain shift and label scarcity heavily limit deep learning applications to
various medical image analysis tasks. Unsupervised domain adaptation (UDA)
techniques have recently achieved promising cross-modality medical image
segmentation by transferring knowledge from a label-rich source domain to an
unlabeled target domain. However, it is also difficult to collect annotations
from the source domain in many clinical applications, rendering most prior
works suboptimal with the label-scarce source domain, particularly for few-shot
scenarios, where only a few source labels are accessible. To achieve efficient
few-shot cross-modality segmentation, we propose a novel
transformation-consistent meta-hallucination framework, meta-hallucinator, with
the goal of learning to diversify data distributions and generate useful
examples for enhancing cross-modality performance. In our framework,
hallucination and segmentation models are jointly trained with the
gradient-based meta-learning strategy to synthesize examples that lead to good
segmentation performance on the target domain. To further facilitate data
hallucination and cross-domain knowledge transfer, we develop a self-ensembling
model with a hallucination-consistent property. Our meta-hallucinator can
seamlessly collaborate with the meta-segmenter for learning to hallucinate with
mutual benefits from a combined view of meta-learning and self-ensembling
learning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiac
segmentation demonstrate that our method performs favorably against various
approaches by a lot in the few-shot UDA scenario.
</p></li>
</ul>

<h3>Title: Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.07005">http://arxiv.org/abs/2305.07005</a></li>
<li>Code URL: <a href="https://github.com/francois-meyer/ssmt">https://github.com/francois-meyer/ssmt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.07005] Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation](http://arxiv.org/abs/2305.07005) #segmentation</code></li>
<li>Summary: <p>Subword segmenters like BPE operate as a preprocessing step in neural machine
translation and other (conditional) language models. They are applied to
datasets before training, so translation or text generation quality relies on
the quality of segmentations. We propose a departure from this paradigm, called
subword segmental machine translation (SSMT). SSMT unifies subword segmentation
and MT in a single trainable model. It learns to segment target sentence words
while jointly learning to generate target sentences. To use SSMT during
inference we propose dynamic decoding, a text generation algorithm that adapts
segmentations as it generates translations. Experiments across 6 translation
directions show that SSMT improves chrF scores for morphologically rich
agglutinative languages. Gains are strongest in the very low-resource scenario.
SSMT also learns subwords that are closer to morphemes compared to baselines
and proves more robust on a test set constructed for evaluating morphological
compositional generalisation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
