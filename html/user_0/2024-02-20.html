<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-20</h1>
<h3>Title: Taxonomy-based CheckList for Large Language Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Damin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10899">https://arxiv.org/abs/2402.10899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10899">https://arxiv.org/pdf/2402.10899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10899]] Taxonomy-based CheckList for Large Language Model Evaluation(https://arxiv.org/abs/2402.10899)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) have been used in many downstream tasks, the internal stereotypical representation may affect the fairness of the outputs. In this work, we introduce human knowledge into natural language interventions and study pre-trained language models' (LMs) behaviors within the context of gender bias. Inspired by CheckList behavioral testing, we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA). We design three comparison studies to evaluate LMs from four aspects: consistency, biased tendency, model preference, and gender preference switch. We probe one transformer-based QA model trained on SQuAD-v2 dataset and one autoregressive large language model. Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation. Our proposed task provides the first dataset that involves human knowledge for LLM bias evaluation.</li>
</ul>

<h3>Title: LLM-Assisted Crisis Management: Building Advanced LLM Platforms for  Effective Emergency Response and Public Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Hakan T. Otal, M. Abdullah Canbaz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10908">https://arxiv.org/abs/2402.10908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10908">https://arxiv.org/pdf/2402.10908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10908]] LLM-Assisted Crisis Management: Building Advanced LLM Platforms for  Effective Emergency Response and Public Collaboration(https://arxiv.org/abs/2402.10908)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelmed, by assisting the users with simple instructions and informing authorities with their location and emergency information.</li>
</ul>

<h3>Title: News Source Credibility Assessment: A Reddit Case Study</h3>
<ul>
<li><strong>Authors: </strong>Arash Amini, Yigit Ege Bayiz, Ashwin Ram, Radu Marculescu, Ufuk Topcu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10938">https://arxiv.org/abs/2402.10938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10938">https://arxiv.org/pdf/2402.10938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10938]] News Source Credibility Assessment: A Reddit Case Study(https://arxiv.org/abs/2402.10938)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the era of social media platforms, identifying the credibility of online content is crucial to combat misinformation. We present the CREDiBERT (CREDibility assessment using Bi-directional Encoder Representations from Transformers), a source credibility assessment model fine-tuned for Reddit submissions focusing on political discourse as the main contribution. We adopt a semi-supervised training approach for CREDiBERT, leveraging Reddit's community-based structure. By encoding submission content using CREDiBERT and integrating it into a Siamese neural network, we significantly improve the binary classification of submission credibility, achieving a 9% increase in F1 score compared to existing methods. Additionally, we introduce a new version of the post-to-post network in Reddit that efficiently encodes user interactions to enhance the binary classification task by nearly 8% in F1 score. Finally, we employ CREDiBERT to evaluate the susceptibility of subreddits with respect to different topics.</li>
</ul>

<h3>Title: Text2Data: Low-Resource Data Generation with Textual Control</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Wang, Yihao Feng, Tian Lan, Ning Yu, Yu Bai, Ran Xu, Huan Wang, Caiming Xiong, Silvio Savarese</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10941">https://arxiv.org/abs/2402.10941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10941">https://arxiv.org/pdf/2402.10941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10941]] Text2Data: Low-Resource Data Generation with Textual Control(https://arxiv.org/abs/2402.10941)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model. Subsequently, it undergoes controllable finetuning via a novel constraint optimization-based learning objective that ensures controllability and effectively counteracts catastrophic forgetting. Comprehensive experiments demonstrate that Text2Data is able to achieve enhanced performance regarding controllability across various modalities, including molecules, motions and time series, when compared to existing baselines.</li>
</ul>

<h3>Title: Advances and Limitations in Open Source Arabic-Script OCR: A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Kiessling (PSL), Gennady Kurin, Matthew Thomas Miller, Kader Smail</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10943">https://arxiv.org/abs/2402.10943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10943">https://arxiv.org/pdf/2402.10943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10943]] Advances and Limitations in Open Source Arabic-Script OCR: A Case Study(https://arxiv.org/abs/2402.10943)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This work presents an accuracy study of the open source OCR engine, Kraken, on the leading Arabic scholarly journal, al-Abhath. In contrast with other commercially available OCR engines, Kraken is shown to be capable of producing highly accurate Arabic-script OCR. The study also assesses the relative accuracy of typeface-specific and generalized models on the al-Abhath data and provides a microanalysis of the ``error instances'' and the contextual features that may have contributed to OCR misrecognition. Building on this analysis, the paper argues that Arabic-script OCR can be significantly improved through (1) a more systematic approach to training data production, and (2) the development of key technological components, especially multi-language models and improved line segmentation and layout analysis. Cet article pr{\'e}sente une {\'e}tude d'exactitude du moteur ROC open source, Krakan, sur la revue acad{\'e}mique arabe de premier rang, al-Abhath. Contrairement {\`a} d'autres moteurs ROC disponibles sur le march{\'e}, Kraken se r{\'e}v{\`e}le {\^e}tre capable de produire de la ROC extr{\^e}mement exacte de l'{\'e}criture arabe. L'{\'e}tude {\'e}value aussi l'exactitude relative des mod{\`e}les sp{\'e}cifiquement configur{\'e}s {\`a} des polices et celle des mod{\`e}les g{\'e}n{\'e}ralis{\'e}s sur les donn{\'e}es d'al-Abhath et fournit une microanalyse des "occurrences d'erreurs", ainsi qu'une microanalyse des {\'e}l{\'e}ments contextuels qui pourraient avoir contribu{\'e} {\`a} la m{\'e}reconnaissance ROC. S'appuyant sur cette analyse, cet article fait valoir que la ROC de l'{\'e}criture arabe peut {\^e}tre consid{\'e}rablement am{\'e}lior{\'e}e gr{\^a}ce {\`a} (1) une approche plus syst{\'e}matique d'entra{\^i}nement de la production de donn{\'e}es et (2) gr{\^a}ce au d{\'e}veloppement de composants technologiques fondamentaux, notammentl'am{\'e}lioration des mod{\`e}les multilingues, de la segmentation de ligne et de l'analyse de la mise en page.</li>
</ul>

<h3>Title: CultureLLM: Incorporating Cultural Differences into Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, Xing Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10946">https://arxiv.org/abs/2402.10946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10946">https://arxiv.org/pdf/2402.10946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10946]] CultureLLM: Incorporating Cultural Differences into Large Language  Models(https://arxiv.org/abs/2402.10946)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM significantly outperforms various counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with comparable performance to GPT-4 or even better. Our human study shows that the generated samples are semantically equivalent to the original samples, providing an effective solution for LLMs augmentation.</li>
</ul>

<h3>Title: Zero-shot Explainable Mental Health Analysis on Social Media by  incorporating Mental Scales</h3>
<ul>
<li><strong>Authors: </strong>Wenyu Li, Yinuo Zhu, Xin Lin, Ming Li, Ziyue Jiang, Ziqian Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10948">https://arxiv.org/abs/2402.10948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10948">https://arxiv.org/pdf/2402.10948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10948]] Zero-shot Explainable Mental Health Analysis on Social Media by  incorporating Mental Scales(https://arxiv.org/abs/2402.10948)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our method can generate more rigorous explanation based on the outputs of mental questionnaires.</li>
</ul>

<h3>Title: The Unreasonable Effectiveness of Eccentric Automatic Prompts</h3>
<ul>
<li><strong>Authors: </strong>Rick Battle, Teja Gollapudi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10949">https://arxiv.org/abs/2402.10949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10949">https://arxiv.org/pdf/2402.10949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10949]] The Unreasonable Effectiveness of Eccentric Automatic Prompts(https://arxiv.org/abs/2402.10949)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus computation time, of experimenting with hand-tuning prompts for large black-box models, we then compared the performance of the best "positive thinking" prompt against the output of systematic prompt optimization. We show that employing an automated prompt optimizer emerges as the most effective method for enhancing performance, even when working with smaller open-source models. Additionally, our findings reveal that the highest-scoring, automatically-optimized prompt exhibits a degree of peculiarity far beyond expectations.</li>
</ul>

<h3>Title: DAEDRA: A language model for predicting outcomes in passive  pharmacovigilance reporting</h3>
<ul>
<li><strong>Authors: </strong>Chris von Csefalvay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10951">https://arxiv.org/abs/2402.10951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10951">https://arxiv.org/pdf/2402.10951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10951]] DAEDRA: A language model for predicting outcomes in passive  pharmacovigilance reporting(https://arxiv.org/abs/2402.10951)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Over the recent years, the emergence of large language models (LLMs) has given rise to a proliferation of domain-specific models that are intended to reflect the particularities of linguistic context and content as a correlate of the originating domain. This paper details the conception, design, training and evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes (mortality, ER attendance and hospitalisation) in adverse event reports elicited through passive reporting (PR). While PR is a highly cost-efficient way of eliciting information from a wide and diverse audience -- typically including not only physicians and healthcare providers but also patients, family members and other lay stakeholders --, this diversity makes PR corpora difficult to analyse. Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports. To evaluate the utility of a subdomain-specific language model, an adaptive training approach was adapted, wherein base language model candidates were evaluated on a subset of the corpus, and the best performer was trained on the entire corpus. This yielded a small but significant improvement in $F_1$ (+1%), precision (+2.5%) and recall (+3.8%), at a relatively low training cost and a single-day training time. Subdomain-specific LLMs continue to be viable options for better results when analysing highly specialised corpora.</li>
</ul>

<h3>Title: Relative Preference Optimization: Enhancing LLM Alignment through  Contrasting Responses across Identical and Diverse Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10958">https://arxiv.org/abs/2402.10958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10958">https://arxiv.org/pdf/2402.10958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10958]] Relative Preference Optimization: Enhancing LLM Alignment through  Contrasting Responses across Identical and Diverse Prompts(https://arxiv.org/abs/2402.10958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to leverage insights from a more varied set of prompts. Through empirical tests, including dialogue and summarization tasks, and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a superior ability to align LLMs with user preferences and to improve their adaptability during the training process. The PyTorch code necessary to reproduce the results presented in the paper will be made available on GitHub for public access.</li>
</ul>

<h3>Title: Measuring and Controlling Persona Drift in Language Model Dialogs</h3>
<ul>
<li><strong>Authors: </strong>Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10962">https://arxiv.org/abs/2402.10962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10962">https://arxiv.org/pdf/2402.10962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10962]] Measuring and Controlling Persona Drift in Language Model Dialogs(https://arxiv.org/abs/2402.10962)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.</li>
</ul>

<h3>Title: Generalization in Healthcare AI: Evaluation of a Clinical Large Language  Model</h3>
<ul>
<li><strong>Authors: </strong>Salman Rahman, Lavender Yao Jiang, Saadia Gabriel, Yindalon Aphinyanaphongs, Eric Karl Oermann, Rumi Chunara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10965">https://arxiv.org/abs/2402.10965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10965">https://arxiv.org/pdf/2402.10965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10965]] Generalization in Healthcare AI: Evaluation of a Clinical Large Language  Model(https://arxiv.org/abs/2402.10965)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes for fine-tuning, note content (number of words per note), patient characteristics (comorbidity level, age, insurance type, borough), and health system aspects (hospital, all-cause 30-day readmission, and mortality rates). We used descriptive statistics and supervised classification to identify features. We found that, along with sample size, patient age, number of comorbidities, and the number of words in notes are all important factors related to generalization. Finally, we compared local fine-tuning (hospital specific), instance-based augmented fine-tuning and cluster-based fine-tuning for improving generalization. Among these, local fine-tuning proved most effective, increasing AUC by 0.25% to 11.74% (most helpful in settings with limited data). Overall, this study provides new insights for enhancing the deployment of large language models in the societally important domain of healthcare, and improving their performance for broader populations.</li>
</ul>

<h3>Title: On the Cross-Dataset Generalization of Machine Learning for Network  Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Marco Cantone, Claudio Marrocco, Alessandro Bria</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10974">https://arxiv.org/abs/2402.10974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10974">https://arxiv.org/pdf/2402.10974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10974]] On the Cross-Dataset Generalization of Machine Learning for Network  Intrusion Detection(https://arxiv.org/abs/2402.10974)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Network Intrusion Detection Systems (NIDS) are a fundamental tool in cybersecurity. Their ability to generalize across diverse networks is a critical factor in their effectiveness and a prerequisite for real-world applications. In this study, we conduct a comprehensive analysis on the generalization of machine-learning-based NIDS through an extensive experimentation in a cross-dataset framework. We employ four machine learning classifiers and utilize four datasets acquired from different networks: CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018. Notably, the last dataset is a novel contribution, where we apply corrections based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results show nearly perfect classification performance when the models are trained and tested on the same dataset. However, when training and testing the models in a cross-dataset fashion, the classification accuracy is largely commensurate with random chance except for a few combinations of attacks and datasets. We employ data visualization techniques in order to provide valuable insights on the patterns in the data. Our analysis unveils the presence of anomalies in the data that directly hinder the classifiers capability to generalize the learned knowledge to new scenarios. This study enhances our comprehension of the generalization capabilities of machine-learning-based NIDS, highlighting the significance of acknowledging data heterogeneity.</li>
</ul>

<h3>Title: Generative AI and Process Systems Engineering: The Next Frontier</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Decardi-Nelson, Abdulelah S. Alshehri, Akshay Ajagekar, Fengqi You</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10977">https://arxiv.org/abs/2402.10977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10977">https://arxiv.org/pdf/2402.10977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10977]] Generative AI and Process Systems Engineering: The Next Frontier(https://arxiv.org/abs/2402.10977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.</li>
</ul>

<h3>Title: SportsMetrics: Blending Text and Numerical Data to Understand  Information Fusion in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Dong Yu, Fei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10979">https://arxiv.org/abs/2402.10979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10979">https://arxiv.org/pdf/2402.10979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10979]] SportsMetrics: Blending Text and Numerical Data to Understand  Information Fusion in LLMs(https://arxiv.org/abs/2402.10979)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.</li>
</ul>

<h3>Title: Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of  Conjugate Variables in System Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jun-Jie Zhang, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10983">https://arxiv.org/abs/2402.10983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10983">https://arxiv.org/pdf/2402.10983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10983]] Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of  Conjugate Variables in System Attacks(https://arxiv.org/abs/2402.10983)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Neural networks demonstrate inherent vulnerability to small, non-random perturbations, emerging as adversarial attacks. Such attacks, born from the gradient of the loss function relative to the input, are discerned as input conjugates, revealing a systemic fragility within the network structure. Intriguingly, a mathematical congruence manifests between this mechanism and the quantum physics' uncertainty principle, casting light on a hitherto unanticipated interdisciplinarity. This inherent susceptibility within neural network systems is generally intrinsic, highlighting not only the innate vulnerability of these networks but also suggesting potential advancements in the interdisciplinary area for understanding these black-box networks.</li>
</ul>

<h3>Title: Leveraging AI Planning For Detecting Cloud Security Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Kazdagli, Mohit Tiwari, Akshat Kumar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10985">https://arxiv.org/abs/2402.10985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10985">https://arxiv.org/pdf/2402.10985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10985]] Leveraging AI Planning For Detecting Cloud Security Vulnerabilities(https://arxiv.org/abs/2402.10985)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations. Our system can identify a broad range of security vulnerabilities, which state-of-the-art industry tools cannot detect.</li>
</ul>

<h3>Title: FinTral: A Family of GPT-4 Level Multimodal Financial Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, Muhammad Abdul-Mageed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10986">https://arxiv.org/abs/2402.10986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10986">https://arxiv.org/pdf/2402.10986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10986]] FinTral: A Family of GPT-4 Level Multimodal Financial Large Language  Models(https://arxiv.org/abs/2402.10986)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to excel in real-time analysis and decision-making in diverse financial contexts.</li>
</ul>

<h3>Title: WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10987">https://arxiv.org/abs/2402.10987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10987">https://arxiv.org/pdf/2402.10987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10987]] WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing(https://arxiv.org/abs/2402.10987)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. In this paper, lifelong editing is synonymous with lifelong knowledge editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.</li>
</ul>

<h3>Title: Accelerating Semi-Asynchronous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Changxin Xu, Yuxin Qiao, Zhanxin Zhou, Fanghao Ni, Jize Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10991">https://arxiv.org/abs/2402.10991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10991">https://arxiv.org/pdf/2402.10991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10991]] Accelerating Semi-Asynchronous Federated Learning(https://arxiv.org/abs/2402.10991)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adjusts the contribution of each update based on these factors, which can speed up convergence compared to existing methods.</li>
</ul>

<h3>Title: "Understanding AI": Semantic Grounding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Holger Lyre</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10992">https://arxiv.org/abs/2402.10992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10992">https://arxiv.org/pdf/2402.10992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10992]] "Understanding AI": Semantic Grounding in Large Language Models(https://arxiv.org/abs/2402.10992)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.</li>
</ul>

<h3>Title: ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yangyifei Luo, Zhuo Chen, Lingbing Guo, Qian Li, Wenxuan Zeng, Zhixin Cai, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11000">https://arxiv.org/abs/2402.11000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11000">https://arxiv.org/pdf/2402.11000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11000]] ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment(https://arxiv.org/abs/2402.11000)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results demonstrate the superior performance of ASGEA over the existing embedding-based methods in both EA and Multi-Modal EA (MMEA) tasks. Our code will be available soon.</li>
</ul>

<h3>Title: The Evolution of Statistical Induction Heads: In-Context Learning Markov  Chains</h3>
<ul>
<li><strong>Authors: </strong>Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, Nikolaos Tsilivis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11004">https://arxiv.org/abs/2402.11004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11004">https://arxiv.org/pdf/2402.11004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11004]] The Evolution of Statistical Induction Heads: In-Context Learning Markov  Chains(https://arxiv.org/abs/2402.11004)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning (ICL) capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between the transformer's layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in-context learning of Markov chains (ICL-MC) task to $n$-grams for $n > 2$.</li>
</ul>

<h3>Title: Automated Detection and Analysis of Data Practices Using A Real-World  Corpus</h3>
<ul>
<li><strong>Authors: </strong>Mukund Srinath, Pranav Venkit, Maria Badillo, Florian Schaub, C. Lee Giles, Shomir Wilson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11006">https://arxiv.org/abs/2402.11006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11006">https://arxiv.org/pdf/2402.11006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11006]] Automated Detection and Analysis of Data Practices Using A Real-World  Corpus(https://arxiv.org/abs/2402.11006)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy policies are crucial for informing users about data practices, yet their length and complexity often deter users from reading them. In this paper, we propose an automated approach to identify and visualize data practices within privacy policies at different levels of detail. Leveraging crowd-sourced annotations from the ToS;DR platform, we experiment with various methods to match policy excerpts with predefined data practice descriptions. We further conduct a case study to evaluate our approach on a real-world policy, demonstrating its effectiveness in simplifying complex policies. Experiments show that our approach accurately matches data practice descriptions with policy excerpts, facilitating the presentation of simplified privacy information to users.</li>
</ul>

<h3>Title: Training Bayesian Neural Networks with Sparse Subspace Variational  Inference</h3>
<ul>
<li><strong>Authors: </strong>Junbo Li, Zichen Miao, Qiang Qiu, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11025">https://arxiv.org/abs/2402.11025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11025">https://arxiv.org/pdf/2402.11025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11025]] Training Bayesian Neural Networks with Sparse Subspace Variational  Inference(https://arxiv.org/abs/2402.11025)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the optimal solution with a removal-and-addition strategy, guided by novel criteria based on weight distribution statistics. Our extensive experiments show that SSVI sets new benchmarks in crafting sparse BNNs, achieving, for instance, a 10-20x compression in model size with under 3\% performance drop, and up to 20x FLOPs reduction during training compared with dense VI training. Remarkably, SSVI also demonstrates enhanced robustness to hyperparameters, reducing the need for intricate tuning in VI and occasionally even surpassing VI-trained dense BNNs on both accuracy and uncertainty metrics.</li>
</ul>

<h3>Title: PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal  Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11034">https://arxiv.org/abs/2402.11034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11034">https://arxiv.org/pdf/2402.11034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11034]] PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal  Question-Answering(https://arxiv.org/abs/2402.11034)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. "Who was the US president in 1970?"). Little work has studied questions whose temporal context is relative to the present time (e.g. "Who was the previous US president?"). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model (TEMPREASON-T5) on PAT-Questions through direct prompting and retrieval-augmented generation (RAG). The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.</li>
</ul>

<h3>Title: Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Reichman, Larry Heck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11035">https://arxiv.org/abs/2402.11035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11035">https://arxiv.org/pdf/2402.11035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11035]] Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?(https://arxiv.org/abs/2402.11035)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process to more knowledge so more can be decentralized, (2) inject facts as decentralized representations, (3) model and incorporate knowledge uncertainty in the retrieval process, and (4) directly map internal model knowledge to a knowledge base.</li>
</ul>

<h3>Title: Occlusion Resilient 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11036">https://arxiv.org/abs/2402.11036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11036">https://arxiv.org/pdf/2402.11036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11036]] Occlusion Resilient 3D Human Pose Estimation(https://arxiv.org/abs/2402.11036)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Occlusions remain one of the key challenges in 3D body pose estimation from single-camera video sequences. Temporal consistency has been extensively used to mitigate their impact but the existing algorithms in the literature do not explicitly model them. Here, we apply this by representing the deforming body as a spatio-temporal graph. We then introduce a refinement network that performs graph convolutions over this graph to output 3D poses. To ensure robustness to occlusions, we train this network with a set of binary masks that we use to disable some of the edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods of time and train the network to be immune to that. We demonstrate the effectiveness of this approach compared to state-of-the-art techniques that infer poses from single-camera sequences.</li>
</ul>

<h3>Title: Robustness to Subpopulation Shift with Domain Label Noise via  Regularized Annotation of Domains</h3>
<ul>
<li><strong>Authors: </strong>Nathan Stromberg, Rohan Ayyagari, Monica Welfert, Sanmi Koyejo, Lalitha Sankar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11039">https://arxiv.org/abs/2402.11039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11039">https://arxiv.org/pdf/2402.11039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11039]] Robustness to Subpopulation Shift with Domain Label Noise via  Regularized Annotation of Domains(https://arxiv.org/abs/2402.11039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.</li>
</ul>

<h3>Title: Large Language Models Fall Short: Understanding Complex Relationships in  Detective Narratives</h3>
<ul>
<li><strong>Authors: </strong>Runcong Zhao, Qinglin Zhu, Hainiu Xu, Jiazheng Li, Yuxiang Zhou, Yulan He, Lin Gui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11051">https://arxiv.org/abs/2402.11051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11051">https://arxiv.org/pdf/2402.11051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11051]] Large Language Models Fall Short: Understanding Complex Relationships in  Detective Narratives(https://arxiv.org/abs/2402.11051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts.</li>
</ul>

<h3>Title: Persona-DB: Efficient Large Language Model Personalization for Response  Prediction with Collaborative Data Refinement</h3>
<ul>
<li><strong>Authors: </strong>Chenkai Sun, Ke Yang, Revanth Gangi Reddy, Yi R. Fung, Hou Pong Chan, ChengXiang Zhai, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11060">https://arxiv.org/abs/2402.11060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11060">https://arxiv.org/pdf/2402.11060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11060]] Persona-DB: Efficient Large Language Model Personalization for Response  Prediction with Collaborative Data Refinement(https://arxiv.org/abs/2402.11060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the task of response forecasting, Persona-DB demonstrates superior efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 15% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands.</li>
</ul>

<h3>Title: Towards Financially Inclusive Credit Products Through Financial Time  Series Clustering</h3>
<ul>
<li><strong>Authors: </strong>Tristan Bester, Benjamin Rosman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11066">https://arxiv.org/abs/2402.11066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11066">https://arxiv.org/pdf/2402.11066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11066]] Towards Financially Inclusive Credit Products Through Financial Time  Series Clustering(https://arxiv.org/abs/2402.11066)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Financial inclusion ensures that individuals have access to financial products and services that meet their needs. As a key contributing factor to economic growth and investment opportunity, financial inclusion increases consumer spending and consequently business development. It has been shown that institutions are more profitable when they provide marginalised social groups access to financial services. Customer segmentation based on consumer transaction data is a well-known strategy used to promote financial inclusion. While the required data is available to modern institutions, the challenge remains that segment annotations are usually difficult and/or expensive to obtain. This prevents the usage of time series classification models for customer segmentation based on domain expert knowledge. As a result, clustering is an attractive alternative to partition customers into homogeneous groups based on the spending behaviour encoded within their transaction data. In this paper, we present a solution to one of the key challenges preventing modern financial institutions from providing financially inclusive credit, savings and insurance products: the inability to understand consumer financial behaviour, and hence risk, without the introduction of restrictive conventional credit scoring techniques. We present a novel time series clustering algorithm that allows institutions to understand the financial behaviour of their customers. This enables unique product offerings to be provided based on the needs of the customer, without reliance on restrictive credit practices.</li>
</ul>

<h3>Title: Bridging Causal Discovery and Large Language Models: A Comprehensive  Survey of Integrative Approaches and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Guangya Wan, Yuqi Wu, Mengxuan Hu, Zhixuan Chu, Sheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11068">https://arxiv.org/abs/2402.11068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11068">https://arxiv.org/pdf/2402.11068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11068]] Bridging Causal Discovery and Large Language Models: A Comprehensive  Survey of Integrative Approaches and Future Directions(https://arxiv.org/abs/2402.11068)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature and propose future research directions aimed at harnessing the full potential of LLMs in causality research. To our knowledge, this is the first survey to offer a unified and detailed examination of the synergy between LLMs and CD, setting the stage for future advancements in the field.</li>
</ul>

<h3>Title: AFaCTA: Assisting the Annotation of Factual Claim Detection with  Reliable LLM Annotators</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Ni, Minjing Shi, Dominik Stammbach, Mrinmaya Sachan, Elliott Ash, Markus Leippold</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11073">https://arxiv.org/abs/2402.11073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11073">https://arxiv.org/pdf/2402.11073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11073]] AFaCTA: Assisting the Annotation of Factual Claim Detection with  Reliable LLM Annotators(https://arxiv.org/abs/2402.11073)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.</li>
</ul>

<h3>Title: The AI Security Pyramid of Pain</h3>
<ul>
<li><strong>Authors: </strong>Chris M. Ward, Josh Harguess, Julia Tao, Daniel Christman, Paul Spicer, Mike Tan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11082">https://arxiv.org/abs/2402.11082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11082">https://arxiv.org/pdf/2402.11082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11082]] The AI Security Pyramid of Pain(https://arxiv.org/abs/2402.11082)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>We introduce the AI Security Pyramid of Pain, a framework that adapts the cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats. This framework provides a structured approach to understanding and addressing various levels of AI threats. Starting at the base, the pyramid emphasizes Data Integrity, which is essential for the accuracy and reliability of datasets and AI models, including their weights and parameters. Ensuring data integrity is crucial, as it underpins the effectiveness of all AI-driven decisions and operations. The next level, AI System Performance, focuses on MLOps-driven metrics such as model drift, accuracy, and false positive rates. These metrics are crucial for detecting potential security breaches, allowing for early intervention and maintenance of AI system integrity. Advancing further, the pyramid addresses the threat posed by Adversarial Tools, identifying and neutralizing tools used by adversaries to target AI systems. This layer is key to staying ahead of evolving attack methodologies. At the Adversarial Input layer, the framework addresses the detection and mitigation of inputs designed to deceive or exploit AI models. This includes techniques like adversarial patterns and prompt injection attacks, which are increasingly used in sophisticated attacks on AI systems. Data Provenance is the next critical layer, ensuring the authenticity and lineage of data and models. This layer is pivotal in preventing the use of compromised or biased data in AI systems. At the apex is the tactics, techniques, and procedures (TTPs) layer, dealing with the most complex and challenging aspects of AI security. This involves a deep understanding and strategic approach to counter advanced AI-targeted attacks, requiring comprehensive knowledge and planning.</li>
</ul>

<h3>Title: VQAttack: Transferable Adversarial Attacks on Visual Question Answering  via Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11083">https://arxiv.org/abs/2402.11083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11083">https://arxiv.org/pdf/2402.11083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11083]] VQAttack: Transferable Adversarial Attacks on Visual Question Answering  via Pre-trained Models(https://arxiv.org/abs/2402.11083)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) is a fundamental task in computer vision and natural language process fields. Although the ``pre-training & finetuning'' learning paradigm significantly improves the VQA performance, the adversarial robustness of such a learning paradigm has not been explored. In this paper, we delve into a new problem: using a pre-trained multimodal source model to create adversarial image-text pairs and then transferring them to attack the target VQA models. Correspondingly, we propose a novel VQAttack model, which can iteratively generate both image and text perturbations with the designed modules: the large language model (LLM)-enhanced image attack and the cross-modal joint attack module. At each iteration, the LLM-enhanced image attack module first optimizes the latent representation-based loss to generate feature-level image perturbations. Then it incorporates an LLM to further enhance the image perturbations by optimizing the designed masked answer anti-recovery loss. The cross-modal joint attack module will be triggered at a specific iteration, which updates the image and text perturbations sequentially. Notably, the text perturbation updates are based on both the learned gradients in the word embedding space and word synonym-based substitution. Experimental results on two VQA datasets with five validated models demonstrate the effectiveness of the proposed VQAttack in the transferable attack setting, compared with state-of-the-art baselines. This work reveals a significant blind spot in the ``pre-training & fine-tuning'' paradigm on VQA tasks. Source codes will be released.</li>
</ul>

<h3>Title: The Male CEO and the Female Assistant: Probing Gender Biases in  Text-To-Image Models Through Paired Stereotype Test</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wan, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11089">https://arxiv.org/abs/2402.11089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11089">https://arxiv.org/pdf/2402.11089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11089]] The Male CEO and the Female Assistant: Probing Gender Biases in  Text-To-Image Models Through Paired Stereotype Test(https://arxiv.org/abs/2402.11089)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power. Despite seemingly fair or even anti-stereotype single-person generations, PST still unveils gendered occupational and power associations. Moreover, compared to single-person settings, DALLE-3 generates noticeably more masculine figures under PST for individuals with male-stereotypical identities. PST is therefore effective in revealing underlying gender biases in DALLE-3 that single-person settings cannot capture. Our findings reveal the complicated patterns of gender biases in modern T2I models, further highlighting the critical fairness challenges in multimodal generative systems.</li>
</ul>

<h3>Title: Modular Graph Extraction for Handwritten Circuit Diagram Images</h3>
<ul>
<li><strong>Authors: </strong>Johannes Bayer, Leo van Waveren, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11093">https://arxiv.org/abs/2402.11093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11093">https://arxiv.org/pdf/2402.11093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11093]] Modular Graph Extraction for Handwritten Circuit Diagram Images(https://arxiv.org/abs/2402.11093)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>As digitization in engineering progressed, circuit diagrams (also referred to as schematics) are typically developed and maintained in computer-aided engineering (CAE) systems, thus allowing for automated verification, simulation and further processing in downstream engineering steps. However, apart from printed legacy schematics, hand-drawn circuit diagrams are still used today in the educational domain, where they serve as an easily accessible mean for trainees and students to learn drawing this type of diagrams. Furthermore, hand-drawn schematics are typically used in examinations due to legal constraints. In order to harness the capabilities of digital circuit representations, automated means for extracting the electrical graph from raster graphics are required. While respective approaches have been proposed in literature, they are typically conducted on small or non-disclosed datasets. This paper describes a modular end-to-end solution on a larger, public dataset, in which approaches for the individual sub-tasks are evaluated to form a new baseline. These sub-tasks include object detection (for electrical symbols and texts), binary segmentation (drafter's stroke vs. background), handwritten character recognition and orientation regression for electrical symbols and texts. Furthermore, computer-vision graph assembly and rectification algorithms are presented. All methods are integrated in a publicly available prototype.</li>
</ul>

<h3>Title: Word Embeddings Revisited: Do LLMs Offer Something New?</h3>
<ul>
<li><strong>Authors: </strong>Matthew Freestone, Shubhra Kanti Karmaker Santu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11094">https://arxiv.org/abs/2402.11094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11094">https://arxiv.org/pdf/2402.11094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11094]] Word Embeddings Revisited: Do LLMs Offer Something New?(https://arxiv.org/abs/2402.11094)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings similar to SBERT, a relatively lighter classical model.</li>
</ul>

<h3>Title: GIM: Learning Generalizable Image Matcher From Internet Videos</h3>
<ul>
<li><strong>Authors: </strong>Xuelun Shen, Zhipeng Cai, Wei Yin, Matthias M√ºller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11095">https://arxiv.org/abs/2402.11095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11095">https://arxiv.org/pdf/2402.11095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11095]] GIM: Learning Generalizable Image Matcher From Internet Videos(https://arxiv.org/abs/2402.11095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Applying GIM consistently improves the zero-shot performance of 3 state-of-the-art image matching architectures; with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4%-18.1%. GIM also enables generalization to extreme cross-domain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1(c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The video presentation is available at https://www.youtube.com/watch?v=FU_MJLD8LeY.</li>
</ul>

<h3>Title: When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11100">https://arxiv.org/abs/2402.11100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11100">https://arxiv.org/pdf/2402.11100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11100]] When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for  Large Language Models(https://arxiv.org/abs/2402.11100)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies.</li>
</ul>

<h3>Title: DART: A Principled Approach to Adversarially Robust Unsupervised Domain  Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yunjuan Wang, Hussein Hazimeh, Natalia Ponomareva, Alexey Kurakin, Ibrahim Hammoud, Raman Arora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11120">https://arxiv.org/abs/2402.11120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11120">https://arxiv.org/pdf/2402.11120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11120]] DART: A Principled Approach to Adversarially Robust Unsupervised Domain  Adaptation(https://arxiv.org/abs/2402.11120)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivated by this bound, we develop a novel unified defense framework called Divergence Aware adveRsarial Training (DART), which can be used in conjunction with a variety of standard UDA methods; e.g., DANN [Ganin and Lempitsky, 2015]. DART is applicable to general threat models, including the popular $\ell_p$-norm model, and does not require heuristic regularizers or architectural changes. We also release DomainRobust: a testbed for evaluating robustness of UDA models to adversarial attacks. DomainRobust consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and 7 meta-algorithms with a total of 11 variants. Our large-scale experiments demonstrate that on average, DART significantly enhances model robustness on all benchmarks compared to the state of the art, while maintaining competitive standard accuracy. The relative improvement in robustness from DART reaches up to 29.2% on the source-target domain pairs considered.</li>
</ul>

<h3>Title: Navigating the Dual Facets: A Comprehensive Evaluation of Sequential  Memory Editing in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihao Lin, Mohammad Beigi, Hongxuan Li, Yufan Zhou, Yuxiang Zhang, Qifan Wang, Wenpeng Yin, Lifu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11122">https://arxiv.org/abs/2402.11122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11122">https://arxiv.org/pdf/2402.11122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11122]] Navigating the Dual Facets: A Comprehensive Evaluation of Sequential  Memory Editing in Large Language Models(https://arxiv.org/abs/2402.11122)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast, parameter-preserving ME effectively maintains LLMs' fundamental capabilities but struggles to accurately recall edited knowledge presented in a different format. (ii) We extend our evaluation to different editing settings, such as layers to edit, model size, instruction tuning, etc. Experimental findings indicate several strategies that can potentially mitigate the adverse effects of ME. (iii) We further explain why parameter-modifying ME damages LLMs from three dimensions: parameter changes after editing, language modeling capability, and the in-context learning capability. Our in-depth study advocates more careful use of ME in real-world scenarios.</li>
</ul>

<h3>Title: Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning  (PIML) Methods: Towards Robust Metrics</h3>
<ul>
<li><strong>Authors: </strong>Michael Penwarden, Houman Owhadi, Robert M. Kirby</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11126">https://arxiv.org/abs/2402.11126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11126">https://arxiv.org/pdf/2402.11126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11126]] Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning  (PIML) Methods: Towards Robust Metrics(https://arxiv.org/abs/2402.11126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Physics-informed machine learning (PIML) as a means of solving partial differential equations (PDE) has garnered much attention in the Computational Science and Engineering (CS&E) world. This topic encompasses a broad array of methods and models aimed at solving a single or a collection of PDE problems, called multitask learning. PIML is characterized by the incorporation of physical laws into the training process of machine learning models in lieu of large data when solving PDE problems. Despite the overall success of this collection of methods, it remains incredibly difficult to analyze, benchmark, and generally compare one approach to another. Using Kolmogorov n-widths as a measure of effectiveness of approximating functions, we judiciously apply this metric in the comparison of various multitask PIML architectures. We compute lower accuracy bounds and analyze the model's learned basis functions on various PDE problems. This is the first objective metric for comparing multitask PIML architectures and helps remove uncertainty in model validation from selective sampling and overfitting. We also identify avenues of improvement for model architectures, such as the choice of activation function, which can drastically affect model generalization to "worst-case" scenarios, which is not observed when reporting task-specific errors. We also incorporate this metric into the optimization process through regularization, which improves the models' generalizability over the multitask PDE problem.</li>
</ul>

<h3>Title: BlendFilter: Advancing Retrieval-Augmented Large Language Models via  Query Generation Blending and Knowledge Filtering</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Wang, Tuo Zhao, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11129">https://arxiv.org/abs/2402.11129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11129">https://arxiv.org/pdf/2402.11129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11129]] BlendFilter: Advancing Retrieval-Augmented Large Language Models via  Query Generation Blending and Knowledge Filtering(https://arxiv.org/abs/2402.11129)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.</li>
</ul>

<h3>Title: TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Feuer, Robin Tibor Schirrmeister, Valeriia Cherepanova, Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, Colin White</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11137">https://arxiv.org/abs/2402.11137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11137">https://arxiv.org/pdf/2402.11137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11137]] TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks(https://arxiv.org/abs/2402.11137)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datasets, while having a substantially lower inference time than TabPFN. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective.</li>
</ul>

<h3>Title: Contrastive Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Yan, Fei Wang, James Y. Huang, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11138">https://arxiv.org/abs/2402.11138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11138">https://arxiv.org/pdf/2402.11138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11138]] Contrastive Instruction Tuning(https://arxiv.org/abs/2402.11138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructions with variations across character, word, sentence, and semantic levels by an average of +2.5% in accuracy.</li>
</ul>

<h3>Title: Boosting of Thoughts: Trial-and-Error Problem Solving with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sijia Chen, Baochun Li, Di Niu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11140">https://arxiv.org/abs/2402.11140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11140">https://arxiv.org/pdf/2402.11140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11140]] Boosting of Thoughts: Trial-and-Error Problem Solving with Large  Language Models(https://arxiv.org/abs/2402.11140)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.</li>
</ul>

<h3>Title: Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Thang-Anh-Quan Nguyen, Amine Bourki, M√°ty√°s Macudzinski, Anthony Brunel, Mohammed Bennamoun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11141">https://arxiv.org/abs/2402.11141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11141">https://arxiv.org/pdf/2402.11141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11141]] Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review(https://arxiv.org/abs/2402.11141)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This review thoroughly examines the role of semantically-aware Neural Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of over 250 scholarly papers. It explores how NeRFs adeptly infer 3D representations for both stationary and dynamic objects in a scene. This capability is pivotal for generating high-quality new viewpoints, completing missing scene details (inpainting), conducting comprehensive scene segmentation (panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and extracting object-centric 3D models. A significant aspect of this study is the application of semantic labels as viewpoint-invariant functions, which effectively map spatial coordinates to a spectrum of semantic labels, thus facilitating the recognition of distinct objects within the scene. Overall, this survey highlights the progression and diverse applications of semantically-aware neural radiance fields in the context of visual scene interpretation.</li>
</ul>

<h3>Title: Grasping the Essentials: Tailoring Large Language Models for Zero-Shot  Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Sizhe Zhou, Yu Meng, Bowen Jin, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11142">https://arxiv.org/abs/2402.11142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11142">https://arxiv.org/pdf/2402.11142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11142]] Grasping the Essentials: Tailoring Large Language Models for Zero-Shot  Relation Extraction(https://arxiv.org/abs/2402.11142)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Relation extraction (RE), a crucial task in NLP, aims to identify semantic relationships between entities mentioned in texts. Despite significant advancements in this field, existing models typically rely on extensive annotated data for training, which can be both costly and time-consuming to acquire. Moreover, these models often struggle to adapt to new or unseen relationships. In contrast, few-shot learning settings, which aim to reduce annotation requirements, may offer incomplete and biased supervision for understanding target relation semantics, leading to degraded and unstable performance. To provide the model with accurate and explicit descriptions of the relations types and meanwhile minimize the annotation requirements, we study the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model. Motivated by the strong synthetic data generation power of LLMs, we propose a framework REPaL which consists of three stages: (1) We utilize LLMs to generate initial seed instances based on relation definitions and an unlabeled corpora. (2) We fine-tune a bidirectional Small Language Model (SLM) using these initial seeds to learn the relations for the target domain. (3) We enhance pattern coverage and mitigate bias resulting from the limited number of initial seeds by incorporating feedback acquired from SLM's predictions on unlabeled corpora. To accomplish this, we leverage the multi-turn conversation ability of LLMs to generate new instances in follow-up dialogues. Experiments on two datasets show REPaL achieves better zero-shot performance with large margins over baseline methods.</li>
</ul>

<h3>Title: PANDA (Pedantic ANswer-correctness Determination and  Adjudication):Improving Automatic Evaluation for Question Answering and Text  Generation</h3>
<ul>
<li><strong>Authors: </strong>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11161">https://arxiv.org/abs/2402.11161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11161">https://arxiv.org/pdf/2402.11161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11161]] PANDA (Pedantic ANswer-correctness Determination and  Adjudication):Improving Automatic Evaluation for Question Answering and Text  Generation(https://arxiv.org/abs/2402.11161)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.</li>
</ul>

<h3>Title: KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning  over Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11163">https://arxiv.org/abs/2402.11163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11163">https://arxiv.org/pdf/2402.11163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11163]] KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning  over Knowledge Graph(https://arxiv.org/abs/2402.11163)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.</li>
</ul>

<h3>Title: GenDec: A robust generative Question-decomposition method for Multi-hop  reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jian Wu, Linyi Yang, Yuliang Ji, Wenhao Huang, B√∂rje F. Karlsson, Manabu Okumura</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11166">https://arxiv.org/abs/2402.11166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11166">https://arxiv.org/pdf/2402.11166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11166]] GenDec: A robust generative Question-decomposition method for Multi-hop  reasoning(https://arxiv.org/abs/2402.11166)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex questions and find multiple relevant supporting facts. However, Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer. In this paper, we propose a \textbf{gen}erative question \textbf{dec}omposition method (GenDec) from the perspective of explainable QA by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing LLMs' reasoning ability in RAG. To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small QA systems on paragraph retrieval and QA tasks. We secondly examine the reasoning capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5 combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA, MuSiQue, and PokeMQA datasets.</li>
</ul>

<h3>Title: Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated  Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Fan Huang, Haewoon Kwak, Jisun An</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11167">https://arxiv.org/abs/2402.11167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11167">https://arxiv.org/pdf/2402.11167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11167]] Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated  Text Detection(https://arxiv.org/abs/2402.11167)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.</li>
</ul>

<h3>Title: Trust Regions for Explanations via Black-Box Probabilistic Certification</h3>
<ul>
<li><strong>Authors: </strong>Amit Dhurandhar, Swagatam Haldar, Dennis Wei, Karthikeyan Natesan Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11168">https://arxiv.org/abs/2402.11168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11168">https://arxiv.org/pdf/2402.11168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11168]] Trust Regions for Explanations via Black-Box Probabilistic Certification(https://arxiv.org/abs/2402.11168)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and money by not having to find explanations for every example; and iv) a possible \emph{meta-metric} to compare explanation methods. Our contributions include formalizing this problem, proposing solutions, providing theoretical guarantees for these solutions that are computable, and experimentally showing their efficacy on synthetic and real data.</li>
</ul>

<h3>Title: M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text  Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohanned Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11175">https://arxiv.org/abs/2402.11175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11175">https://arxiv.org/pdf/2402.11175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11175]] M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text  Detection(https://arxiv.org/abs/2402.11175)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows less than random guess performance, demonstrating the challenges to distinguish unique LLMs. Promising results always occur when training and test data distribute within the same domain or generators.</li>
</ul>

<h3>Title: KnowTuning: Knowledge-aware Fine-tuning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11176">https://arxiv.org/abs/2402.11176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11176">https://arxiv.org/pdf/2402.11176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11176]] KnowTuning: Knowledge-aware Fine-tuning for Large Language Models(https://arxiv.org/abs/2402.11176)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effectiveness of KnowTuning, through automatic and human evaluations, across various sizes of LLMs. Finally, we demonstrate that the improvements of KnowTuning generalize to unseen QA datasets.</li>
</ul>

<h3>Title: A Question Answering Based Pipeline for Comprehensive Chinese EHR  Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Huaiyuan Ying, Sheng Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11177">https://arxiv.org/abs/2402.11177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11177">https://arxiv.org/pdf/2402.11177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11177]] A Question Answering Based Pipeline for Comprehensive Chinese EHR  Information Extraction(https://arxiv.org/abs/2402.11177)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Electronic health records (EHRs) hold significant value for research and applications. As a new way of information extraction, question answering (QA) can extract more flexible information than conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. In this paper, we propose a novel approach that automatically generates training data for transfer learning of QA models. Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive QA frameworks, including cases with discontinuous answers and many-to-one relationships. The obtained QA model exhibits excellent performance on subtasks of information extraction in EHRs, and it can effectively handle few-shot or zero-shot settings involving yes-no questions. Case studies and ablation studies demonstrate the necessity of each component in our design, and the resulting model is deemed suitable for practical use.</li>
</ul>

<h3>Title: Minimally Supervised Topological Projections of Self-Organizing Maps for  Phase of Flight Identification</h3>
<ul>
<li><strong>Authors: </strong>Zimeng Lyu, Pujan Thapa, Travis Desell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11185">https://arxiv.org/abs/2402.11185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11185">https://arxiv.org/pdf/2402.11185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11185]] Minimally Supervised Topological Projections of Self-Organizing Maps for  Phase of Flight Identification(https://arxiv.org/abs/2402.11185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Identifying phases of flight is important in the field of general aviation, as knowing which phase of flight data is collected from aircraft flight data recorders can aid in the more effective detection of safety or hazardous events. General aviation flight data for phase of flight identification is usually per-second data, comes on a large scale, and is class imbalanced. It is expensive to manually label the data and training classification models usually faces class imbalance problems. This work investigates the use of a novel method for minimally supervised self-organizing maps (MS-SOMs) which utilize nearest neighbor majority votes in the SOM U-matrix for class estimation. Results show that the proposed method can reach or exceed a naive SOM approach which utilized a full data file of labeled data, with only 30 labeled datapoints per class. Additionally, the minimally supervised SOM is significantly more robust to the class imbalance of the phase of flight data. These results highlight how little data is required for effective phase of flight identification.</li>
</ul>

<h3>Title: LaCo: Large Language Model Pruning via Layer Collapse</h3>
<ul>
<li><strong>Authors: </strong>Yifei Yang, Zouying Cao, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11187">https://arxiv.org/abs/2402.11187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11187">https://arxiv.org/pdf/2402.11187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11187]] LaCo: Large Language Model Pruning via Layer Collapse(https://arxiv.org/abs/2402.11187)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\% at pruning ratios of 25-30\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pruning method effectively inherits the parameters of the original model. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios.</li>
</ul>

<h3>Title: Disclosure and Mitigation of Gender Bias in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11190">https://arxiv.org/abs/2402.11190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11190">https://arxiv.org/pdf/2402.11190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11190]] Disclosure and Mitigation of Gender Bias in LLMs(https://arxiv.org/abs/2402.11190)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explicit genders or stereotypes.</li>
</ul>

<h3>Title: Knowledge Graph Assisted Automatic Sports News Writing</h3>
<ul>
<li><strong>Authors: </strong>Yang Cao, Xinyi Chen, Xin Zhang, Siying Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11191">https://arxiv.org/abs/2402.11191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11191">https://arxiv.org/pdf/2402.11191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11191]] Knowledge Graph Assisted Automatic Sports News Writing(https://arxiv.org/abs/2402.11191)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel method for automatically generating sports news, which employs a unique algorithm that extracts pivotal moments from live text broadcasts and uses them to create an initial draft of the news. This draft is further refined by incorporating key details and background information from a specially designed sports knowledge graph. This graph contains 5,893 entities, which are classified into three distinct conceptual categories, interconnected through four relationship types, and characterized by 27 unique attributes. In addition, we create a multi-stage learning model by combining convolutional neural networks and a transformer encoder. This model expresses entity-task interactions using convolutional neural networks and enriches entity representations in the query set with the transformer encoder. It also includes a processor to compute matching scores for incomplete triples, addressing few-shot knowledge graph completion problem. The efficiency of this approach has been confirmed through both subjective and objective evaluations of 50 selected test cases, demonstrating its capability in revolutionizing the creation of sports news.</li>
</ul>

<h3>Title: I Learn Better If You Speak My Language: Enhancing Large Language Model  Fine-Tuning with Style-Aligned Response Adjustments</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ren, Biao Wu, Lingqiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11192">https://arxiv.org/abs/2402.11192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11192">https://arxiv.org/pdf/2402.11192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11192]] I Learn Better If You Speak My Language: Enhancing Large Language Model  Fine-Tuning with Style-Aligned Response Adjustments(https://arxiv.org/abs/2402.11192)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially maintains its original competencies and effectiveness.</li>
</ul>

<h3>Title: Privacy Impact Assessments in the Wild: A Scoping Review</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Horn Iwaya, Ala Sarah Alaqra, Marit Hansen, Simone Fischer-H√ºbner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11193">https://arxiv.org/abs/2402.11193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11193">https://arxiv.org/pdf/2402.11193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11193]] Privacy Impact Assessments in the Wild: A Scoping Review(https://arxiv.org/abs/2402.11193)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy Impact Assessments (PIAs) offer a systematic process for assessing the privacy impacts of a project or system. As a privacy engineering strategy, PIAs are heralded as one of the main approaches to privacy by design, supporting the early identification of threats and controls. However, there is still a shortage of empirical evidence on their uptake and proven effectiveness in practice. To better understand the current state of literature and research, this paper provides a comprehensive Scoping Review (ScR) on the topic of PIAs "in the wild", following the well-established Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines. As a result, this ScR includes 45 studies, providing an extensive synthesis of the existing body of knowledge, classifying types of research and publications, appraising the methodological quality of primary research, and summarising the positive and negative aspects of PIAs in practice, as reported by studies. This ScR also identifies significant research gaps (e.g., evidence gaps from contradictory results and methodological gaps from research design deficiencies), future research pathways, and implications for researchers, practitioners, and policymakers developing and evaluating PIA frameworks. As we conclude, there is still a significant need for more primary research on the topic, both qualitative and quantitative. A critical appraisal of qualitative studies (n=28) revealed deficiencies in the methodological quality, and only four quantitative studies were identified, suggesting that current primary research remains incipient. Nonetheless, PIAs can be regarded as a prominent sub-area in the broader field of Empirical Privacy Engineering, warranting further research toward more evidence-based practices.</li>
</ul>

<h3>Title: Assessing LLMs' Mathematical Reasoning in Financial Document Question  Answering</h3>
<ul>
<li><strong>Authors: </strong>Pragya Srivastava, Manuj Malik, Tanuja Ganu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11194">https://arxiv.org/abs/2402.11194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11194">https://arxiv.org/pdf/2402.11194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11194]] Assessing LLMs' Mathematical Reasoning in Financial Document Question  Answering(https://arxiv.org/abs/2402.11194)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding of LLMs abilities for such a task.</li>
</ul>

<h3>Title: Maintaining Adversarial Robustness in Continuous Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaolei Ru, Xiaowei Cao, Zijia Liu, Jack Murdoch Moore, Xin-Ya Zhang, Xia Zhu, Wenjia Wei, Gang Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11196">https://arxiv.org/abs/2402.11196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11196">https://arxiv.org/pdf/2402.11196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11196]] Maintaining Adversarial Robustness in Continuous Learning(https://arxiv.org/abs/2402.11196)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial attacks, outperforming the baselines formed by combining the existing defense strategies and continual learning methods.</li>
</ul>

<h3>Title: Achieving Linear Speedup in Asynchronous Federated Learning with  Heterogeneous Clients</h3>
<ul>
<li><strong>Authors: </strong>Xiaolu Wang, Zijian Li, Shi Jin, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11198">https://arxiv.org/abs/2402.11198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11198">https://arxiv.org/pdf/2402.11198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11198]] Achieving Linear Speedup in Asynchronous Federated Learning with  Heterogeneous Clients(https://arxiv.org/abs/2402.11198)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework called Delayed Federated Averaging (DeFedAvg). In DeFedAvg, the clients are allowed to perform local training with different stale global models at their own paces. Theoretical analyses demonstrate that DeFedAvg achieves asymptotic convergence rates that are on par with the results of FedAvg for solving nonconvex problems. More importantly, DeFedAvg is the first AFL algorithm that provably achieves the desirable linear speedup property, which indicates its high scalability. Additionally, we carry out extensive numerical experiments using real datasets to validate the efficiency and scalability of our approach when training deep neural networks.</li>
</ul>

<h3>Title: Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with  Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11199">https://arxiv.org/abs/2402.11199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11199">https://arxiv.org/pdf/2402.11199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11199]] Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with  Knowledge Graphs(https://arxiv.org/abs/2402.11199)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning.</li>
</ul>

<h3>Title: A Decoding Scheme with Successive Aggregation of Multi-Level Features  for Light-Weight Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiwon Yoo, Jangwon Lee, Gyeonghwan Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11201">https://arxiv.org/abs/2402.11201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11201">https://arxiv.org/pdf/2402.11201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11201]] A Decoding Scheme with Successive Aggregation of Multi-Level Features  for Light-Weight Semantic Segmentation(https://arxiv.org/abs/2402.11201)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-scale architecture, including hierarchical vision transformer, has been commonly applied to high-resolution semantic segmentation to deal with computational complexity with minimum performance loss. In this paper, we propose a novel decoding scheme for semantic segmentation in this regard, which takes multi-level features from the encoder with multi-scale architecture. The decoding scheme based on a multi-level vision transformer aims to achieve not only reduced computational expense but also higher segmentation accuracy, by introducing successive cross-attention in aggregation of the multi-level features. Furthermore, a way to enhance the multi-level features by the aggregated semantics is proposed. The effort is focused on maintaining the contextual consistency from the perspective of attention allocation and brings improved performance with significantly lower computational cost. Set of experiments on popular datasets demonstrates superiority of the proposed scheme to the state-of-the-art semantic segmentation models in terms of computational cost without loss of accuracy, and extensive ablation studies prove the effectiveness of ideas proposed.</li>
</ul>

<h3>Title: Hand Biometrics in Digital Forensics</h3>
<ul>
<li><strong>Authors: </strong>Asish Bera, Debotosh Bhattacharjee, Mita Nasipuri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11206">https://arxiv.org/abs/2402.11206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11206">https://arxiv.org/pdf/2402.11206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11206]] Hand Biometrics in Digital Forensics(https://arxiv.org/abs/2402.11206)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, extraction</a></li>
<li><strong>Abstract: </strong>Digital forensic is now an unavoidable part for securing the digital world from identity theft. Higher order of crimes, dealing with a massive database is really very challenging problem for any intelligent system. Biometric is a better solution to win over the problems encountered by digital forensics. Many biometric characteristics are playing their significant roles in forensics over the decades. The potential benefits and scope of hand based modes in forensics have been investigated with an illustration of hand geometry verifi-cation method. It can be applied when effective biometric evidences are properly unavailable; gloves are damaged, and dirt or any kind of liquid can minimize the accessibility and reliability of the fingerprint or palmprint. Due to the crisis of pure uniqueness of hand features for a very large database, it may be relevant for verification only. Some unimodal and multimodal hand based biometrics (e.g. hand geometry, palmprint and hand vein) with several feature extractions, database and verification methods have been discussed with 2D, 3D and infrared images.</li>
</ul>

<h3>Title: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based  Agents</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11208">https://arxiv.org/abs/2402.11208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11208">https://arxiv.org/pdf/2402.11208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11208]] Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based  Agents(https://arxiv.org/abs/2402.11208)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.</li>
</ul>

<h3>Title: Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, Michael R. Lyu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11217">https://arxiv.org/abs/2402.11217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11217">https://arxiv.org/pdf/2402.11217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11217]] Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large  Language Models(https://arxiv.org/abs/2402.11217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, etc.). Grounded in 3 proposed core principles, Asclepius ensures a comprehensive evaluation by encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks, and exempting from train-validate contamination. We further provide an in-depth analysis of 6 Med-MLLMs and compare them with 5 human specialists, providing insights into their competencies and limitations in various medical contexts. Our work not only advances the understanding of Med-MLLMs' capabilities but also sets a precedent for future evaluations and the safe deployment of these models in clinical environments. We launch and maintain a leaderboard for community assessment of Med-MLLM capabilities (https://asclepius-med.github.io/).</li>
</ul>

<h3>Title: Controlled Text Generation for Large Language Model with Dynamic  Attribute Graphs</h3>
<ul>
<li><strong>Authors: </strong>Xun Liang, Hanyu Wang, Shichao Song, Mengting Hu, Xunzhi Wang, Zhiyu Li, Feiyu Xiong, Bo Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11218">https://arxiv.org/abs/2402.11218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11218">https://arxiv.org/pdf/2402.11218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11218]] Controlled Text Generation for Large Language Model with Dynamic  Attribute Graphs(https://arxiv.org/abs/2402.11218)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency.</li>
</ul>

<h3>Title: Neural Networks with (Low-Precision) Polynomial Approximations: New  Insights and Techniques for Accuracy Improvement</h3>
<ul>
<li><strong>Authors: </strong>Chi Zhang, Man Ho Au, Siu Ming Yiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11224">https://arxiv.org/abs/2402.11224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11224">https://arxiv.org/pdf/2402.11224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11224]] Neural Networks with (Low-Precision) Polynomial Approximations: New  Insights and Techniques for Accuracy Improvement(https://arxiv.org/abs/2402.11224)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type of perturbations; and (2) weight regularisation significantly reduces PANN's accuracy. We support our explanation with experiments. Secondly, based on the insights from our investigations, we propose solutions to increase inference accuracy for PANN. Experiments showed that combination of our solutions is very effective: at the same precision, our PANN is 10% to 50% more accurate than state-of-the-arts; and at the same accuracy, our PANN only requires a precision of $2^{-9}$ while state-of-the-art solution requires a precision of $2^{-12}$ using the ResNet-20 model on CIFAR-10 dataset.</li>
</ul>

<h3>Title: On the Role of Similarity in Detecting Masquerading Files</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Oliver, Jue Mo, Susmit Yenkar, Raghav Batta, Sekhar Josyoula</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11227">https://arxiv.org/abs/2402.11227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11227">https://arxiv.org/pdf/2402.11227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11227]] On the Role of Similarity in Detecting Masquerading Files(https://arxiv.org/abs/2402.11227)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Similarity has been applied to a wide range of security applications, typically used in machine learning models. We examine the problem posed by masquerading samples; that is samples crafted by bad actors to be similar or near identical to legitimate samples. We find that these samples potentially create significant problems for machine learning solutions. The primary problem being that bad actors can circumvent machine learning solutions by using masquerading samples. We then examine the interplay between digital signatures and machine learning solutions. In particular, we focus on executable files and code signing. We offer a taxonomy for masquerading files. We use a combination of similarity and clustering to find masquerading files. We use the insights gathered in this process to offer improvements to similarity based and machine learning security solutions.</li>
</ul>

<h3>Title: Enhancing Security in Blockchain Networks: Anomalies, Frauds, and  Advanced Detection Techniques</h3>
<ul>
<li><strong>Authors: </strong>Joerg Osterrieder, Stephen Chan, Jeffrey Chu, Yuanyuan Zhang, Branka Hadji Misheva, Codruta Mare</a></li>
<li><strong>Subjects: </strong>cs.CR, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11231">https://arxiv.org/abs/2402.11231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11231">https://arxiv.org/pdf/2402.11231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11231]] Enhancing Security in Blockchain Networks: Anomalies, Frauds, and  Advanced Detection Techniques(https://arxiv.org/abs/2402.11231)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Blockchain technology, a foundational distributed ledger system, enables secure and transparent multi-party transactions. Despite its advantages, blockchain networks are susceptible to anomalies and frauds, posing significant risks to their integrity and security. This paper offers a detailed examination of blockchain's key definitions and properties, alongside a thorough analysis of the various anomalies and frauds that undermine these networks. It describes an array of detection and prevention strategies, encompassing statistical and machine learning methods, game-theoretic solutions, digital forensics, reputation-based systems, and comprehensive risk assessment techniques. Through case studies, we explore practical applications of anomaly and fraud detection in blockchain networks, extracting valuable insights and implications for both current practice and future research. Moreover, we spotlight emerging trends and challenges within the field, proposing directions for future investigation and technological development. Aimed at both practitioners and researchers, this paper seeks to provide a technical, in-depth overview of anomaly and fraud detection within blockchain networks, marking a significant step forward in the search for enhanced network security and reliability.</li>
</ul>

<h3>Title: ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11235">https://arxiv.org/abs/2402.11235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11235">https://arxiv.org/pdf/2402.11235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11235]] ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs(https://arxiv.org/abs/2402.11235)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Especially, ZeroG, as a zero-shot method, can even achieve results comparable to those of semi-supervised learning on Pubmed.</li>
</ul>

<h3>Title: DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT  Based Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yu Feng, Xing Shi, Mengli Cheng, Yun Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11241">https://arxiv.org/abs/2402.11241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11241">https://arxiv.org/pdf/2402.11241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11241]] DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT  Based Diffusion Model(https://arxiv.org/abs/2402.11241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>As the task of 2D-to-3D reconstruction has gained significant attention in various real-world scenarios, it becomes crucial to be able to generate high-quality point clouds. Despite the recent success of deep learning models in generating point clouds, there are still challenges in producing high-fidelity results due to the disparities between images and point clouds. While vision transformers (ViT) and diffusion models have shown promise in various vision tasks, their benefits for reconstructing point clouds from images have not been demonstrated yet. In this paper, we first propose a neat and powerful architecture called DiffPoint that combines ViT and diffusion models for the task of point cloud reconstruction. At each diffusion step, we divide the noisy point clouds into irregular patches. Then, using a standard ViT backbone that treats all inputs as tokens (including time information, image embeddings, and noisy patches), we train our model to predict target points based on input images. We evaluate DiffPoint on both single-view and multi-view reconstruction tasks and achieve state-of-the-art results. Additionally, we introduce a unified and flexible feature fusion module for aggregating image features from single or multiple input images. Furthermore, our work demonstrates the feasibility of applying unified architectures across languages and images to improve 3D reconstruction tasks.</li>
</ul>

<h3>Title: Can Large Language Models perform Relation-based Argument Mining?</h3>
<ul>
<li><strong>Authors: </strong>Deniz Gorur, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11243">https://arxiv.org/abs/2402.11243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11243">https://arxiv.org/pdf/2402.11243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11243]] Can Large Language Models perform Relation-based Argument Mining?(https://arxiv.org/abs/2402.11243)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.</li>
</ul>

<h3>Title: CoLLaVO: Crayon Large Language and Vision mOdel</h3>
<ul>
<li><strong>Authors: </strong>Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11248">https://arxiv.org/abs/2402.11248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11248">https://arxiv.org/pdf/2402.11248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11248]] CoLLaVO: Crayon Large Language and Vision mOdel(https://arxiv.org/abs/2402.11248)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from 'what objects are in the image?' or 'which object corresponds to a specified bounding box?'. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their zero-shot performance on Vision Language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose Crayon Large Language and Vision mOdel (CoLLaVO), which incorporates instruction tuning with crayon prompt as a new visual prompt tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of Dual QLoRA to preserve object-level image understanding without forgetting it during visual instruction tuning, thereby achieving a significant leap in zero-shot numerous VL benchmarks.</li>
</ul>

<h3>Title: LLM can Achieve Self-Regulation via Hyperparameter Aware Generation</h3>
<ul>
<li><strong>Authors: </strong>Siyin Wang, Shimin Li, Tianxiang Sun, Jinlan Fu, Qinyuan Cheng, Jiasheng Ye, Junjie Ye, Xipeng Qiu, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11251">https://arxiv.org/abs/2402.11251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11251">https://arxiv.org/pdf/2402.11251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11251]] LLM can Achieve Self-Regulation via Hyperparameter Aware Generation(https://arxiv.org/abs/2402.11251)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, offering a more autonomous, self-regulate model behavior. Experimental results spanning six datasets across reasoning, creativity, translation, and mathematics tasks demonstrate that hyperparameter-aware instruction tuning empowers the LLMs to self-regulate the decoding strategy and hyperparameter. HAG extends the current paradigm in the text generation process, highlighting the feasibility of endowing the LLMs with self-regulate decoding strategies.</li>
</ul>

<h3>Title: Aligning Large Language Models by On-Policy Self-Judgment</h3>
<ul>
<li><strong>Authors: </strong>Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11253">https://arxiv.org/abs/2402.11253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11253">https://arxiv.org/pdf/2402.11253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11253]] Aligning Large Language Models by On-Policy Self-Judgment(https://arxiv.org/abs/2402.11253)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that self-rejection with oversampling can improve further without an additional evaluator. Our code is available at https://github.com/oddqueue/self-judge.</li>
</ul>

<h3>Title: C-ICL: Contrastive In-context Learning for Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Ying Mo, Jian Yang, Jiahao Liu, Shun Zhang, Jingang Wang, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11254">https://arxiv.org/abs/2402.11254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11254">https://arxiv.org/pdf/2402.11254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11254]] C-ICL: Contrastive In-context Learning for Information Extraction(https://arxiv.org/abs/2402.11254)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recently, there has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identification and correction of potential interface errors. Specifically, our proposed method taps into the inherent contextual information and valuable information in hard negative samples and the nearest positive neighbors to the test and then applies the in-context learning demonstrations based on LLMs. Our experiments on various datasets indicate that c-ICL outperforms previous few-shot in-context learning methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. These improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.</li>
</ul>

<h3>Title: MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11260">https://arxiv.org/abs/2402.11260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11260">https://arxiv.org/pdf/2402.11260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11260]] MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning(https://arxiv.org/abs/2402.11260)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 30.15% improvement in "RA" for Phi-2-2.7B compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.</li>
</ul>

<h3>Title: Human-AI Interactions in the Communication Era: Autophagy Makes Large  Models Achieving Local Optima</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Lijie Hu, Lu Yu, Muhammad Asif Ali, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11271">https://arxiv.org/abs/2402.11271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11271">https://arxiv.org/pdf/2402.11271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11271]] Human-AI Interactions in the Communication Era: Autophagy Makes Large  Models Achieving Local Optima(https://arxiv.org/abs/2402.11271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of autophagic ("self-consumption") loops to account for the suppression of human-generated information in the exchange of information between humans and AI systems. We generalize the declining diversity of social information and the bottleneck in model performance caused by the above trends to the local optima of large models.</li>
</ul>

<h3>Title: Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo  Labeling Leveraging Strong and Weak Data Augmentation Strategies</h3>
<ul>
<li><strong>Authors: </strong>Yifei Chen, Chenyan Zhang, Yifan Ke, Yiyu Huang, Xuezhou Dai, Feiwei Qin, Yongquan Zhang, Xiaodong Zhang, Changmiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11273">https://arxiv.org/abs/2402.11273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11273">https://arxiv.org/pdf/2402.11273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11273]] Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo  Labeling Leveraging Strong and Weak Data Augmentation Strategies(https://arxiv.org/abs/2402.11273)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Traditional supervised learning methods have historically encountered certain constraints in medical image segmentation due to the challenging collection process, high labeling cost, low signal-to-noise ratio, and complex features characterizing biomedical images. This paper proposes a semi-supervised model, DFCPS, which innovatively incorporates the Fixmatch concept. This significantly enhances the model's performance and generalizability through data augmentation processing, employing varied strategies for unlabeled data. Concurrently, the model design gives appropriate emphasis to the generation, filtration, and refinement processes of pseudo-labels. The novel concept of cross-pseudo-supervision is introduced, integrating consistency learning with self-training. This enables the model to fully leverage pseudo-labels from multiple perspectives, thereby enhancing training diversity. The DFCPS model is compared with both baseline and advanced models using the publicly accessible Kvasir-SEG dataset. Across all four subdivisions containing different proportions of unlabeled data, our model consistently exhibits superior performance. Our source code is available at https://github.com/JustlfC03/DFCPS.</li>
</ul>

<h3>Title: Multi-Perspective Consistency Enhances Confidence Estimation in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11279">https://arxiv.org/abs/2402.11279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11279">https://arxiv.org/pdf/2402.11279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11279]] Multi-Perspective Consistency Enhances Confidence Estimation in Large  Language Models(https://arxiv.org/abs/2402.11279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.</li>
</ul>

<h3>Title: Puzzle Solving using Reasoning of Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11291">https://arxiv.org/abs/2402.11291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11291">https://arxiv.org/pdf/2402.11291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11291]] Puzzle Solving using Reasoning of Large Language Models: A Survey(https://arxiv.org/abs/2402.11291)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and creative problem-solving advancements.</li>
</ul>

<h3>Title: OneBit: Towards Extremely Low-bit Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11295">https://arxiv.org/abs/2402.11295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11295">https://arxiv.org/pdf/2402.11295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11295]] OneBit: Towards Extremely Low-bit Large Language Models(https://arxiv.org/abs/2402.11295)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non-quantized performance) with robust training processes when only using 1-bit weight matrices.</li>
</ul>

<h3>Title: Dissecting Human and LLM Preferences</h3>
<ul>
<li><strong>Authors: </strong>Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11296">https://arxiv.org/abs/2402.11296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11296">https://arxiv.org/pdf/2402.11296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11296]] Dissecting Human and LLM Preferences(https://arxiv.org/abs/2402.11296)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. Interactive Demo: https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset: https://huggingface.co/datasets/GAIR/preference-dissection Code: https://github.com/GAIR-NLP/Preference-Dissection</li>
</ul>

<h3>Title: MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal</h3>
<ul>
<li><strong>Authors: </strong>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11297">https://arxiv.org/abs/2402.11297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11297">https://arxiv.org/pdf/2402.11297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11297]] MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal(https://arxiv.org/abs/2402.11297)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Our contribution introduces a groundbreaking multimodal large language model designed to comprehend multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. Leveraging state-of-the-art models, we utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio inputs. Notably, this multimodal large language model is bilingual, proficient in understanding both English and Malay simultaneously. We proudly unveil two versions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B parameters. With its ability to navigate diverse modalities and languages, our model represents a significant advancement for the Malaysian context and beyond. All models released at https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859</li>
</ul>

<h3>Title: ReViT: Enhancing Vision Transformers with Attention Residual Connections  for Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Anxhelo Diko, Danilo Avola, Marco Cascio, Luigi Cinque</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11301">https://arxiv.org/abs/2402.11301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11301">https://arxiv.org/pdf/2402.11301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11301]] ReViT: Enhancing Vision Transformers with Attention Residual Connections  for Visual Recognition(https://arxiv.org/abs/2402.11301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for object detection and instance segmentation when implemented into spatial-aware transformer models.</li>
</ul>

<h3>Title: FViT: A Focal Vision Transformer with Gabor Filter</h3>
<ul>
<li><strong>Authors: </strong>Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, Zengqiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11303">https://arxiv.org/abs/2402.11303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11303">https://arxiv.org/pdf/2402.11303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11303]] FViT: A Focal Vision Transformer with Gabor Filter(https://arxiv.org/abs/2402.11303)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision transformers have achieved encouraging progress in various computer vision tasks. A common belief is that this is attributed to the competence of self-attention in modeling the global dependencies among feature tokens. Unfortunately, self-attention still faces some challenges in dense prediction tasks, such as the high computational complexity and absence of desirable inductive bias. To address these above issues, we revisit the potential benefits of integrating vision transformer with Gabor filter, and propose a Learnable Gabor Filter (LGF) by using convolution. As an alternative to self-attention, we employ LGF to simulate the response of simple cells in the biological visual system to input images, prompting models to focus on discriminative feature representations of targets from various scales and orientations. Additionally, we designed a Bionic Focal Vision (BFV) block based on the LGF. This block draws inspiration from neuroscience and introduces a Multi-Path Feed Forward Network (MPFFN) to emulate the working way of biological visual cortex processing information in parallel. Furthermore, we develop a unified and efficient pyramid backbone network family called Focal Vision Transformers (FViTs) by stacking BFV blocks. Experimental results show that FViTs exhibit highly competitive performance in various vision tasks. Especially in terms of computational efficiency and scalability, FViTs show significantly advantages compared with other counterparts.</li>
</ul>

<h3>Title: On Good Practices for Task-Specific Distillation of Large Pretrained  Models</h3>
<ul>
<li><strong>Authors: </strong>Juliette Marrie, Michael Arbel, Julien Mairal, Diane Larlus</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11305">https://arxiv.org/abs/2402.11305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11305">https://arxiv.org/pdf/2402.11305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11305]] On Good Practices for Task-Specific Distillation of Large Pretrained  Models(https://arxiv.org/abs/2402.11305)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Large pretrained visual models exhibit remarkable generalization across diverse recognition tasks. Yet, real-world applications often demand compact models tailored to specific problems. Variants of knowledge distillation have been devised for such a purpose, enabling task-specific compact models (the students) to learn from a generic large pretrained one (the teacher). In this paper, we show that the excellent robustness and versatility of recent pretrained models challenge common practices established in the literature, calling for a new set of optimal guidelines for task-specific distillation. To address the lack of samples in downstream tasks, we also show that a variant of Mixup based on stable diffusion complements standard data augmentation. This strategy eliminates the need for engineered text prompts and improves distillation of generic models into streamlined specialized networks.</li>
</ul>

<h3>Title: ICHPro: Intracerebral Hemorrhage Prognosis Classification Via  Joint-attention Fusion-based 3d Cross-modal Network</h3>
<ul>
<li><strong>Authors: </strong>Xinlei Yu, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Taosheng Xu, Xiang Wan, Changmiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11307">https://arxiv.org/abs/2402.11307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11307">https://arxiv.org/pdf/2402.11307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11307]] ICHPro: Intracerebral Hemorrhage Prognosis Classification Via  Joint-attention Fusion-based 3d Cross-modal Network(https://arxiv.org/abs/2402.11307)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Intracerebral Hemorrhage (ICH) is the deadliest subtype of stroke, necessitating timely and accurate prognostic evaluation to reduce mortality and disability. However, the multi-factorial nature and complexity of ICH make methods based solely on computed tomography (CT) image features inadequate. Despite the capacity of cross-modal networks to fuse additional information, the effective combination of different modal features remains a significant challenge. In this study, we propose a joint-attention fusion-based 3D cross-modal network termed ICHPro that simulates the ICH prognosis interpretation process utilized by neurosurgeons. ICHPro includes a joint-attention fusion module to fuse features from CT images with demographic and clinical textual data. To enhance the representation of cross-modal features, we introduce a joint loss function. ICHPro facilitates the extraction of richer cross-modal features, thereby improving classification performance. Upon testing our method using a five-fold cross-validation, we achieved an accuracy of 89.11%, an F1 score of 0.8767, and an AUC value of 0.9429. These results outperform those obtained from other advanced methods based on the test dataset, thereby demonstrating the superior efficacy of ICHPro. The code is available at our Github: https://github.com/YU-deep/ICH.</li>
</ul>

<h3>Title: EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries</h3>
<ul>
<li><strong>Authors: </strong>Jiateng Liu, Pengfei Yu, Yuji Zhang, Sha Li, Zixuan Zhang, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11324">https://arxiv.org/abs/2402.11324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11324">https://arxiv.org/pdf/2402.11324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11324]] EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries(https://arxiv.org/abs/2402.11324)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The dynamic nature of real-world information necessitates efficient knowledge editing (KE) in large language models (LLMs) for knowledge updating. However, current KE approaches, which typically operate on (subject, relation, object) triples, ignore the contextual information and the relation among different knowledge. Such editing methods could thus encounter an uncertain editing boundary, leaving a lot of relevant knowledge in ambiguity: Queries that could be answered pre-edit cannot be reliably answered afterward. In this work, we analyze this issue by introducing a theoretical framework for KE that highlights an overlooked set of knowledge that remains unchanged and aids in knowledge deduction during editing, which we name as the deduction anchor. We further address this issue by proposing a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor to address the issue of indeterminate editing boundaries. We empirically demonstrate the superiority of event-based editing over the existing setting on resolving uncertainty in edited models, and curate a new benchmark dataset EvEdit derived from the CounterFact dataset. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6% consistency improvement while maintaining the naturalness of generation.</li>
</ul>

<h3>Title: ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote  Sensing</h3>
<ul>
<li><strong>Authors: </strong>Zhenghang Yuan, Zhitong Xiong, Lichao Mou, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11325">https://arxiv.org/abs/2402.11325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11325">https://arxiv.org/pdf/2402.11325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11325]] ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote  Sensing(https://arxiv.org/abs/2402.11325)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>An in-depth comprehension of global land cover is essential in Earth observation, forming the foundation for a multitude of applications. Although remote sensing technology has advanced rapidly, leading to a proliferation of satellite imagery, the inherent complexity of these images often makes them difficult for non-expert users to understand. Natural language, as a carrier of human knowledge, can be a bridge between common users and complicated satellite imagery. In this context, we introduce a global-scale, high-quality image-text dataset for remote sensing, providing natural language descriptions for Sentinel-2 data to facilitate the understanding of satellite imagery for common users. Specifically, we utilize Sentinel-2 data for its global coverage as the foundational image source, employing semantic segmentation labels from the European Space Agency's (ESA) WorldCover project to enrich the descriptions of land covers. By conducting in-depth semantic analysis, we formulate detailed prompts to elicit rich descriptions from ChatGPT. To enhance the dataset's quality, we introduce the manual verification process. This step involves manual inspection and correction to refine the dataset, thus significantly improving its accuracy and quality. Finally, we offer the community ChatEarthNet, a large-scale image-text dataset characterized by global coverage, high quality, wide-ranging diversity, and detailed descriptions. ChatEarthNet consists of 163,488 image-text pairs with captions generated by ChatGPT-3.5 and an additional 10,000 image-text pairs with captions generated by ChatGPT-4V(ision). This dataset has significant potential for training vision-language foundation models and evaluating large vision-language models for remote sensing. The dataset will be made publicly available.</li>
</ul>

<h3>Title: Learning by Reconstruction Produces Uninformative Features For  Perception</h3>
<ul>
<li><strong>Authors: </strong>Randall Balestriero, Yann LeCun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11337">https://arxiv.org/abs/2402.11337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11337">https://arxiv.org/pdf/2402.11337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11337]] Learning by Reconstruction Produces Uninformative Features For  Perception(https://arxiv.org/abs/2402.11337)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.</li>
</ul>

<h3>Title: Fair Classification with Partial Feedback: An Exploration-Based  Data-Collection Approach</h3>
<ul>
<li><strong>Authors: </strong>Vijay Keswani, Anay Mehrotra, L. Elisa Celis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11338">https://arxiv.org/abs/2402.11338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11338">https://arxiv.org/pdf/2402.11338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11338]] Fair Classification with Partial Feedback: An Exploration-Based  Data-Collection Approach(https://arxiv.org/abs/2402.11338)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.</li>
</ul>

<h3>Title: Ransomware detection using stacked autoencoder for feature selection</h3>
<ul>
<li><strong>Authors: </strong>Mike Nkongolo, Mahmut Tokmak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11342">https://arxiv.org/abs/2402.11342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11342">https://arxiv.org/pdf/2402.11342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11342]] Ransomware detection using stacked autoencoder for feature selection(https://arxiv.org/abs/2402.11342)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The aim of this study is to propose and evaluate an advanced ransomware detection and classification method that combines a Stacked Autoencoder (SAE) for precise feature selection with a Long Short Term Memory (LSTM) classifier to enhance ransomware stratification accuracy. The proposed approach involves thorough pre processing of the UGRansome dataset and training an unsupervised SAE for optimal feature selection or fine tuning via supervised learning to elevate the LSTM model's classification capabilities. The study meticulously analyzes the autoencoder's learned weights and activations to identify essential features for distinguishing ransomware families from other malware and creates a streamlined feature set for precise classification. Extensive experiments, including up to 400 epochs and varying learning rates, are conducted to optimize the model's performance. The results demonstrate the outstanding performance of the SAE-LSTM model across all ransomware families, boasting high precision, recall, and F1 score values that underscore its robust classification capabilities. Furthermore, balanced average scores affirm the proposed model's ability to generalize effectively across various malware types. The proposed model achieves an exceptional 99% accuracy in ransomware classification, surpassing the Extreme Gradient Boosting (XGBoost) algorithm primarily due to its effective SAE feature selection mechanism. The model also demonstrates outstanding performance in identifying signature attacks, achieving a 98% accuracy rate.</li>
</ul>

<h3>Title: PhaseEvo: Towards Unified In-Context Prompt Optimization for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley Malin, Sricharan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11347">https://arxiv.org/abs/2402.11347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11347">https://arxiv.org/pdf/2402.11347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11347]] PhaseEvo: Towards Unified In-Context Prompt Optimization for Large  Language Models(https://arxiv.org/abs/2402.11347)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Crafting an ideal prompt for Large Language Models (LLMs) is a challenging task that demands significant resources and expert human input. Existing work treats the optimization of prompt instruction and in-context learning examples as distinct problems, leading to sub-optimal prompt performance. This research addresses this limitation by establishing a unified in-context prompt optimization framework, which aims to achieve joint optimization of the prompt instruction and examples. However, formulating such optimization in the discrete and high-dimensional natural language space introduces challenges in terms of convergence and computational efficiency. To overcome these issues, we present PhaseEvo, an efficient automatic prompt optimization framework that combines the generative capability of LLMs with the global search proficiency of evolution algorithms. Our framework features a multi-phase design incorporating innovative LLM-based mutation operators to enhance search efficiency and accelerate convergence. We conduct an extensive evaluation of our approach across 35 benchmark tasks. The results demonstrate that PhaseEvo significantly outperforms the state-of-the-art baseline methods by a large margin whilst maintaining good efficiency.</li>
</ul>

<h3>Title: Tasks That Language Models Don't Learn</h3>
<ul>
<li><strong>Authors: </strong>Bruce W. Lee, JaeHyuk Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11349">https://arxiv.org/abs/2402.11349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11349">https://arxiv.org/pdf/2402.11349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11349]] Tasks That Language Models Don't Learn(https://arxiv.org/abs/2402.11349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of knowledge acquired in the absence of sensory experience.</li>
</ul>

<h3>Title: Data-Driven Stochastic AC-OPF using Gaussian Processes</h3>
<ul>
<li><strong>Authors: </strong>Mile Mitrovic</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11365">https://arxiv.org/abs/2402.11365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11365">https://arxiv.org/pdf/2402.11365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11365]] Data-Driven Stochastic AC-OPF using Gaussian Processes(https://arxiv.org/abs/2402.11365)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases. To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning Gaussian process regression (GPR) model. The full Gaussian process (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs. The proposed approach uses various approximations for GP-uncertainty propagation. The full GP CC-OPF approach exhibits highly competitive and promising results, outperforming the state-of-the-art sample-based chance constraint approaches. To further improve the robustness and complexity/accuracy trade-off of the full GP CC-OPF, a fast data-driven setup is proposed. This setup relies on the sparse and hybrid Gaussian processes (GP) framework to model the power flow equations with input uncertainty.</li>
</ul>

<h3>Title: Secure, Robust, and Energy-Efficient Authenticated Data Sharing in  UAV-Assisted 6G Networks</h3>
<ul>
<li><strong>Authors: </strong>Atefeh Mohseni Ejiyeh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11382">https://arxiv.org/abs/2402.11382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11382">https://arxiv.org/pdf/2402.11382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11382]] Secure, Robust, and Energy-Efficient Authenticated Data Sharing in  UAV-Assisted 6G Networks(https://arxiv.org/abs/2402.11382)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>This paper confronts the pressing challenges of sixth-generation (6G) wireless communication networks by harnessing the unique capabilities of Unmanned Aerial Vehicles (UAVs). With the ambitious promises of 6G, including ultra-reliable 1 Tbps data delivery and ultra-low latency, the demand for innovative solutions becomes imperative. Traditional terrestrial base stations, though effective, exhibit limitations in scenarios requiring ubiquitous connectivity, prompting the integration of UAVs. In response to these challenges, we introduce a comprehensive solution. This involves UAVs collaboratively downloading desired content from service providers, and subsequently establishing secure connections with users for efficient content exchange. Accordingly, we introduce two new protocols: a collaborative group data downloading scheme among UAVs called SeGDS, and SeDDS for secure direct data sharing through out-of-band autonomous Device-to-Device (D2D) communication. Leveraging certificateless signcryption and certificateless multi-receiver encryption, these protocols offer lightweight, certificate-free solutions with features such as user revocation, non-repudiation, and mutual authentication. Prioritizing high availability, the proposed protocols effectively detect Denial of Service (DoS) and free riding attacks. A thorough evaluation underscores the superiority of the proposed protocols in both security and efficiency over existing models; SeDDS reduces overall computation by 3x, imposing a lighter communication load on UAVs, while SeGDS meets swarm UAV security requirements, reducing communication costs by 4x with low computation cost.</li>
</ul>

<h3>Title: k-SemStamp: A Clustering-Based Semantic Watermark for Detection of  Machine-Generated Text</h3>
<ul>
<li><strong>Authors: </strong>Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, Tianxing He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11399">https://arxiv.org/abs/2402.11399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11399">https://arxiv.org/pdf/2402.11399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11399]] k-SemStamp: A Clustering-Based Semantic Watermark for Detection of  Machine-Generated Text(https://arxiv.org/abs/2402.11399)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.</li>
</ul>

<h3>Title: Evaluating the Stability of Deep Learning Latent Feature Spaces</h3>
<ul>
<li><strong>Authors: </strong>Ademide O. Mabadeje, Michael J. Pyrcz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11404">https://arxiv.org/abs/2402.11404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11404">https://arxiv.org/pdf/2402.11404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11404]] Evaluating the Stability of Deep Learning Latent Feature Spaces(https://arxiv.org/abs/2402.11404)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to distill essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked. Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow across 500 autoencoder realizations and three datasets, encompassing both synthetic and real-world scenarios to explain latent space dynamics. Employing k-means clustering and the modified Jonker-Volgenant algorithm for class alignment, alongside anisotropy metrics and convex hull analysis, we introduce adjusted stress and Jaccard dissimilarity as novel stability indicators. Our findings highlight inherent instabilities in latent feature spaces and demonstrate the workflow's efficacy in quantifying and interpreting these instabilities. This work advances the understanding of latent feature spaces, promoting improved model interpretability and quality control for more informed decision-making for diverse analytical workflows that leverage deep learning.</li>
</ul>

<h3>Title: Don't Go To Extremes: Revealing the Excessive Sensitivity and  Calibration Limitations of LLMs in Implicit Hate Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Min Zhang, Jianfeng He, Taoran Ji, Chang-Tien Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11406">https://arxiv.org/abs/2402.11406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11406">https://arxiv.org/pdf/2402.11406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11406]] Don't Go To Extremes: Revealing the Excessive Sensitivity and  Calibration Limitations of LLMs in Implicit Hate Speech Detection(https://arxiv.org/abs/2402.11406)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness.</li>
</ul>

<h3>Title: Multi-dimensional Evaluation of Empathetic Dialog Responses</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Xu, Jiepu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11409">https://arxiv.org/abs/2402.11409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11409">https://arxiv.org/pdf/2402.11409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11409]] Multi-dimensional Evaluation of Empathetic Dialog Responses(https://arxiv.org/abs/2402.11409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Empathy is a critical element of effective and satisfactory conversational communication, yet previous studies in measuring conversational empathy mostly focus on expressed communicative intents -- in which way empathy is expressed, ignoring the fact that conversation is also a collaborative practice involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework that extends upon existing work to measure both expressed intents from the speaker's perspective and perceived empathy from the listener's perspective. Applying the proposed framework to analyzing our internal customer-service dialogue shows that the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlation with the satisfactory level of dialogue sessions. This proposed framework still requires subjective assessments from trained annotators, which can be non-trivial to collect. To scale up evaluation without excessive reliance on carefully annotated data, we explore different modeling options to automatically measure conversational empathy with (1) prompting frozen large language models (LLMs) and (2) training language model-based classifiers. Extensive experiments on both internal and external dialogue datasets show that measuring conversational empathy remains a challenging task for prompting frozen LLMs, reflected by less satisfying performance of GPT-4 and Flan family models. On the other hand, our proposed instruction-finetuned classifiers based on sequence-to-sequence (Seq2Seq) language models is able to achieve the best performance compared to prior works and competitive baselines. Finally, we perform comprehensive ablation studies on the performance of proposed instruction-finetuned classifiers and give recommendations on potentially adopting them as automatic conversational empathy evaluation metrics.</li>
</ul>

<h3>Title: Aligning Modalities in Vision Large Language Models via Preference  Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11411">https://arxiv.org/abs/2402.11411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11411">https://arxiv.org/pdf/2402.11411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11411]] Aligning Modalities in Vision Large Language Models via Preference  Fine-tuning(https://arxiv.org/abs/2402.11411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. Our data and code are available at https://github.com/YiyangZhou/POVID.</li>
</ul>

<h3>Title: A Multispectral Automated Transfer Technique (MATT) for machine-driven  image labeling utilizing the Segment Anything Model (SAM)</h3>
<ul>
<li><strong>Authors: </strong>James E. Gallagher, Aryav Gogia, Edward J. Oughton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11413">https://arxiv.org/abs/2402.11413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11413">https://arxiv.org/pdf/2402.11413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11413]] A Multispectral Automated Transfer Technique (MATT) for machine-driven  image labeling utilizing the Segment Anything Model (SAM)(https://arxiv.org/abs/2402.11413)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Model (SAM) is drastically accelerating the speed and accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB) imagery datasets. However, SAM is unable to segment and label images outside of the visible light spectrum, for example, for multispectral or hyperspectral imagery. Therefore, this paper outlines a method we call the Multispectral Automated Transfer Technique (MATT). By transposing SAM segmentation masks from RGB images we can automatically segment and label multispectral imagery with high precision and efficiency. For example, the results demonstrate that segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time reduction of 87.8% in developing a trained model, reducing roughly 20 hours of manual labeling, to only 2.4 hours. This efficiency gain is associated with only a 6.7% decrease in overall mean average precision (mAP) when training multispectral models via MATT, compared to a manually labeled dataset. We consider this an acceptable level of precision loss when considering the time saved during training, especially for rapidly prototyping experimental modeling methods. This research greatly contributes to the study of multispectral object detection by providing a novel and open-source method to rapidly segment, label, and train multispectral object detection models with minimal human interaction. Future research needs to focus on applying these methods to (i) space-based multispectral, and (ii) drone-based hyperspectral imagery.</li>
</ul>

<h3>Title: LoRETTA: Low-Rank Economic Tensor-Train Adaptation for  Ultra-Low-Parameter Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Yang, Jiajun Zhou, Ngai Wong, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11417">https://arxiv.org/abs/2402.11417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11417">https://arxiv.org/pdf/2402.11417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11417]] LoRETTA: Low-Rank Economic Tensor-Train Adaptation for  Ultra-Low-Parameter Fine-Tuning of Large Language Models(https://arxiv.org/abs/2402.11417)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical results demonstrate that the proposed method effectively improves training efficiency, enjoys better multi-task learning performance, and enhances the anti-overfitting capability. Plug-and-play codes built upon the Huggingface framework and PEFT library will be released.</li>
</ul>

<h3>Title: Rethinking the Roles of Large Language Models in Chinese Grammatical  Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Yinghui Li, Shang Qin, Jingheng Ye, Shirong Ma, Yangning Li, Libo Qin, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11420">https://arxiv.org/abs/2402.11420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11420">https://arxiv.org/pdf/2402.11420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11420]] Rethinking the Roles of Large Language Models in Chinese Grammatical  Error Correction(https://arxiv.org/abs/2402.11420)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences. Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by the subjectivity of the CGEC task. In particular, our work is also an active exploration of how LLMs and small models better collaborate in downstream tasks. Extensive experiments and detailed analyses on widely used datasets verify the effectiveness of our thinking intuition and the proposed methods.</li>
</ul>

<h3>Title: VoltSchemer: Use Voltage Noise to Manipulate Your Wireless Charger</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhan, Yirui Yang, Haoqi Shan, Hanqiu Wang, Yier Jin, Shuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11423">https://arxiv.org/abs/2402.11423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11423">https://arxiv.org/pdf/2402.11423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11423]] VoltSchemer: Use Voltage Noise to Manipulate Your Wireless Charger(https://arxiv.org/abs/2402.11423)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Wireless charging is becoming an increasingly popular charging solution in portable electronic products for a more convenient and safer charging experience than conventional wired charging. However, our research identified new vulnerabilities in wireless charging systems, making them susceptible to intentional electromagnetic interference. These vulnerabilities facilitate a set of novel attack vectors, enabling adversaries to manipulate the charger and perform a series of attacks. In this paper, we propose VoltSchemer, a set of innovative attacks that grant attackers control over commercial-off-the-shelf wireless chargers merely by modulating the voltage from the power supply. These attacks represent the first of its kind, exploiting voltage noises from the power supply to manipulate wireless chargers without necessitating any malicious modifications to the chargers themselves. The significant threats imposed by VoltSchemer are substantiated by three practical attacks, where a charger can be manipulated to: control voice assistants via inaudible voice commands, damage devices being charged through overcharging or overheating, and bypass Qi-standard specified foreign-object-detection mechanism to damage valuable items exposed to intense magnetic fields. We demonstrate the effectiveness and practicality of the VoltSchemer attacks with successful attacks on 9 top-selling COTS wireless chargers. Furthermore, we discuss the security implications of our findings and suggest possible countermeasures to mitigate potential threats.</li>
</ul>

<h3>Title: Data Distribution Distilled Generative Model for Generalized Zero-Shot  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yijie Wang, Mingjian Hong, Luwen Huangfu, Sheng Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11424">https://arxiv.org/abs/2402.11424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11424">https://arxiv.org/pdf/2402.11424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11424]] Data Distribution Distilled Generative Model for Generalized Zero-Shot  Recognition(https://arxiv.org/abs/2402.11424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showcase that D$^3$GZSL elevates the performance of existing generative GZSL methods, underscoring its potential to refine zero-shot learning practices.The code is available at: https://github.com/PJBQ/D3GZSL.git</li>
</ul>

<h3>Title: EventRL: Enhancing Event Extraction with Outcome Supervision for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jun Gao, Huan Zhao, Wei Wang, Changlong Yu, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11430">https://arxiv.org/abs/2402.11430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11430">https://arxiv.org/pdf/2402.11430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11430]] EventRL: Enhancing Event Extraction with Outcome Supervision for Large  Language Models(https://arxiv.org/abs/2402.11430)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.</li>
</ul>

<h3>Title: A Robust Error-Resistant View Selection Method for 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Shaojie Zhang, Yinghui Wang, Bin Nan, Jinlong Yang, Tao Yan, Liangyi Huang, Mingfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11431">https://arxiv.org/abs/2402.11431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11431">https://arxiv.org/pdf/2402.11431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11431]] A Robust Error-Resistant View Selection Method for 3D Reconstruction(https://arxiv.org/abs/2402.11431)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To address the issue of increased triangulation uncertainty caused by selecting views with small camera baselines in Structure from Motion (SFM) view selection, this paper proposes a robust error-resistant view selection method. The method utilizes a triangulation-based computation to obtain an error-resistant model, which is then used to construct an error-resistant matrix. The sorting results of each row in the error-resistant matrix determine the candidate view set for each view. By traversing the candidate view sets of all views and completing the missing views based on the error-resistant matrix, the integrity of 3D reconstruction is ensured. Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are conducted in terms of average reprojection error and absolute trajectory error in the reconstruction results. The proposed method demonstrates an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on the TUM dataset and DTU dataset.</li>
</ul>

<h3>Title: Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark  for Deception Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kang Chen, Zheng Lian, Haiyang Sun, Bin Liu, Jianhua Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11432">https://arxiv.org/abs/2402.11432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11432">https://arxiv.org/pdf/2402.11432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11432]] Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark  for Deception Reasoning(https://arxiv.org/abs/2402.11432)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive parts. This dataset can also be used to evaluate the complex reasoning capability of current large language models and serve as a reasoning benchmark for further research.</li>
</ul>

<h3>Title: Improved Indoor Localization with Machine Learning Techniques for IoT  applications</h3>
<ul>
<li><strong>Authors: </strong>M.W.P. Maduranga</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11433">https://arxiv.org/abs/2402.11433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11433">https://arxiv.org/pdf/2402.11433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11433]] Improved Indoor Localization with Machine Learning Techniques for IoT  applications(https://arxiv.org/abs/2402.11433)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rise of the Internet of Things (IoT) and mobile internet applications has spurred interest in location-based services (LBS) for commercial, military, and social applications. While the global positioning system (GPS) dominates outdoor localization, its efficacy wanes indoors due to signal challenges. Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee, Bluetooth, UWB, selecting based on context. Received signal strength indicator (RSSI) technology, known for its accuracy and simplicity, is widely adopted. This study employs machine learning algorithms in three phases: supervised regressors, supervised classifiers, and ensemble methods for RSSI-based indoor localization. Additionally, it introduces a weighted least squares technique and pseudo-linear solution approach to address non-linear RSSI measurement equations by approximating them with linear equations. An experimental testbed, utilizing diverse wireless technologies and anchor nodes, is designed for data collection, employing IoT cloud architectures. Pre-processing involves investigating filters for data refinement before algorithm training. The study employs machine learning models like linear regression, polynomial regression, support vector regression, random forest regression, and decision tree regressor across various wireless technologies. These models estimate the geographical coordinates of a moving target node, and their performance is evaluated using metrics such as accuracy, root mean square errors, precision, recall, sensitivity, coefficient of determinant, and the f1-score. The experiment's outcomes provide insights into the effectiveness of different supervised machine learning techniques in terms of localization accuracy and robustness in indoor environments.</li>
</ul>

<h3>Title: Momentor: Advancing Video Large Language Model with Fine-Grained  Temporal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, Siliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11435">https://arxiv.org/abs/2402.11435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11435">https://arxiv.org/pdf/2402.11435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11435]] Momentor: Advancing Video Large Language Model with Fine-Grained  Temporal Reasoning(https://arxiv.org/abs/2402.11435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.</li>
</ul>

<h3>Title: Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11436">https://arxiv.org/abs/2402.11436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11436">https://arxiv.org/pdf/2402.11436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11436]] Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models(https://arxiv.org/abs/2402.11436)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.</li>
</ul>

<h3>Title: NestedSGX: Bootstrapping Trust to Enclaves within Confidential VMs</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Linke Song, Benshan Mei, Shuang Liu, Shijun Zhao, Shoumeng Yan, XiaoFeng Wang, Dan Meng, Rui Hou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11438">https://arxiv.org/abs/2402.11438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11438">https://arxiv.org/pdf/2402.11438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11438]] NestedSGX: Bootstrapping Trust to Enclaves within Confidential VMs(https://arxiv.org/abs/2402.11438)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Integrity is critical for maintaining system security, as it ensures that only genuine software is loaded onto a machine. Although confidential virtual machines (CVMs) function within isolated environments separate from the host, it is important to recognize that users still encounter challenges in maintaining control over the integrity of the code running within the trusted execution environments (TEEs). The presence of a sophisticated operating system (OS) raises the possibility of dynamically creating and executing any code, making user applications within TEEs vulnerable to interference or tampering if the guest OS is compromised. This paper introduces NestedSGX, which leverages virtual machine privilege level (VMPL), a recent hardware feature available on AMD SEV-SNP to enable the creation of hardware enclaves within the guest VM. Similar to Intel SGX, NestedSGX considers the guest OS untrusted for loading potentially malicious code. It ensures that only trusted and measured code executed within the enclave can be remotely attested. To seamlessly protect existing applications, NestedSGX aims for compatibility with Intel SGX by simulating SGX leaf functions. We have also ported the SGX SDK to NestedSGX, enabling the use of existing SGX toolchains and applications in the system. Performance evaluations show that context switches in NestedSGX take about 35,000-37,000 cycles, approximately 2-3 times that of Intel SGX. NestedSGX incurs minimal overhead in most real-world applications, with an average overhead below 5% for most workloads and 22.7% for I/O intensive workloads.</li>
</ul>

<h3>Title: InfuserKI: Enhancing Large Language Models with Knowledge Graphs via  Infuser-Guided Knowledge Integration</h3>
<ul>
<li><strong>Authors: </strong>Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11441">https://arxiv.org/abs/2402.11441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11441">https://arxiv.org/pdf/2402.11441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11441]] InfuserKI: Enhancing Large Language Models with Knowledge Graphs via  Infuser-Guided Knowledge Integration(https://arxiv.org/abs/2402.11441)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs demonstrate that InfuserKI can effectively acquire new knowledge and outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge forgetting.</li>
</ul>

<h3>Title: Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and  Improving LLMs</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Wang, Zhongyu Wei, Yejin Choi, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11442">https://arxiv.org/abs/2402.11442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11442">https://arxiv.org/pdf/2402.11442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11442]] Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and  Improving LLMs(https://arxiv.org/abs/2402.11442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs' limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities~\footnote{Code and data are available at \url{https://github.com/SiyuanWangw/ULogic}.}.</li>
</ul>

<h3>Title: Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM  Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11443">https://arxiv.org/abs/2402.11443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11443">https://arxiv.org/pdf/2402.11443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11443]] Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM  Evaluation(https://arxiv.org/abs/2402.11443)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models' capabilities. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks (Code and data are available at https://github.com/NanshineLoong/Self-Evolving-Benchmark).</li>
</ul>

<h3>Title: SciAgent: Tool-augmented Language Models for Scientific Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11451">https://arxiv.org/abs/2402.11451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11451">https://arxiv.org/pdf/2402.11451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11451]] SciAgent: Tool-augmented Language Models for Scientific Reasoning(https://arxiv.org/abs/2402.11451)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.</li>
</ul>

<h3>Title: AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via  Controllable Question Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11452">https://arxiv.org/abs/2402.11452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11452">https://arxiv.org/pdf/2402.11452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11452]] AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via  Controllable Question Decomposition(https://arxiv.org/abs/2402.11452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework AutoPRM that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, AutoPRM can be easily integrated with other orthogonal reasoning pipelines.</li>
</ul>

<h3>Title: MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific  Data Visualization</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11453">https://arxiv.org/abs/2402.11453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11453">https://arxiv.org/pdf/2402.11453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11453]] MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific  Data Visualization(https://arxiv.org/abs/2402.11453)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.</li>
</ul>

<h3>Title: LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11455">https://arxiv.org/abs/2402.11455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11455">https://arxiv.org/pdf/2402.11455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11455]] LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative  Tasks(https://arxiv.org/abs/2402.11455)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few parameters, which can be learned with only 200 training examples. Experiments across six generative tasks demonstrate that our method consistently outperforms baselines with task-level fusion weights. This underscores the necessity of introducing dynamic fusion weights for LoRA combination.</li>
</ul>

<h3>Title: When Do LLMs Need Retrieval Augmentation? Mitigating LLMs'  Overconfidence Helps Retrieval Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11457">https://arxiv.org/abs/2402.11457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11457">https://arxiv.org/pdf/2402.11457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11457]] When Do LLMs Need Retrieval Augmentation? Mitigating LLMs'  Overconfidence Helps Retrieval Augmentation(https://arxiv.org/abs/2402.11457)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls.</li>
</ul>

<h3>Title: Key Patch Proposer: Key Patches Contain Rich Information</h3>
<ul>
<li><strong>Authors: </strong>Jing Xu, Beiwen Tian, Hao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11458">https://arxiv.org/abs/2402.11458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11458">https://arxiv.org/pdf/2402.11458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11458]] Key Patch Proposer: Key Patches Contain Rich Information(https://arxiv.org/abs/2402.11458)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel algorithm named Key Patch Proposer (KPP) designed to select key patches in an image without additional training. Our experiments showcase KPP's robust capacity to capture semantic information by both reconstruction and classification tasks. The efficacy of KPP suggests its potential application in active learning for semantic segmentation. Our source code is publicly available at https://github.com/CA-TT-AC/key-patch-proposer.</li>
</ul>

<h3>Title: A Curious Case of Searching for the Correlation between Training Data  and Adversarial Robustness of Transformer Textual Models</h3>
<ul>
<li><strong>Authors: </strong>Cuong Dang, Dung D. Le, Thai Le</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11469">https://arxiv.org/abs/2402.11469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11469">https://arxiv.org/pdf/2402.11469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11469]] A Curious Case of Searching for the Correlation between Training Data  and Adversarial Robustness of Transformer Textual Models(https://arxiv.org/abs/2402.11469)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness. Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under adversarial training, and (d) robust to statistical randomness. Our code will be publicly available.</li>
</ul>

<h3>Title: Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery  Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11473">https://arxiv.org/abs/2402.11473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11473">https://arxiv.org/pdf/2402.11473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11473]] Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery  Detection(https://arxiv.org/abs/2402.11473)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal</a></li>
<li><strong>Abstract: </strong>The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes \emph{Poisoned Forgery Face} framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses SoTA backdoor baselines with a significant improvement in attack success rate (+16.39\% BD-AUC) and reduction in visibility (-12.65\% $L_\infty$). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. Our codes will be made available at \url{https://github.com/JWLiang007/PFF}</li>
</ul>

<h3>Title: EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule  Endoscopy Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Qiaozhi Tan, Long Bai, Guankun Wang, Mobarakol Islam, Hongliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11476">https://arxiv.org/abs/2402.11476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11476">https://arxiv.org/pdf/2402.11476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11476]] EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule  Endoscopy Diagnosis(https://arxiv.org/abs/2402.11476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Wireless capsule endoscopy (WCE) is a non-invasive diagnostic procedure that enables visualization of the gastrointestinal (GI) tract. Deep learning-based methods have shown effectiveness in disease screening using WCE data, alleviating the burden on healthcare professionals. However, existing capsule endoscopy classification methods mostly rely on pre-defined categories, making it challenging to identify and classify out-of-distribution (OOD) data, such as undefined categories or anatomical landmarks. To address this issue, we propose the Endoscopy Out-of-Distribution (EndoOOD) framework, which aims to effectively handle the OOD detection challenge in WCE diagnosis. The proposed framework focuses on improving the robustness and reliability of WCE diagnostic capabilities by incorporating uncertainty-aware mixup training and long-tailed in-distribution (ID) data calibration techniques. Additionally, virtual-logit matching is employed to accurately distinguish between OOD and ID data while minimizing information loss. To assess the performance of our proposed solution, we conduct evaluations and comparisons with 12 state-of-the-art (SOTA) methods using two publicly available datasets. The results demonstrate the effectiveness of the proposed framework in enhancing diagnostic accuracy and supporting clinical decision-making.</li>
</ul>

<h3>Title: DictLLM: Harnessing Key-Value Data Structures with Large Language Models  for Enhanced Medical Diagnostics</h3>
<ul>
<li><strong>Authors: </strong>YiQiu Guo, Yuchen Yang, Ya Zhang, Yu Wang, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11481">https://arxiv.org/abs/2402.11481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11481">https://arxiv.org/pdf/2402.11481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11481]] DictLLM: Harnessing Key-Value Data Structures with Large Language Models  for Enhanced Medical Diagnostics(https://arxiv.org/abs/2402.11481)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens. We carry out experiments using various LLM models on a comprehensive real-world medical laboratory report dataset for automatic diagnosis generation, our findings illustrate that DictLLM significantly outperforms established baseline methods and few-shot GPT-4 implementations in terms of both Rouge-L and Knowledge F1 scores. Furthermore, our evaluation of the framework's scalability and robustness, through a series of experiments, underscores its exceptional capability in accurately modeling the complex key-value data structure of medical dictionary data.</li>
</ul>

<h3>Title: LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models  with Entity-based Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ikuya Yamada, Ryokan Ri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11485">https://arxiv.org/abs/2402.11485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11485">https://arxiv.org/pdf/2402.11485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11485]] LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models  with Entity-based Data Augmentation(https://arxiv.org/abs/2402.11485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.</li>
</ul>

<h3>Title: Visual Concept-driven Image Generation with Text-to-Image Diffusion  Model</h3>
<ul>
<li><strong>Authors: </strong>Tanzila Rahman, Shweta Mahajan, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Leonid Sigal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11487">https://arxiv.org/abs/2402.11487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11487">https://arxiv.org/pdf/2402.11487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11487]] Visual Concept-driven Image Generation with Text-to-Image Diffusion  Model(https://arxiv.org/abs/2402.11487)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent Dense CRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a bi-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively (through user studies) with a number of examples and use cases that can combine up to three entangled concepts.</li>
</ul>

<h3>Title: What's the Plan? Evaluating and Developing Planning-Aware Techniques for  LLMs</h3>
<ul>
<li><strong>Authors: </strong>Eran Hirsch, Guy Uziel, Ateret Anaby-Tavor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11489">https://arxiv.org/abs/2402.11489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11489">https://arxiv.org/pdf/2402.11489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11489]] What's the Plan? Evaluating and Developing Planning-Aware Techniques for  LLMs(https://arxiv.org/abs/2402.11489)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.</li>
</ul>

<h3>Title: Benchmarking Knowledge Boundary for Large Language Model: A Different  Perspective on Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11493">https://arxiv.org/abs/2402.11493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11493">https://arxiv.org/pdf/2402.11493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11493]] Benchmarking Knowledge Boundary for Large Language Model: A Different  Perspective on Model Evaluation(https://arxiv.org/abs/2402.11493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.</li>
</ul>

<h3>Title: Graph Out-of-Distribution Generalization via Causal Intervention</h3>
<ul>
<li><strong>Authors: </strong>Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11494">https://arxiv.org/abs/2402.11494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11494">https://arxiv.org/pdf/2402.11494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11494]] Graph Out-of-Distribution Generalization via Causal Intervention(https://arxiv.org/abs/2402.11494)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4\% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet.</li>
</ul>

<h3>Title: URLBERT:A Contrastive and Adversarial Pre-trained Model for URL  Classification</h3>
<ul>
<li><strong>Authors: </strong>Yujie Li, Yanbin Wang, Haitao Xu, Zhenhao Guo, Zheng Cao, Lun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11495">https://arxiv.org/abs/2402.11495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11495">https://arxiv.org/pdf/2402.11495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11495]] URLBERT:A Contrastive and Adversarial Pre-trained Model for URL  Classification(https://arxiv.org/abs/2402.11495)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed methods are evaluated on tasks including phishing URL detection, web page classification, and ad filtering, achieving state-of-the-art performance. Importantly, we also explore multi-task learning with URLBERT, and experimental results demonstrate that multi-task learning model based on URLBERT exhibit equivalent effectiveness compared to independently fine-tuned models, showing the simplicity of URLBERT in handling complex task requirements. The code for our work is available at https://github.com/Davidup1/URLBERT.</li>
</ul>

<h3>Title: Thyroid ultrasound diagnosis improvement via multi-view self-supervised  learning and two-stage pre-training</h3>
<ul>
<li><strong>Authors: </strong>Jian Wang, Xin Yang, Xiaohong Jia, Wufeng Xue, Rusi Chen, Yanlin Chen, Xiliang Zhu, Lian Liu, Yan Cao, Jianqiao Zhou, Dong Ni, Ning Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11497">https://arxiv.org/abs/2402.11497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11497">https://arxiv.org/pdf/2402.11497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11497]] Thyroid ultrasound diagnosis improvement via multi-view self-supervised  learning and two-stage pre-training(https://arxiv.org/abs/2402.11497)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Thyroid nodule classification and segmentation in ultrasound images are crucial for computer-aided diagnosis; however, they face limitations owing to insufficient labeled data. In this study, we proposed a multi-view contrastive self-supervised method to improve thyroid nodule classification and segmentation performance with limited manual labels. Our method aligns the transverse and longitudinal views of the same nodule, thereby enabling the model to focus more on the nodule area. We designed an adaptive loss function that eliminates the limitations of the paired data. Additionally, we adopted a two-stage pre-training to exploit the pre-training on ImageNet and thyroid ultrasound images. Extensive experiments were conducted on a large-scale dataset collected from multiple centers. The results showed that the proposed method significantly improves nodule classification and segmentation performance with limited manual labels and outperforms state-of-the-art self-supervised methods. The two-stage pre-training also significantly exceeded ImageNet pre-training.</li>
</ul>

<h3>Title: GenAD: Generative End-to-End Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Wenzhao Zheng, Ruiqi Song, Xianda Guo, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11502">https://arxiv.org/abs/2402.11502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11502">https://arxiv.org/pdf/2402.11502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11502]] GenAD: Generative End-to-End Autonomous Driving(https://arxiv.org/abs/2402.11502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a variational autoencoder to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes benchmark show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency.</li>
</ul>

<h3>Title: Federated Fine-tuning of Large Language Models under Heterogeneous  Language Tasks and Client Resources</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11505">https://arxiv.org/abs/2402.11505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11505">https://arxiv.org/pdf/2402.11505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11505]] Federated Fine-tuning of Large Language Models under Heterogeneous  Language Tasks and Client Resources(https://arxiv.org/abs/2402.11505)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, our experiments validate the efficacy of FlexLoRA, with the federated global model achieving up to a 3.1% average improvement in downstream NLP task performance. FlexLoRA's practicality is further underscored by its seamless integration with existing LoRA-based FL methods and theoretical analysis, offering a path toward scalable, privacy-preserving federated tuning for LLMs.</li>
</ul>

<h3>Title: From Prejudice to Parity: A New Approach to Debiasing Large Language  Model Word Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11512">https://arxiv.org/abs/2402.11512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11512">https://arxiv.org/pdf/2402.11512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11512]] From Prejudice to Parity: A New Approach to Debiasing Large Language  Model Word Embeddings(https://arxiv.org/abs/2402.11512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.</li>
</ul>

<h3>Title: Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM</h3>
<ul>
<li><strong>Authors: </strong>Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, Xiao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11517">https://arxiv.org/abs/2402.11517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11517">https://arxiv.org/pdf/2402.11517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11517]] Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM(https://arxiv.org/abs/2402.11517)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further provide a Reinforcement Learning via Database Feedback (RLDBF) training strategy to guide the DELLM to generate more helpful knowledge for LLMs. Extensive experiments verify DELLM can enhance the state-of-the-art LLMs on text-to-SQL tasks. The model structure and the parameter weight of DELLM are released for further research.</li>
</ul>

<h3>Title: Large Language Model-driven Meta-structure Discovery in Heterogeneous  Information Network</h3>
<ul>
<li><strong>Authors: </strong>Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11518">https://arxiv.org/abs/2402.11518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11518">https://arxiv.org/pdf/2402.11518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11518]] Large Language Model-driven Meta-structure Discovery in Heterogeneous  Information Network(https://arxiv.org/abs/2402.11518)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose a novel REasoning meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate semantically feasible meta-structures. ReStruct also employs performance-oriented evolutionary operations. These two competing forces jointly optimize for semantic explainability and empirical performance of meta-structures. We also design a differential LLM explainer that can produce natural language explanations for the discovered meta-structures, and refine the explanation by reasoning through the search history. Experiments on five datasets demonstrate ReStruct achieve SOTA performance in node classification and link recommendation tasks. Additionally, a survey study involving 73 graduate students shows that the meta-structures and natural language explanations generated by ReStruct are substantially more comprehensible.</li>
</ul>

<h3>Title: Cross-Attention Fusion of Visual and Geometric Features for Large  Vocabulary Arabic Lipreading</h3>
<ul>
<li><strong>Authors: </strong>Samar Daou, Ahmed Rekik, Achraf Ben-Hamadou, Abdelaziz Kallel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11520">https://arxiv.org/abs/2402.11520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11520">https://arxiv.org/pdf/2402.11520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11520]] Cross-Attention Fusion of Visual and Geometric Features for Large  Vocabulary Arabic Lipreading(https://arxiv.org/abs/2402.11520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lipreading involves using visual data to recognize spoken words by analyzing the movements of the lips and surrounding area. It is a hot research topic with many potential applications, such as human-machine interaction and enhancing audio speech recognition. Recent deep-learning based works aim to integrate visual features extracted from the mouth region with landmark points on the lip contours. However, employing a simple combination method such as concatenation may not be the most effective approach to get the optimal feature vector. To address this challenge, firstly, we propose a cross-attention fusion-based approach for large lexicon Arabic vocabulary to predict spoken words in videos. Our method leverages the power of cross-attention networks to efficiently integrate visual and geometric features computed on the mouth region. Secondly, we introduce the first large-scale Lip Reading in the Wild for Arabic (LRW-AR) dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers. The experimental results obtained on LRW-AR and ArabicVisual databases showed the effectiveness and robustness of the proposed approach in recognizing Arabic words. Our work provides insights into the feasibility and effectiveness of applying lipreading techniques to the Arabic language, opening doors for further research in this field. Link to the project page: https://crns-smartvision.github.io/lrwar</li>
</ul>

<h3>Title: Unveiling the Secrets of Engaging Conversations: Factors that Keep Users  Hooked on Role-Playing Dialog Agents</h3>
<ul>
<li><strong>Authors: </strong>Shuai Zhang, Yu Lu, Junwen Liu, Jia Yu, Huachuan Qiu, Yuming Yan, Zhenzhong Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11522">https://arxiv.org/abs/2402.11522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11522">https://arxiv.org/pdf/2402.11522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11522]] Unveiling the Secrets of Engaging Conversations: Factors that Keep Users  Hooked on Role-Playing Dialog Agents(https://arxiv.org/abs/2402.11522)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term simulations that rarely explore such prolonged and real conversations. In this paper, we investigate the factors influencing retention rates in real interactions with roleplaying models. By analyzing a large dataset of interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engagement with role-playing models and provides valuable insights for future improvements in the development of large language models for role-playing purposes.</li>
</ul>

<h3>Title: Measuring Privacy Loss in Distributed Spatio-Temporal Data</h3>
<ul>
<li><strong>Authors: </strong>Tatsuki Koga, Casey Meehan, Kamalika Chaudhuri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11526">https://arxiv.org/abs/2402.11526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11526">https://arxiv.org/pdf/2402.11526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11526]] Measuring Privacy Loss in Distributed Spatio-Temporal Data(https://arxiv.org/abs/2402.11526)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Statistics about traffic flow and people's movement gathered from multiple geographical locations in a distributed manner are the driving force powering many applications, such as traffic prediction, demand prediction, and restaurant occupancy reports. However, these statistics are often based on sensitive location data of people, and hence privacy has to be preserved while releasing them. The standard way to do this is via differential privacy, which guarantees a form of rigorous, worst-case, person-level privacy. In this work, motivated by several counter-intuitive features of differential privacy in distributed location applications, we propose an alternative privacy loss against location reconstruction attacks by an informed adversary. Our experiments on real and synthetic data demonstrate that our privacy loss better reflects our intuitions on individual privacy violation in the distributed spatio-temporal setting.</li>
</ul>

<h3>Title: Efficient Multimodal Learning from Data-centric Perspective</h3>
<ul>
<li><strong>Authors: </strong>Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11530">https://arxiv.org/abs/2402.11530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11530">https://arxiv.org/pdf/2402.11530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11530]] Efficient Multimodal Learning from Data-centric Perspective(https://arxiv.org/abs/2402.11530)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably causes significant performance drop. In this paper, we demonstrate the possibility to beat the scaling law and train a smaller but better MLLM by exploring more informative training data. Specifically, we introduce Bunny, a family of lightweight MLLMs with flexible vision and language backbones for efficient multimodal learning from condensed training data. Remarkably, our Bunny-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-v1.5-13B, on multiple benchmarks. The code, models and data can be found in https://github.com/BAAI-DCAI/Bunny.</li>
</ul>

<h3>Title: Chain-of-Instructions: Compositional Instruction Tuning on Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11532">https://arxiv.org/abs/2402.11532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11532">https://arxiv.org/pdf/2402.11532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11532]] Chain-of-Instructions: Compositional Instruction Tuning on Large  Language Models(https://arxiv.org/abs/2402.11532)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual summarization, demonstrating the generalizability of CoI models on unseen composite downstream tasks.</li>
</ul>

<h3>Title: PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability</h3>
<ul>
<li><strong>Authors: </strong>Dayuan Fu, Jianzhao Huang, Siyuan Lu, Guanting Dong, Yejie Wang, Keqing He, Weiran Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11534">https://arxiv.org/abs/2402.11534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11534">https://arxiv.org/pdf/2402.11534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11534]] PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability(https://arxiv.org/abs/2402.11534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.</li>
</ul>

<h3>Title: Deciphering the lmpact of Pretraining Data on Large Language Models  through Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11537">https://arxiv.org/abs/2402.11537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11537">https://arxiv.org/pdf/2402.11537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11537]] Deciphering the lmpact of Pretraining Data on Large Language Models  through Machine Unlearning(https://arxiv.org/abs/2402.11537)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of LLMs.</li>
</ul>

<h3>Title: CPN: Complementary Proposal Network for Unconstrained Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Longhuang Wu, Shangxuan Tian, Youxin Wang, Pengfei Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11540">https://arxiv.org/abs/2402.11540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11540">https://arxiv.org/pdf/2402.11540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11540]] CPN: Complementary Proposal Network for Unconstrained Text Detection(https://arxiv.org/abs/2402.11540)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released.</li>
</ul>

<h3>Title: Counter-intuitive: Large Language Models Can Better Understand Knowledge  Graphs Than We Thought</h3>
<ul>
<li><strong>Authors: </strong>Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Guilin Qi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11541">https://arxiv.org/abs/2402.11541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11541">https://arxiv.org/pdf/2402.11541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11541]] Counter-intuitive: Large Language Models Can Better Understand Knowledge  Graphs Than We Thought(https://arxiv.org/abs/2402.11541)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG. Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts. This counter-intuitive finding provides substantial insights for future research on LLMs' comprehension of structured knowledge.</li>
</ul>

<h3>Title: Enhancing Energy Sector Resilience: Integrating Security by Design  Principles</h3>
<ul>
<li><strong>Authors: </strong>Dov Shirtz, Inna Koberman, Aviad Elyashar, Rami Puzis, Yuval Elovici</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11543">https://arxiv.org/abs/2402.11543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11543">https://arxiv.org/pdf/2402.11543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11543]] Enhancing Energy Sector Resilience: Integrating Security by Design  Principles(https://arxiv.org/abs/2402.11543)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Security by design, Sbd is a concept for developing and maintaining systems that are, to the greatest extent possible, free from security vulnerabilities and impervious to security attacks. In addition to technical aspects, such as how to develop a robust industrial control systems hardware, software, communication product, etc., SbD includes also soft aspects, such as organizational managerial attitude and behavior, and employee awareness. Under the Sbd concept, systems, ICS in our context, will be considered more trustworthy by users. User's trust in the systems will be derived from the meticulous adherence to the SbD processes and policies. In accordance with the SbD concept, security is considered. Security measures are implemented, at every stage of the product and systems development life cycle, rather than afterwards. This document presents the security requirements for the implementation of the SbD in industrial control systems. The information presented does not negate any existing security and cyber security standards, etc. Instead, we strongly recommend that organizations should implement and comply with those standards and best practices. Security by design is not a one-time process. It starts at the very beginning of the products of the system design and continues through all its lifecycle. Due to the benefits of the SbD, higher level of security, and robustness to cyber attacks, all organizations associated with the energy sector should strive to establish an ecosystem. The requirements presented in this document may be perceived as burdensome by organizations. However, strict compliance with the requirements and existing security standards and best practices, including continuous monitoring, as specified in this document, is essential to realize an ecosystem driven and protected by the SbD</li>
</ul>

<h3>Title: LongAgent: Scaling Language Models to 128k Context through Multi-Agent  Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11550">https://arxiv.org/abs/2402.11550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11550">https://arxiv.org/pdf/2402.11550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11550]] LongAgent: Scaling Language Models to 128k Context through Multi-Agent  Collaboration(https://arxiv.org/abs/2402.11550)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.</li>
</ul>

<h3>Title: Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal  Imputation</h3>
<ul>
<li><strong>Authors: </strong>Yakun Chen, Kaize Shi, Zhangkai Wu, Juan Chen, Xianzhi Wang, Julian McAuley, Guandong Xu, Shui Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11558">https://arxiv.org/abs/2402.11558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11558">https://arxiv.org/pdf/2402.11558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11558]] Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal  Imputation(https://arxiv.org/abs/2402.11558)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare. However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors. Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data. Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions. In contrast, recent deep learning-based methods, leveraging graph and recurrent neural networks, have demonstrated enhanced efficacy. Nonetheless, these approaches are prone to error accumulation. Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future predictions. These models grapple with the challenge of producing unstable results, a particular issue in diffusion-based models. We aim to address these challenges by designing conditional features to guide the generative process and expedite training. Specifically, we introduce C$^2$TSD, a novel approach incorporating trend and seasonal information as conditional features and employing contrastive learning to improve model generalizability. The extensive experiments on three real-world datasets demonstrate the superior performance of C$^2$TSD over various state-of-the-art baselines.</li>
</ul>

<h3>Title: Cobra Effect in Reference-Free Image Captioning Metrics</h3>
<ul>
<li><strong>Authors: </strong>Zheng Ma, Changxin Wang, Yawen Ouyang, Fei Zhao, Jianbing Zhang, Shujian Huang, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11572">https://arxiv.org/abs/2402.11572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11572">https://arxiv.org/pdf/2402.11572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11572]] Cobra Effect in Reference-Free Image Captioning Metrics(https://arxiv.org/abs/2402.11572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating the compatibility between textual descriptions and corresponding images represents a core endeavor within multi-modal research. In recent years, a proliferation of reference-free methods, leveraging visual-language pre-trained models (VLMs), has emerged. Empirical evidence has substantiated that these innovative approaches exhibit a higher correlation with human judgment, marking a significant advancement in the field. However, does a higher correlation with human evaluations alone sufficiently denote the complete of a metric? In response to this question, in this paper, we study if there are any deficiencies in reference-free metrics. Specifically, inspired by the Cobra Effect, we utilize metric scores as rewards to direct the captioning model toward generating descriptions that closely align with the metric's criteria. If a certain metric has flaws, it will be exploited by the model and reflected in the generated sentences. Our findings reveal that descriptions guided by these metrics contain significant flaws, e.g. incoherent statements and excessive repetition. Subsequently, we propose a novel method termed Self-Improving to rectify the identified shortcomings within these metrics. We employ GPT-4V as an evaluative tool to assess generated sentences and the result reveals that our approach achieves state-of-the-art (SOTA) performance. In addition, we also introduce a challenging evaluation benchmark called Flaws Caption to evaluate reference-free image captioning metrics comprehensively. Our code is available at https://github.com/aaronma2020/robust_captioning_metric</li>
</ul>

<h3>Title: BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval  Augmented Long-Context Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kun Luo, Zheng Liu, Shitao Xiao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11573">https://arxiv.org/abs/2402.11573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11573">https://arxiv.org/pdf/2402.11573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11573]] BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval  Augmented Long-Context Large Language Models(https://arxiv.org/abs/2402.11573)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.</li>
</ul>

<h3>Title: Extensible Embedding: A Flexible Multipler For LLM's Context Length</h3>
<ul>
<li><strong>Authors: </strong>Ninglu Shao, Shitao Xiao, Zheng Liu, Peitian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11577">https://arxiv.org/abs/2402.11577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11577">https://arxiv.org/pdf/2402.11577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11577]] Extensible Embedding: A Flexible Multipler For LLM's Context Length(https://arxiv.org/abs/2402.11577)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.</li>
</ul>

<h3>Title: Publicly auditable privacy-preserving electoral rolls</h3>
<ul>
<li><strong>Authors: </strong>Prashant Agrawal, Mahabir Prasad Jhanwar, Subodh Vishnu Sharma, Subhashis Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11582">https://arxiv.org/abs/2402.11582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11582">https://arxiv.org/pdf/2402.11582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11582]] Publicly auditable privacy-preserving electoral rolls(https://arxiv.org/abs/2402.11582)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, fair</a></li>
<li><strong>Abstract: </strong>While existing literature on electronic voting has extensively addressed verifiability of voting protocols, the vulnerability of electoral rolls in large public elections remains a critical concern. To ensure integrity of electoral rolls, the current practice is to either make electoral rolls public or share them with the political parties. However, this enables construction of detailed voter profiles and selective targeting and manipulation of voters, thereby undermining the fundamental principle of free and fair elections. In this paper, we study the problem of designing publicly auditable yet privacy-preserving electoral rolls. We first formulate a threat model and provide formal security definitions. We then present a protocol for creation and maintenance of electoral rolls that mitigates the threats. Eligible voters can verify their inclusion, whereas political parties and auditors can statistically audit the electoral roll. The entire electoral roll is never revealed, which prevents any large-scale systematic voter targeting and manipulation.</li>
</ul>

<h3>Title: PolypNextLSTM: A lightweight and fast polyp video segmentation network  using ConvNext and ConvLSTM</h3>
<ul>
<li><strong>Authors: </strong>Debayan Bhattacharya, Konrad Reuter, Finn Behrendnt, Lennart Maack, Sarah Grube, Alexander Schlaefer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11585">https://arxiv.org/abs/2402.11585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11585">https://arxiv.org/pdf/2402.11585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11585]] PolypNextLSTM: A lightweight and fast polyp video segmentation network  using ConvNext and ConvLSTM(https://arxiv.org/abs/2402.11585)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios, along with videos containing challenging artefacts like fast motion and occlusion.</li>
</ul>

<h3>Title: SDiT: Spiking Diffusion Model with Transformer</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Hanzhi Ma, Chengting Yu, Aili Wang, Er-Ping Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11588">https://arxiv.org/abs/2402.11588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11588">https://arxiv.org/pdf/2402.11588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11588]] SDiT: Spiking Diffusion Model with Transformer(https://arxiv.org/abs/2402.11588)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.</li>
</ul>

<h3>Title: Revisiting Zeroth-Order Optimization for Memory-Efficient LLM  Fine-Tuning: A Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11592">https://arxiv.org/abs/2402.11592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11592">https://arxiv.org/pdf/2402.11592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11592]] Revisiting Zeroth-Order Optimization for Memory-Efficient LLM  Fine-Tuning: A Benchmark(https://arxiv.org/abs/2402.11592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .</li>
</ul>

<h3>Title: Multi-Task Inference: Can Large Language Models Follow Multiple  Instructions at Once?</h3>
<ul>
<li><strong>Authors: </strong>Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, Seungone Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11597">https://arxiv.org/abs/2402.11597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11597">https://arxiv.org/pdf/2402.11597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11597]] Multi-Task Inference: Can Large Language Models Follow Multiple  Instructions at Once?(https://arxiv.org/abs/2402.11597)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this link https://github.com/guijinSON/MTI-Bench.</li>
</ul>

<h3>Title: Metric-Learning Encoding Models Identify Processing Profiles of  Linguistic Features in BERT's Representations</h3>
<ul>
<li><strong>Authors: </strong>Louis Jalouzot, Robin Sobczyk, Bastien Lhopitallier, Jeanne Salle, Nur Lan, Emmanuel Chemla, Yair Lakretz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11608">https://arxiv.org/abs/2402.11608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11608">https://arxiv.org/pdf/2402.11608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11608]] Metric-Learning Encoding Models Identify Processing Profiles of  Linguistic Features in BERT's Representations(https://arxiv.org/abs/2402.11608)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univariate encoding methods, in being able to predict both local and distributed representations. Together, this demonstrates the utility of Metric-Learning Encoding Methods for studying how linguistic features are neurally encoded in language models and the advantage of MLEMs over traditional methods. MLEMs can be extended to other domains (e.g. vision) and to other neural systems, such as the human brain.</li>
</ul>

<h3>Title: Decoding News Narratives: A Critical Analysis of Large Language Models  in Framing Bias Detection</h3>
<ul>
<li><strong>Authors: </strong>Valeria Pastorino, Jasivan A. Sivakumar, Nafise Sadat Moosavi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11621">https://arxiv.org/abs/2402.11621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11621">https://arxiv.org/pdf/2402.11621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11621]] Decoding News Narratives: A Critical Analysis of Large Language Models  in Framing Bias Detection(https://arxiv.org/abs/2402.11621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distinguishing between reporting genuine emotional expression and intentionally use framing bias in news headlines. We further evaluated the models on two subsets of headlines where the presence or absence of framing bias was either clear-cut or more contested, with the results suggesting that these models' can be useful in flagging potential annotation inaccuracies within existing or new datasets. Finally, the study evaluates the models in real-world conditions ("in the wild"), moving beyond the initial dataset focused on U.S. Gun Violence, assessing the models' performance on framed headlines covering a broad range of topics.</li>
</ul>

<h3>Title: SpeCrawler: Generating OpenAPI Specifications from API Documentation  Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Koren Lazar, Matan Vetzler, Guy Uziel, David Boaz, Esther Goldbraich, David Amid, Ateret Anaby-Tavor</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11625">https://arxiv.org/abs/2402.11625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11625">https://arxiv.org/pdf/2402.11625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11625]] SpeCrawler: Generating OpenAPI Specifications from API Documentation  Using Large Language Models(https://arxiv.org/abs/2402.11625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the digital era, the widespread use of APIs is evident. However, scalable utilization of APIs poses a challenge due to structure divergence observed in online API documentation. This underscores the need for automatic tools to facilitate API consumption. A viable approach involves the conversion of documentation into an API Specification format. While previous attempts have been made using rule-based methods, these approaches encountered difficulties in generalizing across diverse documentation. In this paper we introduce SpeCrawler, a comprehensive system that utilizes large language models (LLMs) to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline. By creating a standardized format for numerous APIs, SpeCrawler aids in streamlining integration processes within API orchestrating systems and facilitating the incorporation of tools into LLMs. The paper explores SpeCrawler's methodology, supported by empirical evidence and case studies, demonstrating its efficacy through LLM capabilities.</li>
</ul>

<h3>Title: Metacognitive Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11626">https://arxiv.org/abs/2402.11626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11626">https://arxiv.org/pdf/2402.11626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11626]] Metacognitive Retrieval-Augmented Large Language Models(https://arxiv.org/abs/2402.11626)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.</li>
</ul>

<h3>Title: Neuromorphic Face Analysis: a Survey</h3>
<ul>
<li><strong>Authors: </strong>Federico Becattini, Lorenzo Berlincioni, Luca Cultrera, Alberto Del Bimbo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11631">https://arxiv.org/abs/2402.11631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11631">https://arxiv.org/pdf/2402.11631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11631]] Neuromorphic Face Analysis: a Survey(https://arxiv.org/abs/2402.11631)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Neuromorphic sensors, also known as event cameras, are a class of imaging devices mimicking the function of biological visual systems. Unlike traditional frame-based cameras, which capture fixed images at discrete intervals, neuromorphic sensors continuously generate events that represent changes in light intensity or motion in the visual field with high temporal resolution and low latency. These properties have proven to be interesting in modeling human faces, both from an effectiveness and a privacy-preserving point of view. Neuromorphic face analysis however is still a raw and unstructured field of research, with several attempts at addressing different tasks with no clear standard or benchmark. This survey paper presents a comprehensive overview of capabilities, challenges and emerging applications in the domain of neuromorphic face analysis, to outline promising directions and open issues. After discussing the fundamental working principles of neuromorphic vision and presenting an in-depth overview of the related research, we explore the current state of available data, standard data representations, emerging challenges, and limitations that require further investigation. This paper aims to highlight the recent process in this evolving field to provide to both experienced and newly come researchers an all-encompassing analysis of the state of the art along with its problems and shortcomings.</li>
</ul>

<h3>Title: Self-seeding and Multi-intent Self-instructing LLMs for Generating  Intent-aware Information-Seeking dialogs</h3>
<ul>
<li><strong>Authors: </strong>Arian Askari, Roxana Petcu, Chuan Meng, Mohammad Aliannejadi, Amin Abolghasemi, Evangelos Kanoulas, Suzan Verberne</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11633">https://arxiv.org/abs/2402.11633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11633">https://arxiv.org/pdf/2402.11633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11633]] Self-seeding and Multi-intent Self-instructing LLMs for Generating  Intent-aware Information-Seeking dialogs(https://arxiv.org/abs/2402.11633)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances. Furthermore, we propose SOLID-RL, which is further trained to generate a dialog in one step on the data generated by SOLID. We propose a length-based quality estimation mechanism to assign varying weights to SOLID-generated dialogs based on their quality during the training process of SOLID-RL. We use SOLID and SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size of existing datasets. Experiments show that IP methods trained on dialogs generated by SOLID and SOLID-RL achieve better IP quality than ones trained on human-generated dialogs.</li>
</ul>

<h3>Title: Poisoning Federated Recommender Systems with Fake Users</h3>
<ul>
<li><strong>Authors: </strong>Ming Yin, Yichang Xu, Minghong Fang, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11637">https://arxiv.org/abs/2402.11637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11637">https://arxiv.org/pdf/2402.11637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11637]] Poisoning Federated Recommender Systems with Fake Users(https://arxiv.org/abs/2402.11637)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems. Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in federated recommender systems without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current benchmarks that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space.</li>
</ul>

<h3>Title: Stumbling Blocks: Stress Testing the Robustness of Machine-Generated  Text Detectors Under Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yichen Wang, Shangbin Feng, Abe Bohan Hou, Xiao Pu, Chao Shen, Xiaoming Liu, Yulia Tsvetkov, Tianxing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11638">https://arxiv.org/abs/2402.11638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11638">https://arxiv.org/pdf/2402.11638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11638]] Stumbling Blocks: Stress Testing the Robustness of Machine-Generated  Text Detectors Under Attacks(https://arxiv.org/abs/2402.11638)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.</li>
</ul>

<h3>Title: In-Context Learning with Transformers: Softmax Attention Adapts to  Function Lipschitzness</h3>
<ul>
<li><strong>Authors: </strong>Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, Sanjay Shakkottai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11639">https://arxiv.org/abs/2402.11639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11639">https://arxiv.org/pdf/2402.11639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11639]] In-Context Learning with Transformers: Softmax Attention Adapts to  Function Lipschitzness(https://arxiv.org/abs/2402.11639)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.</li>
</ul>

<h3>Title: Towards Versatile Graph Learning Approach: from the Perspective of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lanning Wei, Jun Gao, Huan Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11641">https://arxiv.org/abs/2402.11641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11641">https://arxiv.org/pdf/2402.11641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11641]] Towards Versatile Graph Learning Approach: from the Perspective of Large  Language Models(https://arxiv.org/abs/2402.11641)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we align the abilities of LLMs with the requirements of each procedure. Finally, we point out the promising directions that could better leverage the strength of LLMs towards versatile graph learning methods.</li>
</ul>

<h3>Title: Learning From Failure: Integrating Negative Examples when Fine-tuning  Large Language Models as Agents</h3>
<ul>
<li><strong>Authors: </strong>Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11651">https://arxiv.org/abs/2402.11651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11651">https://arxiv.org/pdf/2402.11651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11651]] Learning From Failure: Integrating Negative Examples when Fine-tuning  Large Language Models as Agents(https://arxiv.org/abs/2402.11651)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin.</li>
</ul>

<h3>Title: Competition of Mechanisms: Tracing How Language Models Handle Facts and  Counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Francesco Ortu, Zhijing Jin, Diego Doimo, Mrinmaya Sachan, Alberto Cazzaniga, Bernhard Sch√∂lkopf</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11655">https://arxiv.org/abs/2402.11655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11655">https://arxiv.org/pdf/2402.11655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11655]] Competition of Mechanisms: Tracing How Language Models Handle Facts and  Counterfactuals(https://arxiv.org/abs/2402.11655)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at https://github.com/francescortu/Competition_of_Mechanisms.</li>
</ul>

<h3>Title: Interpretable Short-Term Load Forecasting via Multi-Scale Temporal  Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Jiang, Yan Li, Yize Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11664">https://arxiv.org/abs/2402.11664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11664">https://arxiv.org/pdf/2402.11664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11664]] Interpretable Short-Term Load Forecasting via Multi-Scale Temporal  Decomposition(https://arxiv.org/abs/2402.11664)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Rapid progress in machine learning and deep learning has enabled a wide range of applications in the electricity load forecasting of power systems, for instance, univariate and multivariate short-term load forecasting. Though the strong capabilities of learning the non-linearity of the load patterns and the high prediction accuracy have been achieved, the interpretability of typical deep learning models for electricity load forecasting is less studied. This paper proposes an interpretable deep learning method, which learns a linear combination of neural networks that each attends to an input time feature. We also proposed a multi-scale time series decomposition method to deal with the complex time patterns. Case studies have been carried out on the Belgium central grid load dataset and the proposed model demonstrated better accuracy compared to the frequently applied baseline model. Specifically, the proposed multi-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52, 0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed method displays generalization capability. On the other hand, it can demonstrate not only the feature but also the temporal interpretability compared to other baseline methods. Besides, the global time feature interpretabilities are also obtained. Obtaining global feature interpretabilities allows us to catch the overall patterns, trends, and cyclicality in load data while also revealing the significance of various time-related features in forming the final outputs.</li>
</ul>

<h3>Title: Challenging the Black Box: A Comprehensive Evaluation of Attribution  Maps of CNN Applications in Agriculture and Forestry</h3>
<ul>
<li><strong>Authors: </strong>Lars Nieradzik, Henrike Stephani, J√∂rdis Sieburg-Rockel, Stephanie Helmling, Andrea Olbrich, Janis Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11670">https://arxiv.org/abs/2402.11670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11670">https://arxiv.org/pdf/2402.11670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11670]] Challenging the Black Box: A Comprehensive Evaluation of Attribution  Maps of CNN Applications in Agriculture and Forestry(https://arxiv.org/abs/2402.11670)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification. The opaque nature of these models, often considered 'black boxes', is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps. Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations. Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts. These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks. Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a better understanding of neural networks in these application areas.</li>
</ul>

<h3>Title: Autocorrect for Estonian texts: final report from project EKTB25</h3>
<ul>
<li><strong>Authors: </strong>Agnes Luhtaru, Martin Vainikko, Krista Liin, Kais Allkivi-Metsoja, Jaagup Kippar, Pille Eslon, Mark Fishel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11671">https://arxiv.org/abs/2402.11671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11671">https://arxiv.org/pdf/2402.11671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11671]] Autocorrect for Estonian texts: final report from project EKTB25(https://arxiv.org/abs/2402.11671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models. We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail. There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-language support, has been created. We took into account the existence of the model when adjusting plans and in the report we present a comparison with the ability of GPT4 to improve the Estonian language text. The final results show that the approach we have developed provides better scores than GPT4 and the result is usable but not entirely reliable yet. The report also contains ideas on how GPT4 and other major language models can be implemented in the future, focusing on open-source solutions. All results of this project are open-data/open-source, with licenses that allow them to be used for purposes including commercial ones.</li>
</ul>

<h3>Title: MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of  LiDAR-Camera Fusion for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Till Beemelmanns, Quan Zhang, Lutz Eckstein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11677">https://arxiv.org/abs/2402.11677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11677">https://arxiv.org/pdf/2402.11677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11677]] MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of  LiDAR-Camera Fusion for 3D Object Detection(https://arxiv.org/abs/2402.11677)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of their resistance ability. Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy. We provide insights into which multi-modal design choices make such models robust against certain perturbations. The dataset generation code and benchmark are open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.</li>
</ul>

<h3>Title: Opening the black box of language acquisition</h3>
<ul>
<li><strong>Authors: </strong>J√©r√¥me Michaud, Anna Jon-and</a></li>
<li><strong>Subjects: </strong>cs.CL, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11681">https://arxiv.org/abs/2402.11681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11681">https://arxiv.org/pdf/2402.11681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11681]] Opening the black box of language acquisition(https://arxiv.org/abs/2402.11681)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory as a key component of the language learning process. Since other animals do not seem to have a faithful sequence memory, this may explain why only humans have developed complex languages.</li>
</ul>

<h3>Title: One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Tejpalsingh Siledar, Swaroop Nath, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Swaprava Nath, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11683">https://arxiv.org/abs/2402.11683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11683">https://arxiv.org/pdf/2402.11683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11683]] One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation(https://arxiv.org/abs/2402.11683)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 with humans, outperforming all previous approaches. To the best of our knowledge, we are the first to investigate LLMs as evaluators on both closed-source and open-source models in the opinion summarization domain.</li>
</ul>

<h3>Title: Why Lift so Heavy? Slimming Large Language Models by Cutting Off the  Layers</h3>
<ul>
<li><strong>Authors: </strong>Shuzhou Yuan, Ercong Nie, Bolei Ma, Michael F√§rber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11700">https://arxiv.org/abs/2402.11700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11700">https://arxiv.org/pdf/2402.11700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11700]] Why Lift so Heavy? Slimming Large Language Models by Cutting Off the  Layers(https://arxiv.org/abs/2402.11700)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while preserving their performance, thereby opening avenues for significantly more efficient use of LLMs.</li>
</ul>

<h3>Title: GNNavi: Navigating the Information Flow in Large Language Models by  Graph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Shuzhou Yuan, Ercong Nie, Michael F√§rber, Helmut Schmid, Hinrich Sch√ºtze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11709">https://arxiv.org/abs/2402.11709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11709">https://arxiv.org/pdf/2402.11709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11709]] GNNavi: Navigating the Information Flow in Large Language Models by  Graph Neural Network(https://arxiv.org/abs/2402.11709)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process.</li>
</ul>

<h3>Title: MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement  Learning for Discrete Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yasaman Jafari, Dheeraj Mekala, Rose Yu, Taylor Berg-Kirkpatrick</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11711">https://arxiv.org/abs/2402.11711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11711">https://arxiv.org/pdf/2402.11711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11711]] MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement  Learning for Discrete Prompt Optimization(https://arxiv.org/abs/2402.11711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions.</li>
</ul>

<h3>Title: Modelling Political Coalition Negotiations Using LLM-based Agents</h3>
<ul>
<li><strong>Authors: </strong>Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11712">https://arxiv.org/abs/2402.11712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11712">https://arxiv.org/pdf/2402.11712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11712]] Modelling Political Coalition Negotiations Using LLM-based Agents(https://arxiv.org/abs/2402.11712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.</li>
</ul>

<h3>Title: How Susceptible are Large Language Models to Ideological Manipulation?</h3>
<ul>
<li><strong>Authors: </strong>Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11725">https://arxiv.org/abs/2402.11725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11725">https://arxiv.org/pdf/2402.11725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11725]] How Susceptible are Large Language Models to Ideological Manipulation?(https://arxiv.org/abs/2402.11725)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.</li>
</ul>

<h3>Title: The Effectiveness of Random Forgetting for Robust Generalization</h3>
<ul>
<li><strong>Authors: </strong>Vijaya Raghavan T Ramkumar, Bahram Zonooz, Elahe Arani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11733">https://arxiv.org/abs/2402.11733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11733">https://arxiv.org/pdf/2402.11733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11733]] The Effectiveness of Random Forgetting for Robust Generalization(https://arxiv.org/abs/2402.11733)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.</li>
</ul>

<h3>Title: Extraction of nonlinearity in neural networks and model compression with  Koopman operator</h3>
<ul>
<li><strong>Authors: </strong>Naoki Sugishita, Kayo Kinjo, Jun Ohkubo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11740">https://arxiv.org/abs/2402.11740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11740">https://arxiv.org/pdf/2402.11740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11740]] Extraction of nonlinearity in neural networks and model compression with  Koopman operator(https://arxiv.org/abs/2402.11740)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.</li>
</ul>

<h3>Title: In-Context Learning Demonstration Selection via Influence Analysis</h3>
<ul>
<li><strong>Authors: </strong>Vinay M.S., Minh-Hao Van, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11750">https://arxiv.org/abs/2402.11750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11750">https://arxiv.org/pdf/2402.11750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11750]] In-Context Learning Demonstration Selection via Influence Analysis(https://arxiv.org/abs/2402.11750)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance. To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.</li>
</ul>

<h3>Title: ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11753">https://arxiv.org/abs/2402.11753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11753">https://arxiv.org/pdf/2402.11753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11753]] ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs(https://arxiv.org/abs/2402.11753)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs.</li>
</ul>

<h3>Title: SPML: A DSL for Defending Language Models Against Prompt Attacks</h3>
<ul>
<li><strong>Authors: </strong>Reshabh K Sharma, Vinayak Gupta, Dan Grossman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11755">https://arxiv.org/abs/2402.11755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11755">https://arxiv.org/pdf/2402.11755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11755]] SPML: A DSL for Defending Language Models Against Prompt Attacks(https://arxiv.org/abs/2402.11755)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language design challenges. Additionally, we introduce a groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering the inaugural language and benchmark for chatbot definition evaluation. Experiments across datasets demonstrate SPML's proficiency in understanding attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data and codes are publicly available at: https://prompt-compiler.github.io/SPML/.</li>
</ul>

<h3>Title: MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in  Generative LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11756">https://arxiv.org/abs/2402.11756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11756">https://arxiv.org/pdf/2402.11756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11756]] MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in  Generative LLMs(https://arxiv.org/abs/2402.11756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found https://anonymous.4open.science/r/LLM_Uncertainity-309B.</li>
</ul>

<h3>Title: Reinforcement Learning as a Parsimonious Alternative to Prediction  Cascades: A Case Study on Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bharat Srikishan, Anika Tabassum, Srikanth Allu, Ramakrishnan Kannan, Nikhil Muralidhar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11760">https://arxiv.org/abs/2402.11760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11760">https://arxiv.org/pdf/2402.11760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11760]] Reinforcement Learning as a Parsimonious Alternative to Prediction  Cascades: A Case Study on Image Segmentation(https://arxiv.org/abs/2402.11760)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at https://github.com/scailab/paser .</li>
</ul>

<h3>Title: ChatGPT Based Data Augmentation for Improved Parameter-Efficient  Debiasing of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Pengrui Han, Rafal Kocielnik, Adhithya Saravanan, Roy Jiang, Or Sharir, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11764">https://arxiv.org/abs/2402.11764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11764">https://arxiv.org/pdf/2402.11764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11764]] ChatGPT Based Data Augmentation for Improved Parameter-Efficient  Debiasing of LLMs(https://arxiv.org/abs/2402.11764)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained LLM; and (3) synthetic data exhibits generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic data in advancing the fairness of LLMs with minimal retraining cost.</li>
</ul>

<h3>Title: Structured Chain-of-Thought Prompting for Few-Shot Generation of  Content-Grounded QA Conversations</h3>
<ul>
<li><strong>Authors: </strong>Md Arafat Sultan, Jatin Ganhotra, Ram√≥n Fernandez Astudillo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11770">https://arxiv.org/abs/2402.11770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11770">https://arxiv.org/pdf/2402.11770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11770]] Structured Chain-of-Thought Prompting for Few-Shot Generation of  Content-Grounded QA Conversations(https://arxiv.org/abs/2402.11770)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for example, we observe improvements of up to 13.9% over target domain gold data when the latter is augmented with our generated examples.</li>
</ul>

<h3>Title: Towards Theoretical Understandings of Self-Consuming Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Shi Fu, Sen Zhang, Yingjie Wang, Xinmei Tian, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11778">https://arxiv.org/abs/2402.11778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11778">https://arxiv.org/pdf/2402.11778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11778]] Towards Theoretical Understandings of Self-Consuming Generative Models(https://arxiv.org/abs/2402.11778)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declines beyond a threshold point. Finally, we specialize our general results to diffusion models, delivering nuanced insights such as the efficacy of optimal early stopping within the self-consuming loop.</li>
</ul>

<h3>Title: MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast  Cancer Through Multimodal Data Fusion</h3>
<ul>
<li><strong>Authors: </strong>Raktim Kumar Mondol, Ewan K.A. Millar, Arcot Sowmya, Erik Meijering</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11788">https://arxiv.org/abs/2402.11788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11788">https://arxiv.org/pdf/2402.11788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11788]] MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast  Cancer Through Multimodal Data Fusion(https://arxiv.org/abs/2402.11788)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.</li>
</ul>

<h3>Title: SDGE: Stereo Guided Depth Estimation for 360¬∞ Camera Sets</h3>
<ul>
<li><strong>Authors: </strong>Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11791">https://arxiv.org/abs/2402.11791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11791">https://arxiv.org/pdf/2402.11791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11791]] SDGE: Stereo Guided Depth Estimation for 360¬∞ Camera Sets(https://arxiv.org/abs/2402.11791)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360{\deg} perception. These 360{\deg} camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360{\deg} cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both supervised and self-supervised depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D object detection and occupancy prediction.</li>
</ul>

<h3>Title: Generative Kaleidoscopic Networks</h3>
<ul>
<li><strong>Authors: </strong>Harsh Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11793">https://arxiv.org/abs/2402.11793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11793">https://arxiv.org/pdf/2402.11793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11793]] Generative Kaleidoscopic Networks(https://arxiv.org/abs/2402.11793)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper the MLP, higher is the quality of samples recovered. Scope: We observed this phenomenon to various degrees for the other deep learning architectures like CNNs, Transformers & U-Nets and we are currently investigating them further.</li>
</ul>

<h3>Title: Unveiling the Magic: Investigating Attention Distillation in  Retrieval-augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zizhong Li, Haopeng Zhang, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11794">https://arxiv.org/abs/2402.11794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11794">https://arxiv.org/pdf/2402.11794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11794]] Unveiling the Magic: Investigating Attention Distillation in  Retrieval-augmented Generation(https://arxiv.org/abs/2402.11794)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.</li>
</ul>

<h3>Title: Generation Meets Verification: Accelerating Large Language Model  Inference with Smart Parallel Auto-Correct Decoding</h3>
<ul>
<li><strong>Authors: </strong>Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11809">https://arxiv.org/abs/2402.11809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11809">https://arxiv.org/pdf/2402.11809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11809]] Generation Meets Verification: Accelerating Large Language Model  Inference with Smart Parallel Auto-Correct Decoding(https://arxiv.org/abs/2402.11809)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.</li>
</ul>

<h3>Title: FIPO: Free-form Instruction-oriented Prompt Optimization with Preference  Dataset and Modular Fine-tuning Schema</h3>
<ul>
<li><strong>Authors: </strong>Junru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, Xing Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11811">https://arxiv.org/abs/2402.11811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11811">https://arxiv.org/pdf/2402.11811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11811]] FIPO: Free-form Instruction-oriented Prompt Optimization with Preference  Dataset and Modular Fine-tuning Schema(https://arxiv.org/abs/2402.11811)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference dataset is meticulously constructed using the optimal and suboptimal LLMs, undergoing rigorous cross-verification by human experts and analytical models. Applying the insights from the data with Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO schema across five public benchmarks. Codes, data and scripts are here: https://github.com/LuJunru/FIPO_Project.</li>
</ul>

<h3>Title: An Empirical Evaluation of LLMs for Solving Offensive Security  Challenges</h3>
<ul>
<li><strong>Authors: </strong>Minghao Shao, Boyuan Chen, Sofija Jancheska, Brendan Dolan-Gavitt, Siddharth Garg, Ramesh Karri, Muhammad Shafique</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11814">https://arxiv.org/abs/2402.11814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11814">https://arxiv.org/pdf/2402.11814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11814]] An Empirical Evaluation of LLMs for Solving Offensive Security  Challenges(https://arxiv.org/abs/2402.11814)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Capture The Flag (CTF) challenges are puzzles related to computer security scenarios. With the advent of large language models (LLMs), more and more CTF participants are using LLMs to understand and solve the challenges. However, so far no work has evaluated the effectiveness of LLMs in solving CTF challenges with a fully automated workflow. We develop two CTF-solving workflows, human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability to solve a selected set of CTF challenges, prompted with information about the question. We collect human contestants' results on the same set of questions, and find that LLMs achieve higher success rate than an average human participant. This work provides a comprehensive evaluation of the capability of LLMs in solving real world CTF challenges, from real competition to fully automated workflow. Our results provide references for applying LLMs in cybersecurity education and pave the way for systematic evaluation of offensive cybersecurity capabilities in LLMs.</li>
</ul>

<h3>Title: HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to  Detect Machine-Generated Text?</h3>
<ul>
<li><strong>Authors: </strong>Shubhashis Roy Dipta, Sadat Shahriar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11815">https://arxiv.org/abs/2402.11815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11815">https://arxiv.org/pdf/2402.11815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11815]] HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to  Detect Machine-Generated Text?(https://arxiv.org/abs/2402.11815)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a single base model can have comparable performance with the help of data augmentation and contrastive learning.</li>
</ul>

<h3>Title: Where It Really Matters: Few-Shot Environmental Conservation Media  Monitoring for Low-Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Sameer Jain, Sedrick Scott Keh, Shova Chettri, Karun Dewan, Pablo Izquierdo, Johanna Prussman, Pooja Shreshtha, Cesar Suarez, Zheyuan Ryan Shi, Lei Li, Fei Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11818">https://arxiv.org/abs/2402.11818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11818">https://arxiv.org/pdf/2402.11818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11818]] Where It Really Matters: Few-Shot Environmental Conservation Media  Monitoring for Low-Resource Languages(https://arxiv.org/abs/2402.11818)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot methods and achieves comparable performance with models fully fine-tuned using thousands of examples. The World Wide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in Nepal, significantly reducing their operational burden, and ensuring that AI tools for conservation actually reach the communities that need them the most. NewsSerow has also been deployed for countries with other languages like Colombia.</li>
</ul>

<h3>Title: Head-wise Shareable Attention for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zouying Cao, Yifei Yang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11819">https://arxiv.org/abs/2402.11819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11819">https://arxiv.org/pdf/2402.11819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11819]] Head-wise Shareable Attention for Large Language Models(https://arxiv.org/abs/2402.11819)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\textit{$\textbf{head-wise shareable attention for large language models}$}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, denoted as $\textbf{DirectShare}$. The second method first post-trains with constraint on weight matrix similarity and then shares, denoted as $\textbf{PostShare}$. Experimental results reveal our head-wise shared models still maintain satisfactory capabilities, demonstrating the feasibility of fine-grained weight sharing applied to LLMs.</li>
</ul>

<h3>Title: Microstructures and Accuracy of Graph Recall by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanbang Wang, Hejie Cui, Jon Kleinberg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11821">https://arxiv.org/abs/2402.11821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11821">https://arxiv.org/pdf/2402.11821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11821]] Microstructures and Accuracy of Graph Recall by Large Language Models(https://arxiv.org/abs/2402.11821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.</li>
</ul>

<h3>Title: Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging  Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Jialei Xu, Xianming Liu, Junjun Jiang, Kui Jiang, Rui Li, Kai Cheng, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11826">https://arxiv.org/abs/2402.11826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11826">https://arxiv.org/pdf/2402.11826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11826]] Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging  Scenarios(https://arxiv.org/abs/2402.11826)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation from RGB images plays a pivotal role in 3D vision. However, its accuracy can deteriorate in challenging environments such as nighttime or adverse weather conditions. While long-wave infrared cameras offer stable imaging in such challenging conditions, they are inherently low-resolution, lacking rich texture and semantics as delivered by the RGB image. Current methods focus solely on a single modality due to the difficulties to identify and integrate faithful depth cues from both sources. To address these issues, this paper presents a novel approach that identifies and integrates dominant cross-modality depth features with a learning-based framework. Concretely, we independently compute the coarse depth maps with separate networks by fully utilizing the individual depth cues from each modality. As the advantageous depth spreads across both modalities, we propose a novel confidence loss steering a confidence predictor network to yield a confidence map specifying latent potential depth areas. With the resulting confidence map, we propose a multi-modal fusion network that fuses the final depth in an end-to-end manner. Harnessing the proposed pipeline, our method demonstrates the ability of robust depth estimation in a variety of difficult scenarios. Experimental results on the challenging MS$^2$ and ViViD++ datasets demonstrate the effectiveness and robustness of our method.</li>
</ul>

<h3>Title: Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced  Tracking and Security Features</h3>
<ul>
<li><strong>Authors: </strong>Iqtiar Md Siddique, Selim Molla, MD Rakib Hasan, Anamika Ahmed Siddique</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11829">https://arxiv.org/abs/2402.11829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11829">https://arxiv.org/pdf/2402.11829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11829]] Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced  Tracking and Security Features(https://arxiv.org/abs/2402.11829)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>This study focuses on the implementation of modern and intelligent logistics vehicles equipped with advanced tracking and security features. In response to the evolving landscape of logistics management, the proposed system integrates cutting edge technologies to enhance efficiency and ensure the security of the entire logistics process. The core component of this implementation is the incorporation of state-of-the art tracking mechanisms, enabling real-time monitoring of vehicle locations and movements. Furthermore, the system addresses the paramount concern of security by introducing advanced security measures. Through the utilization of sophisticated tracking technologies and security protocols, the proposed logistics vehicles aim to safeguard both customer and provider data. The implementation includes the integration of QR code concepts, creating a binary image system that conceals sensitive information and ensures access only to authorized users. In addition to tracking and security, the study delves into the realm of information mining, employing techniques such as classification, clustering, and recommendation to extract meaningful patterns from vast datasets. Collaborative filtering techniques are incorporated to enhance customer experience by recommending services based on user preferences and historical data. This abstract encapsulates the comprehensive approach of deploying modern logistics vehicles, emphasizing their intelligence through advanced tracking, robust security measures, and data-driven insights. The proposed system aims to revolutionize logistics management, providing a seamless and secure experience for both customers and service providers in the dynamic logistics landscape.</li>
</ul>

<h3>Title: Rock Classification Based on Residual Networks</h3>
<ul>
<li><strong>Authors: </strong>Sining Zhoubian, Yuyang Wang, Zhihuan Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11831">https://arxiv.org/abs/2402.11831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11831">https://arxiv.org/pdf/2402.11831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11831]] Rock Classification Based on Residual Networks(https://arxiv.org/abs/2402.11831)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Rock Classification is an essential geological problem since it provides important formation information. However, exploration on this problem using convolutional neural networks is not sufficient. To tackle this problem, we propose two approaches using residual neural networks. We first adopt data augmentation methods to enlarge our dataset. By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model. This boosts the model's performance, achieving an accuracy of 73.7% on the test dataset. We also explore how the number of bottleneck transformer blocks may influence model performance. We discover that models with more than one bottleneck transformer block may not further improve performance. Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures.</li>
</ul>

<h3>Title: Self-Guided Robust Graph Structure Refinement</h3>
<ul>
<li><strong>Authors: </strong>Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, Chanyoung Park</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11837">https://arxiv.org/abs/2402.11837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11837">https://arxiv.org/pdf/2402.11837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11837]] Self-Guided Robust Graph Structure Refinement(https://arxiv.org/abs/2402.11837)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction</a></li>
<li><strong>Abstract: </strong>Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels. Our code is available at https://github.com/yeonjun-in/torch-SG-GSR.</li>
</ul>

<h3>Title: UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal  Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11838">https://arxiv.org/abs/2402.11838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11838">https://arxiv.org/pdf/2402.11838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11838]] UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal  Prediction(https://arxiv.org/abs/2402.11838)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts that align and leverage intrinsic and shared knowledge across scenarios. These designs together unlock the potential of a one-for-all model for spatio-temporal prediction with powerful generalization capability. Extensive experiments on 15 cities and 6 domains demonstrate the universality of UniST in advancing state-of-the-art prediction performance, especially in few-shot and zero-shot scenarios.</li>
</ul>

<h3>Title: WildFake: A Large-scale Challenging Dataset for AI-Generated Images  Detection</h3>
<ul>
<li><strong>Authors: </strong>Yan Hong, Jianfu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11843">https://arxiv.org/abs/2402.11843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11843">https://arxiv.org/pdf/2402.11843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11843]] WildFake: A Large-scale Challenging Dataset for AI-Generated Images  Detection(https://arxiv.org/abs/2402.11843)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from GANs, diffusion models, to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake's considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake's unique hierarchical structure.</li>
</ul>

<h3>Title: Modularized Networks for Few-shot Hateful Meme Detection</h3>
<ul>
<li><strong>Authors: </strong>Rui Cao, Roy Ka-Wei Lee, Jing Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11845">https://arxiv.org/abs/2402.11845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11845">https://arxiv.org/pdf/2402.11845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11845]] Modularized Networks for Few-shot Hateful Meme Detection(https://arxiv.org/abs/2402.11845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a few-shot learning context. The proposed method demonstrates superior performance to traditional in-context learning, which is also more computationally intensive during inference.</li>
</ul>

<h3>Title: UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning  for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11846">https://arxiv.org/abs/2402.11846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11846">https://arxiv.org/pdf/2402.11846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11846]] UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning  for Diffusion Models(https://arxiv.org/abs/2402.11846)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of diffusion models (DMs) has not only transformed various real-world industries but has also introduced negative societal concerns, including the generation of harmful content, copyright disputes, and the rise of stereotypes and biases. To mitigate these issues, machine unlearning (MU) has emerged as a potential solution, demonstrating its ability to remove undesired generative capabilities of DMs in various applications. However, by examining existing MU evaluation methods, we uncover several key challenges that can result in incomplete, inaccurate, or biased evaluations for MU in DMs. To address them, we enhance the evaluation metrics for MU, including the introduction of an often-overlooked retainability measurement for DMs post-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates us to evaluate the unlearning of artistic painting styles in conjunction with associated image objects. We show that this dataset plays a pivotal role in establishing a standardized and automated evaluation framework for MU techniques on DMs, featuring 7 quantitative metrics to address various aspects of unlearning effectiveness. Through extensive experiments, we benchmark 5 state-of-the-art MU methods, revealing novel insights into their pros and cons, and the underlying unlearning mechanisms. Furthermore, we demonstrate the potential of UnlearnCanvas to benchmark other generative modeling tasks, such as style transfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce all the results in this work can be found at https://github.com/OPTML-Group/UnlearnCanvas.</li>
</ul>

<h3>Title: ComFusion: Personalized Subject Generation in Multiple Specific Scenes  From Single Image</h3>
<ul>
<li><strong>Authors: </strong>Yan Hong, Jianfu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11849">https://arxiv.org/abs/2402.11849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11849">https://arxiv.org/pdf/2402.11849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11849]] ComFusion: Personalized Subject Generation in Multiple Specific Scenes  From Single Image(https://arxiv.org/abs/2402.11849)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in personalizing text-to-image (T2I) diffusion models have shown the capability to generate images based on personalized visual concepts using a limited number of user-provided examples. However, these models often struggle with maintaining high visual fidelity, particularly in manipulating scenes as defined by textual inputs. Addressing this, we introduce ComFusion, a novel approach that leverages pretrained models generating composition of a few user-provided subject images and predefined-text scenes, effectively fusing visual-subject instances with textual-specific scenes, resulting in the generation of high-fidelity instances within diverse scenes. ComFusion integrates a class-scene prior preservation regularization, which leverages composites the subject class and scene-specific knowledge from pretrained models to enhance generation fidelity. Additionally, ComFusion uses coarse generated images, ensuring they align effectively with both the instance image and scene texts. Consequently, ComFusion maintains a delicate balance between capturing the essence of the subject and maintaining scene fidelity.Extensive evaluations of ComFusion against various baselines in T2I personalization have demonstrated its qualitative and quantitative superiority.</li>
</ul>

<h3>Title: How Interpretable are Reasoning Explanations from Prompting Large  Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Yeo Wei Jie, Ranjan Satapathy, Goh Siow Mong, Rick, Erik Cambria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11863">https://arxiv.org/abs/2402.11863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11863">https://arxiv.org/pdf/2402.11863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11863]] How Interpretable are Reasoning Explanations from Prompting Large  Language Models?(https://arxiv.org/abs/2402.11863)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/wj210/CoT_interpretability</li>
</ul>

<h3>Title: LoRA Training in the NTK Regime has No Spurious Local Minima</h3>
<ul>
<li><strong>Authors: </strong>Uijeong Jang, Jason D. Lee, Ernest K. Ryu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11867">https://arxiv.org/abs/2402.11867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11867">https://arxiv.org/pdf/2402.11867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11867]] LoRA Training in the NTK Regime has No Spurious Local Minima(https://arxiv.org/abs/2402.11867)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.</li>
</ul>

<h3>Title: NOTE: Notable generation Of patient Text summaries through Efficient  approach based on direct preference optimization</h3>
<ul>
<li><strong>Authors: </strong>Imjin Ahn (1 and 2), Hansle Gwon (1 and 2), Young-Hak Kim (1 and 3), Tae Joon Jun (1 and 3), Sanghyun Park (2) ((1) INMED DATA, Seoul, Republic of Korea, (2) Yonsei University, Seoul, Republic of Korea (3) Asan Medical Center, Seoul, Republic of Korea)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11882">https://arxiv.org/abs/2402.11882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11882">https://arxiv.org/pdf/2402.11882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11882]] NOTE: Notable generation Of patient Text summaries through Efficient  approach based on direct preference optimization(https://arxiv.org/abs/2402.11882)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge. Providing a summary of the patient's progress is crucial, as it significantly influences future care and planning. Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary. Therefore, we propose "NOTE", which stands for "Notable generation Of patient Text summaries through an Efficient approach based on direct preference optimization". NOTE is based on Medical Information Mart for Intensive Care- III dataset and summarizes a single hospitalization of a patient. Patient events are sequentially combined and used to generate a discharge summary for each hospitalization. In the present circumstances, large language models' application programming interfaces (LLMs' APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions. Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital. Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior performance. To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software. In the future, we will aim to deploy the software available for actual use by clinicians in hospital. NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient's journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency.</li>
</ul>

<h3>Title: InMD-X: Large Language Models for Internal Medicine Doctors</h3>
<ul>
<li><strong>Authors: </strong>Hansle Gwon (1), Imjin Ahn (1), Hyoje Jung (2), Byeolhee Kim (2), Young-Hak Kim (3), Tae Joon Jun (4) ((1) INMED DATA, Seoul, Republic of Korea (2) Department of Information Medicine, Asan Medical Center, Seoul, Republic of Korea (3) Division of Cardiology, Department of Information Medicine, Asan Medical Center, University of Ulsan College of Medicine, Seoul, Republic of Korea (4) Big Data Research Center, Asan Institute for Life Sciences, Asan Medical Center, Seoul, Republic of Korea)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11883">https://arxiv.org/abs/2402.11883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11883">https://arxiv.org/pdf/2402.11883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11883]] InMD-X: Large Language Models for Internal Medicine Doctors(https://arxiv.org/abs/2402.11883)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce InMD-X, a collection of multiple large language models specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models fine-tuned for various aspects of the internal medicine field. These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation. InMD-X's versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research. Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical text analysis and decision support. This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information. We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios.</li>
</ul>

<h3>Title: The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional  Supporters for Queer Youth</h3>
<ul>
<li><strong>Authors: </strong>Shir Lissak, Nitay Calderon, Geva Shenkman, Yaakov Ophir, Eyal Fruchter, Anat Brunstein Klomek, Roi Reichart</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11886">https://arxiv.org/abs/2402.11886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11886">https://arxiv.org/pdf/2402.11886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11886]] The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional  Supporters for Queer Youth(https://arxiv.org/abs/2402.11886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.</li>
</ul>

<h3>Title: Generative Semi-supervised Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11887">https://arxiv.org/abs/2402.11887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11887">https://arxiv.org/pdf/2402.11887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11887]] Generative Semi-supervised Graph Anomaly Detection(https://arxiv.org/abs/2402.11887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and as a result, they fail to take account of the graph structure information. Our approach tackles this problem by generating graph structure-aware outlier nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space. Comprehensive experiments on four real-world datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes. Code will be made available at https://github.com/mala-lab/GGAD.</li>
</ul>

<h3>Title: ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large  Language Models with Reverse Prompt Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11889">https://arxiv.org/abs/2402.11889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11889">https://arxiv.org/pdf/2402.11889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11889]] ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large  Language Models with Reverse Prompt Contrastive Decoding(https://arxiv.org/abs/2402.11889)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs. In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it.</li>
</ul>

<h3>Title: Have Seen Me Before? Automating Dataset Updates Towards Reliable and  Timely Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Ying, Yixin Cao, Bo Wang, Wei Tang, Yizhe Yang, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11894">https://arxiv.org/abs/2402.11894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11894">https://arxiv.org/pdf/2402.11894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11894]] Have Seen Me Before? Automating Dataset Updates Towards Reliable and  Timely Evaluation(https://arxiv.org/abs/2402.11894)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poorly, we design an extending strategy that adjusts the difficulty of the generated samples according to varying cognitive levels. This not only makes our evaluation more systematic, but also, with a balanced difficulty, even discern model capabilities better at fine-grained levels.</li>
</ul>

<h3>Title: SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Wen, Jie Zhang, Yuan Fang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11896">https://arxiv.org/abs/2402.11896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11896">https://arxiv.org/pdf/2402.11896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11896]] SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2402.11896)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively.</li>
</ul>

<h3>Title: Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11900">https://arxiv.org/abs/2402.11900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11900">https://arxiv.org/pdf/2402.11900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11900]] Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large  Language Models(https://arxiv.org/abs/2402.11900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts.</li>
</ul>

<h3>Title: SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11903">https://arxiv.org/abs/2402.11903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11903">https://arxiv.org/pdf/2402.11903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11903]] SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning(https://arxiv.org/abs/2402.11903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurate solutions within polynomial loops. We evaluate the performance of SoLA on various datasets and empirically demonstrate its consistent outperformance against existing symbolic solvers (including Z3 and Kissat) and tool-learning methods in terms of efficiency in large-scale problem-solving.</li>
</ul>

<h3>Title: Learning to Edit: Aligning LLMs with Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11905">https://arxiv.org/abs/2402.11905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11905">https://arxiv.org/pdf/2402.11905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11905]] Learning to Edit: Aligning LLMs with Knowledge Editing(https://arxiv.org/abs/2402.11905)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of "Teach a man to fish." LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE's superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are available at https://github.com/YJiangcm/LTE.</li>
</ul>

<h3>Title: Direct Large Language Model Alignment Through Self-Rewarding Contrastive  Prompt Distillation</h3>
<ul>
<li><strong>Authors: </strong>Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, Lijie Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11907">https://arxiv.org/abs/2402.11907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11907">https://arxiv.org/pdf/2402.11907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11907]] Direct Large Language Model Alignment Through Self-Rewarding Contrastive  Prompt Distillation(https://arxiv.org/abs/2402.11907)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \texttt{RLHF} method without relying on human-annotated preference data.</li>
</ul>

<h3>Title: Semantic Textual Similarity Assessment in Chest X-ray Reports Using a  Domain-Specific Cosine-Based Metric</h3>
<ul>
<li><strong>Authors: </strong>Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11908">https://arxiv.org/abs/2402.11908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11908">https://arxiv.org/pdf/2402.11908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11908]] Semantic Textual Similarity Assessment in Chest X-ray Reports Using a  Domain-Specific Cosine-Based Metric(https://arxiv.org/abs/2402.11908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data. These multimodal data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes. The success of these models relies on the ability to extract and consolidate semantic information from clinical text. This paper addresses the need for more robust methods to evaluate the semantic content of medical reports. Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and machine translation, often failing to capture the complex semantic meanings inherent in medical content. In this study, we introduce a novel approach designed specifically for assessing the semantic similarity between generated medical reports and the ground truth. Our approach is validated, demonstrating its efficiency in assessing domain-specific semantic similarity within medical contexts. By applying our metric to state-of-the-art Chest X-ray report generation models, we obtain results that not only align with conventional metrics but also provide more contextually meaningful scores in the considered medical domain.</li>
</ul>

<h3>Title: One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11909">https://arxiv.org/abs/2402.11909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11909">https://arxiv.org/pdf/2402.11909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11909]] One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation(https://arxiv.org/abs/2402.11909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation.</li>
</ul>

<h3>Title: PhySU-Net: Long Temporal Context Transformer for rPPG with  Self-Supervised Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Marko Savic, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11913">https://arxiv.org/abs/2402.11913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11913">https://arxiv.org/pdf/2402.11913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11913]] PhySU-Net: Long Temporal Context Transformer for rPPG with  Self-Supervised Pre-training(https://arxiv.org/abs/2402.11913)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos. Most recent approaches utilize convolutional networks with limited temporal modeling capability or ignore long temporal context. Supervised rPPG methods are also severely limited by scarce data availability. In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG transformer network and a self-supervised pre-training strategy that exploits unlabeled data to improve our model. Our strategy leverages traditional methods and image masking to provide pseudo-labels for self-supervised pre-training. Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in supervised training. Furthermore, we demonstrate that our self-supervised pre-training strategy further improves our model's performance by leveraging representations learned from unlabeled data.</li>
</ul>

<h3>Title: A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step  Reasoning Task</h3>
<ul>
<li><strong>Authors: </strong>Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, Christian Bartelt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11917">https://arxiv.org/abs/2402.11917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11917">https://arxiv.org/pdf/2402.11917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11917]] A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step  Reasoning Task(https://arxiv.org/abs/2402.11917)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models.</li>
</ul>

<h3>Title: A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer  Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11922">https://arxiv.org/abs/2402.11922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11922">https://arxiv.org/pdf/2402.11922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11922]] A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer  Learning(https://arxiv.org/abs/2402.11922)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/PLUTO-SCY/GPDiff.</li>
</ul>

<h3>Title: MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition</h3>
<ul>
<li><strong>Authors: </strong>Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11924">https://arxiv.org/abs/2402.11924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11924">https://arxiv.org/pdf/2402.11924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11924]] MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition(https://arxiv.org/abs/2402.11924)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3\% right reasoning chain. We believe this new Multi-hop QA evaluation benchmark and novel evaluation methods will facilitate the development of trustworthy LLM evaluation on the MHQA task.</li>
</ul>

<h3>Title: DiLightNet: Fine-grained Lighting Control for Diffusion-based Image  Generation</h3>
<ul>
<li><strong>Authors: </strong>Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, Xin Tong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11929">https://arxiv.org/abs/2402.11929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11929">https://arxiv.org/pdf/2402.11929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11929]] DiLightNet: Fine-grained Lighting Control for Diffusion-based Image  Generation(https://arxiv.org/abs/2402.11929)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.</li>
</ul>

<h3>Title: AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11940">https://arxiv.org/abs/2402.11940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11940">https://arxiv.org/pdf/2402.11940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11940]] AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization(https://arxiv.org/abs/2402.11940)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchmark datasets with multiple victim models. The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output.</li>
</ul>

<h3>Title: Comprehensive Cognitive LLM Agent for Smartphone GUI Automation</h3>
<ul>
<li><strong>Authors: </strong>Xinbei Ma, Zhuosheng Zhang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11941">https://arxiv.org/abs/2402.11941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11941">https://arxiv.org/pdf/2402.11941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11941]] Comprehensive Cognitive LLM Agent for Smartphone GUI Automation(https://arxiv.org/abs/2402.11941)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios.</li>
</ul>

<h3>Title: LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with  External Knowledge Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Keyang Xuan, Li Yi, Fan Yang, Ruochen Wu, Yi R. Fung, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11943">https://arxiv.org/abs/2402.11943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11943">https://arxiv.org/pdf/2402.11943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11943]] LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with  External Knowledge Augmentation(https://arxiv.org/abs/2402.11943)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively.</li>
</ul>

<h3>Title: Stealing the Invisible: Unveiling Pre-Trained CNN Models through  Adversarial Examples and Timing Side-Channels</h3>
<ul>
<li><strong>Authors: </strong>Shubhi Shukla, Manaar Alam, Pabitra Mitra, Debdeep Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11953">https://arxiv.org/abs/2402.11953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11953">https://arxiv.org/pdf/2402.11953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11953]] Stealing the Invisible: Unveiling Pre-Trained CNN Models through  Adversarial Examples and Timing Side-Channels(https://arxiv.org/abs/2402.11953)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal, transformer</a></li>
<li><strong>Abstract: </strong>Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned Convolutional Neural Network (CNN) and Vision Transformer (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different CNN and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20.</li>
</ul>

<h3>Title: Automatic Evaluation for Mental Health Counseling using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Anqi Li, Yu Lu, Nirui Song, Shuai Zhang, Lizhi Ma, Zhenzhong Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11958">https://arxiv.org/abs/2402.11958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11958">https://arxiv.org/pdf/2402.11958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11958]] Automatic Evaluation for Mental Health Counseling using LLMs(https://arxiv.org/abs/2402.11958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming. To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of LLMs as supervisory tools for psychotherapists. By integrating LLMs into the evaluation process, our approach offers a cost-effective and dependable means of assessing counseling quality, enhancing overall effectiveness.</li>
</ul>

<h3>Title: DB-LLM: Accurate Dual-Binarization for Efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11960">https://arxiv.org/abs/2402.11960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11960">https://arxiv.org/pdf/2402.11960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11960]] DB-LLM: Accurate Dual-Binarization for Efficient LLMs(https://arxiv.org/abs/2402.11960)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while retaining the inherent high sparsity of ultra-low bit quantization. For the macro-level, we find the distortion that exists in the prediction of LLM after quantization, which is specified as the deviations related to the ambiguity of samples. We propose the Deviation-Aware Distillation (DAD) method, enabling the model to focus differently on various samples. Comprehensive experiments show that our DB-LLM not only significantly surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization (eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional 20\% reduction in computational consumption compared to the SOTA method under the same bit-width. Our code will be released soon.</li>
</ul>

<h3>Title: Weakly Supervised Object Detection in Chest X-Rays with Differentiable  ROI Proposal Networks and Soft ROI Pooling</h3>
<ul>
<li><strong>Authors: </strong>Philip M√ºller, Felix Meissen, Georgios Kaissis, Daniel Rueckert</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11985">https://arxiv.org/abs/2402.11985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11985">https://arxiv.org/pdf/2402.11985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11985]] Weakly Supervised Object Detection in Chest X-Rays with Differentiable  ROI Proposal Networks and Soft ROI Pooling(https://arxiv.org/abs/2402.11985)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn</li>
</ul>

<h3>Title: Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11989">https://arxiv.org/abs/2402.11989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11989">https://arxiv.org/pdf/2402.11989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11989]] Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models(https://arxiv.org/abs/2402.11989)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, diffusion</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.</li>
</ul>

<h3>Title: Network Inversion of Binarised Neural Nets</h3>
<ul>
<li><strong>Authors: </strong>Pirzada Suhail, Supratik Chakraborty, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11995">https://arxiv.org/abs/2402.11995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11995">https://arxiv.org/pdf/2402.11995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11995]] Network Inversion of Binarised Neural Nets(https://arxiv.org/abs/2402.11995)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model's learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous "garbage" to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network's structure, allowing for both inference and inversion.</li>
</ul>

<h3>Title: ISCUTE: Instance Segmentation of Cables Using Text Embedding</h3>
<ul>
<li><strong>Authors: </strong>Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11996">https://arxiv.org/abs/2402.11996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11996">https://arxiv.org/pdf/2402.11996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11996]] ISCUTE: Instance Segmentation of Cables Using Text Embedding(https://arxiv.org/abs/2402.11996)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.</li>
</ul>

<h3>Title: Remember This Event That Year? Assessing Temporal Information and  Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Beniwal, Kowsik Nandagopan D, Mayank Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.11997">https://arxiv.org/abs/2402.11997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.11997">https://arxiv.org/pdf/2402.11997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.11997]] Remember This Event That Year? Assessing Temporal Information and  Reasoning in Large Language Models(https://arxiv.org/abs/2402.11997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).</li>
</ul>

<h3>Title: Direct Consistency Optimization for Compositional Text-to-Image  Personalization</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12004">https://arxiv.org/abs/2402.12004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12004">https://arxiv.org/pdf/2402.12004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12004]] Direct Consistency Optimization for Compositional Text-to-Image  Personalization(https://arxiv.org/abs/2402.12004)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( https://dco-t2i.github.io/).</li>
</ul>

<h3>Title: Distilling Large Language Models for Text-Attributed Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12022">https://arxiv.org/abs/2402.12022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12022">https://arxiv.org/pdf/2402.12022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12022]] Distilling Large Language Models for Text-Attributed Graph Learning(https://arxiv.org/abs/2402.12022)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed framework.</li>
</ul>

<h3>Title: Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on  Chain of Thought</h3>
<ul>
<li><strong>Authors: </strong>Yuying Du, Xueyan Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12023">https://arxiv.org/abs/2402.12023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12023">https://arxiv.org/pdf/2402.12023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12023]] Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on  Chain of Thought(https://arxiv.org/abs/2402.12023)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Smart contracts, as a key component of blockchain technology, play a crucial role in ensuring the automation of transactions and adherence to protocol rules. However, smart contracts are susceptible to security vulnerabilities, which, if exploited, can lead to significant asset losses. This study explores the potential of enhancing smart contract security audits using the GPT-4 model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate GPT-4's ability to identify seven common types of vulnerabilities. Moreover, we assessed GPT-4's performance in code parsing and vulnerability capture by simulating a professional auditor's auditing process using CoT(Chain of Thought) prompts based on the audit reports of eight groups of smart contracts. We also evaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through experimentation, we found that GPT-4 performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection. Meanwhile, it demonstrated good contract code parsing capabilities, with an average comprehensive score of 6.5, capable of identifying the background information and functional relationships of smart contracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4 has significant potential application in PoC writing. These experimental results indicate that GPT-4 lacks the ability to detect smart contract vulnerabilities effectively, but its performance in contract code parsing and PoC writing demonstrates its significant potential as an auxiliary tool in enhancing the efficiency and effectiveness of smart contract security audits.</li>
</ul>

<h3>Title: Speech Translation with Speech Foundation Models and Large Language  Models: What is There and What is Missing?</h3>
<ul>
<li><strong>Authors: </strong>Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12025">https://arxiv.org/abs/2402.12025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12025">https://arxiv.org/pdf/2402.12025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12025]] Speech Translation with Speech Foundation Models and Large Language  Models: What is There and What is Missing?(https://arxiv.org/abs/2402.12025)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.</li>
</ul>

<h3>Title: Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space</h3>
<ul>
<li><strong>Authors: </strong>Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12026">https://arxiv.org/abs/2402.12026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12026">https://arxiv.org/pdf/2402.12026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12026]] Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space(https://arxiv.org/abs/2402.12026)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are available at https://github.com/ZrW00/MuScleLoRA.</li>
</ul>

<h3>Title: Towards Cross-Tokenizer Distillation: the Universal Logit Distillation  Loss for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Boizard, Kevin El-Haddad, C√©line Hudelot, Pierre Colombo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12030">https://arxiv.org/abs/2402.12030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12030">https://arxiv.org/pdf/2402.12030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12030]] Towards Cross-Tokenizer Distillation: the Universal Logit Distillation  Loss for LLMs(https://arxiv.org/abs/2402.12030)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread use of distillation techniques.</li>
</ul>

<h3>Title: Class-incremental Learning for Time Series: Benchmark and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H Le, P.N.Suganthan, Xudong Jiang, Ramasamy Savitha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12035">https://arxiv.org/abs/2402.12035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12035">https://arxiv.org/pdf/2402.12035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12035]] Class-incremental Learning for Time Series: Benchmark and Evaluation(https://arxiv.org/abs/2402.12035)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the advanced methodologies. Further, based on standardized settings, we develop a unified experimental framework that supports the rapid development of new algorithms, easy integration of new datasets, and standardization of the evaluation process. Using this framework, we conduct a comprehensive evaluation of various generic and time-series-specific CIL methods in both standard and privacy-sensitive scenarios. Our extensive experiments not only provide a standard baseline to support future research but also shed light on the impact of various design factors such as normalization layers or memory budget thresholds. Codes are available at https://github.com/zqiao11/TSCIL.</li>
</ul>

<h3>Title: Self-AMPLIFY: Improving Small Language Models with Self Post Hoc  Explanations</h3>
<ul>
<li><strong>Authors: </strong>Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12038">https://arxiv.org/abs/2402.12038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12038">https://arxiv.org/pdf/2402.12038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12038]] Self-AMPLIFY: Improving Small Language Models with Self Post Hoc  Explanations(https://arxiv.org/abs/2402.12038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a fully automated manner.</li>
</ul>

<h3>Title: Attack Tree Generation via Process Mining</h3>
<ul>
<li><strong>Authors: </strong>Alyzia-Maria Konsta, Gemma Di Federico, Alberto Lluch Lafuente, Andrea Burattin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12040">https://arxiv.org/abs/2402.12040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12040">https://arxiv.org/pdf/2402.12040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12040]] Attack Tree Generation via Process Mining(https://arxiv.org/abs/2402.12040)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Attack Trees are a graphical model of security used to study threat scenarios. While visually appealing and supported by solid theories and effective tools, one of their main drawbacks remains the amount of effort required by security experts to design them from scratch. This work aims to remedy this by providing a method for the automatic generation of Attack Trees from attack logs. The main original feature of our approach w.r.t existing ones is the use of Process Mining algorithms to synthesize Attack Trees, which allow users to customize the way a set of logs are summarized as an Attack Tree, for example by discarding statistically irrelevant events. Our approach is supported by a prototype that, apart from the derivation and translation of the model, provides the user with an Attack Tree in the RisQFLan format, a tool used for quantitative risk modeling and analysis with Attack Trees. We illustrate our approach with the case study of attacks on a communication protocol, produced by a state-of-the-art protocol analyzer.</li>
</ul>

<h3>Title: A Lightweight Parallel Framework for Blind Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Qunyue Huang, Bin Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12043">https://arxiv.org/abs/2402.12043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12043">https://arxiv.org/pdf/2402.12043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12043]] A Lightweight Parallel Framework for Blind Image Quality Assessment(https://arxiv.org/abs/2402.12043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on convolutional neural networks (CNNs) or transformer. In addition, some BIQA methods enhance the performance of the model in a two-stage training manner. Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources. To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA. First, we extract the visual features using a pre-trained feature extraction network. Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information. To improve the robustness of the latent representations, we present two novel self-supervised subtasks, including a sample-level category prediction task and a batch-level quality comparison task. The sample-level category prediction task is presented to help the model with coarse-grained distortion perception. The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations. Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches. Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed.</li>
</ul>

<h3>Title: Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, Chao Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12048">https://arxiv.org/abs/2402.12048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12048">https://arxiv.org/pdf/2402.12048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12048]] Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large  Language Models(https://arxiv.org/abs/2402.12048)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\leq$ 10\%) of fine-tuned parameters, maintaining $\sim$ 99\% effectiveness on original tasks versus pre-training, and achieving $\sim$ 97\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the "model patch", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to "decorate the patch", enhancing the model's performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.</li>
</ul>

<h3>Title: Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When  and What to Retrieve for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12052">https://arxiv.org/abs/2402.12052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12052">https://arxiv.org/pdf/2402.12052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12052]] Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When  and What to Retrieve for LLMs(https://arxiv.org/abs/2402.12052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.</li>
</ul>

<h3>Title: Are LLM-based Evaluators Confusing NLG Quality Criteria?</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12055">https://arxiv.org/abs/2402.12055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12055">https://arxiv.org/pdf/2402.12055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12055]] Are LLM-based Evaluators Confusing NLG Quality Criteria?(https://arxiv.org/abs/2402.12055)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further research and improvements for LLM-based evaluation.</li>
</ul>

<h3>Title: WKVQuant: Quantizing Weight and Key/Value Cache for Large Language  Models Gains More</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12065">https://arxiv.org/abs/2402.12065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12065">https://arxiv.org/pdf/2402.12065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12065]] WKVQuant: Quantizing Weight and Key/Value Cache for Large Language  Models Gains More(https://arxiv.org/abs/2402.12065)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization.</li>
</ul>

<h3>Title: EmoBench: Evaluating the Emotional Intelligence of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Juanzi Li, Tatia M.C. Lee, Rada Mihalcea, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12071">https://arxiv.org/abs/2402.12071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12071">https://arxiv.org/pdf/2402.12071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12071]] EmoBench: Evaluating the Emotional Intelligence of Large Language Models(https://arxiv.org/abs/2402.12071)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data will be publicly available from https://github.com/Sahandfer/EmoBench.</li>
</ul>

<h3>Title: LVCHAT: Facilitating Long Video Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12079">https://arxiv.org/abs/2402.12079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12079">https://arxiv.org/pdf/2402.12079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12079]] LVCHAT: Facilitating Long Video Comprehension(https://arxiv.org/abs/2402.12079)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LVChat significantly outperforms existing methods by up to 27\% in accuracy on long-video QA datasets and long-video captioning benchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</li>
</ul>

<h3>Title: Can LLMs Compute with Reasons?</h3>
<ul>
<li><strong>Authors: </strong>Harshit Sandilya, Peehu Raj, Jainit Sushil Bafna, Srija Mukhopadhyay, Shivansh Sharma, Ellwil Sharma, Arastu Sharma, Neeta Trivedi, Manish Shrivastava, Rajesh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12080">https://arxiv.org/abs/2402.12080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12080">https://arxiv.org/pdf/2402.12080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12080]] Can LLMs Compute with Reasons?(https://arxiv.org/abs/2402.12080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often struggle with complex mathematical tasks, prone to "hallucinating" incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data. To address this challenge, we propose an "Inductive Learning" approach utilizing a distributed network of SLMs. This network leverages error-based learning and hint incorporation to refine the reasoning capabilities of SLMs. Our goal is to provide a framework that empowers SLMs to approach the level of logic-based applications achieved by high-parameter models, potentially benefiting any language model. Ultimately, this novel concept paves the way for bridging the logical gap between humans and LLMs across various fields.</li>
</ul>

<h3>Title: Do Large Language Models Understand Logic or Just Mimick Context?</h3>
<ul>
<li><strong>Authors: </strong>Junbing Yan, Chengyu Wang, Jun Huang, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12091">https://arxiv.org/abs/2402.12091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12091">https://arxiv.org/pdf/2402.12091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12091]] Do Large Language Models Understand Logic or Just Mimick Context?(https://arxiv.org/abs/2402.12091)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses. This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs.</li>
</ul>

<h3>Title: Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient  Based Target Localization</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Kuriyal, Vaibhav Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12098">https://arxiv.org/abs/2402.12098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12098">https://arxiv.org/pdf/2402.12098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12098]] Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient  Based Target Localization(https://arxiv.org/abs/2402.12098)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving. While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge. This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers. Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point. This allows us to better understand how SS models make their predictions and identify potential areas for improvement. Relevant codes are available at https://github.com/geoai4cities/pGS-CAM.</li>
</ul>

<h3>Title: Human Video Translation via Query Warping</h3>
<ul>
<li><strong>Authors: </strong>Haiming Zhu, Yangyang Xu, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12099">https://arxiv.org/abs/2402.12099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12099">https://arxiv.org/pdf/2402.12099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12099]] Human Video Translation via Query Warping(https://arxiv.org/abs/2402.12099)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present QueryWarp, a novel framework for temporally coherent human motion video translation. Existing diffusion-based video editing approaches that rely solely on key and value tokens to ensure temporal consistency, which scarifies the preservation of local and structural regions. In contrast, we aim to consider complementary query priors by constructing the temporal correlations among query tokens from different frames. Initially, we extract appearance flows from source poses to capture continuous human foreground motion. Subsequently, during the denoising process of the diffusion model, we employ appearance flows to warp the previous frame's query token, aligning it with the current frame's query. This query warping imposes explicit constraints on the outputs of self-attention layers, effectively guaranteeing temporally coherent translation. We perform experiments on various human motion video translation tasks, and the results demonstrate that our QueryWarp framework surpasses state-of-the-art methods both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Groot: Adversarial Testing for Generative Text-to-Image Models with  Tree-based Semantic Transformation</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Guowei Yang, Gelei Deng, Feiyue Chen, Yuqi Chen, Ling Shi, Tianwei Zhang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12100">https://arxiv.org/abs/2402.12100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12100">https://arxiv.org/pdf/2402.12100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12100]] Groot: Adversarial Testing for Generative Text-to-Image Models with  Tree-based Semantic Transformation(https://arxiv.org/abs/2402.12100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney.</li>
</ul>

<h3>Title: Is It a Free Lunch for Removing Outliers during Pretraining?</h3>
<ul>
<li><strong>Authors: </strong>Baohao Liao, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12102">https://arxiv.org/abs/2402.12102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12102">https://arxiv.org/pdf/2402.12102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12102]] Is It a Free Lunch for Removing Outliers during Pretraining?(https://arxiv.org/abs/2402.12102)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models.</li>
</ul>

<h3>Title: 3D Vascular Segmentation Supervised by 2D Annotation of Maximum  Intensity Projection</h3>
<ul>
<li><strong>Authors: </strong>Zhanqiang Guo, Zimeng Tan, Jianjiang Feng, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12128">https://arxiv.org/abs/2402.12128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12128">https://arxiv.org/pdf/2402.12128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12128]] 3D Vascular Segmentation Supervised by 2D Annotation of Maximum  Intensity Projection(https://arxiv.org/abs/2402.12128)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully supervised segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of weakly-supervised approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly supervised methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by fine-tuning the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: https://github.com/gzq17/Weakly-Supervised-by-MIP.</li>
</ul>

<h3>Title: Perceiving Longer Sequences With Bi-Directional Cross-Attention  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Markus Hiller, Krista A. Ehinger, Tom Drummond</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12138">https://arxiv.org/abs/2402.12138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12138">https://arxiv.org/pdf/2402.12138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12138]] Perceiving Longer Sequences With Bi-Directional Cross-Attention  Transformers(https://arxiv.org/abs/2402.12138)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We present a novel bi-directional Transformer architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (`what') and location (`where') to develop alongside each other over multiple layers -- allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full Transformer architecture, BiXT can process longer sequences like point clouds or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation and image classification.</li>
</ul>

<h3>Title: Federated Bayesian Network Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Florian van Daalen, Lianne Ippel, Andre Dekker, Inigo Bermejo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12142">https://arxiv.org/abs/2402.12142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12142">https://arxiv.org/pdf/2402.12142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12142]] Federated Bayesian Network Ensembles(https://arxiv.org/abs/2402.12142)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, interpretability</a></li>
<li><strong>Abstract: </strong>Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location. In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federated learning toolbox, especially when local populations are heavily biased, or there is a strong imbalance in population size across parties. We discuss the advantages and disadvantages of this approach in terms of time complexity, model accuracy, privacy protection, and model interpretability.</li>
</ul>

<h3>Title: Meta Ranking: Less Capable Language Models are Capable for Single  Response Judgement</h3>
<ul>
<li><strong>Authors: </strong>Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12146">https://arxiv.org/abs/2402.12146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12146">https://arxiv.org/pdf/2402.12146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12146]] Meta Ranking: Less Capable Language Models are Capable for Single  Response Judgement(https://arxiv.org/abs/2402.12146)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be used to enhance the performance of LLMs in two practical applications: query routing and iterative training data filtering. The former achieves GPT-4-turbo comparable performance with less than half the token consumption, while the latter makes the instruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpass Alpaca-13B over fewer training samples, underscoring the high potential of our proposed method.</li>
</ul>

<h3>Title: End-to-end multilingual fact-checking at scale</h3>
<ul>
<li><strong>Authors: </strong>Vinay Setty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12147">https://arxiv.org/abs/2402.12147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12147">https://arxiv.org/pdf/2402.12147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12147]] End-to-end multilingual fact-checking at scale(https://arxiv.org/abs/2402.12147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.</li>
</ul>

<h3>Title: MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore  the Momentum in Competitive Sports</h3>
<ul>
<li><strong>Authors: </strong>Ruixin Peng, Ziqing Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12149">https://arxiv.org/abs/2402.12149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12149">https://arxiv.org/pdf/2402.12149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12149]] MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore  the Momentum in Competitive Sports(https://arxiv.org/abs/2402.12149)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum. This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches. Based on the tennis Grand Slam men's singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas. For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players. Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established. For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different methods were used to visualize the momentum. For further analysis of the momentum fluctuation, it is based on the popular CUMSUM algorithm in the industry as well as the RUN Test, and the result shows the momentum is not random and the trend might be random. At last, the robustness of the fusion model is analyzed by Monte Carlo simulation.</li>
</ul>

<h3>Title: Your Large Language Model is Secretly a Fairness Proponent and You  Should Prompt it Like One</h3>
<ul>
<li><strong>Authors: </strong>Tianlin Li, Xiaoyu Zhang, Chao Du, Tianyu Pang, Qian Liu, Qing Guo, Chao Shen, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12150">https://arxiv.org/abs/2402.12150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12150">https://arxiv.org/pdf/2402.12150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12150]] Your Large Language Model is Secretly a Fairness Proponent and You  Should Prompt it Like One(https://arxiv.org/abs/2402.12150)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performance.</li>
</ul>

<h3>Title: Transformer-based Causal Language Models Perform Clustering</h3>
<ul>
<li><strong>Authors: </strong>Xinbo Wu, Lav R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12151">https://arxiv.org/abs/2402.12151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12151">https://arxiv.org/pdf/2402.12151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12151]] Transformer-based Causal Language Models Perform Clustering(https://arxiv.org/abs/2402.12151)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.</li>
</ul>

<h3>Title: Endowing Pre-trained Graph Models with Provable Fairness</h3>
<ul>
<li><strong>Authors: </strong>Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu, Chuan Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12161">https://arxiv.org/abs/2402.12161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12161">https://arxiv.org/pdf/2402.12161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12161]] Endowing Pre-trained Graph Models with Provable Fairness(https://arxiv.org/abs/2402.12161)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the fairness of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on node representations, to extend the node representations with different sensitive attribute semantics for each node. The extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the fairness of each node is provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and fairness on node classification task. Furthermore, based on our GraphPAR, around 90\% nodes have provable fairness.</li>
</ul>

<h3>Title: SCARF: Securing Chips with a Robust Framework against Fabrication-time  Hardware Trojans</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Eslami, Tara Ghasempouri, Samuel Pagliarini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12162">https://arxiv.org/abs/2402.12162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12162">https://arxiv.org/pdf/2402.12162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12162]] SCARF: Securing Chips with a Robust Framework against Fabrication-time  Hardware Trojans(https://arxiv.org/abs/2402.12162)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, robust</a></li>
<li><strong>Abstract: </strong>The globalization of the semiconductor industry has introduced security challenges to Integrated Circuits (ICs), particularly those related to the threat of Hardware Trojans (HTs) - malicious logic that can be introduced during IC fabrication. While significant efforts are directed towards verifying the correctness and reliability of ICs, their security is often overlooked. In this paper, we propose a comprehensive approach to enhance IC security from the front-end to back-end stages of design. Initially, we outline a systematic method to transform existing verification assets into potent security checkers by repurposing verification assertions. To further improve security, we introduce an innovative technique for integrating online monitors during physical synthesis - a back-end insertion providing an additional layer of defense. Experimental results demonstrate a significant increase in security, measured by our introduced metric, Security Coverage (SC), with a marginal rise in area and power consumption, typically under 20\%. The insertion of online monitors during physical synthesis enhances security metrics by up to 33.5\%. This holistic approach offers a comprehensive and resilient defense mechanism across the entire spectrum of IC design.</li>
</ul>

<h3>Title: Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12168">https://arxiv.org/abs/2402.12168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12168">https://arxiv.org/pdf/2402.12168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12168]] Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2402.12168)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.</li>
</ul>

<h3>Title: Unsupervised LLM Adaptation for Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12170">https://arxiv.org/abs/2402.12170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12170">https://arxiv.org/pdf/2402.12170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12170]] Unsupervised LLM Adaptation for Question Answering(https://arxiv.org/abs/2402.12170)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers for questions about the target domain even though they do not see any questions about the information described in the unlabeled documents, but (ii) they have difficulties in accessing information located in the middle or at the end of documents, and (iii) this challenge can be partially mitigated by replacing input tokens with random ones during adaptation.</li>
</ul>

<h3>Title: BIDER: Bridging Knowledge Inconsistency for Efficient  Retrieval-Augmented LLMs via Key Supporting Evidence</h3>
<ul>
<li><strong>Authors: </strong>Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12174">https://arxiv.org/abs/2402.12174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12174">https://arxiv.org/pdf/2402.12174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12174]] BIDER: Bridging Knowledge Inconsistency for Efficient  Retrieval-Augmented LLMs via Key Supporting Evidence(https://arxiv.org/abs/2402.12174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential information for accurate question answering.</li>
</ul>

<h3>Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12177">https://arxiv.org/abs/2402.12177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12177">https://arxiv.org/pdf/2402.12177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12177]] Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning(https://arxiv.org/abs/2402.12177)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.</li>
</ul>

<h3>Title: ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for  Complicated Chart Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12185">https://arxiv.org/abs/2402.12185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12185">https://arxiv.org/pdf/2402.12185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12185]] ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for  Complicated Chart Reasoning(https://arxiv.org/abs/2402.12185)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, many versatile Multi-modal Large Language Models (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in reasoning based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously benchmark the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling multi-modal tasks that strongly depend on interpretable patterns, such as reasoning tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related large models, achieving results comparable to GPT-4V. We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable multi-modal models. Both ChartX and ChartVLM are available at: https://github.com/UniModal4Reasoning/ChartVLM</li>
</ul>

<h3>Title: Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep  Learning via Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12187">https://arxiv.org/abs/2402.12187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12187">https://arxiv.org/pdf/2402.12187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12187]] Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep  Learning via Adversarial Training(https://arxiv.org/abs/2402.12187)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algorithm based on contrastive learning to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous adversarial contrastive learning methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by data augmentation using a recent diffusion model, achieves state-of-the-art accuracy and robustness.</li>
</ul>

<h3>Title: Amplifying Training Data Exposure through Fine-Tuning with  Pseudo-Labeled Memberships</h3>
<ul>
<li><strong>Authors: </strong>Myung Gyo Oh, Hong Eun Ahn, Leo Hyun Park, Taekyoung Kwon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12189">https://arxiv.org/abs/2402.12189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12189">https://arxiv.org/pdf/2402.12189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12189]] Amplifying Training Data Exposure through Fine-Tuning with  Pseudo-Labeled Memberships(https://arxiv.org/abs/2402.12189)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction</a></li>
<li><strong>Abstract: </strong>Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions.</li>
</ul>

<h3>Title: A Chinese Dataset for Evaluating the Safeguards in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxia Wang, Zenan Zhai, Haonan Li, Xudong Han, Lizhi Lin, Zhenxuan Zhang, Jingru Zhao, Preslav Nakov, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12193">https://arxiv.org/abs/2402.12193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12193">https://arxiv.org/pdf/2402.12193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12193]] A Chinese Dataset for Evaluating the Safeguards in Large Language Models(https://arxiv.org/abs/2402.12193)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with. Warning: this paper contains example data that may be offensive, harmful, or biased.</li>
</ul>

<h3>Title: Browse and Concentrate: Comprehending Multimodal Content via prior-LLM  Context Fusion</h3>
<ul>
<li><strong>Authors: </strong>Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12195">https://arxiv.org/abs/2402.12195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12195">https://arxiv.org/pdf/2402.12195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12195]] Browse and Concentrate: Comprehending Multimodal Content via prior-LLM  Context Fusion(https://arxiv.org/abs/2402.12195)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially "browses" through the inputs for essential insights, and then revisits the inputs to "concentrate" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the multimodal inputs. Additionally, we develop training strategies specifically to enhance the understanding of multi-image inputs. Our method markedly boosts the performance on 7 multi-image scenarios, contributing to increments on average accuracy by 2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs, respectively.</li>
</ul>

<h3>Title: Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic  Interpretability: A Case Study on Othello-GPT</h3>
<ul>
<li><strong>Authors: </strong>Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12201">https://arxiv.org/abs/2402.12201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12201">https://arxiv.org/pdf/2402.12201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12201]] Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic  Interpretability: A Case Study on Othello-GPT(https://arxiv.org/abs/2402.12201)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.</li>
</ul>

<h3>Title: Enhancing Multilingual Capabilities of Large Language Models through  Self-Distillation from Resource-Rich Languages</h3>
<ul>
<li><strong>Authors: </strong>Yuanchi Zhang, Yile Wang, Zijun Liu, Shuo Wang, Xiaolong Wang, Peng Li, Maosong Sun, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12204">https://arxiv.org/abs/2402.12204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12204">https://arxiv.org/pdf/2402.12204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12204]] Enhancing Multilingual Capabilities of Large Language Models through  Self-Distillation from Resource-Rich Languages(https://arxiv.org/abs/2402.12204)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.</li>
</ul>

<h3>Title: Polarization of Autonomous Generative AI Agents Under Echo Chambers</h3>
<ul>
<li><strong>Authors: </strong>Masaya Ohagi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12212">https://arxiv.org/abs/2402.12212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12212">https://arxiv.org/pdf/2402.12212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12212]] Polarization of Autonomous Generative AI Agents Under Echo Chambers(https://arxiv.org/abs/2402.12212)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group's opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is caused by ChatGPT's high prompt understanding ability to update its opinion by considering its own and surrounding agents' opinions. We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize. As a result, we identified factors that strongly influence polarization, such as the agent's persona. These factors should be monitored to prevent the polarization of AI agents.</li>
</ul>

<h3>Title: Reformatted Alignment</h3>
<ul>
<li><strong>Authors: </strong>Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12219">https://arxiv.org/abs/2402.12219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12219">https://arxiv.org/pdf/2402.12219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12219]] Reformatted Alignment(https://arxiv.org/abs/2402.12219)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs. Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.</li>
</ul>

<h3>Title: CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement  Learning for LLM-based Mutation</h3>
<ul>
<li><strong>Authors: </strong>Jueon Eom, Seyeon Jeong, Taekyoung Kwon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12222">https://arxiv.org/abs/2402.12222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12222">https://arxiv.org/pdf/2402.12222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12222]] CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement  Learning for LLM-based Mutation(https://arxiv.org/abs/2402.12222)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving vulnerability detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results indicate that CovRL-Fuzz outperforms the state-of-the-art fuzzers in terms of code coverage and bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related bugs in the latest JavaScript engines, including 39 previously unknown vulnerabilities and 11 CVEs.</li>
</ul>

<h3>Title: AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12226">https://arxiv.org/abs/2402.12226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12226">https://arxiv.org/pdf/2402.12226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12226]] AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling(https://arxiv.org/abs/2402.12226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/</li>
</ul>

<h3>Title: Diffusion Tempering Improves Parameter Estimation with Probabilistic  Integrators for Ordinary Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Jonas Beck, Nathanael Bosch, Michael Deistler, Kyra L. Kadhim, Jakob H. Macke, Philipp Hennig, Philipp Berens</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12231">https://arxiv.org/abs/2402.12231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12231">https://arxiv.org/pdf/2402.12231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12231]] Diffusion Tempering Improves Parameter Estimation with Probabilistic  Integrators for Ordinary Differential Equations(https://arxiv.org/abs/2402.12231)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.</li>
</ul>

<h3>Title: Empirical Study on Updating Key-Value Memories in Transformer  Feed-forward Layers</h3>
<ul>
<li><strong>Authors: </strong>Zihan Qiu, Zeyu Huang, Youcheng Huang, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12233">https://arxiv.org/abs/2402.12233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12233">https://arxiv.org/pdf/2402.12233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12233]] Empirical Study on Updating Key-Value Memories in Transformer  Feed-forward Layers(https://arxiv.org/abs/2402.12233)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further. Code is available at $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\,repo}$.</li>
</ul>

<h3>Title: Task-Oriented Dialogue with In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tom Bocklisch, Thomas Werkmeister, Daksh Varshneya, Alan Nichol</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12234">https://arxiv.org/abs/2402.12234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12234">https://arxiv.org/pdf/2402.12234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12234]] Task-Oriented Dialogue with In-Context Learning(https://arxiv.org/abs/2402.12234)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We describe a system for building task-oriented dialogue systems combining the in-context learning abilities of large language models (LLMs) with the deterministic execution of business logic. LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling task-oriented dialogue systems to a large number of tasks. We make our implementation available for use and further study.</li>
</ul>

<h3>Title: Mixed Gaussian Flow for Diverse Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Chen, Jinkun Cao, Dahua Lin, Kris Kitani, Jiangmiao Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12238">https://arxiv.org/abs/2402.12238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12238">https://arxiv.org/pdf/2402.12238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12238]] Mixed Gaussian Flow for Diverse Trajectory Prediction(https://arxiv.org/abs/2402.12238)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and out-of-distribution trajectories. Code is available at https://github.com/mulplue/MGF.</li>
</ul>

<h3>Title: Synthetic location trajectory generation using categorical diffusion  models</h3>
<ul>
<li><strong>Authors: </strong>Simon Dirmeier, Ye Hong, Fernando Perez-Cruz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12242">https://arxiv.org/abs/2402.12242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12242">https://arxiv.org/pdf/2402.12242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12242]] Synthetic location trajectory generation using categorical diffusion  models(https://arxiv.org/abs/2402.12242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation. Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals. ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making. We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space. We demonstrate that our model can synthesize realistic ILPs by comparing conditionally and unconditionally generated sequences to real-world ILPs from a GNSS tracking data set which suggests the potential use of our model for synthetic data generation, for example, for benchmarking models used in mobility research.</li>
</ul>

<h3>Title: Analysis of Levenshtein Transformer's Decoder and Its Variants</h3>
<ul>
<li><strong>Authors: </strong>Ruiyang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12249">https://arxiv.org/abs/2402.12249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12249">https://arxiv.org/pdf/2402.12249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12249]] Analysis of Levenshtein Transformer's Decoder and Its Variants(https://arxiv.org/abs/2402.12249)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Levenshtein transformer (LevT) is a non-autoregressive machine translation model with high decoding efficiency and comparable translation quality in terms of bleu score, due to its parallel decoding and iterative refinement procedure. Are there any deficiencies of its translations and what improvements could be made? In this report, we focus on LevT's decoder and analyse the decoding results length, subword generation, and deletion module's capability. We hope to identify weaknesses of the decoder for future improvements. We also compare translations of the original LevT, knowledge-distilled LevT, LevT with translation memory, and the KD-LevT with translation memory to see how KD and translation memory can help.</li>
</ul>

<h3>Title: An Interview Study on Third-Party Cyber Threat Hunting Processes in the  U.S. Department of Homeland Security</h3>
<ul>
<li><strong>Authors: </strong>William P. Maxam III, James C. Davis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12252">https://arxiv.org/abs/2402.12252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12252">https://arxiv.org/pdf/2402.12252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12252]] An Interview Study on Third-Party Cyber Threat Hunting Processes in the  U.S. Department of Homeland Security(https://arxiv.org/abs/2402.12252)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Cybersecurity is a major challenge for large organizations. Traditional cybersecurity defense is reactive. Cybersecurity operations centers keep out adversaries and incident response teams clean up after break-ins. Recently a proactive stage has been introduced: Cyber Threat Hunting (TH) looks for potential compromises missed by other cyber defenses. TH is mandated for federal executive agencies and government contractors. As threat hunting is a new cybersecurity discipline, most TH teams operate without a defined process. The practices and challenges of TH have not yet been documented. To address this gap, this paper describes the first interview study of threat hunt practitioners. We obtained access and interviewed 11 threat hunters associated with the U.S. government's Department of Homeland Security. Hour-long interviews were conducted. We analyzed the transcripts with process and thematic coding.We describe the diversity among their processes, show that their processes differ from the TH processes reported in the literature, and unify our subjects' descriptions into a single TH process.We enumerate common TH challenges and solutions according to the subjects. The two most common challenges were difficulty in assessing a Threat Hunter's expertise, and developing and maintaining automation. We conclude with recommendations for TH teams (improve planning, focus on automation, and apprentice new members) and highlight directions for future work (finding a TH process that balances flexibility and formalism, and identifying assessments for TH team performance).</li>
</ul>

<h3>Title: NEO-BENCH: Evaluating Robustness of Large Language Models with  Neologisms</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Zheng, Alan Ritter, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12261">https://arxiv.org/abs/2402.12261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12261">https://arxiv.org/pdf/2402.12261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12261]] NEO-BENCH: Evaluating Robustness of Large Language Models with  Neologisms(https://arxiv.org/abs/2402.12261)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.</li>
</ul>

<h3>Title: Uncertainty quantification in fine-tuned LLMs using LoRA ensembles</h3>
<ul>
<li><strong>Authors: </strong>Oleksandr Balabanov, Hampus Linander</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12264">https://arxiv.org/abs/2402.12264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12264">https://arxiv.org/pdf/2402.12264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12264]] Uncertainty quantification in fine-tuned LLMs using LoRA ensembles(https://arxiv.org/abs/2402.12264)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.</li>
</ul>

<h3>Title: On the Byzantine-Resilience of Distillation-Based Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Christophe Roux, Max Zimmer, Sebastian Pokutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12265">https://arxiv.org/abs/2402.12265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12265">https://arxiv.org/pdf/2402.12265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12265]] On the Byzantine-Resilience of Distillation-Based Federated Learning(https://arxiv.org/abs/2402.12265)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of KD-based FL algorithms and demonstrate its efficacy. Finally, we provide a general method to make attacks harder to detect, improving their effectiveness.</li>
</ul>

<h3>Title: High-quality Data-to-Text Generation for Severely Under-Resourced  Languages with Out-of-the-box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michela Lorandi, Anya Belz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12267">https://arxiv.org/abs/2402.12267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12267">https://arxiv.org/pdf/2402.12267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12267]] High-quality Data-to-Text Generation for Severely Under-Resourced  Languages with Out-of-the-box Large Language Models(https://arxiv.org/abs/2402.12267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric's suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.</li>
</ul>

<h3>Title: End-to-end Supervised Prediction of Arbitrary-size Graphs with  Partially-Masked Fused Gromov-Wasserstein Matching</h3>
<ul>
<li><strong>Authors: </strong>Paul Krzakala, Junjie Yang, R√©mi Flamary, Florence d'Alch√© Buc, Charlotte Laclau, Matthieu Labeau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12269">https://arxiv.org/abs/2402.12269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12269">https://arxiv.org/pdf/2402.12269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12269]] End-to-end Supervised Prediction of Arbitrary-size Graphs with  Partially-Masked Fused Gromov-Wasserstein Matching(https://arxiv.org/abs/2402.12269)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.</li>
</ul>

<h3>Title: Key ingredients for effective zero-shot cross-lingual knowledge transfer  in generative tasks</h3>
<ul>
<li><strong>Authors: </strong>Nadezhda Chirkova, Vassilina Nikoulina</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12279">https://arxiv.org/abs/2402.12279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12279">https://arxiv.org/pdf/2402.12279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12279]] Key ingredients for effective zero-shot cross-lingual knowledge transfer  in generative tasks(https://arxiv.org/abs/2402.12279)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases. Our final models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual generation.</li>
</ul>

<h3>Title: Adaptive Skeleton Graph Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shuowei Jin, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Matthew Lentz, Z. Morley Mao, Atul Prakash, Feng Qian, Danyang Zhuo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12280">https://arxiv.org/abs/2402.12280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12280">https://arxiv.org/pdf/2402.12280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12280]] Adaptive Skeleton Graph Decoding(https://arxiv.org/abs/2402.12280)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality. Compared to standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while improving quality by up to 51%.</li>
</ul>

<h3>Title: Refining Minimax Regret for Unsupervised Environment Design</h3>
<ul>
<li><strong>Authors: </strong>Michael Beukman, Samuel Coward, Michael Matthews, Mattie Fellows, Minqi Jiang, Michael Dennis, Jakob Foerster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12284">https://arxiv.org/abs/2402.12284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12284">https://arxiv.org/pdf/2402.12284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12284]] Refining Minimax Regret for Unsupervised Environment Design(https://arxiv.org/abs/2402.12284)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence. We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning.</li>
</ul>

<h3>Title: DriveVLM: The Convergence of Autonomous Driving and Large  Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12289">https://arxiv.org/abs/2402.12289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12289">https://arxiv.org/pdf/2402.12289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12289]] DriveVLM: The Convergence of Autonomous Driving and Large  Vision-Language Models(https://arxiv.org/abs/2402.12289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of chain-of-thought (CoT) modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. DriveVLM-Dual achieves robust spatial understanding and real-time inference speed. Extensive experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing methods in complex and unpredictable driving conditions.</li>
</ul>

<h3>Title: KARL: Knowledge-Aware Retrieval and Representations aid Retention and  Learning in Students</h3>
<ul>
<li><strong>Authors: </strong>Matthew Shu, Nishant Balepur, Shi Feng, Jordan Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12291">https://arxiv.org/abs/2402.12291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12291">https://arxiv.org/pdf/2402.12291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12291]] KARL: Knowledge-Aware Retrieval and Representations aid Retention and  Learning in Students(https://arxiv.org/abs/2402.12291)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based on 27 learners and 32 6-day study trajectories, KARL shows the ability to enhance medium-term educational learning, proving its efficacy for scheduling.</li>
</ul>

<h3>Title: Is Open-Source There Yet? A Comparative Study on Commercial and  Open-Source LLMs in Their Ability to Label Chest X-Ray Reports</h3>
<ul>
<li><strong>Authors: </strong>Felix J. Dorfner, Liv J√ºrgensen, Leonhard Donle, Fares Al Mohamad, Tobias R. Bodenmann, Mason C. Cleveland, Felix Busch, Lisa C. Adams, James Sato, Thomas Schultz, Albert E. Kim, Jameson Merkow, Keno K. Bressem, Christopher P. Bridge</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12298">https://arxiv.org/abs/2402.12298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12298">https://arxiv.org/pdf/2402.12298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12298]] Is Open-Source There Yet? A Comparative Study on Commercial and  Open-Source LLMs in Their Ability to Label Chest X-Ray Reports(https://arxiv.org/abs/2402.12298)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models. Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text reports using different prompting techniques. Results: On the ImaGenome dataset, the best performing open-source model was Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot prompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984, respectively. On the institutional dataset, the best performing open-source model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and few-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.973, respectively. Conclusion: In this paper, we show that while GPT-4 is superior to open-source models in zero-shot report labeling, the implementation of few-shot prompting can bring open-source models on par with GPT-4. This shows that open-source models could be a performant and privacy preserving alternative to GPT-4 for the task of radiology report classification.</li>
</ul>

<h3>Title: Multi-View Conformal Learning for Heterogeneous Sensor Fusion</h3>
<ul>
<li><strong>Authors: </strong>Enrique Garcia-Ceja</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12307">https://arxiv.org/abs/2402.12307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12307">https://arxiv.org/pdf/2402.12307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12307]] Multi-View Conformal Learning for Heterogeneous Sensor Fusion(https://arxiv.org/abs/2402.12307)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comprehensive experimentation, we show that multi-view models perform better than single-view models not only in terms of accuracy-based performance metrics (as it has already been shown in several previous works) but also in conformal measures that provide uncertainty estimation. Our results also showed that multi-view models generate prediction sets with less uncertainty compared to single-view models.</li>
</ul>

<h3>Title: ARKS: Active Retrieval in Knowledge Soup for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12317">https://arxiv.org/abs/2402.12317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12317">https://arxiv.org/pdf/2402.12317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12317]] ARKS: Active Retrieval in Knowledge Soup for Code Generation(https://arxiv.org/abs/2402.12317)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on LLMs. The analysis confirms the effectiveness of our proposed knowledge soup and active retrieval strategies, offering rich insights into the construction of effective retrieval-augmented code generation (RACG) pipelines. Our model, code, and data are available at https://arks-codegen.github.io.</li>
</ul>

<h3>Title: Dynamic Environment Responsive Online Meta-Learning with Fairness  Awareness</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12319">https://arxiv.org/abs/2402.12319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12319">https://arxiv.org/pdf/2402.12319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12319]] Dynamic Environment Responsive Online Meta-Learning with Fairness  Awareness(https://arxiv.org/abs/2402.12319)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.</li>
</ul>

<h3>Title: Landmark Stereo Dataset for Landmark Recognition and Moving Node  Localization in a Non-GPS Battlefield Environment</h3>
<ul>
<li><strong>Authors: </strong>Ganesh Sapkota, Sanjay Madria</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12320">https://arxiv.org/abs/2402.12320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12320">https://arxiv.org/pdf/2402.12320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12320]] Landmark Stereo Dataset for Landmark Recognition and Moving Node  Localization in a Non-GPS Battlefield Environment(https://arxiv.org/abs/2402.12320)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkStereov1. We trained the YOLOv5 model with MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5: 0.95] IoU. We calculated the distance from a node to the landmark utilizing the bounding box coordinates and the depth map generated by the improved SGM algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from the detection result and the distances calculated by the SGM algorithm are stored as the virtual coordinates of a node. In future work, we will use these virtual coordinates to obtain the location of a node using an efficient trilateration algorithm and optimize the node position using the appropriate optimization method.</li>
</ul>

<h3>Title: Query-Based Adversarial Prompt Generation</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tram√®r, Milad Nasr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12329">https://arxiv.org/abs/2402.12329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12329">https://arxiv.org/pdf/2402.12329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12329]] Query-Based Adversarial Prompt Generation(https://arxiv.org/abs/2402.12329)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.</li>
</ul>

<h3>Title: Generating Survival Interpretable Trajectories and Data</h3>
<ul>
<li><strong>Authors: </strong>Andrei V. Konstantinov, Stanislav R. Kirpichenko, Lev V. Utkin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12331">https://arxiv.org/abs/2402.12331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12331">https://arxiv.org/pdf/2402.12331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12331]] Generating Survival Interpretable Trajectories and Data(https://arxiv.org/abs/2402.12331)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classification task. The paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets. The code of the algorithm implementing the proposed model is publicly available.</li>
</ul>

<h3>Title: Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings  for Robust Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12336">https://arxiv.org/abs/2402.12336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12336">https://arxiv.org/pdf/2402.12336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12336]] Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings  for Robust Large Vision-Language Models(https://arxiv.org/abs/2402.12336)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM</li>
</ul>

<h3>Title: Emulated Disalignment: Safety Alignment for Large Language Models May  Backfire!</h3>
<ul>
<li><strong>Authors: </strong>Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12343">https://arxiv.org/abs/2402.12343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12343">https://arxiv.org/pdf/2402.12343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12343]] Emulated Disalignment: Safety Alignment for Large Language Models May  Backfire!(https://arxiv.org/abs/2402.12343)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.</li>
</ul>

<h3>Title: GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via  Game-Theoretic Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12348">https://arxiv.org/abs/2402.12348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12348">https://arxiv.org/pdf/2402.12348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12348]] GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via  Game-Theoretic Evaluations(https://arxiv.org/abs/2402.12348)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs, e.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are also provided for a better understanding of LLMs' behavior.</li>
</ul>

<h3>Title: Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12352">https://arxiv.org/abs/2402.12352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12352">https://arxiv.org/pdf/2402.12352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12352]] Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge(https://arxiv.org/abs/2402.12352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the context via maximum similarity search over text embeddings. In this study, we show that RAG methods leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel information-retrieval method that leverages a knowledge graph to downsample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be advantageously combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models.</li>
</ul>

<h3>Title: Universal Physics Transformers</h3>
<ul>
<li><strong>Authors: </strong>Benedikt Alkin, Andreas F√ºrst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12365">https://arxiv.org/abs/2402.12365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12365">https://arxiv.org/pdf/2402.12365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12365]] Universal Physics Transformers(https://arxiv.org/abs/2402.12365)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics. Project page: https://ml-jku.github.io/UPT</li>
</ul>

<h3>Title: A Critical Evaluation of AI Feedback for Aligning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, Thomas Kollar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12366">https://arxiv.org/abs/2402.12366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12366">https://arxiv.org/pdf/2402.12366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12366]] A Critical Evaluation of AI Feedback for Aligning Large Language Models(https://arxiv.org/abs/2402.12366)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice.</li>
</ul>

<h3>Title: HunFlair2 in a cross-corpus evaluation of named entity recognition and  normalization tools</h3>
<ul>
<li><strong>Authors: </strong>Mario S√§nger, Samuele Garda, Xing David Wang, Leon Weber-Genzel, Pia Droop, Benedikt Fuchs, Alan Akbik, Ulf Leser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12372">https://arxiv.org/abs/2402.12372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12372">https://arxiv.org/pdf/2402.12372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12372]] HunFlair2 in a cross-corpus evaluation of named entity recognition and  normalization tools(https://arxiv.org/abs/2402.12372)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training. Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types. Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting. HunFlair2 showed the best performance on average, being closely followed by PubTator. Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust.</li>
</ul>

<h3>Title: Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12374">https://arxiv.org/abs/2402.12374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12374">https://arxiv.org/pdf/2402.12374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12374]] Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding(https://arxiv.org/abs/2402.12374)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\times$, $3.84\times$, and $2.37\times$, and Llama2-70B offloading by up to $10.33\times$ on L40.</li>
</ul>

<h3>Title: FiT: Flexible Vision Transformer for Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12376">https://arxiv.org/abs/2402.12376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12376">https://arxiv.org/pdf/2402.12376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12376]] FiT: Flexible Vision Transformer for Diffusion Model(https://arxiv.org/abs/2402.12376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at https://github.com/whlzy/FiT.</li>
</ul>

<h3>Title: Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based  View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Christian Reiser, Stephan Garbin, Pratul P. Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T. Barron, Peter Hedman, Andreas Geiger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12377">https://arxiv.org/abs/2402.12377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12377">https://arxiv.org/pdf/2402.12377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12377]] Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based  View Synthesis(https://arxiv.org/abs/2402.12377)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene's geometry as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent geometry in a "fuzzy" manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface geometry by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
