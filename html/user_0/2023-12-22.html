<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2023-12-22</h1>
<h2>secure</h2>
<h3>Title: Secure Information Embedding in Images with Hybrid Firefly Algorithm. (arXiv:2312.13519v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13519">http://arxiv.org/abs/2312.13519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13519]] Secure Information Embedding in Images with Hybrid Firefly Algorithm(http://arxiv.org/abs/2312.13519)</code></li>
<li>Summary: <p>Various methods have been proposed to secure access to sensitive information
over time, such as the many cryptographic methods in use to facilitate secure
communications on the internet. But other methods like steganography have been
overlooked which may be more suitable in cases where the act of transmission of
sensitive information itself should remain a secret. Multiple techniques that
are commonly discussed for such scenarios suffer from low capacity and high
distortion in the output signal. This research introduces a novel
steganographic approach for concealing a confidential portable document format
(PDF) document within a host image by employing the Hybrid Firefly algorithm
(HFA) proposed to select the pixel arrangement. This algorithm combines two
widely used optimization algorithms to improve their performance. The suggested
methodology utilizes the HFA algorithm to conduct a search for optimal pixel
placements in the spatial domain. The purpose of this search is to accomplish
two main goals: increasing the host image's capacity and reducing distortion.
Moreover, the proposed approach intends to reduce the time required for the
embedding procedure. The findings indicate a decrease in image distortion and
an accelerated rate of convergence in the search process. The resultant
embeddings exhibit robustness against steganalytic assaults, hence rendering
the identification of the embedded data a formidable undertaking.
</p></li>
</ul>

<h3>Title: Benchmark Evaluation of Anomaly-Based Intrusion Detection Systems in the Context of Smart Grids. (arXiv:2312.13705v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13705">http://arxiv.org/abs/2312.13705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13705]] Benchmark Evaluation of Anomaly-Based Intrusion Detection Systems in the Context of Smart Grids(http://arxiv.org/abs/2312.13705)</code></li>
<li>Summary: <p>The increasing digitization of smart grids has made addressing cybersecurity
issues crucial in order to secure the power supply. Anomaly detection has
emerged as a key technology for cybersecurity in smart grids, enabling the
detection of unknown threats. Many research efforts have proposed various
machine-learning-based approaches for anomaly detection in grid operations.
However, there is a need for a reproducible and comprehensive evaluation
environment to investigate and compare different approaches to anomaly
detection. The assessment process is highly dependent on the specific
application and requires an evaluation that considers representative datasets
from the use case as well as the specific characteristics of the use case. In
this work, we present an evaluation environment for anomaly detection methods
in smart grids that facilitates reproducible and comprehensive evaluation of
different anomaly detection methods.
</p></li>
</ul>

<h3>Title: Efficient quantum algorithms for some instances of the semidirect discrete logarithm problem. (arXiv:2312.14028v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14028">http://arxiv.org/abs/2312.14028</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14028]] Efficient quantum algorithms for some instances of the semidirect discrete logarithm problem(http://arxiv.org/abs/2312.14028)</code></li>
<li>Summary: <p>The semidirect discrete logarithm problem (SDLP) is the following analogue of
the standard discrete logarithm problem in the semidirect product semigroup
$G\rtimes \mathrm{End}(G)$ for a finite semigroup $G$. Given $g\in G, \sigma\in
\mathrm{End}(G)$, and $h=\prod_{i=0}^{t-1}\sigma^i(g)$ for some integer $t$,
the SDLP$(G,\sigma)$, for $g$ and $h$, asks to determine $t$. As Shor's
algorithm crucially depends on commutativity, it is believed not to be
applicable to the SDLP. Previously, the best known algorithm for the SDLP was
based on Kuperberg's subexponential time quantum algorithm. Still, the problem
plays a central role in the security of certain proposed cryptosystems in the
family of \textit{semidirect product key exchange}. This includes a recently
proposed signature protocol called SPDH-Sign. In this paper, we show that the
SDLP is even easier in some important special cases. Specifically, for a finite
group $G$, we describe quantum algorithms for the SDLP in $G\rtimes
\mathrm{Aut}(G)$ for the following two classes of instances: the first one is
when $G$ is solvable and the second is when $G$ is a matrix group and a power
of $\sigma$ with a polynomially small exponent is an inner automorphism of $G$.
We further extend the results to groups composed of factors from these classes.
A consequence is that SPDH-Sign and similar cryptosystems whose security
assumption is based on the presumed hardness of the SDLP in the cases described
above are insecure against quantum attacks. The quantum ingredients we rely on
are not new: these are Shor's factoring and discrete logarithm algorithms and
well-known generalizations.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Fortify Your Defenses: Strategic Budget Allocation to Enhance Power Grid Cybersecurity. (arXiv:2312.13476v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13476">http://arxiv.org/abs/2312.13476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13476]] Fortify Your Defenses: Strategic Budget Allocation to Enhance Power Grid Cybersecurity(http://arxiv.org/abs/2312.13476)</code></li>
<li>Summary: <p>The abundance of cyber-physical components in modern day power grid with
their diverse hardware and software vulnerabilities has made it difficult to
protect them from advanced persistent threats (APTs). An attack graph depicting
the propagation of potential cyber-attack sequences from the initial access
point to the end objective is vital to identify critical weaknesses of any
cyber-physical system. A cyber security personnel can accordingly plan
preventive mitigation measures for the identified weaknesses addressing the
cyber-attack sequences. However, limitations on available cybersecurity budget
restrict the choice of mitigation measures. We address this aspect through our
framework, which solves the following problem: given potential cyber-attack
sequences for a cyber-physical component in the power grid, find the optimal
manner to allocate an available budget to implement necessary preventive
mitigation measures. We formulate the problem as a mixed integer linear program
(MILP) to identify the optimal budget partition and set of mitigation measures
which minimize the vulnerability of cyber-physical components to potential
attack sequences. We assume that the allocation of budget affects the efficacy
of the mitigation measures. We show how altering the budget allocation for
tasks such as asset management, cybersecurity infrastructure improvement,
incident response planning and employee training affects the choice of the
optimal set of preventive mitigation measures and modifies the associated
cybersecurity risk. The proposed framework can be used by cyber policymakers
and system owners to allocate optimal budgets for various tasks required to
improve the overall security of a cyber-physical system.
</p></li>
</ul>

<h3>Title: HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion. (arXiv:2312.13530v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13530">http://arxiv.org/abs/2312.13530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13530]] HW-V2W-Map: Hardware Vulnerability to Weakness Mapping Framework for Root Cause Analysis with GPT-assisted Mitigation Suggestion(http://arxiv.org/abs/2312.13530)</code></li>
<li>Summary: <p>The escalating complexity of modern computing frameworks has resulted in a
surge in the cybersecurity vulnerabilities reported to the National
Vulnerability Database (NVD) by practitioners. Despite the fact that the
stature of NVD is one of the most significant databases for the latest insights
into vulnerabilities, extracting meaningful trends from such a large amount of
unstructured data is still challenging without the application of suitable
technological methodologies. Previous efforts have mostly concentrated on
software vulnerabilities; however, a holistic strategy incorporates approaches
for mitigating vulnerabilities, score prediction, and a knowledge-generating
system that may extract relevant insights from the Common Weakness Enumeration
(CWE) and Common Vulnerability Exchange (CVE) databases is notably absent. As
the number of hardware attacks on Internet of Things (IoT) devices continues to
rapidly increase, we present the Hardware Vulnerability to Weakness Mapping
(HW-V2W-Map) Framework, which is a Machine Learning (ML) framework focusing on
hardware vulnerabilities and IoT security. The architecture that we have
proposed incorporates an Ontology-driven Storytelling framework, which
automates the process of updating the ontology in order to recognize patterns
and evolution of vulnerabilities over time and provides approaches for
mitigating the vulnerabilities. The repercussions of vulnerabilities can be
mitigated as a result of this, and conversely, future exposures can be
predicted and prevented. Furthermore, our proposed framework utilized
Generative Pre-trained Transformer (GPT) Large Language Models (LLMs) to
provide mitigation suggestions.
</p></li>
</ul>

<h3>Title: A Forecasting-Based DLP Approach for Data Security. (arXiv:2312.13704v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13704">http://arxiv.org/abs/2312.13704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13704]] A Forecasting-Based DLP Approach for Data Security(http://arxiv.org/abs/2312.13704)</code></li>
<li>Summary: <p>Sensitive data leakage is the major growing problem being faced by
enterprises in this technical era. Data leakage causes severe threats for
organization of data safety which badly affects the reputation of
organizations. Data leakage is the flow of sensitive data/information from any
data holder to an unauthorized destination. Data leak prevention (DLP) is set
of techniques that try to alleviate the threats which may hinder data security.
DLP unveils guilty user responsible for data leakage and ensures that user
without appropriate permission cannot access sensitive data and also provides
protection to sensitive data if sensitive data is shared accidentally. In this
paper, data leakage prevention (DLP) model is used to restrict/grant data
access permission to user, based on the forecast of their access to data. This
study provides a DLP solution using data statistical analysis to forecast the
data access possibilities of any user in future based on the access to data in
the past. The proposed approach makes use of renowned simple piecewise linear
function for learning/training to model. The results show that the proposed DLP
approach with high level of precision can correctly classify between users even
in cases of extreme data access.
</p></li>
</ul>

<h3>Title: A Learning oriented DLP System based on Classification Model. (arXiv:2312.13711v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13711">http://arxiv.org/abs/2312.13711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13711]] A Learning oriented DLP System based on Classification Model(http://arxiv.org/abs/2312.13711)</code></li>
<li>Summary: <p>Data is the key asset for organizations and data sharing is lifeline for
organization growth; which may lead to data loss. Data leakage is the most
critical issue being faced by organizations. In order to mitigate the data
leakage issues data leakage prevention systems (DLPSs) are deployed at various
levels by the organizations. DLPSs are capable to protect all kind of data i.e.
DAR, DIM/DIT, DIU. Statistical analysis, regular expression, data
fingerprinting are common approaches exercised in DLP system. Out of these
techniques; statistical analysis approach is most appropriate for proposed DLP
model of data security. This paper defines a statistical DLP model for document
classification. Model uses various statistical approaches like TF-IDF (Term
Frequency- Inverse Document Frequency) a renowned term count/weighing function,
Vectorization, Gradient boosting document classification etc. to classify the
documents before allowing any access to it. Machine learning is used to test
and train the model. Proposed model also introduces an extremely efficient and
more accurate approach; IGBCA (Improvised Gradient Boosting Classification
Algorithm); for document classification, to prevent them from possible data
leakage. Results depicts that proposed model can classify documents with high
accuracy and on basis of which data can be prevented from being loss.
</p></li>
</ul>

<h3>Title: Manipulating Trajectory Prediction with Backdoors. (arXiv:2312.13863v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13863">http://arxiv.org/abs/2312.13863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13863]] Manipulating Trajectory Prediction with Backdoors(http://arxiv.org/abs/2312.13863)</code></li>
<li>Summary: <p>Autonomous vehicles ought to predict the surrounding agents' trajectories to
allow safe maneuvers in uncertain and complex traffic situations. As companies
increasingly apply trajectory prediction in the real world, security becomes a
relevant concern. In this paper, we focus on backdoors - a security threat
acknowledged in other fields but so far overlooked for trajectory prediction.
To this end, we describe and investigate four triggers that could affect
trajectory prediction. We then show that these triggers (for example, a braking
vehicle), when correlated with a desired output (for example, a curve) during
training, cause the desired output of a state-of-the-art trajectory prediction
model. In other words, the model has good benign performance but is vulnerable
to backdoors. This is the case even if the trigger maneuver is performed by a
non-casual agent behind the target vehicle. As a side-effect, our analysis
reveals interesting limitations within trajectory prediction models. Finally,
we evaluate a range of defenses against backdoors. While some, like simple
offroad checks, do not enable detection for all triggers, clustering is a
promising candidate to support manual inspection to find backdoors.
</p></li>
</ul>

<h3>Title: Asynchronous Authentication. (arXiv:2312.13967v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13967">http://arxiv.org/abs/2312.13967</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13967]] Asynchronous Authentication(http://arxiv.org/abs/2312.13967)</code></li>
<li>Summary: <p>A myriad of authentication mechanisms embody a continuous evolution from
verbal passwords in ancient times to contemporary multi-factor authentication.
Nevertheless, digital asset heists and numerous identity theft cases illustrate
the urgent need to revisit the fundamentals of user authentication. We abstract
away credential details and formalize the general, common case of asynchronous
authentication, with unbounded message propagation time. Our model, which might
be of independent interest, allows for eventual message delivery, while
bounding execution time to maintain cryptographic guarantees. Given
credentials' fault probabilities (e.g., loss or leak), we seek mechanisms with
the highest success probability. We show that every mechanism is dominated by
some Boolean mechanism -- defined by a monotonic Boolean function on presented
credentials. We present an algorithm for finding approximately optimal
mechanisms. Previous work analyzed Boolean mechanisms specifically, but used
brute force, which quickly becomes prohibitively complex. We leverage the
problem structure to reduce complexity by orders of magnitude. The algorithm is
readily applicable to practical settings. For example, we revisit the common
approach in cryptocurrency wallets that use a handful of high-quality
credentials. We show that adding low-quality credentials improves security by
orders of magnitude.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Multi-label Learning from Privacy-Label. (arXiv:2312.13312v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13312">http://arxiv.org/abs/2312.13312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13312]] Multi-label Learning from Privacy-Label(http://arxiv.org/abs/2312.13312)</code></li>
<li>Summary: <p>Multi-abel Learning (MLL) often involves the assignment of multiple relevant
labels to each instance, which can lead to the leakage of sensitive information
(such as smoking, diseases, etc.) about the instances. However, existing MLL
suffer from failures in protection for sensitive information. In this paper, we
propose a novel setting named Multi-Label Learning from Privacy-Label (MLLPL),
which Concealing Labels via Privacy-Label Unit (CLPLU). Specifically, during
the labeling phase, each privacy-label is randomly combined with a non-privacy
label to form a Privacy-Label Unit (PLU). If any label within a PLU is
positive, the unit is labeled as positive; otherwise, it is labeled negative,
as shown in Figure 1. PLU ensures that only non-privacy labels are appear in
the label set, while the privacy-labels remain concealed. Moreover, we further
propose a Privacy-Label Unit Loss (PLUL) to learn the optimal classifier by
minimizing the empirical risk of PLU. Experimental results on multiple
benchmark datasets demonstrate the effectiveness and superiority of the
proposed method.
</p></li>
</ul>

<h3>Title: Transparency and Privacy: The Role of Explainable AI and Federated Learning in Financial Fraud Detection. (arXiv:2312.13334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13334">http://arxiv.org/abs/2312.13334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13334]] Transparency and Privacy: The Role of Explainable AI and Federated Learning in Financial Fraud Detection(http://arxiv.org/abs/2312.13334)</code></li>
<li>Summary: <p>Fraudulent transactions and how to detect them remain a significant problem
for financial institutions around the world. The need for advanced fraud
detection systems to safeguard assets and maintain customer trust is paramount
for financial institutions, but some factors make the development of effective
and efficient fraud detection systems a challenge. One of such factors is the
fact that fraudulent transactions are rare and that many transaction datasets
are imbalanced; that is, there are fewer significant samples of fraudulent
transactions than legitimate ones. This data imbalance can affect the
performance or reliability of the fraud detection model. Moreover, due to the
data privacy laws that all financial institutions are subject to follow,
sharing customer data to facilitate a higher-performing centralized model is
impossible. Furthermore, the fraud detection technique should be transparent so
that it does not affect the user experience. Hence, this research introduces a
novel approach using Federated Learning (FL) and Explainable AI (XAI) to
address these challenges. FL enables financial institutions to collaboratively
train a model to detect fraudulent transactions without directly sharing
customer data, thereby preserving data privacy and confidentiality. Meanwhile,
the integration of XAI ensures that the predictions made by the model can be
understood and interpreted by human experts, adding a layer of transparency and
trust to the system. Experimental results, based on realistic transaction
datasets, reveal that the FL-based fraud detection system consistently
demonstrates high performance metrics. This study grounds FL's potential as an
effective and privacy-preserving tool in the fight against fraud.
</p></li>
</ul>

<h3>Title: Conciliating Privacy and Utility in Data Releases via Individual Differential Privacy and Microaggregation. (arXiv:2312.13712v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13712">http://arxiv.org/abs/2312.13712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13712]] Conciliating Privacy and Utility in Data Releases via Individual Differential Privacy and Microaggregation(http://arxiv.org/abs/2312.13712)</code></li>
<li>Summary: <p>$\epsilon$-Differential privacy (DP) is a well-known privacy model that
offers strong privacy guarantees. However, when applied to data releases, DP
significantly deteriorates the analytical utility of the protected outcomes. To
keep data utility at reasonable levels, practical applications of DP to data
releases have used weak privacy parameters (large $\epsilon$), which dilute the
privacy guarantees of DP. In this work, we tackle this issue by using an
alternative formulation of the DP privacy guarantees, named
$\epsilon$-individual differential privacy (iDP), which causes less data
distortion while providing the same protection as DP to subjects. We enforce
iDP in data releases by relying on attribute masking plus a pre-processing step
based on data microaggregation. The goal of this step is to reduce the
sensitivity to record changes, which determines the amount of noise required to
enforce iDP (and DP). Specifically, we propose data microaggregation strategies
designed for iDP whose sensitivities are significantly lower than those used in
DP. As a result, we obtain iDP-protected data with significantly better utility
than with DP. We report on experiments that show how our approach can provide
strong privacy (small $\epsilon$) while yielding protected data that do not
significantly degrade the accuracy of secondary data analysis.
</p></li>
</ul>

<h3>Title: R\'enyi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration. (arXiv:2312.13985v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13985">http://arxiv.org/abs/2312.13985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13985]] R\'enyi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration(http://arxiv.org/abs/2312.13985)</code></li>
<li>Summary: <p>Pufferfish privacy is a flexible generalization of differential privacy that
allows to model arbitrary secrets and adversary's prior knowledge about the
data. Unfortunately, designing general and tractable Pufferfish mechanisms that
do not compromise utility is challenging. Furthermore, this framework does not
provide the composition guarantees needed for a direct use in iterative machine
learning algorithms. To mitigate these issues, we introduce a R\'enyi
divergence-based variant of Pufferfish and show that it allows us to extend the
applicability of the Pufferfish framework. We first generalize the Wasserstein
mechanism to cover a wide range of noise distributions and introduce several
ways to improve its utility. We also derive stronger guarantees against
out-of-distribution adversaries. Finally, as an alternative to composition, we
prove privacy amplification results for contractive noisy iterations and
showcase the first use of Pufferfish in private convex optimization. A common
ingredient underlying our results is the use and extension of shift reduction
lemmas.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Investigation of Multi-stage Attack and Defense Simulation for Data Synthesis. (arXiv:2312.13697v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13697">http://arxiv.org/abs/2312.13697</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13697]] Investigation of Multi-stage Attack and Defense Simulation for Data Synthesis(http://arxiv.org/abs/2312.13697)</code></li>
<li>Summary: <p>The power grid is a critical infrastructure that plays a vital role in modern
society. Its availability is of utmost importance, as a loss can endanger human
lives. However, with the increasing digitalization of the power grid, it also
becomes vulnerable to new cyberattacks that can compromise its availability. To
counter these threats, intrusion detection systems are developed and deployed
to detect cyberattacks targeting the power grid. Among intrusion detection
systems, anomaly detection models based on machine learning have shown
potential in detecting unknown attack vectors. However, the scarcity of data
for training these models remains a challenge due to confidentiality concerns.
To overcome this challenge, this study proposes a model for generating
synthetic data of multi-stage cyber attacks in the power grid, using attack
trees to model the attacker's sequence of steps and a game-theoretic approach
to incorporate the defender's actions. This model aims to create diverse attack
data on which machine learning algorithms can be trained.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Open-Set: ID Card Presentation Attack Detection using Neural Transfer Style. (arXiv:2312.13993v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13993">http://arxiv.org/abs/2312.13993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13993]] Open-Set: ID Card Presentation Attack Detection using Neural Transfer Style(http://arxiv.org/abs/2312.13993)</code></li>
<li>Summary: <p>The accurate detection of ID card Presentation Attacks (PA) is becoming
increasingly important due to the rising number of online/remote services that
require the presentation of digital photographs of ID cards for digital
onboarding or authentication. Furthermore, cybercriminals are continuously
searching for innovative ways to fool authentication systems to gain
unauthorized access to these services. Although advances in neural network
design and training have pushed image classification to the state of the art,
one of the main challenges faced by the development of fraud detection systems
is the curation of representative datasets for training and evaluation. The
handcrafted creation of representative presentation attack samples often
requires expertise and is very time-consuming, thus an automatic process of
obtaining high-quality data is highly desirable. This work explores ID card
Presentation Attack Instruments (PAI) in order to improve the generation of
samples with four Generative Adversarial Networks (GANs) based image
translation models and analyses the effectiveness of the generated data for
training fraud detection systems. Using open-source data, we show that
synthetic attack presentations are an adequate complement for additional real
attack presentations, where we obtain an EER performance increase of 0.63%
points for print attacks and a loss of 0.29% for screen capture attacks.
</p></li>
</ul>

<h3>Title: An Approach to Abstract Multi-stage Cyberattack Data Generation for ML-Based IDS in Smart Grids. (arXiv:2312.13737v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13737">http://arxiv.org/abs/2312.13737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13737]] An Approach to Abstract Multi-stage Cyberattack Data Generation for ML-Based IDS in Smart Grids(http://arxiv.org/abs/2312.13737)</code></li>
<li>Summary: <p>Power grids are becoming more digitized, resulting in new opportunities for
the grid operation but also new challenges, such as new threats from the
cyber-domain. To address these challenges, cybersecurity solutions are being
considered in the form of preventive, detective, and reactive measures. Machine
learning-based intrusion detection systems are used as part of detection
efforts to detect and defend against cyberattacks. However, training and
testing data for these systems are often not available or suitable for use in
machine learning models for detecting multi-stage cyberattacks in smart grids.
In this paper, we propose a method to generate synthetic data using a
graph-based approach for training machine learning models in smart grids. We
use an abstract form of multi-stage cyberattacks defined via graph formulations
and simulate the propagation behavior of attacks in the network. Within the
selected scenarios, we observed promising results, but a larger number of
scenarios need to be studied to draw a more informed conclusion about the
suitability of synthesized data.
</p></li>
</ul>

<h3>Title: Dynamic Mining Interval to Improve Blockchain Throughput. (arXiv:2312.14038v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14038">http://arxiv.org/abs/2312.14038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14038]] Dynamic Mining Interval to Improve Blockchain Throughput(http://arxiv.org/abs/2312.14038)</code></li>
<li>Summary: <p>Decentralized Finance (DeFi), propelled by Blockchain technology, has
revolutionized traditional financial systems, improving transparency, reducing
costs, and fostering financial inclusion. However, transaction activities in
these systems fluctuate significantly and the throughput can be effected. To
address this issue, we propose a Dynamic Mining Interval (DMI) mechanism that
adjusts mining intervals in response to block size and trading volume to
enhance the transaction throughput of Blockchain platforms. Besides, in the
context of public Blockchains such as Bitcoin, Ethereum, and Litecoin, a shift
towards transaction fees dominance over coin-based rewards is projected in near
future. As a result, the ecosystem continues to face threats from deviant
mining activities such as Undercutting Attacks, Selfish Mining, and Pool
Hopping, among others. In recent years, Dynamic Transaction Storage (DTS)
strategies were proposed to allocate transactions dynamically based on fees
thereby stabilizing block incentives. However, DTS' utilization of Merkle tree
leaf nodes can reduce system throughput. To alleviate this problem, in this
paper, we propose an approach for combining DMI and DTS. Besides, we also
discuss the DMI selection mechanism for adjusting mining intervals based on
various factors.
</p></li>
</ul>

<h3>Title: Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples. (arXiv:2312.13628v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13628">http://arxiv.org/abs/2312.13628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13628]] Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples(http://arxiv.org/abs/2312.13628)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have been demonstrated to be vulnerable to
well-crafted \emph{adversarial examples}, which are generated through either
well-conceived $\mathcal{L}_p$-norm restricted or unrestricted attacks.
Nevertheless, the majority of those approaches assume that adversaries can
modify any features as they wish, and neglect the causal generating process of
the data, which is unreasonable and unpractical. For instance, a modification
in income would inevitably impact features like the debt-to-income ratio within
a banking system. By considering the underappreciated causal generating
process, first, we pinpoint the source of the vulnerability of DNNs via the
lens of causality, then give theoretical results to answer \emph{where to
attack}. Second, considering the consequences of the attack interventions on
the current state of the examples to generate more realistic adversarial
examples, we propose CADE, a framework that can generate
\textbf{C}ounterfactual \textbf{AD}versarial \textbf{E}xamples to answer
\emph{how to attack}. The empirical results demonstrate CADE's effectiveness,
as evidenced by its competitive performance across diverse attack scenarios,
including white-box, transfer-based, and random intervention attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural Networks. (arXiv:2312.13575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13575">http://arxiv.org/abs/2312.13575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13575]] ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural Networks(http://arxiv.org/abs/2312.13575)</code></li>
<li>Summary: <p>Network binarization exhibits great potential for deployment on
resource-constrained devices due to its low computational cost. Despite the
critical importance, the security of binarized neural networks (BNNs) is rarely
investigated. In this paper, we present ARBiBench, a comprehensive benchmark to
evaluate the robustness of BNNs against adversarial perturbations on CIFAR-10
and ImageNet. We first evaluate the robustness of seven influential BNNs on
various white-box and black-box attacks. The results reveal that 1) The
adversarial robustness of BNNs exhibits a completely opposite performance on
the two datasets under white-box attacks. 2) BNNs consistently exhibit better
adversarial robustness under black-box attacks. 3) Different BNNs exhibit
certain similarities in their robustness performance. Then, we conduct
experiments to analyze the adversarial robustness of BNNs based on these
insights. Our research contributes to inspiring future research on enhancing
the robustness of BNNs and advancing their application in real-world scenarios.
</p></li>
</ul>

<h3>Title: Pose-based Tremor Type and Level Analysis for Parkinson's Disease from Video. (arXiv:2312.13776v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13776">http://arxiv.org/abs/2312.13776</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13776]] Pose-based Tremor Type and Level Analysis for Parkinson's Disease from Video(http://arxiv.org/abs/2312.13776)</code></li>
<li>Summary: <p>Purpose:Current methods for diagnosis of PD rely on clinical examination. The
accuracy of diagnosis ranges between 73% and 84%, and is influenced by the
experience of the clinical assessor. Hence, an automatic, effective and
interpretable supporting system for PD symptom identification would support
clinicians in making more robust PD diagnostic decisions. Methods: We propose
to analyze Parkinson's tremor (PT) to support the analysis of PD, since PT is
one of the most typical symptoms of PD with broad generalizability. To realize
the idea, we present SPA-PTA, a deep learning-based PT classification and
severity estimation system that takes consumer-grade videos of front-facing
humans as input. The core of the system is a novel attention module with a
lightweight pyramidal channel-squeezing-fusion architecture that effectively
extracts relevant PT information and filters noise. It enhances modeling
performance while improving system interpretability. Results:We validate our
system via individual-based leave-one-out cross-validation on two tasks: the PT
classification task and the tremor severity rating estimation task. Our system
presents a 91.3% accuracy and 80.0% F1-score in classifying PT with non-PT
class, while providing a 76.4% accuracy and 76.7% F1-score in more complex
multiclass tremor rating classification task. Conclusion: Our system offers a
cost-effective PT classification and tremor severity estimation results as
warning signs of PD for undiagnosed patients with PT symptoms. In addition, it
provides a potential solution for supporting PD diagnosis in regions with
limited clinical resources.
</p></li>
</ul>

<h3>Title: Universal Noise Annotation: Unveiling the Impact of Noisy annotation on Object Detection. (arXiv:2312.13822v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13822">http://arxiv.org/abs/2312.13822</a></li>
<li>Code URL: <a href="https://github.com/ryoo72/una">https://github.com/ryoo72/una</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13822]] Universal Noise Annotation: Unveiling the Impact of Noisy annotation on Object Detection(http://arxiv.org/abs/2312.13822)</code></li>
<li>Summary: <p>For object detection task with noisy labels, it is important to consider not
only categorization noise, as in image classification, but also localization
noise, missing annotations, and bogus bounding boxes. However, previous studies
have only addressed certain types of noise (e.g., localization or
categorization). In this paper, we propose Universal-Noise Annotation (UNA), a
more practical setting that encompasses all types of noise that can occur in
object detection, and analyze how UNA affects the performance of the detector.
We analyzed the development direction of previous works of detection algorithms
and examined the factors that impact the robustness of detection model learning
method. We open-source the code for injecting UNA into the dataset and all the
training log and weight are also shared.
</p></li>
</ul>

<h3>Title: Geometric Awareness in Neural Fields for 3D Human Registration. (arXiv:2312.14024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14024">http://arxiv.org/abs/2312.14024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14024]] Geometric Awareness in Neural Fields for 3D Human Registration(http://arxiv.org/abs/2312.14024)</code></li>
<li>Summary: <p>Aligning a template to 3D human point clouds is a long-standing problem
crucial for tasks like animation, reconstruction, and enabling supervised
learning pipelines. Recent data-driven methods leverage predicted surface
correspondences; however, they are not robust to varied poses or distributions.
In contrast, industrial solutions often rely on expensive manual annotations or
multi-view capturing systems. Recently, neural fields have shown promising
results, but their purely data-driven nature lacks geometric awareness, often
resulting in a trivial misalignment of the template registration. In this work,
we propose two solutions: LoVD, a novel neural field model that predicts the
direction towards the localized SMPL vertices on the target surface; and INT,
the first self-supervised task dedicated to neural fields that, at test time,
refines the backbone, exploiting the target geometry. We combine them into
INLoVD, a robust 3D Human body registration pipeline trained on a large MoCap
dataset. INLoVD is efficient (takes less than a minute), solidly achieves the
state of the art over public benchmarks, and provides unprecedented
generalization on out-of-distribution data. We will release code and
checkpoints in \url{url}.
</p></li>
</ul>

<h3>Title: Structured Probabilistic Coding. (arXiv:2312.13933v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13933">http://arxiv.org/abs/2312.13933</a></li>
<li>Code URL: <a href="https://github.com/zerohd4869/SPC">https://github.com/zerohd4869/SPC</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13933]] Structured Probabilistic Coding(http://arxiv.org/abs/2312.13933)</code></li>
<li>Summary: <p>This paper presents a new supervised representation learning framework,
namely Structured Probabilistic Coding (SPC), to learn compact and informative
representations from input related to the target task. SPC is an encoder-only
probabilistic coding technology with a structured regularization from the
target label space. By extracting compact and informative representations from
input related to the target task, SPC can enhance the generalization ability of
pre-trained language models for better language understanding. Specifically,
the hidden representation is encoded into a Gaussian distribution space, while
maximizing the prior entropy of latent representations concerning label space.
This technique can simultaneously perform information encoding and task
prediction in one module to more fully utilize the effective information from
input data, and use variational inference in the output space to reduce
randomness and uncertainty. To better control the probability distribution in
the latent space, a structured regularization is proposed to promote
class-level uniformity in the latent space. With the regularization term, SPC
can preserve the Gaussian distribution structure of latent code as well as
better cover the hidden space with class uniformly. We conduct evaluations on
12 natural language understanding tasks. The results show that our SPC can
effectively improve the performance of pre-trained language models for various
classification and regression tasks. Experiments demonstrate that SPC can
enhance the generalization capability, robustness to label noise, and
clustering quality of output representations.
</p></li>
</ul>

<h3>Title: Automatic Curriculum Learning with Gradient Reward Signals. (arXiv:2312.13565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13565">http://arxiv.org/abs/2312.13565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13565]] Automatic Curriculum Learning with Gradient Reward Signals(http://arxiv.org/abs/2312.13565)</code></li>
<li>Summary: <p>This paper investigates the impact of using gradient norm reward signals in
the context of Automatic Curriculum Learning (ACL) for deep reinforcement
learning (DRL). We introduce a framework where the teacher model, utilizing the
gradient norm information of a student model, dynamically adapts the learning
curriculum. This approach is based on the hypothesis that gradient norms can
provide a nuanced and effective measure of learning progress. Our experimental
setup involves several reinforcement learning environments (PointMaze, AntMaze,
and AdroitHandRelocate), to assess the efficacy of our method. We analyze how
gradient norm rewards influence the teacher's ability to craft challenging yet
achievable learning sequences, ultimately enhancing the student's performance.
Our results show that this approach not only accelerates the learning process
but also leads to improved generalization and adaptability in complex tasks.
The findings underscore the potential of gradient norm signals in creating more
efficient and robust ACL systems, opening new avenues for research in
curriculum learning and reinforcement learning.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: EPNet: An Efficient Pyramid Network for Enhanced Single-Image Super-Resolution with Reduced Computational Requirements. (arXiv:2312.13396v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13396">http://arxiv.org/abs/2312.13396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13396]] EPNet: An Efficient Pyramid Network for Enhanced Single-Image Super-Resolution with Reduced Computational Requirements(http://arxiv.org/abs/2312.13396)</code></li>
<li>Summary: <p>Single-image super-resolution (SISR) has seen significant advancements
through the integration of deep learning. However, the substantial
computational and memory requirements of existing methods often limit their
practical application. This paper introduces a new Efficient Pyramid Network
(EPNet) that harmoniously merges an Edge Split Pyramid Module (ESPM) with a
Panoramic Feature Extraction Module (PFEM) to overcome the limitations of
existing methods, particularly in terms of computational efficiency. The ESPM
applies a pyramid-based channel separation strategy, boosting feature
extraction while maintaining computational efficiency. The PFEM, a novel fusion
of CNN and Transformer structures, enables the concurrent extraction of local
and global features, thereby providing a panoramic view of the image landscape.
Our architecture integrates the PFEM in a manner that facilitates the
streamlined exchange of feature information and allows for the further
refinement of image texture details. Experimental results indicate that our
model outperforms existing state-of-the-art methods in image resolution
quality, while considerably decreasing computational and memory costs. This
research contributes to the ongoing evolution of efficient and practical SISR
methodologies, bearing broader implications for the field of computer vision.
</p></li>
</ul>

<h3>Title: Data Transformation to Construct a Dataset for Generating Entity-Relationship Model from Natural Language. (arXiv:2312.13694v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13694">http://arxiv.org/abs/2312.13694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13694]] Data Transformation to Construct a Dataset for Generating Entity-Relationship Model from Natural Language(http://arxiv.org/abs/2312.13694)</code></li>
<li>Summary: <p>In order to reduce the manual cost of designing ER models, recent approaches
have been proposed to address the task of NL2ERM, i.e., automatically
generating entity-relationship (ER) models from natural language (NL)
utterances such as software requirements. These approaches are typically
rule-based ones, which rely on rigid heuristic rules; these approaches cannot
generalize well to various linguistic ways of describing the same requirement.
Despite having better generalization capability than rule-based approaches,
deep-learning-based models are lacking for NL2ERM due to lacking a large-scale
dataset. To address this issue, in this paper, we report our insight that there
exists a high similarity between the task of NL2ERM and the increasingly
popular task of text-to-SQL, and propose a data transformation algorithm that
transforms the existing data of text-to-SQL into the data of NL2ERM. We apply
our data transformation algorithm on Spider, one of the most popular
text-to-SQL datasets, and we also collect some data entries with different NL
types, to obtain a large-scale NL2ERM dataset. Because NL2ERM can be seen as a
special information extraction (IE) task, we train two state-of-the-art IE
models on our dataset. The experimental results show that both the two models
achieve high performance and outperform existing baselines.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Continual Novel Class Learning. (arXiv:2312.13500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13500">http://arxiv.org/abs/2312.13500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13500]] Federated Continual Novel Class Learning(http://arxiv.org/abs/2312.13500)</code></li>
<li>Summary: <p>In a privacy-focused era, Federated Learning (FL) has emerged as a promising
machine learning technique. However, most existing FL studies assume that the
data distribution remains nearly fixed over time, while real-world scenarios
often involve dynamic and continual changes. To equip FL systems with continual
model evolution capabilities, we focus on an important problem called Federated
Continual Novel Class Learning (FedCN) in this work. The biggest challenge in
FedCN is to merge and align novel classes that are discovered and learned by
different clients without compromising privacy. To address this, we propose a
Global Alignment Learning (GAL) framework that can accurately estimate the
global novel class number and provide effective guidance for local training
from a global perspective, all while maintaining privacy protection.
Specifically, GAL first locates high-density regions in the representation
space through a bi-level clustering mechanism to estimate the novel class
number, with which the global prototypes corresponding to novel classes can be
constructed. Then, GAL uses a novel semantic weighted loss to capture all
possible correlations between these prototypes and the training data for
mitigating the impact of pseudo-label noise and data heterogeneity. Extensive
experiments on various datasets demonstrate GAL's superior performance over
state-of-the-art novel class discovery methods. In particular, GAL achieves
significant improvements in novel-class performance, increasing the accuracy by
5.1% to 10.6% in the case of one novel class learning stage and by 7.8% to
17.9% in the case of two novel class learning stages, without sacrificing
known-class performance. Moreover, GAL is shown to be effective in equipping a
variety of different mainstream FL algorithms with novel class discovery and
learning capability, highlighting its potential for many real-world
applications.
</p></li>
</ul>

<h3>Title: ProvFL: Client-Driven Interpretability of Global Model Predictions in Federated Learning. (arXiv:2312.13632v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13632">http://arxiv.org/abs/2312.13632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13632]] ProvFL: Client-Driven Interpretability of Global Model Predictions in Federated Learning(http://arxiv.org/abs/2312.13632)</code></li>
<li>Summary: <p>Federated Learning (FL) trains a collaborative machine learning model by
aggregating multiple privately trained clients' models over several training
rounds. Such a long, continuous action of model aggregations poses significant
challenges in reasoning about the origin and composition of such a global
model. Regardless of the quality of the global model or if it has a fault,
understanding the model's origin is equally important for debugging,
interpretability, and explainability in federated learning. FL application
developers often question: (1) what clients contributed towards a global model
and (2) if a global model predicts a label, which clients are responsible for
it?
</p>
<p>We introduce, neuron provenance, a fine-grained lineage capturing mechanism
that tracks the flow of information between the individual participating
clients in FL and the final global model. We operationalize this concept in
ProvFL that functions on two key principles. First, recognizing that monitoring
every neuron of every client's model statically is ineffective and noisy due to
the uninterpretable nature of individual neurons, ProvFL dynamically isolates
influential and sensitive neurons in the global model, significantly reducing
the search space. Second, as multiple clients' models are fused in each round
to form a global model, tracking each client's contribution becomes
challenging. ProvFL leverages the invertible nature of fusion algorithms to
precisely isolate each client's contribution derived from selected neurons.
When asked to localize the clients responsible for the given behavior (i.e.,
prediction) of the global model, ProvFL successfully localizes them with an
average provenance accuracy of 97%. Additionally, ProvFL outperforms the
state-of-the-art FL fault localization approach by an average margin of 50%.
</p></li>
</ul>

<h3>Title: Towards Fair Graph Federated Learning via Incentive Mechanisms. (arXiv:2312.13306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13306">http://arxiv.org/abs/2312.13306</a></li>
<li>Code URL: <a href="https://github.com/zjunet/fairgraphfl">https://github.com/zjunet/fairgraphfl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13306]] Towards Fair Graph Federated Learning via Incentive Mechanisms(http://arxiv.org/abs/2312.13306)</code></li>
<li>Summary: <p>Graph federated learning (FL) has emerged as a pivotal paradigm enabling
multiple agents to collaboratively train a graph model while preserving local
data privacy. Yet, current efforts overlook a key issue: agents are
self-interested and would hesitant to share data without fair and satisfactory
incentives. This paper is the first endeavor to address this issue by studying
the incentive mechanism for graph federated learning. We identify a unique
phenomenon in graph federated learning: the presence of agents posing potential
harm to the federation and agents contributing with delays. This stands in
contrast to previous FL incentive mechanisms that assume all agents contribute
positively and in a timely manner. In view of this, this paper presents a novel
incentive mechanism tailored for fair graph federated learning, integrating
incentives derived from both model gradient and payoff. To achieve this, we
first introduce an agent valuation function aimed at quantifying agent
contributions through the introduction of two criteria: gradient alignment and
graph diversity. Moreover, due to the high heterogeneity in graph federated
learning, striking a balance between accuracy and fairness becomes particularly
crucial. We introduce motif prototypes to enhance accuracy, communicated
between the server and agents, enhancing global model aggregation and aiding
agents in local model optimization. Extensive experiments show that our model
achieves the best trade-off between accuracy and the fairness of model
gradient, as well as superior payoff fairness.
</p></li>
</ul>

<h3>Title: Fed-QSSL: A Framework for Personalized Federated Learning under Bitwidth and Data Heterogeneity. (arXiv:2312.13380v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13380">http://arxiv.org/abs/2312.13380</a></li>
<li>Code URL: <a href="https://github.com/yiyuec/fed-qssl">https://github.com/yiyuec/fed-qssl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13380]] Fed-QSSL: A Framework for Personalized Federated Learning under Bitwidth and Data Heterogeneity(http://arxiv.org/abs/2312.13380)</code></li>
<li>Summary: <p>Motivated by high resource costs of centralized machine learning schemes as
well as data privacy concerns, federated learning (FL) emerged as an efficient
alternative that relies on aggregating locally trained models rather than
collecting clients' potentially private data. In practice, available resources
and data distributions vary from one client to another, creating an inherent
system heterogeneity that leads to deterioration of the performance of
conventional FL algorithms. In this work, we present a federated
quantization-based self-supervised learning scheme (Fed-QSSL) designed to
address heterogeneity in FL systems. At clients' side, to tackle data
heterogeneity we leverage distributed self-supervised learning while utilizing
low-bit quantization to satisfy constraints imposed by local infrastructure and
limited communication resources. At server's side, Fed-QSSL deploys
de-quantization, weighted aggregation and re-quantization, ultimately creating
models personalized to both data distribution as well as specific
infrastructure of each client's device. We validated the proposed algorithm on
real world datasets, demonstrating its efficacy, and theoretically analyzed
impact of low-bit training on the convergence and robustness of the learned
models.
</p></li>
</ul>

<h3>Title: Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast. (arXiv:2312.13508v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13508">http://arxiv.org/abs/2312.13508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13508]] Multimodal Federated Learning with Missing Modality via Prototype Mask and Contrast(http://arxiv.org/abs/2312.13508)</code></li>
<li>Summary: <p>In real-world scenarios, multimodal federated learning often faces the
practical challenge of intricate modality missing, which poses constraints on
building federated frameworks and significantly degrades model inference
accuracy. Existing solutions for addressing missing modalities generally
involve developing modality-specific encoders on clients and training modality
fusion modules on servers. However, these methods are primarily constrained to
specific scenarios with either unimodal clients or complete multimodal clients,
struggling to generalize effectively in the intricate modality missing
scenarios. In this paper, we introduce a prototype library into the
FedAvg-based Federated Learning framework, thereby empowering the framework
with the capability to alleviate the global model performance degradation
resulting from modality missing during both training and testing. The proposed
method utilizes prototypes as masks representing missing modalities to
formulate a task-calibrated training loss and a model-agnostic uni-modality
inference strategy. In addition, a proximal term based on prototypes is
constructed to enhance local training. Experimental results demonstrate the
state-of-the-art performance of our approach. Compared to the baselines, our
method improved inference accuracy by 3.7\% with 50\% modality missing during
training and by 23.8\% during uni-modality inference. Code is available at
https://github.com/BaoGuangYin/PmcmFL.
</p></li>
</ul>

<h3>Title: Topology Learning for Heterogeneous Decentralized Federated Learning over Unreliable D2D Networks. (arXiv:2312.13611v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13611">http://arxiv.org/abs/2312.13611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13611]] Topology Learning for Heterogeneous Decentralized Federated Learning over Unreliable D2D Networks(http://arxiv.org/abs/2312.13611)</code></li>
<li>Summary: <p>With the proliferation of intelligent mobile devices in wireless
device-to-device (D2D) networks, decentralized federated learning (DFL) has
attracted significant interest. Compared to centralized federated learning
(CFL), DFL mitigates the risk of central server failures due to communication
bottlenecks. However, DFL faces several challenges, such as the severe
heterogeneity of data distributions in diverse environments, and the
transmission outages and package errors caused by the adoption of the User
Datagram Protocol (UDP) in D2D networks. These challenges often degrade the
convergence of training DFL models. To address these challenges, we conduct a
thorough theoretical convergence analysis for DFL and derive a convergence
bound. By defining a novel quantity named unreliable links-aware neighborhood
discrepancy in this convergence bound, we formulate a tractable optimization
objective, and develop a novel Topology Learning method considering the
Representation Discrepancy and Unreliable Links in DFL, named ToLRDUL.
Intensive experiments under both feature skew and label skew settings have
validated the effectiveness of our proposed method, demonstrating improved
convergence speed and test accuracy, consistent with our theoretical findings.
</p></li>
</ul>

<h3>Title: Sparse Training for Federated Learning with Regularized Error Correction. (arXiv:2312.13795v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13795">http://arxiv.org/abs/2312.13795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13795]] Sparse Training for Federated Learning with Regularized Error Correction(http://arxiv.org/abs/2312.13795)</code></li>
<li>Summary: <p>Federated Learning (FL) has attracted much interest due to the significant
advantages it brings to training deep neural network (DNN) models. However,
since communications and computation resources are limited, training DNN models
in FL systems face challenges such as elevated computational and communication
costs in complex tasks. Sparse training schemes gain increasing attention in
order to scale down the dimensionality of each client (i.e., node)
transmission. Specifically, sparsification with error correction methods is a
promising technique, where only important updates are sent to the parameter
server (PS) and the rest are accumulated locally. While error correction
methods have shown to achieve a significant sparsification level of the
client-to-PS message without harming convergence, pushing sparsity further
remains unresolved due to the staleness effect. In this paper, we propose a
novel algorithm, dubbed Federated Learning with Accumulated Regularized
Embeddings (FLARE), to overcome this challenge. FLARE presents a novel sparse
training approach via accumulated pulling of the updated models with
regularization on the embeddings in the FL process, providing a powerful
solution to the staleness effect, and pushing sparsity to an exceptional level.
The performance of FLARE is validated through extensive experiments on diverse
and complex models, achieving a remarkable sparsity level (10 times and more
beyond the current state-of-the-art) along with significantly improved
accuracy. Additionally, an open-source software package has been developed for
the benefit of researchers and developers in related fields.
</p></li>
</ul>

<h3>Title: Fed-CO$_{2}$: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning. (arXiv:2312.13923v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13923">http://arxiv.org/abs/2312.13923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13923]] Fed-CO$_{2}$: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning(http://arxiv.org/abs/2312.13923)</code></li>
<li>Summary: <p>Federated Learning (FL) has emerged as a promising distributed learning
paradigm that enables multiple clients to learn a global model collaboratively
without sharing their private data. However, the effectiveness of FL is highly
dependent on the quality of the data that is being used for training. In
particular, data heterogeneity issues, such as label distribution skew and
feature skew, can significantly impact the performance of FL. Previous studies
in FL have primarily focused on addressing label distribution skew data
heterogeneity, while only a few recent works have made initial progress in
tackling feature skew issues. Notably, these two forms of data heterogeneity
have been studied separately and have not been well explored within a unified
FL framework. To address this gap, we propose Fed-CO$_{2}$, a universal FL
framework that handles both label distribution skew and feature skew within a
\textbf{C}ooperation mechanism between the \textbf{O}nline and \textbf{O}ffline
models. Specifically, the online model learns general knowledge that is shared
among all clients, while the offline model is trained locally to learn the
specialized knowledge of each individual client. To further enhance model
cooperation in the presence of feature shifts, we design an intra-client
knowledge transfer mechanism that reinforces mutual learning between the online
and offline models, and an inter-client knowledge transfer mechanism to
increase the models' domain generalization ability. Extensive experiments show
that our Fed-CO$_{2}$ outperforms a wide range of existing personalized
federated learning algorithms in terms of handling label distribution skew and
feature skew, both individually and collectively. The empirical results are
supported by our convergence analyses in a simplified setting.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis. (arXiv:2312.13834v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13834">http://arxiv.org/abs/2312.13834</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13834]] Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis(http://arxiv.org/abs/2312.13834)</code></li>
<li>Summary: <p>In this paper, we introduce Fairy, a minimalist yet robust adaptation of
image-editing diffusion models, enhancing them for video editing applications.
Our approach centers on the concept of anchor-based cross-frame attention, a
mechanism that implicitly propagates diffusion features across frames, ensuring
superior temporal coherence and high-fidelity synthesis. Fairy not only
addresses limitations of previous models, including memory and processing
speed. It also improves temporal consistency through a unique data augmentation
strategy. This strategy renders the model equivariant to affine transformations
in both source and target images. Remarkably efficient, Fairy generates
120-frame 512x384 videos (4-second duration at 30 FPS) in just 14 seconds,
outpacing prior works by at least 44x. A comprehensive user study, involving
1000 generated samples, confirms that our approach delivers superior quality,
decisively outperforming established methods.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: MFABA: A More Faithful and Accelerated Boundary-based Attribution Method for Deep Neural Networks. (arXiv:2312.13630v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13630">http://arxiv.org/abs/2312.13630</a></li>
<li>Code URL: <a href="https://github.com/lmbtough/mfaba">https://github.com/lmbtough/mfaba</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13630]] MFABA: A More Faithful and Accelerated Boundary-based Attribution Method for Deep Neural Networks(http://arxiv.org/abs/2312.13630)</code></li>
<li>Summary: <p>To better understand the output of deep neural networks (DNN), attribution
based methods have been an important approach for model interpretability, which
assign a score for each input dimension to indicate its importance towards the
model outcome. Notably, the attribution methods use the axioms of sensitivity
and implementation invariance to ensure the validity and reliability of
attribution results. Yet, the existing attribution methods present challenges
for effective interpretation and efficient computation. In this work, we
introduce MFABA, an attribution algorithm that adheres to axioms, as a novel
method for interpreting DNN. Additionally, we provide the theoretical proof and
in-depth analysis for MFABA algorithm, and conduct a large scale experiment.
The results demonstrate its superiority by achieving over 101.5142 times faster
speed than the state-of-the-art attribution algorithms. The effectiveness of
MFABA is thoroughly evaluated through the statistical analysis in comparison to
other methods, and the full implementation package is open-source at:
https://github.com/LMBTough/MFABA
</p></li>
</ul>

<h3>Title: Q-SENN: Quantized Self-Explaining Neural Networks. (arXiv:2312.13839v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13839">http://arxiv.org/abs/2312.13839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13839]] Q-SENN: Quantized Self-Explaining Neural Networks(http://arxiv.org/abs/2312.13839)</code></li>
<li>Summary: <p>Explanations in Computer Vision are often desired, but most Deep Neural
Networks can only provide saliency maps with questionable faithfulness.
Self-Explaining Neural Networks (SENN) extract interpretable concepts with
fidelity, diversity, and grounding to combine them linearly for
decision-making. While they can explain what was recognized, initial
realizations lack accuracy and general applicability. We propose the
Quantized-Self-Explaining Neural Network Q-SENN. Q-SENN satisfies or exceeds
the desiderata of SENN while being applicable to more complex datasets and
maintaining most or all of the accuracy of an uninterpretable baseline model,
out-performing previous work in all considered metrics. Q-SENN describes the
relationship between every class and feature as either positive, negative or
neutral instead of an arbitrary number of possible relations, enforcing more
binary human-friendly features. Since every class is assigned just 5
interpretable features on average, Q-SENN shows convincing local and global
interpretability. Additionally, we propose a feature alignment method, capable
of aligning learned features with human language-based concepts without
additional supervision. Thus, what is learned can be more easily verbalized.
The code is published: https://github.com/ThomasNorr/Q-SENN
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13307">http://arxiv.org/abs/2312.13307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13307]] Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models(http://arxiv.org/abs/2312.13307)</code></li>
<li>Summary: <p>Diffusion models have demonstrated remarkable efficacy in various generative
tasks with the predictive prowess of denoising model. Currently, these models
employ a uniform denoising approach across all timesteps. However, the inherent
variations in noisy latents at each timestep lead to conflicts during training,
constraining the potential of diffusion models. To address this challenge, we
propose a novel two-stage training strategy termed Step-Adaptive Training. In
the initial stage, a base denoising model is trained to encompass all
timesteps. Subsequently, we partition the timesteps into distinct groups,
fine-tuning the model within each group to achieve specialized denoising
capabilities. Recognizing that the difficulties of predicting noise at
different timesteps vary, we introduce a diverse model size requirement. We
dynamically adjust the model size for each timestep by estimating task
difficulty based on its signal-to-noise ratio before fine-tuning. This
adjustment is facilitated by a proxy-based structural importance assessment
mechanism, enabling precise and efficient pruning of the base denoising model.
Our experiments validate the effectiveness of the proposed training strategy,
demonstrating an improvement in the FID score on CIFAR10 by over 0.3 while
utilizing only 80\% of the computational resources. This innovative approach
not only enhances model performance but also significantly reduces
computational costs, opening new avenues for the development and application of
diffusion models.
</p></li>
</ul>

<h3>Title: Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style. (arXiv:2312.13309v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13309">http://arxiv.org/abs/2312.13309</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13309]] Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style(http://arxiv.org/abs/2312.13309)</code></li>
<li>Summary: <p>The state-of-the-art methods for e-commerce product background generation
suffer from the inefficiency of designing product-wise prompts when scaling up
the production, as well as the ineffectiveness of describing fine-grained
styles when customizing personalized backgrounds for some specific brands. To
address these obstacles, we integrate the category commonality and personalized
style into diffusion models. Concretely, we propose a Category-Wise Generator
to enable large-scale background generation for the first time. A unique
identifier in the prompt is assigned to each category, whose attention is
located on the background by a mask-guided cross attention layer to learn the
category-wise style. Furthermore, for products with specific and fine-grained
requirements in layout, elements, etc, a Personality-Wise Generator is devised
to learn such personalized style directly from a reference image to resolve
textual ambiguities, and is trained in a self-supervised manner for more
efficient training data usage. To advance research in this field, the first
large-scale e-commerce product background generation dataset BG60k is
constructed, which covers more than 60k product images from over 2k categories.
Experiments demonstrate that our method could generate high-quality backgrounds
for different categories, and maintain the personalized background style of
reference images. The link to BG60k and codes will be available soon.
</p></li>
</ul>

<h3>Title: Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13314">http://arxiv.org/abs/2312.13314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13314]] Unlocking Pre-trained Image Backbones for Semantic Image Synthesis(http://arxiv.org/abs/2312.13314)</code></li>
<li>Summary: <p>Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
</p></li>
</ul>

<h3>Title: ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors. (arXiv:2312.13324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13324">http://arxiv.org/abs/2312.13324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13324]] ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors(http://arxiv.org/abs/2312.13324)</code></li>
<li>Summary: <p>We introduce ShowRoom3D, a three-stage approach for generating high-quality
3D room-scale scenes from texts. Previous methods using 2D diffusion priors to
optimize neural radiance fields for generating room-scale scenes have shown
unsatisfactory quality. This is primarily attributed to the limitations of 2D
priors lacking 3D awareness and constraints in the training methodology. In
this paper, we utilize a 3D diffusion prior, MVDiffusion, to optimize the 3D
room-scale scene. Our contributions are in two aspects. Firstly, we propose a
progressive view selection process to optimize NeRF. This involves dividing the
training process into three stages, gradually expanding the camera sampling
scope. Secondly, we propose the pose transformation method in the second stage.
It will ensure MVDiffusion provide the accurate view guidance. As a result,
ShowRoom3D enables the generation of rooms with improved structural integrity,
enhanced clarity from any view, reduced content repetition, and higher
consistency across different perspectives. Extensive experiments demonstrate
that our method, significantly outperforms state-of-the-art approaches by a
large margin in terms of user study.
</p></li>
</ul>

<h3>Title: DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation. (arXiv:2312.13578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13578">http://arxiv.org/abs/2312.13578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13578]] DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation(http://arxiv.org/abs/2312.13578)</code></li>
<li>Summary: <p>The generation of emotional talking faces from a single portrait image
remains a significant challenge. The simultaneous achievement of expressive
emotional talking and accurate lip-sync is particularly difficult, as
expressiveness is often compromised for the accuracy of lip-sync. As widely
adopted by many prior works, the LSTM network often fails to capture the
subtleties and variations of emotional expressions. To address these
challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven
framework, tailored for generating diverse expressions and accurate lip-sync
concurrently. In the first stage, we propose EmoDiff, a novel diffusion module
that generates diverse highly dynamic emotional expressions and head poses in
accordance with the audio and the referenced emotion style. Given the strong
correlation between lip motion and audio, we then refine the dynamics with
enhanced lip-sync accuracy using audio features and emotion style. To this end,
we deploy a video-to-video rendering module to transfer the expressions and lip
motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively
and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of
expressiveness, lip-sync accuracy and perceptual quality.
</p></li>
</ul>

<h3>Title: Diff-Oracle: Diffusion Model for Oracle Character Generation with Controllable Styles and Contents. (arXiv:2312.13631v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13631">http://arxiv.org/abs/2312.13631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13631]] Diff-Oracle: Diffusion Model for Oracle Character Generation with Controllable Styles and Contents(http://arxiv.org/abs/2312.13631)</code></li>
<li>Summary: <p>Deciphering the oracle bone script plays a significant role in Chinese
archaeology and philology. However, it is significantly challenging due to the
scarcity of oracle character images. To overcome this issue, we propose
Diff-Oracle, based on diffusion models (DMs), to generate sufficient
controllable oracle characters. In contrast to most DMs that rely on text
prompts, we incorporate a style encoder to control style information during the
generation process. This encoder extracts style prompts from existing oracle
character images, where style details are converted from a CLIP model into a
text embedding format. Inspired by ControlNet, we introduce a content encoder
to capture desired content information from content images, ensuring the
fidelity of character glyphs. To train Diff-Oracle effectively, we propose to
obtain pixel-level paired oracle character images (i.e., style and content
images) by a pre-trained image-to-image translation model. Extensive
qualitative and quantitative experiments conducted on two benchmark datasets,
Oracle-241 and OBC306, demonstrate that our Diff-Oracle outperforms existing
generative methods in terms of image generation, further enhancing recognition
accuracy. Source codes will be available.
</p></li>
</ul>

<h3>Title: Free-Editor: Zero-shot Text-driven 3D Scene Editing. (arXiv:2312.13663v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13663">http://arxiv.org/abs/2312.13663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13663]] Free-Editor: Zero-shot Text-driven 3D Scene Editing(http://arxiv.org/abs/2312.13663)</code></li>
<li>Summary: <p>Text-to-Image (T2I) diffusion models have gained popularity recently due to
their multipurpose and easy-to-use nature, e.g. image and video generation as
well as editing. However, training a diffusion model specifically for 3D scene
editing is not straightforward due to the lack of large-scale datasets. To
date, editing 3D scenes requires either re-training the model to adapt to
various 3D edited scenes or design-specific methods for each special editing
type. Furthermore, state-of-the-art (SOTA) methods require multiple
synchronized edited images from the same scene to facilitate the scene editing.
Due to the current limitations of T2I models, it is very challenging to apply
consistent editing effects to multiple images, i.e. multi-view inconsistency in
editing. This in turn compromises the desired 3D scene editing performance if
these images are used. In our work, we propose a novel training-free 3D scene
editing technique, Free-Editor, which allows users to edit 3D scenes without
further re-training the model during test time. Our proposed method
successfully avoids the multi-view style inconsistency issue in SOTA methods
with the help of a "single-view editing" scheme. Specifically, we show that
editing a particular 3D scene can be performed by only modifying a single view.
To this end, we introduce an Edit Transformer that enforces intra-view
consistency and inter-view style transfer by utilizing self- and
cross-attention, respectively. Since it is no longer required to re-train the
model and edit every view in a scene, the editing time, as well as memory
resources, are reduced significantly, e.g., the runtime being $\sim \textbf{20}
\times$ faster than SOTA. We have conducted extensive experiments on a wide
range of benchmark datasets and achieve diverse editing capabilities with our
proposed technique.
</p></li>
</ul>

<h3>Title: DreamTuner: Single Image is Enough for Subject-Driven Generation. (arXiv:2312.13691v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13691">http://arxiv.org/abs/2312.13691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13691]] DreamTuner: Single Image is Enough for Subject-Driven Generation(http://arxiv.org/abs/2312.13691)</code></li>
<li>Summary: <p>Diffusion-based models have demonstrated impressive capabilities for
text-to-image generation and are expected for personalized applications of
subject-driven generation, which require the generation of customized concepts
with one or a few reference images. However, existing methods based on
fine-tuning fail to balance the trade-off between subject learning and the
maintenance of the generation capabilities of pretrained models. Moreover,
other methods that utilize additional image encoders tend to lose important
details of the subject due to encoding compression. To address these
challenges, we propose DreamTurner, a novel method that injects reference
information from coarse to fine to achieve subject-driven image generation more
effectively. DreamTurner introduces a subject-encoder for coarse subject
identity preservation, where the compressed general subject features are
introduced through an attention layer before visual-text cross-attention. We
then modify the self-attention layers within pretrained text-to-image models to
self-subject-attention layers to refine the details of the target subject. The
generated image queries detailed features from both the reference image and
itself in self-subject-attention. It is worth emphasizing that
self-subject-attention is an effective, elegant, and training-free method for
maintaining the detailed features of customized subjects and can serve as a
plug-and-play solution during inference. Finally, with additional
subject-driven fine-tuning, DreamTurner achieves remarkable performance in
subject-driven image generation, which can be controlled by a text or other
conditions such as pose. For further details, please visit the project page at
https://dreamtuner-diffusion.github.io/.
</p></li>
</ul>

<h3>Title: Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. (arXiv:2312.13763v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13763">http://arxiv.org/abs/2312.13763</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13763]] Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models(http://arxiv.org/abs/2312.13763)</code></li>
<li>Summary: <p>Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
</p></li>
</ul>

<h3>Title: Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models. (arXiv:2312.13913v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13913">http://arxiv.org/abs/2312.13913</a></li>
<li>Code URL: <a href="https://github.com/opentexture/paint3d">https://github.com/opentexture/paint3d</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13913]] Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models(http://arxiv.org/abs/2312.13913)</code></li>
<li>Summary: <p>This paper presents Paint3D, a novel coarse-to-fine generative framework that
is capable of producing high-resolution, lighting-less, and diverse 2K UV
texture maps for untextured 3D meshes conditioned on text or image inputs. The
key challenge addressed is generating high-quality textures without embedded
illumination information, which allows the textures to be re-lighted or
re-edited within modern graphics pipelines. To achieve this, our method first
leverages a pre-trained depth-aware 2D diffusion model to generate
view-conditional images and perform multi-view texture fusion, producing an
initial coarse texture map. However, as 2D models cannot fully represent 3D
shapes and disable lighting effects, the coarse texture map exhibits incomplete
areas and illumination artifacts. To resolve this, we train separate UV
Inpainting and UVHD diffusion models specialized for the shape-aware refinement
of incomplete areas and the removal of illumination artifacts. Through this
coarse-to-fine process, Paint3D can produce high-quality 2K UV textures that
maintain semantic consistency while being lighting-less, significantly
advancing the state-of-the-art in texturing 3D objects.
</p></li>
</ul>

<h3>Title: Controllable 3D Face Generation with Conditional Style Code Diffusion. (arXiv:2312.13941v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13941">http://arxiv.org/abs/2312.13941</a></li>
<li>Code URL: <a href="https://github.com/sxl142/tex-face">https://github.com/sxl142/tex-face</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13941]] Controllable 3D Face Generation with Conditional Style Code Diffusion(http://arxiv.org/abs/2312.13941)</code></li>
<li>Summary: <p>Generating photorealistic 3D faces from given conditions is a challenging
task. Existing methods often rely on time-consuming one-by-one optimization
approaches, which are not efficient for modeling the same distribution content,
e.g., faces. Additionally, an ideal controllable 3D face generation model
should consider both facial attributes and expressions. Thus we propose a novel
approach called TEx-Face(TExt &amp; Expression-to-Face) that addresses these
challenges by dividing the task into three components, i.e., 3D GAN Inversion,
Conditional Style Code Diffusion, and 3D Face Decoding. For 3D GAN inversion,
we introduce two methods which aim to enhance the representation of style codes
and alleviate 3D inconsistencies. Furthermore, we design a style code denoiser
to incorporate multiple conditions into the style code and propose a data
augmentation strategy to address the issue of insufficient paired
visual-language data. Extensive experiments conducted on FFHQ, CelebA-HQ, and
CelebA-Dialog demonstrate the promising performance of our TEx-Face in
achieving the efficient and controllable generation of photorealistic 3D faces.
The code will be available at https://github.com/sxl142/TEx-Face.
</p></li>
</ul>

<h3>Title: Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning. (arXiv:2312.13980v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13980">http://arxiv.org/abs/2312.13980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13980]] Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion Models with RL Finetuning(http://arxiv.org/abs/2312.13980)</code></li>
<li>Summary: <p>Recent advancements in the text-to-3D task leverage finetuned text-to-image
diffusion models to generate multi-view images, followed by NeRF
reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still
suffer from multi-view inconsistency and the resulting NeRF artifacts. Although
training longer with SFT improves consistency, it also causes distribution
shift, which reduces diversity and realistic details. We argue that the SFT of
multi-view diffusion models resembles the instruction finetuning stage of the
LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.
Essentially, RLFT methods optimize models beyond their SFT data distribution by
using their own outputs, effectively mitigating distribution shift. To this
end, we introduce Carve3D, a RLFT method coupled with the Multi-view
Reconstruction Consistency (MRC) metric, to improve the consistency of
multi-view diffusion models. To compute MRC on a set of multi-view images, we
compare them with their corresponding renderings of the reconstructed NeRF at
the same viewpoints. We validate the robustness of MRC with extensive
experiments conducted under controlled inconsistency levels. We enhance the
base RLFT algorithm to stabilize the training process, reduce distribution
shift, and identify scaling laws. Through qualitative and quantitative
experiments, along with a user study, we demonstrate Carve3D's improved
multi-view consistency, the resulting superior NeRF reconstruction quality, and
minimal distribution shift compared to longer SFT. Project webpage:
https://desaixie.github.io/carve-3d.
</p></li>
</ul>

<h3>Title: HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models. (arXiv:2312.14091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14091">http://arxiv.org/abs/2312.14091</a></li>
<li>Code URL: <a href="https://github.com/picsart-ai-research/hd-painter">https://github.com/picsart-ai-research/hd-painter</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14091]] HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models(http://arxiv.org/abs/2312.14091)</code></li>
<li>Summary: <p>Recent progress in text-guided image inpainting, based on the unprecedented
success of text-to-image diffusion models, has led to exceptionally realistic
and visually plausible results. However, there is still significant potential
for improvement in current text-to-image inpainting models, particularly in
better aligning the inpainted area with user prompts and performing
high-resolution inpainting. Therefore, in this paper we introduce HD-Painter, a
completely training-free approach that accurately follows to prompts and
coherently scales to high-resolution image inpainting. To this end, we design
the Prompt-Aware Introverted Attention (PAIntA) layer enhancing self-attention
scores by prompt information and resulting in better text alignment
generations. To further improve the prompt coherence we introduce the
Reweighting Attention Score Guidance (RASG) mechanism seamlessly integrating a
post-hoc sampling strategy into general form of DDIM to prevent
out-of-distribution latent shifts. Moreover, HD-Painter allows extension to
larger scales by introducing a specialized super-resolution technique
customized for inpainting, enabling the completion of missing regions in images
of up to 2K resolution. Our experiments demonstrate that HD-Painter surpasses
existing state-of-the-art approaches qualitatively and quantitatively,
achieving an impressive generation accuracy improvement of 61.4% vs 51.9%. We
will make the codes publicly available at:
https://github.com/Picsart-AI-Research/HD-Painter
</p></li>
</ul>

<h3>Title: Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation. (arXiv:2312.14124v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14124">http://arxiv.org/abs/2312.14124</a></li>
<li>Code URL: <a href="https://github.com/lmb-freiburg/neural-point-cloud-diffusion">https://github.com/lmb-freiburg/neural-point-cloud-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14124]] Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation(http://arxiv.org/abs/2312.14124)</code></li>
<li>Summary: <p>Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.
</p></li>
</ul>

<h3>Title: Diffusion Reward: Learning Rewards via Conditional Video Diffusion. (arXiv:2312.14134v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14134">http://arxiv.org/abs/2312.14134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14134]] Diffusion Reward: Learning Rewards via Conditional Video Diffusion(http://arxiv.org/abs/2312.14134)</code></li>
<li>Summary: <p>Learning rewards from expert videos offers an affordable and effective
solution to specify the intended behaviors for reinforcement learning tasks. In
this work, we propose Diffusion Reward, a novel framework that learns rewards
from expert videos via conditional video diffusion models for solving complex
visual RL problems. Our key insight is that lower generative diversity is
observed when conditioned on expert trajectories. Diffusion Reward is
accordingly formalized by the negative of conditional entropy that encourages
productive exploration of expert-like behaviors. We show the efficacy of our
method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual
input and sparse reward. Moreover, Diffusion Reward could even solve unseen
tasks successfully and effectively, largely surpassing baseline methods.
Project page and code: https://diffusion-reward.github.io/.
</p></li>
</ul>

<h3>Title: Navigating the Structured What-If Spaces: Counterfactual Generation via Structured Diffusion. (arXiv:2312.13616v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13616">http://arxiv.org/abs/2312.13616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13616]] Navigating the Structured What-If Spaces: Counterfactual Generation via Structured Diffusion(http://arxiv.org/abs/2312.13616)</code></li>
<li>Summary: <p>Generating counterfactual explanations is one of the most effective
approaches for uncovering the inner workings of black-box neural network models
and building user trust. While remarkable strides have been made in generative
modeling using diffusion models in domains like vision, their utility in
generating counterfactual explanations in structured modalities remains
unexplored. In this paper, we introduce Structured Counterfactual Diffuser or
SCD, the first plug-and-play framework leveraging diffusion for generating
counterfactual explanations in structured data. SCD learns the underlying data
distribution via a diffusion model which is then guided at test time to
generate counterfactuals for any arbitrary black-box model, input, and desired
prediction. Our experiments show that our counterfactuals not only exhibit high
plausibility compared to the existing state-of-the-art but also show
significantly better proximity and diversity.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: CR-SAM: Curvature Regularized Sharpness-Aware Minimization. (arXiv:2312.13555v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13555">http://arxiv.org/abs/2312.13555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13555]] CR-SAM: Curvature Regularized Sharpness-Aware Minimization(http://arxiv.org/abs/2312.13555)</code></li>
<li>Summary: <p>The capacity to generalize to future unseen data stands as one of the utmost
crucial attributes of deep neural networks. Sharpness-Aware Minimization (SAM)
aims to enhance the generalizability by minimizing worst-case loss using
one-step gradient ascent as an approximation. However, as training progresses,
the non-linearity of the loss landscape increases, rendering one-step gradient
ascent less effective. On the other hand, multi-step gradient ascent will incur
higher training cost. In this paper, we introduce a normalized Hessian trace to
accurately measure the curvature of loss landscape on {\em both} training and
test sets. In particular, to counter excessive non-linearity of loss landscape,
we propose Curvature Regularized SAM (CR-SAM), integrating the normalized
Hessian trace as a SAM regularizer. Additionally, we present an efficient way
to compute the trace via finite differences with parallelism. Our theoretical
analysis based on PAC-Bayes bounds establishes the regularizer's efficacy in
reducing generalization error. Empirical evaluation on CIFAR and ImageNet
datasets shows that CR-SAM consistently enhances classification performance for
ResNet and Vision Transformer (ViT) models across various datasets. Our code is
available at https://github.com/TrustAIoT/CR-SAM.
</p></li>
</ul>

<h3>Title: The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction. (arXiv:2312.13558v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13558">http://arxiv.org/abs/2312.13558</a></li>
<li>Code URL: <a href="https://github.com/pratyushasharma/laser">https://github.com/pratyushasharma/laser</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13558]] The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction(http://arxiv.org/abs/2312.13558)</code></li>
<li>Summary: <p>Transformer-based Large Language Models (LLMs) have become a fixture in
modern machine learning. Correspondingly, significant resources are allocated
towards research that aims to further advance this technology, typically
resulting in models of increasing size that are trained on increasing amounts
of data. This work, however, demonstrates the surprising result that it is
often possible to significantly improve the performance of LLMs by selectively
removing higher-order components of their weight matrices. This simple
intervention, which we call LAyer-SElective Rank reduction (LASER), can be done
on a model after training has completed, and requires no additional parameters
or data. We show extensive experiments demonstrating the generality of this
finding across language models and datasets, and provide in-depth analyses
offering insights into both when LASER is effective and the mechanism by which
it operates.
</p></li>
</ul>

<h3>Title: Ponymation: Learning 3D Animal Motions from Unlabeled Online Videos. (arXiv:2312.13604v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13604">http://arxiv.org/abs/2312.13604</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13604]] Ponymation: Learning 3D Animal Motions from Unlabeled Online Videos(http://arxiv.org/abs/2312.13604)</code></li>
<li>Summary: <p>We introduce Ponymation, a new method for learning a generative model of
articulated 3D animal motions from raw, unlabeled online videos. Unlike
existing approaches for motion synthesis, our model does not require any pose
annotations or parametric shape models for training, and is learned purely from
a collection of raw video clips obtained from the Internet. We build upon a
recent work, MagicPony, which learns articulated 3D animal shapes purely from
single image collections, and extend it on two fronts. First, instead of
training on static images, we augment the framework with a video training
pipeline that incorporates temporal regularizations, achieving more accurate
and temporally consistent reconstructions. Second, we learn a generative model
of the underlying articulated 3D motion sequences via a spatio-temporal
transformer VAE, simply using 2D reconstruction losses without relying on any
explicit pose annotations. At inference time, given a single 2D image of a new
animal instance, our model reconstructs an articulated, textured 3D mesh, and
generates plausible 3D animations by sampling from the learned motion latent
space.
</p></li>
</ul>

<h3>Title: DECO: Query-Based End-to-End Object Detection with ConvNets. (arXiv:2312.13735v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13735">http://arxiv.org/abs/2312.13735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13735]] DECO: Query-Based End-to-End Object Detection with ConvNets(http://arxiv.org/abs/2312.13735)</code></li>
<li>Summary: <p>Detection Transformer (DETR) and its variants have shown great potential for
accurate object detection in recent years. The mechanism of object query
enables DETR family to directly obtain a fixed number of object predictions and
streamlines the detection pipeline. Meanwhile, recent studies also reveal that
with proper architecture design, convolution networks (ConvNets) also achieve
competitive performance with transformers, \eg, ConvNeXt. To this end, in this
paper we explore whether we could build a query-based end-to-end object
detection framework with ConvNets instead of sophisticated transformer
architecture. The proposed framework, \ie, Detection ConvNet (DECO), is
composed of a backbone and convolutional encoder-decoder architecture. We
carefully design the DECO encoder and propose a novel mechanism for our DECO
decoder to perform interaction between object queries and image features via
convolutional layers. We compare the proposed DECO against prior detectors on
the challenging COCO benchmark. Despite its simplicity, our DECO achieves
competitive performance in terms of detection accuracy and running speed.
Specifically, with the ResNet-50 and ConvNeXt-Tiny backbone, DECO obtains
$38.6\%$ and $40.8\%$ AP on COCO \textit{val} set with $35$ and $28$ FPS
respectively and outperforms the DETR model. Incorporated with advanced
multi-scale feature module, our DECO+ achieves $47.8\%$ AP with $34$ FPS. We
hope the proposed DECO brings another perspective for designing object
detection framework.
</p></li>
</ul>

<h3>Title: A Strong Baseline for Temporal Video-Text Alignment. (arXiv:2312.14055v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14055">http://arxiv.org/abs/2312.14055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14055]] A Strong Baseline for Temporal Video-Text Alignment(http://arxiv.org/abs/2312.14055)</code></li>
<li>Summary: <p>In this paper, we consider the problem of temporally aligning the video and
texts from instructional videos, specifically, given a long-term video, and
associated text sentences, our goal is to determine their corresponding
timestamps in the video. To this end, we establish a simple, yet strong model
that adopts a Transformer-based architecture with all texts as queries,
iteratively attending to the visual features, to infer the optimal timestamp.
We conduct thorough experiments to investigate: (i) the effect of upgrading ASR
systems to reduce errors from speech recognition, (ii) the effect of various
visual-textual backbones, ranging from CLIP to S3D, to the more recent
InternVideo, (iii) the effect of transforming noisy ASR transcripts into
descriptive steps by prompting a large language model (LLM), to summarize the
core activities within the ASR transcript as a new training dataset. As a
result, our proposed simple model demonstrates superior performance on both
narration alignment and procedural step grounding tasks, surpassing existing
state-of-the-art methods by a significant margin on three public benchmarks,
namely, 9.3% on HT-Step, 3.4% on HTM-Align and 4.7% on CrossTask. We believe
the proposed model and dataset with descriptive steps can be treated as a
strong baseline for future research in temporal video-text alignment. All
codes, models, and the resulting dataset will be publicly released to the
research community.
</p></li>
</ul>

<h3>Title: DUSt3R: Geometric 3D Vision Made Easy. (arXiv:2312.14132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14132">http://arxiv.org/abs/2312.14132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14132]] DUSt3R: Geometric 3D Vision Made Easy(http://arxiv.org/abs/2312.14132)</code></li>
<li>Summary: <p>Multi-view stereo reconstruction (MVS) in the wild requires to first estimate
the camera parameters e.g. intrinsic and extrinsic parameters. These are
usually tedious and cumbersome to obtain, yet they are mandatory to triangulate
corresponding pixels in 3D space, which is the core of all best performing MVS
algorithms. In this work, we take an opposite stance and introduce DUSt3R, a
radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction
of arbitrary image collections, i.e. operating without prior information about
camera calibration nor viewpoint poses. We cast the pairwise reconstruction
problem as a regression of pointmaps, relaxing the hard constraints of usual
projective camera models. We show that this formulation smoothly unifies the
monocular and binocular reconstruction cases. In the case where more than two
images are provided, we further propose a simple yet effective global alignment
strategy that expresses all pairwise pointmaps in a common reference frame. We
base our network architecture on standard Transformer encoders and decoders,
allowing us to leverage powerful pretrained models. Our formulation directly
provides a 3D model of the scene as well as depth information, but
interestingly, we can seamlessly recover from it, pixel matches, relative and
absolute camera. Exhaustive experiments on all these tasks showcase that the
proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on
monocular/multi-view depth estimation as well as relative pose estimation. In
summary, DUSt3R makes many geometric 3D vision tasks easy.
</p></li>
</ul>

<h3>Title: SimQ-NAS: Simultaneous Quantization Policy and Neural Architecture Search. (arXiv:2312.13301v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13301">http://arxiv.org/abs/2312.13301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13301]] SimQ-NAS: Simultaneous Quantization Policy and Neural Architecture Search(http://arxiv.org/abs/2312.13301)</code></li>
<li>Summary: <p>Recent one-shot Neural Architecture Search algorithms rely on training a
hardware-agnostic super-network tailored to a specific task and then extracting
efficient sub-networks for different hardware platforms. Popular approaches
separate the training of super-networks from the search for sub-networks, often
employing predictors to alleviate the computational overhead associated with
search. Additionally, certain methods also incorporate the quantization policy
within the search space. However, while the quantization policy search for
convolutional neural networks is well studied, the extension of these methods
to transformers and especially foundation models remains under-explored. In
this paper, we demonstrate that by using multi-objective search algorithms
paired with lightly trained predictors, we can efficiently search for both the
sub-network architecture and the corresponding quantization policy and
outperform their respective baselines across different performance objectives
such as accuracy, model size, and latency. Specifically, we demonstrate that
our approach performs well across both uni-modal (ViT and BERT) and multi-modal
(BEiT-3) transformer-based architectures as well as convolutional architectures
(ResNet). For certain networks, we demonstrate an improvement of up to $4.80x$
and $3.44x$ for latency and model size respectively, without degradation in
accuracy compared to the fully quantized INT8 baselines.
</p></li>
</ul>

<h3>Title: Anchoring Path for Inductive Relation Prediction in Knowledge Graphs. (arXiv:2312.13596v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13596">http://arxiv.org/abs/2312.13596</a></li>
<li>Code URL: <a href="https://github.com/zhixiangsu/apst">https://github.com/zhixiangsu/apst</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13596]] Anchoring Path for Inductive Relation Prediction in Knowledge Graphs(http://arxiv.org/abs/2312.13596)</code></li>
<li>Summary: <p>Aiming to accurately predict missing edges representing relations between
entities, which are pervasive in real-world Knowledge Graphs (KGs), relation
prediction plays a critical role in enhancing the comprehensiveness and utility
of KGs. Recent research focuses on path-based methods due to their inductive
and explainable properties. However, these methods face a great challenge when
lots of reasoning paths do not form Closed Paths (CPs) in the KG. To address
this challenge, we propose Anchoring Path Sentence Transformer (APST) by
introducing Anchoring Paths (APs) to alleviate the reliance of CPs.
Specifically, we develop a search-based description retrieval method to enrich
entity descriptions and an assessment mechanism to evaluate the rationality of
APs. APST takes both APs and CPs as the inputs of a unified Sentence
Transformer architecture, enabling comprehensive predictions and high-quality
explanations. We evaluate APST on three public datasets and achieve
state-of-the-art (SOTA) performance in 30 of 36 transductive, inductive, and
few-shot experimental settings.
</p></li>
</ul>

<h3>Title: Critic-Guided Decision Transformer for Offline Reinforcement Learning. (arXiv:2312.13716v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13716">http://arxiv.org/abs/2312.13716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13716]] Critic-Guided Decision Transformer for Offline Reinforcement Learning(http://arxiv.org/abs/2312.13716)</code></li>
<li>Summary: <p>Recent advancements in offline reinforcement learning (RL) have underscored
the capabilities of Return-Conditioned Supervised Learning (RCSL), a paradigm
that learns the action distribution based on target returns for each state in a
supervised manner. However, prevailing RCSL methods largely focus on
deterministic trajectory modeling, disregarding stochastic state transitions
and the diversity of future trajectory distributions. A fundamental challenge
arises from the inconsistency between the sampled returns within individual
trajectories and the expected returns across multiple trajectories.
Fortunately, value-based methods offer a solution by leveraging a value
function to approximate the expected returns, thereby addressing the
inconsistency effectively. Building upon these insights, we propose a novel
approach, termed the Critic-Guided Decision Transformer (CGDT), which combines
the predictability of long-term returns from value-based methods with the
trajectory modeling capability of the Decision Transformer. By incorporating a
learned value function, known as the critic, CGDT ensures a direct alignment
between the specified target returns and the expected returns of actions. This
integration bridges the gap between the deterministic nature of RCSL and the
probabilistic characteristics of value-based methods. Empirical evaluations on
stochastic environments and D4RL benchmark datasets demonstrate the superiority
of CGDT over traditional RCSL methods. These results highlight the potential of
CGDT to advance the state of the art in offline RL and extend the applicability
of RCSL to a wide range of RL tasks.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: SPDGAN: A Generative Adversarial Network based on SPD Manifold Learning for Automatic Image Colorization. (arXiv:2312.13506v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13506">http://arxiv.org/abs/2312.13506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13506]] SPDGAN: A Generative Adversarial Network based on SPD Manifold Learning for Automatic Image Colorization(http://arxiv.org/abs/2312.13506)</code></li>
<li>Summary: <p>This paper addresses the automatic colorization problem, which converts a
gray-scale image to a colorized one. Recent deep-learning approaches can
colorize automatically grayscale images. However, when it comes to different
scenes which contain distinct color styles, it is difficult to accurately
capture the color characteristics. In this work, we propose a fully automatic
colorization approach based on Symmetric Positive Definite (SPD) Manifold
Learning with a generative adversarial network (SPDGAN) that improves the
quality of the colorization results. Our SPDGAN model establishes an
adversarial game between two discriminators and a generator. The latter is
based on ResNet architecture with few alterations. Its goal is to generate fake
colorized images without losing color information across layers through
residual connections. Then, we employ two discriminators from different
domains. The first one is devoted to the image pixel domain, while the second
one is to the Riemann manifold domain which helps to avoid color misalignment.
Extensive experiments are conducted on the Places365 and COCO-stuff databases
to test the effect of each component of our SPDGAN. In addition, quantitative
and qualitative comparisons with state-of-the-art methods demonstrate the
effectiveness of our model by achieving more realistic colorized images with
less artifacts visually, and good results of PSNR, SSIM, and FID values.
</p></li>
</ul>

<h3>Title: A Comprehensive End-to-End Computer Vision Framework for Restoration and Recognition of Low-Quality Engineering Drawings. (arXiv:2312.13620v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13620">http://arxiv.org/abs/2312.13620</a></li>
<li>Code URL: <a href="https://github.com/Lattle-y/AI-recognition-for-lq-ed">https://github.com/Lattle-y/AI-recognition-for-lq-ed</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13620]] A Comprehensive End-to-End Computer Vision Framework for Restoration and Recognition of Low-Quality Engineering Drawings(http://arxiv.org/abs/2312.13620)</code></li>
<li>Summary: <p>The digitization of engineering drawings is crucial for efficient reuse,
distribution, and archiving. Existing computer vision approaches for digitizing
engineering drawings typically assume the input drawings have high quality.
However, in reality, engineering drawings are often blurred and distorted due
to improper scanning, storage, and transmission, which may jeopardize the
effectiveness of existing approaches. This paper focuses on restoring and
recognizing low-quality engineering drawings, where an end-to-end framework is
proposed to improve the quality of the drawings and identify the graphical
symbols on them. The framework uses K-means clustering to classify different
engineering drawing patches into simple and complex texture patches based on
their gray level co-occurrence matrix statistics. Computer vision operations
and a modified Enhanced Super-Resolution Generative Adversarial Network
(ESRGAN) model are then used to improve the quality of the two types of
patches, respectively. A modified Faster Region-based Convolutional Neural
Network (Faster R-CNN) model is used to recognize the quality-enhanced
graphical symbols. Additionally, a multi-stage task-driven collaborative
learning strategy is proposed to train the modified ESRGAN and Faster R-CNN
models to improve the resolution of engineering drawings in the direction that
facilitates graphical symbol recognition, rather than human visual perception.
A synthetic data generation method is also proposed to construct
quality-degraded samples for training the framework. Experiments on real-world
electrical diagrams show that the proposed framework achieves an accuracy of
98.98% and a recall of 99.33%, demonstrating its superiority over previous
approaches. Moreover, the framework is integrated into a widely-used power
system software application to showcase its practicality.
</p></li>
</ul>

<h3>Title: Gaussian Splitting Algorithm with Color and Opacity Depended on Viewing Direction. (arXiv:2312.13729v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13729">http://arxiv.org/abs/2312.13729</a></li>
<li>Code URL: <a href="https://github.com/gmum/ViewingDirectionGaussianSplatting">https://github.com/gmum/ViewingDirectionGaussianSplatting</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13729]] Gaussian Splitting Algorithm with Color and Opacity Depended on Viewing Direction(http://arxiv.org/abs/2312.13729)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of
neural networks to capture the intricacies of 3D objects. By encoding the shape
and color information within neural network weights, NeRFs excel at producing
strikingly sharp novel views of 3D objects. Recently, numerous generalizations
of NeRFs utilizing generative models have emerged, expanding its versatility.
In contrast, Gaussian Splatting (GS) offers a similar renders quality with
faster training and inference as it does not need neural networks to work. We
encode information about the 3D objects in the set of Gaussian distributions
that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are
difficult to condition since they usually require circa hundred thousand
Gaussian components. To mitigate the caveats of both models, we propose a
hybrid model that uses GS representation of the 3D object's shape and
NeRF-based encoding of color and opacity. Our model uses Gaussian distributions
with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of
Gaussian), color and opacity, and neural network, which takes parameters of
Gaussian and viewing direction to produce changes in color and opacity.
Consequently, our model better describes shadows, light reflections, and
transparency of 3D objects.
</p></li>
</ul>

<h3>Title: HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs. (arXiv:2312.14140v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14140">http://arxiv.org/abs/2312.14140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14140]] HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs(http://arxiv.org/abs/2312.14140)</code></li>
<li>Summary: <p>Current advances in human head modeling allow to generate plausible-looking
3D head models via neural representations. Nevertheless, constructing complete
high-fidelity head models with explicitly controlled animation remains an
issue. Furthermore, completing the head geometry based on a partial
observation, e.g. coming from a depth sensor, while preserving details is often
problematic for the existing methods. We introduce a generative model for
detailed 3D head meshes on top of an articulated 3DMM which allows explicit
animation and high-detail preservation at the same time. Our method is trained
in two stages. First, we register a parametric head model with vertex
displacements to each mesh of the recently introduced NPHM dataset of accurate
3D head scans. The estimated displacements are baked into a hand-crafted UV
layout. Second, we train a StyleGAN model in order to generalize over the UV
maps of displacements. The decomposition of the parametric model and
high-quality vertex displacements allows us to animate the model and modify it
semantically. We demonstrate the results of unconditional generation and
fitting to the full or partial observation. The project page is available at
https://seva100.github.io/headcraft.
</p></li>
</ul>

<h3>Title: Virtual Pets: Animatable Animal Generation in 3D Scenes. (arXiv:2312.14154v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14154">http://arxiv.org/abs/2312.14154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14154]] Virtual Pets: Animatable Animal Generation in 3D Scenes(http://arxiv.org/abs/2312.14154)</code></li>
<li>Summary: <p>Toward unlocking the potential of generative models in immersive 4D
experiences, we introduce Virtual Pet, a novel pipeline to model realistic and
diverse motions for target animal species within a 3D environment. To
circumvent the limited availability of 3D motion data aligned with
environmental geometry, we leverage monocular internet videos and extract
deformable NeRF representations for the foreground and static NeRF
representations for the background. For this, we develop a reconstruction
strategy, encompassing species-level shared template learning and per-video
fine-tuning. Utilizing the reconstructed data, we then train a conditional 3D
motion model to learn the trajectory and articulation of foreground animals in
the context of 3D backgrounds. We showcase the efficacy of our pipeline with
comprehensive qualitative and quantitative evaluations using cat videos. We
also demonstrate versatility across unseen cats and indoor environments,
producing temporally coherent 4D outputs for enriched virtual experiences.
</p></li>
</ul>

<h3>Title: Structure-Aware Path Inference for Neural Finite State Transducers. (arXiv:2312.13614v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13614">http://arxiv.org/abs/2312.13614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13614]] Structure-Aware Path Inference for Neural Finite State Transducers(http://arxiv.org/abs/2312.13614)</code></li>
<li>Summary: <p>Neural finite-state transducers (NFSTs) form an expressive family of
neurosymbolic sequence transduction models. An NFST models each string pair as
having been generated by a latent path in a finite-state transducer. As they
are deep generative models, both training and inference of NFSTs require
inference networks that approximate posterior distributions over such latent
variables. In this paper, we focus on the resulting challenge of imputing the
latent alignment path that explains a given pair of input and output strings
(e.g., during training). We train three autoregressive approximate models for
amortized inference of the path, which can then be used as proposal
distributions for importance sampling. All three models perform lookahead. Our
most sophisticated (and novel) model leverages the FST structure to consider
the graph of future paths; unfortunately, we find that it loses out to the
simpler approaches -- except on an artificial task that we concocted to confuse
the simpler approaches.
</p></li>
</ul>

<h3>Title: ChatGPT as a commenter to the news: can LLMs generate human-like opinions?. (arXiv:2312.13961v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13961">http://arxiv.org/abs/2312.13961</a></li>
<li>Code URL: <a href="https://github.com/raydentseng/generated_opinions">https://github.com/raydentseng/generated_opinions</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13961]] ChatGPT as a commenter to the news: can LLMs generate human-like opinions?(http://arxiv.org/abs/2312.13961)</code></li>
<li>Summary: <p>ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn
significant attention since their release, and the abilities of these models
have been investigated for a wide variety of tasks. In this research we
investigate to what extent GPT-3.5 can generate human-like comments on Dutch
news articles. We define human likeness as `not distinguishable from human
comments', approximated by the difficulty of automatic classification between
human and GPT comments. We analyze human likeness across multiple prompting
techniques. In particular, we utilize zero-shot, few-shot and context prompts,
for two generated personas. We found that our fine-tuned BERT models can easily
distinguish human-written comments from GPT-3.5 generated comments, with none
of the used prompting methods performing noticeably better. We further analyzed
that human comments consistently showed higher lexical diversity than
GPT-generated comments. This indicates that although generative LLMs can
generate fluent text, their capability to create human-like opinionated
comments is still limited.
</p></li>
</ul>

<h3>Title: RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios. (arXiv:2312.13303v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13303">http://arxiv.org/abs/2312.13303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13303]] RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios(http://arxiv.org/abs/2312.13303)</code></li>
<li>Summary: <p>Simulation plays a crucial role in the development of autonomous vehicles
(AVs) due to the potential risks associated with real-world testing. Although
significant progress has been made in the visual aspects of simulators,
generating complex behavior among agents remains a formidable challenge. It is
not only imperative to ensure realism in the scenarios generated but also
essential to incorporate preferences and conditions to facilitate controllable
generation for AV training and evaluation. Traditional methods, mainly relying
on memorizing the distribution of training datasets, often fall short in
generating unseen scenarios. Inspired by the success of retrieval augmented
generation in large language models, we present RealGen, a novel
retrieval-based in-context learning framework for traffic scenario generation.
RealGen synthesizes new scenarios by combining behaviors from multiple
retrieved examples in a gradient-free way, which may originate from templates
or tagged scenarios. This in-context learning framework endows versatile
generative capabilities, including the ability to edit scenarios, compose
various behaviors, and produce critical scenarios. Evaluations show that
RealGen offers considerable flexibility and controllability, marking a new
direction in the field of controllable traffic scenario generation. Check our
project website for more information: https://realgen.github.io.
</p></li>
</ul>

<h3>Title: Fine-tuning Graph Neural Networks by Preserving Graph Generative Patterns. (arXiv:2312.13583v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13583">http://arxiv.org/abs/2312.13583</a></li>
<li>Code URL: <a href="https://github.com/zjunet/G-Tuning">https://github.com/zjunet/G-Tuning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13583]] Fine-tuning Graph Neural Networks by Preserving Graph Generative Patterns(http://arxiv.org/abs/2312.13583)</code></li>
<li>Summary: <p>Recently, the paradigm of pre-training and fine-tuning graph neural networks
has been intensively studied and applied in a wide range of graph mining tasks.
Its success is generally attributed to the structural consistency between
pre-training and downstream datasets, which, however, does not hold in many
real-world scenarios. Existing works have shown that the structural divergence
between pre-training and downstream graphs significantly limits the
transferability when using the vanilla fine-tuning strategy. This divergence
leads to model overfitting on pre-training graphs and causes difficulties in
capturing the structural properties of the downstream graphs. In this paper, we
identify the fundamental cause of structural divergence as the discrepancy of
generative patterns between the pre-training and downstream graphs.
Furthermore, we propose G-Tuning to preserve the generative patterns of
downstream graphs. Given a downstream graph G, the core idea is to tune the
pre-trained GNN so that it can reconstruct the generative patterns of G, the
graphon W. However, the exact reconstruction of a graphon is known to be
computationally expensive. To overcome this challenge, we provide a theoretical
analysis that establishes the existence of a set of alternative graphons called
graphon bases for any given graphon. By utilizing a linear combination of these
graphon bases, we can efficiently approximate W. This theoretical finding forms
the basis of our proposed model, as it enables effective learning of the
graphon bases and their associated coefficients. Compared with existing
algorithms, G-Tuning demonstrates an average improvement of 0.5% and 2.6% on
in-domain and out-of-domain transfer learning experiments, respectively.
</p></li>
</ul>

<h3>Title: Adapt & Align: Continual Learning with Generative Models Latent Space Alignment. (arXiv:2312.13699v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13699">http://arxiv.org/abs/2312.13699</a></li>
<li>Code URL: <a href="https://github.com/jrx-napoli/cl_classifier">https://github.com/jrx-napoli/cl_classifier</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13699]] Adapt & Align: Continual Learning with Generative Models Latent Space Alignment(http://arxiv.org/abs/2312.13699)</code></li>
<li>Summary: <p>In this work, we introduce Adapt &amp; Align, a method for continual learning of
neural networks by aligning latent representations in generative models. Neural
Networks suffer from abrupt loss in performance when retrained with additional
training data from different distributions. At the same time, training with
additional data without access to the previous examples rarely improves the
model's performance. In this work, we propose a new method that mitigates those
problems by employing generative models and splitting the process of their
update into two parts. In the first one, we train a local generative model
using only data from a new task. In the second phase, we consolidate latent
representations from the local model with a global one that encodes knowledge
of all past experiences. We introduce our approach with Variational
Auteoncoders and Generative Adversarial Networks. Moreover, we show how we can
use those generative models as a general method for continual knowledge
consolidation that can be used in downstream tasks such as classification.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training. (arXiv:2312.13316v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13316">http://arxiv.org/abs/2312.13316</a></li>
<li>Code URL: <a href="https://github.com/tonichopp/ecamp">https://github.com/tonichopp/ecamp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13316]] ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training(http://arxiv.org/abs/2312.13316)</code></li>
<li>Summary: <p>Despite significant advancements in medical vision-language pre-training,
existing methods have largely overlooked the inherent entity-specific context
within radiology reports and the complex cross-modality contextual
relationships between text and images. To close this gap, we propose a novel
Entity-centered Context-aware Medical Vision-language Pre-training (ECAMP)
framework, which is designed to enable a more entity-centered and
context-sensitive interpretation of medical data. Utilizing the recent powerful
large language model, we distill entity-centered context from medical reports,
which enables ECAMP to gain more effective supervision from the text modality.
By further pre-training our model with carefully designed entity-aware,
context-enhanced masked language modeling and context-guided super-resolution
tasks, ECAMP significantly refines the interplay between text and image
modalities, leading to an enhanced ability to extract entity-centered
contextual features. Besides, our proposed multi-scale context fusion design
also improves the semantic integration of both coarse and fine-level image
representations, prompting better performance for multi-scale downstream
applications. Combining these components leads to significant performance leaps
over current state-of-the-art methods and establishes a new standard for
cross-modality learning in medical imaging, whose effectiveness is demonstrated
by our extensive experiments on various tasks including classification,
segmentation, and detection across several public datasets. Code and models are
available at https://github.com/ToniChopp/ECAMP.
</p></li>
</ul>

<h3>Title: AppAgent: Multimodal Agents as Smartphone Users. (arXiv:2312.13771v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13771">http://arxiv.org/abs/2312.13771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13771]] AppAgent: Multimodal Agents as Smartphone Users(http://arxiv.org/abs/2312.13771)</code></li>
<li>Summary: <p>Recent advancements in large language models (LLMs) have led to the creation
of intelligent agents capable of performing complex tasks. This paper
introduces a novel LLM-based multimodal agent framework designed to operate
smartphone applications. Our framework enables the agent to operate smartphone
applications through a simplified action space, mimicking human-like
interactions such as tapping and swiping. This novel approach bypasses the need
for system back-end access, thereby broadening its applicability across diverse
apps. Central to our agent's functionality is its innovative learning method.
The agent learns to navigate and use new apps either through autonomous
exploration or by observing human demonstrations. This process generates a
knowledge base that the agent refers to for executing complex tasks across
different applications. To demonstrate the practicality of our agent, we
conducted extensive testing over 50 tasks in 10 different applications,
including social media, email, maps, shopping, and sophisticated image editing
tools. The results affirm our agent's proficiency in handling a diverse array
of high-level tasks.
</p></li>
</ul>

<h3>Title: LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding. (arXiv:2312.14074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14074">http://arxiv.org/abs/2312.14074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14074]] LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding(http://arxiv.org/abs/2312.14074)</code></li>
<li>Summary: <p>Recently, Large Language Models (LLMs) and Multimodal Large Language Models
(MLLMs) have shown promise in instruction following and 2D image understanding.
While these models are powerful, they have not yet been developed to comprehend
the more challenging 3D physical scenes, especially when it comes to the sparse
outdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw
LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs
to gain a comprehensive understanding of outdoor 3D scenes. The central insight
of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a
language modeling problem, encompassing tasks such as 3D captioning, 3D
grounding, 3D question answering, etc. Specifically, due to the scarcity of 3D
LiDAR-text pairing data, we introduce a three-stage training strategy and
generate relevant datasets, progressively aligning the 3D modality with the
language embedding space of LLM. Furthermore, we design a View-Aware
Transformer (VAT) to connect the 3D encoder with the LLM, which effectively
bridges the modality gap and enhances the LLM's spatial orientation
comprehension of visual features. Our experiments show that LiDAR-LLM possesses
favorable capabilities to comprehend various instructions regarding 3D scenes
and engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the
3D captioning task and achieves a 63.1\% classification accuracy and a 14.3\%
BEV mIoU on the 3D grounding task. Web page:
https://sites.google.com/view/lidar-llm
</p></li>
</ul>

<h3>Title: VideoPoet: A Large Language Model for Zero-Shot Video Generation. (arXiv:2312.14125v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14125">http://arxiv.org/abs/2312.14125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14125]] VideoPoet: A Large Language Model for Zero-Shot Video Generation(http://arxiv.org/abs/2312.14125)</code></li>
<li>Summary: <p>We present VideoPoet, a language model capable of synthesizing high-quality
video, with matching audio, from a large variety of conditioning signals.
VideoPoet employs a decoder-only transformer architecture that processes
multimodal inputs -- including images, videos, text, and audio. The training
protocol follows that of Large Language Models (LLMs), consisting of two
stages: pretraining and task-specific adaptation. During pretraining, VideoPoet
incorporates a mixture of multimodal generative objectives within an
autoregressive Transformer framework. The pretrained LLM serves as a foundation
that can be adapted for a range of video generation tasks. We present empirical
results demonstrating the model's state-of-the-art capabilities in zero-shot
video generation, specifically highlighting VideoPoet's ability to generate
high-fidelity motions. Project page: <a href="http://sites.research.google/videopoet/">this http URL</a>
</p></li>
</ul>

<h3>Title: Developing Interactive Tourism Planning: A Dialogue Robot System Powered by a Large Language Mode. (arXiv:2312.13545v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13545">http://arxiv.org/abs/2312.13545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13545]] Developing Interactive Tourism Planning: A Dialogue Robot System Powered by a Large Language Mode(http://arxiv.org/abs/2312.13545)</code></li>
<li>Summary: <p>In recent years, large language models (LLMs) have rapidly proliferated and
have been utilized in various tasks, including research in dialogue systems. We
aimed to construct a system that not only leverages the flexible conversational
abilities of LLMs but also their advanced planning capabilities to reduce the
speaking load on human interlocutors and efficiently plan trips. Furthermore,
we propose a method that divides the complex task of a travel agency into
multiple subtasks, managing each as a separate phase to effectively accomplish
the task. Our proposed system confirmed a certain level of success by achieving
fourth place in the Dialogue Robot Competition 2023 preliminaries rounds. We
report on the challenges identified through the competition.
</p></li>
</ul>

<h3>Title: How to Prune Your Language Model: Recovering Accuracy on the "Sparsity May Cry'' Benchmark. (arXiv:2312.13547v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13547">http://arxiv.org/abs/2312.13547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13547]] How to Prune Your Language Model: Recovering Accuracy on the "Sparsity May Cry'' Benchmark(http://arxiv.org/abs/2312.13547)</code></li>
<li>Summary: <p>Pruning large language models (LLMs) from the BERT family has emerged as a
standard compression benchmark, and several pruning methods have been proposed
for this task. The recent ``Sparsity May Cry'' (SMC) benchmark put into
question the validity of all existing methods, exhibiting a more complex setup
where many known pruning methods appear to fail. We revisit the question of
accurate BERT-pruning during fine-tuning on downstream datasets, and propose a
set of general guidelines for successful pruning, even on the challenging SMC
benchmark. First, we perform a cost-vs-benefits analysis of pruning model
components, such as the embeddings and the classification head; second, we
provide a simple-yet-general way of scaling training, sparsification and
learning rate schedules relative to the desired target sparsity; finally, we
investigate the importance of proper parametrization for Knowledge Distillation
in the context of LLMs. Our simple insights lead to state-of-the-art results,
both on classic BERT-pruning benchmarks, as well as on the SMC benchmark,
showing that even classic gradual magnitude pruning (GMP) can yield competitive
results, with the right approach.
</p></li>
</ul>

<h3>Title: Speech Translation with Large Language Models: An Industrial Practice. (arXiv:2312.13585v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13585">http://arxiv.org/abs/2312.13585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13585]] Speech Translation with Large Language Models: An Industrial Practice(http://arxiv.org/abs/2312.13585)</code></li>
<li>Summary: <p>Given the great success of large language models (LLMs) across various tasks,
in this paper, we introduce LLM-ST, a novel and effective speech translation
model constructed upon a pre-trained LLM. By integrating the large language
model (LLM) with a speech encoder and employing multi-task instruction tuning,
LLM-ST can produce accurate timestamped transcriptions and translations, even
from long audio inputs. Furthermore, our findings indicate that the
implementation of Chain-of-Thought (CoT) prompting can yield advantages in the
context of LLM-ST. Through rigorous experimentation on English and Chinese
datasets, we showcase the exceptional performance of LLM-ST, establishing a new
benchmark in the field of speech translation. Demo:
https://speechtranslation.github.io/llm-st/.
</p></li>
</ul>

<h3>Title: Text2Analysis: A Benchmark of Table Question Answering with Advanced Data Analysis and Unclear Queries. (arXiv:2312.13671v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13671">http://arxiv.org/abs/2312.13671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13671]] Text2Analysis: A Benchmark of Table Question Answering with Advanced Data Analysis and Unclear Queries(http://arxiv.org/abs/2312.13671)</code></li>
<li>Summary: <p>Tabular data analysis is crucial in various fields, and large language models
show promise in this area. However, current research mostly focuses on
rudimentary tasks like Text2SQL and TableQA, neglecting advanced analysis like
forecasting and chart generation. To address this gap, we developed the
Text2Analysis benchmark, incorporating advanced analysis tasks that go beyond
the SQL-compatible operations and require more in-depth analysis. We also
develop five innovative and effective annotation methods, harnessing the
capabilities of large language models to enhance data quality and quantity.
Additionally, we include unclear queries that resemble real-world user
questions to test how well models can understand and tackle such challenges.
Finally, we collect 2249 query-result pairs with 347 tables. We evaluate five
state-of-the-art models using three different metrics and the results show that
our benchmark presents introduces considerable challenge in the field of
tabular data analysis, paving the way for more advanced research opportunities.
</p></li>
</ul>

<h3>Title: On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning. (arXiv:2312.13772v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13772">http://arxiv.org/abs/2312.13772</a></li>
<li>Code URL: <a href="https://github.com/cambridgeltl/ensembled-sicl">https://github.com/cambridgeltl/ensembled-sicl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13772]] On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning(http://arxiv.org/abs/2312.13772)</code></li>
<li>Summary: <p>Following the standard supervised fine-tuning (SFT) paradigm, in-context
learning (ICL) has become an efficient approach propelled by the recent
advancements in large language models (LLMs), yielding promising performance
across various tasks in few-shot data setups. However, both paradigms are prone
to suffer from the critical problem of overconfidence (i.e., miscalibration),
especially in such limited data setups. In this work, we deliver an in-depth
analysis of the behavior across different choices of learning methods from the
perspective of both performance and calibration, as well as their interplay.
Through extensive controlled experiments, we find that simultaneous gains for
both task performance and calibration are difficult to achieve, and the problem
of miscalibration exists across all learning methods in low-resource
scenarios.To address this challenging trade-off between performance and
calibration, we then investigate the potential of self-ensembling techniques
applied at different modeling stages (e.g., variations of in-context examples
or variations in prompts or different ensembling strategies). We justify the
feasibility of self-ensembling on SFT in addition to ICL, to make the
predictions more calibrated and have comparable or even better performance. Our
work sheds light on which learning paradigm to choose and how to enhance both
task performance and calibration of LLMs.
</p></li>
</ul>

<h3>Title: Capture the Flag: Uncovering Data Insights with Large Language Models. (arXiv:2312.13876v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13876">http://arxiv.org/abs/2312.13876</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13876]] Capture the Flag: Uncovering Data Insights with Large Language Models(http://arxiv.org/abs/2312.13876)</code></li>
<li>Summary: <p>The extraction of a small number of relevant insights from vast amounts of
data is a crucial component of data-driven decision-making. However,
accomplishing this task requires considerable technical skills, domain
expertise, and human labor. This study explores the potential of using Large
Language Models (LLMs) to automate the discovery of insights in data,
leveraging recent advances in reasoning and code generation techniques. We
propose a new evaluation methodology based on a "capture the flag" principle,
measuring the ability of such models to recognize meaningful and pertinent
information (flags) in a dataset. We further propose two proof-of-concept
agents, with different inner workings, and compare their ability to capture
such flags in a real-world sales dataset. While the work reported here is
preliminary, our results are sufficiently interesting to mandate future
exploration by the community.
</p></li>
</ul>

<h3>Title: Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs. (arXiv:2312.13881v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13881">http://arxiv.org/abs/2312.13881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13881]] Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs(http://arxiv.org/abs/2312.13881)</code></li>
<li>Summary: <p>Recent advances in natural language processing (NLP) owe their success to
pre-training language models on large amounts of unstructured data. Still,
there is an increasing effort to combine the unstructured nature of LMs with
structured knowledge and reasoning. Particularly in the rapidly evolving field
of biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as
promising tools to bridge the gap between large language models and
domain-specific knowledge, considering the available biomedical knowledge
graphs (KGs) curated by experts over the decades. In this paper, we develop an
approach that uses lightweight adapter modules to inject structured biomedical
knowledge into pre-trained language models (PLMs). We use two large KGs, the
biomedical knowledge system UMLS and the novel biochemical ontology OntoChem,
with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT. The approach
includes partitioning knowledge graphs into smaller subgraphs, fine-tuning
adapter modules for each subgraph, and combining the knowledge in a fusion
layer. We test the performance on three downstream tasks: document
classification,question answering, and natural language inference. We show that
our methodology leads to performance improvements in several instances while
keeping requirements in computing power low. Finally, we provide a detailed
interpretation of the results and report valuable insights for future work.
</p></li>
</ul>

<h3>Title: Typhoon: Thai Large Language Models. (arXiv:2312.13951v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13951">http://arxiv.org/abs/2312.13951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13951]] Typhoon: Thai Large Language Models(http://arxiv.org/abs/2312.13951)</code></li>
<li>Summary: <p>Typhoon is a series of Thai large language models (LLMs) developed
specifically for the Thai language. This technical report presents challenges
and insights in developing Thai LLMs, including data preparation, pretraining,
instruction-tuning, and evaluation. As one of the challenges of low-resource
languages is the amount of pretraining data, we apply continual training to
transfer existing world knowledge from a strong LLM. To evaluate the Thai
knowledge encapsulated in each model from the pretraining stage, we develop
ThaiExam, a benchmark based on examinations for high-school students and
investment professionals in Thailand. In addition, we fine-tune Typhoon to
follow Thai instructions, and we evaluate instruction-tuned models on Thai
instruction datasets as well as translation, summarization, and
question-answering tasks. Experimental results on a suite of Thai benchmarks
show that Typhoon outperforms all open-source Thai language models, and its
performance is on par with GPT-3.5 in Thai while having only 7 billion
parameters and being 2.62 times more efficient in tokenizing Thai text.
</p></li>
</ul>

<h3>Title: T-Eval: Evaluating the Tool Utilization Capability Step by Step. (arXiv:2312.14033v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14033">http://arxiv.org/abs/2312.14033</a></li>
<li>Code URL: <a href="https://github.com/open-compass/t-eval">https://github.com/open-compass/t-eval</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14033]] T-Eval: Evaluating the Tool Utilization Capability Step by Step(http://arxiv.org/abs/2312.14033)</code></li>
<li>Summary: <p>Large language models (LLM) have achieved remarkable performance on various
NLP tasks and are augmented by tools for broader applications. Yet, how to
evaluate and analyze the tool-utilization capability of LLMs is still
under-explored. In contrast to previous works that evaluate models
holistically, we comprehensively decompose the tool utilization into multiple
sub-processes, including instruction following, planning, reasoning, retrieval,
understanding, and review. Based on that, we further introduce \shortname~to
evaluate the tool utilization capability step by step. \shortname~disentangles
the tool utilization evaluation into several sub-domains along model
capabilities, facilitating the inner understanding of both holistic and
isolated competency of LLMs. We conduct extensive experiments on \shortname~and
in-depth analysis of various LLMs. \shortname~ not only exhibits consistency
with the outcome-oriented evaluation but also provides a more fine-grained
analysis of the capabilities of LLMs, providing a new perspective in LLM
evaluation on tool-utilization ability. The benchmark will be available at
\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: DVIS++: Improved Decoupled Framework for Universal Video Segmentation. (arXiv:2312.13305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13305">http://arxiv.org/abs/2312.13305</a></li>
<li>Code URL: <a href="https://github.com/zhang-tao-whu/DVIS_Plus">https://github.com/zhang-tao-whu/DVIS_Plus</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13305]] DVIS++: Improved Decoupled Framework for Universal Video Segmentation(http://arxiv.org/abs/2312.13305)</code></li>
<li>Summary: <p>We present the \textbf{D}ecoupled \textbf{VI}deo \textbf{S}egmentation (DVIS)
framework, a novel approach for the challenging task of universal video
segmentation, including video instance segmentation (VIS), video semantic
segmentation (VSS), and video panoptic segmentation (VPS). Unlike previous
methods that model video segmentation in an end-to-end manner, our approach
decouples video segmentation into three cascaded sub-tasks: segmentation,
tracking, and refinement. This decoupling design allows for simpler and more
effective modeling of the spatio-temporal representations of objects,
especially in complex scenes and long videos. Accordingly, we introduce two
novel components: the referring tracker and the temporal refiner. These
components track objects frame by frame and model spatio-temporal
representations based on pre-aligned features. To improve the tracking
capability of DVIS, we propose a denoising training strategy and introduce
contrastive learning, resulting in a more robust framework named DVIS++.
Furthermore, we evaluate DVIS++ in various settings, including open vocabulary
and using a frozen pre-trained backbone. By integrating CLIP with DVIS++, we
present OV-DVIS++, the first open-vocabulary universal video segmentation
framework. We conduct extensive experiments on six mainstream benchmarks,
including the VIS, VSS, and VPS datasets. Using a unified architecture, DVIS++
significantly outperforms state-of-the-art specialized methods on these
benchmarks in both close- and open-vocabulary settings.
Code:~\url{https://github.com/zhang-tao-whu/DVIS_Plus}.
</p></li>
</ul>

<h3>Title: MGAug: Multimodal Geometric Augmentation in Latent Spaces of Image Deformations. (arXiv:2312.13440v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13440">http://arxiv.org/abs/2312.13440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13440]] MGAug: Multimodal Geometric Augmentation in Latent Spaces of Image Deformations(http://arxiv.org/abs/2312.13440)</code></li>
<li>Summary: <p>Geometric transformations have been widely used to augment the size of
training images. Existing methods often assume a unimodal distribution of the
underlying transformations between images, which limits their power when data
with multimodal distributions occur. In this paper, we propose a novel model,
Multimodal Geometric Augmentation (MGAug), that for the first time generates
augmenting transformations in a multimodal latent space of geometric
deformations. To achieve this, we first develop a deep network that embeds the
learning of latent geometric spaces of diffeomorphic transformations (a.k.a.
diffeomorphisms) in a variational autoencoder (VAE). A mixture of multivariate
Gaussians is formulated in the tangent space of diffeomorphisms and serves as a
prior to approximate the hidden distribution of image transformations. We then
augment the original training dataset by deforming images using randomly
sampled transformations from the learned multimodal latent space of VAE. To
validate the efficiency of our model, we jointly learn the augmentation
strategy with two distinct domain-specific tasks: multi-class classification on
2D synthetic datasets and segmentation on real 3D brain magnetic resonance
images (MRIs). We also compare MGAug with state-of-the-art transformation-based
image augmentation algorithms. Experimental results show that our proposed
approach outperforms all baselines by significantly improved prediction
accuracy. Our code is publicly available at
https://github.com/tonmoy-hossain/MGAug.
</p></li>
</ul>

<h3>Title: Weakly Supervised Semantic Segmentation for Driving Scenes. (arXiv:2312.13646v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13646">http://arxiv.org/abs/2312.13646</a></li>
<li>Code URL: <a href="https://github.com/k0u-id/carb">https://github.com/k0u-id/carb</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13646]] Weakly Supervised Semantic Segmentation for Driving Scenes(http://arxiv.org/abs/2312.13646)</code></li>
<li>Summary: <p>State-of-the-art techniques in weakly-supervised semantic segmentation (WSSS)
using image-level labels exhibit severe performance degradation on driving
scene datasets such as Cityscapes. To address this challenge, we develop a new
WSSS framework tailored to driving scene datasets. Based on extensive analysis
of dataset characteristics, we employ Contrastive Language-Image Pre-training
(CLIP) as our baseline to obtain pseudo-masks. However, CLIP introduces two key
challenges: (1) pseudo-masks from CLIP lack in representing small object
classes, and (2) these masks contain notable noise. We propose solutions for
each issue as follows. (1) We devise Global-Local View Training that seamlessly
incorporates small-scale patches during model training, thereby enhancing the
model's capability to handle small-sized yet critical objects in driving scenes
(e.g., traffic light). (2) We introduce Consistency-Aware Region Balancing
(CARB), a novel technique that discerns reliable and noisy regions through
evaluating the consistency between CLIP masks and segmentation predictions. It
prioritizes reliable pixels over noisy pixels via adaptive loss weighting.
Notably, the proposed method achieves 51.8\% mIoU on the Cityscapes test
dataset, showcasing its potential as a strong WSSS baseline on driving scene
datasets. Experimental results on CamVid and WildDash2 demonstrate the
effectiveness of our method across diverse datasets, even with small-scale
datasets or visually challenging conditions. The code is available at
https://github.com/k0u-id/CARB.
</p></li>
</ul>

<h3>Title: A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties. (arXiv:2312.13764v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13764">http://arxiv.org/abs/2312.13764</a></li>
<li>Code URL: <a href="https://github.com/lambert-x/prolab">https://github.com/lambert-x/prolab</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13764]] A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties(http://arxiv.org/abs/2312.13764)</code></li>
<li>Summary: <p>This paper introduces ProLab, a novel approach using property-level label
space for creating strong interpretable segmentation models. Instead of relying
solely on category-specific annotations, ProLab uses descriptive properties
grounded in common sense knowledge for supervising segmentation models. It is
based on two core designs. First, we employ Large Language Models (LLMs) and
carefully crafted prompts to generate descriptions of all involved categories
that carry meaningful common sense knowledge and follow a structured format.
Second, we introduce a description embedding model preserving semantic
correlation across descriptions and then cluster them into a set of descriptive
properties (e.g., 256) using K-Means. These properties are based on
interpretable common sense knowledge consistent with theories of human
recognition. We empirically show that our approach makes segmentation models
perform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal
Context, Cityscapes, and BDD). Our method also shows better scalability with
extended training steps than category-level supervision. Our interpretable
segmentation framework also emerges with the generalization ability to segment
out-of-domain or unknown categories using only in-domain descriptive
properties. Code is available at https://github.com/lambert-x/ProLab.
</p></li>
</ul>

<h3>Title: Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection. (arXiv:2312.13783v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13783">http://arxiv.org/abs/2312.13783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13783]] Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection(http://arxiv.org/abs/2312.13783)</code></li>
<li>Summary: <p>Logical anomalies (LA) refer to data violating underlying logical constraints
e.g., the quantity, arrangement, or composition of components within an image.
Detecting accurately such anomalies requires models to reason about various
component types through segmentation. However, curation of pixel-level
annotations for semantic segmentation is both time-consuming and expensive.
Although there are some prior few-shot or unsupervised co-part segmentation
algorithms, they often fail on images with industrial object. These images have
components with similar textures and shapes, and a precise differentiation
proves challenging. In this study, we introduce a novel component segmentation
model for LA detection that leverages a few labeled samples and unlabeled
images sharing logical constraints. To ensure consistent segmentation across
unlabeled images, we employ a histogram matching loss in conjunction with an
entropy loss. As segmentation predictions play a crucial role, we propose to
enhance both local and global sample validity detection by capturing key
aspects from visual semantics via three memory banks: class histograms,
component composition embeddings and patch-level representations. For effective
LA detection, we propose an adaptive scaling strategy to standardize anomaly
scores from different memory banks in inference. Extensive experiments on the
public benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA
detection vs. 89.6% from competing methods.
</p></li>
</ul>

<h3>Title: TinySAM: Pushing the Envelope for Efficient Segment Anything Model. (arXiv:2312.13789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.13789">http://arxiv.org/abs/2312.13789</a></li>
<li>Code URL: <a href="https://github.com/xinghaochen/tinysam">https://github.com/xinghaochen/tinysam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.13789]] TinySAM: Pushing the Envelope for Efficient Segment Anything Model(http://arxiv.org/abs/2312.13789)</code></li>
<li>Summary: <p>Recently segment anything model (SAM) has shown powerful segmentation
capability and has drawn great attention in computer vision fields. Massive
following works have developed various applications based on the pretrained SAM
and achieved impressive performance on downstream vision tasks. However, SAM
consists of heavy architectures and requires massive computational capacity,
which hinders the further application of SAM on computation constrained edge
devices. To this end, in this paper we propose a framework to obtain a tiny
segment anything model (TinySAM) while maintaining the strong zero-shot
performance. We first propose a full-stage knowledge distillation method with
online hard prompt sampling strategy to distill a lightweight student model. We
also adapt the post-training quantization to the promptable segmentation task
and further reduce the computational cost. Moreover, a hierarchical segmenting
everything strategy is proposed to accelerate the everything inference by
$2\times$ with almost no performance degradation. With all these proposed
methods, our TinySAM leads to orders of magnitude computational reduction and
pushes the envelope for efficient segment anything task. Extensive experiments
on various zero-shot transfer tasks demonstrate the significantly advantageous
performance of our TinySAM against counterpart methods. Pre-trained models and
codes will be available at https://github.com/xinghaochen/TinySAM and
https://gitee.com/mindspore/models/tree/master/research/cv/TinySAM.
</p></li>
</ul>

<h3>Title: Dual Attention U-Net with Feature Infusion: Pushing the Boundaries of Multiclass Defect Segmentation. (arXiv:2312.14053v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14053">http://arxiv.org/abs/2312.14053</a></li>
<li>Code URL: <a href="https://github.com/rashaalshawi/dual-attention-u-net-with-feature-infusion-pushing-the-boundaries-of-multiclass-defect-segmentation">https://github.com/rashaalshawi/dual-attention-u-net-with-feature-infusion-pushing-the-boundaries-of-multiclass-defect-segmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14053]] Dual Attention U-Net with Feature Infusion: Pushing the Boundaries of Multiclass Defect Segmentation(http://arxiv.org/abs/2312.14053)</code></li>
<li>Summary: <p>The proposed architecture, Dual Attentive U-Net with Feature Infusion (DAU-FI
Net), addresses challenges in semantic segmentation, particularly on multiclass
imbalanced datasets with limited samples. DAU-FI Net integrates multiscale
spatial-channel attention mechanisms and feature injection to enhance precision
in object localization. The core employs a multiscale depth-separable
convolution block, capturing localized patterns across scales. This block is
complemented by a spatial-channel squeeze and excitation (scSE) attention unit,
modeling inter-dependencies between channels and spatial regions in feature
maps. Additionally, additive attention gates refine segmentation by connecting
encoder-decoder pathways.
</p>
<p>To augment the model, engineered features using Gabor filters for textural
analysis, Sobel and Canny filters for edge detection are injected guided by
semantic masks to expand the feature space strategically. Comprehensive
experiments on a challenging sewer pipe and culvert defect dataset and a
benchmark dataset validate DAU-FI Net's capabilities. Ablation studies
highlight incremental benefits from attention blocks and feature injection.
DAU-FI Net achieves state-of-the-art mean Intersection over Union (IoU) of
95.6% and 98.8% on the defect test set and benchmark respectively, surpassing
prior methods by 8.9% and 12.6%, respectively. Ablation studies highlight
incremental benefits from attention blocks and feature injection. The proposed
architecture provides a robust solution, advancing semantic segmentation for
multiclass problems with limited training data. Our sewer-culvert defects
dataset, featuring pixel-level annotations, opens avenues for further research
in this crucial domain. Overall, this work delivers key innovations in
architecture, attention, and feature engineering to elevate semantic
segmentation efficacy.
</p></li>
</ul>

<h3>Title: TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification. (arXiv:2312.14149v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14149">http://arxiv.org/abs/2312.14149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14149]] TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification(http://arxiv.org/abs/2312.14149)</code></li>
<li>Summary: <p>The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, \textit{e.g.}, the vision encoder
struggles in localizing an attribute-specified object. In this work, we propose
an embarrassingly simple approach to better align image and text features with
no need of additional data formats other than image-text pairs. Concretely,
given an image and its paired text, we manage to parse objects (\textit{e.g.},
cat) and attributes (\textit{e.g.}, black) from the description, which are
highly likely to exist in the image. It is noteworthy that the parsing pipeline
is fully automatic and thus enjoys good scalability. With these parsed
semantics as supervision signals, we can complement the commonly used
image-text contrastive loss with the multi-tag classification loss. Extensive
experimental results on a broad suite of semantic segmentation datasets
substantiate the average 3.65\% improvement of our framework over existing
alternatives. Furthermore, the visualization results indicate that attribute
supervision makes vision-language models accurately localize
attribute-specified objects. Project page can be found at
https://qinying-liu.github.io/Tag-Align/
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
