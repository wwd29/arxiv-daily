<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes. (arXiv:2307.14713v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14713">http://arxiv.org/abs/2307.14713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14713] GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes](http://arxiv.org/abs/2307.14713) #security</code></li>
<li>Summary: <p>Gait, the manner of walking, has been proven to be a reliable biometric with
uses in surveillance, marketing and security. A promising new direction for the
field is training gait recognition systems without explicit human annotations,
through self-supervised learning approaches. Such methods are heavily reliant
on strong augmentations for the same walking sequence to induce more data
variability and to simulate additional walking variations. Current data
augmentation schemes are heuristic and cannot provide the necessary data
variation as they are only able to provide simple temporal and spatial
distortions. In this work, we propose GaitMorph, a novel method to modify the
walking variation for an input gait sequence. Our method entails the training
of a high-compression model for gait skeleton sequences that leverages
unlabelled data to construct a discrete and interpretable latent space, which
preserves identity-related features. Furthermore, we propose a method based on
optimal transport theory to learn latent transport maps on the discrete
codebook that morph gait sequences between variations. We perform extensive
experiments and show that our method is suitable to synthesize additional views
for an input sequence.
</p></li>
</ul>

<h3>Title: Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models. (arXiv:2307.14539v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14539">http://arxiv.org/abs/2307.14539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14539] Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models](http://arxiv.org/abs/2307.14539) #security</code></li>
<li>Summary: <p>The rapid growth and increasing popularity of incorporating additional
modalities (e.g., vision) into large language models (LLMs) has raised
significant security concerns. This expansion of modality, akin to adding more
doors to a house, unintentionally creates multiple access points for
adversarial attacks. In this paper, by introducing adversarial embedding space
attacks, we emphasize the vulnerabilities present in multi-modal systems that
originate from incorporating off-the-shelf components like public pre-trained
encoders in a plug-and-play manner into these systems. In contrast to existing
work, our approach does not require access to the multi-modal system's weights
or parameters but instead relies on the huge under-explored embedding space of
such pre-trained encoders. Our proposed embedding space attacks involve seeking
input images that reside within the dangerous or targeted regions of the
extensive embedding space of these pre-trained components. These crafted
adversarial images pose two major threats: 'Context Contamination' and 'Hidden
Prompt Injection'-both of which can compromise multi-modal models like LLaVA
and fully change the behavior of the associated language model. Our findings
emphasize the need for a comprehensive examination of the underlying
components, particularly pre-trained encoders, before incorporating them into
systems in a plug-and-play manner to ensure robust security.
</p></li>
</ul>

<h3>Title: An Asynchronous and Low-Power True Random Number Generator using STT-MTJ. (arXiv:2307.14476v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14476">http://arxiv.org/abs/2307.14476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14476] An Asynchronous and Low-Power True Random Number Generator using STT-MTJ](http://arxiv.org/abs/2307.14476) #security</code></li>
<li>Summary: <p>The emerging Spin Transfer Torque Magnetic Tunnel Junction (STT-MTJ)
technology exhibits interesting stochastic behavior combined with small area
and low operation energy. It is, therefore, a promising technology for security
applications, specifically the generation of random numbers. In this paper,
STT-MTJ is used to construct an asynchronous true random number generator
(TRNG) with low power and a high entropy rate. The asynchronous design enables
decoupling of the random number generation from the system clock, allowing it
to be embedded in low-power devices. The proposed TRNG is evaluated by a
numerical simulation, using the Landau-Lifshitz-Gilbert (LLG) equation as the
model of the STT-MTJ devices. Design considerations, attack analysis, and
process variation are discussed and evaluated. We show that our design is
robust to process variation, achieving an entropy generating rate between
99.7Mbps and 127.8Mbps with 6-7.7 pJ per bit for 90% of the instances.
</p></li>
</ul>

<h3>Title: PSOFuzz: Fuzzing Processors with Particle Swarm Optimization. (arXiv:2307.14480v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14480">http://arxiv.org/abs/2307.14480</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14480] PSOFuzz: Fuzzing Processors with Particle Swarm Optimization](http://arxiv.org/abs/2307.14480) #security</code></li>
<li>Summary: <p>Hardware security vulnerabilities in computing systems compromise the
security defenses of not only the hardware but also the software running on it.
Recent research has shown that hardware fuzzing is a promising technique to
efficiently detect such vulnerabilities in large-scale designs such as modern
processors. However, the current fuzzing techniques do not adjust their
strategies dynamically toward faster and higher design space exploration,
resulting in slow vulnerability detection, evident through their low design
coverage. To address this problem, we propose PSOFuzz, which uses particle
swarm optimization (PSO) to schedule the mutation operators and to generate
initial input programs dynamically with the objective of detecting
vulnerabilities quickly. Unlike traditional PSO, which finds a single optimal
solution, we use a modified PSO that dynamically computes the optimal solution
for selecting mutation operators required to explore new design regions in
hardware. We also address the challenge of inefficient initial input generation
by employing PSO-based input generation. Including these optimizations, our
final formulation outperforms fuzzers without PSO. Experiments show that
PSOFuzz achieves up to 15.25$\times$ speedup for vulnerability detection and up
to 2.22$\times$ speedup for coverage compared to the state-of-the-art
simulation-based hardware fuzzer.
</p></li>
</ul>

<h3>Title: Accelerating Polynomial Modular Multiplication with Crossbar-Based Compute-in-Memory. (arXiv:2307.14557v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14557">http://arxiv.org/abs/2307.14557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14557] Accelerating Polynomial Modular Multiplication with Crossbar-Based Compute-in-Memory](http://arxiv.org/abs/2307.14557) #security</code></li>
<li>Summary: <p>Lattice-based cryptographic algorithms built on ring learning with error
theory are gaining importance due to their potential for providing post-quantum
security. However, these algorithms involve complex polynomial operations, such
as polynomial modular multiplication (PMM), which is the most time-consuming
part of these algorithms. Accelerating PMM is crucial to make lattice-based
cryptographic algorithms widely adopted by more applications. This work
introduces a novel high-throughput and compact PMM accelerator, X-Poly, based
on the crossbar (XB)-type compute-in-memory (CIM). We identify the most
appropriate PMM algorithm for XB-CIM. We then propose a novel bit-mapping
technique to reduce the area and energy of the XB-CIM fabric, and conduct
processing engine (PE)-level optimization to increase memory utilization and
support different problem sizes with a fixed number of XB arrays. X-Poly design
achieves 3.1X10^6 PMM operations/s throughput and offers 200X latency
improvement compared to the CPU-based implementation. It also achieves 3.9X
throughput per area improvement compared with the state-of-the-art CIM
accelerators.
</p></li>
</ul>

<h3>Title: Smart Contract Migration: Security Analysis and Recommendations from Ethereum to Arbitrum. (arXiv:2307.14773v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14773">http://arxiv.org/abs/2307.14773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14773] Smart Contract Migration: Security Analysis and Recommendations from Ethereum to Arbitrum](http://arxiv.org/abs/2307.14773) #security</code></li>
<li>Summary: <p>This research aims to explore the security risks posed by compatibility and
protocol differences in smart contract migration, using the migration of smart
contracts from Ethereum to Arbitrum as a case study. Through literature review,
online data collection, expert participation, and analysis of smart contract
vulnerability cases, this paper conducts an in-depth research of the
differences between Ethereum and Arbitrum in areas such as Messaging, Block
Properties, Contract Address Alias, and Gas Fees. The research findings
indicate the presence of certain security issues during the migration process
from Ethereum to Arbitrum, such as abnormal operation of the sequencer
resulting in outdated off-chain data retrieval, time-based logical errors,
failed permission checks, DOS attacks, and gas loss due to L1-to-L2 transaction
failures. To address these security issues, this paper proposes corresponding
solutions and recommendations to ensure the security and meet the requirements
of the migration process. Additionally, this research emphasizes the continued
attention and support for the security issues of smart contract migration
through the case of smart contract migration from Ethereum to Arbitrum. It is
worth noting that this research is the first in-depth research of smart
contract security migration from Ethereum to Arbitrum.
</p></li>
</ul>

<h3>Title: A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15008">http://arxiv.org/abs/2307.15008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15008] A LLM Assisted Exploitation of AI-Guardian](http://arxiv.org/abs/2307.15008) #security</code></li>
<li>Summary: <p>Large language models (LLMs) are now highly capable at a diverse range of
tasks. This paper studies whether or not GPT-4, one such LLM, is capable of
assisting researchers in the field of adversarial machine learning. As a case
study, we evaluate the robustness of AI-Guardian, a recent defense to
adversarial examples published at IEEE S&amp;P 2023, a top computer security
conference. We completely break this defense: the proposed scheme does not
increase robustness compared to an undefended baseline.
</p></li>
</ul>

<p>We write none of the code to attack this model, and instead prompt GPT-4 to
implement all attack algorithms following our instructions and guidance. This
process was surprisingly effective and efficient, with the language model at
times producing code from ambiguous instructions faster than the author of this
paper could have done. We conclude by discussing (1) the warning signs present
in the evaluation that suggested to us AI-Guardian would be broken, and (2) our
experience with designing attacks and performing novel research using the most
recent advances in language modeling.
</p>

<h2>privacy</h2>
<h3>Title: FakeTracer: Proactively Defending Against Face-swap DeepFakes via Implanting Traces in Training. (arXiv:2307.14593v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14593">http://arxiv.org/abs/2307.14593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14593] FakeTracer: Proactively Defending Against Face-swap DeepFakes via Implanting Traces in Training](http://arxiv.org/abs/2307.14593) #privacy</code></li>
<li>Summary: <p>Face-swap DeepFake is an emerging AI-based face forgery technique that can
replace the original face in a video with a generated face of the target
identity while retaining consistent facial attributes such as expression and
orientation. Due to the high privacy of faces, the misuse of this technique can
raise severe social concerns, drawing tremendous attention to defend against
DeepFakes recently. In this paper, we describe a new proactive defense method
called FakeTracer to expose face-swap DeepFakes via implanting traces in
training. Compared to general face-synthesis DeepFake, the face-swap DeepFake
is more complex as it involves identity change, is subjected to the
encoding-decoding process, and is trained unsupervised, increasing the
difficulty of implanting traces into the training phase. To effectively defend
against face-swap DeepFake, we design two types of traces, sustainable trace
(STrace) and erasable trace (ETrace), to be added to training faces. During the
training, these manipulated faces affect the learning of the face-swap DeepFake
model, enabling it to generate faces that only contain sustainable traces. In
light of these two traces, our method can effectively expose DeepFakes by
identifying them. Extensive experiments are conducted on the Celeb-DF dataset,
compared with recent passive and proactive defense methods, and are studied
thoroughly regarding various factors, corroborating the efficacy of our method
on defending against face-swap DeepFake.
</p></li>
</ul>

<h3>Title: Online Context-aware Data Release with Sequence Information Privacy. (arXiv:2307.14388v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14388">http://arxiv.org/abs/2307.14388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14388] Online Context-aware Data Release with Sequence Information Privacy](http://arxiv.org/abs/2307.14388) #privacy</code></li>
<li>Summary: <p>Publishing streaming data in a privacy-preserving manner has been a key
research focus for many years. This issue presents considerable challenges,
particularly due to the correlations prevalent within the data stream. Existing
approaches either fall short in effectively leveraging these correlations,
leading to a suboptimal utility-privacy tradeoff, or they involve complex
mechanism designs that increase the computation complexity with respect to the
sequence length. In this paper, we introduce Sequence Information Privacy
(SIP), a new privacy notion designed to guarantee privacy for an entire data
stream, taking into account the intrinsic data correlations. We show that SIP
provides a similar level of privacy guarantee compared to local differential
privacy (LDP), and it also enjoys a lightweight modular mechanism design. We
further study two online data release models (instantaneous or batched) and
propose corresponding privacy-preserving data perturbation mechanisms. We
provide a numerical evaluation of how correlations influence noise addition in
data streams. Lastly, we conduct experiments using real-world data to compare
the utility-privacy tradeoff offered by our approaches with those from existing
literature. The results reveal that our mechanisms offer utility improvements
more than twice those based on LDP-based mechanisms.
</p></li>
</ul>

<h3>Title: LinkDID: A Privacy-Preserving, Sybil-Resistant and Key-Recoverable Decentralized Identity Scheme. (arXiv:2307.14679v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14679">http://arxiv.org/abs/2307.14679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14679] LinkDID: A Privacy-Preserving, Sybil-Resistant and Key-Recoverable Decentralized Identity Scheme](http://arxiv.org/abs/2307.14679) #privacy</code></li>
<li>Summary: <p>Decentralized identity mechanisms endeavor to endow users with complete
sovereignty over their digital assets within the Web3 ecosystem. Unfortunately,
this benefit frequently comes at the expense of users' credential and identity
privacy. Additionally, existing schemes fail to resist Sybil attacks that have
long plagued Web3, and lack reasonable key recovery mechanisms to regain
control of digital assets after loss. In this work, we propose LinkDID, a
privacy-preserving, Sybil-resistant, and key-recoverable decentralized identity
scheme that supports selective disclosure of credentials for arbitrary
predicates while maintaining privacy for credentials and identities. Through an
identifier association mechanism, LinkDID can privately and forcibly aggregate
users' identifiers, providing Sybil resistance without relying on any external
data or collateral from benign users. To enable key recovery, LinkDID permits
users to establish proofs of ownership for identifiers with lost keys and
request an update of corresponding keys from the decentralized ledger. We
provide a detailed theoretical analysis and security proofs of LinkDID, along
with an exhaustive performance evaluation that shows its ability to complete
interactions in less than 10 seconds on consumer-grade devices.
</p></li>
</ul>

<h3>Title: EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14381">http://arxiv.org/abs/2307.14381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14381] EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence](http://arxiv.org/abs/2307.14381) #privacy</code></li>
<li>Summary: <p>Deep edge intelligence aims to deploy deep learning models that demand
computationally expensive training in the edge network with limited
computational power. Moreover, many deep edge intelligence applications require
handling distributed data that cannot be transferred to a central server due to
privacy concerns. Decentralized learning methods, such as federated learning,
offer solutions where models are learned collectively by exchanging learned
weights. However, they often require complex models that edge devices may not
handle and multiple rounds of network communication to achieve state-of-the-art
performances. This study proposes a convolutional ensemble learning approach,
coined EdgeConvEns, that facilitates training heterogeneous weak models on edge
and learning to ensemble them where data on edge are heterogeneously
distributed. Edge models are implemented and trained independently on
Field-Programmable Gate Array (FPGA) devices with various computational
capacities. Learned data representations are transferred to a central server
where the ensemble model is trained with the learned features received from the
edge devices to boost the overall prediction performance. Extensive experiments
demonstrate that the EdgeConvEns can outperform the state-of-the-art
performance with fewer communications and less data in various training
scenarios.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: SEV-Step: A Single-Stepping Framework for AMD-SEV. (arXiv:2307.14757v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14757">http://arxiv.org/abs/2307.14757</a></li>
<li>Code URL: <a href="https://github.com/sev-step/sev-step">https://github.com/sev-step/sev-step</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14757] SEV-Step: A Single-Stepping Framework for AMD-SEV](http://arxiv.org/abs/2307.14757) #protect</code></li>
<li>Summary: <p>The ever increasing popularity and availability of Trusted Execution
Environments (TEEs) had a stark influence on microarchitectural attack research
in academia, as their strong attacker model both boosts existing attack vectors
and introduces several new ones. While many works have focused on Intel SGX,
other TEEs like AMD SEV have recently also started to receive more attention. A
common technique when attacking SGX enclaves is single-stepping, where the
system's APIC timer is used to interrupt the enclave after every instruction.
Single-stepping increases the temporal resolution of subsequent
microarchitectural attacks to a maximum. A key driver in the proliferation of
this complex attack technique was the SGX-Step framework, which offered a
stable reference implementation for single-stepping and a relatively easy
setup. In this paper, we demonstrate that SEV VMs can also be reliably
single-stepped. To lay the foundation for further microarchitectural attack
research against SEV, we introduce the reusable SEV-Step framework. Besides
reliable single-stepping, SEV-Step provides easy access to common attack
primitives like page fault tracking and cache attacks against SEV. All features
can be used interactively from user space. We demonstrate SEV-Step's
capabilities by carrying out an end-to-end cache attack against SEV that leaks
the volume key of a LUKS2-encrypted disk. Finally, we show for the first time
that SEV is vulnerable to Nemesis-style attacks, which allow to extract
information about the type and operands of single-stepped instructions from
SEV-protected VMs.
</p></li>
</ul>

<h3>Title: Don't Shoot the Messenger: Localization Prevention of Satellite Internet Users. (arXiv:2307.14879v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14879">http://arxiv.org/abs/2307.14879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14879] Don't Shoot the Messenger: Localization Prevention of Satellite Internet Users](http://arxiv.org/abs/2307.14879) #protect</code></li>
<li>Summary: <p>Satellite Internet plays an increasingly important role in geopolitical
conflicts. This notion was affirmed in the Ukrainian conflict escalating at the
beginning of 2022, with the large-scale deployment of the Starlink satellite
Internet service which consequently demonstrated the strategic importance of a
free flow of information. Aside from military use, many citizens publish
sensitive information on social media platforms to influence the public
narrative. However, the use of satellite communication has proven to be
dangerous, as the signals can be monitored by other satellites and used to
triangulate the source on the ground. Unfortunately, the targeted killings of
journalists have shown this threat to be effective. While the increasing
deployment of satellite Internet systems gives citizens an unprecedented
mouthpiece in conflicts, protecting them against localization is an unaddressed
problem.
</p></li>
</ul>

<p>To address this threat, we present AnonSat, a novel scheme to protect
satellite Internet users from triangulation. AnonSat works with cheap
off-the-shelf devices, leveraging long-range wireless communication to span a
local network among satellite base stations. This allows rerouting users'
communication to other satellite base stations, some distance away from each
user, thus, preventing their localization. AnonSat is designed for easy
deployment and usability, which we demonstrate with a prototype implementation.
Our large-scale network simulations using real-world data sets show the
effectiveness of AnonSat in various practical settings.
</p>

<h2>defense</h2>
<h3>Title: Lateral-Direction Localization Attack in High-Level Autonomous Driving: Domain-Specific Defense Opportunity via Lane Detection. (arXiv:2307.14540v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14540">http://arxiv.org/abs/2307.14540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14540] Lateral-Direction Localization Attack in High-Level Autonomous Driving: Domain-Specific Defense Opportunity via Lane Detection](http://arxiv.org/abs/2307.14540) #defense</code></li>
<li>Summary: <p>Localization in high-level Autonomous Driving (AD) systems is highly security
critical. While the popular Multi-Sensor Fusion (MSF) based design can be more
robust against single-source sensor spoofing attacks, it is found recently that
state-of-the-art MSF algorithms is vulnerable to GPS spoofing alone due to
practical factors, which can cause various road hazards such as driving off
road or onto the wrong way. In this work, we perform the first systematic
exploration of the novel usage of lane detection (LD) to defend against such
attacks. We first systematically analyze the potentials of such a
domain-specific defense opportunity, and then design a novel LD-based defense
approach, $LD^3$, that aims at not only detecting such attacks effectively in
the real time, but also safely stopping the victim in the ego lane upon
detection considering the absence of onboard human drivers.
</p></li>
</ul>

<p>We evaluate $LD^3$ on real-world sensor traces and find that it can achieve
effective and timely detection against existing attack with 100% true positive
rates and 0% false positive rates. Results also show that $LD^3$ is robust to
diverse environmental conditions and is effective at steering the AD vehicle to
safely stop within the current traffic lane. We implement $LD^3$ on two
open-source high-level AD systems, Baidu Apollo and Autoware, and validate its
defense capability in both simulation and the physical world in end-to-end
driving. We further conduct adaptive attack evaluations and find that $LD^3$ is
effective at bounding the deviations from reaching the attack goals in stealthy
attacks and is robust to latest LD-side attack.
</p>

<h2>attack</h2>
<h3>Title: Unified Adversarial Patch for Visible-Infrared Cross-modal Attacks in the Physical World. (arXiv:2307.14682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14682">http://arxiv.org/abs/2307.14682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14682] Unified Adversarial Patch for Visible-Infrared Cross-modal Attacks in the Physical World](http://arxiv.org/abs/2307.14682) #attack</code></li>
<li>Summary: <p>Physical adversarial attacks have put a severe threat to DNN-based object
detectors. To enhance security, a combination of visible and infrared sensors
is deployed in various scenarios, which has proven effective in disabling
existing single-modal physical attacks. To further demonstrate the potential
risks in such cases, we design a unified adversarial patch that can perform
cross-modal physical attacks, achieving evasion in both modalities
simultaneously with a single patch. Given the different imaging mechanisms of
visible and infrared sensors, our work manipulates patches' shape features,
which can be captured in different modalities when they undergo changes. To
deal with challenges, we propose a novel boundary-limited shape optimization
approach that aims to achieve compact and smooth shapes for the adversarial
patch, making it easy to implement in the physical world. And a score-aware
iterative evaluation method is also introduced to balance the fooling degree
between visible and infrared detectors during optimization, which guides the
adversarial patch to iteratively reduce the predicted scores of the multi-modal
sensors. Furthermore, we propose an Affine-Transformation-based enhancement
strategy that makes the learnable shape robust to various angles, thus
mitigating the issue of shape deformation caused by different shooting angles
in the real world. Our method is evaluated against several state-of-the-art
object detectors, achieving an Attack Success Rate (ASR) of over 80%. We also
demonstrate the effectiveness of our approach in physical-world scenarios under
various settings, including different angles, distances, postures, and scenes
for both visible and infrared sensors.
</p></li>
</ul>

<h3>Title: NSA: Naturalistic Support Artifact to Boost Network Confidence. (arXiv:2307.14917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14917">http://arxiv.org/abs/2307.14917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14917] NSA: Naturalistic Support Artifact to Boost Network Confidence](http://arxiv.org/abs/2307.14917) #attack</code></li>
<li>Summary: <p>Visual AI systems are vulnerable to natural and synthetic physical corruption
in the real-world. Such corruption often arises unexpectedly and alters the
model's performance. In recent years, the primary focus has been on adversarial
attacks. However, natural corruptions (e.g., snow, fog, dust) are an
omnipresent threat to visual AI systems and should be considered equally
important. Many existing works propose interesting solutions to train robust
models against natural corruption. These works either leverage image
augmentations, which come with the additional cost of model training, or place
suspicious patches in the scene to design unadversarial examples. In this work,
we propose the idea of naturalistic support artifacts (NSA) for robust
prediction. The NSAs are shown to be beneficial in scenarios where model
parameters are inaccessible and adding artifacts in the scene is feasible. The
NSAs are natural looking objects generated through artifact training using
DC-GAN to have high visual fidelity in the scene. We test against natural
corruptions on the Imagenette dataset and observe the improvement in prediction
confidence score by four times. We also demonstrate NSA's capability to
increase adversarial accuracy by 8\% on average. Lastly, we qualitatively
analyze NSAs using saliency maps to understand how they help improve prediction
confidence.
</p></li>
</ul>

<h3>Title: Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15043">http://arxiv.org/abs/2307.15043</a></li>
<li>Code URL: <a href="https://github.com/llm-attacks/llm-attacks">https://github.com/llm-attacks/llm-attacks</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15043] Universal and Transferable Adversarial Attacks on Aligned Language Models](http://arxiv.org/abs/2307.15043) #attack</code></li>
<li>Summary: <p>Because "out-of-the-box" large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called "jailbreaks" against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.
</p></li>
</ul>

<p>Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.
</p>

<h3>Title: Dual-Space Attacks against Random-Walk-based Anomaly Detection. (arXiv:2307.14387v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14387">http://arxiv.org/abs/2307.14387</a></li>
<li>Code URL: <a href="https://github.com/yuni-lai/dualattackrw">https://github.com/yuni-lai/dualattackrw</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14387] Dual-Space Attacks against Random-Walk-based Anomaly Detection](http://arxiv.org/abs/2307.14387) #attack</code></li>
<li>Summary: <p>Random Walks-based Anomaly Detection (RWAD) is commonly used to identify
anomalous patterns in various applications. An intriguing characteristic of
RWAD is that the input graph can either be pre-existing or constructed from raw
features. Consequently, there are two potential attack surfaces against RWAD:
graph-space attacks and feature-space attacks. In this paper, we explore this
vulnerability by designing practical dual-space attacks, investigating the
interplay between graph-space and feature-space attacks. To this end, we
conduct a thorough complexity analysis, proving that attacking RWAD is NP-hard.
Then, we proceed to formulate the graph-space attack as a bi-level optimization
problem and propose two strategies to solve it: alternative iteration
(alterI-attack) or utilizing the closed-form solution of the random walk model
(cf-attack). Finally, we utilize the results from the graph-space attacks as
guidance to design more powerful feature-space attacks (i.e., graph-guided
attacks). Comprehensive experiments demonstrate that our proposed attacks are
effective in enabling the target nodes from RWAD with a limited attack budget.
In addition, we conduct transfer attack experiments in a black-box setting,
which show that our feature attack significantly decreases the anomaly scores
of target nodes. Our study opens the door to studying the dual-space attack
against graph anomaly detection in which the graph space relies on the feature
space.
</p></li>
</ul>

<h3>Title: Backdoor Attacks for In-Context Learning with Language Models. (arXiv:2307.14692v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14692">http://arxiv.org/abs/2307.14692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14692] Backdoor Attacks for In-Context Learning with Language Models](http://arxiv.org/abs/2307.14692) #attack</code></li>
<li>Summary: <p>Because state-of-the-art language models are expensive to train, most
practitioners must make use of one of the few publicly available language
models or language model APIs. This consolidation of trust increases the
potency of backdoor attacks, where an adversary tampers with a machine learning
model in order to make it perform some malicious behavior on inputs that
contain a predefined backdoor trigger. We show that the in-context learning
ability of large language models significantly complicates the question of
developing backdoor attacks, as a successful backdoor must work against various
prompting strategies and should not affect the model's general purpose
capabilities. We design a new attack for eliciting targeted misclassification
when language models are prompted to perform a particular target task and
demonstrate the feasibility of this attack by backdooring multiple large
language models ranging in size from 1.3 billion to 6 billion parameters.
Finally we study defenses to mitigate the potential harms of our attack: for
example, while in the white-box setting we show that fine-tuning models for as
few as 500 steps suffices to remove the backdoor behavior, in the black-box
setting we are unable to develop a successful defense that relies on prompt
engineering alone.
</p></li>
</ul>

<h3>Title: FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14751">http://arxiv.org/abs/2307.14751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14751] FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks](http://arxiv.org/abs/2307.14751) #attack</code></li>
<li>Summary: <p>We propose FLARE, the first fingerprinting mechanism to verify whether a
suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of
another (victim) policy. We first show that it is possible to find
non-transferable, universal adversarial masks, i.e., perturbations, to generate
adversarial examples that can successfully transfer from a victim policy to its
modified versions but not to independently trained policies. FLARE employs
these masks as fingerprints to verify the true ownership of stolen DRL policies
by measuring an action agreement value over states perturbed via such masks.
Our empirical evaluations show that FLARE is effective (100% action agreement
on stolen copies) and does not falsely accuse independent policies (no false
positives). FLARE is also robust to model modification attacks and cannot be
easily evaded by more informed adversaries without negatively impacting agent
performance. We also show that not all universal adversarial masks are suitable
candidates for fingerprints due to the inherent characteristics of DRL
policies. The spatio-temporal dynamics of DRL problems and sequential
decision-making process make characterizing the decision boundary of DRL
policies more difficult, as well as searching for universal masks that capture
the geometry of it.
</p></li>
</ul>

<h3>Title: A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory. (arXiv:2307.14732v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14732">http://arxiv.org/abs/2307.14732</a></li>
<li>Code URL: <a href="https://github.com/calvinyeungck/Football-1-vs-1-Shot-Taking-Situations-Analysis">https://github.com/calvinyeungck/Football-1-vs-1-Shot-Taking-Situations-Analysis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14732] A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory](http://arxiv.org/abs/2307.14732) #attack</code></li>
<li>Summary: <p>Complex interactions between two opposing agents frequently occur in domains
of machine learning, game theory, and other application domains. Quantitatively
analyzing the strategies involved can provide an objective basis for
decision-making. One such critical scenario is shot-taking in football, where
decisions, such as whether the attacker should shoot or pass the ball and
whether the defender should attempt to block the shot, play a crucial role in
the outcome of the game. However, there are currently no effective data-driven
and/or theory-based approaches to analyzing such situations. To address this
issue, we proposed a novel framework to analyze such scenarios based on game
theory, where we estimate the expected payoff with machine learning (ML)
models, and additional features for ML models were extracted with a
theory-based shot block model. Conventionally, successes or failures (1 or 0)
are used as payoffs, while a success shot (goal) is extremely rare in football.
Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metric
to evaluate players' actions even if the shot results in no goal; this allows
for effective differentiation and comparison between different shots and even
enables counterfactual shot situation analysis. In our experiments, we have
validated the framework by comparing it with baseline and ablated models.
Furthermore, we have observed a high correlation between the xSOT and existing
metrics. This alignment of information suggests that xSOT provides valuable
insights. Lastly, as an illustration, we studied optimal strategies in the
World Cup 2022 and analyzed a shot situation in EURO 2020.
</p></li>
</ul>

<h3>Title: Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning. (arXiv:2307.14952v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14952">http://arxiv.org/abs/2307.14952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14952] Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning](http://arxiv.org/abs/2307.14952) #attack</code></li>
<li>Summary: <p>As the network scale increases, existing fully distributed solutions start to
lag behind the real-world challenges such as (1) slow information propagation,
(2) network communication failures, and (3) external adversarial attacks. In
this paper, we focus on hierarchical system architecture and address the
problem of non-Bayesian learning over networks that are vulnerable to
communication failures and adversarial attacks. On network communication, we
consider packet-dropping link failures.
</p></li>
</ul>

<p>We first propose a hierarchical robust push-sum algorithm that can achieve
average consensus despite frequent packet-dropping link failures. We provide a
sparse information fusion rule between the parameter server and arbitrarily
selected network representatives. Then, interleaving the consensus update step
with a dual averaging update with Kullback-Leibler (KL) divergence as the
proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian
learning algorithm with provable convergence guarantees.
</p>
<p>On external adversarial attacks, we consider Byzantine attacks in which the
compromised agents can send maliciously calibrated messages to others
(including both the agents and the parameter server). To avoid the curse of
dimensionality of Byzantine consensus, we solve the non-Bayesian learning
problem via running multiple dynamics, each of which only involves Byzantine
consensus with scalar inputs. To facilitate resilient information propagation
across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at
the parameter server.
</p>

<h2>robust</h2>
<h3>Title: MiDaS v3.1 -- A Model Zoo for Robust Monocular Relative Depth Estimation. (arXiv:2307.14460v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14460">http://arxiv.org/abs/2307.14460</a></li>
<li>Code URL: <a href="https://github.com/isl-org/MiDaS">https://github.com/isl-org/MiDaS</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14460] MiDaS v3](http://arxiv.org/abs/2307.14460) #robust</code></li>
<li>Summary: <p>We release MiDaS v3.1 for monocular depth estimation, offering a variety of
new models based on different encoder backbones. This release is motivated by
the success of transformers in computer vision, with a large variety of
pretrained vision transformers now available. We explore how using the most
promising vision transformers as image encoders impacts depth estimation
quality and runtime of the MiDaS architecture. Our investigation also includes
recent convolutional approaches that achieve comparable quality to vision
transformers in image classification tasks. While the previous release MiDaS
v3.0 solely leverages the vanilla vision transformer ViT, MiDaS v3.1 offers
additional models based on BEiT, Swin, SwinV2, Next-ViT and LeViT. These models
offer different performance-runtime tradeoffs. The best model improves the
depth estimation quality by 28% while efficient models enable downstream tasks
requiring high frame rates. We also describe the general process for
integrating new backbones. A video summarizing the work can be found at
https://youtu.be/UjaeNNFf9sE and the code is available at
https://github.com/isl-org/MiDaS.
</p></li>
</ul>

<h3>Title: Robust Detection, Assocation, and Localization of Vehicle Lights: A Context-Based Cascaded CNN Approach and Evaluations. (arXiv:2307.14571v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14571">http://arxiv.org/abs/2307.14571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14571] Robust Detection, Assocation, and Localization of Vehicle Lights: A Context-Based Cascaded CNN Approach and Evaluations](http://arxiv.org/abs/2307.14571) #robust</code></li>
<li>Summary: <p>Vehicle light detection is required for important downstream safe autonomous
driving tasks, such as predicting a vehicle's light state to determine if the
vehicle is making a lane change or turning. Currently, many vehicle light
detectors use single-stage detectors which predict bounding boxes to identify a
vehicle light, in a manner decoupled from vehicle instances. In this paper, we
present a method for detecting a vehicle light given an upstream vehicle
detection and approximation of a visible light's center. Our method predicts
four approximate corners associated with each vehicle light. We experiment with
CNN architectures, data augmentation, and contextual preprocessing methods
designed to reduce surrounding-vehicle confusion. We achieve an average
distance error from the ground truth corner of 5.09 pixels, about 17.24% of the
size of the vehicle light on average. We train and evaluate our model on the
LISA Lights dataset, allowing us to thoroughly evaluate our vehicle light
corner detection model on a large variety of vehicle light shapes and lighting
conditions. We propose that this model can be integrated into a pipeline with
vehicle detection and vehicle light center detection to make a fully-formed
vehicle light detection network, valuable to identifying trajectory-informative
signals in driving scenes.
</p></li>
</ul>

<h3>Title: GADER: GAit DEtection and Recognition in the Wild. (arXiv:2307.14578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14578">http://arxiv.org/abs/2307.14578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14578] GADER: GAit DEtection and Recognition in the Wild](http://arxiv.org/abs/2307.14578) #robust</code></li>
<li>Summary: <p>Gait recognition holds the promise of robustly identifying subjects based on
their walking patterns instead of color information. While previous approaches
have performed well for curated indoor scenes, they have significantly impeded
applicability in unconstrained situations, e.g. outdoor, long distance scenes.
We propose an end-to-end GAit DEtection and Recognition (GADER) algorithm for
human authentication in challenging outdoor scenarios. Specifically, GADER
leverages a Double Helical Signature to detect the fragment of human movement
and incorporates a novel gait recognition method, which learns representations
by distilling from an auxiliary RGB recognition model. At inference time, GADER
only uses the silhouette modality but benefits from a more robust
representation. Extensive experiments on indoor and outdoor datasets
demonstrate that the proposed method outperforms the State-of-The-Arts for gait
recognition and verification, with a significant 20.6% improvement on
unconstrained, long distance scenes.
</p></li>
</ul>

<h3>Title: The detection and rectification for identity-switch based on unfalsified control. (arXiv:2307.14591v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14591">http://arxiv.org/abs/2307.14591</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14591] The detection and rectification for identity-switch based on unfalsified control](http://arxiv.org/abs/2307.14591) #robust</code></li>
<li>Summary: <p>The purpose of multi-object tracking (MOT) is to continuously track and
identify objects detected in videos. Currently, most methods for multi-object
tracking model the motion information and combine it with appearance
information to determine and track objects. In this paper, unfalsified control
is employed to address the ID-switch problem in multi-object tracking. We
establish sequences of appearance information variations for the trajectories
during the tracking process and design a detection and rectification module
specifically for ID-switch detection and recovery. We also propose a simple and
effective strategy to address the issue of ambiguous matching of appearance
information during the data association process. Experimental results on
publicly available MOT datasets demonstrate that the tracker exhibits excellent
effectiveness and robustness in handling tracking errors caused by occlusions
and rapid movements.
</p></li>
</ul>

<h3>Title: Clustering based Point Cloud Representation Learning for 3D Analysis. (arXiv:2307.14605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14605">http://arxiv.org/abs/2307.14605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14605] Clustering based Point Cloud Representation Learning for 3D Analysis](http://arxiv.org/abs/2307.14605) #robust</code></li>
<li>Summary: <p>Point cloud analysis (such as 3D segmentation and detection) is a challenging
task, because of not only the irregular geometries of many millions of
unordered points, but also the great variations caused by depth, viewpoint,
occlusion, etc. Current studies put much focus on the adaption of neural
networks to the complex geometries of point clouds, but are blind to a
fundamental question: how to learn an appropriate point embedding space that is
aware of both discriminative semantics and challenging variations? As a
response, we propose a clustering based supervised learning scheme for point
cloud analysis. Unlike current de-facto, scene-wise training paradigm, our
algorithm conducts within-class clustering on the point embedding space for
automatically discovering subclass patterns which are latent yet representative
across scenes. The mined patterns are, in turn, used to repaint the embedding
space, so as to respect the underlying distribution of the entire training
dataset and improve the robustness to the variations. Our algorithm is
principled and readily pluggable to modern point cloud segmentation networks
during training, without extra overhead during testing. With various 3D network
architectures (i.e., voxel-based, point-based, Transformer-based, automatically
searched), our algorithm shows notable improvements on famous point cloud
segmentation datasets (i.e.,2.0-2.6% on single-scan and 2.0-2.2% multi-scan of
SemanticKITTI, 1.8-1.9% on S3DIS, in terms of mIoU). Our algorithm also
demonstrates utility in 3D detection, showing 2.0-3.4% mAP gains on KITTI.
</p></li>
</ul>

<h3>Title: Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining. (arXiv:2307.14768v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14768">http://arxiv.org/abs/2307.14768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14768] Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining](http://arxiv.org/abs/2307.14768) #robust</code></li>
<li>Summary: <p>Sign Language Translation (SLT) is a challenging task due to its cross-domain
nature, involving the translation of visual-gestural language to text. Many
previous methods employ an intermediate representation, i.e., gloss sequences,
to facilitate SLT, thus transforming it into a two-stage task of sign language
recognition (SLR) followed by sign language translation (SLT). However, the
scarcity of gloss-annotated sign language data, combined with the information
bottleneck in the mid-level gloss representation, has hindered the further
development of the SLT task. To address this challenge, we propose a novel
Gloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improves
SLT by inheriting language-oriented prior knowledge from pre-trained models,
without any gloss annotation assistance. Our approach involves two stages: (i)
integrating Contrastive Language-Image Pre-training (CLIP) with masked
self-supervised learning to create pre-tasks that bridge the semantic gap
between visual and textual representations and restore masked sentences, and
(ii) constructing an end-to-end architecture with an encoder-decoder-like
structure that inherits the parameters of the pre-trained Visual Encoder and
Text Decoder from the first stage. The seamless combination of these novel
designs forms a robust sign language representation and significantly improves
gloss-free sign language translation. In particular, we have achieved
unprecedented improvements in terms of BLEU-4 score on the PHOENIX14T dataset
(>+5) and the CSL-Daily dataset (>+3) compared to state-of-the-art gloss-free
SLT methods. Furthermore, our approach also achieves competitive results on the
PHOENIX14T dataset when compared with most of the gloss-based methods. Our code
is available at https://github.com/zhoubenjia/GFSLT-VLP.
</p></li>
</ul>

<h3>Title: Comparative Evaluation of Digital and Analog Chest Radiographs to Identify Tuberculosis using Deep Learning Model. (arXiv:2307.14859v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14859">http://arxiv.org/abs/2307.14859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14859] Comparative Evaluation of Digital and Analog Chest Radiographs to Identify Tuberculosis using Deep Learning Model](http://arxiv.org/abs/2307.14859) #robust</code></li>
<li>Summary: <p>Purpose:Chest X-ray (CXR) is an essential tool and one of the most prescribed
imaging to detect pulmonary abnormalities, with a yearly estimate of over 2
billion imaging performed worldwide. However, the accurate and timely diagnosis
of TB remains an unmet goal. The prevalence of TB is highest in
low-middle-income countries, and the requirement of a portable, automated, and
reliable solution is required. In this study, we compared the performance of
DL-based devices on digital and analog CXR. The evaluated DL-based device can
be used in resource-constraint settings. Methods: A total of 10,000 CXR
DICOMs(.dcm) and printed photos of the films acquired with three different
cellular phones - Samsung S8, iPhone 8, and iPhone XS along with their
radiological report were retrospectively collected from various sites across
India from April 2020 to March 2021. Results: 10,000 chest X-rays were utilized
to evaluate the DL-based device in identifying radiological signs of TB. The
AUC of qXR for detecting signs of tuberculosis on the original DICOMs dataset
was 0.928 with a sensitivity of 0.841 at a specificity of 0.806. At an optimal
threshold, the difference in the AUC of three cellular smartphones with the
original DICOMs is 0.024 (2.55%), 0.048 (5.10%), and 0.038 (1.91%). The minimum
difference demonstrates the robustness of the DL-based device in identifying
radiological signs of TB in both digital and analog CXR.
</p></li>
</ul>

<h3>Title: The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation. (arXiv:2307.15061v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15061">http://arxiv.org/abs/2307.15061</a></li>
<li>Code URL: <a href="https://github.com/ldkong1205/robodepth">https://github.com/ldkong1205/robodepth</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15061] The RoboDepth Challenge: Methods and Advancements Towards Robust Depth Estimation](http://arxiv.org/abs/2307.15061) #robust</code></li>
<li>Summary: <p>Accurate depth estimation under out-of-distribution (OoD) scenarios, such as
adverse weather conditions, sensor failure, and noise contamination, is
desirable for safety-critical applications. Existing depth estimation systems,
however, suffer inevitably from real-world corruptions and perturbations and
are struggled to provide reliable depth predictions under such cases. In this
paper, we summarize the winning solutions from the RoboDepth Challenge -- an
academic competition designed to facilitate and advance robust OoD depth
estimation. This challenge was developed based on the newly established KITTI-C
and NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis
on robust self-supervised and robust fully-supervised depth estimation,
respectively. Out of more than two hundred participants, nine unique and
top-performing solutions have appeared, with novel designs ranging from the
following aspects: spatial- and frequency-domain augmentations, masked image
modeling, image restoration and super-resolution, adversarial training,
diffusion-based noise suppression, vision-language pre-training, learned model
ensembling, and hierarchical feature enhancement. Extensive experimental
analyses along with insightful observations are drawn to better understand the
rationale behind each design. We hope this challenge could lay a solid
foundation for future research on robust and reliable depth estimation and
beyond. The datasets, competition toolkit, workshop recordings, and source code
from the winning teams are publicly available on the challenge website.
</p></li>
</ul>

<h3>Title: Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14632">http://arxiv.org/abs/2307.14632</a></li>
<li>Code URL: <a href="https://github.com/nlp-ku/metric-based-in-context-learning">https://github.com/nlp-ku/metric-based-in-context-learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14632] Metric-Based In-context Learning: A Case Study in Text Simplification](http://arxiv.org/abs/2307.14632) #robust</code></li>
<li>Summary: <p>In-context learning (ICL) for large language models has proven to be a
powerful approach for many natural language processing tasks. However,
determining the best method to select examples for ICL is nontrivial as the
results can vary greatly depending on the quality, quantity, and order of
examples used. In this paper, we conduct a case study on text simplification
(TS) to investigate how to select the best and most robust examples for ICL. We
propose Metric-Based in-context Learning (MBL) method that utilizes commonly
used TS metrics such as SARI, compression ratio, and BERT-Precision for
selection. Through an extensive set of experiments with various-sized GPT
models on standard TS benchmarks such as TurkCorpus and ASSET, we show that
examples selected by the top SARI scores perform the best on larger models such
as GPT-175B, while the compression ratio generally performs better on smaller
models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is
generally robust to example orderings and out-of-domain test sets, and
outperforms strong baselines and state-of-the-art finetuned language models.
Finally, we show that the behaviour of large GPT models can be implicitly
controlled by the chosen metric. Our research provides a new framework for
selecting examples in ICL, and demonstrates its effectiveness in text
simplification tasks, breaking new ground for more accurate and efficient NLG
systems.
</p></li>
</ul>

<h3>Title: Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14743">http://arxiv.org/abs/2307.14743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14743] Turning Whisper into Real-Time Transcription System](http://arxiv.org/abs/2307.14743) #robust</code></li>
<li>Summary: <p>Whisper is one of the recent state-of-the-art multilingual speech recognition
and translation models, however, it is not designed for real time
transcription. In this paper, we build on top of Whisper and create
Whisper-Streaming, an implementation of real-time speech transcription and
translation of Whisper-like models. Whisper-Streaming uses local agreement
policy with self-adaptive latency to enable streaming transcription. We show
that Whisper-Streaming achieves high quality and 3.3 seconds latency on
unsegmented long-form speech transcription test set, and we demonstrate its
robustness and practical usability as a component in live transcription service
at a multilingual conference.
</p></li>
</ul>

<h3>Title: Models of reference production: How do they withstand the test of time?. (arXiv:2307.14817v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14817">http://arxiv.org/abs/2307.14817</a></li>
<li>Code URL: <a href="https://github.com/fsame/reg_grec-wsj">https://github.com/fsame/reg_grec-wsj</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14817] Models of reference production: How do they withstand the test of time?](http://arxiv.org/abs/2307.14817) #robust</code></li>
<li>Summary: <p>In recent years, many NLP studies have focused solely on performance
improvement. In this work, we focus on the linguistic and scientific aspects of
NLP. We use the task of generating referring expressions in context
(REG-in-context) as a case study and start our analysis from GREC, a
comprehensive set of shared tasks in English that addressed this topic over a
decade ago. We ask what the performance of models would be if we assessed them
(1) on more realistic datasets, and (2) using more advanced methods. We test
the models using different evaluation metrics and feature selection
experiments. We conclude that GREC can no longer be regarded as offering a
reliable assessment of models' ability to mimic human reference production,
because the results are highly impacted by the choice of corpus and evaluation
metrics. Our results also suggest that pre-trained language models are less
dependent on the choice of corpus than classic Machine Learning models, and
therefore make more robust class predictions.
</p></li>
</ul>

<h3>Title: Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14856">http://arxiv.org/abs/2307.14856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14856] Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners](http://arxiv.org/abs/2307.14856) #robust</code></li>
<li>Summary: <p>In-context learning, which offers substantial advantages over fine-tuning, is
predominantly observed in decoder-only models, while encoder-decoder (i.e.,
seq2seq) models excel in methods that rely on weight updates. Recently, a few
studies have demonstrated the feasibility of few-shot learning with seq2seq
models; however, this has been limited to tasks that align well with the
seq2seq architecture, such as summarization and translation. Inspired by these
initial studies, we provide a first-ever extensive experiment comparing the
in-context few-shot learning capabilities of decoder-only and encoder-decoder
models on a broad range of tasks. Furthermore, we propose two methods to more
effectively elicit in-context learning ability in seq2seq models:
objective-aligned prompting and a fusion-based approach. Remarkably, our
approach outperforms a decoder-only model that is six times larger and exhibits
significant performance improvements compared to conventional seq2seq models
across a variety of settings. We posit that, with the right configuration and
prompt design, seq2seq models can be highly effective few-shot learners for a
wide spectrum of applications.
</p></li>
</ul>

<h3>Title: MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities. (arXiv:2307.14878v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14878">http://arxiv.org/abs/2307.14878</a></li>
<li>Code URL: <a href="https://github.com/thukelab/mesed">https://github.com/thukelab/mesed</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14878] MESED: A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities](http://arxiv.org/abs/2307.14878) #robust</code></li>
<li>Summary: <p>The Entity Set Expansion (ESE) task aims to expand a handful of seed entities
with new entities belonging to the same semantic class. Conventional ESE
methods are based on mono-modality (i.e., literal modality), which struggle to
deal with complex entities in the real world such as: (1) Negative entities
with fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous
entities. (4) Long-tailed entities. These challenges prompt us to propose
Multi-modal Entity Set Expansion (MESE), where models integrate information
from multiple modalities to represent entities. Intuitively, the benefits of
multi-modal information for ESE are threefold: (1) Different modalities can
provide complementary information. (2) Multi-modal information provides a
unified signal via common visual properties for the same semantic class or
entity. (3) Multi-modal information offers robust alignment signal for
synonymous entities. To assess the performance of model in MESE and facilitate
further research, we constructed the MESED dataset which is the first
multi-modal dataset for ESE with large-scale and elaborate manual calibration.
A powerful multi-modal model MultiExpan is proposed which is pre-trained on
four multimodal pre-training tasks. The extensive experiments and analyses on
MESED demonstrate the high quality of the dataset and the effectiveness of our
MultiExpan, as well as pointing the direction for future research.
</p></li>
</ul>

<h3>Title: Scaling TransNormer to 175 Billion Parameters. (arXiv:2307.14995v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14995">http://arxiv.org/abs/2307.14995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14995] Scaling TransNormer to 175 Billion Parameters](http://arxiv.org/abs/2307.14995) #robust</code></li>
<li>Summary: <p>We present TransNormerLLM, the first linear attention-based Large Language
Model (LLM) that outperforms conventional softmax attention-based models in
terms of both accuracy and efficiency. TransNormerLLM evolves from the previous
linear attention architecture TransNormer by making advanced modifications that
include positional embedding, linear attention acceleration, gating mechanism,
tensor normalization, inference acceleration and stabilization. Specifically,
we use LRPE together with an exponential decay to avoid attention dilution
issues while allowing the model to retain global interactions between tokens.
Additionally, we propose Lightning Attention, a cutting-edge technique that
accelerates linear attention by more than twice in runtime and reduces memory
usage by a remarkable four times. To further enhance the performance of
TransNormer, we leverage a gating mechanism to smooth training and a new tensor
normalization scheme to accelerate the model, resulting in an impressive
acceleration of over 20%. Furthermore, we have developed a robust inference
algorithm that ensures numerical stability and consistent inference speed,
regardless of the sequence length, showcasing superior efficiency during both
training and inference stages. Scalability is at the heart of our model's
design, enabling seamless deployment on large-scale clusters and facilitating
expansion to even more extensive models, all while maintaining outstanding
performance metrics. Rigorous validation of our model design is achieved
through a series of comprehensive experiments on our self-collected corpus,
boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure
data quality and relevance, we implement a new self-cleaning strategy to filter
our collected data. Our pre-trained models will be released to foster community
advancements in efficient LLMs.
</p></li>
</ul>

<h3>Title: Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design. (arXiv:2307.14374v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14374">http://arxiv.org/abs/2307.14374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14374] Forecasting, capturing and activation of carbon-dioxide (CO$_2$): Integration of Time Series Analysis, Machine Learning, and Material Design](http://arxiv.org/abs/2307.14374) #robust</code></li>
<li>Summary: <p>This study provides a comprehensive time series analysis of daily
industry-specific, country-wise CO$_2$ emissions from January 2019 to February</li>
<li>The research focuses on the Power, Industry, Ground Transport, Domestic
Aviation, and International Aviation sectors in European countries (EU27 &amp; UK,
Italy, Germany, Spain) and India, utilizing near-real-time activity data from
the Carbon Monitor research initiative. To identify regular emission patterns,
the data from the year 2020 is excluded due to the disruptive effects caused by
the COVID-19 pandemic. The study then performs a principal component analysis
(PCA) to determine the key contributors to CO$_2$ emissions. The analysis
reveals that the Power, Industry, and Ground Transport sectors account for a
significant portion of the variance in the dataset. A 7-day moving averaged
dataset is employed for further analysis to facilitate robust predictions. This
dataset captures both short-term and long-term trends and enhances the quality
of the data for prediction purposes. The study utilizes Long Short-Term Memory
(LSTM) models on the 7-day moving averaged dataset to effectively predict
emissions and provide insights for policy decisions, mitigation strategies, and
climate change efforts. During the training phase, the stability and
convergence of the LSTM models are ensured, which guarantees their reliability
in the testing phase. The evaluation of the loss function indicates this
reliability. The model achieves high efficiency, as demonstrated by $R^2$
values ranging from 0.8242 to 0.995 for various countries and sectors.
Furthermore, there is a proposal for utilizing scandium and
boron/aluminium-based thin films as exceptionally efficient materials for
capturing CO$_2$ (with a binding energy range from -3.0 to -3.5 eV). These
materials are shown to surpass the affinity of graphene and boron nitride
sheets in this regard.
</p></li>
</ul>

<h3>Title: DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm. (arXiv:2307.14375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14375">http://arxiv.org/abs/2307.14375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14375] DBGSA: A Novel Data Adaptive Bregman Clustering Algorithm](http://arxiv.org/abs/2307.14375) #robust</code></li>
<li>Summary: <p>With the development of Big data technology, data analysis has become
increasingly important. Traditional clustering algorithms such as K-means are
highly sensitive to the initial centroid selection and perform poorly on
non-convex datasets. In this paper, we address these problems by proposing a
data-driven Bregman divergence parameter optimization clustering algorithm
(DBGSA), which combines the Universal Gravitational Algorithm to bring similar
points closer in the dataset. We construct a gravitational coefficient equation
with a special property that gradually reduces the influence factor as the
iteration progresses. Furthermore, we introduce the Bregman divergence
generalized power mean information loss minimization to identify cluster
centers and build a hyperparameter identification optimization model, which
effectively solves the problems of manual adjustment and uncertainty in the
improved dataset. Extensive experiments are conducted on four simulated
datasets and six real datasets. The results demonstrate that DBGSA
significantly improves the accuracy of various clustering algorithms by an
average of 63.8\% compared to other similar approaches like enhanced clustering
algorithms and improved datasets. Additionally, a three-dimensional grid search
was established to compare the effects of different parameter values within
threshold conditions, and it was discovered the parameter set provided by our
model is optimal. This finding provides strong evidence of the high accuracy
and robustness of the algorithm.
</p></li>
</ul>

<h3>Title: Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations. (arXiv:2307.14380v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14380">http://arxiv.org/abs/2307.14380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14380] Robust Assignment of Labels for Active Learning with Sparse and Noisy Annotations](http://arxiv.org/abs/2307.14380) #robust</code></li>
<li>Summary: <p>Supervised classification algorithms are used to solve a growing number of
real-life problems around the globe. Their performance is strictly connected
with the quality of labels used in training. Unfortunately, acquiring
good-quality annotations for many tasks is infeasible or too expensive to be
done in practice. To tackle this challenge, active learning algorithms are
commonly employed to select only the most relevant data for labeling. However,
this is possible only when the quality and quantity of labels acquired from
experts are sufficient. Unfortunately, in many applications, a trade-off
between annotating individual samples by multiple annotators to increase label
quality vs. annotating new samples to increase the total number of labeled
instances is necessary. In this paper, we address the issue of faulty data
annotations in the context of active learning. In particular, we propose two
novel annotation unification algorithms that utilize unlabeled parts of the
sample space. The proposed methods require little to no intersection between
samples annotated by different experts. Our experiments on four public datasets
indicate the robustness and superiority of the proposed methods in both, the
estimation of the annotator's reliability, and the assignment of actual labels,
against the state-of-the-art algorithms and the simple majority voting.
</p></li>
</ul>

<h3>Title: Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14675">http://arxiv.org/abs/2307.14675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14675] Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification](http://arxiv.org/abs/2307.14675) #robust</code></li>
<li>Summary: <p>The ever-growing use of wind energy makes necessary the optimization of
turbine operations through pitch angle controllers and their maintenance with
early fault detection. It is crucial to have accurate and robust models
imitating the behavior of wind turbines, especially to predict the generated
power as a function of the wind speed. Existing empirical and physics-based
models have limitations in capturing the complex relations between the input
variables and the power, aggravated by wind variability. Data-driven methods
offer new opportunities to enhance wind turbine modeling of large datasets by
improving accuracy and efficiency. In this study, we used physics-informed
neural networks to reproduce historical data coming from 4 turbines in a wind
farm, while imposing certain physical constraints to the model. The developed
models for regression of the power, torque, and power coefficient as output
variables showed great accuracy for both real data and physical equations
governing the system. Lastly, introducing an efficient evidential layer
provided uncertainty estimations of the predictions, proved to be consistent
with the absolute error, and made possible the definition of a confidence
interval in the power curve.
</p></li>
</ul>

<h3>Title: A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs. (arXiv:2307.14940v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14940">http://arxiv.org/abs/2307.14940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14940] A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs](http://arxiv.org/abs/2307.14940) #robust</code></li>
<li>Summary: <p>The continuous dynamics of natural systems has been effectively modelled
using Neural Ordinary Differential Equations (Neural ODEs). However, for
accurate and meaningful predictions, it is crucial that the models follow the
underlying rules or laws that govern these systems. In this work, we propose a
self-adaptive penalty algorithm for Neural ODEs to enable modelling of
constrained natural systems. The proposed self-adaptive penalty function can
dynamically adjust the penalty parameters. The explicit introduction of prior
knowledge helps to increase the interpretability of Neural ODE -based models.
We validate the proposed approach by modelling three natural systems with prior
knowledge constraints: population growth, chemical reaction evolution, and
damped harmonic oscillator motion. The numerical experiments and a comparison
with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE,
demonstrate the effectiveness of the proposed self-adaptive penalty algorithm
for Neural ODEs in modelling constrained natural systems. Moreover, the
self-adaptive penalty approach provides more accurate and robust models with
reliable and meaningful predictions.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions. (arXiv:2307.14617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14617">http://arxiv.org/abs/2307.14617</a></li>
<li>Code URL: <a href="https://github.com/renmin1991/dyamic-graph-representation">https://github.com/renmin1991/dyamic-graph-representation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14617] Multiscale Dynamic Graph Representation for Biometric Recognition with Occlusions](http://arxiv.org/abs/2307.14617) #biometric</code></li>
<li>Summary: <p>Occlusion is a common problem with biometric recognition in the wild. The
generalization ability of CNNs greatly decreases due to the adverse effects of
various occlusions. To this end, we propose a novel unified framework
integrating the merits of both CNNs and graph models to overcome occlusion
problems in biometric recognition, called multiscale dynamic graph
representation (MS-DGR). More specifically, a group of deep features reflected
on certain subregions is recrafted into a feature graph (FG). Each node inside
the FG is deemed to characterize a specific local region of the input sample,
and the edges imply the co-occurrence of non-occluded regions. By analyzing the
similarities of the node representations and measuring the topological
structures stored in the adjacent matrix, the proposed framework leverages
dynamic graph matching to judiciously discard the nodes corresponding to the
occluded parts. The multiscale strategy is further incorporated to attain more
diverse nodes representing regions of various sizes. Furthermore, the proposed
framework exhibits a more illustrative and reasonable inference by showing the
paired nodes. Extensive experiments demonstrate the superiority of the proposed
framework, which boosts the accuracy in both natural and occlusion-simulated
cases by a large margin compared with that of baseline methods.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance. (arXiv:2307.14657v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14657">http://arxiv.org/abs/2307.14657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14657] Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance](http://arxiv.org/abs/2307.14657) #extraction</code></li>
<li>Summary: <p>Many studies have proposed machine-learning (ML) models for malware detection
and classification, reporting an almost-perfect performance. However, they
assemble ground-truth in different ways, use diverse static- and
dynamic-analysis techniques for feature extraction, and even differ on what
they consider a malware family. As a consequence, our community still lacks an
understanding of malware classification results: whether they are tied to the
nature and distribution of the collected dataset, to what extent the number of
families and samples in the training dataset influence performance, and how
well static and dynamic features complement each other.
</p></li>
</ul>

<p>This work sheds light on those open questions. by investigating the key
factors influencing ML-based malware detection and classification. For this, we
collect the largest balanced malware dataset so far with 67K samples from 670
families (100 samples each), and train state-of-the-art models for malware
detection and family classification using our dataset. Our results reveal that
static features perform better than dynamic features, and that combining both
only provides marginal improvement over static features. We discover no
correlation between packing and classification accuracy, and that missing
behaviors in dynamically-extracted features highly penalize their performance.
We also demonstrate how a larger number of families to classify make the
classification harder, while a higher number of samples per family increases
accuracy. Finally, we find that models trained on a uniform distribution of
samples per family better generalize on unseen data.
</p>

<h3>Title: Rapid and Scalable Bayesian AB Testing. (arXiv:2307.14628v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14628">http://arxiv.org/abs/2307.14628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14628] Rapid and Scalable Bayesian AB Testing](http://arxiv.org/abs/2307.14628) #extraction</code></li>
<li>Summary: <p>AB testing aids business operators with their decision making, and is
considered the gold standard method for learning from data to improve digital
user experiences. However, there is usually a gap between the requirements of
practitioners, and the constraints imposed by the statistical hypothesis
testing methodologies commonly used for analysis of AB tests. These include the
lack of statistical power in multivariate designs with many factors,
correlations between these factors, the need of sequential testing for early
stopping, and the inability to pool knowledge from past tests. Here, we propose
a solution that applies hierarchical Bayesian estimation to address the above
limitations. In comparison to current sequential AB testing methodology, we
increase statistical power by exploiting correlations between factors, enabling
sequential testing and progressive early stopping, without incurring excessive
false positive risk. We also demonstrate how this methodology can be extended
to enable the extraction of composite global learnings from past AB tests, to
accelerate future tests. We underpin our work with a solid theoretical
framework that articulates the value of hierarchical estimation. We demonstrate
its utility using both numerical simulations and a large set of real-world AB
tests. Together, these results highlight the practical value of our approach
for statistical inference in the technology industry.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification. (arXiv:2307.14959v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14959">http://arxiv.org/abs/2307.14959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14959] Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification](http://arxiv.org/abs/2307.14959) #federate</code></li>
<li>Summary: <p>In the medical field, federated learning commonly deals with highly
imbalanced datasets, including skin lesions and gastrointestinal images.
Existing federated methods under highly imbalanced datasets primarily focus on
optimizing a global model without incorporating the intra-class variations that
can arise in medical imaging due to different populations, findings, and
scanners. In this paper, we study the inter-client intra-class variations with
publicly available self-supervised auxiliary networks. Specifically, we find
that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on
every client yields consistent divergence measurements. Based on these
findings, we derive a dynamic balanced model aggregation via self-supervised
priors (MAS) to guide the global model optimization. Fed-MAS can be utilized
with different local learning methods for effective model aggregation toward a
highly robust and unbiased global model. Our code is available at
\url{https://github.com/xmed-lab/Fed-MAS}.
</p></li>
</ul>

<h3>Title: Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15017">http://arxiv.org/abs/2307.15017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15017] Samplable Anonymous Aggregation for Private Federated Data Analysis](http://arxiv.org/abs/2307.15017) #federate</code></li>
<li>Summary: <p>We revisit the problem of designing scalable protocols for private statistics
and private federated learning when each device holds its private data. Our
first contribution is to propose a simple primitive that allows for efficient
implementation of several commonly used algorithms, and allows for privacy
accounting that is close to that in the central setting without requiring the
strong trust assumptions it entails. Second, we propose a system architecture
that implements this primitive and perform a security analysis of the proposed
system.
</p></li>
</ul>

<h3>Title: HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14384">http://arxiv.org/abs/2307.14384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14384] HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning](http://arxiv.org/abs/2307.14384) #federate</code></li>
<li>Summary: <p>Federated learning (FL) collaboratively models user data in a decentralized
way. However, in the real world, non-identical and independent data
distributions (non-IID) among clients hinder the performance of FL due to three
issues, i.e., (1) the class statistics shifting, (2) the insufficient
hierarchical information utilization, and (3) the inconsistency in aggregating
clients. To address the above issues, we propose HyperFed which contains three
main modules, i.e., hyperbolic prototype Tammes initialization (HPTI),
hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly,
HPTI in the server constructs uniformly distributed and fixed class prototypes,
and shares them with clients to match class statistics, further guiding
consistent feature representation for local clients. Secondly, HPL in each
client captures the hierarchical information in local data with the supervision
of shared class prototypes in the hyperbolic model space. Additionally, CA in
the server mitigates the impact of the inconsistent deviations from clients to
server. Extensive studies of four datasets prove that HyperFed is effective in
enhancing the performance of FL under the non-IID set.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Explainable Disparity Compensation for Efficient Fair Ranking. (arXiv:2307.14366v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14366">http://arxiv.org/abs/2307.14366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14366] Explainable Disparity Compensation for Efficient Fair Ranking](http://arxiv.org/abs/2307.14366) #fair</code></li>
<li>Summary: <p>Ranking functions that are used in decision systems often produce disparate
results for different populations because of bias in the underlying data.
Addressing, and compensating for, these disparate outcomes is a critical
problem for fair decision-making. Recent compensatory measures have mostly
focused on opaque transformations of the ranking functions to satisfy fairness
guarantees or on the use of quotas or set-asides to guarantee a minimum number
of positive outcomes to members of underrepresented groups. In this paper we
propose easily explainable data-driven compensatory measures for ranking
functions. Our measures rely on the generation of bonus points given to members
of underrepresented groups to address disparity in the ranking function. The
bonus points can be set in advance, and can be combined, allowing for
considering the intersections of representations and giving better transparency
to stakeholders. We propose efficient sampling-based algorithms to calculate
the number of bonus points to minimize disparity. We validate our algorithms
using real-world school admissions and recidivism datasets, and compare our
results with that of existing fair ranking algorithms.
</p></li>
</ul>

<h3>Title: Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment. (arXiv:2307.14668v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14668">http://arxiv.org/abs/2307.14668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14668] Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment](http://arxiv.org/abs/2307.14668) #fair</code></li>
<li>Summary: <p>Algorithmic fairness has been a serious concern and received lots of interest
in machine learning community. In this paper, we focus on the bipartite ranking
scenario, where the instances come from either the positive or negative class
and the goal is to learn a ranking function that ranks positive instances
higher than negative ones. While there could be a trade-off between fairness
and performance, we propose a model agnostic post-processing framework xOrder
for achieving fairness in bipartite ranking and maintaining the algorithm
classification performance. In particular, we optimize a weighted sum of the
utility as identifying an optimal warping path across different protected
groups and solve it through a dynamic programming process. xOrder is compatible
with various classification models and ranking fairness metrics, including
supervised and unsupervised fairness metrics. In addition to binary groups,
xOrder can be applied to multiple protected groups. We evaluate our proposed
algorithm on four benchmark data sets and two real-world patient electronic
health record repositories. xOrder consistently achieves a better balance
between the algorithm utility and ranking fairness on a variety of datasets
with different metrics. From the visualization of the calibrated ranking
scores, xOrder mitigates the score distribution shifts of different groups
compared with baselines. Moreover, additional analytical results verify that
xOrder achieves a robust performance when faced with fewer samples and a bigger
difference between training and testing ranking score distributions.
</p></li>
</ul>

<h3>Title: Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14754">http://arxiv.org/abs/2307.14754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14754] Fair Machine Unlearning: Data Removal while Mitigating Disparities](http://arxiv.org/abs/2307.14754) #fair</code></li>
<li>Summary: <p>As public consciousness regarding the collection and use of personal
information by corporations grows, it is of increasing importance that
consumers be active participants in the curation of corporate datasets. In
light of this, data governance frameworks such as the General Data Protection
Regulation (GDPR) have outlined the right to be forgotten as a key principle
allowing individuals to request that their personal data be deleted from the
databases and models used by organizations. To achieve forgetting in practice,
several machine unlearning methods have been proposed to address the
computational inefficiencies of retraining a model from scratch with each
unlearning request. While efficient online alternatives to retraining, it is
unclear how these methods impact other properties critical to real-world
applications, such as fairness. In this work, we propose the first fair machine
unlearning method that can provably and efficiently unlearn data instances
while preserving group fairness. We derive theoretical results which
demonstrate that our method can provably unlearn data instances while
maintaining fairness objectives. Extensive experimentation with real-world
datasets highlight the efficacy of our method at unlearning data instances
while preserving fairness.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography. (arXiv:2307.14433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14433">http://arxiv.org/abs/2307.14433</a></li>
<li>Code URL: <a href="https://github.com/hooman007/protoasnet">https://github.com/hooman007/protoasnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14433] ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography](http://arxiv.org/abs/2307.14433) #interpretability</code></li>
<li>Summary: <p>Aortic stenosis (AS) is a common heart valve disease that requires accurate
and timely diagnosis for appropriate treatment. Most current automatic AS
severity detection methods rely on black-box models with a low level of
trustworthiness, which hinders clinical adoption. To address this issue, we
propose ProtoASNet, a prototypical network that directly detects AS from B-mode
echocardiography videos, while making interpretable predictions based on the
similarity between the input and learned spatio-temporal prototypes. This
approach provides supporting evidence that is clinically relevant, as the
prototypes typically highlight markers such as calcification and restricted
movement of aortic valve leaflets. Moreover, ProtoASNet utilizes abstention
loss to estimate aleatoric uncertainty by defining a set of prototypes that
capture ambiguity and insufficient information in the observed data. This
provides a reliable system that can detect and explain when it may fail. We
evaluate ProtoASNet on a private dataset and the publicly available TMED-2
dataset, where it outperforms existing state-of-the-art methods with an
accuracy of 80.0% and 79.7%, respectively. Furthermore, ProtoASNet provides
interpretability and an uncertainty measure for each prediction, which can
improve transparency and facilitate the interactive usage of deep networks to
aid clinical decision-making. Our source code is available at:
https://github.com/hooman007/ProtoASNet.
</p></li>
</ul>

<h3>Title: Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15007">http://arxiv.org/abs/2307.15007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15007] Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability](http://arxiv.org/abs/2307.15007) #interpretability</code></li>
<li>Summary: <p>With the increased deployment of machine learning models in various
real-world applications, researchers and practitioners alike have emphasized
the need for explanations of model behaviour. To this end, two broad strategies
have been outlined in prior literature to explain models. Post hoc explanation
methods explain the behaviour of complex black-box models by highlighting
features that are critical to model predictions; however, prior work has shown
that these explanations may not be faithful, and even more concerning is our
inability to verify them. Specifically, it is nontrivial to evaluate if a given
attribution is correct with respect to the underlying model. Inherently
interpretable models, on the other hand, circumvent these issues by explicitly
encoding explanations into model architecture, meaning their explanations are
naturally faithful and verifiable, but they often exhibit poor predictive
performance due to their limited expressive power. In this work, we aim to
bridge the gap between the aforementioned strategies by proposing Verifiability
Tuning (VerT), a method that transforms black-box models into models that
naturally yield faithful and verifiable feature attributions. We begin by
introducing a formal theoretical framework to understand verifiability and show
that attributions produced by standard models cannot be verified. We then
leverage this framework to propose a method to build verifiable models and
feature attributions out of fully trained black-box models. Finally, we perform
extensive experiments on semi-synthetic and real-world datasets, and show that
VerT produces models that (1) yield explanations that are correct and
verifiable and (2) are faithful to the original black-box models they are meant
to explain.
</p></li>
</ul>

<h3>Title: Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14395">http://arxiv.org/abs/2307.14395</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14395] Learning to simulate partially known spatio-temporal dynamics with trainable difference operators](http://arxiv.org/abs/2307.14395) #interpretability</code></li>
<li>Summary: <p>Recently, using neural networks to simulate spatio-temporal dynamics has
received a lot of attention. However, most existing methods adopt pure
data-driven black-box models, which have limited accuracy and interpretability.
By combining trainable difference operators with black-box models, we propose a
new hybrid architecture explicitly embedded with partial prior knowledge of the
underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options
called the trainable flipping difference layer (TFDL) and the trainable dynamic
difference layer (TDDL) for the difference operators. Numerous numerical
experiments have demonstrated that PDE-Net++ has superior prediction accuracy
and better extrapolation performance than black-box models.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models. (arXiv:2307.14648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14648">http://arxiv.org/abs/2307.14648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14648] Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2307.14648) #diffusion</code></li>
<li>Summary: <p>In this paper, we study the denoising diffusion probabilistic model (DDPM) in
wavelet space, instead of pixel space, for visual synthesis. Considering the
wavelet transform represents the image in spatial and frequency domains, we
carefully design a novel architecture SFUNet to effectively capture the
correlation for both domains. Specifically, in the standard denoising U-Net for
pixel data, we supplement the 2D convolutions and spatial-only attention layers
with our spatial frequency-aware convolution and attention modules to jointly
model the complementary information from spatial and frequency domains in
wavelet data. Our new architecture can be used as a drop-in replacement to the
pixel-based network and is compatible with the vanilla DDPM training process.
By explicitly modeling the wavelet signals, we find our model is able to
generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and
LSUN-Church datasets, than the pixel-based counterpart.
</p></li>
</ul>

<h3>Title: LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement. (arXiv:2307.14659v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14659">http://arxiv.org/abs/2307.14659</a></li>
<li>Code URL: <a href="https://github.com/taowangzj/lldiffusion">https://github.com/taowangzj/lldiffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14659] LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement](http://arxiv.org/abs/2307.14659) #diffusion</code></li>
<li>Summary: <p>Current deep learning methods for low-light image enhancement (LLIE)
typically rely on pixel-wise mapping learned from paired data. However, these
methods often overlook the importance of considering degradation
representations, which can lead to sub-optimal outcomes. In this paper, we
address this limitation by proposing a degradation-aware learning scheme for
LLIE using diffusion models, which effectively integrates degradation and image
priors into the diffusion process, resulting in improved image enhancement. Our
proposed degradation-aware learning scheme is based on the understanding that
degradation representations play a crucial role in accurately modeling and
capturing the specific degradation patterns present in low-light images. To
this end, First, a joint learning framework for both image generation and image
enhancement is presented to learn the degradation representations. Second, to
leverage the learned degradation representations, we develop a Low-Light
Diffusion model (LLDiffusion) with a well-designed dynamic diffusion module.
This module takes into account both the color map and the latent degradation
representations to guide the diffusion process. By incorporating these
conditioning factors, the proposed LLDiffusion can effectively enhance
low-light images, considering both the inherent degradation patterns and the
desired color fidelity. Finally, we evaluate our proposed method on several
well-known benchmark datasets, including synthetic and real-world unpaired
datasets. Extensive experiments on public benchmarks demonstrate that our
LLDiffusion outperforms state-of-the-art LLIE methods both quantitatively and
qualitatively. The source code and pre-trained models are available at
https://github.com/TaoWangzj/LLDiffusion.
</p></li>
</ul>

<h3>Title: TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis. (arXiv:2307.15042v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15042">http://arxiv.org/abs/2307.15042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15042] TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis](http://arxiv.org/abs/2307.15042) #diffusion</code></li>
<li>Summary: <p>The gradual nature of a diffusion process that synthesizes samples in small
increments constitutes a key ingredient of Denoising Diffusion Probabilistic
Models (DDPM), which have presented unprecedented quality in image synthesis
and been recently explored in the motion domain. In this work, we propose to
adapt the gradual diffusion concept (operating along a diffusion time-axis)
into the temporal-axis of the motion sequence. Our key idea is to extend the
DDPM framework to support temporally varying denoising, thereby entangling the
two axes. Using our special formulation, we iteratively denoise a motion buffer
that contains a set of increasingly-noised poses, which auto-regressively
produces an arbitrarily long stream of frames. With a stationary diffusion
time-axis, in each diffusion step we increment only the temporal-axis of the
motion such that the framework produces a new, clean frame which is removed
from the beginning of the buffer, followed by a newly drawn noise vector that
is appended to it. This new mechanism paves the way towards a new framework for
long-term motion synthesis with applications to character animation and other
domains.
</p></li>
</ul>

<h3>Title: Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14613">http://arxiv.org/abs/2307.14613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14613] Self-Contrastive Graph Diffusion Network](http://arxiv.org/abs/2307.14613) #diffusion</code></li>
<li>Summary: <p>Augmentation techniques and sampling strategies are crucial in contrastive
learning, but in most existing works, augmentation techniques require careful
design, and their sampling strategies can only capture a small amount of
intrinsic supervision information. Additionally, the existing methods require
complex designs to obtain two different representations of the data. To
overcome these limitations, we propose a novel framework called the
Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two
main components: the Attentional Module (AttM) and the Diffusion Module (DiFM).
AttM aggregates higher-order structure and feature information to get an
excellent embedding, while DiFM balances the state of each node in the graph
through Laplacian diffusion learning and allows the cooperative evolution of
adjacency and feature information in the graph. Unlike existing methodologies,
SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic
drift, without the need for pre-training. We conduct a high-quality sampling of
samples based on structure and feature information. If two nodes are neighbors,
they are considered positive samples of each other. If two disconnected nodes
are also unrelated on $k$NN graph, they are considered negative samples for
each other. The contrastive objective reasonably uses our proposed sampling
strategies, and the redundancy reduction term minimizes redundant information
in the embedding and can well retain more discriminative information. In this
novel framework, the graph self-contrastive learning paradigm gives expression
to a powerful force. SCGDN effectively balances between preserving high-order
structure information and avoiding overfitting. The results manifest that SCGDN
can consistently generate outperformance over both the contrastive methods and
the classical methods.
</p></li>
</ul>

<h3>Title: Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14619">http://arxiv.org/abs/2307.14619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14619] Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior](http://arxiv.org/abs/2307.14619) #diffusion</code></li>
<li>Summary: <p>We propose a theoretical framework for studying the imitation of stochastic,
non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations
in nonlinear dynamical systems. Our framework invokes low-level controllers -
either learned or implicit in position-command control - to stabilize imitation
policies around expert demonstrations. We show that with (a) a suitable
low-level stability guarantee and (b) a stochastic continuity property of the
learned policy we call "total variation continuity" (TVC), an imitator that
accurately estimates actions on the demonstrator's state distribution closely
matches the demonstrator's distribution over entire trajectories. We then show
that TVC can be ensured with minimal degradation of accuracy by combining a
popular data-augmentation regimen with a novel algorithmic trick: adding
augmentation noise at execution time. We instantiate our guarantees for
policies parameterized by diffusion models and prove that if the learner
accurately estimates the score of the (noise-augmented) expert policy, then the
distribution of imitator trajectories is close to the demonstrator distribution
in a natural optimal transport distance. Our analysis constructs intricate
couplings between noise-augmented trajectories, a technique that may be of
independent interest. We conclude by empirically validating our algorithmic
recommendations.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: HTNet for micro-expression recognition. (arXiv:2307.14637v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14637">http://arxiv.org/abs/2307.14637</a></li>
<li>Code URL: <a href="https://github.com/wangzhifengharrison/htnet">https://github.com/wangzhifengharrison/htnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14637] HTNet for micro-expression recognition](http://arxiv.org/abs/2307.14637) #transformer</code></li>
<li>Summary: <p>Facial expression is related to facial muscle contractions and different
muscle movements correspond to different emotional states. For micro-expression
recognition, the muscle movements are usually subtle, which has a negative
impact on the performance of current facial emotion recognition algorithms.
Most existing methods use self-attention mechanisms to capture relationships
between tokens in a sequence, but they do not take into account the inherent
spatial relationships between facial landmarks. This can result in sub-optimal
performance on micro-expression recognition tasks.Therefore, learning to
recognize facial muscle movements is a key challenge in the area of
micro-expression recognition. In this paper, we propose a Hierarchical
Transformer Network (HTNet) to identify critical areas of facial muscle
movement. HTNet includes two major components: a transformer layer that
leverages the local temporal features and an aggregation layer that extracts
local and global semantical facial features. Specifically, HTNet divides the
face into four different facial areas: left lip area, left eye area, right eye
area and right lip area. The transformer layer is used to focus on representing
local minor muscle movement with local self-attention in each area. The
aggregation layer is used to learn the interactions between eye areas and lip
areas. The experiments on four publicly available micro-expression datasets
show that the proposed approach outperforms previous methods by a large margin.
The codes and models are available at:
\url{https://github.com/wangzhifengharrison/HTNet}
</p></li>
</ul>

<h3>Title: Pre-training Vision Transformers with Very Limited Synthesized Images. (arXiv:2307.14710v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14710">http://arxiv.org/abs/2307.14710</a></li>
<li>Code URL: <a href="https://github.com/ryoo-nakamura/ofdb">https://github.com/ryoo-nakamura/ofdb</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14710] Pre-training Vision Transformers with Very Limited Synthesized Images](http://arxiv.org/abs/2307.14710) #transformer</code></li>
<li>Summary: <p>Formula-driven supervised learning (FDSL) is a pre-training method that
relies on synthetic images generated from mathematical formulae such as
fractals. Prior work on FDSL has shown that pre-training vision transformers on
such synthetic datasets can yield competitive accuracy on a wide range of
downstream tasks. These synthetic images are categorized according to the
parameters in the mathematical formula that generate them. In the present work,
we hypothesize that the process for generating different instances for the same
category in FDSL, can be viewed as a form of data augmentation. We validate
this hypothesis by replacing the instances with data augmentation, which means
we only need a single image per category. Our experiments shows that this
one-instance fractal database (OFDB) performs better than the original dataset
where instances were explicitly generated. We further scale up OFDB to 21,000
categories and show that it matches, or even surpasses, the model pre-trained
on ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is
21k, whereas ImageNet-21k has 14M. This opens new possibilities for
pre-training vision transformers with much smaller datasets.
</p></li>
</ul>

<h3>Title: pCTFusion: Point Convolution-Transformer Fusion with Semantic Aware Loss for Outdoor LiDAR Point Cloud Segmentation. (arXiv:2307.14777v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14777">http://arxiv.org/abs/2307.14777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14777] pCTFusion: Point Convolution-Transformer Fusion with Semantic Aware Loss for Outdoor LiDAR Point Cloud Segmentation](http://arxiv.org/abs/2307.14777) #transformer</code></li>
<li>Summary: <p>LiDAR-generated point clouds are crucial for perceiving outdoor environments.
The segmentation of point clouds is also essential for many applications.
Previous research has focused on using self-attention and convolution (local
attention) mechanisms individually in semantic segmentation architectures.
However, there is limited work on combining the learned representations of
these attention mechanisms to improve performance. Additionally, existing
research that combines convolution with self-attention relies on global
attention, which is not practical for processing large point clouds. To address
these challenges, this study proposes a new architecture, pCTFusion, which
combines kernel-based convolutions and self-attention mechanisms for better
feature learning and capturing local and global dependencies in segmentation.
The proposed architecture employs two types of self-attention mechanisms, local
and global, based on the hierarchical positions of the encoder blocks.
Furthermore, the existing loss functions do not consider the semantic and
position-wise importance of the points, resulting in reduced accuracy,
particularly at sharp class boundaries. To overcome this, the study models a
novel attention-based loss function called Pointwise Geometric Anisotropy
(PGA), which assigns weights based on the semantic distribution of points in a
neighborhood. The proposed architecture is evaluated on SemanticKITTI outdoor
dataset and showed a 5-7% improvement in performance compared to the
state-of-the-art architectures. The results are particularly encouraging for
minor classes, often misclassified due to class imbalance, lack of space, and
neighbor-aware feature encoding. These developed methods can be leveraged for
the segmentation of complex datasets and can drive real-world applications of
LiDAR point cloud.
</p></li>
</ul>

<h3>Title: IML-ViT: Image Manipulation Localization by Vision Transformer. (arXiv:2307.14863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14863">http://arxiv.org/abs/2307.14863</a></li>
<li>Code URL: <a href="https://github.com/sunnyhaze/iml-vit">https://github.com/sunnyhaze/iml-vit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14863] IML-ViT: Image Manipulation Localization by Vision Transformer](http://arxiv.org/abs/2307.14863) #transformer</code></li>
<li>Summary: <p>Advanced image tampering techniques are increasingly challenging the
trustworthiness of multimedia, leading to the development of Image Manipulation
Localization (IML). But what makes a good IML model? The answer lies in the way
to capture artifacts. Exploiting artifacts requires the model to extract
non-semantic discrepancies between the manipulated and authentic regions, which
needs to compare differences between these two areas explicitly. With the
self-attention mechanism, naturally, the Transformer is the best candidate.
Besides, artifacts are sensitive to image resolution, amplified under
multi-scale features, and massive at the manipulation border. Therefore, we
formulate the answer to the former question as building a ViT with
high-resolution capacity, multi-scale feature extraction capability, and
manipulation edge supervision. We term this simple but effective ViT paradigm
as the IML-ViT, which has great potential to become a new benchmark for IML.
Extensive experiments on five benchmark datasets verified our model outperforms
the state-of-the-art manipulation localization methods. Code and models are
available at \url{https://github.com/SunnyHaze/IML-ViT}
</p></li>
</ul>

<h3>Title: Self-Supervised Graph Transformer for Deepfake Detection. (arXiv:2307.15019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15019">http://arxiv.org/abs/2307.15019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15019] Self-Supervised Graph Transformer for Deepfake Detection](http://arxiv.org/abs/2307.15019) #transformer</code></li>
<li>Summary: <p>Deepfake detection methods have shown promising results in recognizing
forgeries within a given dataset, where training and testing take place on the
in-distribution dataset. However, their performance deteriorates significantly
when presented with unseen samples. As a result, a reliable deepfake detection
system must remain impartial to forgery types, appearance, and quality for
guaranteed generalizable detection performance. Despite various attempts to
enhance cross-dataset generalization, the problem remains challenging,
particularly when testing against common post-processing perturbations, such as
video compression or blur. Hence, this study introduces a deepfake detection
framework, leveraging a self-supervised pre-training model that delivers
exceptional generalization ability, withstanding common corruptions and
enabling feature explainability. The framework comprises three key components:
a feature extractor based on vision Transformer architecture that is
pre-trained via self-supervised contrastive learning methodology, a graph
convolution network coupled with a Transformer discriminator, and a graph
Transformer relevancy map that provides a better understanding of manipulated
regions and further explains the model's decision. To assess the effectiveness
of the proposed framework, several challenging experiments are conducted,
including in-data distribution performance, cross-dataset, cross-manipulation
generalization, and robustness against common post-production perturbations.
The results achieved demonstrate the remarkable effectiveness of the proposed
deepfake detection framework, surpassing the current state-of-the-art
approaches.
</p></li>
</ul>

<h3>Title: A Transformer-based Approach for Arabic Offline Handwritten Text Recognition. (arXiv:2307.15045v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15045">http://arxiv.org/abs/2307.15045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15045] A Transformer-based Approach for Arabic Offline Handwritten Text Recognition](http://arxiv.org/abs/2307.15045) #transformer</code></li>
<li>Summary: <p>Handwriting recognition is a challenging and critical problem in the fields
of pattern recognition and machine learning, with applications spanning a wide
range of domains. In this paper, we focus on the specific issue of recognizing
offline Arabic handwritten text. Existing approaches typically utilize a
combination of convolutional neural networks for image feature extraction and
recurrent neural networks for temporal modeling, with connectionist temporal
classification used for text generation. However, these methods suffer from a
lack of parallelization due to the sequential nature of recurrent neural
networks. Furthermore, these models cannot account for linguistic rules,
necessitating the use of an external language model in the post-processing
stage to boost accuracy. To overcome these issues, we introduce two alternative
architectures, namely the Transformer Transducer and the standard
sequence-to-sequence Transformer, and compare their performance in terms of
accuracy and speed. Our approach can model language dependencies and relies
only on the attention mechanism, thereby making it more parallelizable and less
complex. We employ pre-trained Transformers for both image understanding and
language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that
our proposed method outperforms the current state-of-the-art approaches for
recognizing offline Arabic handwritten text.
</p></li>
</ul>

<h3>Title: Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14544">http://arxiv.org/abs/2307.14544</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14544] Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span](http://arxiv.org/abs/2307.14544) #transformer</code></li>
<li>Summary: <p>This paper presents a novel approach to assist students with dyslexia, ADHD,
and short attention span in digesting any text-based information more
efficiently. The proposed solution utilizes the Multilayer Perceptron (MLP)
algorithm for complex text processing and summarization tasks. The tool
leverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face,
which treats every NLP task as a text generation task. The model is fine-tuned
on specific tasks using a smaller dataset. The NLTK's Punkt Sentence Tokenizer
is used to divide a text into a list of sentences. The application is served
using Flask, a lightweight web server and framework. The tool also applies
principles from Bionic Reading to enhance readability, which includes a bolding
function and adjustments to line, word, and character spacing. The paper
discusses the methodology, implementation, and results of the AI-based speed
reading tool.
</p></li>
</ul>

<h3>Title: Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training. (arXiv:2307.14666v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14666">http://arxiv.org/abs/2307.14666</a></li>
<li>Code URL: <a href="https://github.com/fraunhofer-iais/arabic_nlp">https://github.com/fraunhofer-iais/arabic_nlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14666] Improving Natural Language Inference in Arabic using Transformer Models and Linguistically Informed Pre-Training](http://arxiv.org/abs/2307.14666) #transformer</code></li>
<li>Summary: <p>This paper addresses the classification of Arabic text data in the field of
Natural Language Processing (NLP), with a particular focus on Natural Language
Inference (NLI) and Contradiction Detection (CD). Arabic is considered a
resource-poor language, meaning that there are few data sets available, which
leads to limited availability of NLP methods. To overcome this limitation, we
create a dedicated data set from publicly available resources. Subsequently,
transformer-based machine learning models are being trained and evaluated. We
find that a language-specific model (AraBERT) performs competitively with
state-of-the-art multilingual approaches, when we apply linguistically informed
pre-training methods such as Named Entity Recognition (NER). To our knowledge,
this is the first large-scale evaluation for this task in Arabic, as well as
the first application of multi-task pre-training in this context.
</p></li>
</ul>

<h3>Title: Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model. (arXiv:2307.14785v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14785">http://arxiv.org/abs/2307.14785</a></li>
<li>Code URL: <a href="https://github.com/pauli31/srl-aspect-based-sentiment">https://github.com/pauli31/srl-aspect-based-sentiment</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14785] Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model](http://arxiv.org/abs/2307.14785) #transformer</code></li>
<li>Summary: <p>This paper presents a series of approaches aimed at enhancing the performance
of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic
information from a Semantic Role Labeling (SRL) model. We propose a novel
end-to-end Semantic Role Labeling model that effectively captures most of the
structured semantic information within the Transformer hidden state. We believe
that this end-to-end model is well-suited for our newly proposed models that
incorporate semantic information. We evaluate the proposed models in two
languages, English and Czech, employing ELECTRA-small models. Our combined
models improve ABSA performance in both languages. Moreover, we achieved new
state-of-the-art results on the Czech ABSA.
</p></li>
</ul>

<h3>Title: ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection. (arXiv:2307.14912v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14912">http://arxiv.org/abs/2307.14912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14912] ARC-NLP at PAN 2023: Hierarchical Long Text Classification for Trigger Detection](http://arxiv.org/abs/2307.14912) #transformer</code></li>
<li>Summary: <p>Fanfiction, a popular form of creative writing set within established
fictional universes, has gained a substantial online following. However,
ensuring the well-being and safety of participants has become a critical
concern in this community. The detection of triggering content, material that
may cause emotional distress or trauma to readers, poses a significant
challenge. In this paper, we describe our approach for the Trigger Detection
shared task at PAN CLEF 2023, where we want to detect multiple triggering
content in a given Fanfiction document. For this, we build a hierarchical model
that uses recurrence over Transformer-based language models. In our approach,
we first split long documents into smaller sized segments and use them to
fine-tune a Transformer model. Then, we extract feature embeddings from the
fine-tuned Transformer model, which are used as input in the training of
multiple LSTM models for trigger detection in a multi-label setting. Our model
achieves an F1-macro score of 0.372 and F1-micro score of 0.736 on the
validation set, which are higher than the baseline results shared at PAN CLEF
2023.
</p></li>
</ul>

<h3>Title: ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection. (arXiv:2307.14913v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14913">http://arxiv.org/abs/2307.14913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14913] ARC-NLP at PAN 2023: Transition-Focused Natural Language Inference for Writing Style Detection](http://arxiv.org/abs/2307.14913) #transformer</code></li>
<li>Summary: <p>The task of multi-author writing style detection aims at finding any
positions of writing style change in a given text document. We formulate the
task as a natural language inference problem where two consecutive paragraphs
are paired. Our approach focuses on transitions between paragraphs while
truncating input tokens for the task. As backbone models, we employ different
Transformer-based encoders with warmup phase during training. We submit the
model version that outperforms baselines and other proposed model versions in
our experiments. For the easy and medium setups, we submit transition-focused
natural language inference based on DeBERTa with warmup training, and the same
model without transition for the hard setup.
</p></li>
</ul>

<h3>Title: Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs. (arXiv:2307.14988v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14988">http://arxiv.org/abs/2307.14988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14988] Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs](http://arxiv.org/abs/2307.14988) #transformer</code></li>
<li>Summary: <p>Deep learning often faces the challenge of efficiently processing dynamic
inputs, such as sensor data or user inputs. For example, an AI writing
assistant is required to update its suggestions in real time as a document is
edited. Re-running the model each time is expensive, even with compression
techniques like knowledge distillation, pruning, or quantization. Instead, we
take an incremental computing approach, looking to reuse calculations as the
inputs change. However, the dense connectivity of conventional architectures
poses a major obstacle to incremental computation, as even minor input changes
cascade through the network and restrict information reuse. To address this, we
use vector quantization to discretize intermediate values in the network, which
filters out noisy and unnecessary modifications to hidden neurons, facilitating
the reuse of their values. We apply this approach to the transformers
architecture, creating an efficient incremental inference algorithm with
complexity proportional to the fraction of the modified inputs. Our experiments
with adapting the OPT-125M pre-trained language model demonstrate comparable
accuracy on document classification while requiring 12.1X (median) fewer
operations for processing sequences of atomic edits.
</p></li>
</ul>

<h3>Title: HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting. (arXiv:2307.14596v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14596">http://arxiv.org/abs/2307.14596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14596] HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting](http://arxiv.org/abs/2307.14596) #transformer</code></li>
<li>Summary: <p>Traffic forecasting, which aims to predict traffic conditions based on
historical observations, has been an enduring research topic and is widely
recognized as an essential component of intelligent transportation. Recent
proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made
significant progress by combining sequential models with graph convolution
networks. However, due to high complexity issues, STGNNs only focus on
short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more
practical long-term forecasting. In this paper, we make the first attempt to
explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we
first reveal its unique challenges in exploiting multi-scale representations.
Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address
the issues of long-term traffic forecasting. HUTFormer consists of a
hierarchical encoder and decoder to jointly generate and utilize multi-scale
representations of traffic data. Specifically, for the encoder, we propose
window self-attention and segment merging to extract multi-scale
representations from long-term traffic data. For the decoder, we design a
cross-scale attention mechanism to effectively incorporate multi-scale
representations. In addition, HUTFormer employs an efficient input embedding
strategy to address the complexity issues. Extensive experiments on four
traffic datasets show that the proposed HUTFormer significantly outperforms
state-of-the-art traffic forecasting and long time series forecasting
baselines.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14397">http://arxiv.org/abs/2307.14397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14397] A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot](http://arxiv.org/abs/2307.14397) #generative</code></li>
<li>Summary: <p>In machine learning, generative modeling aims to learn to generate new data
statistically similar to the training data distribution. In this paper, we
survey learning generative models under limited data, few shots and zero shot,
referred to as Generative Modeling under Data Constraint (GM-DC). This is an
important topic when data acquisition is challenging, e.g. healthcare
applications. We discuss background, challenges, and propose two taxonomies:
one on GM-DC tasks and another on GM-DC approaches. Importantly, we study
interactions between different GM-DC tasks and approaches. Furthermore, we
highlight research gaps, research trends, and potential avenues for future
exploration. Project website: https://gmdc-survey.github.io.
</p></li>
</ul>

<h3>Title: EqGAN: Feature Equalization Fusion for Few-shot Image Generation. (arXiv:2307.14638v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14638">http://arxiv.org/abs/2307.14638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14638] EqGAN: Feature Equalization Fusion for Few-shot Image Generation](http://arxiv.org/abs/2307.14638) #generative</code></li>
<li>Summary: <p>Due to the absence of fine structure and texture information, existing
fusion-based few-shot image generation methods suffer from unsatisfactory
generation quality and diversity. To address this problem, we propose a novel
feature Equalization fusion Generative Adversarial Network (EqGAN) for few-shot
image generation. Unlike existing fusion strategies that rely on either deep
features or local representations, we design two separate branches to fuse
structures and textures by disentangling encoded features into shallow and deep
contents. To refine image contents at all feature levels, we equalize the fused
structure and texture semantics at different scales and supplement the decoder
with richer information by skip connections. Since the fused structures and
textures may be inconsistent with each other, we devise a consistent
equalization loss between the equalized features and the intermediate output of
the decoder to further align the semantics. Comprehensive experiments on three
public datasets demonstrate that, EqGAN not only significantly improves
generation performance with FID score (by up to 32.7%) and LPIPS score (by up
to 4.19%), but also outperforms the state-of-the-arts in terms of accuracy (by
up to 1.97%) for downstream classification tasks.
</p></li>
</ul>

<h3>Title: MIM-OOD: Generative Masked Image Modelling for Out-of-Distribution Detection in Medical Images. (arXiv:2307.14701v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14701">http://arxiv.org/abs/2307.14701</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14701] MIM-OOD: Generative Masked Image Modelling for Out-of-Distribution Detection in Medical Images](http://arxiv.org/abs/2307.14701) #generative</code></li>
<li>Summary: <p>Unsupervised Out-of-Distribution (OOD) detection consists in identifying
anomalous regions in images leveraging only models trained on images of healthy
anatomy. An established approach is to tokenize images and model the
distribution of tokens with Auto-Regressive (AR) models. AR models are used to
1) identify anomalous tokens and 2) in-paint anomalous representations with
in-distribution tokens. However, AR models are slow at inference time and prone
to error accumulation issues which negatively affect OOD detection performance.
Our novel method, MIM-OOD, overcomes both speed and error accumulation issues
by replacing the AR model with two task-specific networks: 1) a transformer
optimized to identify anomalous tokens and 2) a transformer optimized to
in-paint anomalous tokens using masked image modelling (MIM). Our experiments
with brain MRI anomalies show that MIM-OOD substantially outperforms AR models
(DICE 0.458 vs 0.301) while achieving a nearly 25x speedup (9.5s vs 244s).
</p></li>
</ul>

<h3>Title: Semantic Image Completion and Enhancement using GANs. (arXiv:2307.14748v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14748">http://arxiv.org/abs/2307.14748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14748] Semantic Image Completion and Enhancement using GANs](http://arxiv.org/abs/2307.14748) #generative</code></li>
<li>Summary: <p>Semantic inpainting or image completion alludes to the task of inferring
arbitrary large missing regions in images based on image semantics. Since the
prediction of image pixels requires an indication of high-level context, this
makes it significantly tougher than image completion, which is often more
concerned with correcting data corruption and removing entire objects from the
input image. On the other hand, image enhancement attempts to eliminate
unwanted noise and blur from the image, along with sustaining most of the image
details. Efficient image completion and enhancement model should be able to
recover the corrupted and masked regions in images and then refine the image
further to increase the quality of the output image. Generative Adversarial
Networks (GAN), have turned out to be helpful in picture completion tasks. In
this chapter, we will discuss the underlying GAN architecture and how they can
be used used for image completion tasks.
</p></li>
</ul>

<h3>Title: GET3D--: Learning GET3D from Unconstrained Image Collections. (arXiv:2307.14918v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14918">http://arxiv.org/abs/2307.14918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14918] GET3D--: Learning GET3D from Unconstrained Image Collections](http://arxiv.org/abs/2307.14918) #generative</code></li>
<li>Summary: <p>The demand for efficient 3D model generation techniques has grown
exponentially, as manual creation of 3D models is time-consuming and requires
specialized expertise. While generative models have shown potential in creating
3D textured shapes from 2D images, their applicability in 3D industries is
limited due to the lack of a well-defined camera distribution in real-world
scenarios, resulting in low-quality shapes. To overcome this limitation, we
propose GET3D--, the first method that directly generates textured 3D shapes
from 2D images with unknown pose and scale. GET3D-- comprises a 3D shape
generator and a learnable camera sampler that captures the 6D external changes
on the camera. In addition, We propose a novel training schedule to stably
optimize both the shape generator and camera sampler in a unified framework. By
controlling external variations using the learnable camera sampler, our method
can generate aligned shapes with clear textures. Extensive experiments
demonstrate the efficacy of GET3D--, which precisely fits the 6D camera pose
distribution and generates high-quality shapes on both synthetic and realistic
unconstrained datasets.
</p></li>
</ul>

<h3>Title: Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14971">http://arxiv.org/abs/2307.14971</a></li>
<li>Code URL: <a href="https://github.com/wangzy22/tap">https://github.com/wangzy22/tap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14971] Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models](http://arxiv.org/abs/2307.14971) #generative</code></li>
<li>Summary: <p>With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.
</p></li>
</ul>

<h3>Title: How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15016">http://arxiv.org/abs/2307.15016</a></li>
<li>Code URL: <a href="https://github.com/htqin/googlebard-visunderstand">https://github.com/htqin/googlebard-visunderstand</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15016] How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges](http://arxiv.org/abs/2307.15016) #generative</code></li>
<li>Summary: <p>Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard's
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard's performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand
</p></li>
</ul>

<h3>Title: Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14712">http://arxiv.org/abs/2307.14712</a></li>
<li>Code URL: <a href="https://github.com/shuzhouyuan/eval_g2t_genmodels">https://github.com/shuzhouyuan/eval_g2t_genmodels</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14712] Evaluating Generative Models for Graph-to-Text Generation](http://arxiv.org/abs/2307.14712) #generative</code></li>
<li>Summary: <p>Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.
</p></li>
</ul>

<h3>Title: Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction. (arXiv:2307.14788v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14788">http://arxiv.org/abs/2307.14788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14788] Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction](http://arxiv.org/abs/2307.14788) #generative</code></li>
<li>Summary: <p>Autonomous systems in the road transportation network require intelligent
mechanisms that cope with uncertainty to foresee the future. In this paper, we
propose a multi-stage probabilistic approach for trajectory forecasting:
trajectory transformation to displacement space, clustering of displacement
time series, trajectory proposals, and ranking proposals. We introduce a new
deep feature clustering method, underlying self-conditioned GAN, which copes
better with distribution shifts than traditional methods. Additionally, we
propose novel distance-based ranking proposals to assign probabilities to the
generated trajectories that are more efficient yet accurate than an auxiliary
neural network. The overall system surpasses context-free deep generative
models in human and road agents trajectory data while performing similarly to
point estimators when comparing the most probable trajectory.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14377">http://arxiv.org/abs/2307.14377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14377] How Can Large Language Models Help Humans in Design and Manufacturing?](http://arxiv.org/abs/2307.14377) #large language model</code></li>
<li>Summary: <p>The advancement of Large Language Models (LLMs), including GPT-4, provides
exciting new opportunities for generative design. We investigate the
application of this tool across the entire design and manufacturing workflow.
Specifically, we scrutinize the utility of LLMs in tasks such as: converting a
text-based prompt into a design specification, transforming a design into
manufacturing instructions, producing a design space and design variations,
computing the performance of a design, and searching for designs predicated on
performance. Through a series of examples, we highlight both the benefits and
the limitations of the current LLMs. By exposing these limitations, we aspire
to catalyze the continued improvement and progression of these models.
</p></li>
</ul>

<h3>Title: CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions. (arXiv:2307.14522v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14522">http://arxiv.org/abs/2307.14522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14522] CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions](http://arxiv.org/abs/2307.14522) #large language model</code></li>
<li>Summary: <p>A clinical trial is a study that evaluates new biomedical interventions. To
design new trials, researchers draw inspiration from those current and
completed. In 2022, there were on average more than 100 clinical trials
submitted to ClinicalTrials.gov every day, with each trial having a mean of
approximately 1500 words [1]. This makes it nearly impossible to keep up to
date. To mitigate this issue, we have created a batch clinical trial summarizer
called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first
tool able to provide real-time, truthful, and comprehensive summaries of
clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions
(approximately 10,500 words) into a concise 200-word summary with references
and limited hallucinations. We have tested CliniDigest on its ability to
summarize 457 trials divided across 27 medical subdomains. For each field,
CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which
utilizes $\mu=54\%,\ \sigma=30\% $ of the sources. A more comprehensive
evaluation is planned and outlined in this paper.
</p></li>
</ul>

<h3>Title: ArcGPT: A Large Language Model Tailored for Real-world Archival Applications. (arXiv:2307.14852v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14852">http://arxiv.org/abs/2307.14852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14852] ArcGPT: A Large Language Model Tailored for Real-world Archival Applications](http://arxiv.org/abs/2307.14852) #large language model</code></li>
<li>Summary: <p>Archives play a crucial role in preserving information and knowledge, and the
exponential growth of such data necessitates efficient and automated tools for
managing and utilizing archive information resources. Archival applications
involve managing massive data that are challenging to process and analyze.
Although LLMs have made remarkable progress in diverse domains, there are no
publicly available archives tailored LLM. Addressing this gap, we introduce
ArcGPT, to our knowledge, the first general-purpose LLM tailored to the
archival field. To enhance model performance on real-world archival tasks,
ArcGPT has been pre-trained on massive and extensive archival domain data.
Alongside ArcGPT, we release AMBLE, a benchmark comprising four real-world
archival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing
state-of-the-art models, marking a substantial step forward in effective
archival data management. Ultimately, ArcGPT aims to better serve the archival
community, aiding archivists in their crucial role of preserving and harnessing
our collective information and knowledge.
</p></li>
</ul>

<h3>Title: PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14936">http://arxiv.org/abs/2307.14936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14936] PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](http://arxiv.org/abs/2307.14936) #large language model</code></li>
<li>Summary: <p>Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.
</p></li>
</ul>

<h3>Title: SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15020">http://arxiv.org/abs/2307.15020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15020] SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark](http://arxiv.org/abs/2307.15020) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have shown the potential to be integrated into
human daily lives. Therefore, user preference is the most critical criterion
for assessing LLMs' performance in real-world scenarios. However, existing
benchmarks mainly focus on measuring models' accuracy using multi-choice
questions, which limits the understanding of their capabilities in real
applications. We fill this gap by proposing a comprehensive Chinese benchmark
SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE
encompasses three sub-tasks: actual users' queries and ratings derived from an
LLM battle platform (CArena), open-ended questions with single and
multiple-turn dialogues (OPEN), and closed-ended questions with the same stems
as open-ended single-turn ones (CLOSE). Our study shows that accuracy on
closed-ended questions is insufficient to reflect human preferences achieved on
open-ended ones. At the same time, they can complement each other to predict
actual user preferences. We also demonstrate that GPT-4 is a reliable judge to
automatically evaluate human preferences on open-ended questions in a Chinese
context. Our benchmark will be released at https://www.CLUEbenchmarks.com
</p></li>
</ul>

<h3>Title: Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15051">http://arxiv.org/abs/2307.15051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15051] Matching Patients to Clinical Trials with Large Language Models](http://arxiv.org/abs/2307.15051) #large language model</code></li>
<li>Summary: <p>Clinical trials are vital in advancing drug development and evidence-based
medicine, but their success is often hindered by challenges in patient
recruitment. In this work, we investigate the potential of large language
models (LLMs) to assist individual patients and referral physicians in
identifying suitable clinical trials from an extensive selection. Specifically,
we introduce TrialGPT, a novel architecture employing LLMs to predict
criterion-level eligibility with detailed explanations, which are then
aggregated for ranking and excluding candidate clinical trials based on
free-text patient notes. We evaluate TrialGPT on three publicly available
cohorts of 184 patients and 18,238 annotated clinical trials. The experimental
results demonstrate several key findings: First, TrialGPT achieves high
criterion-level prediction accuracy with faithful explanations. Second, the
aggregated trial-level TrialGPT scores are highly correlated with expert
eligibility annotations. Third, these scores prove effective in ranking
clinical trials and exclude ineligible candidates. Our error analysis suggests
that current LLMs still make some mistakes due to limited medical knowledge and
domain-specific context understanding. Nonetheless, we believe the explanatory
capabilities of LLMs are highly valuable. Future research is warranted on how
such AI assistants can be integrated into the routine trial matching workflow
in real-world settings to improve its efficiency.
</p></li>
</ul>

<h3>Title: A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15054">http://arxiv.org/abs/2307.15054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15054] A Geometric Notion of Causal Probing](http://arxiv.org/abs/2307.15054) #large language model</code></li>
<li>Summary: <p>Large language models rely on real-valued representations of text to make
their predictions. These representations contain information learned from the
data that the model has trained on, including knowledge of linguistic
properties and forms of demographic bias, e.g., based on gender. A growing body
of work has considered information about concepts such as these using
orthogonal projections onto subspaces of the representation space. We
contribute to this body of work by proposing a formal definition of intrinsic
information in a subspace of a language model's representation space. We
propose a counterfactual approach that avoids the failure mode of spurious
correlations (Kumar et al., 2022) by treating components in the subspace and
its orthogonal complement independently. We show that our counterfactual notion
of information in a subspace is optimizing by an causal concept subspace.
Furthermore, this intervention allows us to attempt concept controlled
generation by manipulating the value of the conceptual component of a
representation. Empirically, we find that R-LACE (Ravfogel et al., 2022)
returns a one-dimensional subspace containing roughly half of total concept
information under our framework. Our causal controlled intervention shows that,
for at least one model, the subspace returned by R-LACE can be used to
manipulate the concept value of the generated word with precision.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Learned Gridification for Efficient Point Cloud Processing. (arXiv:2307.14354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14354] Learned Gridification for Efficient Point Cloud Processing](http://arxiv.org/abs/2307.14354) #segmentation</code></li>
<li>Summary: <p>Neural operations that rely on neighborhood information are much more
expensive when deployed on point clouds than on grid data due to the irregular
distances between points in a point cloud. In a grid, on the other hand, we can
compute the kernel only once and reuse it for all query positions. As a result,
operations that rely on neighborhood information scale much worse for point
clouds than for grid data, specially for large inputs and large neighborhoods.
</p></li>
</ul>

<p>In this work, we address the scalability issue of point cloud methods by
tackling its root cause: the irregularity of the data. We propose learnable
gridification as the first step in a point cloud processing pipeline to
transform the point cloud into a compact, regular grid. Thanks to
gridification, subsequent layers can use operations defined on regular grids,
e.g., Conv3D, which scale much better than native point cloud methods. We then
extend gridification to point cloud to point cloud tasks, e.g., segmentation,
by adding a learnable de-gridification step at the end of the point cloud
processing pipeline to map the compact, regular grid back to its original point
cloud form. Through theoretical and empirical analysis, we show that gridified
networks scale better in terms of memory and time than networks directly
applied on raw point cloud data, while being able to achieve competitive
results. Our code is publicly available at
https://github.com/computri/gridifier.
</p>

<h3>Title: Human-centric Scene Understanding for 3D Large-scale Scenarios. (arXiv:2307.14392v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14392">http://arxiv.org/abs/2307.14392</a></li>
<li>Code URL: <a href="https://github.com/4dvlab/hucenlife">https://github.com/4dvlab/hucenlife</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14392] Human-centric Scene Understanding for 3D Large-scale Scenarios](http://arxiv.org/abs/2307.14392) #segmentation</code></li>
<li>Summary: <p>Human-centric scene understanding is significant for real-world applications,
but it is extremely challenging due to the existence of diverse human poses and
actions, complex human-environment interactions, severe occlusions in crowds,
etc. In this paper, we present a large-scale multi-modal dataset for
human-centric scene understanding, dubbed HuCenLife, which is collected in
diverse daily-life scenarios with rich and fine-grained annotations. Our
HuCenLife can benefit many 3D perception tasks, such as segmentation,
detection, action recognition, etc., and we also provide benchmarks for these
tasks to facilitate related research. In addition, we design novel modules for
LiDAR-based segmentation and action recognition, which are more applicable for
large-scale human-centric scenarios and achieve state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Self-supervised Few-shot Learning for Semantic Segmentation: An Annotation-free Approach. (arXiv:2307.14446v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14446">http://arxiv.org/abs/2307.14446</a></li>
<li>Code URL: <a href="https://github.com/mindflow-institue/annotation_free_fewshot">https://github.com/mindflow-institue/annotation_free_fewshot</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14446] Self-supervised Few-shot Learning for Semantic Segmentation: An Annotation-free Approach](http://arxiv.org/abs/2307.14446) #segmentation</code></li>
<li>Summary: <p>Few-shot semantic segmentation (FSS) offers immense potential in the field of
medical image analysis, enabling accurate object segmentation with limited
training data. However, existing FSS techniques heavily rely on annotated
semantic classes, rendering them unsuitable for medical images due to the
scarcity of annotations. To address this challenge, multiple contributions are
proposed: First, inspired by spectral decomposition methods, the problem of
image decomposition is reframed as a graph partitioning task. The eigenvectors
of the Laplacian matrix, derived from the feature affinity matrix of
self-supervised networks, are analyzed to estimate the distribution of the
objects of interest from the support images. Secondly, we propose a novel
self-supervised FSS framework that does not rely on any annotation. Instead, it
adaptively estimates the query mask by leveraging the eigenvectors obtained
from the support images. This approach eliminates the need for manual
annotation, making it particularly suitable for medical images with limited
annotated data. Thirdly, to further enhance the decoding of the query image
based on the information provided by the support image, we introduce a
multi-scale large kernel attention module. By selectively emphasizing relevant
features and details, this module improves the segmentation process and
contributes to better object delineation. Evaluations on both natural and
medical image datasets demonstrate the efficiency and effectiveness of our
method. Moreover, the proposed approach is characterized by its generality and
model-agnostic nature, allowing for seamless integration with various deep
architectures. The code is publicly available at
\href{https://github.com/mindflow-institue/annotation_free_fewshot}{\textcolor{magenta}{GitHub}}.
</p></li>
</ul>

<h3>Title: Technical note: ShinyAnimalCV: open-source cloud-based web application for object detection, segmentation, and three-dimensional visualization of animals using computer vision. (arXiv:2307.14487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14487">http://arxiv.org/abs/2307.14487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14487] Technical note: ShinyAnimalCV: open-source cloud-based web application for object detection, segmentation, and three-dimensional visualization of animals using computer vision](http://arxiv.org/abs/2307.14487) #segmentation</code></li>
<li>Summary: <p>Computer vision (CV), a non-intrusive and cost-effective technology, has
furthered the development of precision livestock farming by enabling optimized
decision-making through timely and individualized animal care. The availability
of affordable two- and three-dimensional camera sensors, combined with various
machine learning and deep learning algorithms, has provided a valuable
opportunity to improve livestock production systems. However, despite the
availability of various CV tools in the public domain, applying these tools to
animal data can be challenging, often requiring users to have programming and
data analysis skills, as well as access to computing resources. Moreover, the
rapid expansion of precision livestock farming is creating a growing need to
educate and train animal science students in CV. This presents educators with
the challenge of efficiently demonstrating the complex algorithms involved in
CV. Thus, the objective of this study was to develop ShinyAnimalCV, an
open-source cloud-based web application. This application provides a
user-friendly interface for performing CV tasks, including object segmentation,
detection, three-dimensional surface visualization, and extraction of two- and
three-dimensional morphological features. Nine pre-trained CV models using
top-view animal data are included in the application. ShinyAnimalCV has been
deployed online using cloud computing platforms. The source code of
ShinyAnimalCV is available on GitHub, along with detailed documentation on
training CV models using custom data and deploying ShinyAnimalCV locally to
allow users to fully leverage the capabilities of the application.
ShinyAnimalCV can contribute to CV research and teaching in the animal science
community.
</p></li>
</ul>

<h3>Title: Patterns of Vehicle Lights: Addressing Complexities in Curation and Annotation of Camera-Based Vehicle Light Datasets and Metrics. (arXiv:2307.14521v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14521">http://arxiv.org/abs/2307.14521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14521] Patterns of Vehicle Lights: Addressing Complexities in Curation and Annotation of Camera-Based Vehicle Light Datasets and Metrics](http://arxiv.org/abs/2307.14521) #segmentation</code></li>
<li>Summary: <p>This paper explores the representation of vehicle lights in computer vision
and its implications for various tasks in the field of autonomous driving.
Different specifications for representing vehicle lights, including bounding
boxes, center points, corner points, and segmentation masks, are discussed in
terms of their strengths and weaknesses. Three important tasks in autonomous
driving that can benefit from vehicle light detection are identified: nighttime
vehicle detection, 3D vehicle orientation estimation, and dynamic trajectory
cues. Each task may require a different representation of the light. The
challenges of collecting and annotating large datasets for training data-driven
models are also addressed, leading to introduction of the LISA Vehicle Lights
Dataset and associated Light Visibility Model, which provides light annotations
specifically designed for downstream applications in vehicle detection, intent
and trajectory prediction, and safe path planning. A comparison of existing
vehicle light datasets is provided, highlighting the unique features and
limitations of each dataset. Overall, this paper provides insights into the
representation of vehicle lights and the importance of accurate annotations for
training effective detection models in autonomous driving applications. Our
dataset and model are made available at
https://cvrr.ucsd.edu/vehicle-lights-dataset
</p></li>
</ul>

<h3>Title: GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced Few-Shot Learning in Remote Sensing. (arXiv:2307.14612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14612">http://arxiv.org/abs/2307.14612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14612] GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced Few-Shot Learning in Remote Sensing](http://arxiv.org/abs/2307.14612) #segmentation</code></li>
<li>Summary: <p>Classifying and segmenting patterns from a limited number of examples is a
significant challenge in remote sensing and earth observation due to the
difficulty in acquiring accurately labeled data in large quantities. Previous
studies have shown that meta-learning, which involves episodic training on
query and support sets, is a promising approach. However, there has been little
attention paid to direct fine-tuning techniques. This paper repurposes
contrastive learning as a pre-training method for few-shot learning for
classification and semantic segmentation tasks. Specifically, we introduce a
generator-based contrastive learning framework (GenCo) that pre-trains
backbones and simultaneously explores variants of feature samples. In
fine-tuning, the auxiliary generator can be used to enrich limited labeled data
samples in feature space. We demonstrate the effectiveness of our method in
improving few-shot learning performance on two key remote sensing datasets:
Agriculture-Vision and EuroSAT. Empirically, our approach outperforms purely
supervised training on the nearly 95,000 images in Agriculture-Vision for both
classification and semantic segmentation tasks. Similarly, the proposed
few-shot method achieves better results on the land-cover classification task
on EuroSAT compared to the results obtained from fully supervised model
training on the dataset.
</p></li>
</ul>

<h3>Title: High Dynamic Range Imaging via Visual Attention Modules. (arXiv:2307.14705v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14705">http://arxiv.org/abs/2307.14705</a></li>
<li>Code URL: <a href="https://github.com/alirezaomrani95/hdr-vam">https://github.com/alirezaomrani95/hdr-vam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14705] High Dynamic Range Imaging via Visual Attention Modules](http://arxiv.org/abs/2307.14705) #segmentation</code></li>
<li>Summary: <p>Thanks to High Dynamic Range (HDR) imaging methods, the scope of photography
has seen profound changes recently. To be more specific, such methods try to
reconstruct the lost luminosity of the real world caused by the limitation of
regular cameras from the Low Dynamic Range (LDR) images. Additionally, although
the State-Of-The-Art methods in this topic perform well, they mainly
concentrate on combining different exposures and have less attention to
extracting the informative parts of the images. Thus, this paper aims to
introduce a new model capable of incorporating information from the most
visible areas of each image extracted by a visual attention module (VAM), which
is a result of a segmentation strategy. In particular, the model, based on a
deep learning architecture, utilizes the extracted areas to produce the final
HDR image. The results demonstrate that our method outperformed most of the
State-Of-The-Art algorithms.
</p></li>
</ul>

<h3>Title: vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images. (arXiv:2307.14725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14725">http://arxiv.org/abs/2307.14725</a></li>
<li>Code URL: <a href="https://github.com/mishgon/vox2vec">https://github.com/mishgon/vox2vec</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14725] vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-level Representations in Medical Images](http://arxiv.org/abs/2307.14725) #segmentation</code></li>
<li>Summary: <p>This paper introduces vox2vec - a contrastive method for self-supervised
learning (SSL) of voxel-level representations. vox2vec representations are
modeled by a Feature Pyramid Network (FPN): a voxel representation is a
concatenation of the corresponding feature vectors from different pyramid
levels. The FPN is pre-trained to produce similar representations for the same
voxel in different augmented contexts and distinctive representations for
different voxels. This results in unified multi-scale representations that
capture both global semantics (e.g., body part) and local semantics (e.g.,
different small organs or healthy versus tumor tissue). We use vox2vec to
pre-train a FPN on more than 6500 publicly available computed tomography
images. We evaluate the pre-trained representations by attaching simple heads
on top of them and training the resulting models for 22 segmentation tasks. We
show that vox2vec outperforms existing medical imaging SSL techniques in three
evaluation setups: linear and non-linear probing and end-to-end fine-tuning.
Moreover, a non-linear head trained on top of the frozen vox2vec
representations achieves competitive performance with the FPN trained from
scratch while having 50 times fewer trainable parameters. The code is available
at https://github.com/mishgon/vox2vec .
</p></li>
</ul>

<h3>Title: Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning. (arXiv:2307.14786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.14786">http://arxiv.org/abs/2307.14786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.14786] Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning](http://arxiv.org/abs/2307.14786) #segmentation</code></li>
<li>Summary: <p>Depth-aware panoptic segmentation is an emerging topic in computer vision
which combines semantic and geometric understanding for more robust scene
interpretation. Recent works pursue unified frameworks to tackle this challenge
but mostly still treat it as two individual learning tasks, which limits their
potential for exploring cross-domain information. We propose a deeply unified
framework for depth-aware panoptic segmentation, which performs joint
segmentation and depth estimation both in a per-segment manner with identical
object queries. To narrow the gap between the two tasks, we further design a
geometric query enhancement method, which is able to integrate scene geometry
into object queries using latent representations. In addition, we propose a
bi-directional guidance learning approach to facilitate cross-task feature
learning by taking advantage of their mutual relations. Our method sets the new
state of the art for depth-aware panoptic segmentation on both Cityscapes-DVPS
and SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shown
to deliver performance improvement even under incomplete supervision labels.
</p></li>
</ul>

<h3>Title: Adaptive Segmentation Network for Scene Text Detection. (arXiv:2307.15029v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15029">http://arxiv.org/abs/2307.15029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15029] Adaptive Segmentation Network for Scene Text Detection](http://arxiv.org/abs/2307.15029) #segmentation</code></li>
<li>Summary: <p>Inspired by deep convolution segmentation algorithms, scene text detectors
break the performance ceiling of datasets steadily. However, these methods
often encounter threshold selection bottlenecks and have poor performance on
text instances with extreme aspect ratios. In this paper, we propose to
automatically learn the discriminate segmentation threshold, which
distinguishes text pixels from background pixels for segmentation-based scene
text detectors and then further reduces the time-consuming manual parameter
adjustment. Besides, we design a Global-information Enhanced Feature Pyramid
Network (GE-FPN) for capturing text instances with macro size and extreme
aspect ratios. Following the GE-FPN, we introduce a cascade optimization
structure to further refine the text instances. Finally, together with the
proposed threshold learning strategy and text detection structure, we design an
Adaptive Segmentation Network (ASNet) for scene text detection. Extensive
experiments are carried out to demonstrate that the proposed ASNet can achieve
the state-of-the-art performance on four text detection benchmarks, i.e., ICDAR
2015, MSRA-TD500, ICDAR 2017 MLT and CTW1500. The ablation experiments also
verify the effectiveness of our contributions.
</p></li>
</ul>

<h3>Title: To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation. (arXiv:2307.15063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15063">http://arxiv.org/abs/2307.15063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15063] To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation](http://arxiv.org/abs/2307.15063) #segmentation</code></li>
<li>Summary: <p>The goal of Online Domain Adaptation for semantic segmentation is to handle
unforeseeable domain changes that occur during deployment, like sudden weather
events. However, the high computational costs associated with brute-force
adaptation make this paradigm unfeasible for real-world applications. In this
paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training
framework for real-time domain adaptation. Our approach includes a
hardware-aware back-propagation orchestration agent (HAMT) and a dedicated
domain-shift detector that enables active control over when and how the model
is adapted (LT). Thanks to these advancements, our approach is capable of
performing semantic segmentation while simultaneously adapting at more than
29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and
speed trade-off is demonstrated on OnDA and SHIFT benchmarks through
experimental results.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
