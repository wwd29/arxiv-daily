<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Towards an Accurate and Secure Detector against Adversarial Perturbations. (arXiv:2305.10856v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10856">http://arxiv.org/abs/2305.10856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10856] Towards an Accurate and Secure Detector against Adversarial Perturbations](http://arxiv.org/abs/2305.10856) #secure</code></li>
<li>Summary: <p>The vulnerability of deep neural networks to adversarial perturbations has
been widely perceived in the computer vision community. From a security
perspective, it poses a critical risk for modern vision systems, e.g., the
popular Deep Learning as a Service (DLaaS) frameworks. For protecting
off-the-shelf deep models while not modifying them, current algorithms
typically detect adversarial patterns through discriminative decomposition of
natural-artificial data. However, these decompositions are biased towards
frequency or spatial discriminability, thus failing to capture subtle
adversarial patterns comprehensively. More seriously, they are typically
invertible, meaning successful defense-aware (secondary) adversarial attack
(i.e., evading the detector as well as fooling the model) is practical under
the assumption that the adversary is fully aware of the detector (i.e., the
Kerckhoffs's principle). Motivated by such facts, we propose an accurate and
secure adversarial example detector, relying on a spatial-frequency
discriminative decomposition with secret keys. It expands the above works on
two aspects: 1) the introduced Krawtchouk basis provides better
spatial-frequency discriminability and thereby is more suitable for capturing
adversarial patterns than the common trigonometric or wavelet basis; 2) the
extensive parameters for decomposition are generated by a pseudo-random
function with secret keys, hence blocking the defense-aware adversarial attack.
Theoretical and numerical analysis demonstrates the increased accuracy and
security of our detector w.r.t. a number of state-of-the-art algorithms.
</p></li>
</ul>

<h3>Title: Amplification by Shuffling without Shuffling. (arXiv:2305.10867v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10867">http://arxiv.org/abs/2305.10867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10867] Amplification by Shuffling without Shuffling](http://arxiv.org/abs/2305.10867) #secure</code></li>
<li>Summary: <p>Motivated by recent developments in the shuffle model of differential
privacy, we propose a new approximate shuffling functionality called
Alternating Shuffle, and provide a protocol implementing alternating shuffling
in a single-server threat model where the adversary observes all communication.
Unlike previous shuffling protocols in this threat model, the per-client
communication of our protocol only grows sub-linearly in the number of clients.
Moreover, we study the concrete efficiency of our protocol and show it can
improve per-client communication by one or more orders of magnitude with
respect to previous (approximate) shuffling protocols. We also show a
differential privacy amplification result for alternating shuffling analogous
to the one for uniform shuffling, and demonstrate that shuffling-based
protocols for secure summation based a construction of Ishai et al. (FOCS'06)
remain secure under the Alternating Shuffle. In the process we also develop a
protocol for exact shuffling in single-server threat model with amortized
logarithmic communication per-client which might be of independent interest.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10445">http://arxiv.org/abs/2305.10445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10445] Memorization for Good: Encryption with Autoregressive Language Models](http://arxiv.org/abs/2305.10445) #security</code></li>
<li>Summary: <p>Over-parameterized neural language models (LMs) can memorize and recite long
sequences of training data. While such memorization is normally associated with
undesired properties such as overfitting and information leaking, our work
casts memorization as an unexplored capability of LMs. We propose the first
symmetric encryption algorithm with autoregressive language models (SELM). We
show that autoregressive LMs can encode arbitrary data into a compact
real-valued vector (i.e., encryption) and then losslessly decode the vector to
the original message (i.e., decryption) via random subspace optimization and
greedy decoding. While SELM is not amenable to conventional cryptanalysis, we
investigate its security through a novel empirical variant of the classic
IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and
datasets are available at https://github.com/OSU-NLP-Group/SELM.
</p></li>
</ul>

<h3>Title: MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10668">http://arxiv.org/abs/2305.10668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10668] MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection](http://arxiv.org/abs/2305.10668) #security</code></li>
<li>Summary: <p>Graph anomaly detection has long been an important problem in various domains
pertaining to information security such as financial fraud, social spam,
network intrusion, etc. The majority of existing methods are performed in an
unsupervised manner, as labeled anomalies in a large scale are often too
expensive to acquire. However, the identified anomalies may turn out to be data
noises or uninteresting data instances due to the lack of prior knowledge on
the anomalies. In realistic scenarios, it is often feasible to obtain limited
labeled anomalies, which have great potential to advance graph anomaly
detection. However, the work exploring limited labeled anomalies and a large
amount of unlabeled nodes in graphs to detect anomalies is rather limited.
Therefore, in this paper, we study a novel problem of few-shot graph anomaly
detection. We propose a new framework MetaGAD to learn to meta-transfer the
knowledge between unlabeled and labeled nodes for graph anomaly detection.
Experimental results on six real-world datasets with synthetic anomalies and
"organic" anomalies (available in the dataset) demonstrate the effectiveness of
the proposed approach in detecting anomalies with limited labeled anomalies.
</p></li>
</ul>

<h3>Title: GraphMoco:a Graph Momentum Contrast Model that Using Multimodel Structure Information for Large-scale Binary Function Representation Learning. (arXiv:2305.10826v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10826">http://arxiv.org/abs/2305.10826</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10826] GraphMoco:a Graph Momentum Contrast Model that Using Multimodel Structure Information for Large-scale Binary Function Representation Learning](http://arxiv.org/abs/2305.10826) #security</code></li>
<li>Summary: <p>The ability to compute similarity scores of binary code at the function level
is essential for cyber security. A single binary file can contain tens of
thousands of functions. A deployable learning framework for cybersecurity
applications needs to work not only accurately but also efficiently with large
amounts of data. Traditional methods suffer from two drawbacks. First, it is
very difficult to annotate different pairs of functions with accurate labels.
These supervised learning methods can easily be overtrained with inaccurate
labels. The second is that they either use the pre-trained encoder or use the
fine-grained graph comparison. However, these methods have shortcomings in
terms of time or memory consumption. We focus on large-scale Binary Code
Similarity Detection (BCSD) and to mitigate the traditional problems, we
propose GraphMoco: a graph momentum contrast model that uses multimodal
structure information for large-scale binary function representation learning.
We take an unsupervised learning approach and make full use of the structural
information in the binary code. It does not require manually labelled similar
or dissimilar information. Our models perform efficiently on large amounts of
training data. Our experimental results show that our method outperforms the
state-of-the-art in terms of accuracy.
</p></li>
</ul>

<h3>Title: Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation. (arXiv:2305.11039v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11039">http://arxiv.org/abs/2305.11039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11039] Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation](http://arxiv.org/abs/2305.11039) #security</code></li>
<li>Summary: <p>Recent advancements in artificial intelligence (AI) and machine learning (ML)
algorithms, coupled with the availability of faster computing infrastructure,
have enhanced the security posture of cybersecurity operations centers
(defenders) through the development of ML-aided network intrusion detection
systems (NIDS). Concurrently, the abilities of adversaries to evade security
have also increased with the support of AI/ML models. Therefore, defenders need
to proactively prepare for evasion attacks that exploit the detection
mechanisms of NIDS. Recent studies have found that the perturbation of
flow-based and packet-based features can deceive ML models, but these
approaches have limitations. Perturbations made to the flow-based features are
difficult to reverse-engineer, while samples generated with perturbations to
the packet-based features are not playable.
</p></li>
</ul>

<p>Our methodological framework, Deep PackGen, employs deep reinforcement
learning to generate adversarial packets and aims to overcome the limitations
of approaches in the literature. By taking raw malicious network packets as
inputs and systematically making perturbations on them, Deep PackGen
camouflages them as benign packets while still maintaining their functionality.
In our experiments, using publicly available data, Deep PackGen achieved an
average adversarial success rate of 66.4\% against various ML models and across
different attack types. Our investigation also revealed that more than 45\% of
the successful adversarial samples were out-of-distribution packets that evaded
the decision boundaries of the classifiers. The knowledge gained from our study
on the adversary's ability to make specific evasive perturbations to different
types of malicious packets can help defenders enhance the robustness of their
NIDS against evolving adversarial attacks.
</p>

<h2>privacy</h2>
<h3>Title: INCLG: Inpainting for Non-Cleft Lip Generation with a Multi-Task Image Processing Network. (arXiv:2305.10589v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10589">http://arxiv.org/abs/2305.10589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10589] INCLG: Inpainting for Non-Cleft Lip Generation with a Multi-Task Image Processing Network](http://arxiv.org/abs/2305.10589) #privacy</code></li>
<li>Summary: <p>We present a software that predicts non-cleft facial images for patients with
cleft lip, thereby facilitating the understanding, awareness and discussion of
cleft lip surgeries. To protect patients privacy, we design a software
framework using image inpainting, which does not require cleft lip images for
training, thereby mitigating the risk of model leakage. We implement a novel
multi-task architecture that predicts both the non-cleft facial image and
facial landmarks, resulting in better performance as evaluated by surgeons. The
software is implemented with PyTorch and is usable with consumer-level color
images with a fast prediction speed, enabling effective deployment.
</p></li>
</ul>

<h3>Title: Learning Differentially Private Probabilistic Models for Privacy-Preserving Image Generation. (arXiv:2305.10662v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10662">http://arxiv.org/abs/2305.10662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10662] Learning Differentially Private Probabilistic Models for Privacy-Preserving Image Generation](http://arxiv.org/abs/2305.10662) #privacy</code></li>
<li>Summary: <p>A number of deep models trained on high-quality and valuable images have been
deployed in practical applications, which may pose a leakage risk of data
privacy. Learning differentially private generative models can sidestep this
challenge through indirect data access. However, such differentially private
generative models learned by existing approaches can only generate images with
a low-resolution of less than 128x128, hindering the widespread usage of
generated images in downstream training. In this work, we propose learning
differentially private probabilistic models (DPPM) to generate high-resolution
images with differential privacy guarantee. In particular, we first train a
model to fit the distribution of the training data and make it satisfy
differential privacy by performing a randomized response mechanism during
training process. Then we perform Hamiltonian dynamics sampling along with the
differentially private movement direction predicted by the trained
probabilistic model to obtain the privacy-preserving images. In this way, it is
possible to apply these images to different downstream tasks while protecting
private information. Notably, compared to other state-of-the-art differentially
private generative approaches, our approach can generate images up to 256x256
with remarkable visual quality and data utility. Extensive experiments show the
effectiveness of our approach.
</p></li>
</ul>

<h3>Title: Free Lunch for Privacy Preserving Distributed Graph Learning. (arXiv:2305.10869v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10869">http://arxiv.org/abs/2305.10869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10869] Free Lunch for Privacy Preserving Distributed Graph Learning](http://arxiv.org/abs/2305.10869) #privacy</code></li>
<li>Summary: <p>Learning on graphs is becoming prevalent in a wide range of applications
including social networks, robotics, communication, medicine, etc. These
datasets belonging to entities often contain critical private information. The
utilization of data for graph learning applications is hampered by the growing
privacy concerns from users on data sharing. Existing privacy-preserving
methods pre-process the data to extract user-side features, and only these
features are used for subsequent learning. Unfortunately, these methods are
vulnerable to adversarial attacks to infer private attributes. We present a
novel privacy-respecting framework for distributed graph learning and
graph-based machine learning. In order to perform graph learning and other
downstream tasks on the server side, this framework aims to learn features as
well as distances without requiring actual features while preserving the
original structural properties of the raw data. The proposed framework is quite
generic and highly adaptable. We demonstrate the utility of the Euclidean
space, but it can be applied with any existing method of distance approximation
and graph learning for the relevant spaces. Through extensive experimentation
on both synthetic and real datasets, we demonstrate the efficacy of the
framework in terms of comparing the results obtained without data sharing to
those obtained with data sharing as a benchmark. This is, to our knowledge, the
first privacy-preserving distributed graph learning framework.
</p></li>
</ul>

<h3>Title: Understanding how Differentially Private Generative Models Spend their Privacy Budget. (arXiv:2305.10994v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10994">http://arxiv.org/abs/2305.10994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10994] Understanding how Differentially Private Generative Models Spend their Privacy Budget](http://arxiv.org/abs/2305.10994) #privacy</code></li>
<li>Summary: <p>Generative models trained with Differential Privacy (DP) are increasingly
used to produce synthetic data while reducing privacy risks. Navigating their
specific privacy-utility tradeoffs makes it challenging to determine which
models would work best for specific settings/tasks. In this paper, we fill this
gap in the context of tabular data by analyzing how DP generative models
distribute privacy budgets across rows and columns, arguably the main source of
utility degradation. We examine the main factors contributing to how privacy
budgets are spent, including underlying modeling techniques, DP mechanisms, and
data dimensionality.
</p></li>
</ul>

<p>Our extensive evaluation of both graphical and deep generative models sheds
light on the distinctive features that render them suitable for different
settings and tasks. We show that graphical models distribute the privacy budget
horizontally and thus cannot handle relatively wide datasets while the
performance on the task they were optimized for monotonically increases with
more data. Deep generative models spend their budget per iteration, so their
behavior is less predictable with varying dataset dimensions but could perform
better if trained on more features. Also, low levels of privacy
($\epsilon\geq100$) could help some models generalize, achieving better results
than without applying DP.
</p>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: How Deep Learning Sees the World: A Survey on Adversarial Attacks &amp; Defenses. (arXiv:2305.10862v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10862">http://arxiv.org/abs/2305.10862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10862] How Deep Learning Sees the World: A Survey on Adversarial Attacks &amp; Defenses](http://arxiv.org/abs/2305.10862) #defense</code></li>
<li>Summary: <p>Deep Learning is currently used to perform multiple tasks, such as object
recognition, face recognition, and natural language processing. However, Deep
Neural Networks (DNNs) are vulnerable to perturbations that alter the network
prediction (adversarial examples), raising concerns regarding its usage in
critical areas, such as self-driving vehicles, malware detection, and
healthcare. This paper compiles the most recent adversarial attacks, grouped by
the attacker capacity, and modern defenses clustered by protection strategies.
We also present the new advances regarding Vision Transformers, summarize the
datasets and metrics used in the context of adversarial settings, and compare
the state-of-the-art results under different attacks, finishing with the
identification of open issues.
</p></li>
</ul>

<h3>Title: Architecture-agnostic Iterative Black-box Certified Defense against Adversarial Patches. (arXiv:2305.10929v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10929">http://arxiv.org/abs/2305.10929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10929] Architecture-agnostic Iterative Black-box Certified Defense against Adversarial Patches](http://arxiv.org/abs/2305.10929) #defense</code></li>
<li>Summary: <p>The adversarial patch attack aims to fool image classifiers within a bounded,
contiguous region of arbitrary changes, posing a real threat to computer vision
systems (e.g., autonomous driving, content moderation, biometric
authentication, medical imaging) in the physical world. To address this problem
in a trustworthy way, proposals have been made for certified patch defenses
that ensure the robustness of classification models and prevent future patch
attacks from breaching the defense. State-of-the-art certified defenses can be
compatible with any model architecture, as well as achieve high clean and
certified accuracy. Although the methods are adaptive to arbitrary patch
positions, they inevitably need to access the size of the adversarial patch,
which is unreasonable and impractical in real-world attack scenarios. To
improve the feasibility of the architecture-agnostic certified defense in a
black-box setting (i.e., position and size of the patch are both unknown), we
propose a novel two-stage Iterative Black-box Certified Defense method, termed
IBCD.In the first stage, it estimates the patch size in a search-based manner
by evaluating the size relationship between the patch and mask with pixel
masking. In the second stage, the accuracy results are calculated by the
existing white-box certified defense methods with the estimated patch size. The
experiments conducted on two popular model architectures and two datasets
verify the effectiveness and efficiency of IBCD.
</p></li>
</ul>

<h3>Title: In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10947">http://arxiv.org/abs/2305.10947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10947] In Defense of Pure 16-bit Floating-Point Neural Networks](http://arxiv.org/abs/2305.10947) #defense</code></li>
<li>Summary: <p>Reducing the number of bits needed to encode the weights and activations of
neural networks is highly desirable as it speeds up their training and
inference time while reducing memory consumption. For these reasons, research
in this area has attracted significant attention toward developing neural
networks that leverage lower-precision computing, such as mixed-precision
training. Interestingly, none of the existing approaches has investigated pure
16-bit floating-point settings. In this paper, we shed light on the overlooked
efficiency of pure 16-bit floating-point neural networks. As such, we provide a
comprehensive theoretical analysis to investigate the factors contributing to
the differences observed between 16-bit and 32-bit models. We formalize the
concepts of floating-point error and tolerance, enabling us to quantitatively
explain the conditions under which a 16-bit model can closely approximate the
results of its 32-bit counterpart. This theoretical exploration offers
perspective that is distinct from the literature which attributes the success
of low-precision neural networks to its regularization effect. This in-depth
analysis is supported by an extensive series of experiments. Our findings
demonstrate that pure 16-bit floating-point neural networks can achieve similar
or even better performance than their mixed-precision and 32-bit counterparts.
We believe the results presented in this paper will have significant
implications for machine learning practitioners, offering an opportunity to
reconsider using pure 16-bit networks in various applications.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Content-based Unrestricted Adversarial Attack. (arXiv:2305.10665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10665">http://arxiv.org/abs/2305.10665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10665] Content-based Unrestricted Adversarial Attack](http://arxiv.org/abs/2305.10665) #attack</code></li>
<li>Summary: <p>Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.
</p></li>
</ul>

<h3>Title: Re-thinking Data Availablity Attacks Against Deep Neural Networks. (arXiv:2305.10691v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10691">http://arxiv.org/abs/2305.10691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10691] Re-thinking Data Availablity Attacks Against Deep Neural Networks](http://arxiv.org/abs/2305.10691) #attack</code></li>
<li>Summary: <p>The unauthorized use of personal data for commercial purposes and the
clandestine acquisition of private data for training machine learning models
continue to raise concerns. In response to these issues, researchers have
proposed availability attacks that aim to render data unexploitable. However,
many current attack methods are rendered ineffective by adversarial training.
In this paper, we re-examine the concept of unlearnable examples and discern
that the existing robust error-minimizing noise presents an inaccurate
optimization objective. Building on these observations, we introduce a novel
optimization paradigm that yields improved protection results with reduced
computational time requirements. We have conducted extensive experiments to
substantiate the soundness of our approach. Moreover, our method establishes a
robust foundation for future research in this area.
</p></li>
</ul>

<h3>Title: Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization. (arXiv:2305.10701v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10701">http://arxiv.org/abs/2305.10701</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10701] Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization](http://arxiv.org/abs/2305.10701) #attack</code></li>
<li>Summary: <p>Although recent personalization methods have democratized high-resolution
image synthesis by enabling swift concept acquisition with minimal examples and
lightweight computation, they also present an exploitable avenue for high
accessible backdoor attacks. This paper investigates a critical and unexplored
aspect of text-to-image (T2I) diffusion models - their potential vulnerability
to backdoor attacks via personalization. Our study focuses on a zero-day
backdoor vulnerability prevalent in two families of personalization methods,
epitomized by Textual Inversion and DreamBooth.Compared to traditional backdoor
attacks, our proposed method can facilitate more precise, efficient, and easily
accessible attacks with a lower barrier to entry. We provide a comprehensive
review of personalization in T2I diffusion models, highlighting the operation
and exploitation potential of this backdoor vulnerability. To be specific, by
studying the prompt processing of Textual Inversion and DreamBooth, we have
devised dedicated backdoor attacks according to the different ways of dealing
with unseen tokens and analyzed the influence of triggers and concept images on
the attack effect. Our empirical study has shown that the nouveau-token
backdoor attack has better attack performance while legacy-token backdoor
attack is potentially harder to defend.
</p></li>
</ul>

<h3>Title: Measurement Based Evaluation and Mitigation of Flood Attacks on a LAN Test-Bed. (arXiv:2305.10565v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10565">http://arxiv.org/abs/2305.10565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10565] Measurement Based Evaluation and Mitigation of Flood Attacks on a LAN Test-Bed](http://arxiv.org/abs/2305.10565) #attack</code></li>
<li>Summary: <p>The IoT's vulnerability to network attacks has motivated the design of
intrusion detection schemes (IDS) using Machine Learning (ML), with a low
computational cost for online detection but intensive offline learning. Such
IDS can have high attack detection accuracy and are easily installed on servers
that communicate with IoT devices. However, they are seldom evaluated in
realistic operational conditions where IDS processing may be held up by the
system overload created by attacks. Thus we first present an experimental study
of UDP Flood Attacks on a Local Area Network Test-Bed, where the first line of
defence is an accurate IDS using an Auto-Associative Dense Random Neural
Network. The experiments reveal that during severe attacks, the packet and
protocol management software overloads the multi-core server, and paralyses IDS
detection. We therefore propose and experimentally evaluate an IDS design where
decisions are made from a very small number of incoming packets, so that
attacking traffic is dropped within milli-seconds after an attack begins and
the paralysing effect of congestion is avoided.
</p></li>
</ul>

<h3>Title: Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks. (arXiv:2305.10596v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10596">http://arxiv.org/abs/2305.10596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10596] Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks](http://arxiv.org/abs/2305.10596) #attack</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have made tremendous progress in the past ten
years and have been applied in various critical applications. However, recent
studies have shown that deep neural networks are vulnerable to backdoor
attacks. By injecting malicious data into the training set, an adversary can
plant the backdoor into the original model. The backdoor can remain hidden
indefinitely until activated by a sample with a specific trigger, which is
hugely concealed, bringing serious security risks to critical applications.
However, one main limitation of current backdoor attacks is that the trigger is
often visible to human perception. Therefore, it is crucial to study the
stealthiness of backdoor triggers. In this paper, we propose a novel
frequency-domain backdooring technique. In particular, our method aims to add a
backdoor trigger in the frequency domain of original images via Discrete
Fourier Transform, thus hidding the trigger. We evaluate our method on three
benchmark datasets: MNIST, CIFAR-10 and Imagenette. Our experiments show that
we can simultaneously fool human inspection and DNN models. We further apply
two image similarity evaluation metrics to illustrate that our method adds the
most subtle perturbation without compromising attack success rate and clean
sample accuracy.
</p></li>
</ul>

<h3>Title: Black-Box Targeted Reward Poisoning Attack Against Online Deep Reinforcement Learning. (arXiv:2305.10681v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10681">http://arxiv.org/abs/2305.10681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10681] Black-Box Targeted Reward Poisoning Attack Against Online Deep Reinforcement Learning](http://arxiv.org/abs/2305.10681) #attack</code></li>
<li>Summary: <p>We propose the first black-box targeted attack against online deep
reinforcement learning through reward poisoning during training time. Our
attack is applicable to general environments with unknown dynamics learned by
unknown algorithms and requires limited attack budgets and computational
resources. We leverage a general framework and find conditions to ensure
efficient attack under a general assumption of the learning algorithms. We show
that our attack is optimal in our framework under the conditions. We
experimentally verify that with limited budgets, our attack efficiently leads
the learning agent to various target policies under a diverse set of popular
DRL environments and state-of-the-art learners.
</p></li>
</ul>

<h3>Title: BrutePrint: Expose Smartphone Fingerprint Authentication to Brute-force Attack. (arXiv:2305.10791v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10791">http://arxiv.org/abs/2305.10791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10791] BrutePrint: Expose Smartphone Fingerprint Authentication to Brute-force Attack](http://arxiv.org/abs/2305.10791) #attack</code></li>
<li>Summary: <p>Fingerprint authentication has been widely adopted on smartphones to
complement traditional password authentication, making it a tempting target for
attackers. The smartphone industry is fully aware of existing threats, and
especially for the presentation attack studied by most prior works, the threats
are nearly eliminated by liveness detection and attempt limit. In this paper,
we study the seemingly impossible fingerprint brute-force attack on
off-the-shelf smartphones and propose a generic attack framework. We implement
BrutePrint to automate the attack, that acts as a middleman to bypass attempt
limit and hijack fingerprint images. Specifically, the bypassing exploits two
zero-day vulnerabilities in smartphone fingerprint authentication (SFA)
framework, and the hijacking leverages the simplicity of SPI protocol.
Moreover, we consider a practical cross-device attack scenario and tackle the
liveness and matching problems with neural style transfer (NST). We also
propose a method based on neural style transfer to generate valid brute-forcing
inputs from arbitrary fingerprint images. A case study shows that we always
bypasses liveness detection and attempt limit while 71% spoofs are accepted. We
evaluate BrutePrint on 10 representative smartphones from top-5 vendors and 3
typical types of applications involving screen lock, payment, and privacy. As
all of them are vulnerable to some extent, fingerprint brute-force attack is
validated on on all devices except iPhone, where the shortest time to unlock
the smartphone without prior knowledge about the victim is estimated at 40
minutes. Furthermore, we suggest software and hardware mitigation measures.
</p></li>
</ul>

<h3>Title: Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10506">http://arxiv.org/abs/2305.10506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10506] Exact Recovery for System Identification with More Corrupt Data than Clean Data](http://arxiv.org/abs/2305.10506) #attack</code></li>
<li>Summary: <p>In this paper, we study the system identification problem for linear
discrete-time systems under adversaries and analyze two lasso-type estimators.
We study both asymptotic and non-asymptotic properties of these estimators in
two separate scenarios, corresponding to deterministic and stochastic models
for the attack times. Since the samples collected from the system are
correlated, the existing results on lasso are not applicable. We show that when
the system is stable and the attacks are injected periodically, the sample
complexity for the exact recovery of the system dynamics is O(n), where n is
the dimension of the states. When the adversarial attacks occur at each time
instance with probability p, the required sample complexity for the exact
recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure
convergence to the true system dynamics under the asymptotic regime. As a
by-product, even when more than half of the data is compromised, our estimators
still learn the system correctly. This paper provides the first mathematical
guarantee in the literature on learning from correlated data for dynamical
systems in the case when there is less clean data than corrupt data.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution. (arXiv:2305.10465v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10465">http://arxiv.org/abs/2305.10465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10465] Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution](http://arxiv.org/abs/2305.10465) #robust</code></li>
<li>Summary: <p>Estimating the 3DoF rotation from a single RGB image is an important yet
challenging problem. As a popular approach, probabilistic rotation modeling
additionally carries prediction uncertainty information, compared to
single-prediction rotation regression. For modeling probabilistic distribution
over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix
Fisher, however they are shown to be sensitive to outlier predictions, e.g.
$180^\circ$ error and thus are unlikely to converge with optimal performance.
In this paper, we draw inspiration from multivariate Laplace distribution and
propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace
distribution is robust to the disturbance of outliers and enforces much
gradient to the low-error region that it can improve. In addition, we show that
our method also exhibits robustness to small noises and thus tolerates
imperfect annotations. With this benefit, we demonstrate its advantages in
semi-supervised rotation regression, where the pseudo labels are noisy. To
further capture the multi-modal rotation solution space for symmetric objects,
we extend our distribution to rotation Laplace mixture model and demonstrate
its effectiveness. Our extensive experiments show that our proposed
distribution and the mixture model achieve state-of-the-art performance in all
the rotation regression experiments over both probabilistic and
non-probabilistic baselines.
</p></li>
</ul>

<h3>Title: Manifold-Aware Self-Training for Unsupervised Domain Adaptation on Regressing 6D Object Pose. (arXiv:2305.10808v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10808">http://arxiv.org/abs/2305.10808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10808] Manifold-Aware Self-Training for Unsupervised Domain Adaptation on Regressing 6D Object Pose](http://arxiv.org/abs/2305.10808) #robust</code></li>
<li>Summary: <p>Domain gap between synthetic and real data in visual regression (\eg 6D pose
estimation) is bridged in this paper via global feature alignment and local
refinement on the coarse classification of discretized anchor classes in target
space, which imposes a piece-wise target manifold regularization into
domain-invariant representation learning. Specifically, our method incorporates
an explicit self-supervised manifold regularization, revealing consistent
cumulative target dependency across domains, to a self-training scheme (\eg the
popular Self-Paced Self-Training) to encourage more discriminative transferable
representations of regression tasks. Moreover, learning unified implicit neural
functions to estimate relative direction and distance of targets to their
nearest class bins aims to refine target classification predictions, which can
gain robust performance against inconsistent feature scaling sensitive to UDA
regressors. Experiment results on three public benchmarks of the challenging 6D
pose estimation task can verify the effectiveness of our method, consistently
achieving superior performance to the state-of-the-art for UDA on 6D pose
estimation.
</p></li>
</ul>

<h3>Title: Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding. (arXiv:2305.10448v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10448">http://arxiv.org/abs/2305.10448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10448] Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding](http://arxiv.org/abs/2305.10448) #robust</code></li>
<li>Summary: <p>This paper presents GenDoc, a general sequence-to-sequence document
understanding model pre-trained with unified masking across three modalities:
text, image, and layout. The proposed model utilizes an encoder-decoder
architecture, which allows for increased adaptability to a wide range of
downstream tasks with diverse output formats, in contrast to the encoder-only
models commonly employed in document understanding. In addition to the
traditional text infilling task used in previous encoder-decoder models, our
pre-training extends to include tasks of masked image token prediction and
masked layout prediction. We also design modality-specific instruction and
adopt both disentangled attention and the mixture-of-modality-experts strategy
to effectively capture the information leveraged by each modality. Evaluation
of the proposed model through extensive experiments on several downstream tasks
in document understanding demonstrates its ability to achieve superior or
competitive performance compared to state-of-the-art approaches. Our analysis
further suggests that GenDoc is more robust than the encoder-only models in
scenarios where the OCR quality is imperfect.
</p></li>
</ul>

<h3>Title: Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10713">http://arxiv.org/abs/2305.10713</a></li>
<li>Code URL: <a href="https://github.com/shadowkiller33/flatness">https://github.com/shadowkiller33/flatness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10713] Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency](http://arxiv.org/abs/2305.10713) #robust</code></li>
<li>Summary: <p>With growing capabilities of large language models, prompting them has become
the dominant way to access them. This has motivated the development of
strategies for automatically selecting effective language prompts. In this
paper, we introduce prompt flatness, a new metric to quantify the expected
utility of a language prompt. This metric is inspired by flatness
regularization in statistical learning that quantifies the robustness of the
model towards its parameter perturbations. We provide theoretical foundations
for this metric and its relationship with other prompt selection metrics,
providing a comprehensive understanding of existing methods. Empirically, we
show that combining prompt flatness with existing metrics improves both
performance and sample efficiency. Our metric outperforms the previous prompt
selection metrics with an average increase of 5% in accuracy and 10% in Pearson
correlation across 6 classification benchmarks.
</p></li>
</ul>

<h3>Title: Model-Free Robust Average-Reward Reinforcement Learning. (arXiv:2305.10504v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10504">http://arxiv.org/abs/2305.10504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10504] Model-Free Robust Average-Reward Reinforcement Learning](http://arxiv.org/abs/2305.10504) #robust</code></li>
<li>Summary: <p>Robust Markov decision processes (MDPs) address the challenge of model
uncertainty by optimizing the worst-case performance over an uncertainty set of
MDPs. In this paper, we focus on the robust average-reward MDPs under the
model-free setting. We first theoretically characterize the structure of
solutions to the robust average-reward Bellman equation, which is essential for
our later convergence analysis. We then design two model-free algorithms,
robust relative value iteration (RVI) TD and robust RVI Q-learning, and
theoretically prove their convergence to the optimal solution. We provide
several widely used uncertainty sets as examples, including those defined by
the contamination model, total variation, Chi-squared divergence,
Kullback-Leibler (KL) divergence and Wasserstein distance.
</p></li>
</ul>

<h3>Title: Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10638">http://arxiv.org/abs/2305.10638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10638] Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis](http://arxiv.org/abs/2305.10638) #robust</code></li>
<li>Summary: <p>The task of root cause analysis (RCA) is to identify the root causes of
system faults/failures by analyzing system monitoring data. Efficient RCA can
greatly accelerate system failure recovery and mitigate system damages or
financial losses. However, previous research has mostly focused on developing
offline RCA algorithms, which often require manually initiating the RCA
process, a significant amount of time and data to train a robust model, and
then being retrained from scratch for a new system fault.
</p></li>
</ul>

<p>In this paper, we propose CORAL, a novel online RCA framework that can
automatically trigger the RCA process and incrementally update the RCA model.
CORAL consists of Trigger Point Detection, Incremental Disentangled Causal
Graph Learning, and Network Propagation-based Root Cause Localization. The
Trigger Point Detection component aims to detect system state transitions
automatically and in near-real-time. To achieve this, we develop an online
trigger point detection approach based on multivariate singular spectrum
analysis and cumulative sum statistics. To efficiently update the RCA model, we
propose an incremental disentangled causal graph learning approach to decouple
the state-invariant and state-dependent information. After that, CORAL applies
a random walk with restarts to the updated causal graph to accurately identify
root causes. The online RCA process terminates when the causal graph and the
generated root cause list converge. Extensive experiments on three real-world
datasets with case studies demonstrate the effectiveness and superiority of the
proposed framework.
</p>

<h3>Title: Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs. (arXiv:2305.10673v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10673">http://arxiv.org/abs/2305.10673</a></li>
<li>Code URL: <a href="https://github.com/edisonleeeee/step">https://github.com/edisonleeeee/step</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10673] Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs](http://arxiv.org/abs/2305.10673) #robust</code></li>
<li>Summary: <p>The prevalence of large-scale graphs poses great challenges in time and
storage for training and deploying graph neural networks (GNNs). Several recent
works have explored solutions for pruning the large original graph into a small
and highly-informative one, such that training and inference on the pruned and
large graphs have comparable performance. Although empirically effective,
current researches focus on static or non-temporal graphs, which are not
directly applicable to dynamic scenarios. In addition, they require labels as
ground truth to learn the informative structure, limiting their applicability
to new problem domains where labels are hard to obtain. To solve the dilemma,
we propose and study the problem of unsupervised graph pruning on dynamic
graphs. We approach the problem by our proposed STEP, a self-supervised
temporal pruning framework that learns to remove potentially redundant edges
from input dynamic graphs. From a technical and industrial viewpoint, our
method overcomes the trade-offs between the performance and the time &amp; memory
overheads. Our results on three real-world datasets demonstrate the advantages
on improving the efficacy, robustness, and efficiency of GNNs on dynamic node
classification tasks. Most notably, STEP is able to prune more than 50% of
edges on a million-scale industrial graph Alipay (7M nodes, 21M edges) while
approximating up to 98% of the original performance. Code is available at
https://github.com/EdisonLeeeee/STEP.
</p></li>
</ul>

<h3>Title: Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping. (arXiv:2305.10721v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10721">http://arxiv.org/abs/2305.10721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10721] Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping](http://arxiv.org/abs/2305.10721) #robust</code></li>
<li>Summary: <p>Long-term time series forecasting has gained significant attention in recent
years. While there are various specialized designs for capturing temporal
dependency, previous studies have demonstrated that a single linear layer can
achieve competitive forecasting performance compared to other complex
architectures. In this paper, we thoroughly investigate the intrinsic
effectiveness of recent approaches and make three key observations: 1) linear
mapping is critical to prior long-term time series forecasting efforts; 2)
RevIN (reversible normalization) and CI (Channel Independent) play a vital role
in improving overall forecasting performance; and 3) linear mapping can
effectively capture periodic features in time series and has robustness for
different periods across channels when increasing the input horizon. We provide
theoretical and experimental explanations to support our findings and also
discuss the limitations and future works. Our framework's code is available at
\url{https://github.com/plumprc/RTSF}.
</p></li>
</ul>

<h3>Title: RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (arXiv:2305.10906v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10906">http://arxiv.org/abs/2305.10906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10906] RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search](http://arxiv.org/abs/2305.10906) #robust</code></li>
<li>Summary: <p>The trustworthiness of DNNs is often challenged by their vulnerability to
minor adversarial perturbations, which may not only undermine prediction
accuracy (robustness) but also cause biased predictions for similar inputs
(individual fairness). Accurate fairness has been recently proposed to enforce
a harmonic balance between accuracy and individual fairness. It induces the
notion of fairness confusion matrix to categorize predictions as true fair,
true biased, false fair, and false biased. This paper proposes a harmonic
evaluation approach, RobustFair, for the accurate fairness of DNNs, using
adversarial perturbations crafted through fairness confusion directed gradient
search. By using Taylor expansions to approximate the ground truths of
adversarial instances, RobustFair can particularly identify the robustness
defects entangled for spurious fairness, which are often elusive in robustness
evaluation, and missing in individual fairness evaluation. RobustFair can boost
robustness and individual fairness evaluations by identifying robustness or
fairness defects simultaneously. Empirical case studies on fairness benchmark
datasets show that, compared with the state-of-the-art white-box robustness and
individual fairness testing approaches, RobustFair detects significantly
1.77-11.87 times adversarial perturbations, yielding 1.83-13.12 times biased
and 1.53-8.22 times false instances. The adversarial instances can then be
effectively exploited to improve the accurate fairness (and hence accuracy and
individual fairness) of the original deep neural network through retraining.
The empirical case studies further show that the adversarial instances
identified by RobustFair outperform those identified by the other testing
approaches, in promoting 21% accurate fairness and 19% individual fairness on
multiple sensitive attributes, without losing accuracy at all or even promoting
it by up to 4%.
</p></li>
</ul>

<h3>Title: Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks. (arXiv:2305.10997v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10997">http://arxiv.org/abs/2305.10997</a></li>
<li>Code URL: <a href="https://github.com/dmiu-shell/deeprl-shell">https://github.com/dmiu-shell/deeprl-shell</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10997] Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks](http://arxiv.org/abs/2305.10997) #robust</code></li>
<li>Summary: <p>Lifelong learning agents aim to learn multiple tasks sequentially over a
lifetime. This involves the ability to exploit previous knowledge when learning
new tasks and to avoid forgetting. Modulating masks, a specific type of
parameter isolation approach, have recently shown promise in both supervised
and reinforcement learning. While lifelong learning algorithms have been
investigated mainly within a single-agent approach, a question remains on how
multiple agents can share lifelong learning knowledge with each other. We show
that the parameter isolation mechanism used by modulating masks is particularly
suitable for exchanging knowledge among agents in a distributed and
decentralized system of lifelong learners. The key idea is that the isolation
of specific task knowledge to specific masks allows agents to transfer only
specific knowledge on-demand, resulting in robust and effective distributed
lifelong learning. We assume fully distributed and asynchronous scenarios with
dynamic agent numbers and connectivity. An on-demand communication protocol
ensures agents query their peers for specific masks to be transferred and
integrated into their policies when facing each task. Experiments indicate that
on-demand mask communication is an effective way to implement distributed
lifelong reinforcement learning and provides a lifelong learning benefit with
respect to distributed RL baselines such as DD-PPO, IMPALA, and PPO+EWC. The
system is particularly robust to connection drops and demonstrates rapid
learning due to knowledge exchange.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Scribble-Supervised Target Extraction Method Based on Inner Structure-Constraint for Remote Sensing Images. (arXiv:2305.10661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10661">http://arxiv.org/abs/2305.10661</a></li>
<li>Code URL: <a href="https://github.com/yitongli123/isc-te">https://github.com/yitongli123/isc-te</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10661] Scribble-Supervised Target Extraction Method Based on Inner Structure-Constraint for Remote Sensing Images](http://arxiv.org/abs/2305.10661) #extraction</code></li>
<li>Summary: <p>Weakly supervised learning based on scribble annotations in target extraction
of remote sensing images has drawn much interest due to scribbles' flexibility
in denoting winding objects and low cost of manually labeling. However,
scribbles are too sparse to identify object structure and detailed information,
bringing great challenges in target localization and boundary description. To
alleviate these problems, in this paper, we construct two inner
structure-constraints, a deformation consistency loss and a trainable active
contour loss, together with a scribble-constraint to supervise the optimization
of the encoder-decoder network without introducing any auxiliary module or
extra operation based on prior cues. Comprehensive experiments demonstrate our
method's superiority over five state-of-the-art algorithms in this field.
Source code is available at https://github.com/yitongli123/ISC-TE.
</p></li>
</ul>

<h3>Title: Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search. (arXiv:2305.10561v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10561">http://arxiv.org/abs/2305.10561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10561] Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search](http://arxiv.org/abs/2305.10561) #extraction</code></li>
<li>Summary: <p>In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual,
zero-shot event extraction system and accompanying user interface for event
visualization &amp; search. Using only English training data, ISI-Clear makes
global events available on-demand, processing user-supplied text in 100
languages ranging from Afrikaans to Yiddish. We provide multiple event-centric
views of extracted events, including both a graphical representation and a
document-level summary. We also integrate existing cross-lingual search
algorithms with event extraction capabilities to provide cross-lingual
event-centric search, allowing English-speaking users to search over events
automatically extracted from a corpus of non-English documents, using either
English natural language queries (e.g. cholera outbreaks in Iran) or structured
queries (e.g. find all events of type Disease-Outbreak with agent cholera and
location Iran).
</p></li>
</ul>

<h3>Title: Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora. (arXiv:2305.10848v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10848">http://arxiv.org/abs/2305.10848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10848] Advancing Full-Text Search Lemmatization Techniques with Paradigm Retrieval from OpenCorpora](http://arxiv.org/abs/2305.10848) #extraction</code></li>
<li>Summary: <p>In this paper, we unveil a groundbreaking method to amplify full-text search
lemmatization, utilizing the OpenCorpora dataset and a bespoke paradigm
retrieval algorithm. Our primary aim is to streamline the extraction of a
word's primary form or lemma - a crucial factor in full-text search.
Additionally, we propose a compact dictionary storage strategy, significantly
boosting the speed and precision of lemma retrieval.
</p></li>
</ul>

<h3>Title: Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10928">http://arxiv.org/abs/2305.10928</a></li>
<li>Code URL: <a href="https://github.com/nadavborenstein/ee-from-historical-ads">https://github.com/nadavborenstein/ee-from-historical-ads</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10928] Multilingual Event Extraction from Historical Newspaper Adverts](http://arxiv.org/abs/2305.10928) #extraction</code></li>
<li>Summary: <p>NLP methods can aid historians in analyzing textual materials in greater
volumes than manually feasible. Developing such methods poses substantial
challenges though. First, acquiring large, annotated historical datasets is
difficult, as only domain experts can reliably label them. Second, most
available off-the-shelf NLP models are trained on modern language texts,
rendering them significantly less effective when applied to historical corpora.
This is particularly problematic for less well studied tasks, and for languages
other than English. This paper addresses these challenges while focusing on the
under-explored task of event extraction from a novel domain of historical
texts. We introduce a new multilingual dataset in English, French, and Dutch
composed of newspaper ads from the early modern colonial period reporting on
enslaved people who liberated themselves from enslavement. We find that: 1)
even with scarce annotated data, it is possible to achieve surprisingly good
results by formulating the problem as an extractive QA task and leveraging
existing datasets and models for modern languages; and 2) cross-lingual
low-resource learning for historical languages is highly challenging, and
machine translation of the historical datasets to the considered target
languages is, in practice, often the best-performing solution.
</p></li>
</ul>

<h3>Title: Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction. (arXiv:2305.10985v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10985">http://arxiv.org/abs/2305.10985</a></li>
<li>Code URL: <a href="https://github.com/mainlp/crossre">https://github.com/mainlp/crossre</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10985] Multi-CrossRE A Multi-Lingual Multi-Domain Dataset for Relation Extraction](http://arxiv.org/abs/2305.10985) #extraction</code></li>
<li>Summary: <p>Most research in Relation Extraction (RE) involves the English language,
mainly due to the lack of multi-lingual resources. We propose Multi-CrossRE,
the broadest multi-lingual dataset for RE, including 26 languages in addition
to English, and covering six text domains. Multi-CrossRE is a machine
translated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion
including more than 200 sentences in seven diverse languages checked by native
speakers. We run a baseline model over the 26 new datasets and--as sanity
check--over the 26 back-translations to English. Results on the back-translated
data are consistent with the ones on the original English CrossRE, indicating
high quality of the translation and the resulting dataset.
</p></li>
</ul>

<h3>Title: Silver Syntax Pre-training for Cross-Domain Relation Extraction. (arXiv:2305.11016v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11016">http://arxiv.org/abs/2305.11016</a></li>
<li>Code URL: <a href="https://github.com/mainlp/syntax-pre-training-for-re">https://github.com/mainlp/syntax-pre-training-for-re</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11016] Silver Syntax Pre-training for Cross-Domain Relation Extraction](http://arxiv.org/abs/2305.11016) #extraction</code></li>
<li>Summary: <p>Relation Extraction (RE) remains a challenging task, especially when
considering realistic out-of-domain evaluations. One of the main reasons for
this is the limited training size of current RE datasets: obtaining
high-quality (manually annotated) data is extremely expensive and cannot
realistically be repeated for each new domain. An intermediate training step on
data from related tasks has shown to be beneficial across many NLP
tasks.However, this setup still requires supplementary annotated data, which is
often not available. In this paper, we investigate intermediate pre-training
specifically for RE. We exploit the affinity between syntactic structure and
semantic RE, and identify the syntactic relations which are closely related to
RE by being on the shortest dependency path between two entities. We then take
advantage of the high accuracy of current syntactic parsers in order to
automatically obtain large amounts of low-cost pre-training data. By
pre-training our RE model on the relevant syntactic relations, we are able to
outperform the baseline in five out of six cross-domain setups, without any
additional annotated data.
</p></li>
</ul>

<h3>Title: Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11029">http://arxiv.org/abs/2305.11029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11029] Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction](http://arxiv.org/abs/2305.11029) #extraction</code></li>
<li>Summary: <p>Document-level relation extraction (DocRE) aims to infer complex semantic
relations among entities in a document. Distant supervision (DS) is able to
generate massive auto-labeled data, which can improve DocRE performance. Recent
works leverage pseudo labels generated by the pre-denoising model to reduce
noise in DS data. However, unreliable pseudo labels bring new noise, e.g.,
adding false pseudo labels and losing correct DS labels. Therefore, how to
select effective pseudo labels to denoise DS data is still a challenge in
document-level distant relation extraction. To tackle this issue, we introduce
uncertainty estimation technology to determine whether pseudo labels can be
trusted. In this work, we propose a Document-level distant Relation Extraction
framework with Uncertainty Guided label denoising, UGDRE. Specifically, we
propose a novel instance-level uncertainty estimation method, which measures
the reliability of the pseudo labels with overlapping relations. By further
considering the long-tail problem, we design dynamic uncertainty thresholds for
different types of relations to filter high-uncertainty pseudo labels. We
conduct experiments on two public datasets. Our framework outperforms strong
baselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset.
</p></li>
</ul>

<h3>Title: Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement. (arXiv:2305.11034v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11034">http://arxiv.org/abs/2305.11034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11034] Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement](http://arxiv.org/abs/2305.11034) #extraction</code></li>
<li>Summary: <p>State-of-the-art target-oriented opinion word extraction (TOWE) models
typically use BERT-based text encoders that operate on the word level, along
with graph convolutional networks (GCNs) that incorporate syntactic information
extracted from syntax trees. These methods achieve limited gains with GCNs and
have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to
be effective at representing rare words or words with insufficient context
information. To address this issue, this work trades syntax trees for BERT
wordpieces by entirely removing the GCN component from the methods'
architectures. To enhance TOWE performance, we tackle the issue of aspect
representation loss during encoding. Instead of solely utilizing a sentence as
the input, we use a sentence-aspect pair. Our relatively simple approach
achieves state-of-the-art results on benchmark datasets and should serve as a
strong baseline for further research.
</p></li>
</ul>

<h3>Title: ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph. (arXiv:2305.11068v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11068">http://arxiv.org/abs/2305.11068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11068] ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph](http://arxiv.org/abs/2305.11068) #extraction</code></li>
<li>Summary: <p>The purpose of this work is to describe the Orkg-Leaderboard software
designed to extract leaderboards defined as Task-Dataset-Metric tuples
automatically from large collections of empirical research papers in Artificial
Intelligence (AI). The software can support both the main workflows of
scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the
system is integrated with the Open Research Knowledge Graph (ORKG) platform,
which fosters the machine-actionable publishing of scholarly findings. Thus the
system output, when integrated within the ORKG's supported Semantic Web
infrastructure of representing machine-actionable 'resources' on the Web,
enables: 1) broadly, the integration of empirical results of researchers across
the world, thus enabling transparency in empirical research with the potential
to also being complete contingent on the underlying data source(s) of
publications; and 2) specifically, enables researchers to track the progress in
AI with an overview of the state-of-the-art (SOTA) across the most common AI
tasks and their corresponding datasets via dynamic ORKG frontend views
leveraging tables and visualization charts over the machine-actionable data.
Our best model achieves performances above 90% F1 on the \textit{leaderboard}
extraction task, thus proving Orkg-Leaderboards a practically viable tool for
real-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the
leaderboard extraction task to an automated digitalization task, which has
been, for a long time in the community, a crowdsourced endeavor.
</p></li>
</ul>

<h3>Title: Time Series Clustering With Random Convolutional Kernels. (arXiv:2305.10457v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10457">http://arxiv.org/abs/2305.10457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10457] Time Series Clustering With Random Convolutional Kernels](http://arxiv.org/abs/2305.10457) #extraction</code></li>
<li>Summary: <p>Time series can describe a wide range of natural and social phenomena. A few
samples are climate and seismic measures trends, stock prices, or website
visits. Time-series clustering helps to find outliers that, related to these
instances, could represent temperature anomalies, imminent volcanic eruptions,
market disturbances, or fraudulent web traffic. Founded on the success of
automatic feature extraction techniques, specifically employing random kernels,
we develop a new method for time series clustering consisting of two steps.
First, a random convolutional structure transforms the data into an enhanced
feature representation. Afterwards, a clustering algorithm classifies the
transformed data. The method improves state-of-the-art results on time series
clustering benchmarks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Model-Contrastive Federated Domain Adaptation. (arXiv:2305.10432v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10432">http://arxiv.org/abs/2305.10432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10432] Model-Contrastive Federated Domain Adaptation](http://arxiv.org/abs/2305.10432) #federate</code></li>
<li>Summary: <p>Federated domain adaptation (FDA) aims to collaboratively transfer knowledge
from source clients (domains) to the related but different target client,
without communicating the local data of any client. Moreover, the source
clients have different data distributions, leading to extremely challenging in
knowledge transfer. Despite the recent progress in FDA, we empirically find
that existing methods can not leverage models of heterogeneous domains and thus
they fail to achieve excellent performance. In this paper, we propose a
model-based method named FDAC, aiming to address {\bf F}ederated {\bf D}omain
{\bf A}daptation based on {\bf C}ontrastive learning and Vision Transformer
(ViT). In particular, contrastive learning can leverage the unlabeled data to
train excellent models and the ViT architecture performs better than
convolutional neural networks (CNNs) in extracting adaptable features. To the
best of our knowledge, FDAC is the first attempt to learn transferable
representations by manipulating the latent architecture of ViT under the
federated setting. Furthermore, FDAC can increase the target data diversity by
compensating from each source model with insufficient knowledge of samples and
features, based on domain augmentation and semantic matching. Extensive
experiments on several real datasets demonstrate that FDAC outperforms all the
comparative methods in most conditions. Moreover, FDCA can also improve
communication efficiency which is another key factor in the federated setting.
</p></li>
</ul>

<h3>Title: The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond. (arXiv:2305.10697v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10697">http://arxiv.org/abs/2305.10697</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10697] The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond](http://arxiv.org/abs/2305.10697) #federate</code></li>
<li>Summary: <p>When the data used for reinforcement learning (RL) are collected by multiple
agents in a distributed manner, federated versions of RL algorithms allow
collaborative learning without the need of sharing local data. In this paper,
we consider federated Q-learning, which aims to learn an optimal Q-function by
periodically aggregating local Q-estimates trained on local data alone.
Focusing on infinite-horizon tabular Markov decision processes, we provide
sample complexity guarantees for both the synchronous and asynchronous variants
of federated Q-learning. In both cases, our bounds exhibit a linear speedup
with respect to the number of agents and sharper dependencies on other salient
problem parameters. Moreover, existing approaches to federated Q-learning adopt
an equally-weighted averaging of local Q-estimates, which can be highly
sub-optimal in the asynchronous setting since the local trajectories can be
highly heterogeneous due to different local behavior policies. Existing sample
complexity scales inverse proportionally to the minimum entry of the stationary
state-action occupancy distributions over all agents, requiring that every
agent covers the entire state-action space. Instead, we propose a novel
importance averaging algorithm, giving larger weights to more frequently
visited state-action pairs. The improved sample complexity scales inverse
proportionally to the minimum entry of the average stationary state-action
occupancy distribution of all agents, thus only requiring the agents
collectively cover the entire state-action space, unveiling the blessing of
heterogeneity.
</p></li>
</ul>

<h3>Title: FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10730">http://arxiv.org/abs/2305.10730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10730] FedMR: Federated Learning via Model Recombination](http://arxiv.org/abs/2305.10730) #federate</code></li>
<li>Summary: <p>Although Federated Learning (FL) enables global model training across clients
without compromising their raw data, existing Federated Averaging
(FedAvg)-based methods suffer from the problem of low inference performance,
especially for unevenly distributed data among clients. This is mainly because
i) FedAvg initializes client models with the same global models, which makes
the local training hard to escape from the local search for optimal solutions;
and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the
individual characteristics of local models. To address such issues that
strongly limit the inference capability of FL, we propose a novel and effective
FL paradigm named FedMR (Federated Model Recombination). Unlike conventional
FedAvg-based methods, the cloud server of FedMR shuffles each layer of
collected local models and recombines them to achieve new models for local
training on clients. Due to the diversified initialization models for clients
coupled with fine-grained model recombination, FedMR can converge to a
well-generalized global model for all the clients, leading to a superior
inference performance. Experimental results show that, compared with
state-of-the-art FL methods, FedMR can significantly improve inference accuracy
in a quicker manner without exposing client privacy.
</p></li>
</ul>

<h3>Title: Client Selection for Federated Policy Optimization with Environment Heterogeneity. (arXiv:2305.10978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10978">http://arxiv.org/abs/2305.10978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10978] Client Selection for Federated Policy Optimization with Environment Heterogeneity](http://arxiv.org/abs/2305.10978) #federate</code></li>
<li>Summary: <p>The development of Policy Iteration (PI) has inspired many recent algorithms
for Reinforcement Learning (RL), including several policy gradient methods,
that gained both theoretical soundness and empirical success on a variety of
tasks. The theory of PI is rich in the context of centralized learning, but its
study is still in the infant stage under the federated setting. This paper
explores the federated version of Approximate PI (API) and derives its error
bound, taking into account the approximation error introduced by environment
heterogeneity. We theoretically prove that a proper client selection scheme can
reduce this error bound. Based on the theoretical result, we propose a client
selection algorithm to alleviate the additional approximation error caused by
environment heterogeneity. Experiment results show that the proposed algorithm
outperforms other biased and unbiased client selection methods on the federated
mountain car problem by effectively selecting clients with a lower level of
heterogeneity from the population distribution.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11004">http://arxiv.org/abs/2305.11004</a></li>
<li>Code URL: <a href="https://github.com/Lokilankaaa/TaxBox">https://github.com/Lokilankaaa/TaxBox</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11004] Taxonomy Completion with Probabilistic Scorer via Box Embedding](http://arxiv.org/abs/2305.11004) #fair</code></li>
<li>Summary: <p>Taxonomy completion, a task aimed at automatically enriching an existing
taxonomy with new concepts, has gained significant interest in recent years.
Previous works have introduced complex modules, external information, and
pseudo-leaves to enrich the representation and unify the matching process of
attachment and insertion. While they have achieved good performance, these
introductions may have brought noise and unfairness during training and
scoring. In this paper, we present TaxBox, a novel framework for taxonomy
completion that maps taxonomy concepts to box embeddings and employs two
probabilistic scorers for concept attachment and insertion, avoiding the need
for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a
graph aggregation module to leverage the structural information of the taxonomy
and two lightweight decoders that map features to box embedding and capture
complex relationships between concepts; (2) two probabilistic scorers that
correspond to attachment and insertion operations and ensure the avoidance of
pseudo-leaves; and (3) three learning objectives that assist the model in
mapping concepts more granularly onto the box embedding space. Experimental
results on four real-world datasets suggest that TaxBox outperforms baseline
methods by a considerable margin and surpasses previous state-of-art methods to
a certain extent.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance. (arXiv:2305.10696v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10696">http://arxiv.org/abs/2305.10696</a></li>
<li>Code URL: <a href="https://github.com/zheyuaqazhang/unbiasedgbm">https://github.com/zheyuaqazhang/unbiasedgbm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10696] Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance](http://arxiv.org/abs/2305.10696) #interpretability</code></li>
<li>Summary: <p>Gradient Boosting Decision Tree (GBDT) has achieved remarkable success in a
wide variety of applications. The split finding algorithm, which determines the
tree construction process, is one of the most crucial components of GBDT.
However, the split finding algorithm has long been criticized for its bias
towards features with a large number of potential splits. This bias introduces
severe interpretability and overfitting issues in GBDT. To this end, we provide
a fine-grained analysis of bias in GBDT and demonstrate that the bias
originates from 1) the systematic bias in the gain estimation of each split and
2) the bias in the split finding algorithm resulting from the use of the same
data to evaluate the split improvement and determine the best split. Based on
the analysis, we propose unbiased gain, a new unbiased measurement of gain
importance using out-of-bag samples. Moreover, we incorporate the unbiased
property into the split finding algorithm and develop UnbiasedGBM to solve the
overfitting issue of GBDT. We assess the performance of UnbiasedGBM and
unbiased gain in a large-scale empirical study comprising 60 datasets and show
that: 1) UnbiasedGBM exhibits better performance than popular GBDT
implementations such as LightGBM, XGBoost, and Catboost on average on the 60
datasets and 2) unbiased gain achieves better average performance in feature
selection than popular feature importance methods. The codes are available at
https://github.com/ZheyuAqaZhang/UnbiasedGBM.
</p></li>
</ul>

<h3>Title: Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10748">http://arxiv.org/abs/2305.10748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10748] Physics Inspired Approaches Towards Understanding Gaussian Processes](http://arxiv.org/abs/2305.10748) #interpretability</code></li>
<li>Summary: <p>Prior beliefs about the latent function to shape inductive biases can be
incorporated into a Gaussian Process (GP) via the kernel. However, beyond
kernel choices, the decision-making process of GP models remains poorly
understood. In this work, we contribute an analysis of the loss landscape for
GP models using methods from physics. We demonstrate $\nu$-continuity for
Matern kernels and outline aspects of catastrophe theory at critical points in
the loss landscape. By directly including $\nu$ in the hyperparameter
optimisation for Matern kernels, we find that typical values of $\nu$ are far
from optimal in terms of performance, yet prevail in the literature due to the
increased computational speed. We also provide an a priori method for
evaluating the effect of GP ensembles and discuss various voting approaches
based on physical properties of the loss landscape. The utility of these
approaches is demonstrated for various synthetic and real datasets. Our
findings provide an enhanced understanding of the decision-making process
behind GPs and offer practical guidance for improving their performance and
interpretability in a range of applications.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Multi-spectral Class Center Network for Face Manipulation Detection and Localization. (arXiv:2305.10794v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10794">http://arxiv.org/abs/2305.10794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10794] Multi-spectral Class Center Network for Face Manipulation Detection and Localization](http://arxiv.org/abs/2305.10794) #explainability</code></li>
<li>Summary: <p>As Deepfake contents continue to proliferate on the internet, advancing face
manipulation forensics has become a pressing issue. To combat this emerging
threat, previous methods mainly focus on studying how to distinguish authentic
and manipulated face images. Despite impressive, image-level classification
lacks explainability and is limited to some specific application scenarios.
Existing forgery localization methods suffer from imprecise and inconsistent
pixel-level annotations. To alleviate these problems, this paper first
re-constructs the FaceForensics++ dataset by introducing pixel-level
annotations, then builds an extensive benchmark for localizing tampered
regions. Next, a novel Multi-Spectral Class Center Network (MSCCNet) is
proposed for face manipulation detection and localization. Specifically,
inspired by the power of frequency-related forgery traces, we design
Multi-Spectral Class Center (MSCC) module to learn more generalizable and
semantic-agnostic features. Based on the features of different frequency bands,
the MSCC module collects multispectral class centers and computes
pixel-to-class relations. Applying multi-spectral class-level representations
suppresses the semantic information of the visual concepts, which is
insensitive to manipulations. Furthermore, we propose a Multi-level Features
Aggregation (MFA) module to employ more low-level forgery artifacts and
structure textures. Experimental results quantitatively and qualitatively
indicate the effectiveness and superiority of the proposed MSCCNet on
comprehensive localization benchmarks. We expect this work to inspire more
studies on pixel-level face manipulation localization. The annotations and code
will be available.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10474">http://arxiv.org/abs/2305.10474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10474] Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](http://arxiv.org/abs/2305.10474) #diffusion</code></li>
<li>Summary: <p>Despite tremendous progress in generating high-quality images using diffusion
models, synthesizing a sequence of animated frames that are both photorealistic
and temporally coherent is still in its infancy. While off-the-shelf
billion-scale datasets for image generation are available, collecting similar
video data of the same scale is still challenging. Also, training a video
diffusion model is computationally much more expensive than its image
counterpart. In this work, we explore finetuning a pretrained image diffusion
model with video data as a practical solution for the video synthesis task. We
find that naively extending the image noise prior to video noise prior in video
diffusion leads to sub-optimal performance. Our carefully designed video noise
prior leads to substantially better performance. Extensive experimental
validation shows that our model, Preserve Your Own Correlation (PYoCo), attains
SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It
also achieves SOTA video generation quality on the small-scale UCF-101
benchmark with a $10\times$ smaller model using significantly less computation
than the prior art.
</p></li>
</ul>

<h3>Title: PTQD: Accurate Post-Training Quantization for Diffusion Models. (arXiv:2305.10657v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10657">http://arxiv.org/abs/2305.10657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10657] PTQD: Accurate Post-Training Quantization for Diffusion Models](http://arxiv.org/abs/2305.10657) #diffusion</code></li>
<li>Summary: <p>Diffusion models have recently dominated image synthesis and other related
generative tasks. However, the iterative denoising process is expensive in
computations at inference time, making diffusion models less practical for
low-latency and scalable real-world applications. Post-training quantization of
diffusion models can significantly reduce the model size and accelerate the
sampling process without requiring any re-training. Nonetheless, applying
existing post-training quantization methods directly to low-bit diffusion
models can significantly impair the quality of generated samples. Specifically,
for each denoising step, quantization noise leads to deviations in the
estimated mean and mismatches with the predetermined variance schedule.
Moreover, as the sampling process proceeds, the quantization noise may
accumulate, resulting in a low signal-to-noise ratio (SNR) in late denoising
steps. To address these challenges, we propose a unified formulation for the
quantization noise and diffusion perturbed noise in the quantized denoising
process. We first disentangle the quantization noise into its correlated and
residual uncorrelated parts regarding its full-precision counterpart. The
correlated part can be easily corrected by estimating the correlation
coefficient. For the uncorrelated part, we calibrate the denoising variance
schedule to absorb the excess variance resulting from quantization. Moreover,
we propose a mixed-precision scheme to choose the optimal bitwidth for each
denoising step, which prefers low bits to accelerate the early denoising steps
while high bits maintain the high SNR for the late steps. Extensive experiments
demonstrate that our method outperforms previous post-training quantized
diffusion models in generating high-quality samples, with only a 0.06 increase
in FID score compared to full-precision LDM-4 on ImageNet 256x256, while saving
19.9x bit operations.
</p></li>
</ul>

<h3>Title: Discriminative Diffusion Models as Few-shot Vision and Language Learners. (arXiv:2305.10722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10722">http://arxiv.org/abs/2305.10722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10722] Discriminative Diffusion Models as Few-shot Vision and Language Learners](http://arxiv.org/abs/2305.10722) #diffusion</code></li>
<li>Summary: <p>Diffusion models, such as Stable Diffusion, have shown incredible performance
on text-to-image generation. Since text-to-image generation often requires
models to generate visual concepts with fine-grained details and attributes
specified in text prompts, can we leverage the powerful representations learned
by pre-trained diffusion models for discriminative tasks such as image-text
matching? To answer this question, we propose a novel approach, Discriminative
Stable Diffusion (DSD), which turns pre-trained text-to-image diffusion models
into few-shot discriminative learners. Our approach uses the cross-attention
score of a Stable Diffusion model to capture the mutual influence between
visual and textual information and fine-tune the model via attention-based
prompt learning to perform image-text matching. By comparing DSD with
state-of-the-art methods on several benchmark datasets, we demonstrate the
potential of using pre-trained diffusion models for discriminative tasks with
superior results on few-shot image-text matching.
</p></li>
</ul>

<h3>Title: Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10769">http://arxiv.org/abs/2305.10769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10769] Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling](http://arxiv.org/abs/2305.10769) #diffusion</code></li>
<li>Summary: <p>Diffusion Probability Models (DPMs) have made impressive advancements in
various machine learning domains. However, achieving high-quality synthetic
samples typically involves performing a large number of sampling steps, which
impedes the possibility of real-time sample synthesis. Traditional accelerated
sampling algorithms via knowledge distillation rely on pre-trained model
weights and discrete time step scenarios, necessitating additional training
sessions to achieve their goals. To address these issues, we propose the
Catch-Up Distillation (CUD), which encourages the current moment output of the
velocity estimation model ``catch up'' with its previous moment output.
Specifically, CUD adjusts the original Ordinary Differential Equation (ODE)
training objective to align the current moment output with both the ground
truth label and the previous moment output, utilizing Runge-Kutta-based
multi-step alignment distillation for precise ODE estimation while preventing
asynchronous updates. Furthermore, we investigate the design space for CUDs
under continuous time-step scenarios and analyze how to determine the suitable
strategies. To demonstrate CUD's effectiveness, we conduct thorough ablation
and comparison experiments on CIFAR-10, MNIST, and ImageNet-64. On CIFAR-10, we
obtain a FID of 2.80 by sampling in 15 steps under one-session training and the
new state-of-the-art FID of 3.37 by sampling in one step with additional
training. This latter result necessitated only 62w iterations with a batch size
of 128, in contrast to Consistency Distillation, which demanded 210w iterations
with a larger batch size of 256.
</p></li>
</ul>

<h3>Title: DiffUTE: Universal Text Editing Diffusion Model. (arXiv:2305.10825v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10825">http://arxiv.org/abs/2305.10825</a></li>
<li>Code URL: <a href="https://github.com/chenhaoxing/DiffUTE">https://github.com/chenhaoxing/DiffUTE</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10825] DiffUTE: Universal Text Editing Diffusion Model](http://arxiv.org/abs/2305.10825) #diffusion</code></li>
<li>Summary: <p>Diffusion model based language-guided image editing has achieved great
success recently. However, existing state-of-the-art diffusion models struggle
with rendering correct text and text style during generation. To tackle this
problem, we propose a universal self-supervised text editing diffusion model
(DiffUTE), which aims to replace or modify words in the source image with
another one while maintaining its realistic appearance. Specifically, we build
our model on a diffusion model and carefully modify the network structure to
enable the model for drawing multilingual characters with the help of glyph and
position information. Moreover, we design a self-supervised learning framework
to leverage large amounts of web data to improve the representation ability of
the model. Experimental results show that our method achieves an impressive
performance and enables controllable editing on in-the-wild images with high
fidelity. Our code will be avaliable in
\url{https://github.com/chenhaoxing/DiffUTE}.
</p></li>
</ul>

<h3>Title: LDM3D: Latent Diffusion Model for 3D. (arXiv:2305.10853v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10853">http://arxiv.org/abs/2305.10853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10853] LDM3D: Latent Diffusion Model for 3D](http://arxiv.org/abs/2305.10853) #diffusion</code></li>
<li>Summary: <p>This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that
generates both image and depth map data from a given text prompt, allowing
users to generate RGBD images from text prompts. The LDM3D model is fine-tuned
on a dataset of tuples containing an RGB image, depth map and caption, and
validated through extensive experiments. We also develop an application called
DepthFusion, which uses the generated RGB images and depth maps to create
immersive and interactive 360-degree-view experiences using TouchDesigner. This
technology has the potential to transform a wide range of industries, from
entertainment and gaming to architecture and design. Overall, this paper
presents a significant contribution to the field of generative AI and computer
vision, and showcases the potential of LDM3D and DepthFusion to revolutionize
content creation and digital experiences. A short video summarizing the
approach can be found at https://t.ly/tdi2.
</p></li>
</ul>

<h3>Title: TextDiffuser: Diffusion Models as Text Painters. (arXiv:2305.10855v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10855">http://arxiv.org/abs/2305.10855</a></li>
<li>Code URL: <a href="https://github.com/microsoft/unilm/tree/master/textdiffuser">https://github.com/microsoft/unilm/tree/master/textdiffuser</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10855] TextDiffuser: Diffusion Models as Text Painters](http://arxiv.org/abs/2305.10855) #diffusion</code></li>
<li>Summary: <p>Diffusion models have gained increasing attention for their impressive
generation abilities but currently struggle with rendering accurate and
coherent text. To address this issue, we introduce \textbf{TextDiffuser},
focusing on generating images with visually appealing text that is coherent
with backgrounds. TextDiffuser consists of two stages: first, a Transformer
model generates the layout of keywords extracted from text prompts, and then
diffusion models generate images conditioned on the text prompt and the
generated layout. Additionally, we contribute the first large-scale text images
dataset with OCR annotations, \textbf{MARIO-10M}, containing 10 million
image-text pairs with text recognition, detection, and character-level
segmentation annotations. We further collect the \textbf{MARIO-Eval} benchmark
to serve as a comprehensive tool for evaluating text rendering quality. Through
experiments and user studies, we show that TextDiffuser is flexible and
controllable to create high-quality text images using text prompts alone or
together with text template images, and conduct text inpainting to reconstruct
incomplete images with text. The code, model, and dataset will be available at
\url{https://aka.ms/textdiffuser}.
</p></li>
</ul>

<h3>Title: VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation. (arXiv:2305.10874v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10874">http://arxiv.org/abs/2305.10874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10874] VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](http://arxiv.org/abs/2305.10874) #diffusion</code></li>
<li>Summary: <p>We present VideoFactory, an innovative framework for generating high-quality
open-domain videos. VideoFactory excels in producing high-definition
(1376x768), widescreen (16:9) videos without watermarks, creating an engaging
user experience. Generating videos guided by text instructions poses
significant challenges, such as modeling the complex relationship between space
and time, and the lack of large-scale text-video paired data. Previous
approaches extend pretrained text-to-image generation models by adding temporal
1D convolution/attention modules for video generation. However, these
approaches overlook the importance of jointly modeling space and time,
inevitably leading to temporal distortions and misalignment between texts and
videos. In this paper, we propose a novel approach that strengthens the
interaction between spatial and temporal perceptions. In particular, we utilize
a swapped cross-attention mechanism in 3D windows that alternates the "query"
role between spatial and temporal blocks, enabling mutual reinforcement for
each other. To fully unlock model capabilities for high-quality video
generation, we curate a large-scale video dataset called HD-VG-130M. This
dataset comprises 130 million text-video pairs from the open-domain, ensuring
high-definition, widescreen and watermark-free characters. Objective metrics
and user studies demonstrate the superiority of our approach in terms of
per-frame quality, temporal correlation, and text-video alignment, with clear
margins.
</p></li>
</ul>

<h3>Title: Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10924">http://arxiv.org/abs/2305.10924</a></li>
<li>Code URL: <a href="https://github.com/vainf/diff-pruning">https://github.com/vainf/diff-pruning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10924] Structural Pruning for Diffusion Models](http://arxiv.org/abs/2305.10924) #diffusion</code></li>
<li>Summary: <p>Generative modeling has recently undergone remarkable advancements, primarily
propelled by the transformative implications of Diffusion Probabilistic Models
(DPMs). The impressive capability of these models, however, often entails
significant computational overhead during both training and inference. To
tackle this challenge, we present Diff-Pruning, an efficient compression method
tailored for learning lightweight diffusion models from pre-existing ones,
without the need for extensive re-training. The essence of Diff-Pruning is
encapsulated in a Taylor expansion over pruned timesteps, a process that
disregards non-contributory diffusion steps and ensembles informative gradients
to identify important weights. Our empirical assessment, undertaken across four
diverse datasets highlights two primary benefits of our proposed method: 1)
Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to
20% of the original training expenditure; 2) Consistency: the pruned diffusion
models inherently preserve generative behavior congruent with their pre-trained
progenitors. Code is available at \url{https://github.com/VainF/Diff-Pruning}.
</p></li>
</ul>

<h3>Title: Unsupervised Pansharpening via Low-rank Diffusion Model. (arXiv:2305.10925v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10925">http://arxiv.org/abs/2305.10925</a></li>
<li>Code URL: <a href="https://github.com/xyrui/plrdiff">https://github.com/xyrui/plrdiff</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10925] Unsupervised Pansharpening via Low-rank Diffusion Model](http://arxiv.org/abs/2305.10925) #diffusion</code></li>
<li>Summary: <p>Pansharpening is a process of merging a highresolution panchromatic (PAN)
image and a low-resolution multispectral (LRMS) image to create a single
high-resolution multispectral (HRMS) image. Most of the existing deep
learningbased pansharpening methods have poor generalization ability and the
traditional model-based pansharpening methods need careful manual exploration
for the image structure prior. To alleviate these issues, this paper proposes
an unsupervised pansharpening method by combining the diffusion model with the
low-rank matrix factorization technique. Specifically, we assume that the HRMS
image is decomposed into the product of two low-rank tensors, i.e., the base
tensor and the coefficient matrix. The base tensor lies on the image field and
has low spectral dimension, we can thus conveniently utilize a pre-trained
remote sensing diffusion model to capture its image structures. Additionally,
we derive a simple yet quite effective way to preestimate the coefficient
matrix from the observed LRMS image, which preserves the spectral information
of the HRMS. Extensive experimental results on some benchmark datasets
demonstrate that our proposed method performs better than traditional
model-based approaches and has better generalization ability than deep
learning-based techniques. The code is released in
https://github.com/xyrui/PLRDiff.
</p></li>
</ul>

<h3>Title: Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11067">http://arxiv.org/abs/2305.11067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11067] Generating coherent comic with rich story using ChatGPT and Stable Diffusion](http://arxiv.org/abs/2305.11067) #diffusion</code></li>
<li>Summary: <p>Past work demonstrated that using neural networks, we can extend unfinished
music pieces while maintaining the music style of the musician. With recent
advancements in large language models and diffusion models, we are now capable
of generating comics with an interesting storyline while maintaining the art
style of the artist. In this paper, we used ChatGPT to generate storylines and
dialogue and then generated the comic using stable diffusion. We introduced a
novel way to evaluate AI-generated stories, and we achieved SOTA performance on
character fidelity and art style by fine-tuning stable diffusion using LoRA,
ControlNet, etc.
</p></li>
</ul>

<h3>Title: Inspecting the Geographical Representativeness of Images from Text-to-Image Models. (arXiv:2305.11080v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11080">http://arxiv.org/abs/2305.11080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11080] Inspecting the Geographical Representativeness of Images from Text-to-Image Models](http://arxiv.org/abs/2305.11080) #diffusion</code></li>
<li>Summary: <p>Recent progress in generative models has resulted in models that produce both
realistic as well as relevant images for most textual inputs. These models are
being used to generate millions of images everyday, and hold the potential to
drastically impact areas such as generative art, digital marketing and data
augmentation. Given their outsized impact, it is important to ensure that the
generated content reflects the artifacts and surroundings across the globe,
rather than over-representing certain parts of the world. In this paper, we
measure the geographical representativeness of common nouns (e.g., a house)
generated through DALL.E 2 and Stable Diffusion models using a crowdsourced
study comprising 540 participants across 27 countries. For deliberately
underspecified inputs without country names, the generated images most reflect
the surroundings of the United States followed by India, and the top
generations rarely reflect surroundings from all other countries (average score
less than 3 out of 5). Specifying the country names in the input increases the
representativeness by 1.44 points on average for DALL.E 2 and 0.75 for Stable
Diffusion, however, the overall scores for many countries still remain low,
highlighting the need for future models to be more geographically inclusive.
Lastly, we examine the feasibility of quantifying the geographical
representativeness of generated images without conducting user studies.
</p></li>
</ul>

<h3>Title: Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces. (arXiv:2305.11089v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11089">http://arxiv.org/abs/2305.11089</a></li>
<li>Code URL: <a href="https://github.com/lanl/blackout-diffusion">https://github.com/lanl/blackout-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11089] Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces](http://arxiv.org/abs/2305.11089) #diffusion</code></li>
<li>Summary: <p>Typical generative diffusion models rely on a Gaussian diffusion process for
training the backward transformations, which can then be used to generate
samples from Gaussian noise. However, real world data often takes place in
discrete-state spaces, including many scientific applications. Here, we develop
a theoretical formulation for arbitrary discrete-state Markov processes in the
forward diffusion process using exact (as opposed to variational) analysis. We
relate the theory to the existing continuous-state Gaussian diffusion as well
as other approaches to discrete diffusion, and identify the corresponding
reverse-time stochastic process and score function in the continuous-time
setting, and the reverse-time mapping in the discrete-time setting. As an
example of this framework, we introduce ``Blackout Diffusion'', which learns to
produce samples from an empty image instead of from noise. Numerical
experiments on the CIFAR-10, Binarized MNIST, and CelebA datasets confirm the
feasibility of our approach. Generalizing from specific (Gaussian) forward
processes to discrete-state processes without a variational approximation sheds
light on how to interpret diffusion models, which we discuss.
</p></li>
</ul>

<h3>Title: Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10818">http://arxiv.org/abs/2305.10818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10818] Democratized Diffusion Language Model](http://arxiv.org/abs/2305.10818) #diffusion</code></li>
<li>Summary: <p>Despite the potential benefits of Diffusion Models for NLP applications,
publicly available implementations, trained models, or reproducible training
procedures currently need to be publicly available. We present the Democratized
Diffusion Language Model (DDLM), based on the Continuous Diffusion for
Categorical Data (CDCD) framework, to address these challenges. We propose a
simplified training procedure for DDLM using the C4 dataset and perform an
in-depth analysis of the trained model's behavior. Furthermore, we introduce a
novel early-exiting strategy for faster sampling with models trained with score
interpolation. Since no previous works aimed at solving downstream tasks with
pre-trained Diffusion LM (e.g., classification tasks), we experimented with
GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this
paper, we propose available training and evaluation pipelines to other
researchers and pre-trained DDLM models, which could be used in future research
with Diffusion LMs.
</p></li>
</ul>

<h3>Title: Sampling, Diffusions, and Stochastic Localization. (arXiv:2305.10690v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10690">http://arxiv.org/abs/2305.10690</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10690] Sampling, Diffusions, and Stochastic Localization](http://arxiv.org/abs/2305.10690) #diffusion</code></li>
<li>Summary: <p>Diffusions are a successful technique to sample from high-dimensional
distributions can be either explicitly given or learnt from a collection of
samples. They implement a diffusion process whose endpoint is a sample from the
target distribution and whose drift is typically represented as a neural
network. Stochastic localization is a successful technique to prove mixing of
Markov Chains and other functional inequalities in high dimension. An
algorithmic version of stochastic localization was introduced in [EAMS2022], to
obtain an algorithm that samples from certain statistical mechanics models.
</p></li>
</ul>

<p>This notes have three objectives: (i) Generalize the construction [EAMS2022]
to other stochastic localization processes; (ii) Clarify the connection between
diffusions and stochastic localization. In particular we show that standard
denoising diffusions are stochastic localizations but other examples that are
naturally suggested by the proposed viewpoint; (iii) Describe some insights
that follow from this viewpoint.
</p>

<h3>Title: Dirichlet Diffusion Score Model for Biological Sequence Generation. (arXiv:2305.10699v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10699">http://arxiv.org/abs/2305.10699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10699] Dirichlet Diffusion Score Model for Biological Sequence Generation](http://arxiv.org/abs/2305.10699) #diffusion</code></li>
<li>Summary: <p>Designing biological sequences is an important challenge that requires
satisfying complex constraints and thus is a natural problem to address with
deep generative modeling. Diffusion generative models have achieved
considerable success in many applications. Score-based generative stochastic
differential equations (SDE) model is a continuous-time diffusion model
framework that enjoys many benefits, but the originally proposed SDEs are not
naturally designed for modeling discrete data. To develop generative SDE models
for discrete data such as biological sequences, here we introduce a diffusion
process defined in the probability simplex space with stationary distribution
being the Dirichlet distribution. This makes diffusion in continuous space
natural for modeling discrete data. We refer to this approach as Dirchlet
diffusion score model. We demonstrate that this technique can generate samples
that satisfy hard constraints using a Sudoku generation task. This generative
model can also solve Sudoku, including hard puzzles, without additional
training. Finally, we applied this approach to develop the first human promoter
DNA sequence design model and showed that designed sequences share similar
properties with natural promoter sequences.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Boost Vision Transformer with GPU-Friendly Sparsity and Quantization. (arXiv:2305.10727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10727">http://arxiv.org/abs/2305.10727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10727] Boost Vision Transformer with GPU-Friendly Sparsity and Quantization](http://arxiv.org/abs/2305.10727) #transformer</code></li>
<li>Summary: <p>The transformer extends its success from the language to the vision domain.
Because of the stacked self-attention and cross-attention blocks, the
acceleration deployment of vision transformer on GPU hardware is challenging
and also rarely studied. This paper thoroughly designs a compression scheme to
maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and
quantization. Specially, an original large model with dense weight parameters
is first pruned into a sparse one by 2:4 structured pruning, which considers
the GPU's acceleration of 2:4 structured sparse pattern with FP16 data type,
then the floating-point sparse model is further quantized into a fixed-point
one by sparse-distillation-aware quantization aware training, which considers
GPU can provide an extra speedup of 2:4 sparse calculation with integer
tensors. A mixed-strategy knowledge distillation is used during the pruning and
quantization process. The proposed compression scheme is flexible to support
supervised and unsupervised learning styles. Experiment results show GPUSQ-ViT
scheme achieves state-of-the-art compression by reducing vision transformer
models 6.4-12.7 times on model size and 30.3-62 times on FLOPs with negligible
accuracy degradation on ImageNet classification, COCO detection and ADE20K
segmentation benchmarking tasks. Moreover, GPUSQ-ViT can boost actual
deployment performance by 1.39-1.79 times and 3.22-3.43 times of latency and
throughput on A100 GPU, and 1.57-1.69 times and 2.11-2.51 times improvement of
latency and throughput on AGX Orin.
</p></li>
</ul>

<h3>Title: Multi-resolution Spatiotemporal Enhanced Transformer Denoising with Functional Diffusive GANs for Constructing Brain Effective Connectivity in MCI analysis. (arXiv:2305.10754v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10754">http://arxiv.org/abs/2305.10754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10754] Multi-resolution Spatiotemporal Enhanced Transformer Denoising with Functional Diffusive GANs for Constructing Brain Effective Connectivity in MCI analysis](http://arxiv.org/abs/2305.10754) #transformer</code></li>
<li>Summary: <p>Effective connectivity can describe the causal patterns among brain regions.
These patterns have the potential to reveal the pathological mechanism and
promote early diagnosis and effective drug development for cognitive disease.
However, the current studies mainly focus on using empirical functional time
series to calculate effective connections, which may not comprehensively
capture the complex causal relationships between brain regions. In this paper,
a novel Multi-resolution Spatiotemporal Enhanced Transformer Denoising (MSETD)
network with an adversarially functional diffusion model is proposed to map
functional magnetic resonance imaging (fMRI) into effective connectivity for
mild cognitive impairment (MCI) analysis. To be specific, the denoising
framework leverages a conditional diffusion process that progressively
translates the noise and conditioning fMRI to effective connectivity in an
end-to-end manner. To ensure reverse diffusion quality and diversity, the
multi-resolution enhanced transformer generator is designed to extract local
and global spatiotemporal features. Furthermore, a multi-scale diffusive
transformer discriminator is devised to capture the temporal patterns at
different scales for generation stability. Evaluations of the ADNI datasets
demonstrate the feasibility and efficacy of the proposed model. The proposed
model not only achieves superior prediction performance compared with other
competing methods but also identifies MCI-related causal connections that are
consistent with clinical studies.
</p></li>
</ul>

<h3>Title: Selecting Learnable Training Samples is All DETRs Need in Crowded Pedestrian Detection. (arXiv:2305.10801v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10801">http://arxiv.org/abs/2305.10801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10801] Selecting Learnable Training Samples is All DETRs Need in Crowded Pedestrian Detection](http://arxiv.org/abs/2305.10801) #transformer</code></li>
<li>Summary: <p>DEtection TRansformer (DETR) and its variants (DETRs) achieved impressive
performance in general object detection. However, in crowded pedestrian
detection, the performance of DETRs is still unsatisfactory due to the
inappropriate sample selection method which results in more false positives. To
settle the issue, we propose a simple but effective sample selection method for
DETRs, Sample Selection for Crowded Pedestrians (SSCP), which consists of the
constraint-guided label assignment scheme (CGLA) and the utilizability-aware
focal loss (UAFL). Our core idea is to select learnable samples for DETRs and
adaptively regulate the loss weights of samples based on their utilizability.
Specifically, in CGLA, we proposed a new cost function to ensure that only
learnable positive training samples are retained and the rest are negative
training samples. Further, considering the utilizability of samples, we
designed UAFL to adaptively assign different loss weights to learnable positive
samples depending on their gradient ratio and IoU. Experimental results show
that the proposed SSCP effectively improves the baselines without introducing
any overhead in inference. Especially, Iter Deformable DETR is improved to
39.7(-2.0)% MR on Crowdhuman and 31.8(-0.4)% MR on Citypersons.
</p></li>
</ul>

<h3>Title: Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10435">http://arxiv.org/abs/2305.10435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10435] Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions](http://arxiv.org/abs/2305.10435) #transformer</code></li>
<li>Summary: <p>The Generative Pre-trained Transformer models represent a notable
breakthrough in the domain of natural language processing, which is propelling
us toward the development of machines that can understand and communicate using
language in a manner that closely resembles that of humans. Generative
Pre-trained Transformer models are based on the transformer architecture, a
deep neural network designed for natural language processing tasks. Due to
their impressive performance on natural language processing tasks and ability
to effectively converse, Generative Pre-trained Transformer models have gained
significant popularity among researchers and industrial communities, making
them one of the most widely used and effective models in natural language
processing and related fields, which motivated to conduct this review. This
review provides a detailed overview of the Generative Pre-trained Transformer,
including its architecture, working process, training procedures, enabling
technologies, and its impact on various applications. In this review, we also
explored the potential challenges and limitations of a Generative Pre-trained
Transformer. Furthermore, we discuss potential solutions and future directions.
Overall, this paper aims to provide a comprehensive understanding of Generative
Pre-trained Transformers, enabling technologies, their impact on various
applications, emerging challenges, and potential solutions.
</p></li>
</ul>

<h3>Title: Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions. (arXiv:2305.10614v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10614">http://arxiv.org/abs/2305.10614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10614] Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions](http://arxiv.org/abs/2305.10614) #transformer</code></li>
<li>Summary: <p>While there is much recent interest in studying why Transformer-based large
language models make predictions the way they do, the complex computations
performed within each layer have traditionally posed a strong bottleneck. To
mitigate this shortcoming, this work presents a linear decomposition of final
hidden states from autoregressive language models based on each initial input
token, which is exact for virtually all contemporary Transformer architectures.
This decomposition allows the definition of probability distributions that
ablate the contribution of specific input tokens, which can be used to analyze
their influence on model probabilities over a sequence of upcoming words with
only one forward pass from the model. Using the change in next-word probability
as a measure of importance, this work first examines which context words make
the biggest contribution to language model predictions. Regression experiments
suggest that Transformer-based language models rely primarily on collocational
associations, followed by linguistic factors such as syntactic dependencies and
coreference relationships in making next-word predictions. Additionally,
analyses using these measures to predict syntactic dependencies and coreferent
mention spans show that collocational association and repetitions of the same
token respectively, largely explain the language model's predictions on the
tasks.
</p></li>
</ul>

<h3>Title: Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10833">http://arxiv.org/abs/2305.10833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10833] Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants](http://arxiv.org/abs/2305.10833) #transformer</code></li>
<li>Summary: <p>The domain of Botany is rich with metaphorical terms. Those terms play an
important role in the description and identification of flowers and plants.
However, the identification of such terms in discourse is an arduous task. This
leads in some cases to committing errors during translation processes and
lexicographic tasks. The process is even more challenging when it comes to
machine translation, both in the cases of single-word terms and multi-word
terms. One of the recent concerns of Natural Language Processing (NLP)
applications and Machine Translation (MT) technologies is the automatic
identification of metaphor-based words in discourse through Deep Learning (DL).
In this study, we seek to fill this gap through the use of thirteen popular
transformer based models, as well as ChatGPT, and we show that discriminative
models perform better than GPT-3.5 model with our best performer reporting
92.2349% F1 score in metaphoric flower and plant names identification task.
</p></li>
</ul>

<h3>Title: Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10835">http://arxiv.org/abs/2305.10835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10835] Ahead-of-Time P-Tuning](http://arxiv.org/abs/2305.10835) #transformer</code></li>
<li>Summary: <p>In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel
parameter-efficient fine-tuning method for pre-trained Language Models (LMs)
that adds input-dependent bias before each Transformer layer. We evaluate AoT
P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa
models, showing that it outperforms BitFit and is comparable or better than
other baseline methods for efficient fine-tuning. Additionally, we assess the
inference overhead of AoT P-Tuning and demonstrate that it introduces
negligible overhead compared to established baseline methods. Our method
enables multi-task inference with a single backbone LM, making it a practical
solution for real-world applications.
</p></li>
</ul>

<h3>Title: A Lexical-aware Non-autoregressive Transformer-based ASR Model. (arXiv:2305.10839v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10839">http://arxiv.org/abs/2305.10839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10839] A Lexical-aware Non-autoregressive Transformer-based ASR Model](http://arxiv.org/abs/2305.10839) #transformer</code></li>
<li>Summary: <p>Non-autoregressive automatic speech recognition (ASR) has become a mainstream
of ASR modeling because of its fast decoding speed and satisfactory result. To
further boost the performance, relaxing the conditional independence assumption
and cascading large-scaled pre-trained models are two active research
directions. In addition to these strategies, we propose a lexical-aware
non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of
an acoustic encoder, a speech-text shared encoder, and a speech-text shared
decoder. The acoustic encoder is used to process the input speech features as
usual, and the speech-text shared encoder and decoder are designed to train
speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR
model aware of lexical information, so the resulting model is expected to
achieve better results by leveraging the learned linguistic knowledge. A series
of experiments are conducted on the AISHELL-1, CSJ, and TEDLIUM 2 datasets.
According to the experiments, the proposed LA-NAT can provide superior results
than other recently proposed non-autoregressive ASR models. In addition, LA-NAT
is a relatively compact model than most non-autoregressive ASR models, and it
is about 58 times faster than the classic autoregressive model.
</p></li>
</ul>

<h3>Title: TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model. (arXiv:2305.10845v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10845">http://arxiv.org/abs/2305.10845</a></li>
<li>Code URL: <a href="https://github.com/pkhdipraja/tapir">https://github.com/pkhdipraja/tapir</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10845] TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model](http://arxiv.org/abs/2305.10845) #transformer</code></li>
<li>Summary: <p>Language is by its very nature incremental in how it is produced and
processed. This property can be exploited by NLP systems to produce fast
responses, which has been shown to be beneficial for real-time interactive
applications. Recent neural network-based approaches for incremental processing
mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct
earlier output, which can be necessary in incremental processing).
Transformers, on the other hand, consume whole sequences, and hence are by
nature non-incremental. A restart-incremental interface that repeatedly passes
longer input prefixes can be used to obtain partial outputs, while providing
the ability to revise. However, this method becomes costly as the sentence
grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision
(TAPIR) and introduce a method to obtain an incremental supervision signal for
learning an adaptive revision policy. Experimental results on sequence
labelling show that our model has better incremental performance and faster
inference speed compared to restart-incremental Transformers, while showing
little degradation on full sequences.
</p></li>
</ul>

<h3>Title: Less is More! A slim architecture for optimal language translation. (arXiv:2305.10991v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10991">http://arxiv.org/abs/2305.10991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10991] Less is More! A slim architecture for optimal language translation](http://arxiv.org/abs/2305.10991) #transformer</code></li>
<li>Summary: <p>The softmax attention mechanism has emerged as a noteworthy development in
the field of Artificial Intelligence research, building on the successes of
Transformer-based architectures. However, their ever increasing sizes
necessitate ever increasing computational memory, that limits their usage. We
propose KgV, a sigmoid gating mechanism that, in conjunction with softmax
attention, significantly boosts performance without increasing architecture
size. To amend the size requirements, we leverage Tensor Chains to identify and
prune the excess parameters. We find that such excess resides primarily within
the embedding layer, and not in the output linear layer. To further improve
embedding and significantly reduce parameters, we introduce H-SoftPOS, a
hierarchical embedding layer which simultaneously enhances performance.
Remarkably, on the WMT14 English-German validation set, our approach yields a
threefold reduction in perplexity, surpassing the current state-of-the-art,
while reducing parameter counts also by a factor of 3. When we further reduce
the number of parameters up to sevenfold, we can still achieve a 21\% decrease
in perplexity with respect to the baseline Transformer. To understand
generalization capabilities, we conduct experiments on the 7 language pairs of
the WMT17 dataset. Our method outperforms existing techniques in terms of test
loss while simultaneously halving the number of parameters. Moreover, we
observe a 70 times reduction in variance with respect to the prior
state-of-the-art. In conclusion, our proposed method yields significant
improvements in performance and much lower memory cost. We call the resulting
architecture Anthe.
</p></li>
</ul>

<h3>Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks. (arXiv:2305.11073v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11073">http://arxiv.org/abs/2305.11073</a></li>
<li>Code URL: <a href="https://github.com/espnet/espnet">https://github.com/espnet/espnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11073] A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks](http://arxiv.org/abs/2305.11073) #transformer</code></li>
<li>Summary: <p>Conformer, a convolution-augmented Transformer variant, has become the de
facto encoder architecture for speech processing due to its superior
performance in various tasks, including automatic speech recognition (ASR),
speech translation (ST) and spoken language understanding (SLU). Recently, a
new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech
ASR benchmark, making it promising for more general speech applications. This
work compares E-Branchformer and Conformer through extensive experiments using
different types of end-to-end sequence-to-sequence models. Results demonstrate
that E-Branchformer achieves comparable or better performance than Conformer in
almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while
being more stable during training. We will release our training configurations
and pre-trained models for reproducibility, which can benefit the speech
community.
</p></li>
</ul>

<h3>Title: Cooperation Is All You Need. (arXiv:2305.10449v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10449">http://arxiv.org/abs/2305.10449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10449] Cooperation Is All You Need](http://arxiv.org/abs/2305.10449) #transformer</code></li>
<li>Summary: <p>Going beyond 'dendritic democracy', we introduce a 'democracy of local
processors', termed Cooperator. Here we compare their capabilities when used in
permutation-invariant neural networks for reinforcement learning (RL), with
machine learning algorithms based on Transformers, such as ChatGPT.
Transformers are based on the long-standing conception of integrate-and-fire
'point' neurons, whereas Cooperator is inspired by recent neurobiological
breakthroughs suggesting that the cellular foundations of mental life depend on
context-sensitive pyramidal neurons in the neocortex which have two
functionally distinct points. We show that when used for RL, an algorithm based
on Cooperator learns far quicker than that based on Transformer, even while
having the same number of parameters.
</p></li>
</ul>

<h3>Title: Short-Term Electricity Load Forecasting Using the Temporal Fusion Transformer: Effect of Grid Hierarchies and Data Sources. (arXiv:2305.10559v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10559">http://arxiv.org/abs/2305.10559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10559] Short-Term Electricity Load Forecasting Using the Temporal Fusion Transformer: Effect of Grid Hierarchies and Data Sources](http://arxiv.org/abs/2305.10559) #transformer</code></li>
<li>Summary: <p>Recent developments related to the energy transition pose particular
challenges for distribution grids. Hence, precise load forecasts become more
and more important for effective grid management. Novel modeling approaches
such as the Transformer architecture, in particular the Temporal Fusion
Transformer (TFT), have emerged as promising methods for time series
forecasting. To date, just a handful of studies apply TFTs to electricity load
forecasting problems, mostly considering only single datasets and a few
covariates. Therefore, we examine the potential of the TFT architecture for
hourly short-term load forecasting across different time horizons (day-ahead
and week-ahead) and network levels (grid and substation level). We find that
the TFT architecture does not offer higher predictive performance than a
state-of-the-art LSTM model for day-ahead forecasting on the entire grid.
However, the results display significant improvements for the TFT when applied
at the substation level with a subsequent aggregation to the upper grid-level,
resulting in a prediction error of 2.43% (MAPE) for the best-performing
scenario. In addition, the TFT appears to offer remarkable improvements over
the LSTM approach for week-ahead forecasting (yielding a predictive error of
2.52% (MAPE) at the lowest). We outline avenues for future research using the
TFT approach for load forecasting, including the exploration of various grid
levels (e.g., grid, substation, and household level).
</p></li>
</ul>

<h3>Title: A Survey on Time-Series Pre-Trained Models. (arXiv:2305.10716v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10716">http://arxiv.org/abs/2305.10716</a></li>
<li>Code URL: <a href="https://github.com/qianlima-lab/time-series-ptms">https://github.com/qianlima-lab/time-series-ptms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10716] A Survey on Time-Series Pre-Trained Models](http://arxiv.org/abs/2305.10716) #transformer</code></li>
<li>Summary: <p>Time-Series Mining (TSM) is an important research area since it shows great
potential in practical applications. Deep learning models that rely on massive
labeled data have been utilized for TSM successfully. However, constructing a
large-scale well-labeled dataset is difficult due to data annotation costs.
Recently, Pre-Trained Models have gradually attracted attention in the time
series domain due to their remarkable performance in computer vision and
natural language processing. In this survey, we provide a comprehensive review
of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding,
applying, and studying TS-PTMs. Specifically, we first briefly introduce the
typical deep learning models employed in TSM. Then, we give an overview of
TS-PTMs according to the pre-training techniques. The main categories we
explore include supervised, unsupervised, and self-supervised TS-PTMs. Further,
extensive experiments are conducted to analyze the advantages and disadvantages
of transfer learning strategies, Transformer-based models, and representative
TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future
work.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Smiling Women Pitching Down: Auditing Representational and Presentational Gender Biases in Image Generative AI. (arXiv:2305.10566v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10566">http://arxiv.org/abs/2305.10566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10566] Smiling Women Pitching Down: Auditing Representational and Presentational Gender Biases in Image Generative AI](http://arxiv.org/abs/2305.10566) #generative</code></li>
<li>Summary: <p>Generative AI models like DALL-E 2 can interpret textual prompts and generate
high-quality images exhibiting human creativity. Though public enthusiasm is
booming, systematic auditing of potential gender biases in AI-generated images
remains scarce. We addressed this gap by examining the prevalence of two
occupational gender biases (representational and presentational biases) in
15,300 DALL-E 2 images spanning 153 occupations, and assessed potential bias
amplification by benchmarking against 2021 census labor statistics and Google
Images. Our findings reveal that DALL-E 2 underrepresents women in
male-dominated fields while overrepresenting them in female-dominated
occupations. Additionally, DALL-E 2 images tend to depict more women than men
with smiling faces and downward-pitching heads, particularly in
female-dominated (vs. male-dominated) occupations. Our computational algorithm
auditing study demonstrates more pronounced representational and presentational
biases in DALL-E 2 compared to Google Images and calls for feminist
interventions to prevent such bias-laden AI-generated images to feedback into
the media ecology.
</p></li>
</ul>

<h3>Title: StawGAN: Structural-Aware Generative Adversarial Networks for Infrared Image Translation. (arXiv:2305.10882v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10882">http://arxiv.org/abs/2305.10882</a></li>
<li>Code URL: <a href="https://github.com/luigisigillo/stawgan">https://github.com/luigisigillo/stawgan</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10882] StawGAN: Structural-Aware Generative Adversarial Networks for Infrared Image Translation](http://arxiv.org/abs/2305.10882) #generative</code></li>
<li>Summary: <p>This paper addresses the problem of translating night-time thermal infrared
images, which are the most adopted image modalities to analyze night-time
scenes, to daytime color images (NTIT2DC), which provide better perceptions of
objects. We introduce a novel model that focuses on enhancing the quality of
the target generation without merely colorizing it. The proposed structural
aware (StawGAN) enables the translation of better-shaped and high-definition
objects in the target domain. We test our model on aerial images of the
DroneVeichle dataset containing RGB-IR paired images. The proposed approach
produces a more accurate translation with respect to other state-of-the-art
image translation models. The source code is available at
https://github.com/LuigiSigillo/StawGAN
</p></li>
</ul>

<h3>Title: Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold. (arXiv:2305.10973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10973">http://arxiv.org/abs/2305.10973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10973] Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](http://arxiv.org/abs/2305.10973) #generative</code></li>
<li>Summary: <p>Synthesizing visual content that meets users' needs often requires flexible
and precise controllability of the pose, shape, expression, and layout of the
generated objects. Existing approaches gain controllability of generative
adversarial networks (GANs) via manually annotated training data or a prior 3D
model, which often lack flexibility, precision, and generality. In this work,
we study a powerful yet much less explored way of controlling GANs, that is, to
"drag" any points of the image to precisely reach target points in a
user-interactive manner, as shown in Fig.1. To achieve this, we propose
DragGAN, which consists of two main components: 1) a feature-based motion
supervision that drives the handle point to move towards the target position,
and 2) a new point tracking approach that leverages the discriminative
generator features to keep localizing the position of the handle points.
Through DragGAN, anyone can deform an image with precise control over where
pixels go, thus manipulating the pose, shape, expression, and layout of diverse
categories such as animals, cars, humans, landscapes, etc. As these
manipulations are performed on the learned generative image manifold of a GAN,
they tend to produce realistic outputs even for challenging scenarios such as
hallucinating occluded content and deforming shapes that consistently follow
the object's rigidity. Both qualitative and quantitative comparisons
demonstrate the advantage of DragGAN over prior approaches in the tasks of
image manipulation and point tracking. We also showcase the manipulation of
real images through GAN inversion.
</p></li>
</ul>

<h3>Title: Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10519">http://arxiv.org/abs/2305.10519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10519] Statistical Knowledge Assessment for Generative Language Models](http://arxiv.org/abs/2305.10519) #generative</code></li>
<li>Summary: <p>Generative Language Models (GLMs) have demonstrated capabilities to store
factual knowledge and answer queries efficiently. Given varying prompts, does a
GLM consistently generate factually correct answers? In this paper, we
introduce a statistical knowledge assessment framework guided by latent
variables and the KaRR metric, which quantifies a model's knowledge by
computing its continuous probability across diverse text forms. We conduct a
comprehensive comparison of knowledge across 14 GLMs using our framework,
including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment
encompasses 600 relation types and exhibits a strong correlation (0.43
Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge
in GLMs with the same backbone architecture adheres to the scaling law, and
that tuning on instruction-following data may compromise the model's ability to
generate factually correct text consistently.
</p></li>
</ul>

<h3>Title: MolXPT: Wrapping Molecules with Text for Generative Pre-training. (arXiv:2305.10688v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10688">http://arxiv.org/abs/2305.10688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10688] MolXPT: Wrapping Molecules with Text for Generative Pre-training](http://arxiv.org/abs/2305.10688) #generative</code></li>
<li>Summary: <p>Generative pre-trained Transformer (GPT) has demonstrates its great success
in natural language processing and related techniques have been adapted into
molecular modeling. Considering that text is the most important record for
scientific discovery, in this paper, we propose MolXPT, a unified language
model of text and molecules pre-trained on SMILES (a sequence representation of
molecules) wrapped by text. Briefly, we detect the molecule names in each
sequence and replace them to the corresponding SMILES. In this way, the SMILES
could leverage the information from surrounding text, and vice versa. The above
wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem
are all fed into a language model for pre-training. Experimental results
demonstrate that MolXPT outperforms strong baselines of molecular property
prediction on MoleculeNet, performs comparably to the best model in
text-molecule translation while using less than half of its parameters, and
enables zero-shot molecular generation without finetuning.
</p></li>
</ul>

<h3>Title: How does agency impact human-AI collaborative design space exploration? A case study on ship design with deep generative models. (arXiv:2305.10451v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10451">http://arxiv.org/abs/2305.10451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10451] How does agency impact human-AI collaborative design space exploration? A case study on ship design with deep generative models](http://arxiv.org/abs/2305.10451) #generative</code></li>
<li>Summary: <p>Typical parametric approaches restrict the exploration of diverse designs by
generating variations based on a baseline design. In contrast, generative
models provide a solution by leveraging existing designs to create compact yet
diverse generative design spaces (GDSs). However, the effectiveness of current
exploration methods in complex GDSs, especially in ship hull design, remains
unclear. To that end, we first construct a GDS using a generative adversarial
network, trained on 52,591 designs of various ship types. Next, we constructed
three modes of exploration, random (REM), semi-automated (SAEM) and automated
(AEM), with varying levels of user involvement to explore GDS for novel and
optimised designs. In REM, users manually explore the GDS based on intuition.
In SAEM, both the users and optimiser drive the exploration. The optimiser
focuses on exploring a diverse set of optimised designs, while the user directs
the exploration towards their design preference. AEM uses an optimiser to
search for the global optimum based on design performance. Our results revealed
that REM generates the most diverse designs, followed by SAEM and AEM. However,
the SAEM and AEM produce better-performing designs. Specifically, SAEM is the
most effective in exploring designs with a high trade-off between novelty and
performance. In conclusion, our study highlights the need for innovative
exploration approaches to fully harness the potential of GDS in design
optimisation.
</p></li>
</ul>

<h3>Title: Massively Parallel Reweighted Wake-Sleep. (arXiv:2305.11022v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11022">http://arxiv.org/abs/2305.11022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11022] Massively Parallel Reweighted Wake-Sleep](http://arxiv.org/abs/2305.11022) #generative</code></li>
<li>Summary: <p>Reweighted wake-sleep (RWS) is a machine learning method for performing
Bayesian inference in a very general class of models. RWS draws $K$ samples
from an underlying approximate posterior, then uses importance weighting to
provide a better estimate of the true posterior. RWS then updates its
approximate posterior towards the importance-weighted estimate of the true
posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that
the number of samples required for effective importance weighting is
exponential in the number of latent variables. Attaining such a large number of
importance samples is intractable in all but the smallest models. Here, we
develop massively parallel RWS, which circumvents this issue by drawing $K$
samples of all $n$ latent variables, and individually reasoning about all $K^n$
possible combinations of samples. While reasoning about $K^n$ combinations
might seem intractable, the required computations can be performed in
polynomial time by exploiting conditional independencies in the generative
model. We show considerable improvements over standard "global" RWS, which
draws $K$ samples from the full joint.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts. (arXiv:2305.10799v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10799">http://arxiv.org/abs/2305.10799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10799] MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts](http://arxiv.org/abs/2305.10799) #large language model</code></li>
<li>Summary: <p>Vision-language pre-training (VLP) models have been demonstrated to be
effective in many computer vision applications. In this paper, we consider
developing a VLP model in the medical domain for making computer-aided
diagnoses (CAD) based on image scans and text descriptions in electronic health
records, as done in practice. To achieve our goal, we present a lightweight CAD
system MedBLIP, a new paradigm for bootstrapping VLP from off-the-shelf frozen
pre-trained image encoders and frozen large language models. We design a
MedQFormer module to bridge the gap between 3D medical images and 2D
pre-trained image encoders and language models as well. To evaluate the
effectiveness of our MedBLIP, we collect more than 30,000 image volumes from
five public Alzheimer's disease (AD) datasets, i.e., ADNI, NACC, OASIS, AIBL,
and MIRIAD. On this largest AD dataset we know, our model achieves the SOTA
performance on the zero-shot classification of healthy, mild cognitive
impairment (MCI), and AD subjects, and shows its capability of making medical
visual question answering (VQA). The code and pre-trained models is available
online: https://github.com/Qybc/MedBLIP.
</p></li>
</ul>

<h3>Title: X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models. (arXiv:2305.10843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10843">http://arxiv.org/abs/2305.10843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10843] X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models](http://arxiv.org/abs/2305.10843) #large language model</code></li>
<li>Summary: <p>This paper introduces a novel explainable image quality evaluation approach
called X-IQE, which leverages visual large language models (LLMs) to evaluate
text-to-image generation methods by generating textual explanations. X-IQE
utilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce
self-consistent, unbiased texts that are highly correlated with human
evaluation. It offers several advantages, including the ability to distinguish
between real and generated images, evaluate text-image alignment, and assess
image aesthetics without requiring model training or fine-tuning. X-IQE is more
cost-effective and efficient compared to human evaluation, while significantly
enhancing the transparency and explainability of deep image quality evaluation
models. We validate the effectiveness of our method as a benchmark using images
generated by prevalent diffusion models. X-IQE demonstrates similar performance
to state-of-the-art (SOTA) evaluation methods on COCO Caption, while overcoming
the limitations of previous evaluation models on DrawBench, particularly in
handling ambiguous generation prompts and text recognition in generated images.
Project website:
https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models
</p></li>
</ul>

<h3>Title: SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues. (arXiv:2305.10436v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10436">http://arxiv.org/abs/2305.10436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10436] SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues](http://arxiv.org/abs/2305.10436) #large language model</code></li>
<li>Summary: <p>In second language vocabulary learning, existing works have primarily focused
on either the learning interface or scheduling personalized retrieval practices
to maximize memory retention. However, the learning content, i.e., the
information presented on flashcards, has mostly remained constant. Keyword
mnemonic is a notable learning strategy that relates new vocabulary to existing
knowledge by building an acoustic and imagery link using a keyword that sounds
alike. Beyond that, producing verbal and visual cues associated with the
keyword to facilitate building these links requires a manual process and is not
scalable. In this paper, we explore an opportunity to use large language models
to automatically generate verbal and visual cues for keyword mnemonics. Our
approach, an end-to-end pipeline for auto-generating verbal and visual cues,
can automatically generate highly memorable cues. We investigate the
effectiveness of our approach via a human participant experiment by comparing
it with manually generated cues.
</p></li>
</ul>

<h3>Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10601">http://arxiv.org/abs/2305.10601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large Language Models](http://arxiv.org/abs/2305.10601) #large language model</code></li>
<li>Summary: <p>Language models are increasingly being deployed for general problem solving
across a wide range of tasks, but are still confined to token-level,
left-to-right decision-making processes during inference. This means they can
fall short in tasks that require exploration, strategic lookahead, or where
initial decisions play a pivotal role. To surmount these challenges, we
introduce a new framework for language model inference, Tree of Thoughts (ToT),
which generalizes over the popular Chain of Thought approach to prompting
language models, and enables exploration over coherent units of text (thoughts)
that serve as intermediate steps toward problem solving. ToT allows LMs to
perform deliberate decision making by considering multiple different reasoning
paths and self-evaluating choices to decide the next course of action, as well
as looking ahead or backtracking when necessary to make global choices. Our
experiments show that ToT significantly enhances language models'
problem-solving abilities on three novel tasks requiring non-trivial planning
or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in
Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of
tasks, our method achieved a success rate of 74%. Code repo with all prompts:
https://github.com/ysymyth/tree-of-thought-llm.
</p></li>
</ul>

<h3>Title: Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning. (arXiv:2305.10613v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10613">http://arxiv.org/abs/2305.10613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10613] Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning](http://arxiv.org/abs/2305.10613) #large language model</code></li>
<li>Summary: <p>Temporal knowledge graph (TKG) forecasting benchmarks challenge models to
predict future facts using knowledge of past facts. In this paper, we apply
large language models (LLMs) to these benchmarks using in-context learning
(ICL). We investigate whether and to what extent LLMs can be used for TKG
forecasting, especially without any fine-tuning or explicit modules for
capturing structural and temporal information. For our experiments, we present
a framework that converts relevant historical facts into prompts and generates
ranked predictions using token probabilities. Surprisingly, we observe that
LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully
designed and trained for TKG forecasting. Our extensive evaluation presents
performances across several models and datasets with different characteristics,
compares alternative heuristics for preparing contextual information, and
contrasts to prominent TKG methods and simple frequency and recency baselines.
We also discover that using numerical indices instead of entity/relation names,
i.e., hiding semantic information, does not significantly affect the
performance ($\pm$0.4\% Hit@1). This shows that prior semantic knowledge is
unnecessary; instead, LLMs can leverage the existing patterns in the context to
achieve such performance. Our analysis also reveals that ICL enables LLMs to
learn irregular patterns from the historical context, going beyond simple
predictions based on common or recent information.
</p></li>
</ul>

<h3>Title: Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10626">http://arxiv.org/abs/2305.10626</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10626] Language Models Meet World Models: Embodied Experiences Enhance Language Models](http://arxiv.org/abs/2305.10626) #large language model</code></li>
<li>Summary: <p>While large language models (LMs) have shown remarkable capabilities across
numerous tasks, they often struggle with simple reasoning and planning in
physical environments, such as understanding object permanence or planning
household activities. The limitation arises from the fact that LMs are trained
only on written text and miss essential embodied knowledge and skills. In this
paper, we propose a new paradigm of enhancing LMs by finetuning them with world
models, to gain diverse embodied knowledge while retaining their general
language capabilities. Our approach deploys an embodied agent in a world model,
particularly a simulator of the physical world (VirtualHome), and acquires a
diverse set of embodied experiences through both goal-oriented planning and
random exploration. These experiences are then used to finetune LMs to teach
diverse abilities of reasoning and acting in the physical world, e.g., planning
and completing goals, object permanence and tracking, etc. Moreover, it is
desirable to preserve the generality of LMs during finetuning, which
facilitates generalizing the embodied knowledge across tasks rather than being
tied to specific simulations. We thus further introduce the classical elastic
weight consolidation (EWC) for selective weight updates, combined with low-rank
adapters (LoRA) for training efficiency. Extensive experiments show our
approach substantially improves base LMs on 18 downstream tasks by 64.28% on
average. In particular, the small LMs (1.3B and 6B) enhanced by our approach
match or even outperform much larger LMs (e.g., ChatGPT).
</p></li>
</ul>

<h3>Title: Are Large Language Models Fit For Guided Reading?. (arXiv:2305.10645v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10645">http://arxiv.org/abs/2305.10645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10645] Are Large Language Models Fit For Guided Reading?](http://arxiv.org/abs/2305.10645) #large language model</code></li>
<li>Summary: <p>This paper looks at the ability of large language models to participate in
educational guided reading. We specifically, evaluate their ability to generate
meaningful questions from the input text, generate diverse questions both in
terms of content coverage and difficulty of the questions and evaluate their
ability to recommend part of the text that a student should re-read based on
the student's responses to the questions. Based on our evaluation of ChatGPT
and Bard, we report that,
</p></li>
</ul>

<p>1) Large language models are able to generate high quality meaningful
questions that have high correlation with the input text, 2) They generate
diverse question that cover most topics in the input text even though this
ability is significantly degraded as the input text increases, 3)The large
language models are able to generate both low and high cognitive questions even
though they are significantly biased toward low cognitive question, 4) They are
able to effectively summarize responses and extract a portion of text that
should be re-read.
</p>

<h3>Title: ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10703">http://arxiv.org/abs/2305.10703</a></li>
<li>Code URL: <a href="https://github.com/yueyu1030/ReGen">https://github.com/yueyu1030/ReGen</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10703] ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval](http://arxiv.org/abs/2305.10703) #large language model</code></li>
<li>Summary: <p>With the development of large language models (LLMs), zero-shot learning has
attracted much attention for various NLP tasks. Different from prior works that
generate training data with billion-scale natural language generation (NLG)
models, we propose a retrieval-enhanced framework to create training data from
a general-domain unlabeled corpus. To realize this, we first conduct
contrastive pretraining to learn an unsupervised dense retriever for extracting
the most relevant documents using class-descriptive verbalizers. We then
further propose two simple strategies, namely Verbalizer Augmentation with
Demonstrations and Self-consistency Guided Filtering to improve the topic
coverage of the dataset while removing noisy examples. Experiments on nine
datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines
and saves around 70% of the time compared to baselines using large NLG models.
Besides, REGEN can be naturally integrated with recently proposed large
language models to boost performance.
</p></li>
</ul>

<h3>Title: Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10847">http://arxiv.org/abs/2305.10847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10847] Large Language Models can be Guided to Evade AI-Generated Text Detection](http://arxiv.org/abs/2305.10847) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated exceptional performance in a
variety of tasks, including essay writing and question answering. However, it
is crucial to address the potential misuse of these models, which can lead to
detrimental outcomes such as plagiarism and spamming. Recently, several
detectors have been proposed, including fine-tuned classifiers and various
statistical methods. In this study, we reveal that with the aid of carefully
crafted prompts, LLMs can effectively evade these detection systems. We propose
a novel Substitution-based In-Context example Optimization method (SICO) to
automatically generate such prompts. On three real-world tasks where LLMs can
be misused, SICO successfully enables ChatGPT to evade six existing detectors,
causing a significant 0.54 AUC drop on average. Surprisingly, in most cases
these detectors perform even worse than random classifiers. These results
firmly reveal the vulnerability of existing detectors. Finally, the strong
performance of SICO suggests itself as a reliable evaluation protocol for any
new detector in this field.
</p></li>
</ul>

<h3>Title: The Web Can Be Your Oyster for Improving Large Language Models. (arXiv:2305.10998v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10998">http://arxiv.org/abs/2305.10998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10998] The Web Can Be Your Oyster for Improving Large Language Models](http://arxiv.org/abs/2305.10998) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) encode a large amount of world knowledge.
However, as such knowledge is frozen at the time of model training, the models
become static and limited by the training data at that time. In order to
further improve the capacity of LLMs for knowledge-intensive tasks, we consider
augmenting LLMs with the large-scale web using search engine. Unlike previous
augmentation sources (e.g., Wikipedia data dump), the web provides broader,
more comprehensive and constantly updated information. In this paper, we
present a web-augmented LLM UNIWEB, which is trained over 16
knowledge-intensive tasks in a unified text-to-text format. Instead of simply
using the retrieved contents from web, our approach has made two major
improvements. Firstly, we propose an adaptive search engine assisted learning
method that can self-evaluate the confidence level of LLM's predictions, and
adaptively determine when to refer to the web for more data, which can avoid
useless or noisy augmentation from web. Secondly, we design a pretraining task,
i.e., continual knowledge learning, based on salient spans prediction, to
reduce the discrepancy between the encoded and retrieved knowledge. Experiments
on a wide range of knowledge-intensive tasks show that our model significantly
outperforms previous retrieval-augmented methods.
</p></li>
</ul>

<h3>Title: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities. (arXiv:2305.11000v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11000">http://arxiv.org/abs/2305.11000</a></li>
<li>Code URL: <a href="https://github.com/0nutation/speechgpt">https://github.com/0nutation/speechgpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11000] SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](http://arxiv.org/abs/2305.11000) #large language model</code></li>
<li>Summary: <p>Multi-modal large language models are regarded as a crucial step towards
Artificial General Intelligence (AGI) and have garnered significant interest
with the emergence of ChatGPT. However, current speech-language models
typically adopt the cascade paradigm, preventing inter-modal knowledge
transfer. In this paper, we propose SpeechGPT, a large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and
generating multi-model content. With discrete speech representations, we first
construct SpeechInstruct, a large-scale cross-modal speech instruction dataset.
Additionally, we employ a three-stage training strategy that includes
modality-adaptation pre-training, cross-modal instruction fine-tuning, and
chain-of-modality instruction fine-tuning. The experimental results demonstrate
that SpeechGPT has an impressive capacity to follow multi-modal human
instructions and highlight the potential of handling multiple modalities with
one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.
</p></li>
</ul>

<h3>Title: ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation. (arXiv:2305.10838v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10838">http://arxiv.org/abs/2305.10838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10838] ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation](http://arxiv.org/abs/2305.10838) #large language model</code></li>
<li>Summary: <p>Recent years have witnessed the growing popularity of domain-specific
accelerators (DSAs), such as Google's TPUs, for accelerating various
applications such as deep learning, search, autonomous driving, etc. To
facilitate DSA designs, high-level synthesis (HLS) is used, which allows a
developer to compile a high-level description in the form of software code in C
and C++ into a design in low-level hardware description languages (such as VHDL
or Verilog) and eventually synthesized into a DSA on an ASIC
(application-specific integrated circuit) or FPGA (field-programmable gate
arrays). However, existing HLS tools still require microarchitecture decisions,
expressed in terms of pragmas (such as directives for parallelization and
pipelining). To enable more people to design DSAs, it is desirable to automate
such decisions with the help of deep learning for predicting the quality of HLS
designs. This requires us a deeper understanding of the program, which is a
combination of original code and pragmas. Naturally, these programs can be
considered as sequence data, for which large language models (LLM) can help. In
addition, these programs can be compiled and converted into a control data flow
graph (CDFG), and the compiler also provides fine-grained alignment between the
code tokens and the CDFG nodes. However, existing works either fail to leverage
both modalities or combine the two in shallow or coarse ways. We propose ProgSG
allowing the source code sequence modality and the graph modalities to interact
with each other in a deep and fine-grained way. To alleviate the scarcity of
labeled designs, a pre-training method is proposed based on a suite of
compiler's data flow analysis tasks. Experimental results on two benchmark
datasets show the superiority of ProgSG over baseline methods that either only
consider one modality or combine the two without utilizing the alignment
information.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Object Segmentation by Mining Cross-Modal Semantics. (arXiv:2305.10469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10469">http://arxiv.org/abs/2305.10469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10469] Object Segmentation by Mining Cross-Modal Semantics](http://arxiv.org/abs/2305.10469) #segmentation</code></li>
<li>Summary: <p>Multi-sensor clues have shown promise for object segmentation, but inherent
noise in each sensor, as well as the calibration error in practice, may bias
the segmentation accuracy. In this paper, we propose a novel approach by mining
the Cross-Modal Semantics to guide the fusion and decoding of multimodal
features, with the aim of controlling the modal contribution based on relative
entropy. We explore semantics among the multimodal inputs in two aspects: the
modality-shared consistency and the modality-specific variation. Specifically,
we propose a novel network, termed XMSNet, consisting of (1) all-round
attentive fusion (AF), (2) coarse-to-fine decoder (CFD), and (3) cross-layer
self-supervision. On the one hand, the AF block explicitly dissociates the
shared and specific representation and learns to weight the modal contribution
by adjusting the proportion, region, and pattern, depending upon the quality.
On the other hand, our CFD initially decodes the shared feature and then
refines the output through specificity-aware querying. Further, we enforce
semantic consistency across the decoding layers to enable interaction across
network hierarchies, improving feature discriminability. Exhaustive comparison
on eleven datasets with depth or thermal clues, and on two challenging tasks,
namely salient and camouflage object segmentation, validate our effectiveness
in terms of both performance and robustness.
</p></li>
</ul>

<h3>Title: OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields. (arXiv:2305.10503v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10503">http://arxiv.org/abs/2305.10503</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10503] OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields](http://arxiv.org/abs/2305.10503) #segmentation</code></li>
<li>Summary: <p>The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has
led to increased interest in 3D scene editing. One important task in editing is
removing objects from a scene while ensuring visual reasonability and multiview
consistency. However, current methods face challenges such as time-consuming
object labelling, limited capability to remove specific targets, and
compromised rendering quality after removal. This paper proposes a novel
object-removing pipeline, named OR-NeRF, that can remove objects from 3D scenes
with either point or text prompts on a single view, achieving better
performance in less time than previous works. Our method uses a points
projection strategy to rapidly spread user annotations to all views,
significantly reducing the processing burden. This algorithm allows us to
leverage the recent 2D segmentation model Segment-Anything (SAM) to predict
masks with improved precision and efficiency. Additionally, we obtain colour
and depth priors through 2D inpainting methods. Finally, our algorithm employs
depth supervision and perceptual loss for scene reconstruction to maintain
consistency in geometry and appearance after object removal. Experimental
results demonstrate that our method achieves better editing quality with less
time than previous works, considering both quality and quantity.
</p></li>
</ul>

<h3>Title: Segment Any Anomaly without Training via Hybrid Prompt Regularization. (arXiv:2305.10724v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10724">http://arxiv.org/abs/2305.10724</a></li>
<li>Code URL: <a href="https://github.com/caoyunkang/segment-any-anomaly">https://github.com/caoyunkang/segment-any-anomaly</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10724] Segment Any Anomaly without Training via Hybrid Prompt Regularization](http://arxiv.org/abs/2305.10724) #segmentation</code></li>
<li>Summary: <p>We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for
zero-shot anomaly segmentation with hybrid prompt regularization to improve the
adaptability of modern foundation models. Existing anomaly segmentation models
typically rely on domain-specific fine-tuning, limiting their generalization
across countless anomaly patterns. In this work, inspired by the great
zero-shot generalization ability of foundation models like Segment Anything, we
first explore their assembly to leverage diverse multi-modal prior knowledge
for anomaly localization. For non-parameter foundation model adaptation to
anomaly segmentation, we further introduce hybrid prompts derived from domain
expert knowledge and target image context as regularization. Our proposed SAA+
model achieves state-of-the-art performance on several anomaly segmentation
benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting.
We will release the code at
\href{https://github.com/caoyunkang/Segment-Any-Anomaly}{https://github.com/caoyunkang/Segment-Any-Anomaly}.
</p></li>
</ul>

<h3>Title: Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation. (arXiv:2305.10868v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10868">http://arxiv.org/abs/2305.10868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10868] Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation](http://arxiv.org/abs/2305.10868) #segmentation</code></li>
<li>Summary: <p>Incremental few-shot semantic segmentation (IFSS) aims to incrementally
extend a semantic segmentation model to novel classes according to only a few
pixel-level annotated data, while preserving its segmentation capability on
previously learned base categories. This task faces a severe semantic-aliasing
issue between base and novel classes due to data imbalance, which makes
segmentation results unsatisfactory. To alleviate this issue, we propose the
Semantic-guided Relation Alignment and Adaptation (SRAA) method that fully
considers the guidance of prior semantic information. Specifically, we first
conduct Semantic Relation Alignment (SRA) in the base step, so as to
semantically align base class representations to their semantics. As a result,
the embeddings of base classes are constrained to have relatively low semantic
correlations to categories that are different from them. Afterwards, based on
the semantically aligned base categories, Semantic-Guided Adaptation (SGA) is
employed during the incremental learning stage. It aims to ensure affinities
between visual and semantic embeddings of encountered novel categories, thereby
making the feature representations be consistent with their semantic
information. In this way, the semantic-aliasing issue can be suppressed. We
evaluate our model on the PASCAL VOC 2012 and the COCO dataset. The
experimental results on both these two datasets exhibit its competitive
performance, which demonstrates the superiority of our method.
</p></li>
</ul>

<h3>Title: Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark. (arXiv:2305.10899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10899">http://arxiv.org/abs/2305.10899</a></li>
<li>Code URL: <a href="https://github.com/jankyee/urur">https://github.com/jankyee/urur</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10899] Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark](http://arxiv.org/abs/2305.10899) #segmentation</code></li>
<li>Summary: <p>With the increasing interest and rapid development of methods for Ultra-High
Resolution (UHR) segmentation, a large-scale benchmark covering a wide range of
scenes with full fine-grained dense annotations is urgently needed to
facilitate the field. To this end, the URUR dataset is introduced, in the
meaning of Ultra-High Resolution dataset with Ultra-Rich Context. As the name
suggests, URUR contains amounts of images with high enough resolution (3,008
images of size 5,120x5,120), a wide range of complex scenes (from 63 cities),
rich-enough context (1 million instances with 8 categories) and fine-grained
annotations (about 80 billion manually annotated pixels), which is far superior
to all the existing UHR datasets including DeepGlobe, Inria Aerial, UDD, etc..
Moreover, we also propose WSDNet, a more efficient and effective framework for
UHR segmentation especially with ultra-rich context. Specifically, multi-level
Discrete Wavelet Transform (DWT) is naturally integrated to release computation
burden while preserve more spatial details, along with a Wavelet Smooth Loss
(WSL) to reconstruct original structured context and texture with a smooth
constrain. Experiments on several UHR datasets demonstrate its state-of-the-art
performance. The dataset is available at https://github.com/jankyee/URUR.
</p></li>
</ul>

<h3>Title: Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping. (arXiv:2305.11003v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11003">http://arxiv.org/abs/2305.11003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11003] Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping](http://arxiv.org/abs/2305.11003) #segmentation</code></li>
<li>Summary: <p>Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment
objects well blended with surrounding environments using sparsely-annotated
data for model training. It remains a challenging task since (1) it is hard to
distinguish concealed objects from the background due to the intrinsic
similarity and (2) the sparsely-annotated training data only provide weak
supervision for model learning. In this paper, we propose a new WSCOS method to
address these two challenges. To tackle the intrinsic similarity challenge, we
design a multi-scale feature grouping module that first groups features at
different granularities and then aggregates these grouping results. By grouping
similar features together, it encourages segmentation coherence, helping obtain
complete segmentation results for both single and multiple-object images. For
the weak supervision challenge, we utilize the recently-proposed vision
foundation model, Segment Anything Model (SAM), and use the provided sparse
annotations as prompts to generate segmentation masks, which are used to train
the model. To alleviate the impact of low-quality segmentation masks, we
further propose a series of strategies, including multi-augmentation result
ensemble, entropy-based pixel-level weighting, and entropy-based image-level
selection. These strategies help provide more reliable supervision to train the
segmentation model. We verify the effectiveness of our method on various WSCOS
tasks, and experiments demonstrate that our method achieves state-of-the-art
performance on these tasks.
</p></li>
</ul>

<h3>Title: SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation. (arXiv:2305.11012v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11012">http://arxiv.org/abs/2305.11012</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11012] SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation](http://arxiv.org/abs/2305.11012) #segmentation</code></li>
<li>Summary: <p>Recent advances in deep learning-based medical image segmentation studies
achieve nearly human-level performance in fully supervised manner. However,
acquiring pixel-level expert annotations is extremely expensive and laborious
in medical imaging fields. Unsupervised domain adaptation (UDA) can alleviate
this problem, which makes it possible to use annotated data in one imaging
modality to train a network that can successfully perform segmentation on
target imaging modality with no labels. In this work, we propose SDC-UDA, a
simple yet effective volumetric UDA framework for slice-direction continuous
cross-modality medical image segmentation which combines intra- and inter-slice
self-attentive image translation, uncertainty-constrained pseudo-label
refinement, and volumetric self-training. Our method is distinguished from
previous methods on UDA for medical image segmentation in that it can obtain
continuous segmentation in the slice direction, thereby ensuring higher
accuracy and potential in clinical practice. We validate SDC-UDA with multiple
publicly available cross-modality medical image segmentation datasets and
achieve state-of-the-art segmentation performance, not to mention the superior
slice-direction continuity of prediction compared to previous studies.
</p></li>
</ul>

<h3>Title: Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.11019">http://arxiv.org/abs/2305.11019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.11019] Annotation-free Audio-Visual Segmentation](http://arxiv.org/abs/2305.11019) #segmentation</code></li>
<li>Summary: <p>The objective of Audio-Visual Segmentation (AVS) is to locate sounding
objects within visual scenes by accurately predicting pixelwise segmentation
masks. In this paper, we present the following contributions: (i), we propose a
scalable and annotation-free pipeline for generating artificial data for the
AVS task. We leverage existing image segmentation and audio datasets to draw
links between category labels, image-mask pairs, and audio samples, which
allows us to easily compose (image, audio, mask) triplets for training AVS
models; (ii), we introduce a novel Audio-Aware Transformer (AuTR) architecture
that features an audio-aware query-based transformer decoder. This architecture
enables the model to search for sounding objects with the guidance of audio
signals, resulting in more accurate segmentation; (iii), we present extensive
experiments conducted on both synthetic and real datasets, which demonstrate
the effectiveness of training AVS models with synthetic data generated by our
proposed pipeline. Additionally, our proposed AuTR architecture exhibits
superior performance and strong generalization ability on public benchmarks.
The project page is https://jinxiang-liu.github.io/anno-free-AVS/.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
