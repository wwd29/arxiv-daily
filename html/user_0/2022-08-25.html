<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: SoK: Content Moderation Schemes in End-to-End Encrypted Systems. (arXiv:2208.11147v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11147">http://arxiv.org/abs/2208.11147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11147] SoK: Content Moderation Schemes in End-to-End Encrypted Systems](http://arxiv.org/abs/2208.11147)</code></li>
<li>Summary: <p>This paper aims to survey various techniques utilized for content moderation
in end-to-end encryption systems. We assess the challenging aspect of content
moderation: maintaining a safe platform while assuring user privacy. We study
the unique features of some content moderation techniques, such as message
franking and perceptual hashing, and highlight their limitations. Currently
implemented content moderation techniques violate the goals of end-to-end
encrypted messaging to some extent. This has led researchers to develop
remediations and design new security primitives to make content moderation
compatible with end-to-end encryption systems. We detail these developments,
analyze the proposed research efforts, assess their security guarantees,
correlate them with other proposed solutions, and determine suitable
improvements under specific scenarios.
</p></li>
</ul>

<h3>Title: Investigating the Requirements for Building a Blockchain Simulator for IoT Applications. (arXiv:2208.11207v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11207">http://arxiv.org/abs/2208.11207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11207] Investigating the Requirements for Building a Blockchain Simulator for IoT Applications](http://arxiv.org/abs/2208.11207)</code></li>
<li>Summary: <p>The pervasiveness of the Internet of Things (IoT) has enabled the
administration of a large number of intelligent devices. However, IoT is based
on centralised models, which introduce a number of problems, such as a single
point of failure and security risks. Blockchain may offer a viable option for
addressing these concerns. Practically, both blockchain and IoT are complex
technologies posing further challenges in assessing application performance.
The availability of a reliable simulation environment for Blockchain based IoT
applications would be a major aid in the development and evaluation of such
applications. Our research has found that currently there are no simulators
with a comprehensive set of features, for the development and evaluation of
blockchain based IoT applications, which is the main motivation for our work.
The purpose of this study is to gather the opinions of experts regarding the
creation of a simulation environment for IoT based blockchain applications. To
do this, we utilise two separate investigations. First, a questionnaire is
developed to ensure that the development of such simulation software would be
of significant use. Second, interviews with participants are performed to gain
their perspectives on the primary issues they face with blockchain-based IoT
applications. In addition, the interviews focused on collecting the
perspectives of participants on how blockchain may improve IoT and how to
identify blockchain's applicability in IoT. Our findings demonstrate that the
participants had a great deal of confidence in blockchain to resolve IoT
issues. However, they lack the tools necessary to assess this concept. This
highlights their requirement for a simulator to analyse the integration of
blockchain and IoT.
</p></li>
</ul>

<h3>Title: ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels. (arXiv:2208.11290v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11290">http://arxiv.org/abs/2208.11290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11290] ADMoE: Anomaly Detection with Mixture-of-Experts from Noisy Labels](http://arxiv.org/abs/2208.11290)</code></li>
<li>Summary: <p>Existing works on anomaly detection (AD) rely on clean labels from human
annotators that are expensive to acquire in practice. In this work, we propose
a method to leverage weak/noisy labels (e.g., risk scores generated by machine
rules for detecting malware) that are cheaper to obtain for anomaly detection.
Specifically, we propose ADMoE, the first framework for anomaly detection
algorithms to learn from noisy labels. In a nutshell, ADMoE leverages
mixture-of-experts (MoE) architecture to encourage specialized and scalable
learning from multiple noisy sources. It captures the similarities among noisy
labels by sharing most model parameters, while encouraging specialization by
building "expert" sub-networks. To further juice out the signals from noisy
labels, ADMoE uses them as input features to facilitate expert learning.
Extensive results on eight datasets (including a proprietary enterprise
security dataset) demonstrate the effectiveness of ADMoE, where it brings up to
34% performance improvement over not using it. Also, it outperforms a total of
13 leading baselines with equivalent network parameters and FLOPS. Notably,
ADMoE is model-agnostic to enable any neural network-based detection methods to
handle noisy labels, where we showcase its results on both multiple-layer
perceptron (MLP) and the leading AD method DeepSAD.
</p></li>
</ul>

<h3>Title: "Please help share!": Security and Privacy Advice on Twitter during the 2022 Russian Invasion of Ukraine. (arXiv:2208.11581v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11581">http://arxiv.org/abs/2208.11581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11581] "Please help share!": Security and Privacy Advice on Twitter during the 2022 Russian Invasion of Ukraine](http://arxiv.org/abs/2208.11581)</code></li>
<li>Summary: <p>The Russian Invasion of Ukraine in early 2022 resulted in a rapidly changing
(cyber) threat environment. This changing environment incentivized the sharing
of security advice on social media, both for the Ukrainian population, as well
as against Russian cyber attacks at large. Previous research found a
significant influence of online security advice on end users.
</p></li>
</ul>

<p>We collected 8,920 tweets posted after the Russian Invasion of Ukraine and
examined 1,228 in detail, including qualitatively coding 232 relevant tweets
and 140 linked documents for security and privacy advice. We identified 221
unique pieces of advice which we divided into seven categories and 21
subcategories, and advice targeted at individuals or organizations. We then
compared our findings to those of prior studies, finding noteworthy
similarities. Our results confirm a lack of advice prioritization found by
prior work, which seems especially detrimental during times of crisis. In
addition, we find offers for individual support to be a valuable tool and
identify misinformation as a rising threat in general and for security advice
specifically.
</p>

<h2>privacy</h2>
<h3>Title: On the Design of Privacy-Aware Cameras: a Study on Deep Neural Networks. (arXiv:2208.11372v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11372">http://arxiv.org/abs/2208.11372</a></li>
<li>Code URL: <a href="https://github.com/upciti/privacy-by-design-semseg">https://github.com/upciti/privacy-by-design-semseg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11372] On the Design of Privacy-Aware Cameras: a Study on Deep Neural Networks](http://arxiv.org/abs/2208.11372)</code></li>
<li>Summary: <p>In spite of the legal advances in personal data protection, the issue of
private data being misused by unauthorized entities is still of utmost
importance. To prevent this, Privacy by Design is often proposed as a solution
for data protection. In this paper, the effect of camera distortions is studied
using Deep Learning techniques commonly used to extract sensitive data. To do
so, we simulate out-of-focus images corresponding to a realistic conventional
camera with fixed focal length, aperture, and focus, as well as grayscale
images coming from a monochrome camera. We then prove, through an experimental
study, that we can build a privacy-aware camera that cannot extract personal
information such as license plate numbers. At the same time, we ensure that
useful non-sensitive data can still be extracted from distorted images. Code is
available at https://github.com/upciti/privacy-by-design-semseg .
</p></li>
</ul>

<h3>Title: Auditing Membership Leakages of Multi-Exit Networks. (arXiv:2208.11180v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11180">http://arxiv.org/abs/2208.11180</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11180] Auditing Membership Leakages of Multi-Exit Networks](http://arxiv.org/abs/2208.11180)</code></li>
<li>Summary: <p>Relying on the fact that not all inputs require the same amount of
computation to yield a confident prediction, multi-exit networks are gaining
attention as a prominent approach for pushing the limits of efficient
deployment. Multi-exit networks endow a backbone model with early exits,
allowing to obtain predictions at intermediate layers of the model and thus
save computation time and/or energy. However, current various designs of
multi-exit networks are only considered to achieve the best trade-off between
resource usage efficiency and prediction accuracy, the privacy risks stemming
from them have never been explored. This prompts the need for a comprehensive
investigation of privacy risks in multi-exit networks.
</p></li>
</ul>

<p>In this paper, we perform the first privacy analysis of multi-exit networks
through the lens of membership leakages. In particular, we first leverage the
existing attack methodologies to quantify the multi-exit networks'
vulnerability to membership leakages. Our experimental results show that
multi-exit networks are less vulnerable to membership leakages and the exit
(number and depth) attached to the backbone model is highly correlated with the
attack performance. Furthermore, we propose a hybrid attack that exploits the
exit information to improve the performance of existing attacks. We evaluate
membership leakage threat caused by our hybrid attack under three different
adversarial setups, ultimately arriving at a model-free and data-free
adversary. These results clearly demonstrate that our hybrid attacks are very
broadly applicable, thereby the corresponding risks are much more severe than
shown by existing membership inference attacks. We further present a defense
mechanism called TimeGuard specifically for multi-exit networks and show that
TimeGuard mitigates the newly proposed attacks perfectly.
</p>

<h3>Title: Reconstruction of the distribution of sensitive data under free-will privacy. (arXiv:2208.11268v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11268">http://arxiv.org/abs/2208.11268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11268] Reconstruction of the distribution of sensitive data under free-will privacy](http://arxiv.org/abs/2208.11268)</code></li>
<li>Summary: <p>The local privacy mechanisms, such as k-RR, RAPPOR, and the
geo-indistinguishability ones, have become quite popular thanks to the fact
that the obfuscation can be effectuated at the users end, thus avoiding the
need of a trusted third party. Another important advantage is that each data
point is sanitized independently from the others, and therefore different users
may use different levels of obfuscation depending on their privacy
requirements, or they may even use entirely different mechanisms depending on
the services they are trading their data for. A challenging requirement in this
setting is to construct the original distribution on the users sensitive data
from their noisy versions. Existing techniques can only estimate that
distribution separately on each obfuscation schema and corresponding noisy data
subset. But the smaller are the subsets, the more imprecise the estimations
are. In this paper we study how to avoid the subsets-fractioning problem when
combining local privacy mechanisms, thus recovering an optimal utility. We
focus on the estimation of the original distribution, and on the two main
methods to estimate it: the matrix-inversion method and the iterative Bayes
update. We consider various cases of combination of local privacy mechanisms,
and compare the flexibility and the performance of the two methods.
</p></li>
</ul>

<h3>Title: On Privacy Preserving Data Aggregation Protocols using BGN cryptosystem. (arXiv:2208.11304v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11304">http://arxiv.org/abs/2208.11304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11304] On Privacy Preserving Data Aggregation Protocols using BGN cryptosystem](http://arxiv.org/abs/2208.11304)</code></li>
<li>Summary: <p>The notion of aggregator oblivious (AO) security for privacy preserving data
aggregation was formalized with a specific construction of AO-secure blinding
technique over a cyclic group by Shi et al. Some of proposals of data
aggregation protocols use the blinding technique of Shi et al. for BGN
cryptosystem, an additive homomorphic encryption. Previously, there have been
some security analysis on some of BGN based data aggregation protocols in the
context of integrity or authenticity of data. Even with such security analysis,
the BGN cryptosystem has been a popular building block of privacy preserving
data aggregation protocol. In this paper, we study the privacy issues in the
blinding technique of Shi et al. used for BGN cryptosystem. We show that the
blinding techniques for the BGN cryptosystem used in several protocols are not
privacy preserving against the recipient, the decryptor. Our analysis is based
on the fact that the BGN cryptosystem uses a pairing e:GxG-->G_T and the
existence of the pairing makes the DDH problem on G easy to solve. We also
suggest how to prevent such privacy leakage in the blinding technique of Shi et
al. used for BGN cryptosystem.
</p></li>
</ul>

<h3>Title: Solving the Kidney Exchange Problem Using Privacy-Preserving Integer Programming. (arXiv:2208.11319v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11319">http://arxiv.org/abs/2208.11319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11319] Solving the Kidney Exchange Problem Using Privacy-Preserving Integer Programming](http://arxiv.org/abs/2208.11319)</code></li>
<li>Summary: <p>The kidney exchange problem (KEP) is to find a constellation of exchanges
that maximizes the number of transplants that can be carried out for a set of
patients with kidney disease and their incompatible donors. Recently, this
problem has been tackled from a privacy perspective in order to protect the
sensitive medical data of patients and donors and to decrease the potential for
manipulation of the computed exchanges. However, the proposed approaches either
do not provide the same functionality as the conventional solutions to the KEP
or they come along with a huge performance impact. In this paper, we provide a
novel privacy-preserving protocol for the KEP which significantly outperforms
the existing approaches by allowing a small information leakage. This leakage
allows us to base our protocol on Integer Programming which is the most
efficient method for solving the KEP in the non privacy-preserving case. We
implement our protocol in the SMPC benchmarking framework MP-SPDZ and compare
its performance to the existing protocols for solving the KEP.
</p></li>
</ul>

<h3>Title: A Plural Decentralized Identity Frontier: Abstraction v. Composability Tradeoffs in Web3. (arXiv:2208.11443v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11443">http://arxiv.org/abs/2208.11443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11443] A Plural Decentralized Identity Frontier: Abstraction v](http://arxiv.org/abs/2208.11443)</code></li>
<li>Summary: <p>In this article, we explore the tension between abstraction and composability
in web3 today, specifically within identity solutions, and argue that the
current standard DID v1.0 is sufficiently under specified, allowing for many
methods and instantiations, including blockchain based certificates. We view
experiments today in web3 identity as additive and complementary, and argue
that often cited differences are of degree and more in form, less in substance.
By way of illustration, we compare decentralized naming services and blockchain
based identity certificates such as soulbound tokens (SBTs) to decentralized
identifiers (DIDs) and verifiable credentials (VCs). Both paradigms, to the
extent they can be meaningfully differentiated, share similar potential as well
as challenges. Specifically, we refer to fears about non consensual
verification (scarlet letters) and show DID method iterations are not immune by
issuing an innocuous public scarlet letter to a DIDs associated public address
for anyone to see. Moreover, we argue that because SBTs are unspecified, one
could characterize SBTs as an iteration, or extension, of VCs that additionally
aspire to achieve composability with web3 smart contracts for correct execution
of code, privacy, coercion resistance, and censorship resistance. We offer
research paths for how VCs can also achieve these properties. We do not comment
on cost, scalability, transferability, or common knowledge as they have been
previously reviewed.
</p></li>
</ul>

<h3>Title: DP2-Pub: Differentially Private High-Dimensional Data Publication with Invariant Post Randomization. (arXiv:2208.11693v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11693">http://arxiv.org/abs/2208.11693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11693] DP2-Pub: Differentially Private High-Dimensional Data Publication with Invariant Post Randomization](http://arxiv.org/abs/2208.11693)</code></li>
<li>Summary: <p>A large amount of high-dimensional and heterogeneous data appear in practical
applications, which are often published to third parties for data analysis,
recommendations, targeted advertising, and reliable predictions. However,
publishing these data may disclose personal sensitive information, resulting in
an increasing concern on privacy violations. Privacy-preserving data publishing
has received considerable attention in recent years. Unfortunately, the
differentially private publication of high dimensional data remains a
challenging problem. In this paper, we propose a differentially private
high-dimensional data publication mechanism (DP2-Pub) that runs in two phases:
a Markov-blanket-based attribute clustering phase and an invariant post
randomization (PRAM) phase. Specifically, splitting attributes into several
low-dimensional clusters with high intra-cluster cohesion and low inter-cluster
coupling helps obtain a reasonable allocation of privacy budget, while a
double-perturbation mechanism satisfying local differential privacy facilitates
an invariant PRAM to ensure no loss of statistical information and thus
significantly preserves data utility. We also extend our DP2-Pub mechanism to
the scenario with a semi-honest server which satisfies local differential
privacy. We conduct extensive experiments on four real-world datasets and the
experimental results demonstrate that our mechanism can significantly improve
the data utility of the published data while satisfying differential privacy.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Trace and Detect Adversarial Attacks on CNNs using Feature Response Maps. (arXiv:2208.11436v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11436">http://arxiv.org/abs/2208.11436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11436] Trace and Detect Adversarial Attacks on CNNs using Feature Response Maps](http://arxiv.org/abs/2208.11436)</code></li>
<li>Summary: <p>The existence of adversarial attacks on convolutional neural networks (CNN)
questions the fitness of such models for serious applications. The attacks
manipulate an input image such that misclassification is evoked while still
looking normal to a human observer -- they are thus not easily detectable. In a
different context, backpropagated activations of CNN hidden layers -- "feature
responses" to a given input -- have been helpful to visualize for a human
"debugger" what the CNN "looks at" while computing its output. In this work, we
propose a novel detection method for adversarial examples to prevent attacks.
We do so by tracking adversarial perturbations in feature responses, allowing
for automatic detection using average local spatial entropy. The method does
not alter the original network architecture and is fully human-interpretable.
Experiments confirm the validity of our approach for state-of-the-art attacks
on large-scale models trained on ImageNet.
</p></li>
</ul>

<h3>Title: Unrestricted Black-box Adversarial Attack Using GAN with Limited Queries. (arXiv:2208.11613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11613">http://arxiv.org/abs/2208.11613</a></li>
<li>Code URL: <a href="https://github.com/ndb796/latenthsja">https://github.com/ndb796/latenthsja</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11613] Unrestricted Black-box Adversarial Attack Using GAN with Limited Queries](http://arxiv.org/abs/2208.11613)</code></li>
<li>Summary: <p>Adversarial examples are inputs intentionally generated for fooling a deep
neural network. Recent studies have proposed unrestricted adversarial attacks
that are not norm-constrained. However, the previous unrestricted attack
methods still have limitations to fool real-world applications in a black-box
setting. In this paper, we present a novel method for generating unrestricted
adversarial examples using GAN where an attacker can only access the top-1
final decision of a classification model. Our method, Latent-HSJA, efficiently
leverages the advantages of a decision-based attack in the latent space and
successfully manipulates the latent vectors for fooling the classification
model.
</p></li>
</ul>

<p>With extensive experiments, we demonstrate that our proposed method is
efficient in evaluating the robustness of classification models with limited
queries in a black-box setting. First, we demonstrate that our targeted attack
method is query-efficient to produce unrestricted adversarial examples for a
facial identity recognition model that contains 307 identities. Then, we
demonstrate that the proposed method can also successfully attack a real-world
celebrity recognition service.
</p>

<h3>Title: Towards an Awareness of Time Series Anomaly Detection Models' Adversarial Vulnerability. (arXiv:2208.11264v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11264">http://arxiv.org/abs/2208.11264</a></li>
<li>Code URL: <a href="https://github.com/shahroztariq/adversarial-attacks-on-timeseries">https://github.com/shahroztariq/adversarial-attacks-on-timeseries</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11264] Towards an Awareness of Time Series Anomaly Detection Models' Adversarial Vulnerability](http://arxiv.org/abs/2208.11264)</code></li>
<li>Summary: <p>Time series anomaly detection is extensively studied in statistics,
economics, and computer science. Over the years, numerous methods have been
proposed for time series anomaly detection using deep learning-based methods.
Many of these methods demonstrate state-of-the-art performance on benchmark
datasets, giving the false impression that these systems are robust and
deployable in many practical and industrial real-world scenarios. In this
paper, we demonstrate that the performance of state-of-the-art anomaly
detection methods is degraded substantially by adding only small adversarial
perturbations to the sensor data. We use different scoring metrics such as
prediction errors, anomaly, and classification scores over several public and
private datasets ranging from aerospace applications, server machines, to
cyber-physical systems in power plants. Under well-known adversarial attacks
from Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD)
methods, we demonstrate that state-of-the-art deep neural networks (DNNs) and
graph neural networks (GNNs) methods, which claim to be robust against
anomalies and have been possibly integrated in real-life systems, have their
performance drop to as low as 0%. To the best of our understanding, we
demonstrate, for the first time, the vulnerabilities of anomaly detection
systems against adversarial attacks. The overarching goal of this research is
to raise awareness towards the adversarial vulnerabilities of time series
anomaly detectors.
</p></li>
</ul>

<h3>Title: Attacking Neural Binary Function Detection. (arXiv:2208.11667v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11667">http://arxiv.org/abs/2208.11667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11667] Attacking Neural Binary Function Detection](http://arxiv.org/abs/2208.11667)</code></li>
<li>Summary: <p>Binary analyses based on deep neural networks (DNNs), or neural binary
analyses (NBAs), have become a hotly researched topic in recent years. DNNs
have been wildly successful at pushing the performance and accuracy envelopes
in the natural language and image processing domains. Thus, DNNs are highly
promising for solving binary analysis problems that are typically hard due to a
lack of complete information resulting from the lossy compilation process.
Despite this promise, it is unclear that the prevailing strategy of repurposing
embeddings and model architectures originally developed for other problem
domains is sound given the adversarial contexts under which binary analysis
often operates.
</p></li>
</ul>

<p>In this paper, we empirically demonstrate that the current state of the art
in neural function boundary detection is vulnerable to both inadvertent and
deliberate adversarial attacks. We proceed from the insight that current
generation NBAs are built upon embeddings and model architectures intended to
solve syntactic problems. We devise a simple, reproducible, and scalable
black-box methodology for exploring the space of inadvertent attacks -
instruction sequences that could be emitted by common compiler toolchains and
configurations - that exploits this syntactic design focus. We then show that
these inadvertent misclassifications can be exploited by an attacker, serving
as the basis for a highly effective black-box adversarial example generation
process. We evaluate this methodology against two state-of-the-art neural
function boundary detectors: XDA and DeepDi. We conclude with an analysis of
the evaluation data and recommendations for how future research might avoid
succumbing to similar attacks.
</p>

<h3>Title: Toward a Reasoning and Learning Architecture for Ad Hoc Teamwork. (arXiv:2208.11556v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11556">http://arxiv.org/abs/2208.11556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11556] Toward a Reasoning and Learning Architecture for Ad Hoc Teamwork](http://arxiv.org/abs/2208.11556)</code></li>
<li>Summary: <p>We present an architecture for ad hoc teamwork, which refers to collaboration
in a team of agents without prior coordination. State of the art methods for
this problem often include a data-driven component that uses a long history of
prior observations to model the behaviour of other agents (or agent types) and
to determine the ad hoc agent's behavior. In many practical domains, it is
challenging to find large training datasets, and necessary to understand and
incrementally extend the existing models to account for changes in team
composition or domain attributes. Our architecture combines the principles of
knowledge-based and data-driven reasoning and learning. Specifically, we enable
an ad hoc agent to perform non-monotonic logical reasoning with prior
commonsense domain knowledge and incrementally-updated simple predictive models
of other agents' behaviour. We use the benchmark simulated multiagent
collaboration domain Fort Attack to demonstrate that our architecture supports
adaptation to unforeseen changes, incremental learning and revision of models
of other agents' behaviour from limited samples, transparency in the ad hoc
agent's decision making, and better performance than a data-driven baseline.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels. (arXiv:2208.11176v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11176">http://arxiv.org/abs/2208.11176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11176] A Study on the Impact of Data Augmentation for Training Convolutional Neural Networks in the Presence of Noisy Labels](http://arxiv.org/abs/2208.11176)</code></li>
<li>Summary: <p>Label noise is common in large real-world datasets, and its presence harms
the training process of deep neural networks. Although several works have
focused on the training strategies to address this problem, there are few
studies that evaluate the impact of data augmentation as a design choice for
training deep neural networks. In this work, we analyse the model robustness
when using different data augmentations and their improvement on the training
with the presence of noisy labels. We evaluate state-of-the-art and classical
data augmentation strategies with different levels of synthetic noise for the
datasets MNist, CIFAR-10, CIFAR-100, and the real-world dataset Clothing1M. We
evaluate the methods using the accuracy metric. Results show that the
appropriate selection of data augmentation can drastically improve the model
robustness to label noise, increasing up to 177.84% of relative best test
accuracy compared to the baseline with no augmentation, and an increase of up
to 6% in absolute value with the state-of-the-art DivideMix training strategy.
</p></li>
</ul>

<h3>Title: A new explainable DTM generation algorithm with airborne LIDAR data: grounds are smoothly connected eventually. (arXiv:2208.11243v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11243">http://arxiv.org/abs/2208.11243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11243] A new explainable DTM generation algorithm with airborne LIDAR data: grounds are smoothly connected eventually](http://arxiv.org/abs/2208.11243)</code></li>
<li>Summary: <p>The digital terrain model (DTM) is fundamental geospatial data for various
studies in urban, environmental, and Earth science. The reliability of the
results obtained from such studies can be considerably affected by the errors
and uncertainties of the underlying DTM. Numerous algorithms have been
developed to mitigate the errors and uncertainties of DTM. However, most
algorithms involve tricky parameter selection and complicated procedures that
make the algorithm's decision rule obscure, so it is often difficult to explain
and predict the errors and uncertainties of the resulting DTM. Also, previous
algorithms often consider the local neighborhood of each point for
distinguishing non-ground objects, which limits both search radius and
contextual understanding and can be susceptible to errors particularly if point
density varies. This study presents an open-source DTM generation algorithm for
airborne LiDAR data that can consider beyond the local neighborhood and whose
results are easily explainable, predictable, and reliable. The key assumption
of the algorithm is that grounds are smoothly connected while non-grounds are
surrounded by areas having sharp elevation changes. The robustness and
uniqueness of the proposed algorithm were evaluated in geographically complex
environments through tiling evaluation compared to other state-of-the-art
algorithms.
</p></li>
</ul>

<h3>Title: RZSR: Reference-based Zero-Shot Super-Resolution with Depth Guided Self-Exemplars. (arXiv:2208.11313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11313">http://arxiv.org/abs/2208.11313</a></li>
<li>Code URL: <a href="https://github.com/junsang7777/rzsr">https://github.com/junsang7777/rzsr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11313] RZSR: Reference-based Zero-Shot Super-Resolution with Depth Guided Self-Exemplars](http://arxiv.org/abs/2208.11313)</code></li>
<li>Summary: <p>Recent methods for single image super-resolution (SISR) have demonstrated
outstanding performance in generating high-resolution (HR) images from
low-resolution (LR) images. However, most of these methods show their
superiority using synthetically generated LR images, and their generalizability
to real-world images is often not satisfactory. In this paper, we pay attention
to two well-known strategies developed for robust super-resolution (SR), i.e.,
reference-based SR (RefSR) and zero-shot SR (ZSSR), and propose an integrated
solution, called reference-based zero-shot SR (RZSR). Following the principle
of ZSSR, we train an image-specific SR network at test time using training
samples extracted only from the input image itself. To advance ZSSR, we obtain
reference image patches with rich textures and high-frequency details which are
also extracted only from the input image using cross-scale matching. To this
end, we construct an internal reference dataset and retrieve reference image
patches from the dataset using depth information. Using LR patches and their
corresponding HR reference patches, we train a RefSR network that is embodied
with a non-local attention module. Experimental results demonstrate the
superiority of the proposed RZSR compared to the previous ZSSR methods and
robustness to unseen images compared to other fully supervised SISR methods.
</p></li>
</ul>

<h3>Title: Robust Motion Averaging for Multi-view Registration of Point Sets Based Maximum Correntropy Criterion. (arXiv:2208.11327v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11327">http://arxiv.org/abs/2208.11327</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11327] Robust Motion Averaging for Multi-view Registration of Point Sets Based Maximum Correntropy Criterion](http://arxiv.org/abs/2208.11327)</code></li>
<li>Summary: <p>As an efficient algorithm to solve the multi-view registration problem,the
motion averaging (MA) algorithm has been extensively studied and many MA-based
algorithms have been introduced. They aim at recovering global motions from
relative motions and exploiting information redundancy to average accumulative
errors. However, one property of these methods is that they use Guass-Newton
method to solve a least squares problem for the increment of global motions,
which may lead to low efficiency and poor robustness to outliers. In this
paper, we propose a novel motion averaging framework for the multi-view
registration with Laplacian kernel-based maximum correntropy criterion (LMCC).
Utilizing the Lie algebra motion framework and the correntropy measure, we
propose a new cost function that takes all constraints supplied by relative
motions into account. Obtaining the increment used to correct the global
motions, can further be formulated as an optimization problem aimed at
maximizing the cost function. By virtue of the quadratic technique, the
optimization problem can be solved by dividing into two subproblems, i.e.,
computing the weight for each relative motion according to the current
residuals and solving a second-order cone program problem (SOCP) for the
increment in the next iteration. We also provide a novel strategy for
determining the kernel width which ensures that our method can efficiently
exploit information redundancy supplied by relative motions in the presence of
many outliers. Finally, we compare the proposed method with other MA-based
multi-view registration methods to verify its performance. Experimental tests
on synthetic and real data demonstrate that our method achieves superior
performance in terms of efficiency, accuracy and robustness.
</p></li>
</ul>

<h3>Title: Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization. (arXiv:2208.11351v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11351">http://arxiv.org/abs/2208.11351</a></li>
<li>Code URL: <a href="https://github.com/1998v7/self-filtering">https://github.com/1998v7/self-filtering</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11351] Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization](http://arxiv.org/abs/2208.11351)</code></li>
<li>Summary: <p>Sample selection is an effective strategy to mitigate the effect of label
noise in robust learning. Typical strategies commonly apply the small-loss
criterion to identify clean samples. However, those samples lying around the
decision boundary with large losses usually entangle with noisy examples, which
would be discarded with this criterion, leading to the heavy degeneration of
the generalization performance. In this paper, we propose a novel selection
strategy, \textbf{S}elf-\textbf{F}il\textbf{t}ering (SFT), that utilizes the
fluctuation of noisy examples in historical predictions to filter them, which
can avoid the selection bias of the small-loss criterion for the boundary
examples. Specifically, we introduce a memory bank module that stores the
historical predictions of each example and dynamically updates to support the
selection for the subsequent learning iteration. Besides, to reduce the
accumulated error of the sample selection bias of SFT, we devise a
regularization term to penalize the confident output distribution. By
increasing the weight of the misclassified categories with this term, the loss
function is robust to label noise in mild conditions. We conduct extensive
experiments on three benchmarks with variant noise types and achieve the new
state-of-the-art. Ablation studies and further analysis verify the virtue of
SFT for sample selection in robust learning.
</p></li>
</ul>

<h3>Title: Research on Mask Wearing Detection of Natural Population Based on Improved YOLOv4. (arXiv:2208.11353v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11353">http://arxiv.org/abs/2208.11353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11353] Research on Mask Wearing Detection of Natural Population Based on Improved YOLOv4](http://arxiv.org/abs/2208.11353)</code></li>
<li>Summary: <p>Recently, the domestic COVID-19 epidemic situation has been serious, but in
some public places, some people do not wear masks or wear masks incorrectly,
which requires the relevant staff to instantly remind and supervise them to
wear masks correctly. However, in the face of such important and complicated
work, it is necessary to carry out automated mask wearing detection in public
places. This paper proposes a new mask wearing detection method based on the
improved YOLOv4. Specifically, firstly, we add the Coordinate Attention Module
to the backbone to coordinate feature fusion and representation. Secondly, we
conduct a series of network structural improvements to enhance the model
performance and robustness. Thirdly, we deploy the K-means clustering algorithm
to make the nine anchor boxes more suitable for our NPMD dataset. The
experimental results show that the improved YOLOv4 performs better, exceeding
the baseline by 4.06% AP with a comparable speed of 64.37 FPS.
</p></li>
</ul>

<h3>Title: Event-based Image Deblurring with Dynamic Motion Awareness. (arXiv:2208.11398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11398">http://arxiv.org/abs/2208.11398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11398] Event-based Image Deblurring with Dynamic Motion Awareness](http://arxiv.org/abs/2208.11398)</code></li>
<li>Summary: <p>Non-uniform image deblurring is a challenging task due to the lack of
temporal and textural information in the blurry image itself. Complementary
information from auxiliary sensors such event sensors are being explored to
address these limitations. The latter can record changes in a logarithmic
intensity asynchronously, called events, with high temporal resolution and high
dynamic range. Current event-based deblurring methods combine the blurry image
with events to jointly estimate per-pixel motion and the deblur operator. In
this paper, we argue that a divide-and-conquer approach is more suitable for
this task. To this end, we propose to use modulated deformable convolutions,
whose kernel offsets and modulation masks are dynamically estimated from events
to encode the motion in the scene, while the deblur operator is learned from
the combination of blurry image and corresponding events. Furthermore, we
employ a coarse-to-fine multi-scale reconstruction approach to cope with the
inherent sparsity of events in low contrast regions. Importantly, we introduce
the first dataset containing pairs of real RGB blur images and related events
during the exposure time. Our results show better overall robustness when using
events, with improvements in PSNR by up to 1.57dB on synthetic data and 1.08 dB
on real event data.
</p></li>
</ul>

<h3>Title: Self-Supervised Endoscopic Image Key-Points Matching. (arXiv:2208.11424v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11424">http://arxiv.org/abs/2208.11424</a></li>
<li>Code URL: <a href="https://github.com/abenhamadou/Self-Supervised-Endoscopic-Image-Key-Points-Matching">https://github.com/abenhamadou/Self-Supervised-Endoscopic-Image-Key-Points-Matching</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11424] Self-Supervised Endoscopic Image Key-Points Matching](http://arxiv.org/abs/2208.11424)</code></li>
<li>Summary: <p>Feature matching and finding correspondences between endoscopic images is a
key step in many clinical applications such as patient follow-up and generation
of panoramic image from clinical sequences for fast anomalies localization.
Nonetheless, due to the high texture variability present in endoscopic images,
the development of robust and accurate feature matching becomes a challenging
task. Recently, deep learning techniques which deliver learned features
extracted via convolutional neural networks (CNNs) have gained traction in a
wide range of computer vision tasks. However, they all follow a supervised
learning scheme where a large amount of annotated data is required to reach
good performances, which is generally not always available for medical data
databases. To overcome this limitation related to labeled data scarcity, the
self-supervised learning paradigm has recently shown great success in a number
of applications. This paper proposes a novel self-supervised approach for
endoscopic image matching based on deep learning techniques. When compared to
standard hand-crafted local feature descriptors, our method outperformed them
in terms of precision and recall. Furthermore, our self-supervised descriptor
provides a competitive performance in comparison to a selection of
state-of-the-art deep learning based supervised methods in terms of precision
and matching score.
</p></li>
</ul>

<h3>Title: UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering. (arXiv:2208.11435v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11435">http://arxiv.org/abs/2208.11435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11435] UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering](http://arxiv.org/abs/2208.11435)</code></li>
<li>Summary: <p>Visual question answering (VQA) that leverages multi-modality data has
attracted intensive interest in real-life applications, such as home robots and
clinic diagnoses. Nevertheless, one of the challenges is to design robust
learning for different client tasks. This work aims to bridge the gap between
the prerequisite of large-scale training data and the constraint of client data
sharing mainly due to confidentiality. We propose the Unidirectional Split
Learning with Contrastive Loss (UniCon) to tackle VQA tasks training on
distributed data silos. In particular, UniCon trains a global model over the
entire data distribution of different clients learning refined cross-modal
representations via contrastive learning. The learned representations of the
global model aggregate knowledge from different local tasks. Moreover, we
devise a unidirectional split learning framework to enable more efficient
knowledge sharing. The comprehensive experiments with five state-of-the-art VQA
models on the VQA-v2 dataset demonstrated the efficacy of UniCon, achieving an
accuracy of 49.89% in the validation set of VQA-v2. This work is the first
study of VQA under the constraint of data confidentiality using self-supervised
Split Learning.
</p></li>
</ul>

<h3>Title: An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics. (arXiv:2208.11484v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11484">http://arxiv.org/abs/2208.11484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11484] An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics](http://arxiv.org/abs/2208.11484)</code></li>
<li>Summary: <p>This research is the second phase in a series of investigations on developing
an Optical Character Recognition (OCR) of Arabic historical documents and
examining how different modeling procedures interact with the problem. The
first research studied the effect of Transformers on our custom-built Arabic
dataset. One of the downsides of the first research was the size of the
training data, a mere 15000 images from our 30 million images, due to lack of
resources. Also, we add an image enhancement layer, time and space
optimization, and Post-Correction layer to aid the model in predicting the
correct word for the correct context. Notably, we propose an end-to-end text
recognition approach using Vision Transformers as an encoder, namely BEIT, and
vanilla Transformer as a decoder, eliminating CNNs for feature extraction and
reducing the model's complexity. The experiments show that our end-to-end model
outperforms Convolutions Backbones. The model attained a CER of 4.46%.
</p></li>
</ul>

<h3>Title: Motion Robust High-Speed Light-weighted Object Detection with Event Camera. (arXiv:2208.11602v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11602">http://arxiv.org/abs/2208.11602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11602] Motion Robust High-Speed Light-weighted Object Detection with Event Camera](http://arxiv.org/abs/2208.11602)</code></li>
<li>Summary: <p>The event camera produces a large dynamic range event stream with a very high
temporal resolution discarding redundant visual information, thus bringing new
possibilities for object detection tasks. However, the existing methods of
applying the event camera to object detection tasks using deep learning methods
still have many problems. First, existing methods cannot take into account
objects with different velocities relative to the motion of the event camera
due to the global synchronized time window and temporal resolution. Second,
most of the existing methods rely on large parameter neural networks, which
implies a large computational burden and low inference speed, thus contrary to
the high temporal resolution of the event stream.
</p></li>
</ul>

<p>In our work, we design a high-speed lightweight detector called Agile Event
Detector (AED) with a simple but effective data augmentation method. Also, we
propose an event stream representation tensor called Temporal Active Focus
(TAF), which takes full advantage of the asynchronous generation of event
stream data and is robust to the motion of moving objects. It can also be
constructed without much time-consuming. We further propose a module called the
Bifurcated Folding Module (BFM) to extract the rich temporal information in the
TAF tensor at the input layer of the AED detector. We conduct our experiments
on two typical real-scene event camera object detection datasets: the complete
Prophesee GEN1 Automotive Detection Dataset and the Prophesee 1 MEGAPIXEL
Automotive Detection Dataset with partial annotation. Experiments show that our
method is competitive in terms of accuracy, speed, and the number of parameters
simultaneously. Also by classifying the objects into multiple motion levels
based on the optical flow density metric, we illustrated the robustness of our
method for objects with different velocities relative to the camera.
</p>

<h3>Title: Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery. (arXiv:2208.11607v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11607">http://arxiv.org/abs/2208.11607</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11607] Learning crop type mapping from regional label proportions in large-scale SAR and optical imagery](http://arxiv.org/abs/2208.11607)</code></li>
<li>Summary: <p>The application of deep learning algorithms to Earth observation (EO) in
recent years has enabled substantial progress in fields that rely on remotely
sensed data. However, given the data scale in EO, creating large datasets with
pixel-level annotations by experts is expensive and highly time-consuming. In
this context, priors are seen as an attractive way to alleviate the burden of
manual labeling when training deep learning methods for EO. For some
applications, those priors are readily available. Motivated by the great
success of contrastive-learning methods for self-supervised feature
representation learning in many computer-vision tasks, this study proposes an
online deep clustering method using crop label proportions as priors to learn a
sample-level classifier based on government crop-proportion data for a whole
agricultural region. We evaluate the method using two large datasets from two
different agricultural regions in Brazil. Extensive experiments demonstrate
that the method is robust to different data types (synthetic-aperture radar and
optical images), reporting higher accuracy values considering the major crop
types in the target regions. Thus, it can alleviate the burden of large-scale
image annotation in EO applications.
</p></li>
</ul>

<h3>Title: AGO-Net: Association-Guided 3D Point Cloud Object Detection Network. (arXiv:2208.11658v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11658">http://arxiv.org/abs/2208.11658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11658] AGO-Net: Association-Guided 3D Point Cloud Object Detection Network](http://arxiv.org/abs/2208.11658)</code></li>
<li>Summary: <p>The human brain can effortlessly recognize and localize objects, whereas
current 3D object detection methods based on LiDAR point clouds still report
inferior performance for detecting occluded and distant objects: the point
cloud appearance varies greatly due to occlusion, and has inherent variance in
point densities along the distance to sensors. Therefore, designing feature
representations robust to such point clouds is critical. Inspired by human
associative recognition, we propose a novel 3D detection framework that
associates intact features for objects via domain adaptation. We bridge the gap
between the perceptual domain, where features are derived from real scenes with
sub-optimal representations, and the conceptual domain, where features are
extracted from augmented scenes that consist of non-occlusion objects with rich
detailed information. A feasible method is investigated to construct conceptual
scenes without external datasets. We further introduce an attention-based
re-weighting module that adaptively strengthens the feature adaptation of more
informative regions. The network's feature enhancement ability is exploited
without introducing extra cost during inference, which is plug-and-play in
various 3D detection frameworks. We achieve new state-of-the-art performance on
the KITTI 3D detection benchmark in both accuracy and speed. Experiments on
nuScenes and Waymo datasets also validate the versatility of our method.
</p></li>
</ul>

<h3>Title: PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling. (arXiv:2208.11508v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11508">http://arxiv.org/abs/2208.11508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11508] PSSAT: A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling](http://arxiv.org/abs/2208.11508)</code></li>
<li>Summary: <p>Most existing slot filling models tend to memorize inherent patterns of
entities and corresponding contexts from training data. However, these models
can lead to system failure or undesirable outputs when being exposed to spoken
language perturbation or variation in practice. We propose a perturbed semantic
structure awareness transferring method for training perturbation-robust slot
filling models. Specifically, we introduce two MLM-based training strategies to
respectively learn contextual semantic structure and word distribution from
unsupervised language perturbation corpus. Then, we transfer semantic knowledge
learned from upstream training procedure into the original samples and filter
generated data by consistency processing. These procedures aim to enhance the
robustness of slot filling models. Experimental results show that our method
consistently outperforms the previous basic methods and gains strong
generalization while preventing the model from memorizing inherent patterns of
entities and contexts.
</p></li>
</ul>

<h3>Title: Inter- and Intra-Series Embeddings Fusion Network for Epidemiological Forecasting. (arXiv:2208.11515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11515">http://arxiv.org/abs/2208.11515</a></li>
<li>Code URL: <a href="https://github.com/xiefeng69/sefnet">https://github.com/xiefeng69/sefnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11515] Inter- and Intra-Series Embeddings Fusion Network for Epidemiological Forecasting](http://arxiv.org/abs/2208.11515)</code></li>
<li>Summary: <p>The accurate forecasting of infectious epidemic diseases is the key to
effective control of the epidemic situation in a region. Most existing methods
ignore potential dynamic dependencies between regions or the importance of
temporal dependencies and inter-dependencies between regions for prediction. In
this paper, we propose an Inter- and Intra-Series Embeddings Fusion Network
(SEFNet) to improve epidemic prediction performance. SEFNet consists of two
parallel modules, named Inter-Series Embedding Module and Intra-Series
Embedding Module. In Inter-Series Embedding Module, a multi-scale unified
convolution component called Region-Aware Convolution is proposed, which
cooperates with self-attention to capture dynamic dependencies between time
series obtained from multiple regions. The Intra-Series Embedding Module uses
Long Short-Term Memory to capture temporal relationships within each time
series. Subsequently, we learn the influence degree of two embeddings and fuse
them with the parametric-matrix fusion method. To further improve the
robustness, SEFNet also integrates a traditional autoregressive component in
parallel with nonlinear neural networks. Experiments on four real-world
epidemic-related datasets show SEFNet is effective and outperforms
state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: Robustness to Unbounded Smoothness of Generalized SignSGD. (arXiv:2208.11195v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11195">http://arxiv.org/abs/2208.11195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11195] Robustness to Unbounded Smoothness of Generalized SignSGD](http://arxiv.org/abs/2208.11195)</code></li>
<li>Summary: <p>Traditional analyses in non-convex optimization typically rely on the
smoothness assumption, namely requiring the gradients to be Lipschitz. However,
recent evidence shows that this smoothness condition does not capture the
properties of some deep learning objective functions, including the ones
involving Recurrent Neural Networks and LSTMs. Instead, they satisfy a much
more relaxed condition, with potentially unbounded smoothness. Under this
relaxed assumption, it has been theoretically and empirically shown that the
gradient-clipped SGD has an advantage over the vanilla one. In this paper, we
show that clipping is not indispensable for Adam-type algorithms in tackling
such scenarios: we theoretically prove that a generalized SignSGD algorithm can
obtain similar convergence rates as SGD with clipping but does not need
explicit clipping at all. This family of algorithms on one end recovers SignSGD
and on the other end closely resembles the popular Adam algorithm. Our analysis
underlines the critical role that momentum plays in analyzing SignSGD-type and
Adam-type algorithms: it not only reduces the effects of noise, thus removing
the need for large mini-batch in previous analyses of SignSGD-type algorithms,
but it also substantially reduces the effects of unbounded smoothness and
gradient norms. We also compare these algorithms with popular optimizers on a
set of deep learning tasks, observing that we can match the performance of Adam
while beating the others.
</p></li>
</ul>

<h3>Title: Time-to-Green predictions for fully-actuated signal control systems with supervised learning. (arXiv:2208.11344v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11344">http://arxiv.org/abs/2208.11344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11344] Time-to-Green predictions for fully-actuated signal control systems with supervised learning](http://arxiv.org/abs/2208.11344)</code></li>
<li>Summary: <p>Recently, efforts have been made to standardize signal phase and timing
(SPaT) messages. These messages contain signal phase timings of all signalized
intersection approaches. This information can thus be used for efficient motion
planning, resulting in more homogeneous traffic flows and uniform speed
profiles. Despite efforts to provide robust predictions for semi-actuated
signal control systems, predicting signal phase timings for fully-actuated
controls remains challenging. This paper proposes a time series prediction
framework using aggregated traffic signal and loop detector data. We utilize
state-of-the-art machine learning models to predict future signal phases'
duration. The performance of a Linear Regression (LR), a Random Forest (RF),
and a Long-Short-Term-Memory (LSTM) neural network are assessed against a naive
baseline model. Results based on an empirical data set from a fully-actuated
signal control system in Zurich, Switzerland, show that machine learning models
outperform conventional prediction methods. Furthermore, tree-based decision
models such as the RF perform best with an accuracy that meets requirements for
practical applications.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Doc2Graph: a Task Agnostic Document Understanding Framework based on Graph Neural Networks. (arXiv:2208.11168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11168">http://arxiv.org/abs/2208.11168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11168] Doc2Graph: a Task Agnostic Document Understanding Framework based on Graph Neural Networks](http://arxiv.org/abs/2208.11168)</code></li>
<li>Summary: <p>Geometric Deep Learning has recently attracted significant interest in a wide
range of machine learning fields, including document analysis. The application
of Graph Neural Networks (GNNs) has become crucial in various document-related
tasks since they can unravel important structural patterns, fundamental in key
information extraction processes. Previous works in the literature propose
task-driven models and do not take into account the full power of graphs. We
propose Doc2Graph, a task-agnostic document understanding framework based on a
GNN model, to solve different tasks given different types of documents. We
evaluated our approach on two challenging datasets for key information
extraction in form understanding, invoice layout analysis and table detection.
Our code is freely accessible on https://github.com/andreagemelli/doc2graph.
</p></li>
</ul>

<h3>Title: Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents. (arXiv:2208.11203v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11203">http://arxiv.org/abs/2208.11203</a></li>
<li>Code URL: <a href="https://github.com/ailab-unifi/gnn-tableextraction">https://github.com/ailab-unifi/gnn-tableextraction</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11203] Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents](http://arxiv.org/abs/2208.11203)</code></li>
<li>Summary: <p>Tables are widely used in several types of documents since they can bring
important information in a structured way. In scientific papers, tables can sum
up novel discoveries and summarize experimental results, making the research
comparable and easily understandable by scholars. Several methods perform table
analysis working on document images, losing useful information during the
conversion from the PDF files since OCR tools can be prone to recognition
errors, in particular for text inside tables. The main contribution of this
work is to tackle the problem of table extraction, exploiting Graph Neural
Networks. Node features are enriched with suitably designed representation
embeddings. These representations help to better distinguish not only tables
from the other parts of the paper, but also table cells from table headers. We
experimentally evaluated the proposed approach on a new dataset obtained by
merging the information provided in the PubLayNet and PubTables-1M datasets.
</p></li>
</ul>

<h3>Title: Q-Net: Query-Informed Few-Shot Medical Image Segmentation. (arXiv:2208.11451v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11451">http://arxiv.org/abs/2208.11451</a></li>
<li>Code URL: <a href="https://github.com/zjlab-ammi/q-net">https://github.com/zjlab-ammi/q-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11451] Q-Net: Query-Informed Few-Shot Medical Image Segmentation](http://arxiv.org/abs/2208.11451)</code></li>
<li>Summary: <p>Deep learning has achieved tremendous success in computer vision, while
medical image segmentation (MIS) remains a challenge, due to the scarcity of
data annotations. Meta-learning techniques for few-shot segmentation (Meta-FSS)
have been widely used to tackle this challenge, while they neglect possible
distribution shifts between the query image and the support set. In contrast,
an experienced clinician can perceive and address such shifts by borrowing
information from the query image, then fine-tune or calibrate his (her) prior
cognitive model accordingly. Inspired by this, we propose Q-Net, a
Query-informed Meta-FSS approach, which mimics in spirit the learning mechanism
of an expert clinician. We build Q-Net based on ADNet, a recently proposed
anomaly detection-inspired method. Specifically, we add two query-informed
computation modules into ADNet, namely a query-informed threshold adaptation
module and a query-informed prototype refinement module. Combining them with a
dual-path extension of the feature extraction module, Q-Net achieves
state-of-the-art performance on two widely used datasets, which are composed of
abdominal MR images and cardiac MR images, respectively. Our work sheds light
on a novel way to improve Meta-FSS techniques by leveraging query information.
</p></li>
</ul>

<h3>Title: Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages. (arXiv:2208.11467v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11467">http://arxiv.org/abs/2208.11467</a></li>
<li>Code URL: <a href="https://github.com/funkelab/linajea">https://github.com/funkelab/linajea</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11467] Tracking by weakly-supervised learning and graph optimization for whole-embryo C](http://arxiv.org/abs/2208.11467)</code></li>
<li>Summary: <p>Tracking all nuclei of an embryo in noisy and dense fluorescence microscopy
data is a challenging task. We build upon a recent method for nuclei tracking
that combines weakly-supervised learning from a small set of nuclei center
point annotations with an integer linear program (ILP) for optimal cell lineage
extraction. Our work specifically addresses the following challenging
properties of C. elegans embryo recordings: (1) Many cell divisions as compared
to benchmark recordings of other organisms, and (2) the presence of polar
bodies that are easily mistaken as cell nuclei. To cope with (1), we devise and
incorporate a learnt cell division detector. To cope with (2), we employ a
learnt polar body detector. We further propose automated ILP weights tuning via
a structured SVM, alleviating the need for tedious manual set-up of a
respective grid search. Our method outperforms the previous leader of the cell
tracking challenge on the Fluo-N3DH-CE embryo dataset. We report a further
extensive quantitative evaluation on two more C. elegans datasets. We will make
these datasets public to serve as an extended benchmark for future method
development. Our results suggest considerable improvements yielded by our
method, especially in terms of the correctness of division event detection and
the number and length of fully correct track segments. Code:
https://github.com/funkelab/linajea
</p></li>
</ul>

<h3>Title: ssFPN: Scale Sequence (S^2) Feature Based Feature Pyramid Network for Object Detection. (arXiv:2208.11533v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11533">http://arxiv.org/abs/2208.11533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11533] ssFPN: Scale Sequence (S^2) Feature Based Feature Pyramid Network for Object Detection](http://arxiv.org/abs/2208.11533)</code></li>
<li>Summary: <p>Feature Pyramid Network (FPN) has been an essential module for object
detection models to consider various scales of an object. However, average
precision (AP) on small objects is relatively lower than AP on medium and large
objects. The reason is why the deeper layer of CNN causes information loss as
feature extraction level. We propose a new scale sequence (S^2) feature
extraction of FPN to strengthen feature information of small objects. We
consider FPN structure as scale-space and extract scale sequence (S^2) feature
by 3D convolution on the level axis of FPN. It is basically scale invariant
feature and is built on high-resolution pyramid feature map for small objects.
Furthermore, the proposed S^2 feature can be extended to most object detection
models based on FPN. We demonstrate the proposed S2 feature can improve the
performance of both one-stage and two-stage detectors on MS COCO dataset. Based
on the proposed S2 feature, we achieve upto 1.3% and 1.1% of AP improvement for
YOLOv4-P5 and YOLOv4-P6, respectively. For Faster RCNN and Mask R-CNN, we
observe upto 2.0% and 1.6% of AP improvement with the suggested S^2 feature,
respectively.
</p></li>
</ul>

<h3>Title: A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis. (arXiv:2208.11283v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11283">http://arxiv.org/abs/2208.11283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11283] A Hierarchical Interactive Network for Joint Span-based Aspect-Sentiment Analysis](http://arxiv.org/abs/2208.11283)</code></li>
<li>Summary: <p>Recently, some span-based methods have achieved encouraging performances for
joint aspect-sentiment analysis, which first extract aspects (aspect
extraction) by detecting aspect boundaries and then classify the span-level
sentiments (sentiment classification). However, most existing approaches either
sequentially extract task-specific features, leading to insufficient feature
interactions, or they encode aspect features and sentiment features in a
parallel manner, implying that feature representation in each task is largely
independent of each other except for input sharing. Both of them ignore the
internal correlations between the aspect extraction and sentiment
classification. To solve this problem, we novelly propose a hierarchical
interactive network (HI-ASA) to model two-way interactions between two tasks
appropriately, where the hierarchical interactions involve two steps:
shallow-level interaction and deep-level interaction. First, we utilize
cross-stitch mechanism to combine the different task-specific features
selectively as the input to ensure proper two-way interactions. Second, the
mutual information technique is applied to mutually constrain learning between
two tasks in the output layer, thus the aspect input and the sentiment input
are capable of encoding features of the other task via backpropagation.
Extensive experiments on three real-world datasets demonstrate HI-ASA's
superiority over baselines.
</p></li>
</ul>

<h3>Title: Molecular Substructure-Aware Network for Drug-Drug Interaction Prediction. (arXiv:2208.11267v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11267">http://arxiv.org/abs/2208.11267</a></li>
<li>Code URL: <a href="https://github.com/Hienyriux/MSAN">https://github.com/Hienyriux/MSAN</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11267] Molecular Substructure-Aware Network for Drug-Drug Interaction Prediction](http://arxiv.org/abs/2208.11267)</code></li>
<li>Summary: <p>Concomitant administration of drugs can cause drug-drug interactions (DDIs).
Some drug combinations are beneficial, but other ones may cause negative
effects which are previously unrecorded. Previous works on DDI prediction
usually rely on hand-engineered domain knowledge, which is laborious to obtain.
In this work, we propose a novel model, Molecular Substructure-Aware Network
(MSAN), to effectively predict potential DDIs from molecular structures of drug
pairs. We adopt a Transformer-like substructure extraction module to acquire a
fixed number of representative vectors that are associated with various
substructure patterns of the drug molecule. Then, interaction strength between
the two drugs' substructures will be captured by a similarity-based interaction
module. We also perform a substructure dropping augmentation before graph
encoding to alleviate overfitting. Experimental results from a real-world
dataset reveal that our proposed model achieves the state-of-the-art
performance. We also show that the predictions of our model are highly
interpretable through a case study.
</p></li>
</ul>

<h3>Title: A Review of Knowledge Graph Completion. (arXiv:2208.11652v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11652">http://arxiv.org/abs/2208.11652</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11652] A Review of Knowledge Graph Completion](http://arxiv.org/abs/2208.11652)</code></li>
<li>Summary: <p>Information extraction methods proved to be effective at triple extraction
from structured or unstructured data. The organization of such triples in the
form of (head entity, relation, tail entity) is called the construction of
Knowledge Graphs (KGs). Most of the current knowledge graphs are incomplete. In
order to use KGs in downstream tasks, it is desirable to predict missing links
in KGs. Different approaches have been recently proposed for representation
learning of KGs by embedding both entities and relations into a low-dimensional
vector space aiming to predict unknown triples based on previously visited
triples. According to how the triples will be treated independently or
dependently, we divided the task of knowledge graph completion into
conventional and graph neural network representation learning and we discuss
them in more detail. In conventional approaches, each triple will be processed
independently and in GNN-based approaches, triples also consider their local
neighborhood. View Full-Text
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Achieving Fairness in Dermatological Disease Diagnosis through Automatic Weight Adjusting Federated Learning and Personalization. (arXiv:2208.11187v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11187">http://arxiv.org/abs/2208.11187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11187] Achieving Fairness in Dermatological Disease Diagnosis through Automatic Weight Adjusting Federated Learning and Personalization](http://arxiv.org/abs/2208.11187)</code></li>
<li>Summary: <p>Dermatological diseases pose a major threat to the global health, affecting
almost one-third of the world's population. Various studies have demonstrated
that early diagnosis and intervention are often critical to prognosis and
outcome. To this end, the past decade has witnessed the rapid evolvement of
deep learning based smartphone apps, which allow users to conveniently and
timely identify issues that have emerged around their skins. In order to
collect sufficient data needed by deep learning and at the same time protect
patient privacy, federated learning is often used, where individual clients
aggregate a global model while keeping datasets local. However, existing
federated learning frameworks are mostly designed to optimize the overall
performance, while common dermatological datasets are heavily imbalanced. When
applying federated learning to such datasets, significant disparities in
diagnosis accuracy may occur. To address such a fairness issue, this paper
proposes a fairness-aware federated learning framework for dermatological
disease diagnosis. The framework is divided into two stages: In the first in-FL
stage, clients with different skin types are trained in a federated learning
process to construct a global model for all skin types. An automatic weight
aggregator is used in this process to assign higher weights to the client with
higher loss, and the intensity of the aggregator is determined by the level of
difference between losses. In the latter post-FL stage, each client fine-tune
its personalized model based on the global model in the in-FL stage. To achieve
better fairness, models from different epochs are selected for each client to
keep the accuracy difference of different skin types within 0.05. Experiments
indicate that our proposed framework effectively improves both fairness and
accuracy compared with the state-of-the-art.
</p></li>
</ul>

<h3>Title: Exact Penalty Method for Federated Learning. (arXiv:2208.11231v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11231">http://arxiv.org/abs/2208.11231</a></li>
<li>Code URL: <a href="https://github.com/shenglongzhou/fedepm">https://github.com/shenglongzhou/fedepm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11231] Exact Penalty Method for Federated Learning](http://arxiv.org/abs/2208.11231)</code></li>
<li>Summary: <p>Federated learning has burgeoned recently in machine learning, giving rise to
a variety of research topics. Popular optimization algorithms are based on the
frameworks of the (stochastic) gradient descent methods or the alternating
direction method of multipliers. In this paper, we deploy an exact penalty
method to deal with federated learning and propose an algorithm, FedEPM, that
enables to tackle four critical issues in federated learning: communication
efficiency, computational complexity, stragglers' effect, and data privacy.
Moreover, it is proven to be convergent and testified to have high numerical
performance.
</p></li>
</ul>

<h3>Title: Adaptive Resource Allocation in Quantum Key Distribution (QKD) for Federated Learning. (arXiv:2208.11270v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11270">http://arxiv.org/abs/2208.11270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11270] Adaptive Resource Allocation in Quantum Key Distribution (QKD) for Federated Learning](http://arxiv.org/abs/2208.11270)</code></li>
<li>Summary: <p>Increasing privacy and security concerns in intelligence-native 6G networks
require quantum key distribution-secured federated learning (QKD-FL), in which
data owners connected via quantum channels can train an FL global model
collaboratively without exposing their local datasets. To facilitate QKD-FL,
the architectural design and routing management framework are essential.
However, effective implementation is still lacking. To this end, we propose a
hierarchical architecture for QKD-FL systems in which QKD resources (i.e.,
wavelengths) and routing are jointly optimized for FL applications. In
particular, we focus on adaptive QKD resource allocation and routing for FL
workers to minimize the deployment cost of QKD nodes under various
uncertainties, including security requirements. The experimental results show
that the proposed architecture and the resource allocation and routing model
can reduce the deployment cost by 7.72\% compared to the CO-QBN algorithm.
</p></li>
</ul>

<h3>Title: Federated Self-Supervised Contrastive Learning and Masked Autoencoder for Dermatological Disease Diagnosis. (arXiv:2208.11278v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11278">http://arxiv.org/abs/2208.11278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11278] Federated Self-Supervised Contrastive Learning and Masked Autoencoder for Dermatological Disease Diagnosis](http://arxiv.org/abs/2208.11278)</code></li>
<li>Summary: <p>In dermatological disease diagnosis, the private data collected by mobile
dermatology assistants exist on distributed mobile devices of patients.
Federated learning (FL) can use decentralized data to train models while
keeping data local. Existing FL methods assume all the data have labels.
However, medical data often comes without full labels due to high labeling
costs. Self-supervised learning (SSL) methods, contrastive learning (CL) and
masked autoencoders (MAE), can leverage the unlabeled data to pre-train models,
followed by fine-tuning with limited labels. However, combining SSL and FL has
unique challenges. For example, CL requires diverse data but each device only
has limited data. For MAE, while Vision Transformer (ViT) based MAE has higher
accuracy over CNNs in centralized learning, MAE's performance in FL with
unlabeled data has not been investigated. Besides, the ViT synchronization
between the server and clients is different from traditional CNNs. Therefore,
special synchronization methods need to be designed. In this work, we propose
two federated self-supervised learning frameworks for dermatological disease
diagnosis with limited labels. The first one features lower computation costs,
suitable for mobile devices. The second one features high accuracy and fits
high-performance servers. Based on CL, we proposed federated contrastive
learning with feature sharing (FedCLF). Features are shared for diverse
contrastive information without sharing raw data for privacy. Based on MAE, we
proposed FedMAE. Knowledge split separates the global and local knowledge
learned from each client. Only global knowledge is aggregated for higher
generalization performance. Experiments on dermatological disease datasets show
superior accuracy of the proposed frameworks over state-of-the-arts.
</p></li>
</ul>

<h3>Title: Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11311">http://arxiv.org/abs/2208.11311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11311] Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments](http://arxiv.org/abs/2208.11311)</code></li>
<li>Summary: <p>We introduce a novel federated learning framework, FedD3, which reduces the
overall communication volume and with that opens up the concept of federated
learning to more application scenarios in network-constrained environments. It
achieves this by leveraging local dataset distillation instead of traditional
learning approaches (i) to significantly reduce communication volumes and (ii)
to limit transfers to one-shot communication, rather than iterative multiway
communication. Instead of sharing model updates, as in other federated learning
approaches, FedD3 allows the connected clients to distill the local datasets
independently, and then aggregates those decentralized distilled datasets
(typically in the form a few unrecognizable images, which are normally smaller
than a model) across the network only once to form the final model. Our
experimental results show that FedD3 significantly outperforms other federated
learning frameworks in terms of needed communication volumes, while it provides
the additional benefit to be able to balance the trade-off between accuracy and
communication cost, depending on usage scenario or target dataset. For
instance, for training an AlexNet model on a Non-IID CIFAR-10 dataset with 10
clients, FedD3 can either increase the accuracy by over 71% with a similar
communication volume, or save 98% of communication volume, while reaching the
same accuracy, comparing to other one-shot federated learning approaches.
</p></li>
</ul>

<h3>Title: Towards Sparsified Federated Neuroimaging Models via Weight Pruning. (arXiv:2208.11669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11669">http://arxiv.org/abs/2208.11669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11669] Towards Sparsified Federated Neuroimaging Models via Weight Pruning](http://arxiv.org/abs/2208.11669)</code></li>
<li>Summary: <p>Federated training of large deep neural networks can often be restrictive due
to the increasing costs of communicating the updates with increasing model
sizes. Various model pruning techniques have been designed in centralized
settings to reduce inference times. Combining centralized pruning techniques
with federated training seems intuitive for reducing communication costs -- by
pruning the model parameters right before the communication step. Moreover,
such a progressive model pruning approach during training can also reduce
training times/costs. To this end, we propose FedSparsify, which performs model
pruning during federated training. In our experiments in centralized and
federated settings on the brain age prediction task (estimating a person's age
from their brain MRI), we demonstrate that models can be pruned up to 95%
sparsity without affecting performance even in challenging federated learning
environments with highly heterogeneous data distributions. One surprising
benefit of model pruning is improved model privacy. We demonstrate that models
with high sparsity are less susceptible to membership inference attacks, a type
of privacy attack.
</p></li>
</ul>

<h3>Title: PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation Model. (arXiv:2208.11625v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11625">http://arxiv.org/abs/2208.11625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11625] PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models -- Federated Learning in Age of Foundation Model](http://arxiv.org/abs/2208.11625)</code></li>
<li>Summary: <p>Quick global aggregation of effective distributed parameters is crucial to
federated learning (FL), which requires adequate bandwidth for parameters
communication and sufficient user data for local training. Otherwise, FL may
cost excessive training time for convergence and produce inaccurate models. In
this paper, we propose a brand-new FL framework, PromptFL, that replaces the
federated model training with the federated prompt training, i.e., let
federated participants train prompts instead of a shared model, to
simultaneously achieve the efficient global aggregation and local training on
insufficient data by exploiting the power of foundation models (FM) in a
distributed way. PromptFL ships an off-the-shelf FM, i.e., CLIP, to distributed
clients who would cooperatively train shared soft prompts based on very few
local data. Since PromptFL only needs to update the prompts instead of the
whole model, both the local training and the global aggregation can be
significantly accelerated. And FM trained over large scale data can provide
strong adaptation capability to distributed users tasks with the trained soft
prompts. We empirically analyze the PromptFL via extensive experiments, and
show its superiority in terms of system feasibility, user privacy, and
performance.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: DeepPicarMicro: Applying TinyML to Autonomous Cyber Physical Systems. (arXiv:2208.11212v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11212">http://arxiv.org/abs/2208.11212</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11212] DeepPicarMicro: Applying TinyML to Autonomous Cyber Physical Systems](http://arxiv.org/abs/2208.11212)</code></li>
<li>Summary: <p>Running deep neural networks (DNNs) on tiny Micro-controller Units (MCUs) is
challenging due to their limitations in computing, memory, and storage
capacity. Fortunately, recent advances in both MCU hardware and machine
learning software frameworks make it possible to run fairly complex neural
networks on modern MCUs, resulting in a new field of study widely known as
TinyML. However, there have been few studies to show the potential for TinyML
applications in cyber physical systems (CPS). In this paper, we present
DeepPicarMicro, a small self-driving RC car testbed, which runs a convolutional
neural network (CNN) on a Raspberry Pi Pico MCU. We apply a state-of-the-art
DNN optimization to successfully fit the well-known PilotNet CNN architecture,
which was used to drive NVIDIA's real self-driving car, on the MCU. We apply a
state-of-art network architecture search (NAS) approach to find further
optimized networks that can effectively control the car in real-time in an
end-to-end manner. From an extensive systematic experimental evaluation study,
we observe an interesting relationship between the accuracy, latency, and
control performance of a system. From this, we propose a joint optimization
strategy that takes both accuracy and latency of a model in the network
architecture search process for AI enabled CPS.
</p></li>
</ul>

<h3>Title: TESTSGD: Interpretable Testing of Neural Networks Against Subtle Group Discrimination. (arXiv:2208.11321v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11321">http://arxiv.org/abs/2208.11321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11321] TESTSGD: Interpretable Testing of Neural Networks Against Subtle Group Discrimination](http://arxiv.org/abs/2208.11321)</code></li>
<li>Summary: <p>Discrimination has been shown in many machine learning applications, which
calls for sufficient fairness testing before their deployment in ethic-relevant
domains such as face recognition, medical diagnosis and criminal sentence.
Existing fairness testing approaches are mostly designed for identifying
individual discrimination, i.e., discrimination against individuals. Yet, as
another widely concerning type of discrimination, testing against group
discrimination, mostly hidden, is much less studied. To address the gap, in
this work, we propose TESTSGD, an interpretable testing approach which
systematically identifies and measures hidden (which we call `subtle' group
discrimination} of a neural network characterized by conditions over
combinations of the sensitive features. Specifically, given a neural network,
TESTSGDfirst automatically generates an interpretable rule set which
categorizes the input space into two groups exposing the model's group
discrimination. Alongside, TESTSGDalso provides an estimated group fairness
score based on sampling the input space to measure the degree of the identified
subtle group discrimination, which is guaranteed to be accurate up to an error
bound. We evaluate TESTSGDon multiple neural network models trained on popular
datasets including both structured data and text data. The experiment results
show that TESTSGDis effective and efficient in identifying and measuring such
subtle group discrimination that has never been revealed before. Furthermore,
we show that the testing results of TESTSGDcan guide generation of new samples
to mitigate such discrimination through retraining with negligible accuracy
drop.
</p></li>
</ul>

<h3>Title: A novel approach for Fair Principal Component Analysis based on eigendecomposition. (arXiv:2208.11362v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11362">http://arxiv.org/abs/2208.11362</a></li>
<li>Code URL: <a href="https://github.com/guilhermepelegrina/fpca">https://github.com/guilhermepelegrina/fpca</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11362] A novel approach for Fair Principal Component Analysis based on eigendecomposition](http://arxiv.org/abs/2208.11362)</code></li>
<li>Summary: <p>Principal component analysis (PCA), a ubiquitous dimensionality reduction
technique in signal processing, searches for a projection matrix that minimizes
the mean squared error between the reduced dataset and the original one. Since
classical PCA is not tailored to address concerns related to fairness, its
application to actual problems may lead to disparity in the reconstruction
errors of different groups (e.g., men and women, whites and blacks, etc.), with
potentially harmful consequences such as the introduction of bias towards
sensitive groups. Although several fair versions of PCA have been proposed
recently, there still remains a fundamental gap in the search for algorithms
that are simple enough to be deployed in real systems. To address this, we
propose a novel PCA algorithm which tackles fairness issues by means of a
simple strategy comprising a one-dimensional search which exploits the
closed-form solution of PCA. As attested by numerical experiments, the proposal
can significantly improve fairness with a very small loss in the overall
reconstruction error and without resorting to complex optimization schemes.
Moreover, our findings are consistent in several real situations as well as in
scenarios with both unbalanced and balanced datasets.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability. (arXiv:2208.11401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11401">http://arxiv.org/abs/2208.11401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11401] Radial Basis Function Networks for Convolutional Neural Networks to Learn Similarity Distance Metric and Improve Interpretability](http://arxiv.org/abs/2208.11401)</code></li>
<li>Summary: <p>Radial basis function neural networks (RBFs) are prime candidates for pattern
classification and regression and have been used extensively in classical
machine learning applications. However, RBFs have not been integrated into
contemporary deep learning research and computer vision using conventional
convolutional neural networks (CNNs) due to their lack of adaptability with
modern architectures. In this paper, we adapt RBF networks as a classifier on
top of CNNs by modifying the training process and introducing a new activation
function to train modern vision architectures end-to-end for image
classification. The specific architecture of RBFs enables the learning of a
similarity distance metric to compare and find similar and dissimilar images.
Furthermore, we demonstrate that using an RBF classifier on top of any CNN
architecture provides new human-interpretable insights about the
decision-making process of the models. Finally, we successfully apply RBFs to a
range of CNN architectures and evaluate the results on benchmark computer
vision datasets.
</p></li>
</ul>

<h3>Title: Hybrid Fusion Based Interpretable Multimodal Emotion Recognition with Insufficient Labelled Data. (arXiv:2208.11450v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.11450">http://arxiv.org/abs/2208.11450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.11450] Hybrid Fusion Based Interpretable Multimodal Emotion Recognition with Insufficient Labelled Data](http://arxiv.org/abs/2208.11450)</code></li>
<li>Summary: <p>This paper proposes a multimodal emotion recognition system, VIsual Spoken
Textual Additive Net (VISTA Net), to classify the emotions reflected by a
multimodal input containing image, speech, and text into discrete classes. A
new interpretability technique, K-Average Additive exPlanation (KAAP), has also
been developed to identify the important visual, spoken, and textual features
leading to predicting a particular emotion class. The VISTA Net fuses the
information from image, speech &amp; text modalities using a hybrid of early and
late fusion. It automatically adjusts the weights of their intermediate outputs
while computing the weighted average without human intervention. The KAAP
technique computes the contribution of each modality and corresponding features
toward predicting a particular emotion class. To mitigate the insufficiency of
multimodal emotion datasets labeled with discrete emotion classes, we have
constructed a large-scale IIT-R MMEmoRec dataset consisting of real-life
images, corresponding speech &amp; text, and emotion labels ('angry,' 'happy,'
'hate,' and 'sad.'). The VISTA Net has resulted in 95.99% emotion recognition
accuracy on considering image, speech, and text modalities, which is better
than the performance on considering the inputs of any one or two modalities.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
