<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Poster: No safety in numbers: traffic analysis of sealed-sender groups in Signal. (arXiv:2305.09799v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09799">http://arxiv.org/abs/2305.09799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09799] Poster: No safety in numbers: traffic analysis of sealed-sender groups in Signal](http://arxiv.org/abs/2305.09799) #secure</code></li>
<li>Summary: <p>Secure messaging applications often offer privacy to users by protecting
their messages from would be observers through end-to-end encryption
techniques. However, the metadata of who communicates with whom cannot be
concealed by encryption alone. Signal's Sealed Sender mechanism attempts to
enhance its protection of this data by obfuscating the sender of any message
sent with the protocol. However, it was shown by Martiny et al. that due to the
message delivery protocols in Signal, the record of who receives messages can
be enough to recover this metadata. In this work we extend the attack presented
from deanonymizing communicating pairs to deanonymizing entire group
conversations.
</p></li>
</ul>

<h3>Title: Entanglement-based Mutual Quantum Distance Bounding. (arXiv:2305.09905v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09905">http://arxiv.org/abs/2305.09905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09905] Entanglement-based Mutual Quantum Distance Bounding](http://arxiv.org/abs/2305.09905) #secure</code></li>
<li>Summary: <p>Mutual distance bounding (DB) protocols enable two distrusting parties to
establish an upper-bound on the distance between them. DB has been so far
mainly considered in classical settings and for classical applications,
especially in wireless settings, e.g., to prevent relay attacks in wireless
authentication and access control systems, and for secure localization. While
recent research has started exploring DB in quantum settings, all current
quantum DB (QDB) protocols employ quantum-bits (qubits) in the rapid-bit
exchange phase and only perform one-way DB. Specifically, the latest QDB
proposals improve the initial ones by adding resistance to photon number
splitting attacks, and improving round complexity by avoiding communication
from the prover to the verifier in the last authentication phase. This paper
presents two new QDB protocols that differ from previously proposed protocols
in several aspects: (1) to the best of our knowledge, our protocols are the
first to utilize entangled qubits in the rapid-bit exchange phase, previous
protocols relied on sending individual qubits, not those from a pair of
entangled ones; (2) our second protocol can perform mutual QDB between two
parties in one execution, previous QDB protocols had to be executed twice with
the prover and verifier roles reversed in each execution; (3) the use of
entangled qubits in our protocols thwarts attacks that previous QDB protocols
were prone to; (4) and finally, our protocols also eliminate the need for
communication from the prover to the verifier in the last authentication phase,
which was necessary in some previous QDB protocols. Our work paves the way for
several interesting research directions which we briefly discuss in detail in
the appendix.
</p></li>
</ul>

<h3>Title: Towards Data Redaction in Bitcoin. (arXiv:2305.10075v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10075">http://arxiv.org/abs/2305.10075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10075] Towards Data Redaction in Bitcoin](http://arxiv.org/abs/2305.10075) #secure</code></li>
<li>Summary: <p>A major issue for many applications of blockchain technology is the tension
between immutability and compliance to regulations. For instance, the GDPR in
the EU requires to guarantee, under some circumstances, the right to be
forgotten. This could imply that at some point one might be forced to delete
some data from a locally stored blockchain, therefore irreparably hurting the
security and transparency of such decentralized platforms.
</p></li>
</ul>

<p>Motivated by such data protection and consistency issues, in this work we
design and implement a mechanism for securely deleting data from Bitcoin
blockchain. We use zero-knowledge proofs to allow any node to delete some data
from Bitcoin transactions, still preserving the public verifiability of the
correctness of the spent and spendable coins. Moreover, we specifically use
STARK proofs to exploit the transparency that they provide.
</p>
<p>Our solution, unlike previous approaches, avoids the complications of asking
nodes to reach consensus on the content to delete. In particular, our design
allows every node to delete some specific data without coordinating this
decision with others. In our implementation, data removal can be performed
(resp., verified) in minutes (resp., seconds) on a standard laptop rather than
in days as required in previous designs based on consensus.
</p>

<h2>security</h2>
<h3>Title: Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09782">http://arxiv.org/abs/2305.09782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09782] Analysis of Visual Question Answering Algorithms with attention model](http://arxiv.org/abs/2305.09782) #security</code></li>
<li>Summary: <p>Visual question answering (VQA) usesimage processing algorithms to process
the image and natural language processing methods to understand and answer the
question. VQA is helpful to a visually impaired person, can be used for the
security surveillance system and online chatbots that learn from the web. It
uses NLP methods to learn the semantic of the question and to derive the
textual features. Computer vision techniques are used for generating image
representation in such a way that they can identify the objects about which
question is asked. The Attention model tries to mimic the human behavior of
giving attention to a different region of an image according to our
understanding of its context. This paper critically examines and reviews
methods of VQA algorithm such as generation of semantics of text,
identification of objects and answer classification techniques that use the
co-attention approach.
</p></li>
</ul>

<h3>Title: Vulnerability Detection Using Two-Stage Deep Learning Models. (arXiv:2305.09673v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09673">http://arxiv.org/abs/2305.09673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09673] Vulnerability Detection Using Two-Stage Deep Learning Models](http://arxiv.org/abs/2305.09673) #security</code></li>
<li>Summary: <p>Application security is an essential part of developing modern software, as
lots of attacks depend on vulnerabilities in software. The number of attacks is
increasing globally due to technological advancements. Companies must include
security in every stage of developing, testing, and deploying their software in
order to prevent data breaches. There are several methods to detect software
vulnerability Non-AI-based such as Static Application Security Testing (SAST)
and Dynamic Application Security Testing (DAST). However, these approaches have
substantial false-positive and false-negative rates. On the other side,
researchers have been interested in developing an AI-based vulnerability
detection system employing deep learning models like BERT, BLSTM, etc. In this
paper, we proposed a two-stage solution, two deep learning models were proposed
for vulnerability detection in C/C++ source codes, the first stage is CNN which
detects if the source code contains any vulnerability (binary classification
model) and the second stage is CNN-LTSM that classifies this vulnerability into
a class of 50 different types of vulnerabilities (multiclass classification
model). Experiments were done on SySeVR dataset. Results show an accuracy of
99% for the first and 98% for the second stage.
</p></li>
</ul>

<h3>Title: Adversarial Security and Differential Privacy in mmWave Beam Prediction in 6G networks. (arXiv:2305.09679v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09679">http://arxiv.org/abs/2305.09679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09679] Adversarial Security and Differential Privacy in mmWave Beam Prediction in 6G networks](http://arxiv.org/abs/2305.09679) #security</code></li>
<li>Summary: <p>In the forthcoming era of 6G, the mmWave communication is envisioned to be
used in dense user scenarios with high bandwidth requirements, that necessitate
efficient and accurate beam prediction. Machine learning (ML) based approaches
are ushering as a critical solution for achieving such efficient beam
prediction for 6G mmWave communications. However, most contemporary ML
classifiers are quite susceptible to adversarial inputs. Attackers can easily
perturb the methodology through noise addition in the model itself. To mitigate
this, the current work presents a defensive mechanism for attenuating the
adversarial attacks against projected ML-based models for mmWave beam
anticipation by incorporating adversarial training. Furthermore, as training 6G
mmWave beam prediction model necessitates the use of large and comprehensive
datasets that could include sensitive information regarding the user's
location, differential privacy (DP) has been introduced as a technique to
preserve the confidentiality of the information by purposefully adding a low
sensitivity controlled noise in the datasets. It ensures that even if the
information about a user location could be retrieved, the attacker would have
no means to determine whether the information is significant or meaningless.
With ray-tracing simulations for various outdoor and indoor scenarios, we
illustrate the advantage of our proposed novel framework in terms of beam
prediction accuracy and effective achievable rate while ensuring the security
and privacy in communications.
</p></li>
</ul>

<h3>Title: Four Factor Authentication with emerging cybersecurity for Mobile Transactions. (arXiv:2305.09740v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09740">http://arxiv.org/abs/2305.09740</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09740] Four Factor Authentication with emerging cybersecurity for Mobile Transactions](http://arxiv.org/abs/2305.09740) #security</code></li>
<li>Summary: <p>Cybersecurity is very essential for Mobile Transactions to complete
seamlessly. Mobile Commerce (Mcom.) is the very basic transaction type, which
is very commonly used (2 in 5 people uses mobile as transaction medium), To
secure this there are various technologies used by this research. The four
factors formally known as Multi-Factor-Authentication are: two of them are
Traditional methods (User Login-password and One Time Password (aka OTP)) with
addition of Geolocation and Facial Recognition. All the data is converted to a
text file, which is hidden in an image (using Babushka algorithm). The
end-point then decrypts the image using same algorithm.
</p></li>
</ul>

<h3>Title: Physical Layer Authentication and Security Design in the Machine Learning Era. (arXiv:2305.09748v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09748">http://arxiv.org/abs/2305.09748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09748] Physical Layer Authentication and Security Design in the Machine Learning Era](http://arxiv.org/abs/2305.09748) #security</code></li>
<li>Summary: <p>Security at the physical layer (PHY) is a salient research topic in wireless
systems, and machine learning (ML) is emerging as a powerful tool for providing
new data-driven security solutions. Therefore, the application of ML techniques
to the PHY security is of crucial importance in the landscape of more and more
data-driven wireless services. In this context, we first summarize the family
of bespoke ML algorithms that are eminently suitable for wireless security.
Then, we review the recent progress in ML-aided PHY security, where the term
"PHY security" is classified into two different types: i) PHY authentication
and ii) secure PHY transmission. Moreover, we treat neural networks as special
types of ML and present how to deal with PHY security optimization problems
using neural networks. Finally, we identify some major challenges and
opportunities in tackling PHY security challenges by applying carefully
tailored ML tools.
</p></li>
</ul>

<h3>Title: Function synthesis for maximizing model counting. (arXiv:2305.10003v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10003">http://arxiv.org/abs/2305.10003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10003] Function synthesis for maximizing model counting](http://arxiv.org/abs/2305.10003) #security</code></li>
<li>Summary: <p>Given a boolean formula $\Phi$(X, Y, Z), the Max#SAT problem asks for
finding a partial model on the set of variables X, maximizing its number of
projected models over the set of variables Y. We investigate a strict
generalization of Max#SAT allowing dependencies for variables in X,
effectively turning it into a synthesis problem. We show that this new problem,
called DQMax#SAT, subsumes the DQBF problem as well. We provide a general
resolution method, based on a reduction to Max#SAT, together with two
improvements for dealing with its inherent complexity. We further discuss a
concrete application of DQMax#SAT for symbolic synthesis of adaptive attackers
in the field of program security. Finally, we report preliminary results
obtained on the resolution on benchmark problems using a prototype DQMax#SAT
solver implementation.
</p></li>
</ul>

<h3>Title: A 334$\mu$W 0.158mm$^2$ ASIC for Post-Quantum Key-Encapsulation Mechanism Saber with Low-latency Striding Toom-Cook Multiplication Authors Version. (arXiv:2305.10368v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10368">http://arxiv.org/abs/2305.10368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10368] A 334$\mu$W 0](http://arxiv.org/abs/2305.10368) #security</code></li>
<li>Summary: <p>The hard mathematical problems that assure the security of our current
public-key cryptography (RSA, ECC) are broken if and when a quantum computer
appears rendering them ineffective for use in the quantum era. Lattice based
cryptography is a novel approach to public key cryptography, of which the
mathematical investigation (so far) resists attacks from quantum computers. By
choosing a module learning with errors (MLWE) algorithm as the next standard,
National Institute of Standard &amp; Technology (NIST) follows this approach. The
multiplication of polynomials is the central bottleneck in the computation of
lattice based cryptography. Because public key cryptography is mostly used to
establish common secret keys, focus is on compact area, power and energy budget
and to a lesser extent on throughput or latency. While most other work focuses
on optimizing number theoretic transform (NTT) based multiplications, in this
paper we highly optimize a Toom-Cook based multiplier. We demonstrate that a
memory-efficient striding Toom-Cook with lazy interpolation, results in a
highly compact, low power implementation, which on top enables a very regular
memory access scheme. To demonstrate the efficiency, we integrate this
multiplier into a Saber post-quantum accelerator, one of the four NIST
finalists. Algorithmic innovation to reduce active memory, timely clock gating
and shift-add multiplier has helped to achieve 38% less power than state-of-the
art PQC core, 4x less memory, 36.8% reduction in multiplier energy and 118x
reduction in active power with respect to state-of-the-art Saber accelerator
(not silicon verified). This accelerator consumes 0.158mm2 active area which is
lowest reported till date despite process disadvantages of the state-of-the-art
designs.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Face Recognition Using Synthetic Face Data. (arXiv:2305.10079v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10079">http://arxiv.org/abs/2305.10079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10079] Face Recognition Using Synthetic Face Data](http://arxiv.org/abs/2305.10079) #privacy</code></li>
<li>Summary: <p>In the field of deep learning applied to face recognition, securing
large-scale, high-quality datasets is vital for attaining precise and reliable
results. However, amassing significant volumes of high-quality real data faces
hurdles such as time limitations, financial burdens, and privacy issues.
Furthermore, prevalent datasets are often impaired by racial biases and
annotation inaccuracies. In this paper, we underscore the promising application
of synthetic data, generated through rendering digital faces via our computer
graphics pipeline, in achieving competitive results with the state-of-the-art
on synthetic data across multiple benchmark datasets. By finetuning the
model,we obtain results that rival those achieved when training with hundreds
of thousands of real images (98.7% on LFW [1]). We further investigate the
contribution of adding intra-class variance factors (e.g., makeup, accessories,
haircuts) on model performance. Finally, we reveal the sensitivity of
pre-trained face recognition models to alternating specific parts of the face
by leveraging the granular control capability in our platform.
</p></li>
</ul>

<h3>Title: Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free. (arXiv:2305.09668v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09668">http://arxiv.org/abs/2305.09668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09668] Mean Estimation Under Heterogeneous Privacy: Some Privacy Can Be Free](http://arxiv.org/abs/2305.09668) #privacy</code></li>
<li>Summary: <p>Differential Privacy (DP) is a well-established framework to quantify privacy
loss incurred by any algorithm. Traditional DP formulations impose a uniform
privacy requirement for all users, which is often inconsistent with real-world
scenarios in which users dictate their privacy preferences individually. This
work considers the problem of mean estimation under heterogeneous DP
constraints, where each user can impose their own distinct privacy level. The
algorithm we propose is shown to be minimax optimal when there are two groups
of users with distinct privacy levels. Our results elicit an interesting
saturation phenomenon that occurs as one group's privacy level is relaxed,
while the other group's privacy level remains constant. Namely, after a certain
point, further relaxing the privacy requirement of the former group does not
improve the performance of the minimax optimal mean estimator. Thus, the
central server can offer a certain degree of privacy without any sacrifice in
performance.
</p></li>
</ul>

<h3>Title: Privacy Loss of Noisy Stochastic Gradient Descent Might Converge Even for Non-Convex Losses. (arXiv:2305.09903v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09903">http://arxiv.org/abs/2305.09903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09903] Privacy Loss of Noisy Stochastic Gradient Descent Might Converge Even for Non-Convex Losses](http://arxiv.org/abs/2305.09903) #privacy</code></li>
<li>Summary: <p>The Noisy-SGD algorithm is widely used for privately training machine
learning models. Traditional privacy analyses of this algorithm assume that the
internal state is publicly revealed, resulting in privacy loss bounds that
increase indefinitely with the number of iterations. However, recent findings
have shown that if the internal state remains hidden, then the privacy loss
might remain bounded. Nevertheless, this remarkable result heavily relies on
the assumption of (strong) convexity of the loss function. It remains an
important open problem to further relax this condition while proving similar
convergent upper bounds on the privacy loss. In this work, we address this
problem for DP-SGD, a popular variant of Noisy-SGD that incorporates gradient
clipping to limit the impact of individual samples on the training process. Our
findings demonstrate that the privacy loss of projected DP-SGD converges
exponentially fast, without requiring convexity or smoothness assumptions on
the loss function. In addition, we analyze the privacy loss of regularized
(unprojected) DP-SGD. To obtain these results, we directly analyze the
hockey-stick divergence between coupled stochastic processes by relying on
non-linear data processing inequalities.
</p></li>
</ul>

<h3>Title: Blockchain-enabled Parametric Solar Energy Insurance via Remote Sensing. (arXiv:2305.09961v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09961">http://arxiv.org/abs/2305.09961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09961] Blockchain-enabled Parametric Solar Energy Insurance via Remote Sensing](http://arxiv.org/abs/2305.09961) #privacy</code></li>
<li>Summary: <p>Despite its popularity, the nature of solar energy is highly uncertain and
weather dependent, affecting the business viability and investment of solar
energy generation, especially for household users. To stabilize the income from
solar energy generation, there have been limited traditional options, such as
using energy storage to pool excessive solar energy in off-peak periods or
financial derivatives from future markets to hedge energy prices. In this
paper, we explore a novel idea of "parametric solar energy insurance", by which
solar panel owners can insure their solar energy generation based on a
verifiable geographically specific index (surface solar irradiation).
Parametric solar energy insurance offers opportunities of financial subsidies
for insufficient solar energy generation and amortizes the fluctuations of
renewable energy generation geographically. Furthermore, we propose to leverage
blockchain and remote sensing (satellite imagery) to provide a publicly
verifiable platform for solar energy insurance, which not only automates the
underwriting and claims of a solar energy insurance policy, but also improves
its accountability and transparency. We utilize the state-of-the-art succinct
zero-knowledge proofs (zk-SNARK) to realize privacy-preserving blockchain-based
solar energy insurance on real-world permissionless blockchain platform
Ethereum.
</p></li>
</ul>

<h3>Title: Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression. (arXiv:2305.09896v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09896">http://arxiv.org/abs/2305.09896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09896] Convergence and Privacy of Decentralized Nonconvex Optimization with Gradient Clipping and Communication Compression](http://arxiv.org/abs/2305.09896) #privacy</code></li>
<li>Summary: <p>Achieving communication efficiency in decentralized machine learning has been
attracting significant attention, with communication compression recognized as
an effective technique in algorithm design. This paper takes a first step to
understand the role of gradient clipping, a popular strategy in practice, in
decentralized nonconvex optimization with communication compression. We propose
PORTER, which considers two variants of gradient clipping added before or after
taking a mini-batch of stochastic gradients, where the former variant PORTER-DP
allows local differential privacy analysis with additional Gaussian
perturbation, and the latter variant PORTER-GC helps to stabilize training. We
develop a novel analysis framework that establishes their convergence
guarantees without assuming the stringent bounded gradient assumption. To the
best of our knowledge, our work provides the first convergence analysis for
decentralized nonconvex optimization with gradient clipping and communication
compression, highlighting the trade-offs between convergence rate, compression
ratio, network connectivity, and privacy.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10036">http://arxiv.org/abs/2305.10036</a></li>
<li>Code URL: <a href="https://github.com/yjw1029/embmarker">https://github.com/yjw1029/embmarker</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10036] Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark](http://arxiv.org/abs/2305.10036) #protect</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated powerful capabilities in both
text understanding and generation. Companies have begun to offer Embedding as a
Service (EaaS) based on these LLMs, which can benefit various natural language
processing (NLP) tasks for customers. However, previous studies have shown that
EaaS is vulnerable to model extraction attacks, which can cause significant
losses for the owners of LLMs, as training these models is extremely expensive.
To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark
method called EmbMarker that implants backdoors on embeddings. Our method
selects a group of moderate-frequency words from a general text corpus to form
a trigger set, then selects a target embedding as the watermark, and inserts it
into the embeddings of texts containing trigger words as the backdoor. The
weight of insertion is proportional to the number of trigger words included in
the text. This allows the watermark backdoor to be effectively transferred to
EaaS-stealer's model for copyright verification while minimizing the adverse
impact on the original embeddings' utility. Our extensive experiments on
various datasets show that our method can effectively protect the copyright of
EaaS models without compromising service quality.
</p></li>
</ul>

<h3>Title: Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10160">http://arxiv.org/abs/2305.10160</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10160] Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks](http://arxiv.org/abs/2305.10160) #protect</code></li>
<li>Summary: <p>Data contamination has become especially prevalent and challenging with the
rise of models pretrained on very large, automatically-crawled corpora. For
closed models, the training data becomes a trade secret, and even for open
models, it is not trivial to ascertain whether a particular test instance has
been compromised. Strategies such as live leaderboards with hidden answers, or
using test data which is guaranteed to be unseen, are expensive and become
fragile with time. Assuming that all relevant actors value clean test data and
will cooperate to mitigate data contamination, what can be done? We propose
three strategies that can make a difference: (1) Test data made public should
be encrypted with a public key and licensed to disallow derivative
distribution; (2) demand training exclusion controls from closed API holders,
and protect your test data by refusing to evaluate until demands are met; (3)
in case of test data based on internet text, avoid data which appears with its
solution on the internet, and release the context of internet-derived data
along with the data. These strategies are practical and can be effective in
preventing data contamination and allowing trustworthy evaluation of models'
capabilities.
</p></li>
</ul>

<h3>Title: Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection. (arXiv:2305.10204v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10204">http://arxiv.org/abs/2305.10204</a></li>
<li>Code URL: <a href="https://github.com/technion-cs-nlp/igbp_nonlinear-removal">https://github.com/technion-cs-nlp/igbp_nonlinear-removal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10204] Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection](http://arxiv.org/abs/2305.10204) #protect</code></li>
<li>Summary: <p>Natural language processing models tend to learn and encode social biases
present in the data. One popular approach for addressing such biases is to
eliminate encoded information from the model's representations. However,
current methods are restricted to removing only linearly encoded information.
In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel
method for removing non-linear encoded concepts from neural representations.
Our method consists of iteratively training neural classifiers to predict a
particular attribute we seek to eliminate, followed by a projection of the
representation on a hypersurface, such that the classifiers become oblivious to
the target attribute. We evaluate the effectiveness of our method on the task
of removing gender and race information as sensitive attributes. Our results
demonstrate that IGBP is effective in mitigating bias through intrinsic and
extrinsic evaluations, with minimal impact on downstream task accuracy.
</p></li>
</ul>

<h3>Title: Anomaly Detection Dataset for Industrial Control Systems. (arXiv:2305.09678v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09678">http://arxiv.org/abs/2305.09678</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09678] Anomaly Detection Dataset for Industrial Control Systems](http://arxiv.org/abs/2305.09678) #protect</code></li>
<li>Summary: <p>Over the past few decades, Industrial Control Systems (ICSs) have been
targeted by cyberattacks and are becoming increasingly vulnerable as more ICSs
are connected to the internet. Using Machine Learning (ML) for Intrusion
Detection Systems (IDS) is a promising approach for ICS cyber protection, but
the lack of suitable datasets for evaluating ML algorithms is a challenge.
Although there are a few commonly used datasets, they may not reflect realistic
ICS network data, lack necessary features for effective anomaly detection, or
be outdated. This paper presents the 'ICS-Flow' dataset, which offers network
data and process state variables logs for supervised and unsupervised ML-based
IDS assessment. The network data includes normal and anomalous network packets
and flows captured from simulated ICS components and emulated networks. The
anomalies were injected into the system through various attack techniques
commonly used by hackers to modify network traffic and compromise ICSs. We also
proposed open-source tools, `ICSFlowGenerator' for generating network flow
parameters from Raw network packets. The final dataset comprises over
25,000,000 raw network packets, network flow records, and process variable
logs. The paper describes the methodology used to collect and label the dataset
and provides a detailed data analysis. Finally, we implement several ML models,
including the decision tree, random forest, and artificial neural network to
detect anomalies and attacks, demonstrating that our dataset can be used
effectively for training intrusion detection ML models.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: SHATTER: Control and Defense-Aware Attack Analytics for Activity-Driven Smart Home Systems. (arXiv:2305.09669v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09669">http://arxiv.org/abs/2305.09669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09669] SHATTER: Control and Defense-Aware Attack Analytics for Activity-Driven Smart Home Systems](http://arxiv.org/abs/2305.09669) #defense</code></li>
<li>Summary: <p>Modern smart home control systems utilize real-time occupancy and activity
monitoring to ensure control efficiency, occupants' comfort, and optimal energy
consumption. Moreover, adopting machine learning-based anomaly detection models
(ADMs) enhances security and reliability. However, sufficient system knowledge
allows adversaries/attackers to alter sensor measurements through stealthy
false data injection (FDI) attacks. Although ADMs limit attack scopes, the
availability of information like occupants' location, conducted activities, and
alteration capability of smart appliances increase the attack surface.
Therefore, performing an attack space analysis of modern home control systems
is crucial to design robust defense solutions. However, state-of-the-art
analyzers do not consider contemporary control and defense solutions and
generate trivial attack vectors. To address this, we propose a control and
defense-aware novel attack analysis framework for a modern smart home control
system, efficiently extracting ADM rules. We verify and validate our framework
using a state-of-the-art dataset and a prototype testbed.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification. (arXiv:2305.09671v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09671">http://arxiv.org/abs/2305.09671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09671] Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks against Deep Image Classification](http://arxiv.org/abs/2305.09671) #attack</code></li>
<li>Summary: <p>Deep image classification models trained on large amounts of web-scraped data
are vulnerable to data poisoning, a mechanism for backdooring models. Even a
few poisoned samples seen during training can entirely undermine the model's
integrity during inference. While it is known that poisoning more samples
enhances an attack's effectiveness and robustness, it is unknown whether
poisoning too many samples weakens an attack by making it more detectable. We
observe a fundamental detectability/robustness trade-off in data poisoning
attacks: Poisoning too few samples renders an attack ineffective and not
robust, but poisoning too many samples makes it detectable. This raises the bar
for data poisoning attackers who have to balance this trade-off to remain
robust and undetectable. Our work proposes two defenses designed to (i) detect
and (ii) repair poisoned models as a post-processing step after training using
a limited amount of trusted image-label pairs. We show that our defenses
mitigate all surveyed attacks and outperform existing defenses using less
trusted data to repair a model. Our defense scales to joint vision-language
models, such as CLIP, and interestingly, we find that attacks on larger models
are more easily detectable but also more robust than those on smaller models.
Lastly, we propose two adaptive attacks demonstrating that while our work
raises the bar for data poisoning attacks, it cannot mitigate all forms of
backdooring.
</p></li>
</ul>

<h3>Title: Stealthy Low-frequency Backdoor Attack against Deep Neural Networks. (arXiv:2305.09677v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09677">http://arxiv.org/abs/2305.09677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09677] Stealthy Low-frequency Backdoor Attack against Deep Neural Networks](http://arxiv.org/abs/2305.09677) #attack</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have gain its popularity in various scenarios in
recent years. However, its excellent ability of fitting complex functions also
makes it vulnerable to backdoor attacks. Specifically, a backdoor can remain
hidden indefinitely until activated by a sample with a specific trigger, which
is hugely concealed. Nevertheless, existing backdoor attacks operate backdoors
in spatial domain, i.e., the poisoned images are generated by adding additional
perturbations to the original images, which are easy to detect. To bring the
potential of backdoor attacks into full play, we propose low-pass attack, a
novel attack scheme that utilizes low-pass filter to inject backdoor in
frequency domain. Unlike traditional poisoned image generation methods, our
approach reduces high-frequency components and preserve original images'
semantic information instead of adding additional perturbations, improving the
capability of evading current defenses. Besides, we introduce "precision mode"
to make our backdoor triggered at a specified level of filtering, which further
improves stealthiness. We evaluate our low-pass attack on four datasets and
demonstrate that even under pollution rate of 0.01, we can perform stealthy
attack without trading off attack performance. Besides, our backdoor attack can
successfully bypass state-of-the-art defending mechanisms. We also compare our
attack with existing backdoor attacks and show that our poisoned images are
nearly invisible and retain higher image quality.
</p></li>
</ul>

<h3>Title: NUANCE: Near Ultrasound Attack On Networked Communication Environments. (arXiv:2305.10358v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10358">http://arxiv.org/abs/2305.10358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10358] NUANCE: Near Ultrasound Attack On Networked Communication Environments](http://arxiv.org/abs/2305.10358) #attack</code></li>
<li>Summary: <p>This study investigates a primary inaudible attack vector on Amazon Alexa
voice services using near ultrasound trojans and focuses on characterizing the
attack surface and examining the practical implications of issuing inaudible
voice commands. The research maps each attack vector to a tactic or technique
from the MITRE ATT&amp;CK matrix, covering enterprise, mobile, and Industrial
Control System (ICS) frameworks. The experiment involved generating and
surveying fifty near-ultrasonic audios to assess the attacks' effectiveness,
with unprocessed commands having a 100% success rate and processed ones
achieving a 58% overall success rate. This systematic approach stimulates
previously unaddressed attack surfaces, ensuring comprehensive detection and
attack design while pairing each ATT&amp;CK Identifier with a tested defensive
method, providing attack and defense tactics for prompt-response options. The
main findings reveal that the attack method employs Single Upper Sideband
Amplitude Modulation (SUSBAM) to generate near-ultrasonic audio from audible
sources, transforming spoken commands into a frequency range beyond human-adult
hearing. By eliminating the lower sideband, the design achieves a 6 kHz minimum
from 16-22 kHz while remaining inaudible after transformation. The research
investigates the one-to-many attack surface where a single device
simultaneously triggers multiple actions or devices. Additionally, the study
demonstrates the reversibility or demodulation of the inaudible signal,
suggesting potential alerting methods and the possibility of embedding secret
messages like audio steganography.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: S$^3$Track: Self-supervised Tracking with Soft Assignment Flow. (arXiv:2305.09981v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09981">http://arxiv.org/abs/2305.09981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09981] S$^3$Track: Self-supervised Tracking with Soft Assignment Flow](http://arxiv.org/abs/2305.09981) #robust</code></li>
<li>Summary: <p>In this work, we study self-supervised multiple object tracking without using
any video-level association labels. We propose to cast the problem of multiple
object tracking as learning the frame-wise associations between detections in
consecutive frames. To this end, we propose differentiable soft object
assignment for object association, making it possible to learn features
tailored to object association with differentiable end-to-end training. With
this training approach in hand, we develop an appearance-based model for
learning instance-aware object features used to construct a cost matrix based
on the pairwise distances between the object features. We train our model using
temporal and multi-view data, where we obtain association pseudo-labels using
optical flow and disparity information. Unlike most self-supervised tracking
methods that rely on pretext tasks for learning the feature correspondences,
our method is directly optimized for cross-object association in complex
scenarios. As such, the proposed method offers a reidentification-based MOT
approach that is robust to training hyperparameters and does not suffer from
local minima, which are a challenge in self-supervised methods. We evaluate our
proposed model on the KITTI, Waymo, nuScenes, and Argoverse datasets,
consistently improving over other unsupervised methods ($7.8\%$ improvement in
association accuracy on nuScenes).
</p></li>
</ul>

<h3>Title: TextSLAM: Visual SLAM with Semantic Planar Text Features. (arXiv:2305.10029v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10029">http://arxiv.org/abs/2305.10029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10029] TextSLAM: Visual SLAM with Semantic Planar Text Features](http://arxiv.org/abs/2305.10029) #robust</code></li>
<li>Summary: <p>We propose a novel visual SLAM method that integrates text objects tightly by
treating them as semantic features via fully exploring their geometric and
semantic prior. The text object is modeled as a texture-rich planar patch whose
semantic meaning is extracted and updated on the fly for better data
association. With the full exploration of locally planar characteristics and
semantic meaning of text objects, the SLAM system becomes more accurate and
robust even under challenging conditions such as image blurring, large
viewpoint changes, and significant illumination variations (day and night). We
tested our method in various scenes with the ground truth data. The results
show that integrating texture features leads to a more superior SLAM system
that can match images across day and night. The reconstructed semantic 3D text
map could be useful for navigation and scene understanding in robotic and mixed
reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .
</p></li>
</ul>

<h3>Title: CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture. (arXiv:2305.10084v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10084">http://arxiv.org/abs/2305.10084</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10084] CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture](http://arxiv.org/abs/2305.10084) #robust</code></li>
<li>Summary: <p>The growing demand for precision agriculture necessitates efficient and
accurate crop-weed recognition and classification systems. Current datasets
often lack the sample size, diversity, and hierarchical structure needed to
develop robust deep learning models for discriminating crops and weeds in
agricultural fields. Moreover, the similar external structure and phenomics of
crops and weeds complicate recognition tasks. To address these issues, we
present the CWD30 dataset, a large-scale, diverse, holistic, and hierarchical
dataset tailored for crop-weed recognition tasks in precision agriculture.
CWD30 comprises over 219,770 high-resolution images of 20 weed species and 10
crop species, encompassing various growth stages, multiple viewing angles, and
environmental conditions. The images were collected from diverse agricultural
fields across different geographic locations and seasons, ensuring a
representative dataset. The dataset's hierarchical taxonomy enables
fine-grained classification and facilitates the development of more accurate,
robust, and generalizable deep learning models. We conduct extensive baseline
experiments to validate the efficacy of the CWD30 dataset. Our experiments
reveal that the dataset poses significant challenges due to intra-class
variations, inter-class similarities, and data imbalance. Additionally, we
demonstrate that minor training modifications like using CWD30 pretrained
backbones can significantly enhance model performance and reduce convergence
time, saving training resources on several downstream tasks. These challenges
provide valuable insights and opportunities for future research in crop-weed
recognition. We believe that the CWD30 dataset will serve as a benchmark for
evaluating crop-weed recognition algorithms, promoting advancements in
precision agriculture, and fostering collaboration among researchers in the
field.
</p></li>
</ul>

<h3>Title: Automatic 3D Registration of Dental CBCT and Face Scan Data using 2D Projection images. (arXiv:2305.10132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10132">http://arxiv.org/abs/2305.10132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10132] Automatic 3D Registration of Dental CBCT and Face Scan Data using 2D Projection images](http://arxiv.org/abs/2305.10132) #robust</code></li>
<li>Summary: <p>This paper presents a fully automatic registration method of dental cone-beam
computed tomography (CBCT) and face scan data. It can be used for a digital
platform of 3D jaw-teeth-face models in a variety of applications, including 3D
digital treatment planning and orthognathic surgery. Difficulties in accurately
merging facial scans and CBCT images are due to the different image acquisition
methods and limited area of correspondence between the two facial surfaces. In
addition, it is difficult to use machine learning techniques because they use
face-related 3D medical data with radiation exposure, which are difficult to
obtain for training. The proposed method addresses these problems by reusing an
existing machine-learning-based 2D landmark detection algorithm in an
open-source library and developing a novel mathematical algorithm that
identifies paired 3D landmarks from knowledge of the corresponding 2D
landmarks. A main contribution of this study is that the proposed method does
not require annotated training data of facial landmarks because it uses a
pre-trained facial landmark detection algorithm that is known to be robust and
generalized to various 2D face image models. Note that this reduces a 3D
landmark detection problem to a 2D problem of identifying the corresponding
landmarks on two 2D projection images generated from two different projection
angles. Here, the 3D landmarks for registration were selected from the
sub-surfaces with the least geometric change under the CBCT and face scan
environments. For the final fine-tuning of the registration, the Iterative
Closest Point method was applied, which utilizes geometrical information around
the 3D landmarks. The experimental results show that the proposed method
achieved an averaged surface distance error of 0.74 mm for three pairs of CBCT
and face scan datasets.
</p></li>
</ul>

<h3>Title: Sharpness &amp; Shift-Aware Self-Supervised Learning. (arXiv:2305.10252v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10252">http://arxiv.org/abs/2305.10252</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10252] Sharpness &amp; Shift-Aware Self-Supervised Learning](http://arxiv.org/abs/2305.10252) #robust</code></li>
<li>Summary: <p>Self-supervised learning aims to extract meaningful features from unlabeled
data for further downstream tasks. In this paper, we consider classification as
a downstream task in phase 2 and develop rigorous theories to realize the
factors that implicitly influence the general loss of this classification task.
Our theories signify that sharpness-aware feature extractors benefit the
classification task in phase 2 and the existing data shift between the ideal
(i.e., the ideal one used in theory development) and practical (i.e., the
practical one used in implementation) distributions to generate positive pairs
also remarkably affects this classification task. Further harvesting these
theoretical findings, we propose to minimize the sharpness of the feature
extractor and a new Fourier-based data augmentation technique to relieve the
data shift in the distributions generating positive pairs, reaching Sharpness &amp;
Shift-Aware Contrastive Learning (SSA-CLR). We conduct extensive experiments to
verify our theoretical findings and demonstrate that sharpness &amp; shift-aware
contrastive learning can remarkably boost the performance as well as obtaining
more robust extracted features compared with the baselines.
</p></li>
</ul>

<h3>Title: Raising the Bar for Certified Adversarial Robustness with Diffusion Models. (arXiv:2305.10388v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10388">http://arxiv.org/abs/2305.10388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10388] Raising the Bar for Certified Adversarial Robustness with Diffusion Models](http://arxiv.org/abs/2305.10388) #robust</code></li>
<li>Summary: <p>Certified defenses against adversarial attacks offer formal guarantees on the
robustness of a model, making them more reliable than empirical methods such as
adversarial training, whose effectiveness is often later reduced by unseen
attacks. Still, the limited certified robustness that is currently achievable
has been a bottleneck for their practical adoption. Gowal et al. and Wang et
al. have shown that generating additional training data using state-of-the-art
diffusion models can considerably improve the robustness of adversarial
training. In this work, we demonstrate that a similar approach can
substantially improve deterministic certified defenses. In addition, we provide
a list of recommendations to scale the robustness of certified training
approaches. One of our main insights is that the generalization gap, i.e., the
difference between the training and test accuracy of the original model, is a
good predictor of the magnitude of the robustness improvement when using
additional generated data. Our approach achieves state-of-the-art deterministic
robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and
$\ell_\infty$ ($\epsilon = 8/255$) threat models, outperforming the previous
best results by $+3.95\%$ and $+1.39\%$, respectively. Furthermore, we report
similar improvements for CIFAR-100.
</p></li>
</ul>

<h3>Title: Variational Classification. (arXiv:2305.10406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10406">http://arxiv.org/abs/2305.10406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10406] Variational Classification](http://arxiv.org/abs/2305.10406) #robust</code></li>
<li>Summary: <p>We present a novel extension of the traditional neural network approach to
classification tasks, referred to as variational classification (VC). By
incorporating latent variable modeling, akin to the relationship between
variational autoencoders and traditional autoencoders, we derive a training
objective based on the evidence lower bound (ELBO), optimized using an
adversarial approach. Our VC model allows for more flexibility in design
choices, in particular class-conditional latent priors, in place of the
implicit assumptions made in off-the-shelf softmax classifiers. Empirical
evaluation on image and text classification datasets demonstrates the
effectiveness of our approach in terms of maintaining prediction accuracy while
improving other desirable properties such as calibration and adversarial
robustness, even when applied to out-of-domain data.
</p></li>
</ul>

<h3>Title: Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10284">http://arxiv.org/abs/2305.10284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10284] Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks](http://arxiv.org/abs/2305.10284) #robust</code></li>
<li>Summary: <p>The evaluation of natural language processing (NLP) systems is crucial for
advancing the field, but current benchmarking approaches often assume that all
systems have scores available for all tasks, which is not always practical. In
reality, several factors such as the cost of running baseline, private systems,
computational limitations, or incomplete data may prevent some systems from
being evaluated on entire tasks. This paper formalize an existing problem in
NLP research: benchmarking when some systems scores are missing on the task,
and proposes a novel approach to address it. Our method utilizes a compatible
partial ranking approach to impute missing data, which is then aggregated using
the Borda count method. It includes two refinements designed specifically for
scenarios where either task-level or instance-level scores are available. We
also introduce an extended benchmark, which contains over 131 million scores,
an order of magnitude larger than existing benchmarks. We validate our methods
and demonstrate their effectiveness in addressing the challenge of missing
system evaluation on an entire task. This work highlights the need for more
comprehensive benchmarking approaches that can handle real-world scenarios
where not all systems are evaluated on the entire task.
</p></li>
</ul>

<h3>Title: Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties. (arXiv:2305.10384v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10384">http://arxiv.org/abs/2305.10384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10384] Logit-Based Ensemble Distribution Distillation for Robust Autoregressive Sequence Uncertainties](http://arxiv.org/abs/2305.10384) #robust</code></li>
<li>Summary: <p>Efficiently and reliably estimating uncertainty is an important objective in
deep learning. It is especially pertinent to autoregressive sequence tasks,
where training and inference costs are typically very high. However, existing
research has predominantly focused on tasks with static data such as image
classification. In this work, we investigate Ensemble Distribution Distillation
(EDD) applied to large-scale natural language sequence-to-sequence data. EDD
aims to compress the superior uncertainty performance of an expensive (teacher)
ensemble into a cheaper (student) single model. Importantly, the ability to
separate knowledge (epistemic) and data (aleatoric) uncertainty is retained.
Existing probability-space approaches to EDD, however, are difficult to scale
to large vocabularies. We show, for modern transformer architectures on
large-scale translation tasks, that modelling the ensemble logits, instead of
softmax probabilities, leads to significantly better students. Moreover, the
students surprisingly even outperform Deep Ensembles by up to ~10% AUROC on
out-of-distribution detection, whilst matching them at in-distribution
translation.
</p></li>
</ul>

<h3>Title: Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10387">http://arxiv.org/abs/2305.10387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10387] Elaborative Simplification as Implicit Questions Under Discussion](http://arxiv.org/abs/2305.10387) #robust</code></li>
<li>Summary: <p>Automated text simplification, a technique useful for making text more
accessible to people such as children and emergent bilinguals, is often thought
of as a monolingual translation task from complex sentences to simplified
sentences using encoder-decoder models. This view fails to account for
elaborative simplification, where new information is added into the simplified
text. This paper proposes to view elaborative simplification through the lens
of the Question Under Discussion (QUD) framework, providing a robust way to
investigate what writers elaborate upon, how they elaborate, and how
elaborations fit into the discourse context by viewing elaborations as explicit
answers to implicit questions. We introduce ElabQUD, consisting of 1.3K
elaborations accompanied with implicit QUDs, to study these phenomena. We show
that explicitly modeling QUD (via question generation) not only provides
essential understanding of elaborative simplification and how the elaborations
connect with the rest of the discourse, but also substantially improves the
quality of elaboration generation.
</p></li>
</ul>

<h3>Title: PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10403">http://arxiv.org/abs/2305.10403</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10403] PaLM 2 Technical Report](http://arxiv.org/abs/2305.10403) #robust</code></li>
<li>Summary: <p>We introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
</p></li>
</ul>

<p>When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report.
</p>

<h3>Title: DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10429">http://arxiv.org/abs/2305.10429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10429] DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining](http://arxiv.org/abs/2305.10429) #robust</code></li>
<li>Summary: <p>The mixture proportions of pretraining data domains (e.g., Wikipedia, books,
web text) greatly affect language model (LM) performance. In this paper, we
propose Domain Reweighting with Minimax Optimization (DoReMi), which first
trains a small proxy model using group distributionally robust optimization
(Group DRO) over domains to produce domain weights (mixture proportions)
without knowledge of downstream tasks. We then resample a dataset with these
domain weights and train a larger, full-sized model. In our experiments, we use
DoReMi on a 280M-parameter proxy model to find domain weights for training an
8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves
perplexity across all domains, even when it downweights a domain. DoReMi
improves average few-shot downstream accuracy by 6.5% over a baseline model
trained using The Pile's default domain weights and reaches the baseline
accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has
no knowledge of downstream tasks, even matches the performance of using domain
weights tuned on downstream tasks.
</p></li>
</ul>

<h3>Title: Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation. (arXiv:2305.09887v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09887">http://arxiv.org/abs/2305.09887</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09887] Simplifying Distributed Neural Network Training on Massive Graphs: Randomized Partitions Improve Model Aggregation](http://arxiv.org/abs/2305.09887) #robust</code></li>
<li>Summary: <p>Distributed training of GNNs enables learning on massive graphs (e.g., social
and e-commerce networks) that exceed the storage and computational capacity of
a single machine. To reach performance comparable to centralized training,
distributed frameworks focus on maximally recovering cross-instance node
dependencies with either communication across instances or periodic fallback to
centralized training, which create overhead and limit the framework
scalability. In this work, we present a simplified framework for distributed
GNN training that does not rely on the aforementioned costly operations, and
has improved scalability, convergence speed and performance over the
state-of-the-art approaches. Specifically, our framework (1) assembles
independent trainers, each of which asynchronously learns a local model on
locally-available parts of the training graph, and (2) only conducts periodic
(time-based) model aggregation to synchronize the local models. Backed by our
theoretical analysis, instead of maximizing the recovery of cross-instance node
dependencies -- which has been considered the key behind closing the
performance gap between model aggregation and centralized training -- , our
framework leverages randomized assignment of nodes or super-nodes (i.e.,
collections of original nodes) to partition the training graph such that it
improves data uniformity and minimizes the discrepancy of gradient and loss
function across instances. In our experiments on social and e-commerce networks
with up to 1.3 billion edges, our proposed RandomTMA and SuperTMA approaches --
despite using less training data -- achieve state-of-the-art performance and
2.31x speedup compared to the fastest baseline, and show better robustness to
trainer failures.
</p></li>
</ul>

<h3>Title: On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations. (arXiv:2305.09904v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09904">http://arxiv.org/abs/2305.09904</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09904] On the ISS Property of the Gradient Flow for Single Hidden-Layer Neural Networks with Linear Activations](http://arxiv.org/abs/2305.09904) #robust</code></li>
<li>Summary: <p>Recent research in neural networks and machine learning suggests that using
many more parameters than strictly required by the initial complexity of a
regression problem can result in more accurate or faster-converging models --
contrary to classical statistical belief. This phenomenon, sometimes known as
``benign overfitting'', raises questions regarding in what other ways might
overparameterization affect the properties of a learning problem. In this work,
we investigate the effects of overfitting on the robustness of gradient-descent
training when subject to uncertainty on the gradient estimation. This
uncertainty arises naturally if the gradient is estimated from noisy data or
directly measured. Our object of study is a linear neural network with a
single, arbitrarily wide, hidden layer and an arbitrary number of inputs and
outputs. In this paper we solve the problem for the case where the input and
output of our neural-network are one-dimensional, deriving sufficient
conditions for robustness of our system based on necessary and sufficient
conditions for convergence in the undisturbed case. We then show that the
general overparametrized formulation introduces a set of spurious equilibria
which lay outside the set where the loss function is minimized, and discuss
directions of future work that might extend our current results for more
general formulations.
</p></li>
</ul>

<h3>Title: Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09913">http://arxiv.org/abs/2305.09913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09913] Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions](http://arxiv.org/abs/2305.09913) #robust</code></li>
<li>Summary: <p>Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized
health interventions developed within the behavioral science community. JITAIs
aim to provide the right type and amount of support by iteratively selecting a
sequence of intervention options from a pre-defined set of components in
response to each individual's time varying state. In this work, we explore the
application of reinforcement learning methods to the problem of learning
intervention option selection policies. We study the effect of context
inference error and partial observability on the ability to learn effective
policies. Our results show that the propagation of uncertainty from context
inferences is critical to improving intervention efficacy as context
uncertainty increases, while policy gradient algorithms can provide remarkable
robustness to partially observed behavioral state information.
</p></li>
</ul>

<h3>Title: The Adversarial Consistency of Surrogate Risks for Binary Classification. (arXiv:2305.09956v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09956">http://arxiv.org/abs/2305.09956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09956] The Adversarial Consistency of Surrogate Risks for Binary Classification](http://arxiv.org/abs/2305.09956) #robust</code></li>
<li>Summary: <p>We study the consistency of surrogate risks for robust binary classification.
It is common to learn robust classifiers by adversarial training, which seeks
to minimize the expected $0$-$1$ loss when each example can be maliciously
corrupted within a small ball. We give a simple and complete characterization
of the set of surrogate loss functions that are \emph{consistent}, i.e., that
can replace the $0$-$1$ loss without affecting the minimizing sequences of the
original adversarial risk, for any data distribution. We also prove a
quantitative version of adversarial consistency for the $\rho$-margin loss. Our
results reveal that the class of adversarially consistent surrogates is
substantially smaller than in the standard setting, where many common
surrogates are known to be consistent.
</p></li>
</ul>

<h3>Title: Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model. (arXiv:2305.10133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10133">http://arxiv.org/abs/2305.10133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10133] Lingo3DMol: Generation of a Pocket-based 3D Molecule using a Language Model](http://arxiv.org/abs/2305.10133) #robust</code></li>
<li>Summary: <p>Structure-based drug design powered by deep generative models have attracted
increasing research interest in recent years. Language models have demonstrated
a robust capacity for generating valid molecules in 2D structures, while
methods based on geometric deep learning can directly produce molecules with
accurate 3D coordinates. Inspired by both methods, this article proposes a
pocket-based 3D molecule generation method that leverages the language model
with the ability to generate 3D coordinates. High quality protein-ligand
complex data are insufficient; hence, a perturbation and restoration
pre-training task is designed that can utilize vast amounts of small-molecule
data. A new molecular representation, a fragment-based SMILES with local and
global coordinates, is also presented, enabling the language model to learn
molecular topological structures and spatial position information effectively.
Ultimately, CrossDocked and DUD-E dataset is employed for evaluation and
additional metrics are introduced. This method achieves state-of-the-art
performance in nearly all metrics, notably in terms of binding patterns,
drug-like properties, rational conformations, and inference speed. Our model is
available as an online service to academic users via sw3dmg.stonewise.cn
</p></li>
</ul>

<h3>Title: Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10235">http://arxiv.org/abs/2305.10235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10235] Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility](http://arxiv.org/abs/2305.10235) #robust</code></li>
<li>Summary: <p>The recent popularity of large language models (LLMs) has brought a
significant impact to boundless fields, particularly through their open-ended
ecosystem such as the APIs, open-sourced models, and plugins. However, with
their widespread deployment, there is a general lack of research that
thoroughly discusses and analyzes the potential risks concealed. In that case,
we intend to conduct a preliminary but pioneering study covering the
robustness, consistency, and credibility of LLMs systems. With most of the
related literature in the era of LLM uncharted, we propose an automated
workflow that copes with an upscaled number of queries/responses. Overall, we
conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,
and OPT. Core to our workflow consists of a data primitive, followed by an
automated interpreter that evaluates these LLMs under different adversarial
metrical systems. As a result, we draw several, and perhaps unfortunate,
conclusions that are quite uncommon from this trendy community. Briefly, they
are: (i)-the minor but inevitable error occurrence in the user-generated query
input may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess
poor consistency when processing semantically similar query input. In addition,
as a side finding, we find that ChatGPT is still capable to yield the correct
answer even when the input is polluted at an extreme level. While this
phenomenon demonstrates the powerful memorization of the LLMs, it raises
serious concerns about using such data for LLM-involved evaluation in academic
development. To deal with it, we propose a novel index associated with a
dataset that roughly decides the feasibility of using such data for
LLM-involved evaluation. Extensive empirical studies are tagged to support the
aforementioned claims.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: From Region to Patch: Attribute-Aware Foreground-Background Contrastive Learning for Fine-Grained Fashion Retrieval. (arXiv:2305.10260v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10260">http://arxiv.org/abs/2305.10260</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10260] From Region to Patch: Attribute-Aware Foreground-Background Contrastive Learning for Fine-Grained Fashion Retrieval](http://arxiv.org/abs/2305.10260) #extraction</code></li>
<li>Summary: <p>Attribute-specific fashion retrieval (ASFR) is a challenging information
retrieval task, which has attracted increasing attention in recent years.
Different from traditional fashion retrieval which mainly focuses on optimizing
holistic similarity, the ASFR task concentrates on attribute-specific
similarity, resulting in more fine-grained and interpretable retrieval results.
As the attribute-specific similarity typically corresponds to the specific
subtle regions of images, we propose a Region-to-Patch Framework (RPF) that
consists of a region-aware branch and a patch-aware branch to extract
fine-grained attribute-related visual features for precise retrieval in a
coarse-to-fine manner. In particular, the region-aware branch is first to be
utilized to locate the potential regions related to the semantic of the given
attribute. Then, considering that the located region is coarse and still
contains the background visual contents, the patch-aware branch is proposed to
capture patch-wise attribute-related details from the previous amplified
region. Such a hybrid architecture strikes a proper balance between region
localization and feature extraction. Besides, different from previous works
that solely focus on discriminating the attribute-relevant foreground visual
features, we argue that the attribute-irrelevant background features are also
crucial for distinguishing the detailed visual contexts in a contrastive
manner. Therefore, a novel E-InfoNCE loss based on the foreground and
background representations is further proposed to improve the discrimination of
attribute-specific representation. Extensive experiments on three datasets
demonstrate the effectiveness of our proposed framework, and also show a decent
generalization of our RPF on out-of-domain fashion images. Our source code is
available at https://github.com/HuiGuanLab/RPF.
</p></li>
</ul>

<h3>Title: Additive manifesto decomposition: A policy domain aware method for understanding party positioning. (arXiv:2305.10136v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10136">http://arxiv.org/abs/2305.10136</a></li>
<li>Code URL: <a href="https://github.com/tceron/additive_manifesto_decomposition">https://github.com/tceron/additive_manifesto_decomposition</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10136] Additive manifesto decomposition: A policy domain aware method for understanding party positioning](http://arxiv.org/abs/2305.10136) #extraction</code></li>
<li>Summary: <p>Automatic extraction of party (dis)similarities from texts such as party
election manifestos or parliamentary speeches plays an increasing role in
computational political science. However, existing approaches are fundamentally
limited to targeting only global party (dis)-similarity: they condense the
relationship between a pair of parties into a single figure, their similarity.
In aggregating over all policy domains (e.g., health or foreign policy), they
do not provide any qualitative insights into which domains parties agree or
disagree on. This paper proposes a workflow for estimating policy domain aware
party similarity that overcomes this limitation. The workflow covers (a)
definition of suitable policy domains; (b) automatic labeling of domains, if no
manual labels are available; (c) computation of domain-level similarities and
aggregation at a global level; (d) extraction of interpretable party positions
on major policy axes via multidimensional scaling. We evaluate our workflow on
manifestos from the German federal elections. We find that our method (a)
yields high correlation when predicting party similarity at a global level and
(b) provides accurate party-specific positions, even with automatically
labelled policy domains.
</p></li>
</ul>

<h3>Title: UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10306">http://arxiv.org/abs/2305.10306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10306] UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective](http://arxiv.org/abs/2305.10306) #extraction</code></li>
<li>Summary: <p>We propose a new paradigm for universal information extraction (IE) that is
compatible with any schema format and applicable to a list of IE tasks, such as
named entity recognition, relation extraction, event extraction and sentiment
analysis. Our approach converts the text-based IE tasks as the token-pair
problem, which uniformly disassembles all extraction targets into joint span
detection, classification and association problems with a unified extractive
framework, namely UniEX. UniEX can synchronously encode schema-based prompt and
textual information, and collaboratively learn the generalized knowledge from
pre-defined information using the auto-encoder language models. We develop a
traffine attention mechanism to integrate heterogeneous factors including
tasks, labels and inside tokens, and obtain the extraction target via a scoring
matrix. Experiment results show that UniEX can outperform generative universal
IE models in terms of performance and inference-speed on $14$ benchmarks IE
datasets with the supervised setting. The state-of-the-art performance in
low-resource scenarios also verifies the transferability and effectiveness of
UniEX.
</p></li>
</ul>

<h3>Title: LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10314">http://arxiv.org/abs/2305.10314</a></li>
<li>Code URL: <a href="https://github.com/xingyaoww/leti">https://github.com/xingyaoww/leti</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10314] LeTI: Learning to Generate from Textual Interactions](http://arxiv.org/abs/2305.10314) #extraction</code></li>
<li>Summary: <p>Finetuning pre-trained language models (LMs) enhances the models'
capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs
(e.g., instruction fine-tuning), or with numerical rewards that gauge the
quality of its outputs (e.g., reinforcement learning from human feedback). We
explore LMs' potential to learn from textual interactions (LeTI) that not only
check their correctness with binary labels, but also pinpoint and explain
errors in their outputs through textual feedback. Our investigation focuses on
the code generation task, where the model produces code pieces in response to
natural language instructions. This setting invites a natural and scalable way
to acquire the textual feedback: the error messages and stack traces from code
execution using a Python interpreter. LeTI iteratively fine-tunes the model,
using the LM objective, on a concatenation of natural language instructions,
LM-generated programs, and textual feedback, which is only provided when the
generated program fails to solve the task. Prepended to this fine-tuning text,
a binary reward token is used to differentiate correct and buggy solutions. On
MBPP, a code generation dataset, LeTI substantially improves the performance of
two base LMs of different scales. LeTI requires no ground-truth outputs for
training and even outperforms a fine-tuned baseline that does. LeTI's strong
performance generalizes to other datasets. Trained on MBPP, it achieves
comparable or better performance than the base LMs on unseen problems in
HumanEval. Furthermore, compared to binary feedback, we observe that textual
feedback leads to improved generation quality and sample efficiency, achieving
the same performance with fewer than half of the gradient steps. LeTI is
equally applicable in natural language tasks when they can be formulated as
code generation, which we empirically verified on event argument extraction.
</p></li>
</ul>

<h3>Title: A Survey on Multi-Objective based Parameter Optimization for Deep Learning. (arXiv:2305.10014v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10014">http://arxiv.org/abs/2305.10014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10014] A Survey on Multi-Objective based Parameter Optimization for Deep Learning](http://arxiv.org/abs/2305.10014) #extraction</code></li>
<li>Summary: <p>Deep learning models form one of the most powerful machine learning models
for the extraction of important features. Most of the designs of deep neural
models, i.e., the initialization of parameters, are still manually tuned.
Hence, obtaining a model with high performance is exceedingly time-consuming
and occasionally impossible. Optimizing the parameters of the deep networks,
therefore, requires improved optimization algorithms with high convergence
rates. The single objective-based optimization methods generally used are
mostly time-consuming and do not guarantee optimum performance in all cases.
Mathematical optimization problems containing multiple objective functions that
must be optimized simultaneously fall under the category of multi-objective
optimization sometimes referred to as Pareto optimization. Multi-objective
optimization problems form one of the alternatives yet useful options for
parameter optimization. However, this domain is a bit less explored. In this
survey, we focus on exploring the effectiveness of multi-objective optimization
strategies for parameter optimization in conjunction with deep neural networks.
The case studies used in this study focus on how the two methods are combined
to provide valuable insights into the generation of predictions and analysis in
multiple applications.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks. (arXiv:2305.09729v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09729">http://arxiv.org/abs/2305.09729</a></li>
<li>Code URL: <a href="https://github.com/cynricfu/fedhgn">https://github.com/cynricfu/fedhgn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09729] FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks](http://arxiv.org/abs/2305.09729) #federate</code></li>
<li>Summary: <p>Heterogeneous graph neural networks (HGNNs) can learn from typed and
relational graph data more effectively than conventional GNNs. With larger
parameter spaces, HGNNs may require more training data, which is often scarce
in real-world applications due to privacy regulations (e.g., GDPR). Federated
graph learning (FGL) enables multiple clients to train a GNN collaboratively
without sharing their local data. However, existing FGL methods mainly focus on
homogeneous GNNs or knowledge graph embeddings; few have considered
heterogeneous graphs and HGNNs. In federated heterogeneous graph learning,
clients may have private graph schemas. Conventional FL/FGL methods attempting
to define a global HGNN model would violate schema privacy. To address these
challenges, we propose FedHGN, a novel and general FGL framework for HGNNs.
FedHGN adopts schema-weight decoupling to enable schema-agnostic knowledge
sharing and employs coefficients alignment to stabilize the training process
and improve HGNN performance. With better privacy preservation, FedHGN
consistently outperforms local training and conventional FL methods on three
widely adopted heterogeneous graph datasets with varying client numbers. The
code is available at https://github.com/cynricfu/FedHGN .
</p></li>
</ul>

<h3>Title: Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients. (arXiv:2305.09856v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09856">http://arxiv.org/abs/2305.09856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09856] Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients](http://arxiv.org/abs/2305.09856) #federate</code></li>
<li>Summary: <p>Federated learning (FL), as an emerging artificial intelligence (AI)
approach, enables decentralized model training across multiple devices without
exposing their local training data. FL has been increasingly gaining popularity
in both academia and industry. While research works have been proposed to
improve the fault tolerance of FL, the real impact of unreliable devices (e.g.,
dropping out, misconfiguration, poor data quality) in real-world applications
is not fully investigated. We carefully chose two representative, real-world
classification problems with a limited numbers of clients to better analyze FL
fault tolerance. Contrary to the intuition, simple FL algorithms can perform
surprisingly well in the presence of unreliable clients.
</p></li>
</ul>

<h3>Title: Mitigating Group Bias in Federated Learning: Beyond Local Fairness. (arXiv:2305.09931v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09931">http://arxiv.org/abs/2305.09931</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09931] Mitigating Group Bias in Federated Learning: Beyond Local Fairness](http://arxiv.org/abs/2305.09931) #federate</code></li>
<li>Summary: <p>The issue of group fairness in machine learning models, where certain
sub-populations or groups are favored over others, has been recognized for some
time. While many mitigation strategies have been proposed in centralized
learning, many of these methods are not directly applicable in federated
learning, where data is privately stored on multiple clients. To address this,
many proposals try to mitigate bias at the level of clients before aggregation,
which we call locally fair training. However, the effectiveness of these
approaches is not well understood. In this work, we investigate the theoretical
foundation of locally fair training by studying the relationship between global
model fairness and local model fairness. Additionally, we prove that for a
broad class of fairness metrics, the global model's fairness can be obtained
using only summary statistics from local clients. Based on that, we propose a
globally fair training algorithm that directly minimizes the penalized
empirical loss. Real-data experiments demonstrate the promising performance of
our proposed approach for enhancing fairness while retaining high accuracy
compared to locally fair training methods.
</p></li>
</ul>

<h3>Title: DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10294">http://arxiv.org/abs/2305.10294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10294] DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime](http://arxiv.org/abs/2305.10294) #federate</code></li>
<li>Summary: <p>We propose a novel training algorithm called DualFL (Dualized Federated
Learning), for solving a distributed optimization problem in federated
learning. Our approach is based on a specific dual formulation of the federated
learning problem. DualFL achieves communication acceleration under various
settings on smoothness and strong convexity of the problem. Moreover, it
theoretically guarantees the use of inexact local solvers, preserving its
optimal communication complexity even with inexact local solutions. DualFL is
the first federated learning algorithm that achieves communication
acceleration, even when the cost function is either nonsmooth or non-strongly
convex. Numerical results demonstrate that the practical performance of DualFL
is comparable to those of state-of-the-art federated learning algorithms, and
it is robust with respect to hyperparameter tuning.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09900">http://arxiv.org/abs/2305.09900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09900] Equivariant Few-Shot Learning from Pretrained Models](http://arxiv.org/abs/2305.09900) #fair</code></li>
<li>Summary: <p>Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of
\cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging
(\textit{equitune}) and optimization-based methods, respectively, over features
from group-transformed inputs to obtain equivariant outputs from
non-equivariant neural networks. While \cite{kaba2022equivariance} are only
concerned with training from scratch, we find that equitune performs poorly on
equivariant zero-shot tasks despite good finetuning results. We hypothesize
that this is because pretrained models provide better quality features for
certain transformations than others and simply averaging them is deleterious.
Hence, we propose $\lambda$-\textit{equitune} that averages the features using
\textit{importance weights}, $\lambda$s. These weights are learned directly
from the data using a small neural network, leading to excellent zero-shot and
finetuned results that outperform equitune. Further, we prove that
$\lambda$-equitune is equivariant and a universal approximator of equivariant
functions. Additionally, we show that the method of \cite{kaba2022equivariance}
used with appropriate loss functions, which we call \textit{equizero}, also
gives excellent zero-shot and finetuned performance. Both equitune and equizero
are special cases of $\lambda$-equitune. To show the simplicity and generality
of our method, we validate on a wide range of diverse applications and models
such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.
</p></li>
</ul>

<h3>Title: Epsilon Sampling Rocks: Investigating Sampling Strategies for \Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09860">http://arxiv.org/abs/2305.09860</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09860] Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation](http://arxiv.org/abs/2305.09860) #fair</code></li>
<li>Summary: <p>Recent advances in machine translation (MT) have shown that Minimum Bayes
Risk (MBR) decoding can be a powerful alternative to beam search decoding,
especially when combined with neural-based utility functions. However, the
performance of MBR decoding depends heavily on how and how many candidates are
sampled from the model. In this paper, we explore how different sampling
approaches for generating candidate lists for MBR decoding affect performance.
We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k
sampling. Based on our insights into their limitations, we experiment with the
recently proposed epsilon-sampling approach, which prunes away all tokens with
a probability smaller than epsilon, ensuring that each token in a sample
receives a fair probability mass. Through extensive human evaluations, we
demonstrate that MBR decoding based on epsilon-sampling significantly
outperforms not only beam search decoding, but also MBR decoding with all other
tested sampling methods across four language pairs.
</p></li>
</ul>

<h3>Title: "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09941">http://arxiv.org/abs/2305.09941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09941] "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation](http://arxiv.org/abs/2305.09941) #fair</code></li>
<li>Summary: <p>Transgender and non-binary (TGNB) individuals disproportionately experience
discrimination and exclusion from daily life. Given the recent popularity and
adoption of language generation technologies, the potential to further
marginalize this population only grows. Although a multitude of NLP fairness
literature focuses on illuminating and addressing gender biases, assessing
gender harms for TGNB identities requires understanding how such identities
uniquely interact with societal gender norms and how they differ from gender
binary-centric perspectives. Such measurement frameworks inherently require
centering TGNB voices to help guide the alignment between gender-inclusive NLP
and whom they are intended to serve. Towards this goal, we ground our work in
the TGNB community and existing interdisciplinary literature to assess how the
social reality surrounding experienced marginalization by TGNB persons
contributes to and persists within Open Language Generation (OLG). By first
understanding their marginalization stressors, we evaluate (1) misgendering and
(2) harmful responses to gender disclosure. To do this, we introduce the TANGO
dataset, comprising of template-based text curated from real-world text within
a TGNB-oriented community. We discover a dominance of binary gender norms
within the models; LLMs least misgendered subjects in generated text when
triggered by prompts whose subjects used binary pronouns. Meanwhile,
misgendering was most prevalent when triggering generation with singular they
and neopronouns. When prompted with gender disclosures, LLM text contained
stigmatizing language and scored most toxic when triggered by TGNB gender
disclosure. Our findings warrant further research on how TGNB harms manifest in
LLMs and serve as a broader case study toward concretely grounding the design
of gender-inclusive AI in community voices and interdisciplinary literature.
</p></li>
</ul>

<h3>Title: Data Bias Management. (arXiv:2305.09686v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09686">http://arxiv.org/abs/2305.09686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09686] Data Bias Management](http://arxiv.org/abs/2305.09686) #fair</code></li>
<li>Summary: <p>Due to the widespread use of data-powered systems in our everyday lives,
concepts like bias and fairness gained significant attention among researchers
and practitioners, in both industry and academia. Such issues typically emerge
from the data, which comes with varying levels of quality, used to train
supervised machine learning systems. With the commercialization and deployment
of such systems that are sometimes delegated to make life-changing decisions,
significant efforts are being made towards the identification and removal of
possible sources of data bias that may resurface to the final end user or in
the decisions being made. In this paper, we present research results that show
how bias in data affects end users, where bias is originated, and provide a
viewpoint about what we should do about it. We argue that data bias is not
something that should necessarily be removed in all cases, and that research
attention should instead shift from bias removal towards the identification,
measurement, indexing, surfacing, and adapting for bias, which we name bias
management.
</p></li>
</ul>

<h3>Title: Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10391">http://arxiv.org/abs/2305.10391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10391] Optimality of Message-Passing Architectures for Sparse Graphs](http://arxiv.org/abs/2305.10391) #fair</code></li>
<li>Summary: <p>We study the node classification problem on feature-decorated graphs in the
sparse setting, i.e., when the expected degree of a node is $O(1)$ in the
number of nodes. Such graphs are typically known to be locally tree-like. We
introduce a notion of Bayes optimality for node classification tasks, called
asymptotic local Bayes optimality, and compute the optimal classifier according
to this criterion for a fairly general statistical data model with arbitrary
distributions of the node features and edge connectivity. The optimal
classifier is implementable using a message-passing graph neural network
architecture. We then compute the generalization error of this classifier and
compare its performance against existing learning methods theoretically on a
well-studied statistical model with naturally identifiable signal-to-noise
ratios (SNRs) in the data. We find that the optimal message-passing
architecture interpolates between a standard MLP in the regime of low graph
signal and a typical convolution in the regime of high graph signal.
Furthermore, we prove a corresponding non-asymptotic result.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A Range-Null Space Decomposition Approach for Fast and Flexible Spectral Compressive Imaging. (arXiv:2305.09746v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09746">http://arxiv.org/abs/2305.09746</a></li>
<li>Code URL: <a href="https://github.com/hustvl/rnd-sci">https://github.com/hustvl/rnd-sci</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09746] A Range-Null Space Decomposition Approach for Fast and Flexible Spectral Compressive Imaging](http://arxiv.org/abs/2305.09746) #interpretability</code></li>
<li>Summary: <p>We present RND-SCI, a novel framework for compressive hyperspectral image
(HSI) reconstruction. Our framework decomposes the reconstructed object into
range-space and null-space components, where the range-space part ensures the
solution conforms to the compression process, and the null-space term
introduces a deep HSI prior to constraining the output to have satisfactory
properties. RND-SCI is not only simple in design with strong interpretability
but also can be easily adapted to various HSI reconstruction networks,
improving the quality of HSIs with minimal computational overhead. RND-SCI
significantly boosts the performance of HSI reconstruction networks in
retraining, fine-tuning or plugging into a pre-trained off-the-shelf model.
Based on the framework and SAUNet, we design an extremely fast HSI
reconstruction network, RND-SAUNet, which achieves an astounding 91 frames per
second while maintaining superior reconstruction accuracy compared to other
less time-consuming methods. Code and models are available at
https://github.com/hustvl/RND-SCI.
</p></li>
</ul>

<h3>Title: Adaptive aggregation of Monte Carlo augmented decomposed filters for efficient group-equivariant convolutional neural network. (arXiv:2305.10110v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10110">http://arxiv.org/abs/2305.10110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10110] Adaptive aggregation of Monte Carlo augmented decomposed filters for efficient group-equivariant convolutional neural network](http://arxiv.org/abs/2305.10110) #interpretability</code></li>
<li>Summary: <p>Filter-decomposition-based group-equivariant convolutional neural networks
(G-CNN) have been demonstrated to increase CNN's data efficiency and contribute
to better interpretability and controllability of CNN models. However, so far
filter-decomposition-based affine G-CNN methods rely on parameter sharing for
achieving high parameter efficiency and suffer from a heavy computational
burden. They also use a limited number of transformations and in particular
ignore the shear transform in the application. In this paper, we address these
problems by emphasizing the importance of the diversity of transformations. We
propose a flexible and efficient strategy based on weighted filter-wise Monte
Carlo sampling. In addition, we introduce shear equivariant CNN to address the
highly sparse representations of natural images. We demonstrate that the
proposed methods are intrinsically an efficient generalization of traditional
CNNs, and we explain the advantage of bottleneck architectures used in the
existing state-of-the-art CNN models such as ResNet, ResNext, and ConvNeXt from
the group-equivariant perspective. Experiments on image classification and
image denoising tasks show that with a set of suitable filter basis, our
methods achieve superior performance to standard CNN with high data efficiency.
The code will be available at https://github.com/ZhaoWenzhao/MCG_CNN.
</p></li>
</ul>

<h3>Title: Principal Uncertainty Quantification with Spatial Correlation for Image Restoration Problems. (arXiv:2305.10124v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10124">http://arxiv.org/abs/2305.10124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10124] Principal Uncertainty Quantification with Spatial Correlation for Image Restoration Problems](http://arxiv.org/abs/2305.10124) #interpretability</code></li>
<li>Summary: <p>Uncertainty quantification for inverse problems in imaging has drawn much
attention lately. Existing approaches towards this task define uncertainty
regions based on probable values per pixel, while ignoring spatial correlations
within the image, resulting in an exaggerated volume of uncertainty. In this
paper, we propose PUQ (Principal Uncertainty Quantification) -- a novel
definition and corresponding analysis of uncertainty regions that takes into
account spatial relationships within the image, thus providing reduced volume
regions. Using recent advancements in stochastic generative models, we derive
uncertainty intervals around principal components of the empirical posterior
distribution, forming an ambiguity region that guarantees the inclusion of true
unseen values with a user confidence probability. To improve computational
efficiency and interpretability, we also guarantee the recovery of true unseen
values using only a few principal directions, resulting in ultimately more
informative uncertainty regions. Our approach is verified through experiments
on image colorization, super-resolution, and inpainting; its effectiveness is
shown through comparison to baseline methods, demonstrating significantly
tighter uncertainty regions.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: XAI for Self-supervised Clustering of Wireless Spectrum Activity. (arXiv:2305.10060v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10060">http://arxiv.org/abs/2305.10060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10060] XAI for Self-supervised Clustering of Wireless Spectrum Activity](http://arxiv.org/abs/2305.10060) #explainability</code></li>
<li>Summary: <p>The so-called black-box deep learning (DL) models are increasingly used in
classification tasks across many scientific disciplines, including wireless
communications domain. In this trend, supervised DL models appear as most
commonly proposed solutions to domain-related classification problems. Although
they are proven to have unmatched performance, the necessity for large labeled
training data and their intractable reasoning, as two major drawbacks, are
constraining their usage. The self-supervised architectures emerged as a
promising solution that reduces the size of the needed labeled data, but the
explainability problem remains. In this paper, we propose a methodology for
explaining deep clustering, self-supervised learning architectures comprised of
a representation learning part based on a Convolutional Neural Network (CNN)
and a clustering part. For the state of the art representation learning part,
our methodology employs Guided Backpropagation to interpret the regions of
interest of the input data. For the clustering part, the methodology relies on
Shallow Trees to explain the clustering result using optimized depth decision
tree. Finally, a data-specific visualizations part enables connection for each
of the clusters to the input data trough the relevant features. We explain on a
use case of wireless spectrum activity clustering how the CNN-based, deep
clustering architecture reasons.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: Decision-based iterative fragile watermarking for model integrity verification. (arXiv:2305.09684v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09684">http://arxiv.org/abs/2305.09684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09684] Decision-based iterative fragile watermarking for model integrity verification](http://arxiv.org/abs/2305.09684) #watermark</code></li>
<li>Summary: <p>Typically, foundation models are hosted on cloud servers to meet the high
demand for their services. However, this exposes them to security risks, as
attackers can modify them after uploading to the cloud or transferring from a
local system. To address this issue, we propose an iterative decision-based
fragile watermarking algorithm that transforms normal training samples into
fragile samples that are sensitive to model changes. We then compare the output
of sensitive samples from the original model to that of the compromised model
during validation to assess the model's completeness.The proposed fragile
watermarking algorithm is an optimization problem that aims to minimize the
variance of the predicted probability distribution outputed by the target model
when fed with the converted sample.We convert normal samples to fragile samples
through multiple iterations. Our method has some advantages: (1) the iterative
update of samples is done in a decision-based black-box manner, relying solely
on the predicted probability distribution of the target model, which reduces
the risk of exposure to adversarial attacks, (2) the small-amplitude multiple
iterations approach allows the fragile samples to perform well visually, with a
PSNR of 55 dB in TinyImageNet compared to the original samples, (3) even with
changes in the overall parameters of the model of magnitude 1e-4, the fragile
samples can detect such changes, and (4) the method is independent of the
specific model structure and dataset. We demonstrate the effectiveness of our
method on multiple models and datasets, and show that it outperforms the
current state-of-the-art.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: A Method for Training-free Person Image Picture Generation. (arXiv:2305.09817v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09817">http://arxiv.org/abs/2305.09817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09817] A Method for Training-free Person Image Picture Generation](http://arxiv.org/abs/2305.09817) #diffusion</code></li>
<li>Summary: <p>The current state-of-the-art Diffusion model has demonstrated excellent
results in generating images. However, the images are monotonous and are mostly
the result of the distribution of images of people in the training set, making
it challenging to generate multiple images for a fixed number of individuals.
This problem can often only be solved by fine-tuning the training of the model.
This means that each individual/animated character image must be trained if it
is to be drawn, and the hardware and cost of this training is often beyond the
reach of the average user, who accounts for the largest number of people. To
solve this problem, the Character Image Feature Encoder model proposed in this
paper enables the user to use the process by simply providing a picture of the
character to make the image of the character in the generated image match the
expectation. In addition, various details can be adjusted during the process
using prompts. Unlike traditional Image-to-Image models, the Character Image
Feature Encoder extracts only the relevant image features, rather than
information about the model's composition or movements. In addition, the
Character Image Feature Encoder can be adapted to different models after
training. The proposed model can be conveniently incorporated into the Stable
Diffusion generation process without modifying the model's ontology or used in
combination with Stable Diffusion as a joint model.
</p></li>
</ul>

<h3>Title: Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?. (arXiv:2305.09847v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09847">http://arxiv.org/abs/2305.09847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09847] Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?](http://arxiv.org/abs/2305.09847) #diffusion</code></li>
<li>Summary: <p>This study examines the impact of optimizing the Stable Diffusion (SD) guided
inference pipeline. We propose optimizing certain denoising steps by limiting
the noise computation to conditional noise and eliminating unconditional noise
computation, thereby reducing the complexity of the target iterations by 50%.
Additionally, we demonstrate that later iterations of the SD are less sensitive
to optimization, making them ideal candidates for applying the suggested
optimization. Our experiments show that optimizing the last 20% of the
denoising loop iterations results in an 8.2% reduction in inference time with
almost no perceivable changes to the human eye. Furthermore, we found that by
extending the optimization to 50% of the last iterations, we can reduce
inference time by approximately 20.3%, while still generating visually pleasing
images.
</p></li>
</ul>

<h3>Title: Pyramid Diffusion Models For Low-light Image Enhancement. (arXiv:2305.10028v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10028">http://arxiv.org/abs/2305.10028</a></li>
<li>Code URL: <a href="https://github.com/limuloo/pydiff">https://github.com/limuloo/pydiff</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10028] Pyramid Diffusion Models For Low-light Image Enhancement](http://arxiv.org/abs/2305.10028) #diffusion</code></li>
<li>Summary: <p>Recovering noise-covered details from low-light images is challenging, and
the results given by previous methods leave room for improvement. Recent
diffusion models show realistic and detailed image generation through a
sequence of denoising refinements and motivate us to introduce them to
low-light image enhancement for recovering realistic details. However, we found
two problems when doing this, i.e., 1) diffusion models keep constant
resolution in one reverse process, which limits the speed; 2) diffusion models
sometimes result in global degradation (e.g., RGB shift). To address the above
problems, this paper proposes a Pyramid Diffusion model (PyDiff) for low-light
image enhancement. PyDiff uses a novel pyramid diffusion method to perform
sampling in a pyramid resolution style (i.e., progressively increasing
resolution in one reverse process). Pyramid diffusion makes PyDiff much faster
than vanilla diffusion models and introduces no performance degradation.
Furthermore, PyDiff uses a global corrector to alleviate the global degradation
that may occur in the reverse process, significantly improving the performance
and making the training of diffusion models easier with little additional
computational consumption. Extensive experiments on popular benchmarks show
that PyDiff achieves superior performance and efficiency. Moreover, PyDiff can
generalize well to unseen noise and illumination distributions.
</p></li>
</ul>

<h3>Title: Controllable Mind Visual Diffusion Model. (arXiv:2305.10135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10135">http://arxiv.org/abs/2305.10135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10135] Controllable Mind Visual Diffusion Model](http://arxiv.org/abs/2305.10135) #diffusion</code></li>
<li>Summary: <p>Brain signal visualization has emerged as an active research area, serving as
a critical interface between the human visual system and computer vision
models. Although diffusion models have shown promise in analyzing functional
magnetic resonance imaging (fMRI) data, including reconstructing high-quality
images consistent with original visual stimuli, their accuracy in extracting
semantic and silhouette information from brain signals remains limited. In this
regard, we propose a novel approach, referred to as Controllable Mind Visual
Diffusion Model (CMVDM). CMVDM extracts semantic and silhouette information
from fMRI data using attribute alignment and assistant networks. Additionally,
a residual block is incorporated to capture information beyond semantic and
silhouette features. We then leverage a control model to fully exploit the
extracted information for image synthesis, resulting in generated images that
closely resemble the visual stimuli in terms of semantics and silhouette.
Through extensive experimentation, we demonstrate that CMVDM outperforms
existing state-of-the-art methods both qualitatively and quantitatively.
</p></li>
</ul>

<h3>Title: FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention. (arXiv:2305.10431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10431">http://arxiv.org/abs/2305.10431</a></li>
<li>Code URL: <a href="https://github.com/mit-han-lab/fastcomposer">https://github.com/mit-han-lab/fastcomposer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10431] FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention](http://arxiv.org/abs/2305.10431) #diffusion</code></li>
<li>Summary: <p>Diffusion models excel at text-to-image generation, especially in
subject-driven generation for personalized images. However, existing methods
are inefficient due to the subject-specific fine-tuning, which is
computationally intensive and hampers efficient deployment. Moreover, existing
methods struggle with multi-subject generation as they often blend features
among subjects. We present FastComposer which enables efficient, personalized,
multi-subject text-to-image generation without fine-tuning. FastComposer uses
subject embeddings extracted by an image encoder to augment the generic text
conditioning in diffusion models, enabling personalized image generation based
on subject images and textual instructions with only forward passes. To address
the identity blending problem in the multi-subject generation, FastComposer
proposes cross-attention localization supervision during training, enforcing
the attention of reference subjects localized to the correct regions in the
target images. Naively conditioning on subject embeddings results in subject
overfitting. FastComposer proposes delayed subject conditioning in the
denoising step to maintain both identity and editability in subject-driven
image generation. FastComposer generates images of multiple unseen individuals
with different styles, actions, and contexts. It achieves
300$\times$-2500$\times$ speedup compared to fine-tuning-based methods and
requires zero extra storage for new subjects. FastComposer paves the way for
efficient, personalized, and high-quality multi-subject image creation. Code,
model, and dataset are available at
https://github.com/mit-han-lab/fastcomposer.
</p></li>
</ul>

<h3>Title: Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting. (arXiv:2305.09703v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09703">http://arxiv.org/abs/2305.09703</a></li>
<li>Code URL: <a href="https://github.com/gorgen2020/dvgnn">https://github.com/gorgen2020/dvgnn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09703] Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting](http://arxiv.org/abs/2305.09703) #diffusion</code></li>
<li>Summary: <p>Graph neural networks (GNNs), especially dynamic GNNs, have become a research
hotspot in spatio-temporal forecasting problems. While many dynamic graph
construction methods have been developed, relatively few of them explore the
causal relationship between neighbour nodes. Thus, the resulting models lack
strong explainability for the causal relationship between the neighbour nodes
of the dynamically generated graphs, which can easily lead to a risk in
subsequent decisions. Moreover, few of them consider the uncertainty and noise
of dynamic graphs based on the time series datasets, which are ubiquitous in
real-world graph structure networks. In this paper, we propose a novel Dynamic
Diffusion-Variational Graph Neural Network (DVGNN) for spatio-temporal
forecasting. For dynamic graph construction, an unsupervised generative model
is devised. Two layers of graph convolutional network (GCN) are applied to
calculate the posterior distribution of the latent node embeddings in the
encoder stage. Then, a diffusion model is used to infer the dynamic link
probability and reconstruct causal graphs in the decoder stage adaptively. The
new loss function is derived theoretically, and the reparameterization trick is
adopted in estimating the probability distribution of the dynamic graphs by
Evidence Lower Bound during the backpropagation period. After obtaining the
generated graphs, dynamic GCN and temporal attention are applied to predict
future states. Experiments are conducted on four real-world datasets of
different graph structures in different domains. The results demonstrate that
the proposed DVGNN model outperforms state-of-the-art approaches and achieves
outstanding Root Mean Squared Error result while exhibiting higher robustness.
Also, by F1-score and probability distribution analysis, we demonstrate that
DVGNN better reflects the causal relationship and uncertainty of dynamic
graphs.
</p></li>
</ul>

<h3>Title: Provably Correct Physics-Informed Neural Networks. (arXiv:2305.10157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10157">http://arxiv.org/abs/2305.10157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10157] Provably Correct Physics-Informed Neural Networks](http://arxiv.org/abs/2305.10157) #diffusion</code></li>
<li>Summary: <p>Recent work provides promising evidence that Physics-informed neural networks
(PINN) can efficiently solve partial differential equations (PDE). However,
previous works have failed to provide guarantees on the worst-case residual
error of a PINN across the spatio-temporal domain - a measure akin to the
tolerance of numerical solvers - focusing instead on point-wise comparisons
between their solution and the ones obtained by a solver on a set of inputs. In
real-world applications, one cannot consider tests on a finite set of points to
be sufficient grounds for deployment, as the performance could be substantially
worse on a different set. To alleviate this issue, we establish tolerance-based
correctness conditions for PINNs over the entire input domain. To verify the
extent to which they hold, we introduce $\partial$-CROWN: a general, efficient
and scalable post-training framework to bound PINN residual errors. We
demonstrate its effectiveness in obtaining tight certificates by applying it to
two classically studied PDEs - Burgers' and Schr\"odinger's equations -, and
two more challenging ones with real-world applications - the Allan-Cahn and
Diffusion-Sorption equations.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09828">http://arxiv.org/abs/2305.09828</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09828] Mimetic Initialization of Self-Attention Layers](http://arxiv.org/abs/2305.09828) #transformer</code></li>
<li>Summary: <p>It is notoriously difficult to train Transformers on small datasets;
typically, large pre-trained models are instead used as the starting point. We
explore the weights of such pre-trained Transformers (particularly for vision)
to attempt to find reasons for this discrepancy. Surprisingly, we find that
simply initializing the weights of self-attention layers so that they "look"
more like their pre-trained counterparts allows us to train vanilla
Transformers faster and to higher final accuracies, particularly on vision
tasks such as CIFAR-10 and ImageNet classification, where we see gains in
accuracy of over 5% and 4%, respectively. Our initialization scheme is closed
form, learning-free, and very simple: we set the product of the query and key
weights to be approximately the identity, and the product of the value and
projection weights to approximately the negative identity. As this mimics the
patterns we saw in pre-trained Transformers, we call the technique "mimetic
initialization".
</p></li>
</ul>

<h3>Title: A survey of the Vision Transformers and its CNN-Transformer based Variants. (arXiv:2305.09880v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09880">http://arxiv.org/abs/2305.09880</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09880] A survey of the Vision Transformers and its CNN-Transformer based Variants](http://arxiv.org/abs/2305.09880) #transformer</code></li>
<li>Summary: <p>Vision transformers have recently become popular as a possible alternative to
convolutional neural networks (CNNs) for a variety of computer vision
applications. These vision transformers due to their ability to focus on global
relationships in images have large capacity, but may result in poor
generalization as compared to CNNs. Very recently, the hybridization of
convolution and self-attention mechanisms in vision transformers is gaining
popularity due to their ability of exploiting both local and global image
representations. These CNN-Transformer architectures also known as hybrid
vision transformers have shown remarkable results for vision applications.
Recently, due to the rapidly growing number of these hybrid vision
transformers, there is a need for a taxonomy and explanation of these
architectures. This survey presents a taxonomy of the recent vision transformer
architectures, and more specifically that of the hybrid vision transformers.
Additionally, the key features of each architecture such as the attention
mechanisms, positional embeddings, multi-scale processing, and convolution are
also discussed. This survey highlights the potential of hybrid vision
transformers to achieve outstanding performance on a variety of computer vision
tasks. Moreover, it also points towards the future directions of this rapidly
evolving field.
</p></li>
</ul>

<h3>Title: CageViT: Convolutional Activation Guided Efficient Vision Transformer. (arXiv:2305.09924v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09924">http://arxiv.org/abs/2305.09924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09924] CageViT: Convolutional Activation Guided Efficient Vision Transformer](http://arxiv.org/abs/2305.09924) #transformer</code></li>
<li>Summary: <p>Recently, Transformers have emerged as the go-to architecture for both vision
and language modeling tasks, but their computational efficiency is limited by
the length of the input sequence. To address this, several efficient variants
of Transformers have been proposed to accelerate computation or reduce memory
consumption while preserving performance. This paper presents an efficient
vision Transformer, called CageViT, that is guided by convolutional activation
to reduce computation. Our CageViT, unlike current Transformers, utilizes a new
encoder to handle the rearranged tokens, bringing several technical
contributions: 1) Convolutional activation is used to pre-process the token
after patchifying the image to select and rearrange the major tokens and minor
tokens, which substantially reduces the computation cost through an additional
fusion layer. 2) Instead of using the class activation map of the convolutional
model directly, we design a new weighted class activation to lower the model
requirements. 3) To facilitate communication between major tokens and fusion
tokens, Gated Linear SRA is proposed to further integrate fusion tokens into
the attention mechanism. We perform a comprehensive validation of CageViT on
the image classification challenge.
</p></li>
</ul>

<p>Experimental results demonstrate that the proposed CageViT outperforms the
most recent state-of-the-art backbones by a large margin in terms of
efficiency, while maintaining a comparable level of accuracy (e.g. a
moderate-sized 43.35M model trained solely on 224 x 224 ImageNet-1K can achieve
Top-1 accuracy of 83.4% accuracy).
</p>

<h3>Title: EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging. (arXiv:2305.10006v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10006">http://arxiv.org/abs/2305.10006</a></li>
<li>Code URL: <a href="https://github.com/ucaswangls/efficientsci">https://github.com/ucaswangls/efficientsci</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10006] EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging](http://arxiv.org/abs/2305.10006) #transformer</code></li>
<li>Summary: <p>Video snapshot compressive imaging (SCI) uses a two-dimensional detector to
capture consecutive video frames during a single exposure time. Following this,
an efficient reconstruction algorithm needs to be designed to reconstruct the
desired video frames. Although recent deep learning-based state-of-the-art
(SOTA) reconstruction algorithms have achieved good results in most tasks, they
still face the following challenges due to excessive model complexity and GPU
memory limitations:
</p></li>
</ul>

<p>1) these models need high computational cost, and
</p>
<p>2) they are usually unable to reconstruct large-scale video frames at high
compression ratios.
</p>
<p>To address these issues, we develop an {\bf{\em efficient network}} for video
SCI by using {\bf {\em dense connections and space-time factorization
mechanism}} within a single residual block, dubbed {\bf \emph{EfficientSCI}}.
The EfficientSCI network can well establish spatial-temporal correlation by
using {\bf {\em convolution in the spatial domain and Transformer in the
temporal domain}}, respectively. We are the first time to show that an UHD
color video with high compression ratio can be reconstructed from a snapshot 2D
measurement using a single end-to-end deep learning model with PSNR above 32
dB. Extensive results on both simulation and real data show that our method
significantly outperforms all previous SOTA algorithms with better real-time
performance. The code is at
\url{https://github.com/ucaswangls/EfficientSCI.git}.
</p>

<h3>Title: Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers. (arXiv:2305.10018v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10018">http://arxiv.org/abs/2305.10018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10018] Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers](http://arxiv.org/abs/2305.10018) #transformer</code></li>
<li>Summary: <p>Fine-grained classification is a challenging task that involves identifying
subtle differences between objects within the same category. This task is
particularly challenging in scenarios where data is scarce. Visual transformers
(ViT) have recently emerged as a powerful tool for image classification, due to
their ability to learn highly expressive representations of visual data using
self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine
tuned using semi-supervised learning techniques, suitable for situations where
we have lack of annotated data. This is particularly common in e-commerce,
where images are readily available but labels are noisy, nonexistent, or
expensive to obtain. Our results demonstrate that Semi-ViT outperforms
traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned
with limited annotated data. These findings indicate that Semi-ViTs hold
significant promise for applications that require precise and fine-grained
classification of visual data.
</p></li>
</ul>

<h3>Title: Two-Stream Regression Network for Dental Implant Position Prediction. (arXiv:2305.10044v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10044">http://arxiv.org/abs/2305.10044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10044] Two-Stream Regression Network for Dental Implant Position Prediction](http://arxiv.org/abs/2305.10044) #transformer</code></li>
<li>Summary: <p>In implant prosthesis treatment, the design of surgical guide requires lots
of manual labors and is prone to subjective variations. When deep learning
based methods has started to be applied to address this problem, the space
between teeth are various and some of them might present similar texture
characteristic with the actual implant region. Both problems make a big
challenge for the implant position prediction. In this paper, we develop a
two-stream implant position regression framework (TSIPR), which consists of an
implant region detector (IRD) and a multi-scale patch embedding regression
network (MSPENet), to address this issue. For the training of IRD, we extend
the original annotation to provide additional supervisory information, which
contains much more rich characteristic and do not introduce extra labeling
costs. A multi-scale patch embedding module is designed for the MSPENet to
adaptively extract features from the images with various tooth spacing. The
global-local feature interaction block is designed to build the encoder of
MSPENet, which combines the transformer and convolution for enriched feature
representation. During inference, the RoI mask extracted from the IRD is used
to refine the prediction results of the MSPENet. Extensive experiments on a
dental implant dataset through five-fold cross-validation demonstrated that the
proposed TSIPR achieves superior performance than existing methods.
</p></li>
</ul>

<h3>Title: Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?. (arXiv:2305.10247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10247">http://arxiv.org/abs/2305.10247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10247] Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?](http://arxiv.org/abs/2305.10247) #transformer</code></li>
<li>Summary: <p>Copy-move forgery detection is a crucial research area within digital image
forensics, as it focuses on identifying instances where objects in an image are
duplicated and placed in different locations. The detection of such forgeries
is particularly important in contexts where they can be exploited for malicious
purposes. Recent years have witnessed an increased interest in distinguishing
between the original and duplicated objects in copy-move forgeries, accompanied
by the development of larger-scale datasets to facilitate this task. However,
existing approaches to copy-move forgery detection and source/target
differentiation often involve two separate steps or the design of individual
end-to-end networks for each task. In this paper, we propose an innovative
method that employs the transformer architecture in an end-to-end deep neural
network. Our method aims to detect instances of copy-move forgery while
simultaneously localizing the source and target regions. By utilizing this
approach, we address the challenges posed by multi-object copy-move scenarios
and report if there is a balance between the detection and differentiation
tasks. To evaluate the performance of our proposed network, we conducted
experiments on two publicly available copy-move datasets. The results and
analysis aims to show the potential significance of our focus in balancing
detection and distinguishment result and transferring the trained model in
different datasets in the field.
</p></li>
</ul>

<h3>Title: CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo. (arXiv:2305.10320v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10320">http://arxiv.org/abs/2305.10320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10320] CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo](http://arxiv.org/abs/2305.10320) #transformer</code></li>
<li>Summary: <p>The core of Multi-view Stereo(MVS) is the matching process among reference
and source pixels. Cost aggregation plays a significant role in this process,
while previous methods focus on handling it via CNNs. This may inherit the
natural limitation of CNNs that fail to discriminate repetitive or incorrect
matches due to limited local receptive fields. To handle the issue, we aim to
involve Transformer into cost aggregation. However, another problem may occur
due to the quadratically growing computational complexity caused by
Transformer, resulting in memory overflow and inference latency. In this paper,
we overcome these limits with an efficient Transformer-based cost aggregation
network, namely CostFormer. The Residual Depth-Aware Cost Transformer(RDACT) is
proposed to aggregate long-range features on cost volume via self-attention
mechanisms along the depth and spatial dimensions. Furthermore, Residual
Regression Transformer(RRT) is proposed to enhance spatial attention. The
proposed method is a universal plug-in to improve learning-based MVS methods.
</p></li>
</ul>

<h3>Title: On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09807">http://arxiv.org/abs/2305.09807</a></li>
<li>Code URL: <a href="https://github.com/fjelenic/al-transfer">https://github.com/fjelenic/al-transfer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09807] On Dataset Transferability in Active Learning for Transformers](http://arxiv.org/abs/2305.09807) #transformer</code></li>
<li>Summary: <p>Active learning (AL) aims to reduce labeling costs by querying the examples
most beneficial for model learning. While the effectiveness of AL for
fine-tuning transformer-based pre-trained language models (PLMs) has been
demonstrated, it is less clear to what extent the AL gains obtained with one
model transfer to others. We consider the problem of transferability of
actively acquired datasets in text classification and investigate whether AL
gains persist when a dataset built using AL coupled with a specific PLM is used
to train a different PLM. We link the AL dataset transferability to the
similarity of instances queried by the different PLMs and show that AL methods
with similar acquisition sequences produce highly transferable datasets
regardless of the models used. Additionally, we show that the similarity of
acquisition sequences is influenced more by the choice of the AL method than
the choice of the model.
</p></li>
</ul>

<h3>Title: A quantitative study of NLP approaches to question difficulty estimation. (arXiv:2305.10236v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10236">http://arxiv.org/abs/2305.10236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10236] A quantitative study of NLP approaches to question difficulty estimation](http://arxiv.org/abs/2305.10236) #transformer</code></li>
<li>Summary: <p>Recent years witnessed an increase in the amount of research on the task of
Question Difficulty Estimation from Text QDET with Natural Language Processing
(NLP) techniques, with the goal of targeting the limitations of traditional
approaches to question calibration. However, almost the entirety of previous
research focused on single silos, without performing quantitative comparisons
between different models or across datasets from different educational domains.
In this work, we aim at filling this gap, by quantitatively analyzing several
approaches proposed in previous research, and comparing their performance on
three publicly available real world datasets containing questions of different
types from different educational domains. Specifically, we consider reading
comprehension Multiple Choice Questions (MCQs), science MCQs, and math
questions. We find that Transformer based models are the best performing across
different educational domains, with DistilBERT performing almost as well as
BERT, and that they outperform other approaches even on smaller datasets. As
for the other models, the hybrid ones often outperform the ones based on a
single type of features, the ones based on linguistic features perform well on
reading comprehension questions, while frequency based features (TF-IDF) and
word embeddings (word2vec) perform better in domain knowledge assessment.
</p></li>
</ul>

<h3>Title: Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10427">http://arxiv.org/abs/2305.10427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10427] Accelerating Transformer Inference for Translation via Parallel Decoding](http://arxiv.org/abs/2305.10427) #transformer</code></li>
<li>Summary: <p>Autoregressive decoding limits the efficiency of transformers for Machine
Translation (MT). The community proposed specific network architectures and
learning-based methods to solve this issue, which are expensive and require
changes to the MT model, trading inference speed at the cost of the translation
quality. In this paper, we propose to address the problem from the point of
view of decoding algorithms, as a less explored but rather compelling
direction. We propose to reframe the standard greedy autoregressive decoding of
MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point
iteration methods for fast inference. This formulation allows to speed up
existing models without training or modifications while retaining translation
quality. We present three parallel decoding algorithms and test them on
different languages and models showing how the parallelization introduces a
speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x
when scaling the method on parallel resources. Finally, we introduce a decoding
dependency graph visualizer (DDGviz) that let us see how the model has learned
the conditional dependence between tokens and inspect the decoding procedure.
</p></li>
</ul>

<h3>Title: Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10308">http://arxiv.org/abs/2305.10308</a></li>
<li>Code URL: <a href="https://github.com/somaonishi/mtr">https://github.com/somaonishi/mtr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10308] Rethinking Data Augmentation for Tabular Data in Deep Learning](http://arxiv.org/abs/2305.10308) #transformer</code></li>
<li>Summary: <p>Tabular data is the most widely used data format in machine learning (ML).
While tree-based methods outperform DL-based methods in supervised learning,
recent literature reports that self-supervised learning with Transformer-based
models outperforms tree-based methods. In the existing literature on
self-supervised learning for tabular data, contrastive learning is the
predominant method. In contrastive learning, data augmentation is important to
generate different views. However, data augmentation for tabular data has been
difficult due to the unique structure and high complexity of tabular data. In
addition, three main components are proposed together in existing methods:
model structure, self-supervised learning methods, and data augmentation.
Therefore, previous works have compared the performance without comprehensively
considering these components, and it is not clear how each component affects
the actual performance.
</p></li>
</ul>

<p>In this study, we focus on data augmentation to address these issues. We
propose a novel data augmentation method, $\textbf{M}$ask $\textbf{T}$oken
$\textbf{R}$eplacement ($\texttt{MTR}$), which replaces the mask token with a
portion of each tokenized column; $\texttt{MTR}$ takes advantage of the
properties of Transformer, which is becoming the predominant DL-based
architecture for tabular data, to perform data augmentation for each column
embedding. Through experiments with 13 diverse public datasets in both
supervised and self-supervised learning scenarios, we show that $\texttt{MTR}$
achieves competitive performance against existing data augmentation methods and
improves model performance. In addition, we discuss specific scenarios in which
$\texttt{MTR}$ is most effective and identify the scope of its application. The
code is available at https://github.com/somaonishi/MTR/.
</p>

<h3>Title: G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks. (arXiv:2305.10329v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10329">http://arxiv.org/abs/2305.10329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10329] G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks](http://arxiv.org/abs/2305.10329) #transformer</code></li>
<li>Summary: <p>It has become a popular paradigm to transfer the knowledge of large-scale
pre-trained models to various downstream tasks via fine-tuning the entire model
parameters. However, with the growth of model scale and the rising number of
downstream tasks, this paradigm inevitably meets the challenges in terms of
computation consumption and memory footprint issues. Recently,
Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a
promising paradigm to alleviate these concerns by updating only a portion of
parameters. Despite these PEFTs having demonstrated satisfactory performance in
natural language processing, it remains under-explored for the question of
whether these techniques could be transferred to graph-based tasks with Graph
Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by
providing extensive benchmarks with traditional PEFTs on a range of graph-based
downstream tasks. Our empirical study shows that it is sub-optimal to directly
transfer existing PEFTs to graph-based tasks due to the issue of feature
distribution shift. To address this issue, we propose a novel structure-aware
PEFT approach, named G-Adapter, which leverages graph convolution operation to
introduce graph structure (e.g., graph adjacent matrix) as an inductive bias to
guide the updating process. Besides, we propose Bregman proximal point
optimization to further alleviate feature distribution shift by preventing the
model from aggressive update. Extensive experiments demonstrate that G-Adapter
obtains the state-of-the-art performance compared to the counterparts on nine
graph benchmark datasets based on two pre-trained GTNs, and delivers tremendous
memory footprint efficiency compared to the conventional paradigm.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10118">http://arxiv.org/abs/2305.10118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10118] Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques](http://arxiv.org/abs/2305.10118) #generative</code></li>
<li>Summary: <p>Acquiring and annotating suitable datasets for training deep learning models
is challenging. This often results in tedious and time-consuming efforts that
can hinder research progress. However, generative models have emerged as a
promising solution for generating synthetic datasets that can replace or
augment real-world data. Despite this, the effectiveness of synthetic data is
limited by their inability to fully capture the complexity and diversity of
real-world data. To address this issue, we explore the use of Generative
Adversarial Networks to generate synthetic datasets for training classifiers
that are subsequently evaluated on real-world images. To improve the quality
and diversity of the synthetic dataset, we propose three novel post-processing
techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion
Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which
applies these techniques in an optimal and coordinated manner to maximise
classification accuracy on real-world data. Our experiments show that GaFi
effectively reduces the gap with real-accuracy scores to an error of 2.03%,
1.78%, and 3.99% on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets,
respectively. These results represent a new state of the art in Classification
Accuracy Score and highlight the effectiveness of post-processing techniques in
improving the quality of synthetic datasets.
</p></li>
</ul>

<h3>Title: Fusion-S2iGan: An Efficient and Effective Single-Stage Framework for Speech-to-Image Generation. (arXiv:2305.10126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10126">http://arxiv.org/abs/2305.10126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10126] Fusion-S2iGan: An Efficient and Effective Single-Stage Framework for Speech-to-Image Generation](http://arxiv.org/abs/2305.10126) #generative</code></li>
<li>Summary: <p>The goal of a speech-to-image transform is to produce a photo-realistic
picture directly from a speech signal. Recently, various studies have focused
on this task and have achieved promising performance. However, current
speech-to-image approaches are based on a stacked modular framework that
suffers from three vital issues: 1) Training separate networks is
time-consuming as well as inefficient and the convergence of the final
generative model strongly depends on the previous generators; 2) The quality of
precursor images is ignored by this architecture; 3) Multiple discriminator
networks are required to be trained. To this end, we propose an efficient and
effective single-stage framework called Fusion-S2iGan to yield perceptually
plausible and semantically consistent image samples on the basis of given
spoken descriptions. Fusion-S2iGan introduces a visual+speech fusion module
(VSFM), constructed with a pixel-attention module (PAM), a speech-modulation
module (SMM) and a weighted-fusion module (WFM), to inject the speech embedding
from a speech encoder into the generator while improving the quality of
synthesized pictures. Fusion-S2iGan spreads the bimodal information over all
layers of the generator network to reinforce the visual feature maps at various
hierarchical levels in the architecture. We conduct a series of experiments on
four benchmark data sets, i.e., CUB birds, Oxford-102, Flickr8k and
Places-subset. The experimental results demonstrate the superiority of the
presented Fusion-S2iGan compared to the state-of-the-art models with a
multi-stage architecture and a performance level that is close to traditional
text-to-image approaches.
</p></li>
</ul>

<h3>Title: What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10400">http://arxiv.org/abs/2305.10400</a></li>
<li>Code URL: <a href="https://github.com/yonatanbitton/wysiwyr">https://github.com/yonatanbitton/wysiwyr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10400] What You See is What You Read? Improving Text-Image Alignment Evaluation](http://arxiv.org/abs/2305.10400) #generative</code></li>
<li>Summary: <p>Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.
</p></li>
</ul>

<h3>Title: PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering. (arXiv:2305.10415v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10415">http://arxiv.org/abs/2305.10415</a></li>
<li>Code URL: <a href="https://github.com/xiaoman-zhang/PMC-VQA">https://github.com/xiaoman-zhang/PMC-VQA</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10415] PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering](http://arxiv.org/abs/2305.10415) #generative</code></li>
<li>Summary: <p>In this paper, we focus on the problem of Medical Visual Question Answering
(MedVQA), which is crucial in efficiently interpreting medical images with
vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as
a generation task that naturally follows the human-machine interaction, we
propose a generative-based model for medical visual understanding by aligning
visual information from a pre-trained vision encoder with a large language
model. Secondly, we establish a scalable pipeline to construct a large-scale
medical visual question-answering dataset, named PMC-VQA, which contains 227k
VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we
pre-train our proposed model on PMC-VQA and then fine-tune it on multiple
public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a
large margin. Additionally, we propose a test set that has undergone manual
verification, which is significantly more challenging, even the best models
struggle to solve.
</p></li>
</ul>

<h3>Title: Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09696">http://arxiv.org/abs/2305.09696</a></li>
<li>Code URL: <a href="https://github.com/zhangtp1996/taptap">https://github.com/zhangtp1996/taptap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09696] Generative Table Pre-training Empowers Models for Tabular Prediction](http://arxiv.org/abs/2305.09696) #generative</code></li>
<li>Summary: <p>Recently, the topic of table pre-training has attracted considerable research
interest. However, how to employ table pre-training to boost the performance of
tabular prediction remains an open challenge. In this paper, we propose TapTap,
the first attempt that leverages table pre-training to empower models for
tabular prediction. After pre-training on a large corpus of real-world tabular
data, TapTap can generate high-quality synthetic tables to support various
applications on tabular data, including privacy protection, low resource
regime, missing value imputation, and imbalanced classification. Extensive
experiments on 12 datasets demonstrate that TapTap outperforms a total of 16
baselines in different scenarios. Meanwhile, it can be easily combined with
various backbone models, including LightGBM, Multilayer Perceptron (MLP) and
Transformer. Moreover, with the aid of table pre-training, models trained using
synthetic data generated by TapTap can even compete with models using the
original dataset on half of the experimental datasets, marking a milestone in
the development of synthetic tabular data generation. The codes are available
at https://github.com/ZhangTP1996/TapTap.
</p></li>
</ul>

<h3>Title: SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09781">http://arxiv.org/abs/2305.09781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09781] SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification](http://arxiv.org/abs/2305.09781) #generative</code></li>
<li>Summary: <p>The high computational and memory requirements of generative large language
models (LLMs) make it challenging to serve them quickly and cheaply. This paper
introduces SpecInfer, an LLM serving system that accelerates generative LLM
inference with speculative inference and token tree verification. A key insight
behind SpecInfer is to combine various collectively boost-tuned small language
models to jointly predict the LLM's outputs; the predictions are organized as a
token tree, whose nodes each represent a candidate token sequence. The
correctness of all candidate token sequences represented by a token tree is
verified by the LLM in parallel using a novel tree-based parallel decoding
mechanism. SpecInfer uses an LLM as a token tree verifier instead of an
incremental decoder, which significantly reduces the end-to-end latency and
computational requirement for serving generative LLMs while provably preserving
model quality.
</p></li>
</ul>

<h3>Title: Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09859">http://arxiv.org/abs/2305.09859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09859] Smaller Language Models are Better Black-box Machine-Generated Text Detectors](http://arxiv.org/abs/2305.09859) #generative</code></li>
<li>Summary: <p>With the advent of fluent generative language models that can produce
convincing utterances very similar to those written by humans, distinguishing
whether a piece of text is machine-generated or human-written becomes more
challenging and more important, as such models could be used to spread
misinformation, fake news, fake reviews and to mimic certain authors and
figures. To this end, there have been a slew of methods proposed to detect
machine-generated text. Most of these methods need access to the logits of the
target model or need the ability to sample from the target. One such black-box
detection method relies on the observation that generated text is locally
optimal under the likelihood function of the generator, while human-written
text is not. We find that overall, smaller and partially-trained models are
better universal text detectors: they can more precisely detect text generated
from both small and larger models. Interestingly, we find that whether the
detector and generator were trained on the same data is not critically
important to the detection success. For instance the OPT-125M model has an AUC
of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT
family, GPTJ-6B, has AUC of 0.45.
</p></li>
</ul>

<h3>Title: Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10163">http://arxiv.org/abs/2305.10163</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10163] Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model](http://arxiv.org/abs/2305.10163) #generative</code></li>
<li>Summary: <p>Generative Pre-Training (GPT) models like ChatGPT have demonstrated
exceptional performance in various Natural Language Processing (NLP) tasks.
Although ChatGPT has been integrated into the overall workflow to boost
efficiency in many domains, the lack of flexibility in the finetuning process
hinders its applications in areas that demand extensive domain expertise and
semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on
the China National Medical Licensing Examination (CNMLE) and propose a novel
approach to improve ChatGPT from two perspectives: integrating medical domain
knowledge and enabling few-shot learning. By using a simple but effective
retrieval method, medical background knowledge is extracted as semantic
instructions to guide the inference of ChatGPT. Similarly, relevant medical
questions are identified and fed as demonstrations to ChatGPT. Experimental
results show that directly applying ChatGPT fails to qualify the CNMLE at a
score of 51 (i.e., only 51\% of questions are answered correctly). While our
knowledge-enhanced model achieves a high score of 70 on CNMLE-2022 which not
only passes the qualification but also surpasses the average score of humans
(61). This research demonstrates the potential of knowledge-enhanced ChatGPT to
serve as versatile medical assistants, capable of analyzing real-world medical
problems in a more accessible, user-friendly, and adaptable manner.
</p></li>
</ul>

<h3>Title: Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents. (arXiv:2305.10383v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10383">http://arxiv.org/abs/2305.10383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10383] Large-Scale Text Analysis Using Generative Language Models: A Case Study in Discovering Public Value Expressions in AI Patents](http://arxiv.org/abs/2305.10383) #generative</code></li>
<li>Summary: <p>Labeling data is essential for training text classifiers but is often
difficult to accomplish accurately, especially for complex and abstract
concepts. Seeking an improved method, this paper employs a novel approach using
a generative language model (GPT-4) to produce labels and rationales for
large-scale text analysis. We apply this approach to the task of discovering
public value expressions in US AI patents. We collect a database comprising
154,934 patent documents using an advanced Boolean query submitted to
InnovationQ+. The results are merged with full patent text from the USPTO,
resulting in 5.4 million sentences. We design a framework for identifying and
labeling public value expressions in these AI patent sentences. A prompt for
GPT-4 is developed which includes definitions, guidelines, examples, and
rationales for text classification. We evaluate the quality of the labels and
rationales produced by GPT-4 using BLEU scores and topic modeling and find that
they are accurate, diverse, and faithful. These rationales also serve as a
chain-of-thought for the model, a transparent mechanism for human verification,
and support for human annotators to overcome cognitive limitations. We conclude
that GPT-4 achieved a high-level of recognition of public value theory from our
framework, which it also uses to discover unseen public value expressions. We
use the labels produced by GPT-4 to train BERT-based classifiers and predict
sentences on the entire database, achieving high F1 scores for the 3-class
(0.85) and 2-class classification (0.91) tasks. We discuss the implications of
our approach for conducting large-scale text analyses with complex and abstract
concepts and suggest that, with careful framework design and interactive human
oversight, generative language models can offer significant advantages in
quality and in reduced time and costs for producing labels and rationales.
</p></li>
</ul>

<h3>Title: BSGAN: A Novel Oversampling Technique for Imbalanced Pattern Recognitions. (arXiv:2305.09777v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09777">http://arxiv.org/abs/2305.09777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09777] BSGAN: A Novel Oversampling Technique for Imbalanced Pattern Recognitions](http://arxiv.org/abs/2305.09777) #generative</code></li>
<li>Summary: <p>Class imbalanced problems (CIP) are one of the potential challenges in
developing unbiased Machine Learning (ML) models for predictions. CIP occurs
when data samples are not equally distributed between the two or multiple
classes. Borderline-Synthetic Minority Oversampling Techniques (SMOTE) is one
of the approaches that has been used to balance the imbalance data by
oversampling the minor (limited) samples. One of the potential drawbacks of
existing Borderline-SMOTE is that it focuses on the data samples that lay at
the border point and gives more attention to the extreme observations,
ultimately limiting the creation of more diverse data after oversampling, and
that is the almost scenario for the most of the borderline-SMOTE based
oversampling strategies. As an effect, marginalization occurs after
oversampling. To address these issues, in this work, we propose a hybrid
oversampling technique by combining the power of borderline SMOTE and
Generative Adversarial Network to generate more diverse data that follow
Gaussian distributions. We named it BSGAN and tested it on four highly
imbalanced datasets: Ecoli, Wine quality, Yeast, and Abalone. Our preliminary
computational results reveal that BSGAN outperformed existing borderline SMOTE
and GAN-based oversampling techniques and created a more diverse dataset that
follows normal distribution after oversampling effect.
</p></li>
</ul>

<h3>Title: Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10120">http://arxiv.org/abs/2305.10120</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10120] Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models](http://arxiv.org/abs/2305.10120) #generative</code></li>
<li>Summary: <p>The recent proliferation of large-scale text-to-image models has led to
growing concerns that such models may be misused to generate harmful,
misleading, and inappropriate content. Motivated by this issue, we derive a
technique inspired by continual learning to selectively forget concepts in
pretrained deep generative models. Our method, dubbed Selective Amnesia,
enables controllable forgetting where a user can specify how a concept should
be forgotten. Selective Amnesia can be applied to conditional variational
likelihood models, which encompass a variety of popular deep generative
frameworks, including variational autoencoders and large-scale text-to-image
diffusion models. Experiments across different models demonstrate that our
approach induces forgetting on a variety of concepts, from entire classes in
standard datasets to celebrity and nudity prompts in text-to-image models. Our
code is publicly available at https://github.com/clear-nus/selective-amnesia.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot. (arXiv:2305.09758v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09758">http://arxiv.org/abs/2305.09758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09758] A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot](http://arxiv.org/abs/2305.09758) #large language model</code></li>
<li>Summary: <p>Multimedia content, such as advertisements and story videos, exhibit a rich
blend of creativity and multiple modalities. They incorporate elements like
text, visuals, audio, and storytelling techniques, employing devices like
emotions, symbolism, and slogans to convey meaning. While previous research in
multimedia understanding has focused mainly on videos with specific actions
like cooking, there is a dearth of large annotated training datasets, hindering
the development of supervised learning models with satisfactory performance for
real-world applications. However, the rise of large language models (LLMs) has
witnessed remarkable zero-shot performance in various natural language
processing (NLP) tasks, such as emotion classification, question-answering, and
topic classification. To bridge this performance gap in multimedia
understanding, we propose verbalizing story videos to generate their
descriptions in natural language and then performing video-understanding tasks
on the generated story as opposed to the original video. Through extensive
experiments on five video-understanding tasks, we demonstrate that our method,
despite being zero-shot, achieves significantly better results than supervised
baselines for video understanding. Further, alleviating a lack of story
understanding benchmarks, we publicly release the first dataset on a crucial
task in computational social science, persuasion strategy identification.
</p></li>
</ul>

<h3>Title: Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10355">http://arxiv.org/abs/2305.10355</a></li>
<li>Code URL: <a href="https://github.com/rucaibox/pope">https://github.com/rucaibox/pope</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10355] Evaluating Object Hallucination in Large Vision-Language Models](http://arxiv.org/abs/2305.10355) #large language model</code></li>
<li>Summary: <p>Inspired by the superior language abilities of large language models (LLM),
large vision-language models (LVLM) have been recently explored by integrating
powerful LLMs for improving the performance on complex multimodal tasks.
Despite the promising progress on LVLMs, we find that LVLMs suffer from the
hallucination problem, i.e. they tend to generate objects that are inconsistent
with the target images in the descriptions. To investigate it, this work
presents the first systematic study on object hallucination of LVLMs. We
conduct the evaluation experiments on several representative LVLMs, and show
that they mostly suffer from severe object hallucination issue. We further
discuss that the visual instructions may influence the hallucination, and find
that: objects that frequently occur in the visual instructions or co-occur with
the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we
find that existing evaluation methods might be affected by the input
instructions and generation styles of LVLMs. Thus, we further design an
improved evaluation method for object hallucination by proposing a
polling-based query method called \emph{POPE}. Experiment results demonstrate
that our POPE can evaluate the object hallucination in a more stable and
flexible way. Our codes and data are publicly available at
https://github.com/RUCAIBox/POPE.
</p></li>
</ul>

<h3>Title: What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning. (arXiv:2305.09731v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09731">http://arxiv.org/abs/2305.09731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09731] What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning](http://arxiv.org/abs/2305.09731) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) exploit in-context learning (ICL) to solve tasks
with only a few demonstrations, but its mechanisms are not yet well-understood.
Some works suggest that LLMs only recall already learned concepts from
pre-training, while others hint that ICL performs implicit learning over
demonstrations. We characterize two ways through which ICL leverages
demonstrations. Task recognition (TR) captures the extent to which LLMs can
recognize a task through demonstrations -- even without ground-truth labels --
and apply their pre-trained priors, whereas task learning (TL) is the ability
to capture new input-label mappings unseen in pre-training. Using a wide range
of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we
design controlled experiments to disentangle the roles of TR and TL in ICL. We
show that (1) models can achieve non-trivial performance with only TR, and TR
does not further improve with larger models or more demonstrations; (2) LLMs
acquire TL as the model scales, and TL's performance consistently improves with
more demonstrations in context. Our findings unravel two different forces
behind ICL and we advocate for discriminating them in future ICL research due
to their distinct nature.
</p></li>
</ul>

<h3>Title: CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09857">http://arxiv.org/abs/2305.09857</a></li>
<li>Code URL: <a href="https://github.com/vipulraheja/coedit">https://github.com/vipulraheja/coedit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09857] CoEdIT: Text Editing by Task-Specific Instruction Tuning](http://arxiv.org/abs/2305.09857) #large language model</code></li>
<li>Summary: <p>Text editing or revision is an essential function of the human writing
process. Understanding the capabilities of LLMs for making high-quality
revisions and collaborating with human writers is a critical step toward
building effective writing assistants. With the prior success of LLMs and
instruction tuning, we leverage instruction-tuned LLMs for text revision to
improve the quality of user-generated text and improve the efficiency of the
process. We introduce CoEdIT, a state-of-the-art text editing model for writing
assistance. CoEdIT takes instructions from the user specifying the attributes
of the desired text, such as "Make the sentence simpler" or "Write it in a more
neutral style," and outputs the edited text. We present a large language model
fine-tuned on a diverse collection of task-specific instructions for text
editing (a total of 82K instructions). Our model (1) achieves state-of-the-art
performance on various text editing benchmarks, (2) is competitive with
publicly available largest-sized LLMs trained on instructions while being
$\sim$60x smaller, (3) is capable of generalizing to unseen edit instructions,
and (4) exhibits compositional comprehension abilities to generalize to
instructions containing different combinations of edit actions. Through
extensive qualitative and quantitative analysis, we show that writers prefer
the edits suggested by CoEdIT, relative to other state-of-the-art text editing
models. Our code and dataset are publicly available.
</p></li>
</ul>

<h3>Title: CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09955">http://arxiv.org/abs/2305.09955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09955] CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge](http://arxiv.org/abs/2305.09955) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) are increasingly adopted for knowledge-intensive
tasks and contexts. Existing approaches improve the knowledge capabilities of
general-purpose LLMs through retrieval or generated knowledge prompting, but
they fall short of reflecting two key properties of knowledge-rich models:
knowledge should be modular, ever-growing, sourced from diverse domains;
knowledge acquisition and production should be a collaborative process, where
diverse stakeholders contribute new information. To this end, we propose CooK,
a novel framework to empower general-purpose large language models with modular
and collaboratively sourced knowledge. We first introduce specialized language
models, autoregressive models trained on corpora from a wide range of domains
and sources. These specialized LMs serve as parametric knowledge repositories
that are later prompted to generate background knowledge for general-purpose
LLMs. We then propose three knowledge filters to dynamically select and retain
information in generated documents by controlling for relevance, brevity, and
factuality. Finally, we propose bottom-up and top-down knowledge integration
approaches to augment general-purpose LLMs with the curated (relevant, factual)
knowledge from community-driven specialized LMs that enable multi-domain
knowledge synthesis and on-demand knowledge requests. Through extensive
experiments, we demonstrate that CooK achieves state-of-the-art performance on
six benchmark datasets. Our results highlight the potential of enriching
general-purpose LLMs with evolving and modular knowledge -- relevant knowledge
that can be continuously updated through the collective efforts of the research
community.
</p></li>
</ul>

<h3>Title: Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10037">http://arxiv.org/abs/2305.10037</a></li>
<li>Code URL: <a href="https://github.com/arthur-heng/nlgraph">https://github.com/arthur-heng/nlgraph</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10037] Can Language Models Solve Graph Problems in Natural Language?](http://arxiv.org/abs/2305.10037) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) are increasingly adopted for a variety of tasks
with implicit graphical structures, such as planning in robotics, multi-hop
question answering or knowledge probing, structured commonsense reasoning, and
more. While LLMs have advanced the state-of-the-art on these tasks with
structure implications, whether LLMs could explicitly process textual
descriptions of graphs and structures, map them to grounded conceptual spaces,
and perform structured operations remains underexplored. To this end, we
propose NLGraph (Natural Language Graph), a comprehensive benchmark of
graph-based problem solving designed in natural language. NLGraph contains
29,370 problems, covering eight graph reasoning tasks with varying complexity
from simple tasks such as connectivity and shortest path up to complex problems
such as maximum flow and simulating graph neural networks. We evaluate LLMs
(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find
that 1) language models do demonstrate preliminary graph reasoning abilities,
2) the benefit of advanced prompting and in-context learning diminishes on more
complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the
face of spurious correlations in graph and problem settings. We then propose
Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based
approaches to enhance LLMs in solving natural language graph problems.
Build-a-Graph and Algorithmic prompting improve the performance of LLMs on
NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to
solve the most complicated graph reasoning tasks in our setup with language
models remains an open research question. The NLGraph benchmark and evaluation
code are available at https://github.com/Arthur-Heng/NLGraph.
</p></li>
</ul>

<h3>Title: Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. (arXiv:2305.10142v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10142">http://arxiv.org/abs/2305.10142</a></li>
<li>Code URL: <a href="https://github.com/franxyao/gpt-bargaining">https://github.com/franxyao/gpt-bargaining</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10142] Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback](http://arxiv.org/abs/2305.10142) #large language model</code></li>
<li>Summary: <p>We study whether multiple large language models (LLMs) can autonomously
improve each other in a negotiation game by playing, reflecting, and
criticizing. We are interested in this question because if LLMs were able to
improve each other, it would imply the possibility of creating strong AI agents
with minimal human intervention. We ask two LLMs to negotiate with each other,
playing the roles of a buyer and a seller, respectively. They aim to reach a
deal with the buyer targeting a lower price and the seller a higher one. A
third language model, playing the critic, provides feedback to a player to
improve the player's negotiation strategies. We let the two agents play
multiple rounds, using previous negotiation history and AI feedback as
in-context demonstrations to improve the model's negotiation strategy
iteratively. We use different LLMs (GPT and Claude) for different roles and use
the deal price as the evaluation metric. Our experiments reveal multiple
intriguing findings: (1) Only a subset of the language models we consider can
self-play and improve the deal price from AI feedback, weaker models either do
not understand the game's rules or cannot incorporate AI feedback for further
improvement. (2) Models' abilities to learn from the feedback differ when
playing different roles. For example, it is harder for Claude-instant to
improve as the buyer than as the seller. (3) When unrolling the game to
multiple rounds, stronger agents can consistently improve their performance by
meaningfully using previous experiences and iterative AI feedback, yet have a
higher risk of breaking the deal. We hope our work provides insightful initial
explorations of having models autonomously improve each other with game playing
and AI feedback.
</p></li>
</ul>

<h3>Title: A Survey on Zero Pronoun Translation. (arXiv:2305.10196v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10196">http://arxiv.org/abs/2305.10196</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10196] A Survey on Zero Pronoun Translation](http://arxiv.org/abs/2305.10196) #large language model</code></li>
<li>Summary: <p>Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.
Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop
languages (e.g. English). This phenomenon has been studied extensively in
machine translation (MT), as it poses a significant challenge for MT systems
due to the difficulty in determining the correct antecedent for the pronoun.
This survey paper highlights the major works that have been undertaken in zero
pronoun translation (ZPT) after the neural revolution, so that researchers can
recognise the current state and future directions of this field. We provide an
organisation of the literature based on evolution, dataset, method and
evaluation. In addition, we compare and analyze competing models and evaluation
metrics on different benchmarks. We uncover a number of insightful findings
such as: 1) ZPT is in line with the development trend of large language model;
2) data limitation causes learning bias in languages and domains; 3)
performance improvements are often reported on single benchmarks, but advanced
methods are still far from real-world use; 4) general-purpose metrics are not
reliable on nuances and complexities of ZPT, emphasizing the necessity of
targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of
gender bias.
</p></li>
</ul>

<h3>Title: MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10250">http://arxiv.org/abs/2305.10250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](http://arxiv.org/abs/2305.10250) #large language model</code></li>
<li>Summary: <p>Revolutionary advancements in Large Language Models have drastically reshaped
our interactions with artificial intelligence systems. Despite this, a notable
hindrance remains-the deficiency of a long-term memory mechanism within these
models. This shortfall becomes increasingly evident in situations demanding
sustained interaction, such as personal companion systems and psychological
counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored
for LLMs. MemoryBank enables the models to summon relevant memories,
continually evolve through continuous memory updates, comprehend, and adapt to
a user personality by synthesizing information from past interactions. To mimic
anthropomorphic behaviors and selectively preserve memory, MemoryBank
incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting
Curve theory, which permits the AI to forget and reinforce memory based on time
elapsed and the relative significance of the memory, thereby offering a
human-like memory mechanism. MemoryBank is versatile in accommodating both
closed-source models like ChatGPT and open-source models like ChatGLM. We
exemplify application of MemoryBank through the creation of an LLM-based
chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned
with psychological dialogs, SiliconFriend displays heightened empathy in its
interactions. Experiment involves both qualitative analysis with real-world
user dialogs and quantitative analysis with simulated dialogs. In the latter,
ChatGPT acts as users with diverse characteristics and generates long-term
dialog contexts covering a wide array of topics. The results of our analysis
reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong
capability for long-term companionship as it can provide emphatic response,
recall relevant memories and understand user personality.
</p></li>
</ul>

<h3>Title: M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10263">http://arxiv.org/abs/2305.10263</a></li>
<li>Code URL: <a href="https://github.com/tjunlp-lab/m3ke">https://github.com/tjunlp-lab/m3ke</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10263] M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models](http://arxiv.org/abs/2305.10263) #large language model</code></li>
<li>Summary: <p>Large language models have recently made tremendous progress in a variety of
aspects, e.g., cross-task generalization, instruction following.
Comprehensively evaluating the capability of large language models in multiple
tasks is of great importance. In this paper, we propose M3KE, a Massive
Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to
measure knowledge acquired by Chinese large language models by testing their
multitask accuracy in zero- and few-shot settings. We have collected 20,477
questions from 71 tasks. Our selection covers all major levels of Chinese
education system, ranging from the primary school to college, as well as a wide
variety of subjects, including humanities, history, politics, law, education,
psychology, science, technology, art and religion. All questions are
multiple-choice questions with four options, hence guaranteeing a standardized
and unified assessment process. We've assessed a number of state-of-the-art
open-source Chinese large language models on the proposed benchmark. The size
of these models varies from 335M to 130B parameters. Experiment results
demonstrate that they perform significantly worse than GPT-3.5 that reaches an
accuracy of ~ 48% on M3KE. The dataset is available at
https://github.com/tjunlp-lab/M3KE.
</p></li>
</ul>

<h3>Title: Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability. (arXiv:2305.10266v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10266">http://arxiv.org/abs/2305.10266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10266] Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](http://arxiv.org/abs/2305.10266) #large language model</code></li>
<li>Summary: <p>Large, multilingual language models exhibit surprisingly good zero- or
few-shot machine translation capabilities, despite having never seen the
intentionally-included translation examples provided to typical neural
translation systems. We investigate the role of incidental bilingualism -- the
unintentional consumption of bilingual signals, including translation examples
-- in explaining the translation capabilities of large language models, taking
the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method
approach to measure and understand incidental bilingualism at scale. We show
that PaLM is exposed to over 30 million translation pairs across at least 44
languages. Furthermore, the amount of incidental bilingual content is highly
correlated with the amount of monolingual in-language content for non-English
languages. We relate incidental bilingual content to zero-shot prompts and show
that it can be used to mine new prompts to improve PaLM's out-of-English
zero-shot translation quality. Finally, in a series of small-scale ablations,
we show that its presence has a substantial impact on translation capabilities,
although this impact diminishes with model scale.
</p></li>
</ul>

<h3>Title: Using a Large Language Model to Control Speaking Style for Expressive TTS. (arXiv:2305.10321v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10321">http://arxiv.org/abs/2305.10321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10321] Using a Large Language Model to Control Speaking Style for Expressive TTS](http://arxiv.org/abs/2305.10321) #large language model</code></li>
<li>Summary: <p>Appropriate prosody is critical for successful spoken communication.
Contextual word embeddings are proven to be helpful in predicting prosody but
do not allow for choosing between plausible prosodic renditions.
Reference-based TTS models attempt to address this by conditioning speech
generation on a reference speech sample. These models can generate expressive
speech but this requires finding an appropriate reference.
</p></li>
</ul>

<p>Sufficiently large generative language models have been used to solve various
language-related tasks. We explore whether such models can be used to suggest
appropriate prosody for expressive TTS. We train a TTS model on a
non-expressive corpus and then prompt the language model to suggest changes to
pitch, energy and duration. The prompt can be designed for any task and we
prompt the model to make suggestions based on target speaking style and
dialogue context. The proposed method is rated most appropriate in 49.9\% of
cases compared to 31.0\% for a baseline model.
</p>

<h3>Title: BAD: BiAs Detection for Large Language Models in the context of candidate screening. (arXiv:2305.10407v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10407">http://arxiv.org/abs/2305.10407</a></li>
<li>Code URL: <a href="https://github.com/namhkoh/bad-bias-detection-in-llms">https://github.com/namhkoh/bad-bias-detection-in-llms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10407] BAD: BiAs Detection for Large Language Models in the context of candidate screening](http://arxiv.org/abs/2305.10407) #large language model</code></li>
<li>Summary: <p>Application Tracking Systems (ATS) have allowed talent managers, recruiters,
and college admissions committees to process large volumes of potential
candidate applications efficiently. Traditionally, this screening process was
conducted manually, creating major bottlenecks due to the quantity of
applications and introducing many instances of human bias. The advent of large
language models (LLMs) such as ChatGPT and the potential of adopting methods to
current automated application screening raises additional bias and fairness
issues that must be addressed. In this project, we wish to identify and
quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the
context of candidate screening in order to demonstrate how the use of these
models could perpetuate existing biases and inequalities in the hiring process.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Integrating Multiple Sources Knowledge for Class Asymmetry Domain Adaptation Segmentation of Remote Sensing Images. (arXiv:2305.09893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09893">http://arxiv.org/abs/2305.09893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09893] Integrating Multiple Sources Knowledge for Class Asymmetry Domain Adaptation Segmentation of Remote Sensing Images](http://arxiv.org/abs/2305.09893) #segmentation</code></li>
<li>Summary: <p>In the existing unsupervised domain adaptation (UDA) methods for remote
sensing images (RSIs) semantic segmentation, class symmetry is an widely
followed ideal assumption, where the source and target RSIs have exactly the
same class space. In practice, however, it is often very difficult to find a
source RSI with exactly the same classes as the target RSI. More commonly,
there are multiple source RSIs available. To this end, a novel class asymmetry
RSIs domain adaptation method with multiple sources is proposed in this paper,
which consists of four key components. Firstly, a multi-branch segmentation
network is built to learn an expert for each source RSI. Secondly, a novel
collaborative learning method with the cross-domain mixing strategy is
proposed, to supplement the class information for each source while achieving
the domain adaptation of each source-target pair. Thirdly, a pseudo-label
generation strategy is proposed to effectively combine strengths of different
experts, which can be flexibly applied to two cases where the source class
union is equal to or includes the target class set. Fourthly, a
multiview-enhanced knowledge integration module is developed for the high-level
knowledge routing and transfer from multiple domains to target predictions.
</p></li>
</ul>

<h3>Title: Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences. (arXiv:2305.09928v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.09928">http://arxiv.org/abs/2305.09928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.09928] Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences](http://arxiv.org/abs/2305.09928) #segmentation</code></li>
<li>Summary: <p>The increasing use of deep learning techniques has reduced interpretation
time and, ideally, reduced interpreter bias by automatically deriving
geological maps from digital outcrop models. However, accurate validation of
these automated mapping approaches is a significant challenge due to the
subjective nature of geological mapping and the difficulty in collecting
quantitative validation data. Additionally, many state-of-the-art deep learning
methods are limited to 2D image data, which is insufficient for 3D digital
outcrops, such as hyperclouds. To address these challenges, we present Tinto, a
multi-sensor benchmark digital outcrop dataset designed to facilitate the
development and validation of deep learning approaches for geological mapping,
especially for non-structured 3D data like point clouds. Tinto comprises two
complementary sets: 1) a real digital outcrop model from Corta Atalaya (Spain),
with spectral attributes and ground-truth data, and 2) a synthetic twin that
uses latent features in the original datasets to reconstruct realistic spectral
data (including sensor noise and processing artifacts) from the ground-truth.
The point cloud is dense and contains 3,242,964 labeled points. We used these
datasets to explore the abilities of different deep learning approaches for
automated geological mapping. By making Tinto publicly available, we hope to
foster the development and adaptation of new deep learning tools for 3D
applications in Earth sciences. The dataset can be accessed through this link:
https://doi.org/10.14278/rodare.2256.
</p></li>
</ul>

<h3>Title: SAM for Poultry Science. (arXiv:2305.10254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10254">http://arxiv.org/abs/2305.10254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10254] SAM for Poultry Science](http://arxiv.org/abs/2305.10254) #segmentation</code></li>
<li>Summary: <p>In recent years, the agricultural industry has witnessed significant
advancements in artificial intelligence (AI), particularly with the development
of large-scale foundational models. Among these foundation models, the Segment
Anything Model (SAM), introduced by Meta AI Research, stands out as a
groundbreaking solution for object segmentation tasks. While SAM has shown
success in various agricultural applications, its potential in the poultry
industry, specifically in the context of cage-free hens, remains relatively
unexplored. This study aims to assess the zero-shot segmentation performance of
SAM on representative chicken segmentation tasks, including part-based
segmentation and the use of infrared thermal images, and to explore
chicken-tracking tasks by using SAM as a segmentation tool. The results
demonstrate SAM's superior performance compared to SegFormer and SETR in both
whole and part-based chicken segmentation. SAM-based object tracking also
provides valuable data on the behavior and movement patterns of broiler birds.
The findings of this study contribute to a better understanding of SAM's
potential in poultry science and lay the foundation for future advancements in
chicken segmentation and tracking.
</p></li>
</ul>

<h3>Title: Explain Any Concept: Segment Anything Meets Concept-Based Explanation. (arXiv:2305.10289v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.10289">http://arxiv.org/abs/2305.10289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.10289] Explain Any Concept: Segment Anything Meets Concept-Based Explanation](http://arxiv.org/abs/2305.10289) #segmentation</code></li>
<li>Summary: <p>EXplainable AI (XAI) is an essential topic to improve human understanding of
deep neural networks (DNNs) given their black-box internals. For computer
vision tasks, mainstream pixel-based XAI methods explain DNN decisions by
identifying important pixels, and emerging concept-based XAI explore forming
explanations with concepts (e.g., a head in an image). However, pixels are
generally hard to interpret and sensitive to the imprecision of XAI methods,
whereas "concepts" in prior works require human annotation or are limited to
pre-defined concept sets. On the other hand, driven by large-scale
pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful
and promotable framework for performing precise and comprehensive instance
segmentation, enabling automatic preparation of concept sets from a given
image. This paper for the first time explores using SAM to augment
concept-based XAI. We offer an effective and flexible concept-based explanation
method, namely Explain Any Concept (EAC), which explains DNN decisions with any
concept. While SAM is highly effective and offers an "out-of-the-box" instance
segmentation, it is costly when being integrated into defacto XAI pipelines. We
thus propose a lightweight per-input equivalent (PIE) scheme, enabling
efficient explanation with a surrogate model. Our evaluation over two popular
datasets (ImageNet and COCO) illustrate the highly encouraging performance of
EAC over commonly-used XAI methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
