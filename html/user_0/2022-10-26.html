<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Differentially Private Language Models for Secure Data Sharing. (arXiv:2210.13918v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13918">http://arxiv.org/abs/2210.13918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13918] Differentially Private Language Models for Secure Data Sharing](http://arxiv.org/abs/2210.13918)</code></li>
<li>Summary: <p>To protect the privacy of individuals whose data is being shared, it is of
high importance to develop methods allowing researchers and companies to
release textual data while providing formal privacy guarantees to its
originators. In the field of NLP, substantial efforts have been directed at
building mechanisms following the framework of local differential privacy,
thereby anonymizing individual text samples before releasing them. In practice,
these approaches are often dissatisfying in terms of the quality of their
output language due to the strong noise required for local differential
privacy. In this paper, we approach the problem at hand using global
differential privacy, particularly by training a generative language model in a
differentially private manner and consequently sampling data from it. Using
natural language prompts and a new prompt-mismatch loss, we are able to create
highly accurate and fluent textual datasets taking on specific desired
attributes such as sentiment or topic and resembling statistical properties of
the training data. We perform thorough experiments indicating that our
synthetic datasets do not leak information from our original data and are of
high language quality and highly suitable for training models for further
analysis on real-world data. Notably, we also demonstrate that training
classifiers on private synthetic data outperforms directly training classifiers
on real data with DP-SGD.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Machine and Deep Learning for IoT Security and Privacy: Applications, Challenges, and Future Directions. (arXiv:2210.13547v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13547">http://arxiv.org/abs/2210.13547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13547] Machine and Deep Learning for IoT Security and Privacy: Applications, Challenges, and Future Directions](http://arxiv.org/abs/2210.13547)</code></li>
<li>Summary: <p>The integration of the Internet of Things (IoT) connects a number of
intelligent devices with a minimum of human interference that can interact with
one another. IoT is rapidly emerging in the areas of computer science. However,
new security problems were posed by the cross-cutting design of the
multidisciplinary elements and IoT systems involved in deploying such schemes.
Ineffective is the implementation of security protocols, i.e., authentication,
encryption, application security, and access network for IoT systems and their
essential weaknesses in security. Current security approaches can also be
improved to protect the IoT environment effectively. In recent years, deep
learning (DL)/ machine learning (ML) has progressed significantly in various
critical implementations. Therefore, DL/ML methods are essential to turn IoT
systems protection from simply enabling safe contact between IoT systems to
intelligence systems in security. This review aims to include an extensive
analysis of ML systems and state-of-the-art developments in DL methods to
improve enhanced IoT device protection methods. On the other hand, various new
insights in machine and deep learning for IoT Securities illustrate how it
could help future research. IoT protection risks relating to emerging or
essential threats are identified, as well as future IoT device attacks and
possible threats associated with each surface. We then carefully analyze DL and
ML IoT protection approaches and present each approach's benefits,
possibilities, and weaknesses. This review discusses a number of potential
challenges and limitations. The future works, recommendations, and suggestions
of DL/ML in IoT security are also included.
</p></li>
</ul>

<h3>Title: Musings on the HashGraph Protocol: Its Security and Its Limitations. (arXiv:2210.13682v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13682">http://arxiv.org/abs/2210.13682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13682] Musings on the HashGraph Protocol: Its Security and Its Limitations](http://arxiv.org/abs/2210.13682)</code></li>
<li>Summary: <p>The HashGraph Protocol is a Byzantine fault tolerant atomic broadcast
protocol. Its novel use of locally stored metadata allows parties to recover a
consistent ordering of their log just by examining their local data, removing
the need for a voting protocol. Our paper's first contribution is to present a
rewritten proof of security for the HashGraph Protocol that follows the
consistency and liveness paradigm used in the atomic broadcast literature. In
our second contribution, we show a novel adversarial strategy that stalls the
protocol from committing data to the log for an expected exponential number of
rounds. This proves tight the exponential upper bound conjectured in the
original paper. We believe that our proof of security will make it easier to
compare HashGraph with other atomic broadcast protocols and to incorporate its
ideas into new constructions. We also believe that our attack might inspire
more research into similar attacks for other DAG-based atomic broadcast
protocols.
</p></li>
</ul>

<h3>Title: Leveraging the Verifier's Dilemma to Double Spend in Bitcoin. (arXiv:2210.14072v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14072">http://arxiv.org/abs/2210.14072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14072] Leveraging the Verifier's Dilemma to Double Spend in Bitcoin](http://arxiv.org/abs/2210.14072)</code></li>
<li>Summary: <p>We describe and analyze perishing mining, a novel block-withholding mining
strategy that lures profit-driven miners away from doing useful work on the
public chain by releasing block headers from a privately maintained chain. We
then introduce the dual private chain (DPC) attack, where an adversary that
aims at double spending increases its success rate by intermittently dedicating
part of its hash power to perishing mining. We detail the DPC attack's Markov
decision process, evaluate its double spending success rate using Monte Carlo
simulations. We show that the DPC attack lowers Bitcoin's security bound in the
presence of profit-driven miners that do not wait to validate the transactions
of a block before mining on it.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Private Online Prediction from Experts: Separations and Faster Rates. (arXiv:2210.13537v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13537">http://arxiv.org/abs/2210.13537</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13537] Private Online Prediction from Experts: Separations and Faster Rates](http://arxiv.org/abs/2210.13537)</code></li>
<li>Summary: <p>Online prediction from experts is a fundamental problem in machine learning
and several works have studied this problem under privacy constraints. We
propose and analyze new algorithms for this problem that improve over the
regret bounds of the best existing algorithms for non-adaptive adversaries. For
approximate differential privacy, our algorithms achieve regret bounds of
$\tilde{O}(\sqrt{T \log d} + \log d/\varepsilon)$ for the stochastic setting
and $\tilde O(\sqrt{T \log d} + T^{1/3} \log d/\varepsilon)$ for oblivious
adversaries (where $d$ is the number of experts). For pure DP, our algorithms
are the first to obtain sub-linear regret for oblivious adversaries in the
high-dimensional regime $d \ge T$. Moreover, we prove new lower bounds for
adaptive adversaries. Our results imply that unlike the non-private setting,
there is a strong separation between the optimal regret for adaptive and
non-adaptive adversaries for this problem. Our lower bounds also show a
separation between pure and approximate differential privacy for adaptive
adversaries where the latter is necessary to achieve the non-private
$O(\sqrt{T})$ regret.
</p></li>
</ul>

<h3>Title: Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From Fano. (arXiv:2210.13662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13662">http://arxiv.org/abs/2210.13662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13662] Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From Fano](http://arxiv.org/abs/2210.13662)</code></li>
<li>Summary: <p>Differential privacy (DP) is by far the most widely accepted framework for
mitigating privacy risks in machine learning. However, exactly how small the
privacy parameter $\epsilon$ needs to be to protect against certain privacy
risks in practice is still not well-understood. In this work, we study data
reconstruction attacks for discrete data and analyze it under the framework of
multiple hypothesis testing. We utilize different variants of the celebrated
Fano's inequality to derive upper bounds on the inferential power of a data
reconstruction adversary when the model is trained differentially privately.
Importantly, we show that if the underlying private data takes values from a
set of size $M$, then the target privacy parameter $\epsilon$ can be $O(\log
M)$ before the adversary gains significant inferential power. Our analysis
offers theoretical evidence for the empirical effectiveness of DP against data
reconstruction attacks even at relatively large values of $\epsilon$.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Towards Robust Recommender Systems via Triple Cooperative Defense. (arXiv:2210.13762v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13762">http://arxiv.org/abs/2210.13762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13762] Towards Robust Recommender Systems via Triple Cooperative Defense](http://arxiv.org/abs/2210.13762)</code></li>
<li>Summary: <p>Recommender systems are often susceptible to well-crafted fake profiles,
leading to biased recommendations. The wide application of recommender systems
makes studying the defense against attack necessary. Among existing defense
methods, data-processing-based methods inevitably exclude normal samples, while
model-based methods struggle to enjoy both generalization and robustness.
Considering the above limitations, we suggest integrating data processing and
robust model and propose a general framework, Triple Cooperative Defense (TCD),
which cooperates to improve model robustness through the co-training of three
models. Specifically, in each round of training, we sequentially use the
high-confidence prediction ratings (consistent ratings) of any two models as
auxiliary training data for the remaining model, and the three models
cooperatively improve recommendation robustness. Notably, TCD adds pseudo label
data instead of deleting abnormal data, which avoids the cleaning of normal
data, and the cooperative training of the three models is also beneficial to
model generalization. Through extensive experiments with five poisoning attacks
on three real-world datasets, the results show that the robustness improvement
of TCD significantly outperforms baselines. It is worth mentioning that TCD is
also beneficial for model generalizations.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Mixed Precision Quantization to Tackle Gradient Leakage Attacks in Federated Learning. (arXiv:2210.13457v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13457">http://arxiv.org/abs/2210.13457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13457] Mixed Precision Quantization to Tackle Gradient Leakage Attacks in Federated Learning](http://arxiv.org/abs/2210.13457)</code></li>
<li>Summary: <p>Federated Learning (FL) enables collaborative model building among a large
number of participants without the need for explicit data sharing. But this
approach shows vulnerabilities when privacy inference attacks are applied to
it. In particular, in the event of a gradient leakage attack, which has a
higher success rate in retrieving sensitive data from the model gradients, FL
models are at higher risk due to the presence of communication in their
inherent architecture. The most alarming thing about this gradient leakage
attack is that it can be performed in such a covert way that it does not hamper
the training performance while the attackers backtrack from the gradients to
get information about the raw data. Two of the most common approaches proposed
as solutions to this issue are homomorphic encryption and adding noise with
differential privacy parameters. These two approaches suffer from two major
drawbacks. They are: the key generation process becomes tedious with the
increasing number of clients, and noise-based differential privacy suffers from
a significant drop in global model accuracy. As a countermeasure, we propose a
mixed-precision quantized FL scheme, and we empirically show that both of the
issues addressed above can be resolved. In addition, our approach can ensure
more robustness as different layers of the deep model are quantized with
different precision and quantization modes. We empirically proved the validity
of our method with three benchmark datasets and found a minimal accuracy drop
in the global model after applying quantization.
</p></li>
</ul>

<h3>Title: Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future. (arXiv:2210.13463v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13463">http://arxiv.org/abs/2210.13463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13463] Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present and Future](http://arxiv.org/abs/2210.13463)</code></li>
<li>Summary: <p>In this paper, we review adversarial pretraining of self-supervised deep
networks including both convolutional neural networks and vision transformers.
Unlike the adversarial training with access to labeled examples, adversarial
pretraining is complicated as it only has access to unlabeled examples. To
incorporate adversaries into pretraining models on either input or feature
level, we find that existing approaches are largely categorized into two
groups: memory-free instance-wise attacks imposing worst-case perturbations on
individual examples, and memory-based adversaries shared across examples over
iterations. In particular, we review several representative adversarial
pretraining models based on Contrastive Learning (CL) and Masked Image Modeling
(MIM), respectively, two popular self-supervised pretraining methods in
literature. We also review miscellaneous issues about computing overheads,
input-/feature-level adversaries, as well as other adversarial pretraining
approaches beyond the above two groups. Finally, we discuss emerging trends and
future directions about the relations between adversarial and cooperative
pretraining, unifying adversarial CL and MIM pretraining, and the trade-off
between accuracy and robustness in adversarial pretraining.
</p></li>
</ul>

<h3>Title: Detection of Real-time DeepFakes in Video Conferencing with Active Probing and Corneal Reflection. (arXiv:2210.14153v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14153">http://arxiv.org/abs/2210.14153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14153] Detection of Real-time DeepFakes in Video Conferencing with Active Probing and Corneal Reflection](http://arxiv.org/abs/2210.14153)</code></li>
<li>Summary: <p>The COVID pandemic has led to the wide adoption of online video calls in
recent years. However, the increasing reliance on video calls provides
opportunities for new impersonation attacks by fraudsters using the advanced
real-time DeepFakes. Real-time DeepFakes pose new challenges to detection
methods, which have to run in real-time as a video call is ongoing. In this
paper, we describe a new active forensic method to detect real-time DeepFakes.
Specifically, we authenticate video calls by displaying a distinct pattern on
the screen and using the corneal reflection extracted from the images of the
call participant's face. This pattern can be induced by a call participant
displaying on a shared screen or directly integrated into the video-call
client. In either case, no specialized imaging or lighting hardware is
required. Through large-scale simulations, we evaluate the reliability of this
approach under a range in a variety of real-world imaging scenarios.
</p></li>
</ul>

<h3>Title: Model-Free Prediction of Adversarial Drop Points in 3D Point Clouds. (arXiv:2210.14164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14164">http://arxiv.org/abs/2210.14164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14164] Model-Free Prediction of Adversarial Drop Points in 3D Point Clouds](http://arxiv.org/abs/2210.14164)</code></li>
<li>Summary: <p>Adversarial attacks pose serious challenges for deep neural network
(DNN)-based analysis of various input signals. In the case of 3D point clouds,
methods have been developed to identify points that play a key role in the
network decision, and these become crucial in generating existing adversarial
attacks. For example, a saliency map approach is a popular method for
identifying adversarial drop points, whose removal would significantly impact
the network decision. Generally, methods for identifying adversarial points
rely on the deep model itself in order to determine which points are critically
important for the model's decision. This paper aims to provide a novel
viewpoint on this problem, in which adversarial points can be predicted
independently of the model. To this end, we define 14 point cloud features and
use multiple linear regression to examine whether these features can be used
for model-free adversarial point prediction, and which combination of features
is best suited for this purpose. Experiments show that a suitable combination
of features is able to predict adversarial points of three different networks
-- PointNet, PointNet++, and DGCNN -- significantly better than a random guess.
The results also provide further insight into DNNs for point cloud analysis, by
showing which features play key roles in their decision-making process.
</p></li>
</ul>

<h3>Title: SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (arXiv:2210.13660v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13660">http://arxiv.org/abs/2210.13660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13660] SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning](http://arxiv.org/abs/2210.13660)</code></li>
<li>Summary: <p>Existing literature on adversarial Machine Learning (ML) focuses either on
showing attacks that break every ML model, or defenses that withstand most
attacks. Unfortunately, little consideration is given to the actual
\textit{cost} of the attack or the defense. Moreover, adversarial samples are
often crafted in the "feature-space", making the corresponding evaluations of
questionable value. Simply put, the current situation does not allow to
estimate the actual threat posed by adversarial attacks, leading to a lack of
secure ML systems.
</p></li>
</ul>

<p>We aim to clarify such confusion in this paper. By considering the
application of ML for Phishing Website Detection (PWD), we formalize the
"evasion-space" in which an adversarial perturbation can be introduced to fool
a ML-PWD -- demonstrating that even perturbations in the "feature-space" are
useful. Then, we propose a realistic threat model describing evasion attacks
against ML-PWD that are cheap to stage, and hence intrinsically more attractive
for real phishers. Finally, we perform the first statistically validated
assessment of state-of-the-art ML-PWD against 12 evasion attacks. Our
evaluation shows (i) the true efficacy of evasion attempts that are more likely
to occur; and (ii) the impact of perturbations crafted in different
evasion-spaces. Our realistic evasion attempts induce a statistically
significant degradation (3-10% at $p\!<$0.05), and their cheap cost makes them
a subtle threat. Notably, however, some ML-PWD are immune to our most realistic
attacks ($p$=0.22). Our contribution paves the way for a much needed
re-assessment of adversarial attacks against ML systems for cybersecurity.
</p>

<h3>Title: Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs. (arXiv:2210.13710v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13710">http://arxiv.org/abs/2210.13710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13710] Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs](http://arxiv.org/abs/2210.13710)</code></li>
<li>Summary: <p>Graph neural network (GNN) with a powerful representation capability has been
widely applied to various areas, such as biological gene prediction, social
recommendation, etc. Recent works have exposed that GNN is vulnerable to the
backdoor attack, i.e., models trained with maliciously crafted training samples
are easily fooled by patched samples. Most of the proposed studies launch the
backdoor attack using a trigger that either is the randomly generated subgraph
(e.g., erd\H{o}s-r\'enyi backdoor) for less computational burden, or the
gradient-based generative subgraph (e.g., graph trojaning attack) to enable a
more effective attack. However, the interpretation of how is the trigger
structure and the effect of the backdoor attack related has been overlooked in
the current literature. Motifs, recurrent and statistically significant
sub-graphs in graphs, contain rich structure information. In this paper, we are
rethinking the trigger from the perspective of motifs, and propose a
motif-based backdoor attack, denoted as Motif-Backdoor. It contributes from
three aspects. (i) Interpretation: it provides an in-depth explanation for
backdoor effectiveness by the validity of the trigger structure from motifs,
leading to some novel insights, e.g., using subgraphs that appear less
frequently in the graph as the trigger can achieve better attack performance.
(ii) Effectiveness: Motif-Backdoor reaches the state-of-the-art (SOTA) attack
performance in both black-box and defensive scenarios. (iii) Efficiency: based
on the graph motif distribution, Motif-Backdoor can quickly obtain an effective
trigger structure without target model feedback or subgraph model generation.
Extensive experimental results show that Motif-Backdoor realizes the SOTA
performance on three popular models and four public datasets compared with five
baselines.
</p></li>
</ul>

<h3>Title: Hindering Adversarial Attacks with Implicit Neural Representations. (arXiv:2210.13982v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13982">http://arxiv.org/abs/2210.13982</a></li>
<li>Code URL: <a href="https://github.com/deepmind/linac">https://github.com/deepmind/linac</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13982] Hindering Adversarial Attacks with Implicit Neural Representations](http://arxiv.org/abs/2210.13982)</code></li>
<li>Summary: <p>We introduce the Lossy Implicit Network Activation Coding (LINAC) defence, an
input transformation which successfully hinders several common adversarial
attacks on CIFAR-$10$ classifiers for perturbations up to $\epsilon = 8/255$ in
$L_\infty$ norm and $\epsilon = 0.5$ in $L_2$ norm. Implicit neural
representations are used to approximately encode pixel colour intensities in
$2\text{D}$ images such that classifiers trained on transformed data appear to
have robustness to small perturbations without adversarial training or large
drops in performance. The seed of the random number generator used to
initialise and train the implicit neural representation turns out to be
necessary information for stronger generic attacks, suggesting its role as a
private key. We devise a Parametric Bypass Approximation (PBA) attack strategy
for key-based defences, which successfully invalidates an existing method in
this category. Interestingly, our LINAC defence also hinders some transfer and
adaptive attacks, including our novel PBA strategy. Our results emphasise the
importance of a broad range of customised attacks despite apparent robustness
according to standard evaluations. LINAC source code and parameters of defended
classifier evaluated throughout this submission are available:
https://github.com/deepmind/linac
</p></li>
</ul>

<h3>Title: A White-Box Adversarial Attack Against a Digital Twin. (arXiv:2210.14018v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14018">http://arxiv.org/abs/2210.14018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14018] A White-Box Adversarial Attack Against a Digital Twin](http://arxiv.org/abs/2210.14018)</code></li>
<li>Summary: <p>Recent research has shown that Machine Learning/Deep Learning (ML/DL) models
are particularly vulnerable to adversarial perturbations, which are small
changes made to the input data in order to fool a machine learning classifier.
The Digital Twin, which is typically described as consisting of a physical
entity, a virtual counterpart, and the data connections in between, is
increasingly being investigated as a means of improving the performance of
physical entities by leveraging computational techniques, which are enabled by
the virtual counterpart. This paper explores the susceptibility of Digital Twin
(DT), a virtual model designed to accurately reflect a physical object using
ML/DL classifiers that operate as Cyber Physical Systems (CPS), to adversarial
attacks. As a proof of concept, we first formulate a DT of a vehicular system
using a deep neural network architecture and then utilize it to launch an
adversarial attack. We attack the DT model by perturbing the input to the
trained model and show how easily the model can be broken with white-box
attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13512">http://arxiv.org/abs/2210.13512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13512] Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup](http://arxiv.org/abs/2210.13512)</code></li>
<li>Summary: <p>Mixup is a data augmentation technique that relies on training using random
convex combinations of data points and their labels. In recent years, Mixup has
become a standard primitive used in the training of state-of-the-art image
classification models due to its demonstrated benefits over empirical risk
minimization with regards to generalization and robustness. In this work, we
try to explain some of this success from a feature learning perspective. We
focus our attention on classification problems in which each class may have
multiple associated features (or views) that can be used to predict the class
correctly. Our main theoretical results demonstrate that, for a non-trivial
class of data distributions with two features per class, training a 2-layer
convolutional network using empirical risk minimization can lead to learning
only one feature for almost all classes while training with a specific
instantiation of Mixup succeeds in learning both features for every class. We
also show empirically that these theoretical insights extend to the practical
settings of image benchmarks modified to have additional synthetic features.
</p></li>
</ul>

<h3>Title: Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and Refinement. (arXiv:2210.13529v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13529">http://arxiv.org/abs/2210.13529</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13529] Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and Refinement](http://arxiv.org/abs/2210.13529)</code></li>
<li>Summary: <p>Estimating 3D poses and shapes in the form of meshes from monocular RGB
images is challenging. Obviously, it is more difficult than estimating 3D poses
only in the form of skeletons or heatmaps. When interacting persons are
involved, the 3D mesh reconstruction becomes more challenging due to the
ambiguity introduced by person-to-person occlusions. To tackle the challenges,
we propose a coarse-to-fine pipeline that benefits from 1) inverse kinematics
from the occlusion-robust 3D skeleton estimation and 2) Transformer-based
relation-aware refinement techniques. In our pipeline, we first obtain
occlusion-robust 3D skeletons for multiple persons from an RGB image. Then, we
apply inverse kinematics to convert the estimated skeletons to deformable 3D
mesh parameters. Finally, we apply the Transformer-based mesh refinement that
refines the obtained mesh parameters considering intra- and inter-person
relations of 3D meshes. Via extensive experiments, we demonstrate the
effectiveness of our method, outperforming state-of-the-arts on 3DPW, MuPoTS
and AGORA datasets.
</p></li>
</ul>

<h3>Title: Video based Object 6D Pose Estimation using Transformers. (arXiv:2210.13540v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13540">http://arxiv.org/abs/2210.13540</a></li>
<li>Code URL: <a href="https://github.com/apoorvabeedu/videopose">https://github.com/apoorvabeedu/videopose</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13540] Video based Object 6D Pose Estimation using Transformers](http://arxiv.org/abs/2210.13540)</code></li>
<li>Summary: <p>We introduce a Transformer based 6D Object Pose Estimation framework
VideoPose, comprising an end-to-end attention based modelling architecture,
that attends to previous frames in order to estimate accurate 6D Object Poses
in videos. Our approach leverages the temporal information from a video
sequence for pose refinement, along with being computationally efficient and
robust. Compared to existing methods, our architecture is able to capture and
reason from long-range dependencies efficiently, thus iteratively refining over
video sequences. Experimental evaluation on the YCB-Video dataset shows that
our approach is on par with the state-of-the-art Transformer methods, and
performs significantly better relative to CNN based approaches. Further, with a
speed of 33 fps, it is also more efficient and therefore applicable to a
variety of applications that require real-time object pose estimation. Training
code and pretrained models are available at
https://github.com/ApoorvaBeedu/VideoPose
</p></li>
</ul>

<h3>Title: The Robustness Limits of SoTA Vision Models to Natural Variation. (arXiv:2210.13604v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13604">http://arxiv.org/abs/2210.13604</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13604] The Robustness Limits of SoTA Vision Models to Natural Variation](http://arxiv.org/abs/2210.13604)</code></li>
<li>Summary: <p>Recent state-of-the-art vision models introduced new architectures, learning
paradigms, and larger pretraining data, leading to impressive performance on
tasks such as classification. While previous generations of vision models were
shown to lack robustness to factors such as pose, it's unclear the extent to
which this next generation of models are more robust. To study this question,
we develop a dataset of more than 7 million images with controlled changes in
pose, position, background, lighting, and size. We study not only how robust
recent state-of-the-art models are, but also the extent to which models can
generalize variation in factors when they're present during training. We
consider a catalog of recent vision models, including vision transformers
(ViT), self-supervised models such as masked autoencoders (MAE), and models
trained on larger datasets such as CLIP. We find out-of-the-box, even today's
best models are not robust to common changes in pose, size, and background.
When some samples varied during training, we found models required a
significant portion of diversity to generalize -- though eventually robustness
did improve. When diversity is only seen for some classes however, we found
models did not generalize to other classes, unless the classes were very
similar to those seen varying during training. We hope our work will shed
further light on the blind spots of SoTA models and spur the development of
more robust vision models.
</p></li>
</ul>

<h3>Title: GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion Estimates. (arXiv:2210.13769v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13769">http://arxiv.org/abs/2210.13769</a></li>
<li>Code URL: <a href="https://github.com/globalflownet/globalflownet">https://github.com/globalflownet/globalflownet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13769] GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion Estimates](http://arxiv.org/abs/2210.13769)</code></li>
<li>Summary: <p>Videos shot by laymen using hand-held cameras contain undesirable shaky
motion. Estimating the global motion between successive frames, in a manner not
influenced by moving objects, is central to many video stabilization
techniques, but poses significant challenges. A large body of work uses 2D
affine transformations or homography for the global motion. However, in this
work, we introduce a more general representation scheme, which adapts any
existing optical flow network to ignore the moving objects and obtain a
spatially smooth approximation of the global motion between video frames. We
achieve this by a knowledge distillation approach, where we first introduce a
low pass filter module into the optical flow network to constrain the predicted
optical flow to be spatially smooth. This becomes our student network, named as
\textsc{GlobalFlowNet}. Then, using the original optical flow network as the
teacher network, we train the student network using a robust loss function.
Given a trained \textsc{GlobalFlowNet}, we stabilize videos using a two stage
process. In the first stage, we correct the instability in affine parameters
using a quadratic programming approach constrained by a user-specified cropping
limit to control loss of field of view. In the second stage, we stabilize the
video further by smoothing global motion parameters, expressed using a small
number of discrete cosine transform coefficients. In extensive experiments on a
variety of different videos, our technique outperforms state of the art
techniques in terms of subjective quality and different quantitative measures
of video stability. The source code is publicly available at
\href{https://github.com/GlobalFlowNet/GlobalFlowNet}{https://github.com/GlobalFlowNet/GlobalFlowNet}
</p></li>
</ul>

<h3>Title: Deep Boosting Robustness of DNN-based Image Watermarking via DBMark. (arXiv:2210.13801v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13801">http://arxiv.org/abs/2210.13801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13801] Deep Boosting Robustness of DNN-based Image Watermarking via DBMark](http://arxiv.org/abs/2210.13801)</code></li>
<li>Summary: <p>In this paper, we present DBMark, a new end-to-end digital image watermarking
framework to deep boost the robustness of DNN-based image watermarking. The key
novelty is the synergy of the Invertible Neural Networks(INNs) and effective
watermark features generation. The framework generates watermark features with
redundancy and error correction ability through message processing, synergized
with the powerful information embedding and extraction capabilities of
Invertible Neural Networks to achieve higher robustness and invisibility.
Extensive experiment results demonstrate the superiority of the proposed
framework compared with the state-of-the-art ones under various distortions.
</p></li>
</ul>

<h3>Title: A Novel Approach for Dimensionality Reduction and Classification of Hyperspectral Images based on Normalized Synergy. (arXiv:2210.13901v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13901">http://arxiv.org/abs/2210.13901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13901] A Novel Approach for Dimensionality Reduction and Classification of Hyperspectral Images based on Normalized Synergy](http://arxiv.org/abs/2210.13901)</code></li>
<li>Summary: <p>During the last decade, hyperspectral images have attracted increasing
interest from researchers worldwide. They provide more detailed information
about an observed area and allow an accurate target detection and precise
discrimination of objects compared to classical RGB and multispectral images.
Despite the great potentialities of hyperspectral technology, the analysis and
exploitation of the large volume data remain a challenging task. The existence
of irrelevant redundant and noisy images decreases the classification accuracy.
As a result, dimensionality reduction is a mandatory step in order to select a
minimal and effective images subset. In this paper, a new filter approach
normalized mutual synergy (NMS) is proposed in order to detect relevant bands
that are complementary in the class prediction better than the original
hyperspectral cube data. The algorithm consists of two steps: images selection
through normalized synergy information and pixel classification. The proposed
approach measures the discriminative power of the selected bands based on a
combination of their maximal normalized synergic information, minimum
redundancy and maximal mutual information with the ground truth. A comparative
study using the support vector machine (SVM) and k-nearest neighbor (KNN)
classifiers is conducted to evaluate the proposed approach compared to the
state of art band selection methods. Experimental results on three benchmark
hyperspectral images proposed by the NASA "Aviris Indiana Pine", "Salinas" and
"Pavia University" demonstrated the robustness, effectiveness and the
discriminative power of the proposed approach over the literature approaches.
</p></li>
</ul>

<p>Keywords: Hyperspectral images; target detection; pixel classification;
dimensionality reduction; band selection; information theory; mutual
information; normalized synergy
</p>

<h3>Title: Connective Reconstruction-based Novelty Detection. (arXiv:2210.13917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13917">http://arxiv.org/abs/2210.13917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13917] Connective Reconstruction-based Novelty Detection](http://arxiv.org/abs/2210.13917)</code></li>
<li>Summary: <p>Detection of out-of-distribution samples is one of the critical tasks for
real-world applications of computer vision. The advancement of deep learning
has enabled us to analyze real-world data which contain unexplained samples,
accentuating the need to detect out-of-distribution instances more than before.
GAN-based approaches have been widely used to address this problem due to their
ability to perform distribution fitting; however, they are accompanied by
training instability and mode collapse. We propose a simple yet efficient
reconstruction-based method that avoids adding complexities to compensate for
the limitations of GAN models while outperforming them. Unlike previous
reconstruction-based works that only utilize reconstruction error or generated
samples, our proposed method simultaneously incorporates both of them in the
detection task. Our model, which we call "Connective Novelty Detection" has two
subnetworks, an autoencoder, and a binary classifier. The autoencoder learns
the representation of the positive class by reconstructing them. Then, the
model creates negative and connected positive examples using real and generated
samples. Negative instances are generated via manipulating the real data, so
their distribution is close to the positive class to achieve a more accurate
boundary for the classifier. To boost the robustness of the detection to
reconstruction error, connected positive samples are created by combining the
real and generated samples. Finally, the binary classifier is trained using
connected positive and negative examples. We demonstrate a considerable
improvement in novelty detection over state-of-the-art methods on MNIST and
Caltech-256 datasets.
</p></li>
</ul>

<h3>Title: Attention Based Relation Network for Facial Action Units Recognition. (arXiv:2210.13988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13988">http://arxiv.org/abs/2210.13988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13988] Attention Based Relation Network for Facial Action Units Recognition](http://arxiv.org/abs/2210.13988)</code></li>
<li>Summary: <p>Facial action unit (AU) recognition is essential to facial expression
analysis. Since there are highly positive or negative correlations between AUs,
some existing AU recognition works have focused on modeling AU relations.
However, previous relationship-based approaches typically embed predefined
rules into their models and ignore the impact of various AU relations in
different crowds. In this paper, we propose a novel Attention Based Relation
Network (ABRNet) for AU recognition, which can automatically capture AU
relations without unnecessary or even disturbing predefined rules. ABRNet uses
several relation learning layers to automatically capture different AU
relations. The learned AU relation features are then fed into a self-attention
fusion module, which aims to refine individual AU features with attention
weights to enhance the feature robustness. Furthermore, we propose an AU
relation dropout strategy and AU relation loss (AUR-Loss) to better model AU
relations, which can further improve AU recognition. Extensive experiments show
that our approach achieves state-of-the-art performance on the DISFA and DISFA+
datasets.
</p></li>
</ul>

<h3>Title: A Comparative Study on Deep-Learning Methods for Dense Image Matching of Multi-angle and Multi-date Remote Sensing Stereo Images. (arXiv:2210.14031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14031">http://arxiv.org/abs/2210.14031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14031] A Comparative Study on Deep-Learning Methods for Dense Image Matching of Multi-angle and Multi-date Remote Sensing Stereo Images](http://arxiv.org/abs/2210.14031)</code></li>
<li>Summary: <p>Deep learning (DL) stereo matching methods gained great attention in remote
sensing satellite datasets. However, most of these existing studies conclude
assessments based only on a few/single stereo images lacking a systematic
evaluation on how robust DL methods are on satellite stereo images with varying
radiometric and geometric configurations. This paper provides an evaluation of
four DL stereo matching methods through hundreds of multi-date multi-site
satellite stereo pairs with varying geometric configurations, against the
traditional well-practiced Census-SGM (Semi-global matching), to
comprehensively understand their accuracy, robustness, generalization
capabilities, and their practical potential. The DL methods include a
learning-based cost metric through convolutional neural networks (MC-CNN)
followed by SGM, and three end-to-end (E2E) learning models using Geometry and
Context Network (GCNet), Pyramid Stereo Matching Network (PSMNet), and
LEAStereo. Our experiments show that E2E algorithms can achieve upper limits of
geometric accuracies, while may not generalize well for unseen data. The
learning-based cost metric and Census-SGM are rather robust and can
consistently achieve acceptable results. All DL algorithms are robust to
geometric configurations of stereo pairs and are less sensitive in comparison
to the Census-SGM, while learning-based cost metrics can generalize on
satellite images when trained on different datasets (airborne or ground-view).
</p></li>
</ul>

<h3>Title: Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation. (arXiv:2210.13459v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13459">http://arxiv.org/abs/2210.13459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13459] Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation](http://arxiv.org/abs/2210.13459)</code></li>
<li>Summary: <p>Overconfidence has been shown to impair generalization and calibration of a
neural network. Previous studies remedy this issue by adding a regularization
term to a loss function, preventing a model from making a peaked distribution.
Label smoothing smoothes target labels with a pre-defined prior label
distribution; as a result, a model is learned to maximize the likelihood of
predicting the soft label. Nonetheless, the amount of smoothing is the same in
all samples and remains fixed in training. In other words, label smoothing does
not reflect the change in probability distribution mapped by a model over the
course of training. To address this issue, we propose a regularization scheme
that brings dynamic nature into the smoothing parameter by taking model
probability distribution into account, thereby varying the parameter per
instance. A model in training self-regulates the extent of smoothing on the fly
during forward propagation. Furthermore, inspired by recent work in bridging
label smoothing and knowledge distillation, our work utilizes self-knowledge as
a prior label distribution in softening target labels, and presents theoretical
support for the regularization effect by knowledge distillation and the dynamic
smoothing parameter. Our regularizer is validated comprehensively, and the
result illustrates marked improvements in model generalization and calibration,
enhancing robustness and trustworthiness of a model.
</p></li>
</ul>

<h3>Title: ExPUNations: Augmenting Puns with Keywords and Explanations. (arXiv:2210.13513v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13513">http://arxiv.org/abs/2210.13513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13513] ExPUNations: Augmenting Puns with Keywords and Explanations](http://arxiv.org/abs/2210.13513)</code></li>
<li>Summary: <p>The tasks of humor understanding and generation are challenging and
subjective even for humans, requiring commonsense and real-world knowledge to
master. Puns, in particular, add the challenge of fusing that knowledge with
the ability to interpret lexical-semantic ambiguity. In this paper, we present
the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of
puns with detailed crowdsourced annotations of keywords denoting the most
distinctive words that make the text funny, pun explanations describing why the
text is funny, and fine-grained funniness ratings. This is the first humor
dataset with such extensive and fine-grained annotations specifically for puns.
Based on these annotations, we propose two tasks: explanation generation to aid
with pun classification and keyword-conditioned pun generation, to challenge
the current state-of-the-art natural language understanding and generation
models' ability to understand and generate humor. We showcase that the
annotated keywords we collect are helpful for generating better novel humorous
texts in human evaluation, and that our natural language explanations can be
leveraged to improve both the accuracy and robustness of humor classifiers.
</p></li>
</ul>

<h3>Title: Does Self-Rationalization Improve Robustness to Spurious Correlations?. (arXiv:2210.13575v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13575">http://arxiv.org/abs/2210.13575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13575] Does Self-Rationalization Improve Robustness to Spurious Correlations?](http://arxiv.org/abs/2210.13575)</code></li>
<li>Summary: <p>Rationalization is fundamental to human reasoning and learning. NLP models
trained to produce rationales along with predictions, called
self-rationalization models, have been investigated for their interpretability
and utility to end-users. However, the extent to which training with
human-written rationales facilitates learning remains an under-explored
question. We ask whether training models to self-rationalize can aid in their
learning to solve tasks for the right reasons. Specifically, we evaluate how
training self-rationalization models with free-text rationales affects
robustness to spurious correlations in fine-tuned encoder-decoder and
decoder-only models of six different sizes. We evaluate robustness to spurious
correlations by measuring performance on 1) manually annotated challenge
datasets and 2) subsets of original test sets where reliance on spurious
correlations would fail to produce correct answers. We find that while
self-rationalization can improve robustness to spurious correlations in
low-resource settings, it tends to hurt robustness in higher-resource settings.
Furthermore, these effects depend on model family and size, as well as on
rationale content. Together, our results suggest that explainability can come
at the cost of robustness; thus, appropriate care should be taken when training
self-rationalizing models with the goal of creating more trustworthy models.
</p></li>
</ul>

<h3>Title: AugCSE: Contrastive Sentence Embedding with Diverse Augmentations. (arXiv:2210.13749v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13749">http://arxiv.org/abs/2210.13749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13749] AugCSE: Contrastive Sentence Embedding with Diverse Augmentations](http://arxiv.org/abs/2210.13749)</code></li>
<li>Summary: <p>Data augmentation techniques have been proven useful in many applications in
NLP fields. Most augmentations are task-specific, and cannot be used as a
general-purpose tool. In our work, we present AugCSE, a unified framework to
utilize diverse sets of data augmentations to achieve a better, general
purpose, sentence embedding model. Building upon the latest sentence embedding
models, our approach uses a simple antagonistic discriminator that
differentiates the augmentation types. With the finetuning objective borrowed
from domain adaptation, we show that diverse augmentations, which often lead to
conflicting contrastive signals, can be tamed to produce a better and more
robust sentence representation. Our methods achieve state-of-the-art results on
downstream transfer tasks and perform competitively on semantic textual
similarity tasks, using only unsupervised data.
</p></li>
</ul>

<h3>Title: On the Robustness of Dataset Inference. (arXiv:2210.13631v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13631">http://arxiv.org/abs/2210.13631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13631] On the Robustness of Dataset Inference](http://arxiv.org/abs/2210.13631)</code></li>
<li>Summary: <p>Machine learning (ML) models are costly to train as they can require a
significant amount of data, computational resources and technical expertise.
Thus, they constitute valuable intellectual property that needs protection from
adversaries wanting to steal them. Ownership verification techniques allow the
victims of model stealing attacks to demonstrate that a suspect model was in
fact stolen from theirs. Although a number of ownership verification techniques
based on watermarking or fingerprinting have been proposed, most of them fall
short either in terms of security guarantees (well-equipped adversaries can
evade verification) or computational cost. A fingerprinting technique
introduced at ICLR '21, Dataset Inference (DI), has been shown to offer better
robustness and efficiency than prior methods. The authors of DI provided a
correctness proof for linear (suspect) models. However, in the same setting, we
prove that DI suffers from high false positives (FPs) -- it can incorrectly
identify an independent model trained with non-overlapping data from the same
distribution as stolen. We further prove that DI also triggers FPs in
realistic, non-linear suspect models. We then confirm empirically that DI leads
to FPs, with high confidence. Second, we show that DI also suffers from false
negatives (FNs) -- an adversary can fool DI by regularising a stolen model's
decision boundaries using adversarial training, thereby leading to an FN. To
this end, we demonstrate that DI fails to identify a model adversarially
trained from a stolen dataset -- the setting where DI is the hardest to evade.
Finally, we discuss the implications of our findings, the viability of
fingerprinting-based ownership verification in general, and suggest directions
for future work.
</p></li>
</ul>

<h3>Title: FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification. (arXiv:2210.13815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13815">http://arxiv.org/abs/2210.13815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13815] FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification](http://arxiv.org/abs/2210.13815)</code></li>
<li>Summary: <p>Recently, a lot of research attention has been devoted to exploring Web
security, a most representative topic is the adversarial robustness of graph
mining algorithms. Especially, a widely deployed adversarial attacks
formulation is the graph manipulation attacks by modifying the relational data
to mislead the Graph Neural Networks' (GNNs) predictions. Naturally, an
intrinsic question one would ask is whether we can accurately identify the
manipulations over graphs - we term this problem as poisoned graph sanitation.
In this paper, we present FocusedCleaner, a poisoned graph sanitation framework
consisting of two modules: bi-level structural learning and victim node
detection. In particular, the structural learning module will reserve the
attack process to steadily sanitize the graph while the detection module
provides the "focus" - a narrowed and more accurate search region - to
structural learning. These two modules will operate in iterations and reinforce
each other to sanitize a poisoned graph step by step. Extensive experiments
demonstrate that FocusedCleaner outperforms the state-of-the-art baselines both
on poisoned graph sanitation and improving robustness.
</p></li>
</ul>

<h3>Title: Machine learning-based approach for online fault Diagnosis of Discrete Event System. (arXiv:2210.13466v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13466">http://arxiv.org/abs/2210.13466</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13466] Machine learning-based approach for online fault Diagnosis of Discrete Event System](http://arxiv.org/abs/2210.13466)</code></li>
<li>Summary: <p>The problem considered in this paper is the online diagnosis of Automated
Production Systems with sensors and actuators delivering discrete binary
signals that can be modeled as Discrete Event Systems. Even though there are
numerous diagnosis methods, none of them can meet all the criteria of
implementing an efficient diagnosis system (such as an intelligent solution, an
average effort, a reasonable cost, an online diagnosis, fewer false alarms,
etc.). In addition, these techniques require either a correct, robust, and
representative model of the system or relevant data or experts' knowledge that
require continuous updates. In this paper, we propose a Machine Learning-based
approach of a diagnostic system. It is considered as a multi-class classifier
that predicts the plant state: normal or faulty and what fault that has arisen
in the case of failing behavior.
</p></li>
</ul>

<h3>Title: Sharpness-aware Minimization for Worst Case Optimization. (arXiv:2210.13533v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13533">http://arxiv.org/abs/2210.13533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13533] Sharpness-aware Minimization for Worst Case Optimization](http://arxiv.org/abs/2210.13533)</code></li>
<li>Summary: <p>Improvement of worst group performance and generalization performance are
core problems of current machine learning. There are diverse efforts to
increase performance, such as weight norm penalty and data augmentation, but
the improvements are limited. Recently, there have been two promising
approaches to increase the worst group performance and generalization
performance, respectively. Distributionally robust optimization (DRO) focuses
on the worst or hardest group to improve the worst-group performance. Besides,
sharpness-aware minimization (SAM) finds the flat minima to increase the
generalization ability on an unseen dataset. They show significant performance
improvements on the worst-group dataset and unseen dataset, respectively.
However, DRO does not guarantee flatness, and SAM does not guarantee the worst
group performance improvement. In other words, DRO and SAM may fail to increase
the worst group performance when the training and test dataset shift occurs. In
this study, we propose a new approach, the sharpness-aware group
distributionally robust optimization (SGDRO). SGDRO finds the flat-minima that
generalizes well on the worst group dataset. Different from DRO and SAM, SGDRO
contributes to improving the generalization ability even the distribution shift
occurs. We validate that SGDRO shows the smaller maximum eigenvalue and
improved performance in the worst group.
</p></li>
</ul>

<h3>Title: Embodied, Situated, and Grounded Intelligence: Implications for AI. (arXiv:2210.13589v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13589">http://arxiv.org/abs/2210.13589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13589] Embodied, Situated, and Grounded Intelligence: Implications for AI](http://arxiv.org/abs/2210.13589)</code></li>
<li>Summary: <p>In April of 2022, the Santa Fe Institute hosted a workshop on embodied,
situated, and grounded intelligence as part of the Institute's Foundations of
Intelligence project. The workshop brought together computer scientists,
psychologists, philosophers, social scientists, and others to discuss the
science of embodiment and related issues in human intelligence, and its
implications for building robust, human-level AI. In this report, we summarize
each of the talks and the subsequent discussions. We also draw out a number of
key themes and identify important frontiers for future research.
</p></li>
</ul>

<h3>Title: Aboveground carbon biomass estimate with Physics-informed deep network. (arXiv:2210.13752v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13752">http://arxiv.org/abs/2210.13752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13752] Aboveground carbon biomass estimate with Physics-informed deep network](http://arxiv.org/abs/2210.13752)</code></li>
<li>Summary: <p>The global carbon cycle is a key process to understand how our climate is
changing. However, monitoring the dynamics is difficult because a
high-resolution robust measurement of key state parameters including the
aboveground carbon biomass (AGB) is required. Here, we use deep neural network
to generate a wall-to-wall map of AGB within the Continental USA (CONUS) with
30-meter spatial resolution for the year 2021. We combine radar and optical
hyperspectral imagery, with a physical climate parameter of SIF-based GPP.
Validation results show that a masked variation of UNet has the lowest
validation RMSE of 37.93 $\pm$ 1.36 Mg C/ha, as compared to 52.30 $\pm$ 0.03 Mg
C/ha for random forest algorithm. Furthermore, models that learn from SIF-based
GPP in addition to radar and optical imagery reduce validation RMSE by almost
10% and the standard deviation by 40%. Finally, we apply our model to measure
losses in AGB from the recent 2021 Caldor wildfire in California, and validate
our analysis with Sentinel-based burn index.
</p></li>
</ul>

<h3>Title: Line Graph Contrastive Learning for Link Prediction. (arXiv:2210.13795v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13795">http://arxiv.org/abs/2210.13795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13795] Line Graph Contrastive Learning for Link Prediction](http://arxiv.org/abs/2210.13795)</code></li>
<li>Summary: <p>Link prediction task aims to predict the connection of two nodes in the
network. Existing works mainly predict links by node pairs similarity
measurements. However, if the local structure doesn't meet such measurement
assumption, the algorithms' performance will deteriorate rapidly. To overcome
these limitations, we propose a Line Graph Contrastive Learning (LGCL) method
to obtain multiview information. Our framework obtains a subgraph view by h-hop
subgraph sampling with target node pairs as the center. After transforming the
sampled subgraph into a line graph, the edge embedding information is directly
accessible, and the link prediction task is converted into a node
classification task. Then, different graph convolution operators learn
representations from double perspectives. Finally, contrastive learning is
adopted to balance the subgraph representations of these perspectives via
maximizing mutual information. With experiments on six public datasets, LGCL
outperforms current benchmarks on link prediction tasks and shows better
generalization performance and robustness.
</p></li>
</ul>

<h3>Title: Multi-Fidelity Bayesian Optimization with Unreliable Information Sources. (arXiv:2210.13937v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13937">http://arxiv.org/abs/2210.13937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13937] Multi-Fidelity Bayesian Optimization with Unreliable Information Sources](http://arxiv.org/abs/2210.13937)</code></li>
<li>Summary: <p>Bayesian optimization (BO) is a powerful framework for optimizing black-box,
expensive-to-evaluate functions. Over the past decade, many algorithms have
been proposed to integrate cheaper, lower-fidelity approximations of the
objective function into the optimization process, with the goal of converging
towards the global optimum at a reduced cost. This task is generally referred
to as multi-fidelity Bayesian optimization (MFBO). However, MFBO algorithms can
lead to higher optimization costs than their vanilla BO counterparts,
especially when the low-fidelity sources are poor approximations of the
objective function, therefore defeating their purpose. To address this issue,
we propose rMFBO (robust MFBO), a methodology to make any GP-based MFBO scheme
robust to the addition of unreliable information sources. rMFBO comes with a
theoretical guarantee that its performance can be bound to its vanilla BO
analog, with high controllable probability. We demonstrate the effectiveness of
the proposed methodology on a number of numerical benchmarks, outperforming
earlier MFBO methods on unreliable sources. We expect rMFBO to be particularly
useful to reliably include human experts with varying knowledge within BO
processes.
</p></li>
</ul>

<h3>Title: Gradient-based Weight Density Balancing for Robust Dynamic Sparse Training. (arXiv:2210.14012v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14012">http://arxiv.org/abs/2210.14012</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14012] Gradient-based Weight Density Balancing for Robust Dynamic Sparse Training](http://arxiv.org/abs/2210.14012)</code></li>
<li>Summary: <p>Training a sparse neural network from scratch requires optimizing connections
at the same time as the weights themselves. Typically, the weights are
redistributed after a predefined number of weight updates, removing a fraction
of the parameters of each layer and inserting them at different locations in
the same layers. The density of each layer is determined using heuristics,
often purely based on the size of the parameter tensor. While the connections
per layer are optimized multiple times during training, the density of each
layer typically remains constant. This leaves great unrealized potential,
especially in scenarios with a high sparsity of 90% and more. We propose Global
Gradient-based Redistribution, a technique which distributes weights across all
layers - adding more weights to the layers that need them most. Our evaluation
shows that our approach is less prone to unbalanced weight distribution at
initialization than previous work and that it is able to find better performing
sparse subnetworks at very high sparsity levels.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks. (arXiv:2210.13826v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13826">http://arxiv.org/abs/2210.13826</a></li>
<li>Code URL: <a href="https://github.com/lizhaoliu-lec/ccse">https://github.com/lizhaoliu-lec/ccse</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13826] Instance Segmentation for Chinese Character Stroke Extraction, Datasets and Benchmarks](http://arxiv.org/abs/2210.13826)</code></li>
<li>Summary: <p>Stroke is the basic element of Chinese character and stroke extraction has
been an important and long-standing endeavor. Existing stroke extraction
methods are often handcrafted and highly depend on domain expertise due to the
limited training data. Moreover, there are no standardized benchmarks to
provide a fair comparison between different stroke extraction methods, which,
we believe, is a major impediment to the development of Chinese character
stroke understanding and related tasks. In this work, we present the first
public available Chinese Character Stroke Extraction (CCSE) benchmark, with two
new large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).
With the large-scale datasets, we hope to leverage the representation power of
deep models such as CNNs to solve the stroke extraction task, which, however,
remains an open question. To this end, we turn the stroke extraction problem
into a stroke instance segmentation problem. Using the proposed datasets to
train a stroke instance segmentation model, we surpass previous methods by a
large margin. Moreover, the models trained with the proposed datasets benefit
the downstream font generation and handwritten aesthetic assessment tasks. We
hope these benchmark results can facilitate further research. The source code
and datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.
</p></li>
</ul>

<h3>Title: THOR-Net: End-to-end Graformer-based Realistic Two Hands and Object Reconstruction with Self-supervision. (arXiv:2210.13853v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13853">http://arxiv.org/abs/2210.13853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13853] THOR-Net: End-to-end Graformer-based Realistic Two Hands and Object Reconstruction with Self-supervision](http://arxiv.org/abs/2210.13853)</code></li>
<li>Summary: <p>Realistic reconstruction of two hands interacting with objects is a new and
challenging problem that is essential for building personalized Virtual and
Augmented Reality environments. Graph Convolutional networks (GCNs) allow for
the preservation of the topologies of hands poses and shapes by modeling them
as a graph. In this work, we propose the THOR-Net which combines the power of
GCNs, Transformer, and self-supervision to realistically reconstruct two hands
and an object from a single RGB image. Our network comprises two stages; namely
the features extraction stage and the reconstruction stage. In the features
extraction stage, a Keypoint RCNN is used to extract 2D poses, features maps,
heatmaps, and bounding boxes from a monocular RGB image. Thereafter, this 2D
information is modeled as two graphs and passed to the two branches of the
reconstruction stage. The shape reconstruction branch estimates meshes of two
hands and an object using our novel coarse-to-fine GraFormer shape network. The
3D poses of the hands and objects are reconstructed by the other branch using a
GraFormer network. Finally, a self-supervised photometric loss is used to
directly regress the realistic textured of each vertex in the hands' meshes.
Our approach achieves State-of-the-art results in Hand shape estimation on the
HO-3D dataset (10.0mm) exceeding ArtiBoost (10.8mm). It also surpasses other
methods in hand pose estimation on the challenging two hands and object (H2O)
dataset by 5mm on the left-hand pose and 1 mm on the right-hand pose.
</p></li>
</ul>

<h3>Title: Better Few-Shot Relation Extraction with Label Prompt Dropout. (arXiv:2210.13733v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13733">http://arxiv.org/abs/2210.13733</a></li>
<li>Code URL: <a href="https://github.com/jzhang38/lpd">https://github.com/jzhang38/lpd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13733] Better Few-Shot Relation Extraction with Label Prompt Dropout](http://arxiv.org/abs/2210.13733)</code></li>
<li>Summary: <p>Few-shot relation extraction aims to learn to identify the relation between
two entities based on very limited training examples. Recent efforts found that
textual labels (i.e., relation names and relation descriptions) could be
extremely useful for learning class representations, which will benefit the
few-shot learning task. However, what is the best way to leverage such label
information in the learning process is an important research question. Existing
works largely assume such textual labels are always present during both
learning and prediction. In this work, we argue that such approaches may not
always lead to optimal results. Instead, we present a novel approach called
label prompt dropout, which randomly removes label descriptions in the learning
process. Our experiments show that our approach is able to lead to improved
class representations, yielding significantly better results on the few-shot
relation extraction task.
</p></li>
</ul>

<h3>Title: IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models. (arXiv:2210.14128v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14128">http://arxiv.org/abs/2210.14128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14128] IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models](http://arxiv.org/abs/2210.14128)</code></li>
<li>Summary: <p>We introduce a new open information extraction (OIE) benchmark for
pre-trained language models (LM). Recent studies have demonstrated that
pre-trained LMs, such as BERT and GPT, may store linguistic and relational
knowledge. In particular, LMs are able to answer ``fill-in-the-blank''
questions when given a pre-defined relation category. Instead of focusing on
pre-defined relations, we create an OIE benchmark aiming to fully examine the
open relational information present in the pre-trained LMs. We accomplish this
by turning pre-trained LMs into zero-shot OIE systems. Surprisingly,
pre-trained LMs are able to obtain competitive performance on both standard OIE
datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets
(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For
instance, the zero-shot pre-trained LMs outperform the F1 score of the
state-of-the-art supervised OIE methods on our factual OIE datasets without
needing to use any training sets. Our code and datasets are available at
https://github.com/cgraywang/IELM
</p></li>
</ul>

<h3>Title: CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization. (arXiv:2210.14190v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14190">http://arxiv.org/abs/2210.14190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14190] CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and Summarization](http://arxiv.org/abs/2210.14190)</code></li>
<li>Summary: <p>Social media has increasingly played a key role in emergency response: first
responders can use public posts to better react to ongoing crisis events and
deploy the necessary resources where they are most needed. Timeline extraction
and abstractive summarization are critical technical tasks to leverage large
numbers of social media posts about events. Unfortunately, there are few
datasets for benchmarking technical approaches for those tasks. This paper
presents CrisisLTLSum, the largest dataset of local crisis event timelines
available to date. CrisisLTLSum contains 1,000 crisis event timelines across
four domains: wildfires, local fires, traffic, and storms. We built
CrisisLTLSum using a semi-automated cluster-then-refine approach to collect
data from the public Twitter stream. Our initial experiments indicate a
significant gap between the performance of strong baselines compared to the
human performance on both tasks. Our dataset, code, and models are publicly
available.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: SUPR: A Sparse Unified Part-Based Human Representation. (arXiv:2210.13861v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13861">http://arxiv.org/abs/2210.13861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13861] SUPR: A Sparse Unified Part-Based Human Representation](http://arxiv.org/abs/2210.13861)</code></li>
<li>Summary: <p>Statistical 3D shape models of the head, hands, and fullbody are widely used
in computer vision and graphics. Despite their wide use, we show that existing
models of the head and hands fail to capture the full range of motion for these
parts. Moreover, existing work largely ignores the feet, which are crucial for
modeling human movement and have applications in biomechanics, animation, and
the footwear industry. The problem is that previous body part models are
trained using 3D scans that are isolated to the individual parts. Such data
does not capture the full range of motion for such parts, e.g. the motion of
head relative to the neck. Our observation is that full-body scans provide
important information about the motion of the body parts. Consequently, we
propose a new learning scheme that jointly trains a full-body model and
specific part models using a federated dataset of full-body and body-part
scans. Specifically, we train an expressive human body model called SUPR
(Sparse Unified Part-Based Human Representation), where each joint strictly
influences a sparse set of model vertices. The factorized representation
enables separating SUPR into an entire suite of body part models. Note that the
feet have received little attention and existing 3D body models have highly
under-actuated feet. Using novel 4D scans of feet, we train a model with an
extended kinematic tree that captures the range of motion of the toes.
Additionally, feet deform due to ground contact. To model this, we include a
novel non-linear deformation function that predicts foot deformation
conditioned on the foot pose, shape, and ground contact. We train SUPR on an
unprecedented number of scans: 1.2 million body, head, hand and foot scans. We
quantitatively compare SUPR and the separated body parts and find that our
suite of models generalizes better than existing models. SUPR is available at
<a href="http://supr.is.tue.mpg.de">this http URL</a>
</p></li>
</ul>

<h3>Title: Subspace Recovery from Heterogeneous Data with Non-isotropic Noise. (arXiv:2210.13497v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13497">http://arxiv.org/abs/2210.13497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13497] Subspace Recovery from Heterogeneous Data with Non-isotropic Noise](http://arxiv.org/abs/2210.13497)</code></li>
<li>Summary: <p>Recovering linear subspaces from data is a fundamental and important task in
statistics and machine learning. Motivated by heterogeneity in Federated
Learning settings, we study a basic formulation of this problem: the principal
component analysis (PCA), with a focus on dealing with irregular noise. Our
data come from $n$ users with user $i$ contributing data samples from a
$d$-dimensional distribution with mean $\mu_i$. Our goal is to recover the
linear subspace shared by $\mu_1,\ldots,\mu_n$ using the data points from all
users, where every data point from user $i$ is formed by adding an independent
mean-zero noise vector to $\mu_i$. If we only have one data point from every
user, subspace recovery is information-theoretically impossible when the
covariance matrices of the noise vectors can be non-spherical, necessitating
additional restrictive assumptions in previous work. We avoid these assumptions
by leveraging at least two data points from each user, which allows us to
design an efficiently-computable estimator under non-spherical and
user-dependent noise. We prove an upper bound for the estimation error of our
estimator in general scenarios where the number of data points and amount of
noise can vary across users, and prove an information-theoretic error lower
bound that not only matches the upper bound up to a constant factor, but also
holds even for spherical Gaussian noise. This implies that our estimator does
not introduce additional estimation error (up to a constant factor) due to
irregularity in the noise. We show additional results for a linear regression
problem in a similar setup.
</p></li>
</ul>

<h3>Title: FedGRec: Federated Graph Recommender System with Lazy Update of Latent Embeddings. (arXiv:2210.13686v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13686">http://arxiv.org/abs/2210.13686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13686] FedGRec: Federated Graph Recommender System with Lazy Update of Latent Embeddings](http://arxiv.org/abs/2210.13686)</code></li>
<li>Summary: <p>Recommender systems are widely used in industry to improve user experience.
Despite great success, they have recently been criticized for collecting
private user data. Federated Learning (FL) is a new paradigm for learning on
distributed data without direct data sharing. Therefore, Federated Recommender
(FedRec) systems are proposed to mitigate privacy concerns to non-distributed
recommender systems. However, FedRec systems have a performance gap to its
non-distributed counterpart. The main reason is that local clients have an
incomplete user-item interaction graph, thus FedRec systems cannot utilize
indirect user-item interactions well. In this paper, we propose the Federated
Graph Recommender System (FedGRec) to mitigate this gap. Our FedGRec system can
effectively exploit the indirect user-item interactions. More precisely, in our
system, users and the server explicitly store latent embeddings for users and
items, where the latent embeddings summarize different orders of indirect
user-item interactions and are used as a proxy of missing interaction graph
during local training. We perform extensive empirical evaluations to verify the
efficacy of using latent embeddings as a proxy of missing interaction graph;
the experimental results show superior performance of our system compared to
various baselines. A short version of the paper is presented in
\href{https://federated-learning.org/fl-neurips-2022/}{the FL-NeurIPS'22
workshop}.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher Mixture Model. (arXiv:2210.13664v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13664">http://arxiv.org/abs/2210.13664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13664] Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher Mixture Model](http://arxiv.org/abs/2210.13664)</code></li>
<li>Summary: <p>In spite of the high performance and reliability of deep learning algorithms
in a wide range of everyday applications, many investigations tend to show that
a lot of models exhibit biases, discriminating against specific subgroups of
the population (e.g. gender, ethnicity). This urges the practitioner to develop
fair systems with a uniform/comparable performance across sensitive groups. In
this work, we investigate the gender bias of deep Face Recognition networks. In
order to measure this bias, we introduce two new metrics, $\mathrm{BFAR}$ and
$\mathrm{BFRR}$, that better reflect the inherent deployment needs of Face
Recognition systems. Motivated by geometric considerations, we mitigate gender
bias through a new post-processing methodology which transforms the deep
embeddings of a pre-trained model to give more representation power to
discriminated subgroups. It consists in training a shallow neural network by
minimizing a Fair von Mises-Fisher loss whose hyperparameters account for the
intra-class variance of each gender. Interestingly, we empirically observe that
these hyperparameters are correlated with our fairness metrics. In fact,
extensive numerical experiments on a variety of datasets show that a careful
selection significantly reduces gender bias.
</p></li>
</ul>

<h3>Title: Energy Pricing in P2P Energy Systems Using Reinforcement Learning. (arXiv:2210.13555v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13555">http://arxiv.org/abs/2210.13555</a></li>
<li>Code URL: <a href="https://github.com/artifitialleap-mbzuai/rl-p2p-price-prediction">https://github.com/artifitialleap-mbzuai/rl-p2p-price-prediction</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13555] Energy Pricing in P2P Energy Systems Using Reinforcement Learning](http://arxiv.org/abs/2210.13555)</code></li>
<li>Summary: <p>The increase in renewable energy on the consumer side gives place to new
dynamics in the energy grids. Participants in a microgrid can produce energy
and trade it with their peers (peer-to-peer) with the permission of the energy
provider. In such a scenario, the stochastic nature of distributed renewable
energy generators and energy consumption increases the complexity of defining
fair prices for buying and selling energy. In this study, we introduce a
reinforcement learning framework to help solve this issue by training an agent
to set the prices that maximize the profit of all components in the microgrid,
aiming to facilitate the implementation of P2P grids in real-life scenarios.
The microgrid considers consumers, prosumers, the service provider, and a
community battery. Experimental results on the \textit{Pymgrid} dataset show a
successful approach to price optimization for all components in the microgrid.
The proposed framework ensures flexibility to account for the interest of these
components, as well as the ratio of consumers and prosumers in the microgrid.
The results also examine the effect of changing the capacity of the community
battery on the profit of the system. The implementation code is available
\href{https://github.com/Artifitialleap-MBZUAI/rl-p2p-price-prediction}{here}.
</p></li>
</ul>

<h3>Title: I Prefer not to Say: Operationalizing Fair and User-guided Data Minimization. (arXiv:2210.13954v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13954">http://arxiv.org/abs/2210.13954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13954] I Prefer not to Say: Operationalizing Fair and User-guided Data Minimization](http://arxiv.org/abs/2210.13954)</code></li>
<li>Summary: <p>To grant users greater authority over their personal data, policymakers have
suggested tighter data protection regulations (e.g., GDPR, CCPA). One key
principle within these regulations is data minimization, which urges companies
and institutions to only collect data that is relevant and adequate for the
purpose of the data analysis. In this work, we take a user-centric perspective
on this regulation, and let individual users decide which data they deem
adequate and relevant to be processed by a machine-learned model. We require
that users who decide to provide optional information should appropriately
benefit from sharing their data, while users who rely on the mandate to leave
their data undisclosed should not be penalized for doing so. This gives rise to
the overlooked problem of fair treatment between individuals providing
additional information and those choosing not to. While the classical fairness
literature focuses on fair treatment between advantaged and disadvantaged
groups, an initial look at this problem through the lens of classical fairness
notions reveals that they are incompatible with these desiderata. We offer a
solution to this problem by proposing the notion of Optional Feature Fairness
(OFF) that follows from our requirements. To operationalize OFF, we derive a
multi-model strategy and a tractable logistic regression model. We analyze the
effect and the cost of applying OFF on several real-world data sets.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Geo-SIC: Learning Deformable Geometric Shapes in Deep Image Classifiers. (arXiv:2210.13704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13704">http://arxiv.org/abs/2210.13704</a></li>
<li>Code URL: <a href="https://github.com/jw4hv/geo-sic">https://github.com/jw4hv/geo-sic</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13704] Geo-SIC: Learning Deformable Geometric Shapes in Deep Image Classifiers](http://arxiv.org/abs/2210.13704)</code></li>
<li>Summary: <p>Deformable shapes provide important and complex geometric features of objects
presented in images. However, such information is oftentimes missing or
underutilized as implicit knowledge in many image analysis tasks. This paper
presents Geo-SIC, the first deep learning model to learn deformable shapes in a
deformation space for an improved performance of image classification. We
introduce a newly designed framework that (i) simultaneously derives features
from both image and latent shape spaces with large intra-class variations; and
(ii) gains increased model interpretability by allowing direct access to the
underlying geometric features of image data. In particular, we develop a
boosted classification network, equipped with an unsupervised learning of
geometric shape representations characterized by diffeomorphic transformations
within each class. In contrast to previous approaches using pre-extracted
shapes, our model provides a more fundamental approach by naturally learning
the most relevant shape features jointly with an image classifier. We
demonstrate the effectiveness of our method on both simulated 2D images and
real 3D brain magnetic resonance (MR) images. Experimental results show that
our model substantially improves the image classification accuracy with an
additional benefit of increased model interpretability. Our code is publicly
available at https://github.com/jw4hv/Geo-SIC
</p></li>
</ul>

<h3>Title: Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts. (arXiv:2210.13836v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.13836">http://arxiv.org/abs/2210.13836</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.13836] Deconfounding Legal Judgment Prediction for European Court of Human Rights Cases Towards Better Alignment with Experts](http://arxiv.org/abs/2210.13836)</code></li>
<li>Summary: <p>This work demonstrates that Legal Judgement Prediction systems without
expert-informed adjustments can be vulnerable to shallow, distracting surface
signals that arise from corpus construction, case distribution, and confounding
factors. To mitigate this, we use domain expertise to strategically identify
statistically predictive but legally irrelevant information. We adopt
adversarial training to prevent the system from relying on it. We evaluate our
deconfounded models by employing interpretability techniques and comparing to
expert annotations. Quantitative experiments and qualitative analysis show that
our deconfounded model consistently aligns better with expert rationales than
baselines trained for prediction only. We further contribute a set of reference
expert annotations to the validation and testing partitions of an existing
benchmark dataset of European Court of Human Rights cases.
</p></li>
</ul>

<h3>Title: Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14037">http://arxiv.org/abs/2210.14037</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14037] Revisiting Softmax for Uncertainty Approximation in Text Classification](http://arxiv.org/abs/2210.14037)</code></li>
<li>Summary: <p>Uncertainty approximation in text classification is an important area with
applications in domain adaptation and interpretability. The most widely used
uncertainty approximation method is Monte Carlo Dropout, which is
computationally expensive as it requires multiple forward passes through the
model. A cheaper alternative is to simply use a softmax to estimate model
uncertainty. However, prior work has indicated that the softmax can generate
overconfident uncertainty estimates and can thus be tricked into producing
incorrect predictions. In this paper, we perform a thorough empirical analysis
of both methods on five datasets with two base neural architectures in order to
reveal insight into the trade-offs between the two. We compare the methods'
uncertainty approximations and downstream text classification performance,
while weighing their performance against their computational complexity as a
cost-benefit analysis, by measuring runtime (cost) and the downstream
performance (benefit). We find that, while Monte Carlo produces the best
uncertainty approximations, using a simple softmax leads to competitive
uncertainty estimation for text classification at a much lower computational
cost, suggesting that softmax can in fact be a sufficient uncertainty estimate
when computational resources are a concern.
</p></li>
</ul>

<h3>Title: Towards Interpretable Summary Evaluation via Allocation of Contextual Embeddings to Reference Text Topics. (arXiv:2210.14174v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14174">http://arxiv.org/abs/2210.14174</a></li>
<li>Code URL: <a href="https://github.com/ibm/misem">https://github.com/ibm/misem</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14174] Towards Interpretable Summary Evaluation via Allocation of Contextual Embeddings to Reference Text Topics](http://arxiv.org/abs/2210.14174)</code></li>
<li>Summary: <p>Despite extensive recent advances in summary generation models, evaluation of
auto-generated summaries still widely relies on single-score systems
insufficient for transparent assessment and in-depth qualitative analysis.
Towards bridging this gap, we propose the multifaceted interpretable summary
evaluation method (MISEM), which is based on allocation of a summary's
contextual token embeddings to semantic topics identified in the reference
text. We further contribute an interpretability toolbox for automated summary
evaluation and interactive visual analysis of summary scoring, topic
identification, and token-topic allocation. MISEM achieves a promising .404
Pearson correlation with human judgment on the TAC'08 dataset.
</p></li>
</ul>

<h3>Title: Influence Functions for Sequence Tagging Models. (arXiv:2210.14177v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.14177">http://arxiv.org/abs/2210.14177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.14177] Influence Functions for Sequence Tagging Models](http://arxiv.org/abs/2210.14177)</code></li>
<li>Summary: <p>Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,
and Semantic Role Labeling) are naturally framed as sequence tagging problems.
However, there has been comparatively little work on interpretability methods
for sequence tagging models. In this paper, we extend influence functions -
which aim to trace predictions back to the training points that informed them -
to sequence tagging tasks. We define the influence of a training instance
segment as the effect that perturbing the labels within this segment has on a
test segment level prediction. We provide an efficient approximation to compute
this, and show that it tracks with the true segment influence, measured
empirically. We show the practical utility of segment influence by using the
method to identify systematic annotation errors in two named entity recognition
corpora. Code to reproduce our results is available at
https://github.com/successar/Segment_Influence_Functions.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
