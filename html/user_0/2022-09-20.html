<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Proofs of Proof-of-Stake with Sublinear Complexity. (arXiv:2209.08673v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08673">http://arxiv.org/abs/2209.08673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08673] Proofs of Proof-of-Stake with Sublinear Complexity](http://arxiv.org/abs/2209.08673)</code></li>
<li>Summary: <p>Popular Ethereum wallets (e.g., MetaMask) entrust centralized infrastructure
providers (e.g., Infura) to run the consensus client logic on their behalf. As
a result, these wallets are light-weight and high-performant, but come with
security risks. A malicious provider can completely mislead the wallet, e.g.,
fake payments and balances, or censor transactions. On the other hand, light
clients, which are not in popular use today, allow decentralization, but at
inefficient linear bootstrapping complexity. This poses a dilemma between
decentralization and performance. In this paper, we design, implement, and
evaluate a new proof-of-stake (PoS) superlight client with logarithmic
bootstrapping complexity. Our key insight is to leverage the standard
existential honesty assumption, i.e., that the verifier (client) is connected
to at least one honest prover (full node). The proofs of PoS take the form of a
Merkle tree of PoS epochs. The verifier enrolls the provers in a bisection
game, in which the honest prover is destined to win once an adversarial Merkle
tree is challenged at sufficient depth. We implement a complete client that is
compatible with mainnet PoS Ethereum to evaluate our construction: compared to
the current light client construction proposed for PoS Ethereum, our client
improves time-to-completion by 9x, communication by 180x, and energy usage by
30x. We prove our construction secure and show how to employ it for other
proof-of-stake systems such as Cardano, Algorand, and Snow White.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Et tu, Blockchain? Outsmarting Smart Contracts via Social Engineering. (arXiv:2209.08356v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08356">http://arxiv.org/abs/2209.08356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08356] Et tu, Blockchain? Outsmarting Smart Contracts via Social Engineering](http://arxiv.org/abs/2209.08356)</code></li>
<li>Summary: <p>We reveal six zero-day social engineering attacks in Ethereum, and subdivide
them into two classes: Address Manipulation and Homograph. We demonstrate the
attacks by embedding them in source codes of five popular smart contracts with
combined market capitalization of over \$29 billion, and show that the attacks
have the ability to remain dormant during the testing phase and activate only
after production deployment. We analyze 85,656 open source smart contracts and
find 1,027 contracts that can be directly used for performing social
engineering attacks. For responsible disclosure, we contact seven smart
contract security firms. In the spirit of open research, we make the source
codes of the attack benchmark, tools, and datasets available to the public.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Hierarchical fuzzy neural networks with privacy preservation for heterogeneous big data. (arXiv:2209.08467v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08467">http://arxiv.org/abs/2209.08467</a></li>
<li>Code URL: <a href="https://github.com/leijiezhang/pp_hfnn">https://github.com/leijiezhang/pp_hfnn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08467] Hierarchical fuzzy neural networks with privacy preservation for heterogeneous big data](http://arxiv.org/abs/2209.08467)</code></li>
<li>Summary: <p>Heterogeneous big data poses many challenges in machine learning. Its
enormous scale, high dimensionality, and inherent uncertainty make almost every
aspect of machine learning difficult, from providing enough processing power to
maintaining model accuracy to protecting privacy. However, perhaps the most
imposing problem is that big data is often interspersed with sensitive personal
data. Hence, we propose a privacy-preserving hierarchical fuzzy neural network
(PP-HFNN) to address these technical challenges while also alleviating privacy
concerns. The network is trained with a two-stage optimization algorithm, and
the parameters at low levels of the hierarchy are learned with a scheme based
on the well-known alternating direction method of multipliers, which does not
reveal local data to other agents. Coordination at high levels of the hierarchy
is handled by the alternating optimization method, which converges very
quickly. The entire training procedure is scalable, fast and does not suffer
from gradient vanishing problems like the methods based on back-propagation.
Comprehensive simulations conducted on both regression and classification tasks
demonstrate the effectiveness of the proposed model.
</p></li>
</ul>

<h3>Title: On PAC Learning Halfspaces in Non-interactive Local Privacy Model with Public Unlabeled Data. (arXiv:2209.08319v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08319">http://arxiv.org/abs/2209.08319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08319] On PAC Learning Halfspaces in Non-interactive Local Privacy Model with Public Unlabeled Data](http://arxiv.org/abs/2209.08319)</code></li>
<li>Summary: <p>In this paper, we study the problem of PAC learning halfspaces in the
non-interactive local differential privacy model (NLDP). To breach the barrier
of exponential sample complexity, previous results studied a relaxed setting
where the server has access to some additional public but unlabeled data. We
continue in this direction. Specifically, we consider the problem under the
standard setting instead of the large margin setting studied before. Under
different mild assumptions on the underlying data distribution, we propose two
approaches that are based on the Massart noise model and self-supervised
learning and show that it is possible to achieve sample complexities that are
only linear in the dimension and polynomial in other terms for both private and
public data, which significantly improve the previous results. Our methods
could also be used for other private PAC learning problems.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Active Defense Analysis of Blockchain Forking through the Spatial-Temporal Lens. (arXiv:2209.08463v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08463">http://arxiv.org/abs/2209.08463</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08463] Active Defense Analysis of Blockchain Forking through the Spatial-Temporal Lens](http://arxiv.org/abs/2209.08463)</code></li>
<li>Summary: <p>Forking breaches the security and performance of blockchain as it is
symptomatic of distributed consensus, spurring wide interest in analyzing and
resolving it. The state-of-the-art works can be categorized into two kinds:
experiment-based and model-based. However, the former falls short in
exclusiveness since the derived observations are scenario-specific. Hence, it
is problematic to abstractly reveal the crystal-clear forking laws. Besides,
the models established in the latter are spatiality-free, which totally
overlook the fact that forking is essentially an undesirable result under a
given topology. Moreover, few of the ongoing studies have yielded to the active
defense mechanisms but only recognized forking passively, which impedes forking
prevention and cannot deter it at the source. In this paper, we fill the gap by
carrying out the active defense analysis of blockchain forking from the
spatial-temporal dimension. Our work is featured by the following two traits:
1) dual dimensions. We consider the spatiality of blockchain overlay network
besides temporal characteristics, based on which, a spatial-temporal model for
information propagation in blockchain is proposed; 2) active defense. We hint
that shrinking the long-range link factor, which indicates the remote
connection ability of a link, can cut down forking completely fundamentally. To
the best of our knowledge, we are the first to inspect forking from the
spatial-temporal perspective, so as to present countermeasures proactively.
Solid theoretical derivations and extensive simulations are conducted to
justify the validity and effectiveness of our analysis.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Decentralization Paradox: A Study of Hegemonic and Risky ERC-20 Tokens. (arXiv:2209.08370v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08370">http://arxiv.org/abs/2209.08370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08370] Decentralization Paradox: A Study of Hegemonic and Risky ERC-20 Tokens](http://arxiv.org/abs/2209.08370)</code></li>
<li>Summary: <p>In this work, we explore the class of Ethereum smart contracts called the
administrated ERC20 tokens. We demonstrate that these contracts are more
owner-controlled and less safe than the services they try to disrupt, such as
banks and centralized online payment systems. We develop a binary classifier
for identification of administrated ERC20 tokens, and conduct extensive data
analysis, which reveals that nearly 9 out of 10 ERC20 tokens on Ethereum are
administrated, and thereby unsafe to engage with even under the assumption of
trust towards their owners. We design and implement SafelyAdministrated - a
Solidity abstract class that safeguards users of administrated ERC20 tokens
from adversarial attacks or frivolous behavior of the tokens' owners.
</p></li>
</ul>

<h3>Title: pFedDef: Defending Grey-Box Attacks for Personalized Federated Learning. (arXiv:2209.08412v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08412">http://arxiv.org/abs/2209.08412</a></li>
<li>Code URL: <a href="https://github.com/tj-kim/pfeddef_v1">https://github.com/tj-kim/pfeddef_v1</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08412] pFedDef: Defending Grey-Box Attacks for Personalized Federated Learning](http://arxiv.org/abs/2209.08412)</code></li>
<li>Summary: <p>Personalized federated learning allows for clients in a distributed system to
train a neural network tailored to their unique local data while leveraging
information at other clients. However, clients' models are vulnerable to
attacks during both the training and testing phases. In this paper we address
the issue of adversarial clients crafting evasion attacks at test time to
deceive other clients. For example, adversaries may aim to deceive spam filters
and recommendation systems trained with personalized federated learning for
monetary gain. The adversarial clients have varying degrees of personalization
based on the method of distributed learning, leading to a "grey-box" situation.
We are the first to characterize the transferability of such internal evasion
attacks for different learning methods and analyze the trade-off between model
accuracy and robustness depending on the degree of personalization and
similarities in client data. We introduce a defense mechanism, pFedDef, that
performs personalized federated adversarial training while respecting resource
limitations at clients that inhibit adversarial training. Overall, pFedDef
increases relative grey-box adversarial robustness by 62% compared to federated
adversarial training and performs well even under limited system resources.
</p></li>
</ul>

<h3>Title: Distribution inference risks: Identifying and mitigating sources of leakage. (arXiv:2209.08541v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08541">http://arxiv.org/abs/2209.08541</a></li>
<li>Code URL: <a href="https://github.com/epfl-dlab/distribution-inference-risks">https://github.com/epfl-dlab/distribution-inference-risks</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08541] Distribution inference risks: Identifying and mitigating sources of leakage](http://arxiv.org/abs/2209.08541)</code></li>
<li>Summary: <p>A large body of work shows that machine learning (ML) models can leak
sensitive or confidential information about their training data. Recently,
leakage due to distribution inference (or property inference) attacks is
gaining attention. In this attack, the goal of an adversary is to infer
distributional information about the training data. So far, research on
distribution inference has focused on demonstrating successful attacks, with
little attention given to identifying the potential causes of the leakage and
to proposing mitigations. To bridge this gap, as our main contribution, we
theoretically and empirically analyze the sources of information leakage that
allows an adversary to perpetrate distribution inference attacks. We identify
three sources of leakage: (1) memorizing specific information about the
$\mathbb{E}[Y|X]$ (expected label given the feature values) of interest to the
adversary, (2) wrong inductive bias of the model, and (3) finiteness of the
training data. Next, based on our analysis, we propose principled mitigation
techniques against distribution inference attacks. Specifically, we demonstrate
that causal learning techniques are more resilient to a particular type of
distribution inference risk termed distributional membership inference than
associative learning methods. And lastly, we present a formalization of
distribution inference that allows for reasoning about more general adversaries
than was previously possible.
</p></li>
</ul>

<h3>Title: Membership Inference Attacks and Generalization: A Causal Perspective. (arXiv:2209.08615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08615">http://arxiv.org/abs/2209.08615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08615] Membership Inference Attacks and Generalization: A Causal Perspective](http://arxiv.org/abs/2209.08615)</code></li>
<li>Summary: <p>Membership inference (MI) attacks highlight a privacy weakness in present
stochastic training methods for neural networks. It is not well understood,
however, why they arise. Are they a natural consequence of imperfect
generalization only? Which underlying causes should we address during training
to mitigate these attacks? Towards answering such questions, we propose the
first approach to explain MI attacks and their connection to generalization
based on principled causal reasoning. We offer causal graphs that
quantitatively explain the observed MI attack performance achieved for $6$
attack variants. We refute several prior non-quantitative hypotheses that
over-simplify or over-estimate the influence of underlying causes, thereby
failing to capture the complex interplay between several factors. Our causal
models also show a new connection between generalization and MI attacks via
their shared causal factors. Our causal models have high predictive power
($0.90$), i.e., their analytical predictions match with observations in unseen
experiments often, which makes analysis via them a pragmatic alternative.
</p></li>
</ul>

<h3>Title: Anomaly Detection in Automatic Generation Control Systems Based on Traffic Pattern Analysis and Deep Transfer Learning. (arXiv:2209.08099v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08099">http://arxiv.org/abs/2209.08099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08099] Anomaly Detection in Automatic Generation Control Systems Based on Traffic Pattern Analysis and Deep Transfer Learning](http://arxiv.org/abs/2209.08099)</code></li>
<li>Summary: <p>In modern highly interconnected power grids, automatic generation control
(AGC) is crucial in maintaining the stability of the power grid. The dependence
of the AGC system on the information and communications technology (ICT) system
makes it vulnerable to various types of cyber-attacks. Thus, information flow
(IF) analysis and anomaly detection became paramount for preventing cyber
attackers from driving the cyber-physical power system (CPPS) to instability.
In this paper, the ICT network traffic rules in CPPSs are explored and the
frequency domain features of the ICT network traffic are extracted, basically
for developing a robust learning algorithm that can learn the normal traffic
pattern based on the ResNeSt convolutional neural network (CNN). Furthermore,
to overcome the problem of insufficient abnormal traffic labeled samples,
transfer learning approach is used. In the proposed data-driven-based method
the deep learning model is trained by traffic frequency features, which makes
our model robust against AGC's parameters uncertainties and modeling
nonlinearities.
</p></li>
</ul>

<h3>Title: EMaP: Explainable AI with Manifold-based Perturbations. (arXiv:2209.08453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08453">http://arxiv.org/abs/2209.08453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08453] EMaP: Explainable AI with Manifold-based Perturbations](http://arxiv.org/abs/2209.08453)</code></li>
<li>Summary: <p>In the last few years, many explanation methods based on the perturbations of
input data have been introduced to improve our understanding of decisions made
by black-box models. The goal of this work is to introduce a novel perturbation
scheme so that more faithful and robust explanations can be obtained. Our study
focuses on the impact of perturbing directions on the data topology. We show
that perturbing along the orthogonal directions of the input manifold better
preserves the data topology, both in the worst-case analysis of the discrete
Gromov-Hausdorff distance and in the average-case analysis via persistent
homology. From those results, we introduce EMaP algorithm, realizing the
orthogonal perturbation scheme. Our experiments show that EMaP not only
improves the explainers' performance but also helps them overcome a
recently-developed attack against perturbation-based methods.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust Ensemble Morph Detection with Domain Generalization. (arXiv:2209.08130v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08130">http://arxiv.org/abs/2209.08130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08130] Robust Ensemble Morph Detection with Domain Generalization](http://arxiv.org/abs/2209.08130)</code></li>
<li>Summary: <p>Although a substantial amount of studies is dedicated to morph detection,
most of them fail to generalize for morph faces outside of their training
paradigm. Moreover, recent morph detection methods are highly vulnerable to
adversarial attacks. In this paper, we intend to learn a morph detection model
with high generalization to a wide range of morphing attacks and high
robustness against different adversarial attacks. To this aim, we develop an
ensemble of convolutional neural networks (CNNs) and Transformer models to
benefit from their capabilities simultaneously. To improve the robust accuracy
of the ensemble model, we employ multi-perturbation adversarial training and
generate adversarial examples with high transferability for several single
models. Our exhaustive evaluations demonstrate that the proposed robust
ensemble model generalizes to several morphing attacks and face datasets. In
addition, we validate that our robust ensemble model gain better robustness
against several adversarial attacks while outperforming the state-of-the-art
studies.
</p></li>
</ul>

<h3>Title: Weakly Supervised Medical Image Segmentation With Soft Labels and Noise Robust Loss. (arXiv:2209.08172v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08172">http://arxiv.org/abs/2209.08172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08172] Weakly Supervised Medical Image Segmentation With Soft Labels and Noise Robust Loss](http://arxiv.org/abs/2209.08172)</code></li>
<li>Summary: <p>Recent advances in deep learning algorithms have led to significant benefits
for solving many medical image analysis problems. Training deep learning models
commonly requires large datasets with expert-labeled annotations. However,
acquiring expert-labeled annotation is not only expensive but also is
subjective, error-prone, and inter-/intra- observer variability introduces
noise to labels. This is particularly a problem when using deep learning models
for segmenting medical images due to the ambiguous anatomical boundaries.
Image-based medical diagnosis tools using deep learning models trained with
incorrect segmentation labels can lead to false diagnoses and treatment
suggestions. Multi-rater annotations might be better suited to train deep
learning models with small training sets compared to single-rater annotations.
The aim of this paper was to develop and evaluate a method to generate
probabilistic labels based on multi-rater annotations and anatomical knowledge
of the lesion features in MRI and a method to train segmentation models using
probabilistic labels using normalized active-passive loss as a "noise-tolerant
loss" function. The model was evaluated by comparing it to binary ground truth
for 17 knees MRI scans for clinical segmentation and detection of bone marrow
lesions (BML). The proposed method successfully improved precision 14, recall
22, and Dice score 8 percent compared to a binary cross-entropy loss function.
Overall, the results of this work suggest that the proposed normalized
active-passive loss using soft labels successfully mitigated the effects of
noisy labels.
</p></li>
</ul>

<h3>Title: Confidence-Guided Data Augmentation for Deep Semi-Supervised Training. (arXiv:2209.08174v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08174">http://arxiv.org/abs/2209.08174</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08174] Confidence-Guided Data Augmentation for Deep Semi-Supervised Training](http://arxiv.org/abs/2209.08174)</code></li>
<li>Summary: <p>We propose a new data augmentation technique for semi-supervised learning
settings that emphasizes learning from the most challenging regions of the
feature space. Starting with a fully supervised reference model, we first
identify low confidence predictions. These samples are then used to train a
Variational AutoEncoder (VAE) that can generate an infinite number of
additional images with similar distribution. Finally, using the originally
labeled data and the synthetically generated labeled and unlabeled data, we
retrain a new model in a semi-supervised fashion. We perform experiments on two
benchmark RGB datasets: CIFAR-100 and STL-10, and show that the proposed scheme
improves classification performance in terms of accuracy and robustness, while
yielding comparable or superior results with respect to existing fully
supervised approaches
</p></li>
</ul>

<h3>Title: RGB-Event Fusion for Moving Object Detection in Autonomous Driving. (arXiv:2209.08323v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08323">http://arxiv.org/abs/2209.08323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08323] RGB-Event Fusion for Moving Object Detection in Autonomous Driving](http://arxiv.org/abs/2209.08323)</code></li>
<li>Summary: <p>Moving Object Detection (MOD) is a critical vision task for successfully
achieving safe autonomous driving. Despite plausible results of deep learning
methods, most existing approaches are only frame-based and may fail to reach
reasonable performance when dealing with dynamic traffic participants. Recent
advances in sensor technologies, especially the Event camera, can naturally
complement the conventional camera approach to better model moving objects.
However, event-based works often adopt a pre-defined time window for event
representation, and simply integrate it to estimate image intensities from
events, neglecting much of the rich temporal information from the available
asynchronous events. Therefore, from a new perspective, we propose RENet, a
novel RGB-Event fusion Network, that jointly exploits the two complementary
modalities to achieve more robust MOD under challenging scenarios for
autonomous driving. Specifically, we first design a temporal multi-scale
aggregation module to fully leverage event frames from both the RGB exposure
time and larger intervals. Then we introduce a bi-directional fusion module to
attentively calibrate and fuse multi-modal features. To evaluate the
performance of our network, we carefully select and annotate a sub-MOD dataset
from the commonly used DSEC dataset. Extensive experiments demonstrate that our
proposed method performs significantly better than the state-of-the-art
RGB-Event fusion alternatives.
</p></li>
</ul>

<h3>Title: Shape Completion with Points in the Shadow. (arXiv:2209.08345v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08345">http://arxiv.org/abs/2209.08345</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08345] Shape Completion with Points in the Shadow](http://arxiv.org/abs/2209.08345)</code></li>
<li>Summary: <p>Single-view point cloud completion aims to recover the full geometry of an
object based on only limited observation, which is extremely hard due to the
data sparsity and occlusion. The core challenge is to generate plausible
geometries to fill the unobserved part of the object based on a partial scan,
which is under-constrained and suffers from a huge solution space. Inspired by
the classic shadow volume technique in computer graphics, we propose a new
method to reduce the solution space effectively. Our method considers the
camera a light source that casts rays toward the object. Such light rays build
a reasonably constrained but sufficiently expressive basis for completion. The
completion process is then formulated as a point displacement optimization
problem. Points are initialized at the partial scan and then moved to their
goal locations with two types of movements for each point: directional
movements along the light rays and constrained local movement for shape
refinement. We design neural networks to predict the ideal point movements to
get the completion results. We demonstrate that our method is accurate, robust,
and generalizable through exhaustive evaluation and comparison. Moreover, it
outperforms state-of-the-art methods qualitatively and quantitatively on MVP
datasets.
</p></li>
</ul>

<h3>Title: Spatial-Temporal Deep Embedding for Vehicle Trajectory Reconstruction from High-Angle Video. (arXiv:2209.08417v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08417">http://arxiv.org/abs/2209.08417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08417] Spatial-Temporal Deep Embedding for Vehicle Trajectory Reconstruction from High-Angle Video](http://arxiv.org/abs/2209.08417)</code></li>
<li>Summary: <p>Spatial-temporal Map (STMap)-based methods have shown great potential to
process high-angle videos for vehicle trajectory reconstruction, which can meet
the needs of various data-driven modeling and imitation learning applications.
In this paper, we developed Spatial-Temporal Deep Embedding (STDE) model that
imposes parity constraints at both pixel and instance levels to generate
instance-aware embeddings for vehicle stripe segmentation on STMap. At pixel
level, each pixel was encoded with its 8-neighbor pixels at different ranges,
and this encoding is subsequently used to guide a neural network to learn the
embedding mechanism. At the instance level, a discriminative loss function is
designed to pull pixels belonging to the same instance closer and separate the
mean value of different instances far apart in the embedding space. The output
of the spatial-temporal affinity is then optimized by the mutex-watershed
algorithm to obtain final clustering results. Based on segmentation metrics,
our model outperformed five other baselines that have been used for STMap
processing and shows robustness under the influence of shadows, static noises,
and overlapping. The designed model is applied to process all public NGSIM
US-101 videos to generate complete vehicle trajectories, indicating a good
scalability and adaptability. Last but not least, the strengths of the scanline
method with STDE and future directions were discussed. Code, STMap dataset and
video trajectory are made publicly available in the online repository. GitHub
Link: shorturl.at/jklT0.
</p></li>
</ul>

<h3>Title: Introspective Learning : A Two-Stage Approach for Inference in Neural Networks. (arXiv:2209.08425v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08425">http://arxiv.org/abs/2209.08425</a></li>
<li>Code URL: <a href="https://github.com/olivesgatech/introspective-learning">https://github.com/olivesgatech/introspective-learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08425] Introspective Learning : A Two-Stage Approach for Inference in Neural Networks](http://arxiv.org/abs/2209.08425)</code></li>
<li>Summary: <p>In this paper, we advocate for two stages in a neural network's decision
making process. The first is the existing feed-forward inference framework
where patterns in given data are sensed and associated with previously learned
patterns. The second stage is a slower reflection stage where we ask the
network to reflect on its feed-forward decision by considering and evaluating
all available choices. Together, we term the two stages as introspective
learning. We use gradients of trained neural networks as a measurement of this
reflection. A simple three-layered Multi Layer Perceptron is used as the second
stage that predicts based on all extracted gradient features. We perceptually
visualize the post-hoc explanations from both stages to provide a visual
grounding to introspection. For the application of recognition, we show that an
introspective network is 4% more robust and 42% less prone to calibration
errors when generalizing to noisy data. We also illustrate the value of
introspective networks in downstream tasks that require generalizability and
calibration including active learning, out-of-distribution detection, and
uncertainty estimation. Finally, we ground the proposed machine introspection
to human introspection for the application of image quality assessment.
</p></li>
</ul>

<h3>Title: TODE-Trans: Transparent Object Depth Estimation with Transformer. (arXiv:2209.08455v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08455">http://arxiv.org/abs/2209.08455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08455] TODE-Trans: Transparent Object Depth Estimation with Transformer](http://arxiv.org/abs/2209.08455)</code></li>
<li>Summary: <p>Transparent objects are widely used in industrial automation and daily life.
However, robust visual recognition and perception of transparent objects have
always been a major challenge. Currently, most commercial-grade depth cameras
are still not good at sensing the surfaces of transparent objects due to the
refraction and reflection of light. In this work, we present a
transformer-based transparent object depth estimation approach from a single
RGB-D input. We observe that the global characteristics of the transformer make
it easier to extract contextual information to perform depth estimation of
transparent areas. In addition, to better enhance the fine-grained features, a
feature fusion module (FFM) is designed to assist coherent prediction. Our
empirical evidence demonstrates that our model delivers significant
improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on
REL compared to previous state-of-the-art convolutional-based counterparts in
ClearGrasp dataset. Extensive results show that our transformer-based model
enables better aggregation of the object's RGB and inaccurate depth information
to obtain a better depth representation. Our code and the pre-trained model
will be available at https://github.com/yuchendoudou/TODE.
</p></li>
</ul>

<h3>Title: EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention. (arXiv:2209.08490v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08490">http://arxiv.org/abs/2209.08490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08490] EMA-VIO: Deep Visual-Inertial Odometry with External Memory Attention](http://arxiv.org/abs/2209.08490)</code></li>
<li>Summary: <p>Accurate and robust localization is a fundamental need for mobile agents.
Visual-inertial odometry (VIO) algorithms exploit the information from camera
and inertial sensors to estimate position and translation. Recent deep learning
based VIO models attract attentions as they provide pose information in a
data-driven way, without the need of designing hand-crafted algorithms.
Existing learning based VIO models rely on recurrent models to fuse multimodal
data and process sensor signal, which are hard to train and not efficient
enough. We propose a novel learning based VIO framework with external memory
attention that effectively and efficiently combines visual and inertial
features for states estimation. Our proposed model is able to estimate pose
accurately and robustly, even in challenging scenarios, e.g., on overcast days
and water-filled ground , which are difficult for traditional VIO algorithms to
extract visual features. Experiments validate that it outperforms both
traditional and learning based VIO baselines in different scenes.
</p></li>
</ul>

<h3>Title: Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution. (arXiv:2209.08503v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08503">http://arxiv.org/abs/2209.08503</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08503] Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution](http://arxiv.org/abs/2209.08503)</code></li>
<li>Summary: <p>We propose a robust and fast bundle adjustment solution that estimates the
6-DoF pose of the camera and the geometry of the environment based on
measurements from a rolling shutter (RS) camera. This tackles the challenges in
the existing works, namely relying on additional sensors, high frame rate video
as input, restrictive assumptions on camera motion, readout direction, and poor
efficiency. To this end, we first investigate the influence of normalization to
the image point on RSBA performance and show its better approximation in
modelling the real 6-DoF camera motion. Then we present a novel analytical
model for the visual residual covariance, which can be used to standardize the
reprojection error during the optimization, consequently improving the overall
accuracy. More importantly, the combination of normalization and covariance
standardization weighting in RSBA (NW-RSBA) can avoid common planar degeneracy
without needing to constrain the filming manner. Besides, we propose an
acceleration strategy for NW-RSBA based on the sparsity of its Jacobian matrix
and Schur complement. The extensive synthetic and real data experiments verify
the effectiveness and efficiency of the proposed solution over the
state-of-the-art works. We also demonstrate the proposed method can be easily
implemented and plug-in famous GSSfM and GSSLAM systems as completed RSSfM and
RSSLAM solutions.
</p></li>
</ul>

<h3>Title: ActiveNeRF: Learning where to See with Uncertainty Estimation. (arXiv:2209.08546v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08546">http://arxiv.org/abs/2209.08546</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08546] ActiveNeRF: Learning where to See with Uncertainty Estimation](http://arxiv.org/abs/2209.08546)</code></li>
<li>Summary: <p>Recently, Neural Radiance Fields (NeRF) has shown promising performances on
reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D
images. Albeit effective, the performance of NeRF is highly influenced by the
quality of training samples. With limited posed images from the scene, NeRF
fails to generalize well to novel views and may collapse to trivial solutions
in unobserved regions. This makes NeRF impractical under resource-constrained
scenarios. In this paper, we present a novel learning framework, ActiveNeRF,
aiming to model a 3D scene with a constrained input budget. Specifically, we
first incorporate uncertainty estimation into a NeRF model, which ensures
robustness under few observations and provides an interpretation of how NeRF
understands the scene. On this basis, we propose to supplement the existing
training set with newly captured samples based on an active learning scheme. By
evaluating the reduction of uncertainty given new inputs, we select the samples
that bring the most information gain. In this way, the quality of novel view
synthesis can be improved with minimal additional resources. Extensive
experiments validate the performance of our model on both realistic and
synthetic scenes, especially with scarcer training data. Code will be released
at \url{https://github.com/LeapLabTHU/ActiveNeRF}.
</p></li>
</ul>

<h3>Title: ASAP: Adaptive Scheme for Asynchronous Processing of Event-based Vision Algorithms. (arXiv:2209.08597v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08597">http://arxiv.org/abs/2209.08597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08597] ASAP: Adaptive Scheme for Asynchronous Processing of Event-based Vision Algorithms](http://arxiv.org/abs/2209.08597)</code></li>
<li>Summary: <p>Event cameras can capture pixel-level illumination changes with very high
temporal resolution and dynamic range. They have received increasing research
interest due to their robustness to lighting conditions and motion blur. Two
main approaches exist in the literature to feed the event-based processing
algorithms: packaging the triggered events in event packages and sending them
one-by-one as single events. These approaches suffer limitations from either
processing overflow or lack of responsivity. Processing overflow is caused by
high event generation rates when the algorithm cannot process all the events in
real-time. Conversely, lack of responsivity happens in cases of low event
generation rates when the event packages are sent at too low frequencies. This
paper presents ASAP, an adaptive scheme to manage the event stream through
variable-size packages that accommodate to the event package processing times.
The experimental results show that ASAP is capable of feeding an asynchronous
event-by-event clustering algorithm in a responsive and efficient manner and at
the same time prevents overflow.
</p></li>
</ul>

<h3>Title: Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features. (arXiv:2209.08614v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08614">http://arxiv.org/abs/2209.08614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08614] Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features](http://arxiv.org/abs/2209.08614)</code></li>
<li>Summary: <p>Imaging of facial affects may be used to measure psychophysiological
attributes of children through their adulthood, especially for monitoring
lifelong conditions like Autism Spectrum Disorder. Deep convolutional neural
networks have shown promising results in classifying facial expressions of
adults. However, classifier models trained with adult benchmark data are
unsuitable for learning child expressions due to discrepancies in
psychophysical development. Similarly, models trained with child data perform
poorly in adult expression classification. We propose domain adaptation to
concurrently align distributions of adult and child expressions in a shared
latent space to ensure robust classification of either domain. Furthermore, age
variations in facial images are studied in age-invariant face recognition yet
remain unleveraged in adult-child expression classification. We take
inspiration from multiple fields and propose deep adaptive FACial Expressions
fusing BEtaMix SElected Landmark Features (FACE-BE-SELF) for adult-child facial
expression classification. For the first time in the literature, a mixture of
Beta distributions is used to decompose and select facial features based on
correlations with expression, domain, and identity factors. We evaluate
FACE-BE-SELF on two pairs of adult-child data sets. Our proposed FACE-BE-SELF
approach outperforms adult-child transfer learning and other baseline domain
adaptation methods in aligning latent representations of adult and child
expressions.
</p></li>
</ul>

<h3>Title: RVSL: Robust Vehicle Similarity Learning in Real Hazy Scenes Based on Semi-supervised Learning. (arXiv:2209.08630v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08630">http://arxiv.org/abs/2209.08630</a></li>
<li>Code URL: <a href="https://github.com/cihsaing/rvsl-robust-vehicle-similarity-learning--eccv22">https://github.com/cihsaing/rvsl-robust-vehicle-similarity-learning--eccv22</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08630] RVSL: Robust Vehicle Similarity Learning in Real Hazy Scenes Based on Semi-supervised Learning](http://arxiv.org/abs/2209.08630)</code></li>
<li>Summary: <p>Recently, vehicle similarity learning, also called re-identification (ReID),
has attracted significant attention in computer vision. Several algorithms have
been developed and obtained considerable success. However, most existing
methods have unpleasant performance in the hazy scenario due to poor
visibility. Though some strategies are possible to resolve this problem, they
still have room to be improved due to the limited performance in real-world
scenarios and the lack of real-world clear ground truth. Thus, to resolve this
problem, inspired by CycleGAN, we construct a training paradigm called
\textbf{RVSL} which integrates ReID and domain transformation techniques. The
network is trained on semi-supervised fashion and does not require to employ
the ID labels and the corresponding clear ground truths to learn hazy vehicle
ReID mission in the real-world haze scenes. To further constrain the
unsupervised learning process effectively, several losses are developed.
Experimental results on synthetic and real-world datasets indicate that the
proposed method can achieve state-of-the-art performance on hazy vehicle ReID
problems. It is worth mentioning that although the proposed method is trained
without real-world label information, it can achieve competitive performance
compared to existing supervised methods trained on complete label information.
</p></li>
</ul>

<h3>Title: Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness. (arXiv:2209.08647v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08647">http://arxiv.org/abs/2209.08647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08647] Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness](http://arxiv.org/abs/2209.08647)</code></li>
<li>Summary: <p>Surgical action triplet recognition provides a better understanding of the
surgical scene. This task is of high relevance as it provides to the surgeon
with context-aware support and safety. The current go-to strategy for improving
performance is the development of new network mechanisms. However, the
performance of current state-of-the-art techniques is substantially lower than
other surgical tasks. Why is this happening? This is the question that we
address in this work. We present the first study to understand the failure of
existing deep learning models through the lens of robustness and explainabilty.
Firstly, we study current existing models under weak and strong
$\delta-$perturbations via adversarial optimisation scheme. We then provide the
failure modes via feature based explanations. Our study revels that the key for
improving performance and increasing reliability is in the core and spurious
attributes. Our work opens the door to more trustworthiness and reliability
deep learning models in surgical science.
</p></li>
</ul>

<h3>Title: Uncertainty Aware Multitask Pyramid Vision Transformer For UAV-Based Object Re-Identification. (arXiv:2209.08686v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08686">http://arxiv.org/abs/2209.08686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08686] Uncertainty Aware Multitask Pyramid Vision Transformer For UAV-Based Object Re-Identification](http://arxiv.org/abs/2209.08686)</code></li>
<li>Summary: <p>Object Re-IDentification (ReID), one of the most significant problems in
biometrics and surveillance systems, has been extensively studied by image
processing and computer vision communities in the past decades. Learning a
robust and discriminative feature representation is a crucial challenge for
object ReID. The problem is even more challenging in ReID based on Unmanned
Aerial Vehicle (UAV) as the images are characterized by continuously varying
camera parameters (e.g., view angle, altitude, etc.) of a flying drone. To
address this challenge, multiscale feature representation has been considered
to characterize images captured from UAV flying at different altitudes. In this
work, we propose a multitask learning approach, which employs a new multiscale
architecture without convolution, Pyramid Vision Transformer (PVT), as the
backbone for UAV-based object ReID. By uncertainty modeling of intraclass
variations, our proposed model can be jointly optimized using both
uncertainty-aware object ID and camera ID information. Experimental results are
reported on PRAI and VRAI, two ReID data sets from aerial surveillance, to
verify the effectiveness of our proposed approach
</p></li>
</ul>

<h3>Title: Selective Token Generation for Few-shot Natural Language Generation. (arXiv:2209.08206v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08206">http://arxiv.org/abs/2209.08206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08206] Selective Token Generation for Few-shot Natural Language Generation](http://arxiv.org/abs/2209.08206)</code></li>
<li>Summary: <p>Natural language modeling with limited training data is a challenging
problem, and many algorithms make use of large-scale pretrained language models
(PLMs) for this due to its great generalization ability. Among them, additive
learning that incorporates a task-specific adapter on top of the fixed
large-scale PLM has been popularly used in the few-shot setting. However, this
added adapter is still easy to disregard the knowledge of the PLM especially
for few-shot natural language generation (NLG) since an entire sequence is
usually generated by only the newly trained adapter. Therefore, in this work,
we develop a novel additive learning algorithm based on reinforcement learning
(RL) that selectively outputs language tokens between the task-general PLM and
the task-specific adapter during both training and inference. This output token
selection over the two generators allows the adapter to take into account
solely the task-relevant parts in sequence generation, and therefore makes it
more robust to overfitting as well as more stable in RL training. In addition,
to obtain the complementary adapter from the PLM for each few-shot task, we
exploit a separate selecting module that is also simultaneously trained using
RL. Experimental results on various few-shot NLG tasks including question
answering, data-to-text generation and text summarization demonstrate that the
proposed selective token generation significantly outperforms the previous
additive learning algorithms based on the PLMs.
</p></li>
</ul>

<h3>Title: Flexible and Structured Knowledge Grounded Question Answering. (arXiv:2209.08284v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08284">http://arxiv.org/abs/2209.08284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08284] Flexible and Structured Knowledge Grounded Question Answering](http://arxiv.org/abs/2209.08284)</code></li>
<li>Summary: <p>Can language models (LM) ground question-answering (QA) tasks in the
knowledge base via inherent relational reasoning ability? While previous models
that use only LMs have seen some success on many QA tasks, more recent methods
include knowledge graphs (KG) to complement LMs with their more logic-driven
implicit knowledge. However, effectively extracting information from structured
data, like KGs, empowers LMs to remain an open question, and current models
rely on graph techniques to extract knowledge. In this paper, we propose to
solely leverage the LMs to combine the language and knowledge for knowledge
based question-answering with flexibility, breadth of coverage and structured
reasoning. Specifically, we devise a knowledge construction method that
retrieves the relevant context with a dynamic hop, which expresses more
comprehensivenes than traditional GNN-based techniques. And we devise a deep
fusion mechanism to further bridge the information exchanging bottleneck
between the language and the knowledge. Extensive experiments show that our
model consistently demonstrates its state-of-the-art performance over
CommensenseQA benchmark, showcasing the possibility to leverage LMs solely to
robustly ground QA into the knowledge base.
</p></li>
</ul>

<h3>Title: Improving Topic Segmentation by Injecting Discourse Dependencies. (arXiv:2209.08626v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08626">http://arxiv.org/abs/2209.08626</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08626] Improving Topic Segmentation by Injecting Discourse Dependencies](http://arxiv.org/abs/2209.08626)</code></li>
<li>Summary: <p>Recent neural supervised topic segmentation models achieve distinguished
superior effectiveness over unsupervised methods, with the availability of
large-scale training corpora sampled from Wikipedia. These models may, however,
suffer from limited robustness and transferability caused by exploiting simple
linguistic cues for prediction, but overlooking more important inter-sentential
topical consistency. To address this issue, we present a discourse-aware neural
topic segmentation model with the injection of above-sentence discourse
dependency structures to encourage the model make topic boundary prediction
based more on the topical consistency between sentences. Our empirical study on
English evaluation datasets shows that injecting above-sentence discourse
structures to a neural topic segmenter with our proposed strategy can
substantially improve its performances on intra-domain and out-of-domain data,
with little increase of model's complexity.
</p></li>
</ul>

<h3>Title: Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions. (arXiv:2209.08554v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08554">http://arxiv.org/abs/2209.08554</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08554] Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions](http://arxiv.org/abs/2209.08554)</code></li>
<li>Summary: <p>Pruning is one of the predominant approaches for compressing deep neural
networks (DNNs). Lately, coresets (provable data summarizations) were leveraged
for pruning DNNs, adding the advantage of theoretical guarantees on the
trade-off between the compression rate and the approximation error. However,
coresets in this domain were either data-dependent or generated under
restrictive assumptions on both the model's weights and inputs. In real-world
scenarios, such assumptions are rarely satisfied, limiting the applicability of
coresets. To this end, we suggest a novel and robust framework for computing
such coresets under mild assumptions on the model's weights and without any
assumption on the training data. The idea is to compute the importance of each
neuron in each layer with respect to the output of the following layer. This is
achieved by a combination of L\"{o}wner ellipsoid and Caratheodory theorem. Our
method is simultaneously data-independent, applicable to various networks and
datasets (due to the simplified assumptions), and theoretically supported.
Experimental results show that our method outperforms existing coreset based
neural pruning approaches across a wide range of networks and datasets. For
example, our method achieved a $62\%$ compression rate on ResNet50 on ImageNet
with $1.09\%$ drop in accuracy.
</p></li>
</ul>

<h3>Title: Comprehensive identification of Long Covid articles with human-in-the-loop machine learning. (arXiv:2209.08124v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08124">http://arxiv.org/abs/2209.08124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08124] Comprehensive identification of Long Covid articles with human-in-the-loop machine learning](http://arxiv.org/abs/2209.08124)</code></li>
<li>Summary: <p>A significant percentage of COVID-19 survivors experience ongoing
multisystemic symptoms that often affect daily living, a condition known as
Long Covid or post-acute-sequelae of SARS-CoV-2 infection. However, identifying
Long Covid articles is challenging since articles refer to the condition using
a variety of less common terms or refrain from naming it at all. We developed
an iterative human-in-the-loop machine learning framework designed to
effectively leverage the data available and make the most efficient use of
human labels. Specifically, our approach combines data programming with active
learning into a robust ensemble model. Evaluating our model on a holdout set
demonstrates over three times the sensitivity of other methods. We apply our
model to PubMed to create the Long Covid collection, and demonstrate that (1)
most Long Covid articles do not refer to Long Covid by any name (2) when the
condition is named, the name used most frequently in the biomedical literature
is Long Covid, and (3) Long Covid is associated with disorders in a wide
variety of body systems. The Long Covid collection is updated weekly and is
searchable online at the LitCovid portal:
https://www.ncbi.nlm.nih.gov/research/coronavirus/docsum?filters=e_condition.LongCovid
</p></li>
</ul>

<h3>Title: Optimal Scaling for Locally Balanced Proposals in Discrete Spaces. (arXiv:2209.08183v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08183">http://arxiv.org/abs/2209.08183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08183] Optimal Scaling for Locally Balanced Proposals in Discrete Spaces](http://arxiv.org/abs/2209.08183)</code></li>
<li>Summary: <p>Optimal scaling has been well studied for Metropolis-Hastings (M-H)
algorithms in continuous spaces, but a similar understanding has been lacking
in discrete spaces. Recently, a family of locally balanced proposals (LBP) for
discrete spaces has been proved to be asymptotically optimal, but the question
of optimal scaling has remained open. In this paper, we establish, for the
first time, that the efficiency of M-H in discrete spaces can also be
characterized by an asymptotic acceptance rate that is independent of the
target distribution. Moreover, we verify, both theoretically and empirically,
that the optimal acceptance rates for LBP and random walk Metropolis (RWM) are
$0.574$ and $0.234$ respectively. These results also help establish that LBP is
asymptotically $O(N^\frac{2}{3})$ more efficient than RWM with respect to model
dimension $N$. Knowledge of the optimal acceptance rate allows one to
automatically tune the neighborhood size of a proposal distribution in a
discrete space, directly analogous to step-size control in continuous spaces.
We demonstrate empirically that such adaptive M-H sampling can robustly improve
sampling in a variety of target distributions in discrete spaces, including
training deep energy based models.
</p></li>
</ul>

<h3>Title: Towards Robust Off-Policy Evaluation via Human Inputs. (arXiv:2209.08682v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08682">http://arxiv.org/abs/2209.08682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08682] Towards Robust Off-Policy Evaluation via Human Inputs](http://arxiv.org/abs/2209.08682)</code></li>
<li>Summary: <p>Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies
in high-stakes domains such as healthcare, where direct deployment is often
infeasible, unethical, or expensive. When deployment environments are expected
to undergo changes (that is, dataset shifts), it is important for OPE methods
to perform robust evaluation of the policies amidst such changes. Existing
approaches consider robustness against a large class of shifts that can
arbitrarily change any observable property of the environment. This often
results in highly pessimistic estimates of the utilities, thereby invalidating
policies that might have been useful in deployment. In this work, we address
the aforementioned problem by investigating how domain knowledge can help
provide more realistic estimates of the utilities of policies. We leverage
human inputs on which aspects of the environments may plausibly change, and
adapt the OPE methods to only consider shifts on these aspects. Specifically,
we propose a novel framework, Robust OPE (ROPE), which considers shifts on a
subset of covariates in the data based on user inputs, and estimates worst-case
utility under these shifts. We then develop computationally efficient
algorithms for OPE that are robust to the aforementioned shifts for contextual
bandits and Markov decision processes. We also theoretically analyze the sample
complexity of these algorithms. Extensive experimentation with synthetic and
real world datasets from the healthcare domain demonstrates that our approach
not only captures realistic dataset shifts accurately, but also results in less
pessimistic policy evaluations.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: 6DOF Pose Estimation of a 3D Rigid Object based on Edge-enhanced Point Pair Features. (arXiv:2209.08266v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08266">http://arxiv.org/abs/2209.08266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08266] 6DOF Pose Estimation of a 3D Rigid Object based on Edge-enhanced Point Pair Features](http://arxiv.org/abs/2209.08266)</code></li>
<li>Summary: <p>The point pair feature (PPF) is widely used for 6D pose estimation. In this
paper, we propose an efficient 6D pose estimation method based on the PPF
framework. We introduce a well-targeted down-sampling strategy that focuses
more on edge area for efficient feature extraction of complex geometry. A pose
hypothesis validation approach is proposed to resolve the symmetric ambiguity
by calculating edge matching degree. We perform evaluations on two challenging
datasets and one real-world collected dataset, demonstrating the superiority of
our method on pose estimation of geometrically complex, occluded, symmetrical
objects. We further validate our method by applying it to simulated punctures.
</p></li>
</ul>

<h3>Title: Active-Passive SimStereo -- Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods. (arXiv:2209.08305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08305">http://arxiv.org/abs/2209.08305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08305] Active-Passive SimStereo -- Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods](http://arxiv.org/abs/2209.08305)</code></li>
<li>Summary: <p>In stereo vision, self-similar or bland regions can make it difficult to
match patches between two images. Active stereo-based methods mitigate this
problem by projecting a pseudo-random pattern on the scene so that each patch
of an image pair can be identified without ambiguity. However, the projected
pattern significantly alters the appearance of the image. If this pattern acts
as a form of adversarial noise, it could negatively impact the performance of
deep learning-based methods, which are now the de-facto standard for dense
stereo vision. In this paper, we propose the Active-Passive SimStereo dataset
and a corresponding benchmark to evaluate the performance gap between passive
and active stereo images for stereo matching algorithms. Using the proposed
benchmark and an additional ablation study, we show that the feature extraction
and matching modules of a selection of twenty selected deep learning-based
stereo matching methods generalize to active stereo without a problem. However,
the disparity refinement modules of three of the twenty architectures (ACVNet,
CascadeStereo, and StereoNet) are negatively affected by the active stereo
patterns due to their reliance on the appearance of the input images.
</p></li>
</ul>

<h3>Title: ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding. (arXiv:2209.08569v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08569">http://arxiv.org/abs/2209.08569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08569] ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding](http://arxiv.org/abs/2209.08569)</code></li>
<li>Summary: <p>Recent efforts of multimodal Transformers have improved Visually Rich
Document Understanding (VrDU) tasks via incorporating visual and textual
information. However, existing approaches mainly focus on fine-grained elements
such as words and document image patches, making it hard for them to learn from
coarse-grained elements, including natural lexical units like phrases and
salient visual regions like prominent image regions. In this paper, we attach
more importance to coarse-grained elements containing high-density information
and consistent semantics, which are valuable for document understanding. At
first, a document graph is proposed to model complex relationships among
multi-grained multimodal elements, in which salient visual regions are detected
by a cluster-based method. Then, a multi-grained multimodal Transformer called
mmLayout is proposed to incorporate coarse-grained information into existing
pre-trained fine-grained multimodal Transformers based on the graph. In
mmLayout, coarse-grained information is aggregated from fine-grained, and then,
after further processing, is fused back into fine-grained for final prediction.
Furthermore, common sense enhancement is introduced to exploit the semantic
information of natural lexical units. Experimental results on four tasks,
including information extraction and document question answering, show that our
method can improve the performance of multimodal Transformers based on
fine-grained elements and achieve better performance with fewer parameters.
Qualitative analyses show that our method can capture consistent semantics in
coarse-grained elements.
</p></li>
</ul>

<h3>Title: FR: Folded Rationalization with a Unified Encoder. (arXiv:2209.08285v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08285">http://arxiv.org/abs/2209.08285</a></li>
<li>Code URL: <a href="https://github.com/jugechengzi/fr">https://github.com/jugechengzi/fr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08285] FR: Folded Rationalization with a Unified Encoder](http://arxiv.org/abs/2209.08285)</code></li>
<li>Summary: <p>Conventional works generally employ a two-phase model in which a generator
selects the most important pieces, followed by a predictor that makes
predictions based on the selected pieces. However, such a two-phase model may
incur the degeneration problem where the predictor overfits to the noise
generated by a not yet well-trained generator and in turn, leads the generator
to converge to a sub-optimal model that tends to select senseless pieces. To
tackle this challenge, we propose Folded Rationalization (FR) that folds the
two phases of the rationale model into one from the perspective of text
semantic extraction. The key idea of FR is to employ a unified encoder between
the generator and predictor, based on which FR can facilitate a better
predictor by access to valuable information blocked by the generator in the
traditional two-phase model and thus bring a better generator. Empirically, we
show that FR improves the F1 score by up to 10.3% as compared to
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Dynamic Global Memory for Document-level Argument Extraction. (arXiv:2209.08679v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08679">http://arxiv.org/abs/2209.08679</a></li>
<li>Code URL: <a href="https://github.com/xinyadu/memory_docie">https://github.com/xinyadu/memory_docie</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08679] Dynamic Global Memory for Document-level Argument Extraction](http://arxiv.org/abs/2209.08679)</code></li>
<li>Summary: <p>Extracting informative arguments of events from news articles is a
challenging problem in information extraction, which requires a global
contextual understanding of each document. While recent work on document-level
extraction has gone beyond single-sentence and increased the cross-sentence
inference capability of end-to-end models, they are still restricted by certain
input sequence length constraints and usually ignore the global context between
events. To tackle this issue, we introduce a new global neural generation-based
framework for document-level event argument extraction by constructing a
document memory store to record the contextual event information and leveraging
it to implicitly and explicitly help with decoding of arguments for later
events. Empirical results show that our framework outperforms prior methods
substantially and it is more robust to adversarially annotated examples with
our constrained decoding design. (Our code and resources are available at
https://github.com/xinyadu/memory_docie for research purpose.)
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Through a fair looking-glass: mitigating bias in image datasets. (arXiv:2209.08648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08648">http://arxiv.org/abs/2209.08648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08648] Through a fair looking-glass: mitigating bias in image datasets](http://arxiv.org/abs/2209.08648)</code></li>
<li>Summary: <p>With the recent growth in computer vision applications, the question of how
fair and unbiased they are has yet to be explored. There is abundant evidence
that the bias present in training data is reflected in the models, or even
amplified. Many previous methods for image dataset de-biasing, including models
based on augmenting datasets, are computationally expensive to implement. In
this study, we present a fast and effective model to de-bias an image dataset
through reconstruction and minimizing the statistical dependence between
intended variables. Our architecture includes a U-net to reconstruct images,
combined with a pre-trained classifier which penalizes the statistical
dependence between target attribute and the protected attribute. We evaluate
our proposed model on CelebA dataset, compare the results with a
state-of-the-art de-biasing method, and show that the model achieves a
promising fairness-accuracy combination.
</p></li>
</ul>

<h3>Title: A Benchmark for Understanding and Generating Dialogue between Characters in Stories. (arXiv:2209.08524v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08524">http://arxiv.org/abs/2209.08524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08524] A Benchmark for Understanding and Generating Dialogue between Characters in Stories](http://arxiv.org/abs/2209.08524)</code></li>
<li>Summary: <p>Many classical fairy tales, fiction, and screenplays leverage dialogue to
advance story plots and establish characters. We present the first study to
explore whether machines can understand and generate dialogue in stories, which
requires capturing traits of different characters and the relationships between
them. To this end, we propose two new tasks including Masked Dialogue
Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue
turns and predicting speakers for specified dialogue turns, respectively. We
build a new dataset DialStory, which consists of 105k Chinese stories with a
large amount of dialogue weaved into the plots to support the evaluation. We
show the difficulty of the proposed tasks by testing existing models with
automatic and manual evaluation on DialStory. Furthermore, we propose to learn
explicit character representations to improve performance on these tasks.
Extensive experiments and case studies show that our approach can generate more
coherent and informative dialogue, and achieve higher speaker recognition
accuracy than strong baselines.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Linear TreeShap. (arXiv:2209.08192v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08192">http://arxiv.org/abs/2209.08192</a></li>
<li>Code URL: <a href="https://github.com/yupbank/linear_tree_shap">https://github.com/yupbank/linear_tree_shap</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08192] Linear TreeShap](http://arxiv.org/abs/2209.08192)</code></li>
<li>Summary: <p>Decision trees are well-known due to their ease of interpretability. To
improve accuracy, we need to grow deep trees or ensembles of trees. These are
hard to interpret, offsetting their original benefits. Shapley values have
recently become a popular way to explain the predictions of tree-based machine
learning models. It provides a linear weighting to features independent of the
tree structure. The rise in popularity is mainly due to TreeShap, which solves
a general exponential complexity problem in polynomial time. Following
extensive adoption in the industry, more efficient algorithms are required.
This paper presents a more efficient and straightforward algorithm: Linear
TreeShap. Like TreeShap, Linear TreeShap is exact and requires the same amount
of memory.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
