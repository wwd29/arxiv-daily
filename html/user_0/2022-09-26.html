<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Trusted IP solution in multi-tenant cloud FPGA platform. (arXiv:2209.11274v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11274">http://arxiv.org/abs/2209.11274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11274] Trusted IP solution in multi-tenant cloud FPGA platform](http://arxiv.org/abs/2209.11274)</code></li>
<li>Summary: <p>Because FPGAs outperform traditional processing cores like CPUs and GPUs in
terms of performance per watt and flexibility, they are being used more and
more in cloud and data center applications. There are growing worries about the
security risks posed by multi-tenant sharing as the demand for hardware
acceleration increases and gradually gives way to FPGA multi-tenancy in the
cloud. The confidentiality, integrity, and availability of FPGA-accelerated
applications may be compromised if space-shared FPGAs are made available to
many cloud tenants. We propose a root of trust-based trusted execution
mechanism called \textbf{TrustToken} to prevent harmful software-level
attackers from getting unauthorized access and jeopardizing security. With safe
key creation and truly random sources, \textbf{TrustToken} creates a security
block that serves as the foundation of trust-based IP security. By offering
crucial security characteristics, such as secure, isolated execution and
trusted user interaction, \textbf{TrustToken} only permits trustworthy
connection between the non-trusted third-party IP and the rest of the SoC
environment. The suggested approach does this by connecting the third-party IP
interface to the \textbf{TrustToken} Controller and running run-time checks on
the correctness of the IP authorization(Token) signals. With an emphasis on
software-based assaults targeting unauthorized access and information leakage,
we offer a noble hardware/software architecture for trusted execution in
FPGA-accelerated clouds and data centers.
</p></li>
</ul>

<h3>Title: Differentially private partitioned variational inference. (arXiv:2209.11595v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11595">http://arxiv.org/abs/2209.11595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11595] Differentially private partitioned variational inference](http://arxiv.org/abs/2209.11595)</code></li>
<li>Summary: <p>Learning a privacy-preserving model from distributed sensitive data is an
increasingly important problem, often formulated in the federated learning
context. Variational inference has recently been extended to the non-private
federated learning setting via the partitioned variational inference algorithm.
For privacy protection, the current gold standard is called differential
privacy. Differential privacy guarantees privacy in a strong, mathematically
clearly defined sense.
</p></li>
</ul>

<p>In this paper, we present differentially private partitioned variational
inference, the first general framework for learning a variational approximation
to a Bayesian posterior distribution in the federated learning setting while
minimising the number of communication rounds and providing differential
privacy guarantees for data subjects.
</p>
<p>We propose three alternative implementations in the general framework, one
based on perturbing local optimisation done by individual parties, and two
based on perturbing global updates (one using a version of federated averaging,
one adding virtual parties to the protocol), and compare their properties both
theoretically and empirically. We show that perturbing the local optimisation
works well with simple and complex models as long as each party has enough
local data. However, the privacy is always guaranteed independently by each
party. In contrast, perturbing the global updates works best with relatively
simple models. Given access to suitable secure primitives, such as secure
aggregation or secure shuffling, the performance can be improved by all parties
guaranteeing privacy jointly.
</p>

<h2>security</h2>
<h3>Title: Dynamic camera alignment optimization problem based on Fractal Decomposition based Algorithm. (arXiv:2209.11695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11695">http://arxiv.org/abs/2209.11695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11695] Dynamic camera alignment optimization problem based on Fractal Decomposition based Algorithm](http://arxiv.org/abs/2209.11695)</code></li>
<li>Summary: <p>In this work, we tackle the Dynamic Optimization Problem (DOP) of IA in a
real-world application using a Dynamic Optimization Algorithm (DOA) called
Fractal Decomposition Algorithm (FDA), introduced by recently. We used FDA to
perform IA on CCTV camera feed from a tunnel. As the camera viewpoint can
change by multiple reasons such as wind, maintenance, etc. the alignment is
required to guarantee the correct functioning of video-based traffic security
system.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy-Preserving Person Detection Using Low-Resolution Infrared Cameras. (arXiv:2209.11335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11335">http://arxiv.org/abs/2209.11335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11335] Privacy-Preserving Person Detection Using Low-Resolution Infrared Cameras](http://arxiv.org/abs/2209.11335)</code></li>
<li>Summary: <p>In intelligent building management, knowing the number of people and their
location in a room are important for better control of its illumination,
ventilation, and heating with reduced costs and improved comfort. This is
typically achieved by detecting people using compact embedded devices that are
installed on the room's ceiling, and that integrate low-resolution infrared
camera, which conceals each person's identity. However, for accurate detection,
state-of-the-art deep learning models still require supervised training using a
large annotated dataset of images. In this paper, we investigate cost-effective
methods that are suitable for person detection based on low-resolution infrared
images. Results indicate that for such images, we can reduce the amount of
supervision and computation, while still achieving a high level of detection
accuracy. Going from single-shot detectors that require bounding box
annotations of each person in an image, to auto-encoders that only rely on
unlabelled images that do not contain people, allows for considerable savings
in terms of annotation costs, and for models with lower computational costs. We
validate these experimental findings on two challenging top-view datasets with
low-resolution infrared images.
</p></li>
</ul>

<h3>Title: Comparison of synthetic dataset generation methods for medical intervention rooms using medical clothing detection as an example. (arXiv:2209.11493v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11493">http://arxiv.org/abs/2209.11493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11493] Comparison of synthetic dataset generation methods for medical intervention rooms using medical clothing detection as an example](http://arxiv.org/abs/2209.11493)</code></li>
<li>Summary: <p>The availability of real data from areas with high privacy requirements, such
as the medical intervention space, is low and the acquisition legally complex.
Therefore, this work presents a way to create a synthetic dataset for the
medical context, using medical clothing as an example. The goal is to close the
reality gap between the synthetic and real data. For this purpose, methods of
3D-scanned clothing and designed clothing are compared in a
Domain-Randomization and Structured-Domain-Randomization scenario using an
Unreal-Engine plugin or Unity. Additionally a Mixed-Reality dataset in front of
a greenscreen and a target domain dataset were used. Our experiments show, that
Structured-Domain-Randomization of designed clothing together with
Mixed-Reality data provide a baseline achieving 72.0% mAP on a test dataset of
the clinical target domain. When additionally using 15% of available target
domain train data, the gap towards 100% (660 images) target domain train data
could be nearly closed 80.05% mAP (81.95% mAP). Finally we show that when
additionally using 100% target domain train data the accuracy could be
increased to 83.35% mAP.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Catoptric Light can be Dangerous: Effective Physical-World Attack by Natural Phenomenon. (arXiv:2209.11739v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11739">http://arxiv.org/abs/2209.11739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11739] Catoptric Light can be Dangerous: Effective Physical-World Attack by Natural Phenomenon](http://arxiv.org/abs/2209.11739)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have achieved great success in many tasks.
Therefore, it is crucial to evaluate the robustness of advanced DNNs. The
traditional methods use stickers as physical perturbations to fool the
classifiers, which is difficult to achieve stealthiness and there exists
printing loss. Some new types of physical attacks use light beam to perform
attacks (e.g., laser, projector), whose optical patterns are artificial rather
than natural. In this work, we study a new type of physical attack, called
adversarial catoptric light (AdvCL), in which adversarial perturbations are
generated by common natural phenomena, catoptric light, to achieve stealthy and
naturalistic adversarial attacks against advanced DNNs in physical
environments. Carefully designed experiments demonstrate the effectiveness of
the proposed method in simulated and real-world environments. The attack
success rate is 94.90% in a subset of ImageNet and 83.50% in the real-world
environment. We also discuss some of AdvCL's transferability and defense
strategy against this attack.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Towards Frame Rate Agnostic Multi-Object Tracking. (arXiv:2209.11404v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11404">http://arxiv.org/abs/2209.11404</a></li>
<li>Code URL: <a href="https://github.com/helicopt/framot">https://github.com/helicopt/framot</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11404] Towards Frame Rate Agnostic Multi-Object Tracking](http://arxiv.org/abs/2209.11404)</code></li>
<li>Summary: <p>Multi-Object Tracking (MOT) is one of the most fundamental computer vision
tasks which contributes to a variety of video analysis applications. Despite
the recent promising progress, current MOT research is still limited to a fixed
sampling frame rate of the input stream. In fact, we empirically find that the
accuracy of all recent state-of-the-art trackers drops dramatically when the
input frame rate changes. For a more intelligent tracking solution, we shift
the attention of our research work to the problem of Frame Rate Agnostic MOT
(FraMOT). In this paper, we propose a Frame Rate Agnostic MOT framework with
Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first
time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM)
that infers and encodes the frame rate information to aid identity matching
across multi-frame-rate inputs, improving the capability of the learned model
in handling complex motion-appearance relations in FraMOT. Besides, the
association gap between training and inference is enlarged in FraMOT because
those post-processing steps not included in training make a larger difference
in lower frame rate scenarios. To address it, we propose Periodic Training
Scheme (PTS) to reflect all post-processing steps in training via tracking
pattern matching and fusion. Along with the proposed approaches, we make the
first attempt to establish an evaluation method for this new task of FraMOT in
two different modes, i.e., known frame rate and unknown frame rate, aiming to
handle a more complex situation. The quantitative experiments on the
challenging MOT datasets (FraMOT version) have clearly demonstrated that the
proposed approaches can handle different frame rates better and thus improve
the robustness against complicated scenarios.
</p></li>
</ul>

<h3>Title: TeST: Test-time Self-Training under Distribution Shift. (arXiv:2209.11459v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11459">http://arxiv.org/abs/2209.11459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11459] TeST: Test-time Self-Training under Distribution Shift](http://arxiv.org/abs/2209.11459)</code></li>
<li>Summary: <p>Despite their recent success, deep neural networks continue to perform poorly
when they encounter distribution shifts at test time. Many recently proposed
approaches try to counter this by aligning the model to the new distribution
prior to inference. With no labels available this requires unsupervised
objectives to adapt the model on the observed test data. In this paper, we
propose Test-Time Self-Training (TeST): a technique that takes as input a model
trained on some source data and a novel data distribution at test time, and
learns invariant and robust representations using a student-teacher framework.
We find that models adapted using TeST significantly improve over baseline
test-time adaptation algorithms. TeST achieves competitive performance to
modern domain adaptation algorithms, while having access to 5-10x less data at
time of adaption. We thoroughly evaluate a variety of baselines on two tasks:
object detection and image segmentation and find that models adapted with TeST.
We find that TeST sets the new state-of-the art for test-time domain adaptation
algorithms.
</p></li>
</ul>

<h3>Title: MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier. (arXiv:2209.11549v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11549">http://arxiv.org/abs/2209.11549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11549] MAGIC: Mask-Guided Image Synthesis by Inverting a Quasi-Robust Classifier](http://arxiv.org/abs/2209.11549)</code></li>
<li>Summary: <p>We offer a method for one-shot image synthesis that allows controlling
manipulations of a single image by inverting a quasi-robust classifier equipped
with strong regularizers. Our proposed method, entitled Magic, samples
structured gradients from a pre-trained quasi-robust classifier to better
preserve the input semantics while preserving its classification accuracy,
thereby guaranteeing credibility in the synthesis. Unlike current methods that
use complex primitives to supervise the process or use attention maps as a weak
supervisory signal, Magic aggregates gradients over the input, driven by a
guide binary mask that enforces a strong, spatial prior. Magic implements a
series of manipulations with a single framework achieving shape and location
control, intense non-rigid shape deformations, and copy/move operations in the
presence of repeating objects and gives users firm control over the synthesis
by requiring simply specifying binary guide masks. Our study and findings are
supported by various qualitative comparisons with the state-of-the-art on the
same images sampled from ImageNet and quantitative analysis using machine
perception along with a user survey of 100+ participants that endorse our
synthesis quality.
</p></li>
</ul>

<h3>Title: View-Invariant Skeleton-based Action Recognition via Global-Local Contrastive Learning. (arXiv:2209.11634v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11634">http://arxiv.org/abs/2209.11634</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11634] View-Invariant Skeleton-based Action Recognition via Global-Local Contrastive Learning](http://arxiv.org/abs/2209.11634)</code></li>
<li>Summary: <p>Skeleton-based human action recognition has been drawing more interest
recently due to its low sensitivity to appearance changes and the accessibility
of more skeleton data. However, even the 3D skeletons captured in practice are
still sensitive to the viewpoint and direction gave the occlusion of different
human-body joints and the errors in human joint localization. Such view
variance of skeleton data may significantly affect the performance of action
recognition. To address this issue, we propose in this paper a new
view-invariant representation learning approach, without any manual action
labeling, for skeleton-based human action recognition. Specifically, we
leverage the multi-view skeleton data simultaneously taken for the same person
in the network training, by maximizing the mutual information between the
representations extracted from different views, and then propose a global-local
contrastive loss to model the multi-scale co-occurrence relationships in both
spatial and temporal domains. Extensive experimental results show that the
proposed method is robust to the view difference of the input skeleton data and
significantly boosts the performance of unsupervised skeleton-based human
action methods, resulting in new state-of-the-art accuracies on two challenging
multi-view benchmarks of PKUMMD and NTU RGB+D.
</p></li>
</ul>

<h3>Title: Multilevel Robustness for 2D Vector Field Feature Tracking, Selection, and Comparison. (arXiv:2209.11708v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11708">http://arxiv.org/abs/2209.11708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11708] Multilevel Robustness for 2D Vector Field Feature Tracking, Selection, and Comparison](http://arxiv.org/abs/2209.11708)</code></li>
<li>Summary: <p>Critical point tracking is a core topic in scientific visualization for
understanding the dynamic behavior of time-varying vector field data. The
topological notion of robustness has been introduced recently to quantify the
structural stability of critical points, that is, the robustness of a critical
point is the minimum amount of perturbation to the vector field necessary to
cancel it. A theoretical basis has been established previously that relates
critical point tracking with the notion of robustness, in particular, critical
points could be tracked based on their closeness in stability, measured by
robustness, instead of just distance proximities within the domain. However, in
practice, the computation of classic robustness may produce artifacts when a
critical point is close to the boundary of the domain; thus, we do not have a
complete picture of the vector field behavior within its local neighborhood. To
alleviate these issues, we introduce a multilevel robustness framework for the
study of 2D time-varying vector fields. We compute the robustness of critical
points across varying neighborhoods to capture the multiscale nature of the
data and to mitigate the boundary effect suffered by the classic robustness
computation. We demonstrate via experiments that such a new notion of
robustness can be combined seamlessly with existing feature tracking algorithms
to improve the visual interpretability of vector fields in terms of feature
tracking, selection, and comparison for large-scale scientific simulations. We
observe, for the first time, that the minimum multilevel robustness is highly
correlated with physical quantities used by domain scientists in studying a
real-world tropical cyclone dataset. Such observation helps to increase the
physical interpretability of robustness.
</p></li>
</ul>

<h3>Title: Robust Domain Adaptation for Machine Reading Comprehension. (arXiv:2209.11615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11615">http://arxiv.org/abs/2209.11615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11615] Robust Domain Adaptation for Machine Reading Comprehension](http://arxiv.org/abs/2209.11615)</code></li>
<li>Summary: <p>Most domain adaptation methods for machine reading comprehension (MRC) use a
pre-trained question-answer (QA) construction model to generate pseudo QA pairs
for MRC transfer. Such a process will inevitably introduce mismatched pairs
(i.e., noisy correspondence) due to i) the unavailable QA pairs in target
documents, and ii) the domain shift during applying the QA construction model
to the target domain. Undoubtedly, the noisy correspondence will degenerate the
performance of MRC, which however is neglected by existing works. To solve such
an untouched problem, we propose to construct QA pairs by additionally using
the dialogue related to the documents, as well as a new domain adaptation
method for MRC. Specifically, we propose Robust Domain Adaptation for Machine
Reading Comprehension (RMRC) method which consists of an answer extractor (AE),
a question selector (QS), and an MRC model. Specifically, RMRC filters out the
irrelevant answers by estimating the correlation to the document via the AE,
and extracts the questions by fusing the candidate questions in multiple rounds
of dialogue chats via the QS. With the extracted QA pairs, MRC is fine-tuned
and provides the feedback to optimize the QS through a novel reinforced
self-training method. Thanks to the optimization of the QS, our method will
greatly alleviate the noisy correspondence problem caused by the domain shift.
To the best of our knowledge, this could be the first study to reveal the
influence of noisy correspondence in domain adaptation MRC models and show a
feasible way to achieve robustness to mismatched pairs. Extensive experiments
on three datasets demonstrate the effectiveness of our method.
</p></li>
</ul>

<h3>Title: The "Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices. (arXiv:2209.11715v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11715">http://arxiv.org/abs/2209.11715</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11715] The "Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices](http://arxiv.org/abs/2209.11715)</code></li>
<li>Summary: <p>Deep Neural Networks (DNNs) are susceptible to backdoor attacks during
training. The model corrupted in this way functions normally, but when
triggered by certain patterns in the input, produces a predefined target label.
Existing defenses usually rely on the assumption of the universal backdoor
setting in which poisoned samples share the same uniform trigger. However,
recent advanced backdoor attacks show that this assumption is no longer valid
in dynamic backdoors where the triggers vary from input to input, thereby
defeating the existing defenses.
</p></li>
</ul>

<p>In this work, we propose a novel technique, Beatrix (backdoor detection via
Gram matrix). Beatrix utilizes Gram matrix to capture not only the feature
correlations but also the appropriately high-order information of the
representations. By learning class-conditional statistics from activation
patterns of normal samples, Beatrix can identify poisoned samples by capturing
the anomalies in activation patterns. To further improve the performance in
identifying target labels, Beatrix leverages kernel-based testing without
making any prior assumptions on representation distribution. We demonstrate the
effectiveness of our method through extensive evaluation and comparison with
state-of-the-art defensive techniques. The experimental results show that our
approach achieves an F1 score of 91.1% in detecting dynamic backdoors, while
the state of the art can only reach 36.9%.
</p>

<h3>Title: Scalable Gaussian Process Hyperparameter Optimization via Coverage Regularization. (arXiv:2209.11280v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11280">http://arxiv.org/abs/2209.11280</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11280] Scalable Gaussian Process Hyperparameter Optimization via Coverage Regularization](http://arxiv.org/abs/2209.11280)</code></li>
<li>Summary: <p>Gaussian processes (GPs) are Bayesian non-parametric models popular in a
variety of applications due to their accuracy and native uncertainty
quantification (UQ). Tuning GP hyperparameters is critical to ensure the
validity of prediction accuracy and uncertainty; uniquely estimating multiple
hyperparameters in, e.g. the Matern kernel can also be a significant challenge.
Moreover, training GPs on large-scale datasets is a highly active area of
research: traditional maximum likelihood hyperparameter training requires
quadratic memory to form the covariance matrix and has cubic training
complexity. To address the scalable hyperparameter tuning problem, we present a
novel algorithm which estimates the smoothness and length-scale parameters in
the Matern kernel in order to improve robustness of the resulting prediction
uncertainties. Using novel loss functions similar to those in conformal
prediction algorithms in the computational framework provided by the
hyperparameter estimation algorithm MuyGPs, we achieve improved UQ over
leave-one-out likelihood maximization while maintaining a high degree of
scalability as demonstrated in numerical experiments.
</p></li>
</ul>

<h3>Title: Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning. (arXiv:2209.11596v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11596">http://arxiv.org/abs/2209.11596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11596] Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning](http://arxiv.org/abs/2209.11596)</code></li>
<li>Summary: <p>Training a robust policy is critical for policy deployment in real-world
systems or dealing with unknown dynamics mismatch in different dynamic systems.
Domain Randomization~(DR) is a simple and elegant approach that trains a
conservative policy to counter different dynamic systems without expert
knowledge about the target system parameters. However, existing works reveal
that the policy trained through DR tends to be over-conservative and performs
poorly in target domains. Our key insight is that dynamic systems with
different parameters provide different levels of difficulty for the policy, and
the difficulty of behaving well in a system is constantly changing due to the
evolution of the policy. If we can actively sample the systems with proper
difficulty for the policy on the fly, it will stabilize the training process
and prevent the policy from becoming over-conservative or over-optimistic. To
operationalize this idea, we introduce Active Dynamics Preference~(ADP), which
quantifies the informativeness and density of sampled system parameters. ADP
actively selects system parameters with high informativeness and low density.
We validate our approach in four robotic locomotion tasks with various
discrepancies between the training and testing environments. Extensive results
demonstrate that our approach has superior robustness for system inconsistency
compared to several baselines.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: 3DPCT: 3D Point Cloud Transformer with Dual Self-attention. (arXiv:2209.11255v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11255">http://arxiv.org/abs/2209.11255</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11255] 3DPCT: 3D Point Cloud Transformer with Dual Self-attention](http://arxiv.org/abs/2209.11255)</code></li>
<li>Summary: <p>Transformers have resulted in remarkable achievements in the field of image
processing. Inspired by this great success, the application of Transformers to
3D point cloud processing has drawn more and more attention. This paper
presents a novel point cloud representational learning network, 3D Point Cloud
Transformer with Dual Self-attention (3DPCT) and an encoder-decoder structure.
Specifically, 3DPCT has a hierarchical encoder, which contains two local-global
dual-attention modules for the classification task (three modules for the
segmentation task), with each module consisting of a Local Feature Aggregation
(LFA) block and a Global Feature Learning (GFL) block. The GFL block is dual
self-attention, with both point-wise and channel-wise self-attention to improve
feature extraction. Moreover, in LFA, to better leverage the local information
extracted, a novel point-wise self-attention model, named as Point-Patch
Self-Attention (PPSA), is designed. The performance is evaluated on both
classification and segmentation datasets, containing both synthetic and
real-world data. Extensive experiments demonstrate that the proposed method
achieved state-of-the-art results on both classification and segmentation
tasks.
</p></li>
</ul>

<h3>Title: Colonoscopy Landmark Detection using Vision Transformers. (arXiv:2209.11304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11304">http://arxiv.org/abs/2209.11304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11304] Colonoscopy Landmark Detection using Vision Transformers](http://arxiv.org/abs/2209.11304)</code></li>
<li>Summary: <p>Colonoscopy is a routine outpatient procedure used to examine the colon and
rectum for any abnormalities including polyps, diverticula and narrowing of
colon structures. A significant amount of the clinician's time is spent in
post-processing snapshots taken during the colonoscopy procedure, for
maintaining medical records or further investigation. Automating this step can
save time and improve the efficiency of the process. In our work, we have
collected a dataset of 120 colonoscopy videos and 2416 snapshots taken during
the procedure, that have been annotated by experts. Further, we have developed
a novel, vision-transformer based landmark detection algorithm that identifies
key anatomical landmarks (the appendiceal orifice, ileocecal valve/cecum
landmark and rectum retroflexion) from snapshots taken during colonoscopy. Our
algorithm uses an adaptive gamma correction during preprocessing to maintain a
consistent brightness for all images. We then use a vision transformer as the
feature extraction backbone and a fully connected network based classifier head
to categorize a given frame into four classes: the three landmarks or a
non-landmark frame. We compare the vision transformer (ViT-B/16) backbone with
ResNet-101 and ConvNext-B backbones that have been trained similarly. We report
an accuracy of 82% with the vision transformer backbone on a test dataset of
snapshots.
</p></li>
</ul>

<h3>Title: Statistical shape representations for temporal registration of plant components in 3D. (arXiv:2209.11526v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11526">http://arxiv.org/abs/2209.11526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11526] Statistical shape representations for temporal registration of plant components in 3D](http://arxiv.org/abs/2209.11526)</code></li>
<li>Summary: <p>Plants are dynamic organisms. Understanding temporal variations in vegetation
is an essential problem for all robots in the wild. However, associating
repeated 3D scans of plants across time is challenging. A key step in this
process is re-identifying and tracking the same individual plant components
over time. Previously, this has been achieved by comparing their global spatial
or topological location. In this work, we demonstrate how using shape features
improves temporal organ matching. We present a landmark-free shape compression
algorithm, which allows for the extraction of 3D shape features of leaves,
characterises leaf shape and curvature efficiently in few parameters, and makes
the association of individual leaves in feature space possible. The approach
combines 3D contour extraction and further compression using Principal
Component Analysis (PCA) to produce a shape space encoding, which is entirely
learned from data and retains information about edge contours and 3D curvature.
Our evaluation on temporal scan sequences of tomato plants shows, that
incorporating shape features improves temporal leaf-matching. A combination of
shape, location, and rotation information proves most informative for
recognition of leaves over time and yields a true positive rate of 75%, a 15%
improvement on sate-of-the-art methods. This is essential for robotic crop
monitoring, which enables whole-of-lifecycle phenotyping.
</p></li>
</ul>

<h3>Title: ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension. (arXiv:2209.11484v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11484">http://arxiv.org/abs/2209.11484</a></li>
<li>Code URL: <a href="https://github.com/yottaxx/et5">https://github.com/yottaxx/et5</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11484] ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension](http://arxiv.org/abs/2209.11484)</code></li>
<li>Summary: <p>Conversational machine reading comprehension (CMRC) aims to assist computers
to understand an natural language text and thereafter engage in a multi-turn
conversation to answer questions related to the text. Existing methods
typically require three steps: (1) decision making based on entailment
reasoning; (2) span extraction if required by the above decision; (3) question
rephrasing based on the extracted span. However, for nearly all these methods,
the span extraction and question rephrasing steps cannot fully exploit the
fine-grained entailment reasoning information in decision making step because
of their relative independence, which will further enlarge the information gap
between decision making and question phrasing. Thus, to tackle this problem, we
propose a novel end-to-end framework for conversational machine reading
comprehension based on shared parameter mechanism, called entailment reasoning
T5 (ET5). Despite the lightweight of our proposed framework, experimental
results show that the proposed ET5 achieves new state-of-the-art results on the
ShARC leaderboard with the BLEU-4 score of 55.2. Our model and code are
publicly available at https://github.com/Yottaxx/ET5.
</p></li>
</ul>

<h3>Title: StyleTime: Style Transfer for Synthetic Time Series Generation. (arXiv:2209.11306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11306">http://arxiv.org/abs/2209.11306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11306] StyleTime: Style Transfer for Synthetic Time Series Generation](http://arxiv.org/abs/2209.11306)</code></li>
<li>Summary: <p>Neural style transfer is a powerful computer vision technique that can
incorporate the artistic "style" of one image to the "content" of another. The
underlying theory behind the approach relies on the assumption that the style
of an image is represented by the Gram matrix of its features, which is
typically extracted from pre-trained convolutional neural networks (e.g.,
VGG-19). This idea does not straightforwardly extend to time series stylization
since notions of style for two-dimensional images are not analogous to notions
of style for one-dimensional time series. In this work, a novel formulation of
time series style transfer is proposed for the purpose of synthetic data
generation and enhancement. We introduce the concept of stylized features for
time series, which is directly related to the time series realism properties,
and propose a novel stylization algorithm, called StyleTime, that uses explicit
feature extraction techniques to combine the underlying content (trend) of one
time series with the style (distributional properties) of another. Further, we
discuss evaluation metrics, and compare our work to existing state-of-the-art
time series generation and augmentation schemes. To validate the effectiveness
of our methods, we use stylized synthetic data as a means for data augmentation
to improve the performance of recurrent neural network models on several
forecasting tasks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body. (arXiv:2209.11355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11355">http://arxiv.org/abs/2209.11355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11355] Learning Interpretable Dynamics from Images of a Freely Rotating 3D Rigid Body](http://arxiv.org/abs/2209.11355)</code></li>
<li>Summary: <p>In many real-world settings, image observations of freely rotating 3D rigid
bodies, such as satellites, may be available when low-dimensional measurements
are not. However, the high-dimensionality of image data precludes the use of
classical estimation techniques to learn the dynamics and a lack of
interpretability reduces the usefulness of standard deep learning methods. In
this work, we present a physics-informed neural network model to estimate and
predict 3D rotational dynamics from image sequences. We achieve this using a
multi-stage prediction pipeline that maps individual images to a latent
representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities
from latent pairs, and predicts future latent states using the Hamiltonian
equations of motion with a learned representation of the Hamiltonian. We
demonstrate the efficacy of our approach on a new rotating rigid-body dataset
with sequences of rotating cubes and rectangular prisms with uniform and
non-uniform density.
</p></li>
</ul>

<h3>Title: I-SPLIT: Deep Network Interpretability for Split Computing. (arXiv:2209.11607v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11607">http://arxiv.org/abs/2209.11607</a></li>
<li>Code URL: <a href="https://github.com/vips4/i-split">https://github.com/vips4/i-split</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11607] I-SPLIT: Deep Network Interpretability for Split Computing](http://arxiv.org/abs/2209.11607)</code></li>
<li>Summary: <p>This work makes a substantial step in the field of split computing, i.e., how
to split a deep neural network to host its early part on an embedded device and
the rest on a server. So far, potential split locations have been identified
exploiting uniquely architectural aspects, i.e., based on the layer sizes.
Under this paradigm, the efficacy of the split in terms of accuracy can be
evaluated only after having performed the split and retrained the entire
pipeline, making an exhaustive evaluation of all the plausible splitting points
prohibitive in terms of time. Here we show that not only the architecture of
the layers does matter, but the importance of the neurons contained therein
too. A neuron is important if its gradient with respect to the correct class
decision is high. It follows that a split should be applied right after a layer
with a high density of important neurons, in order to preserve the information
flowing until then. Upon this idea, we propose Interpretable Split (I-SPLIT): a
procedure that identifies the most suitable splitting points by providing a
reliable prediction on how well this split will perform in terms of
classification accuracy, beforehand of its effective implementation. As a
further major contribution of I-SPLIT, we show that the best choice for the
splitting point on a multiclass categorization problem depends also on which
specific classes the network has to deal with. Exhaustive experiments have been
carried out on two networks, VGG16 and ResNet-50, and three datasets,
Tiny-Imagenet-200, notMNIST, and Chest X-Ray Pneumonia. The source code is
available at https://github.com/vips4/I-Split.
</p></li>
</ul>

<h3>Title: On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks. (arXiv:2209.11740v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11740">http://arxiv.org/abs/2209.11740</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11740] On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks](http://arxiv.org/abs/2209.11740)</code></li>
<li>Summary: <p>In this paper, we aim to improve the mathematical interpretability of
convolutional neural networks for image classification. When trained on natural
image datasets, such networks tend to learn parameters in the first layer that
closely resemble oriented Gabor filters. By leveraging the properties of
discrete Gabor-like convolutions, we prove that, under specific conditions,
feature maps computed by the subsequent max pooling operator tend to
approximate the modulus of complex Gabor-like coefficients, and as such, are
stable with respect to certain input shifts. We then compute a probabilistic
measure of shift invariance for these layers. More precisely, we show that some
filters, depending on their frequency and orientation, are more likely than
others to produce stable image representations. We experimentally validate our
theory by considering a deterministic feature extractor based on the dual-tree
wavelet packet transform, a particular case of discrete Gabor-like
decomposition. We demonstrate a strong correlation between shift invariance on
the one hand and similarity with complex modulus on the other hand.
</p></li>
</ul>

<h3>Title: Towards Faithful Model Explanation in NLP: A Survey. (arXiv:2209.11326v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.11326">http://arxiv.org/abs/2209.11326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.11326] Towards Faithful Model Explanation in NLP: A Survey](http://arxiv.org/abs/2209.11326)</code></li>
<li>Summary: <p>End-to-end neural NLP architectures are notoriously difficult to understand,
which gives rise to numerous efforts towards model explainability in recent
years. An essential principle of model explanation is Faithfulness, i.e., an
explanation should accurately represent the reasoning process behind the
model's prediction. This survey first discusses the definition and evaluation
of Faithfulness, as well as its significance for explainability. We then
introduce the recent advances in faithful explanation by grouping approaches
into five categories: similarity methods, analysis of model-internal
structures, backpropagation-based methods, counterfactual intervention, and
self-explanatory models. Each category will be illustrated with its
representative studies, advantages, and shortcomings. Finally, we discuss all
the above methods in terms of their common virtues and limitations, and reflect
on future work directions towards faithful explainability. For researchers
interested in studying interpretability, this survey will offer an accessible
and comprehensive overview of the area, laying the basis for further
exploration. For users hoping to better understand their own models, this
survey will be an introductory manual helping with choosing the most suitable
explanation method(s).
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
