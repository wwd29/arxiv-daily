<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Eden: An Ultra Fast, Provably Secure, and Fully Decentralized Blockchain Interoperability Protocol. (arXiv:2311.17454v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17454">http://arxiv.org/abs/2311.17454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17454]] Eden: An Ultra Fast, Provably Secure, and Fully Decentralized Blockchain Interoperability Protocol(http://arxiv.org/abs/2311.17454)</code></li>
<li>Summary: <p>As the blockchain ecosystem continues to evolve and expand, the need for
seamless interoperability between disparate blockchain networks has become
increasingly paramount. Interoperability not only enhances the functionality
and reach of individual blockchains but also fosters a collaborative
environment that can unlock new possibilities for decentralized applications.
In this paper, we present Eden, an elastic decentralized envoy network that
leverage zero-knowledge MapReduce framework to facilitates ultra-fast and
secure cross-chain communication while maintaining complete decentralization.
We detail the Eden's design choices, its comprehensive security model, and the
innovative mechanisms it incorporates to ensure elasticity and resilience, even
under challenging network conditions.
</p></li>
</ul>

<h3>Title: RACED: Routing in Payment Channel Networks Using Distributed Hash Tables. (arXiv:2311.17668v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17668">http://arxiv.org/abs/2311.17668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17668]] RACED: Routing in Payment Channel Networks Using Distributed Hash Tables(http://arxiv.org/abs/2311.17668)</code></li>
<li>Summary: <p>The Bitcoin scalability problem has led to the development of off-chain
financial mechanisms such as payment channel networks (PCNs) which help users
process transactions of varying amounts, including micro-payment transactions,
without writing each transaction to the blockchain. Since PCNs only allow
path-based transactions, effective, secure routing protocols that find a path
between a sender and receiver are fundamental to PCN operations. In this paper,
we propose RACED, a routing protocol that leverages the idea of Distributed
Hash Tables (DHTs) to route transactions in PCNs in a fast and secure way. Our
experiments on real-world transaction datasets show that RACED gives an average
transaction success ratio of 98.74%, an average pathfinding time of 31.242
seconds, which is $1.65*10^3$, $1.8*10^3$, and $4*10^2$ times faster than three
other recent routing protocols that offer comparable security/privacy
properties. We rigorously analyze and prove the security of RACED in the
Universal Composability framework.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: NeRFTAP: Enhancing Transferability of Adversarial Patches on Face Recognition using Neural Radiance Fields. (arXiv:2311.17332v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17332">http://arxiv.org/abs/2311.17332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17332]] NeRFTAP: Enhancing Transferability of Adversarial Patches on Face Recognition using Neural Radiance Fields(http://arxiv.org/abs/2311.17332)</code></li>
<li>Summary: <p>Face recognition (FR) technology plays a crucial role in various
applications, but its vulnerability to adversarial attacks poses significant
security concerns. Existing research primarily focuses on transferability to
different FR models, overlooking the direct transferability to victim's face
images, which is a practical threat in real-world scenarios. In this study, we
propose a novel adversarial attack method that considers both the
transferability to the FR model and the victim's face image, called NeRFTAP.
Leveraging NeRF-based 3D-GAN, we generate new view face images for the source
and target subjects to enhance transferability of adversarial patches. We
introduce a style consistency loss to ensure the visual similarity between the
adversarial UV map and the target UV map under a 0-1 mask, enhancing the
effectiveness and naturalness of the generated adversarial face images.
Extensive experiments and evaluations on various FR models demonstrate the
superiority of our approach over existing attack techniques. Our work provides
valuable insights for enhancing the robustness of FR systems in practical
adversarial settings.
</p></li>
</ul>

<h3>Title: Query-Relevant Images Jailbreak Large Multi-Modal Models. (arXiv:2311.17600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17600">http://arxiv.org/abs/2311.17600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17600]] Query-Relevant Images Jailbreak Large Multi-Modal Models(http://arxiv.org/abs/2311.17600)</code></li>
<li>Summary: <p>Warning: This paper contains examples of harmful language and images, and
reader discretion is recommended. The security concerns surrounding Large
Language Models (LLMs) have been extensively explored, yet the safety of Large
Multi-Modal Models (LMMs) remains understudied. In our study, we present a
novel visual prompt attack that exploits query-relevant images to jailbreak the
open-source LMMs. Our method creates a composite image from one image generated
by diffusion models and another that displays the text as typography, based on
keywords extracted from a malicious query. We show LLMs can be easily attacked
by our approach, even if the employed Large Language Models are safely aligned.
To evaluate the extent of this vulnerability in open-source LMMs, we have
compiled a substantial dataset encompassing 13 scenarios with a total of 5,040
text-image pairs, using our presented attack technique. Our evaluation of 12
cutting-edge LMMs using this dataset shows the vulnerability of existing
multi-modal models on adversarial attacks. This finding underscores the need
for a concerted effort to strengthen and enhance the safety measures of
open-source LMMs against potential malicious exploits. The resource is
available at \href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.
</p></li>
</ul>

<h3>Title: An Internet-wide Penetration Study on NAT Boxes via TCP/IP Side Channel. (arXiv:2311.17392v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17392">http://arxiv.org/abs/2311.17392</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17392]] An Internet-wide Penetration Study on NAT Boxes via TCP/IP Side Channel(http://arxiv.org/abs/2311.17392)</code></li>
<li>Summary: <p>Network Address Translation (NAT) plays an essential role in shielding
devices inside an internal local area network from direct malicious accesses
from the public Internet. However, recent studies show the possibilities of
penetrating NAT boxes in some specific circumstances. The penetrated NAT box
can be exploited by attackers as a pivot to abuse the otherwise inaccessible
internal network resources, leading to serious security consequences. In this
paper, we aim to conduct an Internet-wide penetration testing on NAT boxes. The
main difference between our study and the previous ones is that ours is based
on the TCP/IP side channels. We explore the TCP/IP side channels in the
research literature, and find that the shared-IPID side channel is the most
suitable for NAT-penetration testing, as it satisfies the three requirements of
our study: generality, ethics, and robustness. Based on this side channel, we
develop an adaptive scanner that can accomplish the Internet-wide scanning in 5
days in a very non-aggressive manner. The evaluation shows that our scanner is
effective in both the controlled network and the real network. Our measurement
results reveal that more than 30,000 network middleboxes are potentially
vulnerable to NAT penetration. They are distributed across 154 countries and
4,146 different organizations, showing that NAT-penetration poses a serious
security threat.
</p></li>
</ul>

<h3>Title: Learning-driven Zero Trust in Distributed Computing Continuum Systems. (arXiv:2311.17447v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17447">http://arxiv.org/abs/2311.17447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17447]] Learning-driven Zero Trust in Distributed Computing Continuum Systems(http://arxiv.org/abs/2311.17447)</code></li>
<li>Summary: <p>Converging Zero Trust (ZT) with learning techniques can solve various
operational and security challenges in Distributed Computing Continuum Systems
(DCCS). Implementing centralized ZT architecture is seen as unsuitable for the
computing continuum (e.g., computing entities with limited connectivity and
visibility, etc.). At the same time, implementing decentralized ZT in the
computing continuum requires understanding infrastructure limitations and novel
approaches to enhance resource access management decisions. To overcome such
challenges, we present a novel learning-driven ZT conceptual architecture
designed for DCCS. We aim to enhance ZT architecture service quality by
incorporating lightweight learning strategies such as Representation Learning
(ReL) and distributing ZT components across the computing continuum. The ReL
helps to improve the decision-making process by predicting threats or untrusted
requests. Through an illustrative example, we show how the learning process
detects and blocks the requests, enhances resource access control, and reduces
network and computation overheads. Lastly, we discuss the conceptual
architecture, processes, and provide a research agenda.
</p></li>
</ul>

<h3>Title: Data Driven Approaches to Cybersecurity Governance for Board Decision-Making -- A Systematic Review. (arXiv:2311.17578v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17578">http://arxiv.org/abs/2311.17578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17578]] Data Driven Approaches to Cybersecurity Governance for Board Decision-Making -- A Systematic Review(http://arxiv.org/abs/2311.17578)</code></li>
<li>Summary: <p>Cybersecurity governance influences the quality of strategic decision-making
to ensure cyber risks are managed effectively. Board of Directors are the
decisions-makers held accountable for managing this risk; however, they lack
adequate and efficient information necessary for making such decisions. In
addition to the myriad of challenges they face, they are often insufficiently
versed in the technology or cybersecurity terminology or not provided with the
correct tools to support them to make sound decisions to govern cybersecurity
effectively. A different approach is needed to ensure BoDs are clear on the
approach the business is taking to build a cyber resilient organization. This
systematic literature review investigates the existing risk measurement
instruments, cybersecurity metrics, and associated models for supporting BoDs.
We identified seven conceptual themes through literature analysis that form the
basis of this study's main contribution. The findings showed that, although
sophisticated cybersecurity tools exist and are developing, there is limited
information for Board of Directors to support them in terms of metrics and
models to govern cybersecurity in a language they understand. The review also
provides some recommendations on theories and models that can be further
investigated to provide support to Board of Directors.
</p></li>
</ul>

<h3>Title: sec-certs: Examining the security certification practice for better vulnerability mitigation. (arXiv:2311.17603v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17603">http://arxiv.org/abs/2311.17603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17603]] sec-certs: Examining the security certification practice for better vulnerability mitigation(http://arxiv.org/abs/2311.17603)</code></li>
<li>Summary: <p>Products certified under security certification frameworks such as Common
Criteria undergo significant scrutiny during the costly certification process.
Yet, critical vulnerabilities, including private key recovery (ROCA, Minerva,
TPM-Fail...), get discovered in certified products with high assurance levels.
Furthermore, assessing which certified products are impacted by such
vulnerabilities is complicated due to the large amount of unstructured
certification-related data and unclear relationships between the certificates.
To address these problems, we conducted a large-scale automated analysis of
Common Criteria and FIPS 140 certificates. We trained unsupervised models to
learn which vulnerabilities from NIST's National Vulnerability Database impact
existing certified products and how certified products reference each other.
Our tooling automates the analysis of tens of thousands of
certification-related documents, extracting machine-readable features where
manual analysis is unattainable. Further, we identify the security requirements
that are associated with products being affected by fewer and less severe
vulnerabilities (on average). This indicates which aspects of certification
correlate with higher security. We demonstrate how our tool can be used for
better vulnerability mitigation on four case studies of known, high-profile
vulnerabilities. All tools and continuously updated results are available at
https://seccerts.org.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Unsupervised Multimodal Deepfake Detection Using Intra- and Cross-Modal Inconsistencies. (arXiv:2311.17088v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17088">http://arxiv.org/abs/2311.17088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17088]] Unsupervised Multimodal Deepfake Detection Using Intra- and Cross-Modal Inconsistencies(http://arxiv.org/abs/2311.17088)</code></li>
<li>Summary: <p>Deepfake videos present an increasing threat to society with potentially
negative impact on criminal justice, democracy, and personal safety and
privacy. Meanwhile, detecting deepfakes, at scale, remains a very challenging
tasks that often requires labeled training data from existing deepfake
generation methods. Further, even the most accurate supervised learning,
deepfake detection methods do not generalize to deepfakes generated using new
generation methods. In this paper, we introduce a novel unsupervised approach
for detecting deepfake videos by measuring of intra- and cross-modal
consistency among multimodal features; specifically visual, audio, and identity
features. The fundamental hypothesis behind the proposed detection method is
that since deepfake generation attempts to transfer the facial motion of one
identity to another, these methods will eventually encounter a trade-off
between motion and identity that enviably leads to detectable inconsistencies.
We validate our method through extensive experimentation, demonstrating the
existence of significant intra- and cross- modal inconsistencies in deepfake
videos, which can be effectively utilized to detect them with high accuracy.
Our proposed method is scalable because it does not require pristine samples at
inference, generalizable because it is trained only on real data, and is
explainable since it can pinpoint the exact location of modality
inconsistencies which are then verifiable by a human expert.
</p></li>
</ul>

<h3>Title: The Symmetric alpha-Stable Privacy Mechanism. (arXiv:2311.17789v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17789">http://arxiv.org/abs/2311.17789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17789]] The Symmetric alpha-Stable Privacy Mechanism(http://arxiv.org/abs/2311.17789)</code></li>
<li>Summary: <p>With the rapid growth of digital platforms, there is increasing apprehension
about how personal data is being collected, stored, and used by various
entities. These concerns range from data breaches and cyber-attacks to
potential misuse of personal information for targeted advertising and
surveillance. As a result, differential privacy (DP) has emerged as a prominent
tool for quantifying a system's level of protection. The Gaussian mechanism is
commonly used because the Gaussian density is closed under convolution, a
common method utilized when aggregating datasets. However, the Gaussian
mechanism only satisfies approximate differential privacy. In this work, we
present novel analysis of the Symmetric alpha-Stable (SaS) mechanism. We prove
that the mechanism is purely differentially private while remaining closed
under convolution. From our analysis, we believe the SaS Mechanism is an
appealing choice for privacy focused applications.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: THInImg: Cross-modal Steganography for Presenting Talking Heads in Images. (arXiv:2311.17177v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17177">http://arxiv.org/abs/2311.17177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17177]] THInImg: Cross-modal Steganography for Presenting Talking Heads in Images(http://arxiv.org/abs/2311.17177)</code></li>
<li>Summary: <p>Cross-modal Steganography is the practice of concealing secret signals in
publicly available cover signals (distinct from the modality of the secret
signals) unobtrusively. While previous approaches primarily concentrated on
concealing a relatively small amount of information, we propose THInImg, which
manages to hide lengthy audio data (and subsequently decode talking head video)
inside an identity image by leveraging the properties of human face, which can
be effectively utilized for covert communication, transmission and copyright
protection. THInImg consists of two parts: the encoder and decoder. Inside the
encoder-decoder pipeline, we introduce a novel architecture that substantially
increase the capacity of hiding audio in images. Moreover, our framework can be
extended to iteratively hide multiple audio clips into an identity image,
offering multiple levels of control over permissions. We conduct extensive
experiments to prove the effectiveness of our method, demonstrating that
THInImg can present up to 80 seconds of high quality talking-head video
(including audio) in an identity image with 160x160 resolution.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Rethinking Mixup for Improving the Adversarial Transferability. (arXiv:2311.17087v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17087">http://arxiv.org/abs/2311.17087</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17087]] Rethinking Mixup for Improving the Adversarial Transferability(http://arxiv.org/abs/2311.17087)</code></li>
<li>Summary: <p>Mixup augmentation has been widely integrated to generate adversarial
examples with superior adversarial transferability when immigrating from a
surrogate model to other models. However, the underlying mechanism influencing
the mixup's effect on transferability remains unexplored. In this work, we
posit that the adversarial examples located at the convergence of decision
boundaries across various categories exhibit better transferability and
identify that Admix tends to steer the adversarial examples towards such
regions. However, we find the constraint on the added image in Admix decays its
capability, resulting in limited transferability. To address such an issue, we
propose a new input transformation-based attack called Mixing the Image but
Separating the gradienT (MIST). Specifically, MIST randomly mixes the input
image with a randomly shifted image and separates the gradient of each loss
item for each mixed image. To counteract the imprecise gradient, MIST
calculates the gradient on several mixed images for each input sample.
Extensive experimental results on the ImageNet dataset demonstrate that MIST
outperforms existing SOTA input transformation-based attacks with a clear
margin on both Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) w/wo defense mechanisms, supporting MIST's high effectiveness and
generality.
</p></li>
</ul>

<h3>Title: RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on Face Recognition. (arXiv:2311.17339v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17339">http://arxiv.org/abs/2311.17339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17339]] RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches on Face Recognition(http://arxiv.org/abs/2311.17339)</code></li>
<li>Summary: <p>Face recognition (FR) systems powered by deep learning have become widely
used in various applications. However, they are vulnerable to adversarial
attacks, especially those based on local adversarial patches that can be
physically applied to real-world objects. In this paper, we propose RADAP, a
robust and adaptive defense mechanism against diverse adversarial patches in
both closed-set and open-set FR systems. RADAP employs innovative techniques,
such as FCutout and F-patch, which use Fourier space sampling masks to improve
the occlusion robustness of the FR model and the performance of the patch
segmenter. Moreover, we introduce an edge-aware binary cross-entropy (EBCE)
loss function to enhance the accuracy of patch detection. We also present the
split and fill (SAF) strategy, which is designed to counter the vulnerability
of the patch segmenter to complete white-box adaptive attacks. We conduct
comprehensive experiments to validate the effectiveness of RADAP, which shows
significant improvements in defense performance against various adversarial
patches, while maintaining clean accuracy higher than that of the undefended
Vanilla model.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks. (arXiv:2311.17128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17128">http://arxiv.org/abs/2311.17128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17128]] Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks(http://arxiv.org/abs/2311.17128)</code></li>
<li>Summary: <p>Recent advancements in Optical Character Recognition (OCR) have been driven
by transformer-based models. OCR systems are critical in numerous high-stakes
domains, yet their vulnerability to adversarial attack remains largely
uncharted territory, raising concerns about security and compliance with
emerging AI regulations. In this work we present a novel framework to assess
the resilience of Transformer-based OCR (TrOCR) models. We develop and assess
algorithms for both targeted and untargeted attacks. For the untargeted case,
we measure the Character Error Rate (CER), while for the targeted case we use
the success ratio. We find that TrOCR is highly vulnerable to untargeted
attacks and somewhat less vulnerable to targeted attacks. On a benchmark
handwriting data set, untargeted attacks can cause a CER of more than 1 without
being noticeable to the eye. With a similar perturbation size, targeted attacks
can lead to success rates of around $25\%$ -- here we attacked single tokens,
requiring TrOCR to output the tenth most likely token from a large vocabulary.
</p></li>
</ul>

<h3>Title: Group-wise Sparse and Explainable Adversarial Attacks. (arXiv:2311.17434v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17434">http://arxiv.org/abs/2311.17434</a></li>
<li>Code URL: https://github.com/wagnermoritz/gse</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17434]] Group-wise Sparse and Explainable Adversarial Attacks(http://arxiv.org/abs/2311.17434)</code></li>
<li>Summary: <p>Sparse adversarial attacks fool deep neural networks (DNNs) through minimal
pixel perturbations, typically regularized by the $\ell_0$ norm. Recent efforts
have replaced this norm with a structural sparsity regularizer, such as the
nuclear group norm, to craft group-wise sparse adversarial attacks. The
resulting perturbations are thus explainable and hold significant practical
relevance, shedding light on an even greater vulnerability of DNNs than
previously anticipated. However, crafting such attacks poses an optimization
challenge, as it involves computing norms for groups of pixels within a
non-convex objective. In this paper, we tackle this challenge by presenting an
algorithm that simultaneously generates group-wise sparse attacks within
semantically meaningful areas of an image. In each iteration, the core
operation of our algorithm involves the optimization of a quasinorm adversarial
loss. This optimization is achieved by employing the $1/2$-quasinorm proximal
operator for some iterations, a method tailored for nonconvex programming.
Subsequently, the algorithm transitions to a projected Nesterov's accelerated
gradient descent with $2$-norm regularization applied to perturbation
magnitudes. We rigorously evaluate the efficacy of our novel attack in both
targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets.
When compared to state-of-the-art methods, our attack consistently results in a
remarkable increase in group-wise sparsity, e.g., an increase of $48.12\%$ on
CIFAR-10 and $40.78\%$ on ImageNet (average case, targeted attack), all while
maintaining lower perturbation magnitudes. Notably, this performance is
complemented by a significantly faster computation time and a $100\%$ attack
success rate.
</p></li>
</ul>

<h3>Title: MMA-Diffusion: MultiModal Attack on Diffusion Models. (arXiv:2311.17516v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17516">http://arxiv.org/abs/2311.17516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17516]] MMA-Diffusion: MultiModal Attack on Diffusion Models(http://arxiv.org/abs/2311.17516)</code></li>
<li>Summary: <p>In recent years, Text-to-Image (T2I) models have seen remarkable
advancements, gaining widespread adoption. However, this progress has
inadvertently opened avenues for potential misuse, particularly in generating
inappropriate or Not-Safe-For-Work (NSFW) content. Our work introduces
MMA-Diffusion, a framework that presents a significant and realistic threat to
the security of T2I models by effectively circumventing current defensive
measures in both open-source models and commercial online services. Unlike
previous approaches, MMA-Diffusion leverages both textual and visual modalities
to bypass safeguards like prompt filters and post-hoc safety checkers, thus
exposing and highlighting the vulnerabilities in existing defense mechanisms.
</p></li>
</ul>

<h3>Title: CLIPC8: Face liveness detection algorithm based on image-text pairs and contrastive learning. (arXiv:2311.17583v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17583">http://arxiv.org/abs/2311.17583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17583]] CLIPC8: Face liveness detection algorithm based on image-text pairs and contrastive learning(http://arxiv.org/abs/2311.17583)</code></li>
<li>Summary: <p>Face recognition technology is widely used in the financial field, and
various types of liveness attack behaviors need to be addressed. Existing
liveness detection algorithms are trained on specific training datasets and
tested on testing datasets, but their performance and robustness in
transferring to unseen datasets are relatively poor. To tackle this issue, we
propose a face liveness detection method based on image-text pairs and
contrastive learning, dividing liveness attack problems in the financial field
into eight categories and using text information to describe the images of
these eight types of attacks. The text encoder and image encoder are used to
extract feature vector representations for the classification description text
and face images, respectively. By maximizing the similarity of positive samples
and minimizing the similarity of negative samples, the model learns shared
representations between images and texts. The proposed method is capable of
effectively detecting specific liveness attack behaviors in certain scenarios,
such as those occurring in dark environments or involving the tampering of ID
card photos. Additionally, it is also effective in detecting traditional
liveness attack methods, such as printing photo attacks and screen remake
attacks. The zero-shot capabilities of face liveness detection on five public
datasets, including NUAA, CASIA-FASD, Replay-Attack, OULU-NPU and MSU-MFSD also
reaches the level of commercial algorithms. The detection capability of
proposed algorithm was verified on 5 types of testing datasets, and the results
show that the method outperformed commercial algorithms, and the detection
rates reached 100% on multiple datasets. Demonstrating the effectiveness and
robustness of introducing image-text pairs and contrastive learning into
liveness detection tasks as proposed in this paper.
</p></li>
</ul>

<h3>Title: TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4. (arXiv:2311.17429v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17429">http://arxiv.org/abs/2311.17429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17429]] TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4(http://arxiv.org/abs/2311.17429)</code></li>
<li>Summary: <p>Prompt-based learning has been widely applied in many low-resource NLP tasks
such as few-shot scenarios. However, this paradigm has been shown to be
vulnerable to backdoor attacks. Most of the existing attack methods focus on
inserting manually predefined templates as triggers in the pre-training phase
to train the victim model and utilize the same triggers in the downstream task
to perform inference, which tends to ignore the transferability and
stealthiness of the templates. In this work, we propose a novel approach of
TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models
via GPT4), which is a data-independent attack method. Specifically, we first
utilize GPT4 to reformulate manual templates to generate tone-strong and normal
templates, and the former are injected into the model as a backdoor trigger in
the pre-training phase. Then, we not only directly employ the above templates
in the downstream task, but also use GPT4 to generate templates with similar
tone to the above templates to carry out transferable attacks. Finally we have
conducted extensive experiments on five NLP datasets and three BERT series
models, with experimental results justifying that our TARGET method has better
attack performance and stealthiness compared to the two-external baseline
methods on direct attacks, and in addition achieves satisfactory attack
capability in the unseen tone-similar templates.
</p></li>
</ul>

<h3>Title: Addressing Membership Inference Attack in Federated Learning with Model Compression. (arXiv:2311.17750v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17750">http://arxiv.org/abs/2311.17750</a></li>
<li>Code URL: https://github.com/negedng/ma-fl-mia</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17750]] Addressing Membership Inference Attack in Federated Learning with Model Compression(http://arxiv.org/abs/2311.17750)</code></li>
<li>Summary: <p>Federated Learning (FL) has been proposed as a privacy-preserving solution
for machine learning. However, recent works have shown that Federated Learning
can leak private client data through membership attacks. In this paper, we show
that the effectiveness of these attacks on the clients negatively correlates
with the size of the client datasets and model complexity. Based on this
finding, we propose model-agnostic Federated Learning as a privacy-enhancing
solution because it enables the use of models of varying complexity in the
clients. To this end, we present $\texttt{MaPP-FL}$, a novel privacy-aware FL
approach that leverages model compression on the clients while keeping a full
model on the server. We compare the performance of $\texttt{MaPP-FL}$ against
state-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and
FEMNIST vision datasets. Our experiments show the effectiveness of
$\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy while
achieving competitive classification accuracies.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Self-Supervised Learning of Whole and Component-Based Semantic Representations for Person Re-Identification. (arXiv:2311.17074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17074">http://arxiv.org/abs/2311.17074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17074]] Self-Supervised Learning of Whole and Component-Based Semantic Representations for Person Re-Identification(http://arxiv.org/abs/2311.17074)</code></li>
<li>Summary: <p>Interactive Segmentation Models (ISMs) like the Segment Anything Model have
significantly improved various computer vision tasks, yet their application to
Person Re-identification (ReID) remains limited. On the other hand, existing
semantic pre-training models for ReID often have limitations like predefined
parsing ranges or coarse semantics. Additionally, ReID and Clothes-Changing
ReID (CC-ReID) are usually treated separately due to their different domains.
This paper investigates whether utilizing precise human-centric semantic
representation can boost the ReID performance and improve the generalization
among various ReID tasks. We propose SemReID, a self-supervised ReID model that
leverages ISMs for adaptive part-based semantic extraction, contributing to the
improvement of ReID performance. SemReID additionally refines its semantic
representation through techniques such as image masking and KoLeo
regularization. Evaluation across three types of ReID datasets -- standard
ReID, CC-ReID, and unconstrained ReID -- demonstrates superior performance
compared to state-of-the-art methods. In addition, recognizing the scarcity of
large person datasets with fine-grained semantics, we introduce the novel
LUPerson-Part dataset to assist ReID methods in acquiring the fine-grained part
semantics for robust performance.
</p></li>
</ul>

<h3>Title: Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models. (arXiv:2311.17091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17091">http://arxiv.org/abs/2311.17091</a></li>
<li>Code URL: https://github.com/zhihelu/ensemble_vlm</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17091]] Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models(http://arxiv.org/abs/2311.17091)</code></li>
<li>Summary: <p>Fine-tuning pre-trained vision-language models (VLMs), e.g., CLIP, for the
open-world generalization has gained increasing popularity due to its practical
value. However, performance advancements are limited when relying solely on
intricate algorithmic designs for a single model, even one exhibiting strong
performance, e.g., CLIP-ViT-B/16. This paper, for the first time, explores the
collaborative potential of leveraging much weaker VLMs to enhance the
generalization of a robust single model. The affirmative findings motivate us
to address the generalization problem from a novel perspective, i.e., ensemble
of pre-trained VLMs. We introduce three customized ensemble strategies, each
tailored to one specific scenario. Firstly, we introduce the zero-shot
ensemble, automatically adjusting the logits of different models based on their
confidence when only pre-trained VLMs are available. Furthermore, for scenarios
with extra few-shot samples, we propose the training-free and tuning ensemble,
offering flexibility based on the availability of computing resources. The
proposed ensemble strategies are evaluated on zero-shot, base-to-new, and
cross-dataset generalization, achieving new state-of-the-art performance.
Notably, this work represents an initial stride toward enhancing the
generalization performance of VLMs via ensemble. The code is available at
https://github.com/zhiheLu/Ensemble_VLM.git.
</p></li>
</ul>

<h3>Title: Improved Prototypical Semi-Supervised Learning with Foundation Models: Prototype Selection, Parametric vMF-SNE Pretraining and Multi-view Pseudolabelling. (arXiv:2311.17093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17093">http://arxiv.org/abs/2311.17093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17093]] Improved Prototypical Semi-Supervised Learning with Foundation Models: Prototype Selection, Parametric vMF-SNE Pretraining and Multi-view Pseudolabelling(http://arxiv.org/abs/2311.17093)</code></li>
<li>Summary: <p>In this paper we present an improved approach to prototypical semi-supervised
learning for computer vision, in the context of leveraging a frozen foundation
model as the backbone of our neural network. As a general tool, we propose
parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to create
mappings with neural networks between high-dimensional latent spaces that
preserve local structure. This enables us to pretrain the projection head of
our network using the high-quality embeddings of the foundation model with
vMF-SNE. We also propose soft multi-view pseudolabels, where predictions across
multiple views are combined to provide a more reliable supervision signal
compared to a consistency or swapped assignment approach. We demonstrate that
these ideas improve upon P}redicting View-Assignments with Support Samples
(PAWS), a current state-of-the-art semi-supervised learning method, as well as
Robust PAWS (RoPAWS), over a range of benchmarking datasets. We also introduce
simple $k$-means prototype selection, a technique that provides superior
performance to other unsupervised label selection approaches in this context.
These changes improve upon PAWS by an average of +2.9% for CIFAR-10 and +5.7%
for CIFAR-100 with four labels per class, and by +15.2% for DeepWeeds, a
particularly challenging dataset for semi-supervised learning. We also achieve
new state-of-the-art results in semi-supervised learning in this small label
regime for CIFAR-10 - 95.8% (+0.7%) and CIFAR-100 - 76.6% (+12.0%).
</p></li>
</ul>

<h3>Title: Robust Transductive Few-shot Learning via Joint Message Passing and Prototype-based Soft-label Propagation. (arXiv:2311.17096v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17096">http://arxiv.org/abs/2311.17096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17096]] Robust Transductive Few-shot Learning via Joint Message Passing and Prototype-based Soft-label Propagation(http://arxiv.org/abs/2311.17096)</code></li>
<li>Summary: <p>Few-shot learning (FSL) aims to develop a learning model with the ability to
generalize to new classes using a few support samples. For transductive FSL
tasks, prototype learning and label propagation methods are commonly employed.
Prototype methods generally first learn the representative prototypes from the
support set and then determine the labels of queries based on the metric
between query samples and prototypes. Label propagation methods try to
propagate the labels of support samples on the constructed graph encoding the
relationships between both support and query samples. This paper aims to
integrate these two principles together and develop an efficient and robust
transductive FSL approach, termed Prototype-based Soft-label Propagation
(PSLP). Specifically, we first estimate the soft-label presentation for each
query sample by leveraging prototypes. Then, we conduct soft-label propagation
on our learned query-support graph. Both steps are conducted progressively to
boost their respective performance. Moreover, to learn effective prototypes for
soft-label estimation as well as the desirable query-support graph for
soft-label propagation, we design a new joint message passing scheme to learn
sample presentation and relational graph jointly. Our PSLP method is
parameter-free and can be implemented very efficiently. On four popular
datasets, our method achieves competitive results on both balanced and
imbalanced settings compared to the state-of-the-art methods. The code will be
released upon acceptance.
</p></li>
</ul>

<h3>Title: DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection. (arXiv:2311.17098v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17098">http://arxiv.org/abs/2311.17098</a></li>
<li>Code URL: https://github.com/daeunfullgrace/dyra</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17098]] DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection(http://arxiv.org/abs/2311.17098)</code></li>
<li>Summary: <p>In object detection, achieving constant accuracy is challenging due to the
variability of object sizes. One possible solution to this problem is to
optimize the input resolution, known as a multi-resolution strategy. Previous
approaches for optimizing resolution are often based on pre-defined resolutions
or a dynamic neural network, but there is a lack of study for run-time
resolution optimization for existing architecture. In this paper, we propose an
adaptive resolution scaling network called DyRA, which comprises convolutions
and transformer encoder blocks, for existing detectors. Our DyRA returns a
scale factor from an input image, which enables instance-specific scaling. This
network is jointly trained with detectors with specially designed loss
functions, namely ParetoScaleLoss and BalanceLoss. The ParetoScaleLoss produces
an adaptive scale factor from the image, while the BalanceLoss optimizes the
scale factor according to localization power for the dataset. The loss function
is designed to minimize accuracy drop about the contrasting objective of small
and large objects. Our experiments on COCO, RetinaNet, Faster-RCNN, FCOS, and
Mask-RCNN achieved 1.3%, 1.1%, 1.3%, and 0.8% accuracy improvement than a
multi-resolution baseline with solely resolution adjustment. The code is
available at https://github.com/DaEunFullGrace/DyRA.git.
</p></li>
</ul>

<h3>Title: Robust Diffusion GAN using Semi-Unbalanced Optimal Transport. (arXiv:2311.17101v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17101">http://arxiv.org/abs/2311.17101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17101]] Robust Diffusion GAN using Semi-Unbalanced Optimal Transport(http://arxiv.org/abs/2311.17101)</code></li>
<li>Summary: <p>Diffusion models, a type of generative model, have demonstrated great
potential for synthesizing highly detailed images. By integrating with GAN,
advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach
real-time performance for expansive practical applications. While DDGAN has
effectively addressed the challenges of generative modeling, namely producing
high-quality samples, covering different data modes, and achieving faster
sampling, it remains susceptible to performance drops caused by datasets that
are corrupted with outlier samples. This work introduces a robust training
technique based on semi-unbalanced optimal transport to mitigate the impact of
outliers effectively. Through comprehensive evaluations, we demonstrate that
our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the
aforementioned generative modeling criteria, i.e., image quality, mode coverage
of distribution, and inference speed, and exhibits improved robustness when
dealing with both clean and corrupted datasets.
</p></li>
</ul>

<h3>Title: Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation. (arXiv:2311.17117v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17117">http://arxiv.org/abs/2311.17117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17117]] Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation(http://arxiv.org/abs/2311.17117)</code></li>
<li>Summary: <p>Character Animation aims to generating character videos from still images
through driving signals. Currently, diffusion models have become the mainstream
in visual generation research, owing to their robust generative capabilities.
However, challenges persist in the realm of image-to-video, especially in
character animation, where temporally maintaining consistency with detailed
information from character remains a formidable problem. In this paper, we
leverage the power of diffusion models and propose a novel framework tailored
for character animation. To preserve consistency of intricate appearance
features from reference image, we design ReferenceNet to merge detail features
via spatial attention. To ensure controllability and continuity, we introduce
an efficient pose guider to direct character's movements and employ an
effective temporal modeling approach to ensure smooth inter-frame transitions
between video frames. By expanding the training data, our approach can animate
arbitrary characters, yielding superior results in character animation compared
to other image-to-video methods. Furthermore, we evaluate our method on
benchmarks for fashion video and human dance synthesis, achieving
state-of-the-art results.
</p></li>
</ul>

<h3>Title: TransNeXt: Robust Foveal Visual Perception for Vision Transformers. (arXiv:2311.17132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17132">http://arxiv.org/abs/2311.17132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17132]] TransNeXt: Robust Foveal Visual Perception for Vision Transformers(http://arxiv.org/abs/2311.17132)</code></li>
<li>Summary: <p>Due to the depth degradation effect in residual connections, many efficient
Vision Transformers models that rely on stacking layers for information
exchange often fail to form sufficient information mixing, leading to unnatural
visual perception. To address this issue, in this paper, we propose Aggregated
Attention, a biomimetic design-based token mixer that simulates biological
foveal vision and continuous eye movement while enabling each token on the
feature map to have a global perception. Furthermore, we incorporate learnable
tokens that interact with conventional queries and keys, which further
diversifies the generation of affinity matrices beyond merely relying on the
similarity between queries and keys. Our approach does not rely on stacking for
information exchange, thus effectively avoiding depth degradation and achieving
natural visual perception. Additionally, we propose Convolutional GLU, a
channel mixer that bridges the gap between GLU and SE mechanism, which empowers
each token to have channel attention based on its nearest neighbor image
features, enhancing local modeling capability and model robustness. We combine
aggregated attention and convolutional GLU to create a new visual backbone
called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves
state-of-the-art performance across multiple model sizes. At a resolution of
$224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing
ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet
accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of
$384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic
segmentation mIoU of 54.7.
</p></li>
</ul>

<h3>Title: UniIR: Training and Benchmarking Universal Multimodal Information Retrievers. (arXiv:2311.17136v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17136">http://arxiv.org/abs/2311.17136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17136]] UniIR: Training and Benchmarking Universal Multimodal Information Retrievers(http://arxiv.org/abs/2311.17136)</code></li>
<li>Summary: <p>Existing information retrieval (IR) models often assume a homogeneous format,
limiting their applicability to diverse user needs, such as searching for
images with text descriptions, searching for a news article with a headline
image, or finding a similar photo with a query image. To approach such
different information-seeking demands, we introduce UniIR, a unified
instruction-guided multimodal retriever capable of handling eight distinct
retrieval tasks across modalities. UniIR, a single retrieval system jointly
trained on ten diverse multimodal-IR datasets, interprets user instructions to
execute various retrieval tasks, demonstrating robust performance across
existing datasets and zero-shot generalization to new tasks. Our experiments
highlight that multi-task training and instruction tuning are keys to UniIR's
generalization ability. Additionally, we construct the M-BEIR, a multimodal
retrieval benchmark with comprehensive results, to standardize the evaluation
of universal multimodal information retrieval.
</p></li>
</ul>

<h3>Title: Explaining CLIP's performance disparities on data from blind/low vision users. (arXiv:2311.17315v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17315">http://arxiv.org/abs/2311.17315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17315]] Explaining CLIP's performance disparities on data from blind/low vision users(http://arxiv.org/abs/2311.17315)</code></li>
<li>Summary: <p>Large multi-modal models (LMMs) hold the potential to usher in a new era of
automated visual assistance for people who are blind or low vision (BLV). Yet,
these models have not been systematically evaluated on data captured by BLV
users. We address this by empirically assessing CLIP, a widely-used LMM likely
to underpin many assistive technologies. Testing 25 CLIP variants in a
zero-shot classification task, we find that their accuracy is 15 percentage
points lower on average for images captured by BLV users than web-crawled
images. This disparity stems from CLIP's sensitivities to 1) image content
(e.g. not recognizing disability objects as well as other objects); 2) image
quality (e.g. not being robust to lighting variation); and 3) text content
(e.g. not recognizing objects described by tactile adjectives as well as visual
ones). We delve deeper with a textual analysis of three common pre-training
datasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content
is rarely mentioned. We then provide three examples that illustrate how the
performance disparities extend to three downstream models underpinned by CLIP:
OWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5
images can mitigate CLIP's quality-of-service disparities for BLV users in some
scenarios, which we discuss alongside a set of other possible mitigations.
</p></li>
</ul>

<h3>Title: Long-tailed multi-label classification with noisy label of thoracic diseases from chest X-ray. (arXiv:2311.17334v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17334">http://arxiv.org/abs/2311.17334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17334]] Long-tailed multi-label classification with noisy label of thoracic diseases from chest X-ray(http://arxiv.org/abs/2311.17334)</code></li>
<li>Summary: <p>Chest X-rays (CXR) often reveal rare diseases, demanding precise diagnosis.
However, current computer-aided diagnosis (CAD) methods focus on common
diseases, leading to inadequate detection of rare conditions due to the absence
of comprehensive datasets. To overcome this, we present a novel benchmark for
long-tailed multi-label classification in CXRs, encapsulating both common and
rare thoracic diseases. Our approach includes developing the "LTML-MIMIC-CXR"
dataset, an augmentation of MIMIC-CXR with 26 additional rare diseases. We
propose a baseline method for this classification challenge, integrating
adaptive negative regularization to address negative logits' over-suppression
in tail classes, and a large loss reconsideration strategy for correcting noisy
labels from automated annotations. Our evaluation on LTML-MIMIC-CXR
demonstrates significant advancements in rare disease detection. This work
establishes a foundation for robust CAD methods, achieving a balance in
identifying a spectrum of thoracic diseases in CXRs. Access to our code and
dataset is provided at:https://github.com/laihaoran/LTML-MIMIC-CXR.
</p></li>
</ul>

<h3>Title: 360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries. (arXiv:2311.17389v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17389">http://arxiv.org/abs/2311.17389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17389]] 360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries(http://arxiv.org/abs/2311.17389)</code></li>
<li>Summary: <p>Portable 360$^\circ$ cameras are becoming a cheap and efficient tool to
establish large visual databases. By capturing omnidirectional views of a
scene, these cameras could expedite building environment models that are
essential for visual localization. However, such an advantage is often
overlooked due to the lack of valuable datasets. This paper introduces a new
benchmark dataset, 360Loc, composed of 360$^\circ$ images with ground truth
poses for visual localization. We present a practical implementation of
360$^\circ$ mapping combining 360$^\circ$ images with lidar data to generate
the ground truth 6DoF poses. 360Loc is the first dataset and benchmark that
explores the challenge of cross-device visual positioning, involving
360$^\circ$ reference frames, and query frames from pinhole, ultra-wide FoV
fisheye, and 360$^\circ$ cameras. We propose a virtual camera approach to
generate lower-FoV query frames from 360$^\circ$ images, which ensures a fair
comparison of performance among different query types in visual localization
tasks. We also extend this virtual camera approach to feature matching-based
and pose regression-based methods to alleviate the performance loss caused by
the cross-device domain gap, and evaluate its effectiveness against
state-of-the-art baselines. We demonstrate that omnidirectional visual
localization is more robust in challenging large-scale scenes with symmetries
and repetitive structures. These results provide new insights into 360-camera
mapping and omnidirectional visual localization with cross-device queries.
</p></li>
</ul>

<h3>Title: SpeechAct: Towards Generating Whole-body Motion from Speech. (arXiv:2311.17425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17425">http://arxiv.org/abs/2311.17425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17425]] SpeechAct: Towards Generating Whole-body Motion from Speech(http://arxiv.org/abs/2311.17425)</code></li>
<li>Summary: <p>This paper addresses the problem of generating whole-body motion from speech.
Despite great successes, prior methods still struggle to produce reasonable and
diverse whole-body motions from speech. This is due to their reliance on
suboptimal representations and a lack of strategies for generating diverse
results. To address these challenges, we present a novel hybrid point
representation to achieve accurate and continuous motion generation, e.g.,
avoiding foot skating, and this representation can be transformed into an
easy-to-use representation, i.e., SMPL-X body mesh, for many applications. To
generate whole-body motion from speech, for facial motion, closely tied to the
audio signal, we introduce an encoder-decoder architecture to achieve
deterministic outcomes. However, for the body and hands, which have weaker
connections to the audio signal, we aim to generate diverse yet reasonable
motions. To boost diversity in motion generation, we propose a contrastive
motion learning method to encourage the model to produce more distinctive
representations. Specifically, we design a robust VQ-VAE to learn a quantized
motion codebook using our hybrid representation. Then, we regress the motion
representation from the audio signal by a translation model employing our
contrastive motion learning method. Experimental results validate the superior
performance and the correctness of our model. The project page is available for
research purposes at <a href="http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct.">this http URL</a>
</p></li>
</ul>

<h3>Title: DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Diffusion Model. (arXiv:2311.17456v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17456">http://arxiv.org/abs/2311.17456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17456]] DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Diffusion Model(http://arxiv.org/abs/2311.17456)</code></li>
<li>Summary: <p>Scene flow estimation, which aims to predict per-point 3D displacements of
dynamic scenes, is a fundamental task in the computer vision field. However,
previous works commonly suffer from unreliable correlation caused by locally
constrained searching ranges, and struggle with accumulated inaccuracy arising
from the coarse-to-fine structure. To alleviate these problems, we propose a
novel uncertainty-aware scene flow estimation network (DifFlow3D) with the
diffusion probabilistic model. Iterative diffusion-based refinement is designed
to enhance the correlation robustness and resilience to challenging cases,
e.g., dynamics, noisy inputs, repetitive patterns, etc. To restrain the
generation diversity, three key flow-related features are leveraged as
conditions in our diffusion model. Furthermore, we also develop an uncertainty
estimation module within diffusion to evaluate the reliability of estimated
scene flow. Our DifFlow3D achieves state-of-the-art performance, with 6.7\% and
19.1\% EPE3D reduction respectively on FlyingThings3D and KITTI 2015 datasets.
Notably, our method achieves an unprecedented millimeter-level accuracy
(0.0089m in EPE3D) on the KITTI dataset. Additionally, our diffusion-based
refinement paradigm can be readily integrated as a plug-and-play module into
existing scene flow networks, significantly increasing their estimation
accuracy. Codes will be released later.
</p></li>
</ul>

<h3>Title: StructRe: Rewriting for Structured Shape Modeling. (arXiv:2311.17510v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17510">http://arxiv.org/abs/2311.17510</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17510]] StructRe: Rewriting for Structured Shape Modeling(http://arxiv.org/abs/2311.17510)</code></li>
<li>Summary: <p>Man-made 3D shapes are naturally organized in parts and hierarchies; such
structures provide important constraints for shape reconstruction and
generation. Modeling shape structures is difficult, because there can be
multiple hierarchies for a given shape, causing ambiguity, and across different
categories the shape structures are correlated with semantics, limiting
generalization. We present StructRe, a structure rewriting system, as a novel
approach to structured shape modeling. Given a 3D object represented by points
and components, StructRe can rewrite it upward into more concise structures, or
downward into more detailed structures; by iterating the rewriting process,
hierarchies are obtained. Such a localized rewriting process enables
probabilistic modeling of ambiguous structures and robust generalization across
object categories. We train StructRe on PartNet data and show its
generalization to cross-category and multiple object hierarchies, and test its
extension to ShapeNet. We also demonstrate the benefits of probabilistic and
generalizable structure modeling for shape reconstruction, generation and
editing tasks.
</p></li>
</ul>

<h3>Title: Improving Stability during Upsampling -- on the Importance of Spatial Context. (arXiv:2311.17524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17524">http://arxiv.org/abs/2311.17524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17524]] Improving Stability during Upsampling -- on the Importance of Spatial Context(http://arxiv.org/abs/2311.17524)</code></li>
<li>Summary: <p>State-of-the-art models for pixel-wise prediction tasks such as image
restoration, image segmentation, or disparity estimation, involve several
stages of data resampling, in which the resolution of feature maps is first
reduced to aggregate information and then sequentially increased to generate a
high-resolution output. Several previous works have investigated the effect of
artifacts that are invoked during downsampling and diverse cures have been
proposed that facilitate to improve prediction stability and even robustness
for image classification. However, equally relevant, artifacts that arise
during upsampling have been less discussed. This is significantly relevant as
upsampling and downsampling approaches face fundamentally different challenges.
While during downsampling, aliases and artifacts can be reduced by blurring
feature maps, the emergence of fine details is crucial during upsampling.
Blurring is therefore not an option and dedicated operations need to be
considered. In this work, we are the first to explore the relevance of context
during upsampling by employing convolutional upsampling operations with
increasing kernel size while keeping the encoder unchanged. We find that
increased kernel sizes can in general improve the prediction stability in tasks
such as image restoration or image segmentation, while a block that allows for
a combination of small-size kernels for fine details and large-size kernels for
artifact removal and increased context yields the best results.
</p></li>
</ul>

<h3>Title: VINNA for Neonates -- Orientation Independence through Latent Augmentations. (arXiv:2311.17546v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17546">http://arxiv.org/abs/2311.17546</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17546]] VINNA for Neonates -- Orientation Independence through Latent Augmentations(http://arxiv.org/abs/2311.17546)</code></li>
<li>Summary: <p>Fast and accurate segmentation of neonatal brain images is highly desired to
better understand and detect changes during development and disease. Yet, the
limited availability of ground truth datasets, lack of standardized acquisition
protocols, and wide variations of head positioning pose challenges for method
development. A few automated image analysis pipelines exist for newborn brain
MRI segmentation, but they often rely on time-consuming procedures and require
resampling to a common resolution, subject to loss of information due to
interpolation and down-sampling. Without registration and image resampling,
variations with respect to head positions and voxel resolutions have to be
addressed differently. In deep-learning, external augmentations are
traditionally used to artificially expand the representation of spatial
variability, increasing the training dataset size and robustness. However,
these transformations in the image space still require resampling, reducing
accuracy specifically in the context of label interpolation. We recently
introduced the concept of resolution-independence with the Voxel-size
Independent Neural Network framework, VINN. Here, we extend this concept by
additionally shifting all rigid-transforms into the network architecture with a
four degree of freedom (4-DOF) transform module, enabling resolution-aware
internal augmentations (VINNA). In this work we show that VINNA (i)
significantly outperforms state-of-the-art external augmentation approaches,
(ii) effectively addresses the head variations present specifically in newborn
datasets, and (iii) retains high segmentation accuracy across a range of
resolutions (0.5-1.0 mm). The 4-DOF transform module is a powerful, general
approach to implement spatial augmentation without requiring image or label
interpolation. The specific network application to newborns will be made
publicly available as VINNA4neonates.
</p></li>
</ul>

<h3>Title: LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17593">http://arxiv.org/abs/2311.17593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17593]] LanGWM: Language Grounded World Model(http://arxiv.org/abs/2311.17593)</code></li>
<li>Summary: <p>Recent advances in deep reinforcement learning have showcased its potential
in tackling complex tasks. However, experiments on visual control tasks have
revealed that state-of-the-art reinforcement learning models struggle with
out-of-distribution generalization. Conversely, expressing higher-level
concepts and global contexts is relatively easy using language.
</p>
<p>Building upon recent success of the large language models, our main objective
is to improve the state abstraction technique in reinforcement learning by
leveraging language for robust action selection. Specifically, we focus on
learning language-grounded visual features to enhance the world model learning,
a model-based reinforcement learning technique.
</p>
<p>To enforce our hypothesis explicitly, we mask out the bounding boxes of a few
objects in the image observation and provide the text prompt as descriptions
for these masked objects. Subsequently, we predict the masked objects along
with the surrounding regions as pixel reconstruction, similar to the
transformer-based masked autoencoder approach.
</p>
<p>Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art
performance in out-of-distribution test at the 100K interaction steps
benchmarks of iGibson point navigation tasks. Furthermore, our proposed
technique of explicit language-grounded visual representation learning has the
potential to improve models for human-robot interaction because our extracted
visual features are language grounded.
</p></li>
</ul>

<h3>Title: Topology-Preserving Adversarial Training. (arXiv:2311.17607v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17607">http://arxiv.org/abs/2311.17607</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17607]] Topology-Preserving Adversarial Training(http://arxiv.org/abs/2311.17607)</code></li>
<li>Summary: <p>Despite the effectiveness in improving the robustness of neural networks,
adversarial training has suffered from the natural accuracy degradation
problem, i.e., accuracy on natural samples has reduced significantly. In this
study, we reveal that natural accuracy degradation is highly related to the
disruption of the natural sample topology in the representation space by
quantitative and qualitative experiments. Based on this observation, we propose
Topology-pReserving Adversarial traINing (TRAIN) to alleviate the problem by
preserving the topology structure of natural samples from a standard model
trained only on natural samples during adversarial training. As an additional
regularization, our method can easily be combined with various popular
adversarial training algorithms in a plug-and-play manner, taking advantage of
both sides. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet
show that our proposed method achieves consistent and significant improvements
over various strong baselines in most cases. Specifically, without additional
data, our proposed method achieves up to 8.78% improvement in natural accuracy
and 4.50% improvement in robust accuracy.
</p></li>
</ul>

<h3>Title: Adversarial Robust Memory-Based Continual Learner. (arXiv:2311.17608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17608">http://arxiv.org/abs/2311.17608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17608]] Adversarial Robust Memory-Based Continual Learner(http://arxiv.org/abs/2311.17608)</code></li>
<li>Summary: <p>Despite the remarkable advances that have been made in continual learning,
the adversarial vulnerability of such methods has not been fully discussed. We
delve into the adversarial robustness of memory-based continual learning
algorithms and observe limited robustness improvement by directly applying
adversarial training techniques. Preliminary studies reveal the twin challenges
for building adversarial robust continual learners: accelerated forgetting in
continual learning and gradient obfuscation in adversarial robustness. In this
study, we put forward a novel adversarial robust memory-based continual learner
that adjusts data logits to mitigate the forgetting of pasts caused by
adversarial samples. Furthermore, we devise a gradient-based data selection
mechanism to overcome the gradient obfuscation caused by limited stored data.
The proposed approach can widely integrate with existing memory-based continual
learning as well as adversarial training algorithms in a plug-and-play way.
Extensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate
the effectiveness of our approach, achieving up to 8.13% higher accuracy for
adversarial data.
</p></li>
</ul>

<h3>Title: GenZI: Zero-Shot 3D Human-Scene Interaction Generation. (arXiv:2311.17737v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17737">http://arxiv.org/abs/2311.17737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17737]] GenZI: Zero-Shot 3D Human-Scene Interaction Generation(http://arxiv.org/abs/2311.17737)</code></li>
<li>Summary: <p>Can we synthesize 3D humans interacting with scenes without learning from any
3D human-scene interaction data? We propose GenZI, the first zero-shot approach
to generating 3D human-scene interactions. Key to GenZI is our distillation of
interaction priors from large vision-language models (VLMs), which have learned
a rich semantic space of 2D human-scene compositions. Given a natural language
description and a coarse point location of the desired interaction in a 3D
scene, we first leverage VLMs to imagine plausible 2D human interactions
inpainted into multiple rendered views of the scene. We then formulate a robust
iterative optimization to synthesize the pose and shape of a 3D human model in
the scene, guided by consistency with the 2D interaction hypotheses. In
contrast to existing learning-based approaches, GenZI circumvents the
conventional need for captured 3D interaction data, and allows for flexible
control of the 3D interaction synthesis with easy-to-use text prompts.
Extensive experiments show that our zero-shot approach has high flexibility and
generality, making it applicable to diverse scene types, including both indoor
and outdoor environments.
</p></li>
</ul>

<h3>Title: RETSim: Resilient and Efficient Text Similarity. (arXiv:2311.17264v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17264">http://arxiv.org/abs/2311.17264</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17264]] RETSim: Resilient and Efficient Text Similarity(http://arxiv.org/abs/2311.17264)</code></li>
<li>Summary: <p>This paper introduces RETSim (Resilient and Efficient Text Similarity), a
lightweight, multilingual deep learning model trained to produce robust metric
embeddings for near-duplicate text retrieval, clustering, and dataset
deduplication tasks. We demonstrate that RETSim is significantly more robust
and accurate than MinHash and neural text embeddings, achieving new
state-of-the-art performance on dataset deduplication, adversarial text
retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D
benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual,
near-duplicate text retrieval capabilities under adversarial settings. RETSim
and the W4NT3D benchmark are open-sourced under the MIT License at
https://github.com/google/unisim.
</p></li>
</ul>

<h3>Title: Elo Uncovered: Robustness and Best Practices in Language Model Evaluation. (arXiv:2311.17295v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17295">http://arxiv.org/abs/2311.17295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17295]] Elo Uncovered: Robustness and Best Practices in Language Model Evaluation(http://arxiv.org/abs/2311.17295)</code></li>
<li>Summary: <p>In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through "A vs B" paired
comparisons. However, while popular, the system's suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system's hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.
</p></li>
</ul>

<h3>Title: Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17400">http://arxiv.org/abs/2311.17400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17400]] Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention(http://arxiv.org/abs/2311.17400)</code></li>
<li>Summary: <p>Transformer-based models, such as BERT and GPT, have been widely adopted in
natural language processing (NLP) due to their exceptional performance.
However, recent studies show their vulnerability to textual adversarial attacks
where the model's output can be misled by intentionally manipulating the text
inputs. Despite various methods that have been proposed to enhance the model's
robustness and mitigate this vulnerability, many require heavy consumption
resources (e.g., adversarial training) or only provide limited protection
(e.g., defensive dropout). In this paper, we propose a novel method called
dynamic attention, tailored for the transformer architecture, to enhance the
inherent robustness of the model itself against various adversarial attacks.
Our method requires no downstream task knowledge and does not incur additional
costs. The proposed dynamic attention consists of two modules: (I) attention
rectification, which masks or weakens the attention value of the chosen tokens,
and (ii) dynamic modeling, which dynamically builds the set of candidate
tokens. Extensive experiments demonstrate that dynamic attention significantly
mitigates the impact of adversarial attacks, improving up to 33\% better
performance than previous methods against widely-used adversarial attacks. The
model-level design of dynamic attention enables it to be easily combined with
other defense methods (e.g., adversarial training) to further enhance the
model's robustness. Furthermore, we demonstrate that dynamic attention
preserves the state-of-the-art robustness space of the original model compared
to other dynamic modeling methods.
</p></li>
</ul>

<h3>Title: SenTest: Evaluating Robustness of Sentence Encoders. (arXiv:2311.17722v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17722">http://arxiv.org/abs/2311.17722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17722]] SenTest: Evaluating Robustness of Sentence Encoders(http://arxiv.org/abs/2311.17722)</code></li>
<li>Summary: <p>Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.
</p></li>
</ul>

<h3>Title: Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond. (arXiv:2311.17133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17133">http://arxiv.org/abs/2311.17133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17133]] Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond(http://arxiv.org/abs/2311.17133)</code></li>
<li>Summary: <p>This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.
</p></li>
</ul>

<h3>Title: Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play. (arXiv:2311.17190v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17190">http://arxiv.org/abs/2311.17190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17190]] Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play(http://arxiv.org/abs/2311.17190)</code></li>
<li>Summary: <p>Recent advances in Competitive Self-Play (CSP) have achieved, or even
surpassed, human level performance in complex game environments such as Dota 2
and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL).
One core component of these methods relies on creating a pool of learning
agents -- consisting of the Main Agent, past versions of this agent, and
Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main
Agents. A key drawback of these approaches is the large computational cost and
physical time that is required to train the system, making them impractical to
deploy in highly iterative real-life settings such as video game productions.
In this paper, we propose the Minimax Exploiter, a game theoretic approach to
exploiting Main Agents that leverages knowledge of its opponents, leading to
significant increases in data efficiency. We validate our approach in a
diversity of settings, including simple turn based games, the arcade learning
environment, and For Honor, a modern video game. The Minimax Exploiter
consistently outperforms strong baselines, demonstrating improved stability and
data efficiency, leading to a robust CSP-MARL method that is both flexible and
easy to deploy.
</p></li>
</ul>

<h3>Title: Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning. (arXiv:2311.17565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17565">http://arxiv.org/abs/2311.17565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17565]] Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning(http://arxiv.org/abs/2311.17565)</code></li>
<li>Summary: <p>In goal-conditioned reinforcement learning (GCRL), sparse rewards present
significant challenges, often obstructing efficient learning. Although
multi-step GCRL can boost this efficiency, it can also lead to off-policy
biases in target values. This paper dives deep into these biases, categorizing
them into two distinct categories: "shooting" and "shifting". Recognizing that
certain behavior policies can hasten policy refinement, we present solutions
designed to capitalize on the positive aspects of these biases while minimizing
their drawbacks, enabling the use of larger step sizes to speed up GCRL. An
empirical study demonstrates that our approach ensures a resilient and robust
improvement, even in ten-step learning scenarios, leading to superior learning
efficiency and performance that generally surpass the baseline and several
state-of-the-art multi-step GCRL benchmarks.
</p></li>
</ul>

<h3>Title: Marginal Laplacian Score. (arXiv:2311.17795v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17795">http://arxiv.org/abs/2311.17795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17795]] Marginal Laplacian Score(http://arxiv.org/abs/2311.17795)</code></li>
<li>Summary: <p>High-dimensional imbalanced data poses a machine learning challenge. In the
absence of sufficient or high-quality labels, unsupervised feature selection
methods are crucial for the success of subsequent algorithms. Therefore, there
is a growing need for unsupervised feature selection algorithms focused on
imbalanced data. Thus, we propose a Marginal Laplacian Score (MLS) a
modification of the well-known Laplacian Score (LS) to be better suited for
imbalance data. We introduce an assumption that the minority class or anomalous
appear more frequently in the margin of the features. Consequently, MLS aims to
preserve the local structure of the data set's margin. As MLS is better suited
for handling imbalanced data, we propose its integration into modern feature
selection methods that utilize the Laplacian score. We integrate the MLS
algorithm into the Differentiable Unsupervised Feature Selection (DUFS),
resulting in DUFS-MLS. The proposed methods demonstrate robust and improved
performance on synthetic and public data sets.
</p></li>
</ul>

<h3>Title: On the Adversarial Robustness of Graph Contrastive Learning Methods. (arXiv:2311.17853v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17853">http://arxiv.org/abs/2311.17853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17853]] On the Adversarial Robustness of Graph Contrastive Learning Methods(http://arxiv.org/abs/2311.17853)</code></li>
<li>Summary: <p>Contrastive learning (CL) has emerged as a powerful framework for learning
representations of images and text in a self-supervised manner while enhancing
model robustness against adversarial attacks. More recently, researchers have
extended the principles of contrastive learning to graph-structured data,
giving birth to the field of graph contrastive learning (GCL). However, whether
GCL methods can deliver the same advantages in adversarial robustness as their
counterparts in the image and text domains remains an open question. In this
paper, we introduce a comprehensive robustness evaluation protocol tailored to
assess the robustness of GCL models. We subject these models to adaptive
adversarial attacks targeting the graph structure, specifically in the evasion
scenario. We evaluate node and graph classification tasks using diverse
real-world datasets and attack strategies. With our work, we aim to offer
insights into the robustness of GCL methods and hope to open avenues for
potential future research directions.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking. (arXiv:2311.17085v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17085">http://arxiv.org/abs/2311.17085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17085]] Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking(http://arxiv.org/abs/2311.17085)</code></li>
<li>Summary: <p>Single object tracking aims to locate one specific target in video sequences,
given its initial state. Classical trackers rely solely on visual cues,
restricting their ability to handle challenges such as appearance variations,
ambiguity, and distractions. Hence, Vision-Language (VL) tracking has emerged
as a promising approach, incorporating language descriptions to directly
provide high-level semantics and enhance tracking performance. However, current
VL trackers have not fully exploited the power of VL learning, as they suffer
from limitations such as heavily relying on off-the-shelf backbones for feature
extraction, ineffective VL fusion designs, and the absence of VL-related loss
functions. Consequently, we present a novel tracker that progressively explores
target-centric semantics for VL tracking. Specifically, we propose the first
Synchronous Learning Backbone (SLB) for VL tracking, which consists of two
novel modules: the Target Enhance Module (TEM) and the Semantic Aware Module
(SAM). These modules enable the tracker to perceive target-related semantics
and comprehend the context of both visual and textual modalities at the same
pace, facilitating VL feature extraction and fusion at different semantic
levels. Moreover, we devise the dense matching loss to further strengthen
multi-modal representation learning. Extensive experiments on VL tracking
datasets demonstrate the superiority and effectiveness of our methods.
</p></li>
</ul>

<h3>Title: Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models. (arXiv:2311.17095v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17095">http://arxiv.org/abs/2311.17095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17095]] Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models(http://arxiv.org/abs/2311.17095)</code></li>
<li>Summary: <p>From an enormous amount of image-text pairs, large-scale vision-language
models (VLMs) learn to implicitly associate image regions with words, which is
vital for tasks such as image captioning and visual question answering.
However, leveraging such pre-trained models for open-vocabulary semantic
segmentation remains a challenge. In this paper, we propose a simple, yet
extremely effective, training-free technique, Plug-and-Play Open-Vocabulary
Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with
direct text-to-image cross-attention and an image-text matching loss to produce
semantic segmentation. However, cross-attention alone tends to over-segment,
whereas cross-attention plus GradCAM tend to under-segment. To alleviate this
issue, we introduce Salience Dropout; by iteratively dropping patches that the
model is most attentive to, we are able to better resolve the entire extent of
the segmentation mask. Compared to existing techniques, the proposed method
does not require any neural network training and performs hyperparameter tuning
without the need for any segmentation annotations, even for a validation set.
PnP-OVSS demonstrates substantial improvements over a comparable baseline
(+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS
COCO, +2.4% mIoU on COCO Stuff) and even outperforms most baselines that
conduct additional network training on top of pretrained VLMs.
</p></li>
</ul>

<h3>Title: AdaFocus: Towards End-to-end Weakly Supervised Learning for Long-Video Action Understanding. (arXiv:2311.17118v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17118">http://arxiv.org/abs/2311.17118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17118]] AdaFocus: Towards End-to-end Weakly Supervised Learning for Long-Video Action Understanding(http://arxiv.org/abs/2311.17118)</code></li>
<li>Summary: <p>Developing end-to-end models for long-video action understanding tasks
presents significant computational and memory challenges. Existing works
generally build models on long-video features extracted by off-the-shelf action
recognition models, which are trained on short-video datasets in different
domains, making the extracted features suffer domain discrepancy. To avoid
this, action recognition models can be end-to-end trained on clips, which are
trimmed from long videos and labeled using action interval annotations. Such
fully supervised annotations are expensive to collect. Thus, a weakly
supervised method is needed for long-video action understanding at scale. Under
the weak supervision setting, action labels are provided for the whole video
without precise start and end times of the action clip. To this end, we propose
an AdaFocus framework. AdaFocus estimates the spike-actionness and temporal
positions of actions, enabling it to adaptively focus on action clips that
facilitate better training without the need for precise annotations.
Experiments on three long-video datasets show its effectiveness. Remarkably, on
two of datasets, models trained with AdaFocus under weak supervision outperform
those trained under full supervision. Furthermore, we form a weakly supervised
feature extraction pipeline with our AdaFocus, which enables significant
improvements on three long-video action understanding tasks.
</p></li>
</ul>

<h3>Title: BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling. (arXiv:2311.17218v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17218">http://arxiv.org/abs/2311.17218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17218]] BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling(http://arxiv.org/abs/2311.17218)</code></li>
<li>Summary: <p>Like masked language modeling (MLM) in natural language processing, masked
image modeling (MIM) aims to extract valuable insights from image patches to
enhance the feature extraction capabilities of the underlying deep neural
network (DNN). Contrasted with other training paradigms like supervised
learning and unsupervised contrastive learning, masked image modeling (MIM)
pretraining typically demands significant computational resources in order to
manage large training data batches (e.g., 4096). The significant memory and
computation requirements pose a considerable challenge to its broad adoption.
To mitigate this, we introduce a novel learning framework,
termed~\textit{Block-Wise Masked Image Modeling} (BIM). This framework involves
decomposing the MIM tasks into several sub-tasks with independent computation
patterns, resulting in block-wise back-propagation operations instead of the
traditional end-to-end approach. Our proposed BIM maintains superior
performance compared to conventional MIM while greatly reducing peak memory
consumption. Moreover, BIM naturally enables the concurrent training of
numerous DNN backbones of varying depths. This leads to the creation of
multiple trained DNN backbones, each tailored to different hardware platforms
with distinct computing capabilities. This approach significantly reduces
computational costs in comparison with training each DNN backbone individually.
Our framework offers a promising solution for resource constrained training of
MIM.
</p></li>
</ul>

<h3>Title: PillarNeSt: Embracing Backbone Scaling and Pretraining for Pillar-based 3D Object Detection. (arXiv:2311.17770v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17770">http://arxiv.org/abs/2311.17770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17770]] PillarNeSt: Embracing Backbone Scaling and Pretraining for Pillar-based 3D Object Detection(http://arxiv.org/abs/2311.17770)</code></li>
<li>Summary: <p>This paper shows the effectiveness of 2D backbone scaling and pretraining for
pillar-based 3D object detectors. Pillar-based methods mainly employ randomly
initialized 2D convolution neural network (ConvNet) for feature extraction and
fail to enjoy the benefits from the backbone scaling and pretraining in the
image domain. To show the scaling-up capacity in point clouds, we introduce the
dense ConvNet pretrained on large-scale image datasets (e.g., ImageNet) as the
2D backbone of pillar-based detectors. The ConvNets are adaptively designed
based on the model size according to the specific features of point clouds,
such as sparsity and irregularity. Equipped with the pretrained ConvNets, our
proposed pillar-based detector, termed PillarNeSt, outperforms the existing 3D
object detectors by a large margin on the nuScenes and Argoversev2 datasets.
Our code shall be released upon acceptance.
</p></li>
</ul>

<h3>Title: General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports. (arXiv:2311.17213v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17213">http://arxiv.org/abs/2311.17213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17213]] General-Purpose vs(http://arxiv.org/abs/2311.17213)</code></li>
<li>Summary: <p>Radiologists produce unstructured data that could be valuable for clinical
care when consumed by information systems. However, variability in style limits
usage. Study compares performance of system using domain-adapted language model
(RadLing) and general-purpose large language model (GPT-4) in extracting common
data elements (CDE) from thoracic radiology reports. Three radiologists
annotated a retrospective dataset of 1300 thoracic reports (900 training, 400
test) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate
embeddings for sentences and identify CDEs using cosine-similarity, which were
mapped to values using light-weight mapper. GPT-4 system used OpenAI's
general-purpose embeddings to identify relevant CDEs and used GPT-4 to map to
values. The output CDE:value pairs were compared to the reference standard; an
identical match was considered true positive. Precision (positive predictive
value) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall
(sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4;
the difference was statistically significant (P&lt;.001). RadLing's domain-adapted
embeddings were more sensitive in CDE identification (95% vs 71%) and its
light-weight mapper had comparable precision in value assignment (95.4% vs
95.0%). RadLing system exhibited higher performance than GPT-4 system in
extracting CDEs from radiology reports. RadLing system's domain-adapted
embeddings outperform general-purpose embeddings from OpenAI in CDE
identification and its light-weight value mapper achieves comparable precision
to large GPT-4. RadLing system offers operational advantages including local
deployment and reduced runtime costs. Domain-adapted RadLing system surpasses
GPT-4 system in extracting common data elements from radiology reports, while
providing benefits of local deployment and lower costs.
</p></li>
</ul>

<h3>Title: Optimal EEG Electrode Set for Emotion Recognition From Brain Signals: An Empirical Quest. (arXiv:2311.17204v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17204">http://arxiv.org/abs/2311.17204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17204]] Optimal EEG Electrode Set for Emotion Recognition From Brain Signals: An Empirical Quest(http://arxiv.org/abs/2311.17204)</code></li>
<li>Summary: <p>The human brain is a complex organ, still completely undiscovered, that
controls almost all the parts of the body. Apart from survival, the human brain
stimulates emotions. Recent research indicates that brain signals can be very
effective for emotion recognition. However, which parts of the brain exhibit
most of the emotions is still under-explored. In this study, we empirically
analyze the contribution of each part of the brain in exhibiting emotions. We
use the DEAP dataset to find the most optimal electrode set which eventually
leads to the effective brain part associated with emotions. We use Fast Fourier
Transformation for effective feature extraction and a 1D-CNN with residual
connection for classification. Though 32 electrodes from the DEAP dataset got
an accuracy of 97.34%, only 12 electrodes (F7, P8, O1, F8, C4, T7, PO3, Fp1,
Fp2, O2, P3, and Fz) achieve 95.81% accuracy. This study also shows that adding
more than 10 electrodes does not improve performance significantly. Moreover,
the frontal lobe is the most important for recognizing emotion.
</p></li>
</ul>

<h3>Title: Improving embedding of graphs with missing data by soft manifolds. (arXiv:2311.17598v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17598">http://arxiv.org/abs/2311.17598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17598]] Improving embedding of graphs with missing data by soft manifolds(http://arxiv.org/abs/2311.17598)</code></li>
<li>Summary: <p>Embedding graphs in continous spaces is a key factor in designing and
developing algorithms for automatic information extraction to be applied in
diverse tasks (e.g., learning, inferring, predicting). The reliability of graph
embeddings directly depends on how much the geometry of the continuous space
matches the graph structure. Manifolds are mathematical structure that can
enable to incorporate in their topological spaces the graph characteristics,
and in particular nodes distances. State-of-the-art of manifold-based graph
embedding algorithms take advantage of the assumption that the projection on a
tangential space of each point in the manifold (corresponding to a node in the
graph) would locally resemble a Euclidean space. Although this condition helps
in achieving efficient analytical solutions to the embedding problem, it does
not represent an adequate set-up to work with modern real life graphs, that are
characterized by weighted connections across nodes often computed over sparse
datasets with missing records. In this work, we introduce a new class of
manifold, named soft manifold, that can solve this situation. In particular,
soft manifolds are mathematical structures with spherical symmetry where the
tangent spaces to each point are hypocycloids whose shape is defined according
to the velocity of information propagation across the data points. Using soft
manifolds for graph embedding, we can provide continuous spaces to pursue any
task in data analysis over complex datasets. Experimental results on
reconstruction tasks on synthetic and real datasets show how the proposed
approach enable more accurate and reliable characterization of graphs in
continuous spaces with respect to the state-of-the-art.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Fine-Tuning of Foundation Models via Probabilistic Masking. (arXiv:2311.17299v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17299">http://arxiv.org/abs/2311.17299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17299]] Federated Fine-Tuning of Foundation Models via Probabilistic Masking(http://arxiv.org/abs/2311.17299)</code></li>
<li>Summary: <p>Foundation Models (FMs) have revolutionized machine learning with their
adaptability and high performance across tasks; yet, their integration into
Federated Learning (FL) is challenging due to substantial communication
overhead from their extensive parameterization. Current communication-efficient
FL strategies, such as gradient compression, reduce bitrates to around $1$
bit-per-parameter (bpp). However, these approaches fail to harness the
characteristics of FMs, with their large number of parameters still posing a
challenge to communication efficiency, even at these bitrate regimes. In this
work, we present DeltaMask, a novel method that efficiently fine-tunes FMs in
FL at an ultra-low bitrate, well below 1 bpp. DeltaMask employs stochastic
masking to detect highly effective subnetworks within FMs and leverage
stochasticity and sparsity in client masks to compress updates into a compact
grayscale image using probabilistic filters, deviating from traditional weight
training approaches. Our comprehensive evaluations across various datasets and
architectures demonstrate DeltaMask efficiently achieves bitrates as low as
0.09 bpp, enhancing communication efficiency while maintaining FMs performance,
as measured on 8 datasets and 5 pre-trained models of various network
architectures.
</p></li>
</ul>

<h3>Title: Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17431">http://arxiv.org/abs/2311.17431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17431]] Grounding Foundation Models through Federated Transfer Learning: A General Framework(http://arxiv.org/abs/2311.17431)</code></li>
<li>Summary: <p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p></li>
</ul>

<h3>Title: Federated Online and Bandit Convex Optimization. (arXiv:2311.17586v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17586">http://arxiv.org/abs/2311.17586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17586]] Federated Online and Bandit Convex Optimization(http://arxiv.org/abs/2311.17586)</code></li>
<li>Summary: <p>We study the problems of distributed online and bandit convex optimization
against an adaptive adversary. We aim to minimize the average regret on $M$
machines working in parallel over $T$ rounds with $R$ intermittent
communications. Assuming the underlying cost functions are convex and can be
generated adaptively, our results show that collaboration is not beneficial
when the machines have access to the first-order gradient information at the
queried points. This is in contrast to the case for stochastic functions, where
each machine samples the cost functions from a fixed distribution. Furthermore,
we delve into the more challenging setting of federated online optimization
with bandit (zeroth-order) feedback, where the machines can only access values
of the cost functions at the queried points. The key finding here is
identifying the high-dimensional regime where collaboration is beneficial and
may even lead to a linear speedup in the number of machines. We further
illustrate our findings through federated adversarial linear bandits by
developing novel distributed single and two-point feedback algorithms. Our work
is the first attempt towards a systematic understanding of federated online
optimization with limited feedback, and it attains tight regret bounds in the
intermittent communication setting for both first and zeroth-order feedback.
Our results thus bridge the gap between stochastic and adaptive settings in
federated online optimization.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Weakly-semi-supervised object detection in remotely sensed imagery. (arXiv:2311.17449v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17449">http://arxiv.org/abs/2311.17449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17449]] Weakly-semi-supervised object detection in remotely sensed imagery(http://arxiv.org/abs/2311.17449)</code></li>
<li>Summary: <p>Deep learning for detecting objects in remotely sensed imagery can enable new
technologies for important applications including mitigating climate change.
However, these models often require large datasets labeled with bounding box
annotations which are expensive to curate, prohibiting the development of
models for new tasks and geographies. To address this challenge, we develop
weakly-semi-supervised object detection (WSSOD) models on remotely sensed
imagery which can leverage a small amount of bounding boxes together with a
large amount of point labels that are easy to acquire at scale in geospatial
data. We train WSSOD models which use large amounts of point-labeled images
with varying fractions of bounding box labeled images in FAIR1M and a wind
turbine detection dataset, and demonstrate that they substantially outperform
fully supervised models trained with the same amount of bounding box labeled
images on both datasets. Furthermore, we find that the WSSOD models trained
with 2-10x fewer bounding box labeled images can perform similarly to or
outperform fully supervised models trained on the full set of bounding-box
labeled images. We believe that the approach can be extended to other remote
sensing tasks to reduce reliance on bounding box labels and increase
development of models for impactful applications.
</p></li>
</ul>

<h3>Title: Fair Text-to-Image Diffusion via Fair Mapping. (arXiv:2311.17695v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17695">http://arxiv.org/abs/2311.17695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17695]] Fair Text-to-Image Diffusion via Fair Mapping(http://arxiv.org/abs/2311.17695)</code></li>
<li>Summary: <p>In this paper, we address the limitations of existing text-to-image diffusion
models in generating demographically fair results when given human-related
descriptions. These models often struggle to disentangle the target language
context from sociocultural biases, resulting in biased image generation. To
overcome this challenge, we propose Fair Mapping, a general, model-agnostic,
and lightweight approach that modifies a pre-trained text-to-image model by
controlling the prompt to achieve fair image generation. One key advantage of
our approach is its high efficiency. The training process only requires
updating a small number of parameters in an additional linear mapping network.
This not only reduces the computational cost but also accelerates the
optimization process. We first demonstrate the issue of bias in generated
results caused by language biases in text-guided diffusion models. By
developing a mapping network that projects language embeddings into an unbiased
space, we enable the generation of relatively balanced demographic results
based on a keyword specified in the prompt. With comprehensive experiments on
face image generation, we show that our method significantly improves image
generation performance when prompted with descriptions related to human faces.
By effectively addressing the issue of bias, we produce more fair and diverse
image outputs. This work contributes to the field of text-to-image generation
by enhancing the ability to generate images that accurately reflect the
intended demographic characteristics specified in the text.
</p></li>
</ul>

<h3>Title: Aggregation Model Hyperparameters Matter in Digital Pathology. (arXiv:2311.17804v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17804">http://arxiv.org/abs/2311.17804</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17804]] Aggregation Model Hyperparameters Matter in Digital Pathology(http://arxiv.org/abs/2311.17804)</code></li>
<li>Summary: <p>Digital pathology has significantly advanced disease detection and
pathologist efficiency through the analysis of gigapixel whole-slide images
(WSI). In this process, WSIs are first divided into patches, for which a
feature extractor model is applied to obtain feature vectors, which are
subsequently processed by an aggregation model to predict the respective WSI
label. With the rapid evolution of representation learning, numerous new
feature extractor models, often termed foundational models, have emerged.
Traditional evaluation methods, however, rely on fixed aggregation model
hyperparameters, a framework we identify as potentially biasing the results.
Our study uncovers a co-dependence between feature extractor models and
aggregation model hyperparameters, indicating that performance comparability
can be skewed based on the chosen hyperparameters. By accounting for this
co-dependency, we find that the performance of many current feature extractor
models is notably similar. We support this insight by evaluating seven feature
extractor models across three different datasets with 162 different aggregation
model configurations. This comprehensive approach provides a more nuanced
understanding of the relationship between feature extractors and aggregation
models, leading to a fairer and more accurate assessment of feature extractor
models in digital pathology.
</p></li>
</ul>

<h3>Title: Utilizing Model Residuals to Identify Rental Properties of Interest: The Price Anomaly Score (PAS) and Its Application to Real-time Data in Manhattan. (arXiv:2311.17287v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17287">http://arxiv.org/abs/2311.17287</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17287]] Utilizing Model Residuals to Identify Rental Properties of Interest: The Price Anomaly Score (PAS) and Its Application to Real-time Data in Manhattan(http://arxiv.org/abs/2311.17287)</code></li>
<li>Summary: <p>Understanding whether a property is priced fairly hinders buyers and sellers
since they usually do not have an objective viewpoint of the price distribution
for the overall market of their interest. Drawing from data collected of all
possible available properties for rent in Manhattan as of September 2023, this
paper aims to strengthen our understanding of model residuals; specifically on
machine learning models which generalize for a majority of the distribution of
a well-proportioned dataset. Most models generally perceive deviations from
predicted values as mere inaccuracies, however this paper proposes a different
vantage point: when generalizing to at least 75\% of the data-set, the
remaining deviations reveal significant insights. To harness these insights, we
introduce the Price Anomaly Score (PAS), a metric capable of capturing
boundaries between irregularly predicted prices. By combining relative pricing
discrepancies with statistical significance, the Price Anomaly Score (PAS)
offers a multifaceted view of rental valuations. This metric allows experts to
identify overpriced or underpriced properties within a dataset by aggregating
PAS values, then fine-tuning upper and lower boundaries to any threshold to set
indicators of choice.
</p></li>
</ul>

<h3>Title: The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation. (arXiv:2311.17373v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17373">http://arxiv.org/abs/2311.17373</a></li>
<li>Code URL: https://github.com/zzoomd/fairgkd</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17373]] The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation(http://arxiv.org/abs/2311.17373)</code></li>
<li>Summary: <p>Graph neural networks (GNNs) are being increasingly used in many high-stakes
tasks, and as a result, there is growing attention on their fairness recently.
GNNs have been shown to be unfair as they tend to make discriminatory decisions
toward certain demographic groups, divided by sensitive attributes such as
gender and race. While recent works have been devoted to improving their
fairness performance, they often require accessible demographic information.
This greatly limits their applicability in real-world scenarios due to legal
restrictions. To address this problem, we present a demographic-agnostic method
to learn fair GNNs via knowledge distillation, namely FairGKD. Our work is
motivated by the empirical observation that training GNNs on partial data
(i.e., only node attributes or topology data) can improve their fairness,
albeit at the cost of utility. To make a balanced trade-off between fairness
and utility performance, we employ a set of fairness experts (i.e., GNNs
trained on different partial data) to construct the synthetic teacher, which
distills fairer and informative knowledge to guide the learning of the GNN
student. Experiments on several benchmark datasets demonstrate that FairGKD,
which does not require access to demographic information, significantly
improves the fairness of GNNs by a large margin while maintaining their
utility.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering. (arXiv:2311.17331v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17331">http://arxiv.org/abs/2311.17331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17331]] Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering(http://arxiv.org/abs/2311.17331)</code></li>
<li>Summary: <p>Recently, Vision Language Models (VLMs) have gained significant attention,
exhibiting notable advancements across various tasks by leveraging extensive
image-text paired data. However, prevailing VLMs often treat Visual Question
Answering (VQA) as perception tasks, employing black-box models that overlook
explicit modeling of relationships between different questions within the same
visual scene. Moreover, the existing VQA methods that rely on Knowledge Bases
(KBs) might frequently encounter biases from limited data and face challenges
in relevant information indexing. Attempt to overcome these limitations, this
paper introduces an explainable multi-agent collaboration framework by tapping
into knowledge embedded in Large Language Models (LLMs) trained on extensive
corpora. Inspired by human cognition, our framework uncovers latent information
within the given question by employing three agents, i.e., Seeker, Responder,
and Integrator, to perform a top-down reasoning process. The Seeker agent
generates relevant issues related to the original question. The Responder
agent, based on VLM, handles simple VQA tasks and provides candidate answers.
The Integrator agent combines information from the Seeker agent and the
Responder agent to produce the final VQA answer. Through the above
collaboration mechanism, our framework explicitly constructs a multi-view
knowledge base for a specific image scene, reasoning answers in a top-down
processing manner. We extensively evaluate our method on diverse VQA datasets
and VLMs, demonstrating its broad applicability and interpretability with
comprehensive experimental results.
</p></li>
</ul>

<h3>Title: Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification. (arXiv:2311.17466v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17466">http://arxiv.org/abs/2311.17466</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17466]] Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification(http://arxiv.org/abs/2311.17466)</code></li>
<li>Summary: <p>Whole slide image (WSI) classification requires repetitive zoom-in and out
for pathologists, as only small portions of the slide may be relevant to
detecting cancer. Due to the lack of patch-level labels, multiple instance
learning (MIL) is a common practice for training a WSI classifier. One of the
challenges in MIL for WSIs is the weak supervision coming only from the
slide-level labels, often resulting in severe overfitting. In response,
researchers have considered adopting patch-level augmentation or applying mixup
augmentation, but their applicability remains unverified. Our approach augments
the training dataset by sampling a subset of patches in the WSI without
significantly altering the underlying semantics of the original slides.
Additionally, we introduce an efficient model (Slot-MIL) that organizes patches
into a fixed number of slots, the abstract representation of patches, using an
attention mechanism. We empirically demonstrate that the subsampling
augmentation helps to make more informative slots by restricting the
over-concentration of attention and to improve interpretability. Finally, we
illustrate that combining our attention-based aggregation model with
subsampling and mixup, which has shown limited compatibility in existing MIL
methods, can enhance both generalization and calibration. Our proposed methods
achieve the state-of-the-art performance across various benchmark datasets
including class imbalance and distribution shifts.
</p></li>
</ul>

<h3>Title: XAI for time-series classification leveraging image highlight methods. (arXiv:2311.17110v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17110">http://arxiv.org/abs/2311.17110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17110]] XAI for time-series classification leveraging image highlight methods(http://arxiv.org/abs/2311.17110)</code></li>
<li>Summary: <p>Although much work has been done on explainability in the computer vision and
natural language processing (NLP) fields, there is still much work to be done
to explain methods applied to time series as time series by nature can not be
understood at first sight. In this paper, we present a Deep Neural Network
(DNN) in a teacher-student architecture (distillation model) that offers
interpretability in time-series classification tasks. The explainability of our
approach is based on transforming the time series to 2D plots and applying
image highlight methods (such as LIME and GradCam), making the predictions
interpretable. At the same time, the proposed approach offers increased
accuracy competing with the baseline model with the trade-off of increasing the
training time.
</p></li>
</ul>

<h3>Title: Interpreting Differentiable Latent States for Healthcare Time-series Data. (arXiv:2311.17560v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17560">http://arxiv.org/abs/2311.17560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17560]] Interpreting Differentiable Latent States for Healthcare Time-series Data(http://arxiv.org/abs/2311.17560)</code></li>
<li>Summary: <p>Machine learning enables extracting clinical insights from large temporal
datasets. The applications of such machine learning models include identifying
disease patterns and predicting patient outcomes. However, limited
interpretability poses challenges for deploying advanced machine learning in
digital healthcare. Understanding the meaning of latent states is crucial for
interpreting machine learning models, assuming they capture underlying
patterns. In this paper, we present a concise algorithm that allows for i)
interpreting latent states using highly related input features; ii)
interpreting predictions using subsets of input features via latent states; and
iii) interpreting changes in latent states over time. The proposed algorithm is
feasible for any model that is differentiable. We demonstrate that this
approach enables the identification of a daytime behavioral pattern for
predicting nocturnal behavior in a real-world healthcare dataset.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning. (arXiv:2311.17365v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17365">http://arxiv.org/abs/2311.17365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17365]] Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning(http://arxiv.org/abs/2311.17365)</code></li>
<li>Summary: <p>Human reasoning can be understood as a cooperation between the intuitive,
associative "System-1" and the deliberative, logical "System-2". For existing
System-1-like methods in visual activity understanding, it is crucial to
integrate System-2 processing to improve explainability, generalization, and
data efficiency. One possible path of activity reasoning is building a symbolic
system composed of symbols and rules, where one rule connects multiple symbols,
implying human knowledge and reasoning abilities. Previous methods have made
progress, but are defective with limited symbols from handcraft and limited
rules from visual-based annotations, failing to cover the complex patterns of
activities and lacking compositional generalization. To overcome the defects,
we propose a new symbolic system with two ideal important properties:
broad-coverage symbols and rational rules. Collecting massive human knowledge
via manual annotations is expensive to instantiate this symbolic system.
Instead, we leverage the recent advancement of LLMs (Large Language Models) as
an approximation of the two ideal properties, i.e., Symbols from Large Language
Models (Symbol-LLM). Then, given an image, visual contents from the images are
extracted and checked as symbols and activity semantics are reasoned out based
on rules via fuzzy logic calculation. Our method shows superiority in extensive
activity understanding tasks. Code and data are available at
https://mvig-rhos.com/symbol_llm.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling. (arXiv:2311.17082v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17082">http://arxiv.org/abs/2311.17082</a></li>
<li>Code URL: https://github.com/alexzhou907/dreampropeller</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17082]] DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling(http://arxiv.org/abs/2311.17082)</code></li>
<li>Summary: <p>Recent methods such as Score Distillation Sampling (SDS) and Variational
Score Distillation (VSD) using 2D diffusion models for text-to-3D generation
have demonstrated impressive generation quality. However, the long generation
time of such algorithms significantly degrades the user experience. To tackle
this problem, we propose DreamPropeller, a drop-in acceleration algorithm that
can be wrapped around any existing text-to-3D generation pipeline based on
score distillation. Our framework generalizes Picard iterations, a classical
algorithm for parallel sampling an ODE path, and can account for non-ODE paths
such as momentum-based gradient updates and changes in dimensions during the
optimization process as in many cases of 3D generation. We show that our
algorithm trades parallel compute for wallclock time and empirically achieves
up to 4.7x speedup with a negligible drop in generation quality for all tested
frameworks.
</p></li>
</ul>

<h3>Title: PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation. (arXiv:2311.17086v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17086">http://arxiv.org/abs/2311.17086</a></li>
<li>Code URL: https://github.com/OPPO-Mente-Lab/PEA-Diffusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17086]] PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation(http://arxiv.org/abs/2311.17086)</code></li>
<li>Summary: <p>Text-to-image diffusion models are well-known for their ability to generate
realistic images based on textual prompts. However, the existing works have
predominantly focused on English, lacking support for non-English text-to-image
models. The most commonly used translation methods cannot solve the generation
problem related to language culture, while training from scratch on a specific
language dataset is prohibitively expensive. In this paper, we are inspired to
propose a simple plug-and-play language transfer method based on knowledge
distillation. All we need to do is train a lightweight MLP-like
parameter-efficient adapter (PEA) with only 6M parameters under teacher
knowledge distillation along with a small parallel data corpus. We are
surprised to find that freezing the parameters of UNet can still achieve
remarkable performance on the language-specific prompt evaluation set,
demonstrating that PEA can stimulate the potential generation ability of the
original UNet. Additionally, it closely approaches the performance of the
English text-to-image model on a general prompt evaluation set. Furthermore,
our adapter can be used as a plugin to achieve significant results in
downstream tasks in cross-lingual text-to-image generation. Code will be
available at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion
</p></li>
</ul>

<h3>Title: ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis. (arXiv:2311.17123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17123">http://arxiv.org/abs/2311.17123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17123]] ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis(http://arxiv.org/abs/2311.17123)</code></li>
<li>Summary: <p>In this work, we propose a method to address the challenge of rendering a 3D
human from a single image in a free-view manner. Some existing approaches could
achieve this by using generalizable pixel-aligned implicit fields to
reconstruct a textured mesh of a human or by employing a 2D diffusion model as
guidance with the Score Distillation Sampling (SDS) method, to lift the 2D
image into 3D space. However, a generalizable implicit field often results in
an over-smooth texture field, while the SDS method tends to lead to a
texture-inconsistent novel view with the input image. In this paper, we
introduce a texture-consistent back view synthesis module that could transfer
the reference image content to the back view through depth and text-guided
attention injection. Moreover, to alleviate the color distortion that occurs in
the side region, we propose a visibility-aware patch consistency regularization
for texture mapping and refinement combined with the synthesized back view
texture. With the above techniques, we could achieve high-fidelity and
texture-consistent human rendering from a single image. Experiments conducted
on both real and synthetic data demonstrate the effectiveness of our method and
show that our approach outperforms previous baseline methods.
</p></li>
</ul>

<h3>Title: Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation. (arXiv:2311.17216v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17216">http://arxiv.org/abs/2311.17216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17216]] Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation(http://arxiv.org/abs/2311.17216)</code></li>
<li>Summary: <p>Diffusion-based models have gained significant popularity for text-to-image
generation due to their exceptional image-generation capabilities. A risk with
these models is the potential generation of inappropriate content, such as
biased or harmful images. However, the underlying reasons for generating such
undesired content from the perspective of the diffusion model's internal
representation remain unclear. Previous work interprets vectors in an
interpretable latent space of diffusion models as semantic concepts. However,
existing approaches cannot discover directions for arbitrary concepts, such as
those related to inappropriate concepts. In this work, we propose a novel
self-supervised approach to find interpretable latent directions for a given
concept. With the discovered vectors, we further propose a simple approach to
mitigate inappropriate generation. Extensive experiments have been conducted to
verify the effectiveness of our mitigation approach, namely, for fair
generation, safe generation, and responsible text-enhancing generation.
</p></li>
</ul>

<h3>Title: SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors. (arXiv:2311.17261v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17261">http://arxiv.org/abs/2311.17261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17261]] SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors(http://arxiv.org/abs/2311.17261)</code></li>
<li>Summary: <p>We propose SceneTex, a novel method for effectively generating high-quality
and style-consistent textures for indoor scenes using depth-to-image diffusion
priors. Unlike previous methods that either iteratively warp 2D views onto a
mesh surface or distillate diffusion latent features without accurate geometric
and style cues, SceneTex formulates the texture synthesis task as an
optimization problem in the RGB space where style and geometry consistency are
properly reflected. At its core, SceneTex proposes a multiresolution texture
field to implicitly encode the mesh appearance. We optimize the target texture
via a score-distillation-based objective function in respective RGB renderings.
To further secure the style consistency across views, we introduce a
cross-attention decoder to predict the RGB values by cross-attending to the
pre-sampled reference locations in each instance. SceneTex enables various and
accurate texture synthesis for 3D-FRONT scenes, demonstrating significant
improvements in visual quality and prompt fidelity over the prior texture
generation methods.
</p></li>
</ul>

<h3>Title: VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model. (arXiv:2311.17338v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17338">http://arxiv.org/abs/2311.17338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17338]] VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model(http://arxiv.org/abs/2311.17338)</code></li>
<li>Summary: <p>Identity-consistent video generation seeks to synthesize videos that are
guided by both textual prompts and reference images of entities. Current
approaches typically utilize cross-attention layers to integrate the appearance
of the entity, which predominantly captures semantic attributes, resulting in
compromised fidelity of entities. Moreover, these methods necessitate iterative
fine-tuning for each new entity encountered, thereby limiting their
applicability. To address these challenges, we introduce VideoAssembler, a
novel end-to-end framework for identity-consistent video generation that can
conduct inference directly when encountering new entities. VideoAssembler is
adept at producing videos that are not only flexible with respect to the input
reference entities but also responsive to textual conditions. Additionally, by
modulating the quantity of input images for the entity, VideoAssembler enables
the execution of tasks ranging from image-to-video generation to sophisticated
video editing. VideoAssembler comprises two principal components: the Reference
Entity Pyramid (REP) encoder and the Entity-Prompt Attention Fusion (EPAF)
module. The REP encoder is designed to infuse comprehensive appearance details
into the denoising stages of the stable diffusion model. Concurrently, the EPAF
module is utilized to integrate text-aligned features effectively. Furthermore,
to mitigate the challenge of scarce data, we present a methodology for the
preprocessing of training data. Our evaluation of the VideoAssembler framework
on the UCF-101, MSR-VTT, and DAVIS datasets indicates that it achieves good
performances in both quantitative and qualitative analyses (346.84 in FVD and
48.01 in IS on UCF-101). Our project page is at
https://videoassembler.github.io/videoassembler.
</p></li>
</ul>

<h3>Title: When StyleGAN Meets Stable Diffusion: a $\mathscr{W}_+$ Adapter for Personalized Image Generation. (arXiv:2311.17461v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17461">http://arxiv.org/abs/2311.17461</a></li>
<li>Code URL: https://github.com/csxmli2016/w-plus-adapter</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17461]] When StyleGAN Meets Stable Diffusion: a $\mathscr{W}_+$ Adapter for Personalized Image Generation(http://arxiv.org/abs/2311.17461)</code></li>
<li>Summary: <p>Text-to-image diffusion models have remarkably excelled in producing diverse,
high-quality, and photo-realistic images. This advancement has spurred a
growing interest in incorporating specific identities into generated content.
Most current methods employ an inversion approach to embed a target visual
concept into the text embedding space using a single reference image. However,
the newly synthesized faces either closely resemble the reference image in
terms of facial attributes, such as expression, or exhibit a reduced capacity
for identity preservation. Text descriptions intended to guide the facial
attributes of the synthesized face may fall short, owing to the intricate
entanglement of identity information with identity-irrelevant facial attributes
derived from the reference image. To address these issues, we present the novel
use of the extended StyleGAN embedding space $\mathcal{W}_+$, to achieve
enhanced identity preservation and disentanglement for diffusion models. By
aligning this semantically meaningful human face latent space with
text-to-image diffusion models, we succeed in maintaining high fidelity in
identity preservation, coupled with the capacity for semantic editing.
Additionally, we propose new training objectives to balance the influences of
both prompt and identity conditions, ensuring that the identity-irrelevant
background remains unaffected during facial attribute modifications. Extensive
experiments reveal that our method adeptly generates personalized text-to-image
outputs that are not only compatible with prompt descriptions but also amenable
to common StyleGAN editing directions in diverse settings. Our source code will
be available at \url{https://github.com/csxmli2016/w-plus-adapter}.
</p></li>
</ul>

<h3>Title: Non-Visible Light Data Synthesis and Application: A Case Study for Synthetic Aperture Radar Imagery. (arXiv:2311.17486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17486">http://arxiv.org/abs/2311.17486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17486]] Non-Visible Light Data Synthesis and Application: A Case Study for Synthetic Aperture Radar Imagery(http://arxiv.org/abs/2311.17486)</code></li>
<li>Summary: <p>We explore the "hidden" ability of large-scale pre-trained image generation
models, such as Stable Diffusion and Imagen, in non-visible light domains,
taking Synthetic Aperture Radar (SAR) data for a case study. Due to the
inherent challenges in capturing satellite data, acquiring ample SAR training
samples is infeasible. For instance, for a particular category of ship in the
open sea, we can collect only few-shot SAR images which are too limited to
derive effective ship recognition models. If large-scale models pre-trained
with regular images can be adapted to generating novel SAR images, the problem
is solved. In preliminary study, we found that fine-tuning these models with
few-shot SAR images is not working, as the models can not capture the two
primary differences between SAR and regular images: structure and modality. To
address this, we propose a 2-stage low-rank adaptation method, and we call it
2LoRA. In the first stage, the model is adapted using aerial-view regular image
data (whose structure matches SAR), followed by the second stage where the base
model from the first stage is further adapted using SAR modality data.
Particularly in the second stage, we introduce a novel prototype LoRA (pLoRA),
as an improved version of 2LoRA, to resolve the class imbalance problem in SAR
datasets. For evaluation, we employ the resulting generation model to
synthesize additional SAR data. This augmentation, when integrated into the
training process of SAR classification as well as segmentation models, yields
notably improved performance for minor classes
</p></li>
</ul>

<h3>Title: HiDiffusion: Unlocking High-Resolution Creativity and Efficiency in Low-Resolution Trained Diffusion Models. (arXiv:2311.17528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17528">http://arxiv.org/abs/2311.17528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17528]] HiDiffusion: Unlocking High-Resolution Creativity and Efficiency in Low-Resolution Trained Diffusion Models(http://arxiv.org/abs/2311.17528)</code></li>
<li>Summary: <p>We introduce HiDiffusion, a tuning-free framework comprised of
Resolution-Aware U-Net (RAU-Net) and Modified Shifted Window Multi-head
Self-Attention (MSW-MSA) to enable pretrained large text-to-image diffusion
models to efficiently generate high-resolution images (e.g. 1024$\times$1024)
that surpass the training image resolution. Pretrained diffusion models
encounter unreasonable object duplication in generating images beyond the
training image resolution. We attribute it to the mismatch between the feature
map size of high-resolution images and the receptive field of U-Net's
convolution. To address this issue, we propose a simple yet scalable method
named RAU-Net. RAU-Net dynamically adjusts the feature map size to match the
convolution's receptive field in the deep block of U-Net. Another obstacle in
high-resolution synthesis is the slow inference speed of U-Net. Our
observations reveal that the global self-attention in the top block, which
exhibits locality, however, consumes the majority of computational resources.
To tackle this issue, we propose MSW-MSA. Unlike previous window attention
mechanisms, our method uses a much larger window size and dynamically shifts
windows to better accommodate diffusion models. Extensive experiments
demonstrate that our HiDiffusion can scale diffusion models to generate
1024$\times$1024, 2048$\times$2048, or even 4096$\times$4096 resolution images,
while simultaneously reducing inference time by 40\%-60\%, achieving
state-of-the-art performance on high-resolution image synthesis. The most
significant revelation of our work is that a pretrained diffusion model on
low-resolution images is scalable for high-resolution generation without
further tuning. We hope this revelation can provide insights for future
research on the scalability of diffusion models.
</p></li>
</ul>

<h3>Title: Smooth Video Synthesis with Noise Constraints on Diffusion Models for One-shot Video Tuning. (arXiv:2311.17536v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17536">http://arxiv.org/abs/2311.17536</a></li>
<li>Code URL: https://github.com/spengliang/smoothvideo</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17536]] Smooth Video Synthesis with Noise Constraints on Diffusion Models for One-shot Video Tuning(http://arxiv.org/abs/2311.17536)</code></li>
<li>Summary: <p>Recent one-shot video tuning methods, which fine-tune the network on a
specific video based on pre-trained text-to-image models (e.g., Stable
Diffusion), are popular in the community because of the flexibility. However,
these methods often produce videos marred by incoherence and inconsistency. To
address these limitations, this paper introduces a simple yet effective noise
constraint across video frames. This constraint aims to regulate noise
predictions across their temporal neighbors, resulting in smooth latents. It
can be simply included as a loss term during the training phase. By applying
the loss to existing one-shot video tuning methods, we significantly improve
the overall consistency and smoothness of the generated videos. Furthermore, we
argue that current video evaluation metrics inadequately capture smoothness. To
address this, we introduce a novel metric that considers detailed features and
their temporal dynamics. Experimental results validate the effectiveness of our
approach in producing smoother videos on various one-shot video tuning
baselines. The source codes and video demos are available at
\href{https://github.com/SPengLiang/SmoothVideo}{https://github.com/SPengLiang/SmoothVideo}.
</p></li>
</ul>

<h3>Title: AnyLens: A Generative Diffusion Model with Any Rendering Lens. (arXiv:2311.17609v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17609">http://arxiv.org/abs/2311.17609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17609]] AnyLens: A Generative Diffusion Model with Any Rendering Lens(http://arxiv.org/abs/2311.17609)</code></li>
<li>Summary: <p>State-of-the-art diffusion models can generate highly realistic images based
on various conditioning like text, segmentation, and depth. However, an
essential aspect often overlooked is the specific camera geometry used during
image capture. The influence of different optical systems on the final scene
appearance is frequently overlooked. This study introduces a framework that
intimately integrates a text-to-image diffusion model with the particular lens
geometry used in image rendering. Our method is based on a per-pixel coordinate
conditioning method, enabling the control over the rendering geometry. Notably,
we demonstrate the manipulation of curvature properties, achieving diverse
visual effects, such as fish-eye, panoramic views, and spherical texturing
using a single diffusion model.
</p></li>
</ul>

<h3>Title: Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers. (arXiv:2311.17717v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17717">http://arxiv.org/abs/2311.17717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17717]] Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers(http://arxiv.org/abs/2311.17717)</code></li>
<li>Summary: <p>Concept erasure in text-to-image diffusion models aims to disable pre-trained
diffusion models from generating images related to a target concept. To perform
reliable concept erasure, the properties of robustness and locality are
desirable. The former refrains the model from producing images associated with
the target concept for any paraphrased or learned prompts, while the latter
preserves the model ability in generating images for non-target concepts. In
this paper, we propose Reliable Concept Erasing via Lightweight Erasers
(Receler), which learns a lightweight Eraser to perform concept erasing and
enhances locality and robustness with the proposed concept-localized
regularization and adversarial prompt learning, respectively. Comprehensive
quantitative and qualitative experiments with various concept prompts verify
the superiority of Receler over the previous erasing methods on the above two
desirable properties.
</p></li>
</ul>

<h3>Title: Analyzing and Explaining Image Classifiers via Diffusion Guidance. (arXiv:2311.17833v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17833">http://arxiv.org/abs/2311.17833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17833]] Analyzing and Explaining Image Classifiers via Diffusion Guidance(http://arxiv.org/abs/2311.17833)</code></li>
<li>Summary: <p>While deep learning has led to huge progress in complex image classification
tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call
into question how reliably these classifiers work in the wild. Furthermore, for
safety-critical tasks the black-box nature of their decisions is problematic,
and explanations or at least methods which make decisions plausible are needed
urgently. In this paper, we address these problems by generating images that
optimize a classifier-derived objective using a framework for guided image
generation. We analyze the behavior and decisions of image classifiers by
visual counterfactual explanations (VCEs), detection of systematic mistakes by
analyzing images where classifiers maximally disagree, and visualization of
neurons to verify potential spurious features. In this way, we validate
existing observations, e.g. the shape bias of adversarially robust models, as
well as novel failure modes, e.g. systematic errors of zero-shot CLIP
classifiers, or identify harmful spurious features. Moreover, our VCEs
outperform previous work while being more versatile.
</p></li>
</ul>

<h3>Title: SPiC-E : Structural Priors in 3D Diffusion Models using Cross Entity Attention. (arXiv:2311.17834v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17834">http://arxiv.org/abs/2311.17834</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17834]] SPiC-E : Structural Priors in 3D Diffusion Models using Cross Entity Attention(http://arxiv.org/abs/2311.17834)</code></li>
<li>Summary: <p>We are witnessing rapid progress in automatically generating and manipulating
3D assets due to the availability of pretrained text-image diffusion models.
However, time-consuming optimization procedures are required for synthesizing
each sample, hindering their potential for democratizing 3D content creation.
Conversely, 3D diffusion models now train on million-scale 3D datasets,
yielding high-quality text-conditional 3D samples within seconds. In this work,
we present SPiC-E - a neural network that adds structural guidance to 3D
diffusion models, extending their usage beyond text-conditional generation. At
its core, our framework introduces a cross-entity attention mechanism that
allows for multiple entities (in particular, paired input and guidance 3D
shapes) to interact via their internal representations within the denoising
network. We utilize this mechanism for learning task-specific structural priors
in 3D diffusion models from auxiliary guidance shapes. We show that our
approach supports a variety of applications, including 3D stylization, semantic
shape editing and text-conditional abstraction-to-3D, which transforms
primitive-based abstractions into highly-expressive shapes. Extensive
experiments demonstrate that SPiC-E achieves SOTA performance over these tasks
while often being considerably faster than alternative methods. Importantly,
this is accomplished without tailoring our approach for any specific task.
</p></li>
</ul>

<h3>Title: Leveraging Graph Diffusion Models for Network Refinement Tasks. (arXiv:2311.17856v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17856">http://arxiv.org/abs/2311.17856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17856]] Leveraging Graph Diffusion Models for Network Refinement Tasks(http://arxiv.org/abs/2311.17856)</code></li>
<li>Summary: <p>Most real-world networks are noisy and incomplete samples from an unknown
target distribution. Refining them by correcting corruptions or inferring
unobserved regions typically improves downstream performance. Inspired by the
impressive generative capabilities that have been used to correct corruptions
in images, and the similarities between "in-painting" and filling in missing
nodes and edges conditioned on the observed graph, we propose a novel graph
generative framework, SGDM, which is based on subgraph diffusion. Our framework
not only improves the scalability and fidelity of graph diffusion models, but
also leverages the reverse process to perform novel, conditional generation
tasks. In particular, through extensive empirical analysis and a set of novel
metrics, we demonstrate that our proposed model effectively supports the
following refinement tasks for partially observable networks: T1: denoising
extraneous subgraphs, T2: expanding existing subgraphs and T3: performing
"style" transfer by regenerating a particular subgraph to match the
characteristics of a different node or subgraph.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Neural Texture Puppeteer: A Framework for Neural Geometry and Texture Rendering of Articulated Shapes, Enabling Re-Identification at Interactive Speed. (arXiv:2311.17109v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17109">http://arxiv.org/abs/2311.17109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17109]] Neural Texture Puppeteer: A Framework for Neural Geometry and Texture Rendering of Articulated Shapes, Enabling Re-Identification at Interactive Speed(http://arxiv.org/abs/2311.17109)</code></li>
<li>Summary: <p>In this paper, we present a neural rendering pipeline for textured
articulated shapes that we call Neural Texture Puppeteer. Our method separates
geometry and texture encoding. The geometry pipeline learns to capture spatial
relationships on the surface of the articulated shape from ground truth data
that provides this geometric information. A texture auto-encoder makes use of
this information to encode textured images into a global latent code. This
global texture embedding can be efficiently trained separately from the
geometry, and used in a downstream task to identify individuals. The neural
texture rendering and the identification of individuals run at interactive
speeds. To the best of our knowledge, we are the first to offer a promising
alternative to CNN- or transformer-based approaches for re-identification of
articulated individuals based on neural rendering. Realistic looking novel view
and pose synthesis for different synthetic cow textures further demonstrate the
quality of our method. Restricted by the availability of ground truth data for
the articulated shape's geometry, the quality for real-world data synthesis is
reduced. We further demonstrate the flexibility of our model for real-world
data by applying a synthetic to real-world texture domain shift where we
reconstruct the texture from a real-world 2D RGB image. Thus, our method can be
applied to endangered species where data is limited. Our novel synthetic
texture dataset NePuMoo is publicly available to inspire further development in
the field of neural rendering-based re-identification.
</p></li>
</ul>

<h3>Title: TLControl: Trajectory and Language Control for Human Motion Synthesis. (arXiv:2311.17135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17135">http://arxiv.org/abs/2311.17135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17135]] TLControl: Trajectory and Language Control for Human Motion Synthesis(http://arxiv.org/abs/2311.17135)</code></li>
<li>Summary: <p>Controllable human motion synthesis is essential for applications in AR/VR,
gaming, movies, and embodied AI. Existing methods often focus solely on either
language or full trajectory control, lacking precision in synthesizing motions
aligned with user-specified trajectories, especially for multi-joint control.
To address these issues, we present TLControl, a new method for realistic human
motion synthesis, incorporating both low-level trajectory and high-level
language semantics controls. Specifically, we first train a VQ-VAE to learn a
compact latent motion space organized by body parts. We then propose a Masked
Trajectories Transformer to make coarse initial predictions of full
trajectories of joints based on the learned latent motion space, with
user-specified partial trajectories and text descriptions as conditioning.
Finally, we introduce an efficient test-time optimization to refine these
coarse predictions for accurate trajectory control. Experiments demonstrate
that TLControl outperforms the state-of-the-art in trajectory accuracy and time
efficiency, making it practical for interactive and high-quality animation
generation.
</p></li>
</ul>

<h3>Title: PHG-Net: Persistent Homology Guided Medical Image Classification. (arXiv:2311.17243v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17243">http://arxiv.org/abs/2311.17243</a></li>
<li>Code URL: https://github.com/yaoppeng/topoclassification</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17243]] PHG-Net: Persistent Homology Guided Medical Image Classification(http://arxiv.org/abs/2311.17243)</code></li>
<li>Summary: <p>Modern deep neural networks have achieved great successes in medical image
analysis. However, the features captured by convolutional neural networks
(CNNs) or Transformers tend to be optimized for pixel intensities and neglect
key anatomical structures such as connected components and loops. In this
paper, we propose a persistent homology guided approach (PHG-Net) that explores
topological features of objects for medical image classification. For an input
image, we first compute its cubical persistence diagram and extract topological
features into a vector representation using a small neural network (called the
PH module). The extracted topological features are then incorporated into the
feature map generated by CNN or Transformer for feature fusion. The PH module
is lightweight and capable of integrating topological features into any CNN or
Transformer architectures in an end-to-end fashion. We evaluate our PHG-Net on
three public datasets and demonstrate its considerable improvements on the
target classification tasks over state-of-the-art methods.
</p></li>
</ul>

<h3>Title: eMotions: A Large-Scale Dataset for Emotion Recognition in Short Videos. (arXiv:2311.17335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17335">http://arxiv.org/abs/2311.17335</a></li>
<li>Code URL: https://github.com/xuecwu/emotions</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17335]] eMotions: A Large-Scale Dataset for Emotion Recognition in Short Videos(http://arxiv.org/abs/2311.17335)</code></li>
<li>Summary: <p>Nowadays, short videos (SVs) are essential to information acquisition and
sharing in our life. The prevailing use of SVs to spread emotions leads to the
necessity of emotion recognition in SVs. Considering the lack of SVs emotion
data, we introduce a large-scale dataset named eMotions, comprising 27,996
videos. Meanwhile, we alleviate the impact of subjectivities on labeling
quality by emphasizing better personnel allocations and multi-stage
annotations. In addition, we provide the category-balanced and test-oriented
variants through targeted data sampling. Some commonly used videos (e.g.,
facial expressions and postures) have been well studied. However, it is still
challenging to understand the emotions in SVs. Since the enhanced content
diversity brings more distinct semantic gaps and difficulties in learning
emotion-related features, and there exists information gaps caused by the
emotion incompleteness under the prevalently audio-visual co-expressions. To
tackle these problems, we present an end-to-end baseline method AV-CPNet that
employs the video transformer to better learn semantically relevant
representations. We further design the two-stage cross-modal fusion module to
complementarily model the correlations of audio-visual features. The EP-CE
Loss, incorporating three emotion polarities, is then applied to guide model
optimization. Extensive experimental results on nine datasets verify the
effectiveness of AV-CPNet. Datasets and code will be open on
https://github.com/XuecWu/eMotions.
</p></li>
</ul>

<h3>Title: Generative Hierarchical Temporal Transformer for Hand Action Recognition and Motion Prediction. (arXiv:2311.17366v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17366">http://arxiv.org/abs/2311.17366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17366]] Generative Hierarchical Temporal Transformer for Hand Action Recognition and Motion Prediction(http://arxiv.org/abs/2311.17366)</code></li>
<li>Summary: <p>We present a novel framework that concurrently tackles hand action
recognition and 3D future hand motion prediction. While previous works focus on
either recognition or prediction, we propose a generative Transformer VAE
architecture to jointly capture both aspects, facilitating realistic motion
prediction by leveraging the short-term hand motion and long-term action
consistency observed across timestamps.To ensure faithful representation of the
semantic dependency and different temporal granularity of hand pose and action,
our framework is decomposed into two cascaded VAE blocks. The lower pose block
models short-span poses, while the upper action block models long-span action.
These are connected by a mid-level feature that represents sub-second series of
hand poses.Our framework is trained across multiple datasets, where pose and
action blocks are trained separately to fully utilize pose-action annotations
of different qualities. Evaluations show that on multiple datasets, the joint
modeling of recognition and prediction improves over separate solutions, and
the semantic and temporal hierarchy enables long-term pose and action modeling.
</p></li>
</ul>

<h3>Title: SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation. (arXiv:2311.17428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17428">http://arxiv.org/abs/2311.17428</a></li>
<li>Code URL: https://github.com/liuqi-creat/sigformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17428]] SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation(http://arxiv.org/abs/2311.17428)</code></li>
<li>Summary: <p>Multi-modal human action segmentation is a critical and challenging task with
a wide range of applications. Nowadays, the majority of approaches concentrate
on the fusion of dense signals (i.e., RGB, optical flow, and depth maps).
However, the potential contributions of sparse IoT sensor signals, which can be
crucial for achieving accurate recognition, have not been fully explored. To
make up for this, we introduce a Sparse signalguided Transformer (SigFormer) to
combine both dense and sparse signals. We employ mask attention to fuse
localized features by constraining cross-attention within the regions where
sparse signals are valid. However, since sparse signals are discrete, they lack
sufficient information about the temporal action boundaries. Therefore, in
SigFormer, we propose to emphasize the boundary information at two stages to
alleviate this problem. In the first feature extraction stage, we introduce an
intermediate bottleneck module to jointly learn both category and boundary
features of each dense modality through the inner loss functions. After the
fusion of dense modalities and sparse signals, we then devise a two-branch
architecture that explicitly models the interrelationship between action
category and temporal boundary. Experimental results demonstrate that SigFormer
outperforms the state-of-the-art approaches on a multi-modal action
segmentation dataset from real industrial environments, reaching an outstanding
F1 score of 0.958. The codes and pre-trained models have been available at
https://github.com/LIUQI-creat/SigFormer.
</p></li>
</ul>

<h3>Title: CLiSA: A Hierarchical Hybrid Transformer Model using Orthogonal Cross Attention for Satellite Image Cloud Segmentation. (arXiv:2311.17475v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17475">http://arxiv.org/abs/2311.17475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17475]] CLiSA: A Hierarchical Hybrid Transformer Model using Orthogonal Cross Attention for Satellite Image Cloud Segmentation(http://arxiv.org/abs/2311.17475)</code></li>
<li>Summary: <p>Clouds in optical satellite images are a major concern since their presence
hinders the ability to carry accurate analysis as well as processing. Presence
of clouds also affects the image tasking schedule and results in wastage of
valuable storage space on ground as well as space-based systems. Due to these
reasons, deriving accurate cloud masks from optical remote-sensing images is an
important task. Traditional methods such as threshold-based, spatial filtering
for cloud detection in satellite images suffer from lack of accuracy. In recent
years, deep learning algorithms have emerged as a promising approach to solve
image segmentation problems as it allows pixel-level classification and
semantic-level segmentation. In this paper, we introduce a deep-learning model
based on hybrid transformer architecture for effective cloud mask generation
named CLiSA - Cloud segmentation via Lipschitz Stable Attention network. In
this context, we propose an concept of orthogonal self-attention combined with
hierarchical cross attention model, and we validate its Lipschitz stability
theoretically and empirically. We design the whole setup under adversarial
setting in presence of Lov\'asz-Softmax loss. We demonstrate both qualitative
and quantitative outcomes for multiple satellite image datasets including
Landsat-8, Sentinel-2, and Cartosat-2s. Performing comparative study we show
that our model performs preferably against other state-of-the-art methods and
also provides better generalization in precise cloud extraction from satellite
multi-spectral (MX) images. We also showcase different ablation studies to
endorse our choices corresponding to different architectural elements and
objective functions.
</p></li>
</ul>

<h3>Title: PViT-6D: Overclocking Vision Transformers for 6D Pose Estimation with Confidence-Level Prediction and Pose Tokens. (arXiv:2311.17504v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17504">http://arxiv.org/abs/2311.17504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17504]] PViT-6D: Overclocking Vision Transformers for 6D Pose Estimation with Confidence-Level Prediction and Pose Tokens(http://arxiv.org/abs/2311.17504)</code></li>
<li>Summary: <p>In the current state of 6D pose estimation, top-performing techniques depend
on complex intermediate correspondences, specialized architectures, and
non-end-to-end algorithms. In contrast, our research reframes the problem as a
straightforward regression task by exploring the capabilities of Vision
Transformers for direct 6D pose estimation through a tailored use of
classification tokens. We also introduce a simple method for determining pose
confidence, which can be readily integrated into most 6D pose estimation
frameworks. This involves modifying the transformer architecture by decreasing
the number of query elements based on the network's assessment of the scene
complexity. Our method that we call Pose Vision Transformer or PViT-6D provides
the benefits of simple implementation and being end-to-end learnable while
outperforming current state-of-the-art methods by +0.3% ADD(-S) on
Linemod-Occlusion and +2.7% ADD(-S) on the YCB-V dataset. Moreover, our method
enhances both the model's interpretability and the reliability of its
performance during inference.
</p></li>
</ul>

<h3>Title: LGFCTR: Local and Global Feature Convolutional Transformer for Image Matching. (arXiv:2311.17571v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17571">http://arxiv.org/abs/2311.17571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17571]] LGFCTR: Local and Global Feature Convolutional Transformer for Image Matching(http://arxiv.org/abs/2311.17571)</code></li>
<li>Summary: <p>Image matching that finding robust and accurate correspondences across images
is a challenging task under extreme conditions. Capturing local and global
features simultaneously is an important way to mitigate such an issue but
recent transformer-based decoders were still stuck in the issues that CNN-based
encoders only extract local features and the transformers lack locality.
Inspired by the locality and implicit positional encoding of convolutions, a
novel convolutional transformer is proposed to capture both local contexts and
global structures more sufficiently for detector-free matching. Firstly, a
universal FPN-like framework captures global structures in self-encoder as well
as cross-decoder by transformers and compensates local contexts as well as
implicit positional encoding by convolutions. Secondly, a novel convolutional
transformer module explores multi-scale long range dependencies by a novel
multi-scale attention and further aggregates local information inside
dependencies for enhancing locality. Finally, a novel regression-based
sub-pixel refinement module exploits the whole fine-grained window features for
fine-level positional deviation regression. The proposed method achieves
superior performances on a wide range of benchmarks. The code will be available
on https://github.com/zwh0527/LGFCTR.
</p></li>
</ul>

<h3>Title: Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation. (arXiv:2311.17626v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17626">http://arxiv.org/abs/2311.17626</a></li>
<li>Code URL: https://github.com/wyxdm/amnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17626]] Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation(http://arxiv.org/abs/2311.17626)</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) aims to segment objects of new categories given
only a handful of annotated samples. Previous works focus their efforts on
exploring the support information while paying less attention to the mining of
the critical query branch. In this paper, we rethink the importance of support
information and propose a new query-centric FSS model Adversarial Mining
Transformer (AMFormer), which achieves accurate query image segmentation with
only rough support guidance or even weak support labels. The proposed AMFormer
enjoys several merits. First, we design an object mining transformer (G) that
can achieve the expansion of incomplete region activated by support clue, and a
detail mining transformer (D) to discriminate the detailed local difference
between the expanded mask and the ground truth. Second, we propose to train G
and D via an adversarial process, where G is optimized to generate more
accurate masks approaching ground truth to fool D. We conduct extensive
experiments on commonly used Pascal-5i and COCO-20i benchmarks and achieve
state-of-the-art results across all settings. In addition, the decent
performance with weak support labels in our query-centric paradigm may inspire
the development of more general FSS models. Code will be available at
https://github.com/Wyxdm/AMNet.
</p></li>
</ul>

<h3>Title: Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning. (arXiv:2311.17514v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17514">http://arxiv.org/abs/2311.17514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17514]] Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning(http://arxiv.org/abs/2311.17514)</code></li>
<li>Summary: <p>Query-focused Summarization (QfS) deals with systems that generate summaries
from document(s) based on a query. Motivated by the insight that Reinforcement
Learning (RL) provides a generalization to Supervised Learning (SL) for Natural
Language Generation, and thereby performs better (empirically) than SL, we use
an RL-based approach for this task of QfS. Additionally, we also resolve the
conflict of employing RL in Transformers with Teacher Forcing. We develop
multiple Policy Gradient networks, trained on various reward signals: ROUGE,
BLEU, and Semantic Similarity, which lead to a 10-point improvement over the
State-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5).
We also show performance of our approach in zero-shot setting for another
benchmark dataset (DebatePedia) -- our approach leads to results comparable to
baselines, which were specifically trained on DebatePedia. To aid the RL
training, we propose a better semantic similarity reward, enabled by a novel
Passage Embedding scheme developed using Cluster Hypothesis. Lastly, we
contribute a gold-standard test dataset to further research in QfS and
Long-form Question Answering (LfQA).
</p></li>
</ul>

<h3>Title: Introduction to Transformers: an NLP Perspective. (arXiv:2311.17633v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17633">http://arxiv.org/abs/2311.17633</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17633]] Introduction to Transformers: an NLP Perspective(http://arxiv.org/abs/2311.17633)</code></li>
<li>Summary: <p>Transformers have dominated empirical machine learning models of natural
language processing. In this paper, we introduce basic concepts of Transformers
and present key techniques that form the recent advances of these models. This
includes a description of the standard Transformer architecture, a series of
model refinements, and common applications. Given that Transformers and related
deep learning techniques might be evolving in ways we have never seen, we
cannot dive into all the model details or cover all the technical areas.
Instead, we focus on just those concepts that are helpful for gaining a good
understanding of Transformers and their variants. We also summarize the key
ideas that impact this field, thereby yielding some insights into the strengths
and limitations of these models.
</p></li>
</ul>

<h3>Title: Continual Learning with Low Rank Adaptation. (arXiv:2311.17601v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17601">http://arxiv.org/abs/2311.17601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17601]] Continual Learning with Low Rank Adaptation(http://arxiv.org/abs/2311.17601)</code></li>
<li>Summary: <p>Recent work using pretrained transformers has shown impressive performance
when fine-tuned with data from the downstream problem of interest. However,
they struggle to retain that performance when the data characteristics changes.
In this paper, we focus on continual learning, where a pre-trained transformer
is updated to perform well on new data, while retaining its performance on data
it was previously trained on. Earlier works have tackled this primarily through
methods inspired from prompt tuning. We question this choice, and investigate
the applicability of Low Rank Adaptation (LoRA) to continual learning. On a
range of domain-incremental learning benchmarks, our LoRA-based solution,
CoLoR, yields state-of-the-art performance, while still being as parameter
efficient as the prompt tuning based methods.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers. (arXiv:2311.17072v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17072">http://arxiv.org/abs/2311.17072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17072]] IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers(http://arxiv.org/abs/2311.17072)</code></li>
<li>Summary: <p>Generative training has been demonstrated to be powerful for building
visual-language models. However, on zero-shot discriminative benchmarks, there
is still a performance gap between models trained with generative and
discriminative objectives. In this paper, we aim to narrow this gap by
improving the efficacy of generative training on classification tasks, without
any finetuning processes or additional modules.
</p>
<p>Specifically, we focus on narrowing the gap between the generative captioner
and the CLIP classifier. We begin by analysing the predictions made by the
captioner and classifier and observe that the caption generation inherits the
distribution bias from the language model trained with pure text modality,
making it less grounded on the visual signal. To tackle this problem, we
redesign the scoring objective for the captioner to alleviate the
distributional bias and focus on measuring the gain of information brought by
the visual inputs. We further design a generative training objective to match
the evaluation objective. We name our model trained and evaluated from the
novel procedures as Information Gain (IG) captioner. We pretrain the models on
the public Laion-5B dataset and perform a series of discriminative evaluations.
For the zero-shot classification on ImageNet, IG captioner achieves $&gt; 18\%$
improvements over the standard captioner, achieving comparable performances
with the CLIP classifier. IG captioner also demonstrated strong performance on
zero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this
paper inspires further research towards unifying generative and discriminative
training procedures for visual-language models.
</p></li>
</ul>

<h3>Title: Generative Data Augmentation Improves Scribble-supervised Semantic Segmentation. (arXiv:2311.17121v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17121">http://arxiv.org/abs/2311.17121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17121]] Generative Data Augmentation Improves Scribble-supervised Semantic Segmentation(http://arxiv.org/abs/2311.17121)</code></li>
<li>Summary: <p>Recent advances in generative models, such as diffusion models, have made
generating high-quality synthetic images widely accessible. Prior works have
shown that training on synthetic images improves many perception tasks, such as
image classification, object detection, and semantic segmentation. We are the
first to explore generative data augmentations for scribble-supervised semantic
segmentation. We propose a generative data augmentation method that leverages a
ControlNet diffusion model conditioned on semantic scribbles to produce
high-quality training data. However, naive implementations of generative data
augmentations may inadvertently harm the performance of the downstream
segmentor rather than improve it. We leverage classifier-free diffusion
guidance to enforce class consistency and introduce encode ratios to trade off
data diversity for data realism. Using the guidance scale and encode ratio, we
are able to generate a spectrum of high-quality training images. We propose
multiple augmentation schemes and find that these schemes significantly impact
model performance, especially in the low-data regime. Our framework further
reduces the gap between the performance of scribble-supervised segmentation and
that of fully-supervised segmentation. We also show that our framework
significantly improves segmentation performance on small datasets, even
surpassing fully-supervised segmentation.
</p></li>
</ul>

<h3>Title: Generative Models: What do they know? Do they know things? Let's find out!. (arXiv:2311.17137v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17137">http://arxiv.org/abs/2311.17137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17137]] Generative Models: What do they know? Do they know things? Let's find out!(http://arxiv.org/abs/2311.17137)</code></li>
<li>Summary: <p>Generative models have been shown to be capable of synthesizing highly
detailed and realistic images. It is natural to suspect that they implicitly
learn to model some image intrinsics such as surface normals, depth, or
shadows. In this paper, we present compelling evidence that generative models
indeed internally produce high-quality scene intrinsic maps. We introduce
Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms
any generative model into a scene intrinsic predictor, capable of extracting
intrinsic scene maps directly from the original generator network without
needing additional decoders or fully fine-tuning the original network. Our
method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly
learned parameters that make up less than 0.6% of the total parameters in the
generative model. Optimized with a small set of labeled images, our
model-agnostic approach adapts to various generative architectures, including
Diffusion models, GANs, and Autoregressive models. We show that the scene
intrinsic maps produced by our method compare well with, and in some cases
surpass those generated by leading supervised techniques.
</p></li>
</ul>

<h3>Title: Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now. (arXiv:2311.17138v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17138">http://arxiv.org/abs/2311.17138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17138]] Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry(http://arxiv.org/abs/2311.17138)</code></li>
<li>Summary: <p>Generative models can produce impressively realistic images. This paper
demonstrates that generated images have geometric features different from those
of real images. We build a set of collections of generated images, prequalified
to fool simple, signal-based classifiers into believing they are real. We then
show that prequalified generated images can be identified reliably by
classifiers that only look at geometric properties. We use three such
classifiers. All three classifiers are denied access to image pixels, and look
only at derived geometric features. The first classifier looks at the
perspective field of the image, the second looks at lines detected in the
image, and the third looks at relations between detected objects and shadows.
Our procedure detects generated images more reliably than SOTA local signal
based detectors, for images from a number of distinct generators. Saliency maps
suggest that the classifiers can identify geometric problems reliably. We
conclude that current generators cannot reliably reproduce geometric properties
of real images.
</p></li>
</ul>

<h3>Title: Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation. (arXiv:2311.17409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17409">http://arxiv.org/abs/2311.17409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17409]] Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation(http://arxiv.org/abs/2311.17409)</code></li>
<li>Summary: <p>We study the problem of creating a character model that can be controlled in
real time from a single image of an anime character. A solution to this problem
would greatly reduce the cost of creating avatars, computer games, and other
interactive applications.
</p>
<p>Talking Head Anime 3 (THA3) is an open source project that attempts to
directly addresses the problem. It takes as input (1) an image of an anime
character's upper body and (2) a 45-dimensional pose vector and outputs a new
image of the same character taking the specified pose. The range of possible
movements is expressive enough for personal avatars and certain types of game
characters. However, the system is too slow to generate animations in real time
on common PCs, and its image quality can be improved.
</p>
<p>In this paper, we improve THA3 in two ways. First, we propose new
architectures for constituent networks that rotate the character's head and
body based on U-Nets with attention that are widely used in modern generative
models. The new architectures consistently yield better image quality than the
THA3 baseline. Nevertheless, they also make the whole system much slower: it
takes up to 150 milliseconds to generate a frame. Second, we propose a
technique to distill the system into a small network (less than 2 MB) that can
generate 512x512 animation frames in real time (under 30 FPS) using consumer
gaming GPUs while keeping the image quality close to that of the full system.
This improvement makes the whole system practical for real-time applications.
</p></li>
</ul>

<h3>Title: SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis. (arXiv:2311.17590v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17590">http://arxiv.org/abs/2311.17590</a></li>
<li>Code URL: https://github.com/ZiqiaoPeng/SyncTalk</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17590]] SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis(http://arxiv.org/abs/2311.17590)</code></li>
<li>Summary: <p>Achieving high synchronization in the synthesis of realistic, speech-driven
talking head videos presents a significant challenge. Traditional Generative
Adversarial Networks (GAN) struggle to maintain consistent facial identity,
while Neural Radiance Fields (NeRF) methods, although they can address this
issue, often produce mismatched lip movements, inadequate facial expressions,
and unstable head poses. A lifelike talking head requires synchronized
coordination of subject identity, lip movements, facial expressions, and head
poses. The absence of these synchronizations is a fundamental flaw, leading to
unrealistic and artificial outcomes. To address the critical issue of
synchronization, identified as the "devil" in creating realistic talking heads,
we introduce SyncTalk. This NeRF-based method effectively maintains subject
identity, enhancing synchronization and realism in talking head synthesis.
SyncTalk employs a Face-Sync Controller to align lip movements with speech and
innovatively uses a 3D facial blendshape model to capture accurate facial
expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more
natural head movements. The Portrait-Sync Generator restores hair details and
blends the generated head with the torso for a seamless visual experience.
Extensive experiments and user studies demonstrate that SyncTalk outperforms
state-of-the-art methods in synchronization and realism. We recommend watching
the supplementary video: https://ziqiaopeng.github.io/synctalk
</p></li>
</ul>

<h3>Title: ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model. (arXiv:2311.17618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17618">http://arxiv.org/abs/2311.17618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17618]] ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model(http://arxiv.org/abs/2311.17618)</code></li>
<li>Summary: <p>The advent of large language models, enabling flexibility through
instruction-driven approaches, has revolutionized many traditional generative
tasks, but large models for 3D data, particularly in comprehensively handling
3D shapes with other modalities, are still under-explored. By achieving
instruction-based shape generations, versatile multimodal generative shape
models can significantly benefit various fields like 3D virtual construction
and network-aided design. In this work, we present ShapeGPT, a shape-included
multi-modal framework to leverage strong pre-trained language models to address
multiple shape-relevant tasks. Specifically, ShapeGPT employs a
word-sentence-paragraph framework to discretize continuous shapes into shape
words, further assembles these words for shape sentences, as well as integrates
shape with instructional text for multi-modal paragraphs. To learn this
shape-language model, we use a three-stage training scheme, including shape
representation, multimodal alignment, and instruction-based generation, to
align shape-language codebooks and learn the intricate correlations among these
modalities. Extensive experiments demonstrate that ShapeGPT achieves comparable
performance across shape-relevant tasks, including text-to-shape,
shape-to-text, shape completion, and shape editing.
</p></li>
</ul>

<h3>Title: Variational Bayes image restoration with compressive autoencoders. (arXiv:2311.17744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17744">http://arxiv.org/abs/2311.17744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17744]] Variational Bayes image restoration with compressive autoencoders(http://arxiv.org/abs/2311.17744)</code></li>
<li>Summary: <p>Regularization of inverse problems is of paramount importance in
computational imaging. The ability of neural networks to learn efficient image
representations has been recently exploited to design powerful data-driven
regularizers. While state-of-the-art plug-and-play methods rely on an implicit
regularization provided by neural denoisers, alternative Bayesian approaches
consider Maximum A Posteriori (MAP) estimation in the latent space of a
generative model, thus with an explicit regularization. However,
state-of-the-art deep generative models require a huge amount of training data
compared to denoisers. Besides, their complexity hampers the optimization of
the latent MAP. In this work, we propose to use compressive autoencoders for
latent estimation. These networks, which can be seen as variational
autoencoders with a flexible latent prior, are smaller and easier to train than
state-of-the-art generative models. We then introduce the Variational Bayes
Latent Estimation (VBLE) algorithm, which performs this estimation within the
framework of variational inference. This allows for fast and easy (approximate)
posterior sampling. Experimental results on image datasets BSD and FFHQ
demonstrate that VBLE reaches similar performance than state-of-the-art
plug-and-play methods, while being able to quantify uncertainties faster than
other existing posterior sampling techniques.
</p></li>
</ul>

<h3>Title: Gaussian Shell Maps for Efficient 3D Human Generation. (arXiv:2311.17857v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17857">http://arxiv.org/abs/2311.17857</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17857]] Gaussian Shell Maps for Efficient 3D Human Generation(http://arxiv.org/abs/2311.17857)</code></li>
<li>Summary: <p>Efficient generation of 3D digital humans is important in several industries,
including virtual reality, social media, and cinematic production. 3D
generative adversarial networks (GANs) have demonstrated state-of-the-art
(SOTA) quality and diversity for generated assets. Current 3D GAN
architectures, however, typically rely on volume representations, which are
slow to render, thereby hampering the GAN training and requiring
multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps
(GSMs) as a framework that connects SOTA generator network architectures with
emerging 3D Gaussian rendering primitives using an articulable multi
shell--based scaffold. In this setting, a CNN generates a 3D texture stack with
features that are mapped to the shells. The latter represent inflated and
deflated versions of a template surface of a digital human in a canonical body
pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the
shells whose attributes are encoded in the texture features. These Gaussians
are efficiently and differentiably rendered. The ability to articulate the
shells is important during GAN training and, at inference time, to deform a
body into arbitrary user-defined poses. Our efficient rendering scheme bypasses
the need for view-inconsistent upsamplers and achieves high-quality multi-view
consistent renderings at a native resolution of $512 \times 512$ pixels. We
demonstrate that GSMs successfully generate 3D humans when trained on
single-view datasets, including SHHQ and DeepFashion.
</p></li>
</ul>

<h3>Title: Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A. (arXiv:2311.17371v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17371">http://arxiv.org/abs/2311.17371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17371]] Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A(http://arxiv.org/abs/2311.17371)</code></li>
<li>Summary: <p>Recent advancements in large language models (LLMs) underscore their
potential for responding to medical inquiries. However, ensuring that
generative agents provide accurate and reliable answers remains an ongoing
challenge. In this context, multi-agent debate (MAD) has emerged as a prominent
strategy for enhancing the truthfulness of LLMs. In this work, we provide a
comprehensive benchmark of MAD strategies for medical Q&amp;A, along with
open-source implementations. This explores the effective utilization of various
strategies including the trade-offs between cost, time, and accuracy. We build
upon these insights to provide a novel debate-prompting strategy based on agent
agreement that outperforms previously published strategies on medical Q&amp;A
tasks.
</p></li>
</ul>

<h3>Title: Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models. (arXiv:2311.17394v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17394">http://arxiv.org/abs/2311.17394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17394]] Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models(http://arxiv.org/abs/2311.17394)</code></li>
<li>Summary: <p>With the advent of sophisticated artificial intelligence (AI) technologies,
the proliferation of deepfakes and the spread of m/disinformation have emerged
as formidable threats to the integrity of information ecosystems worldwide.
This paper provides an overview of the current literature. Within the frontier
AI's crucial application in developing defense mechanisms for detecting
deepfakes, we highlight the mechanisms through which generative AI based on
large models (LM-based GenAI) craft seemingly convincing yet fabricated
contents. We explore the multifaceted implications of LM-based GenAI on
society, politics, and individual privacy violations, underscoring the urgent
need for robust defense strategies. To address these challenges, in this study,
we introduce an integrated framework that combines advanced detection
algorithms, cross-platform collaboration, and policy-driven initiatives to
mitigate the risks associated with AI-Generated Content (AIGC). By leveraging
multi-modal analysis, digital watermarking, and machine learning-based
authentication techniques, we propose a defense mechanism adaptable to AI
capabilities of ever-evolving nature. Furthermore, the paper advocates for a
global consensus on the ethical usage of GenAI and implementing cyber-wellness
educational programs to enhance public awareness and resilience against
m/disinformation. Our findings suggest that a proactive and collaborative
approach involving technological innovation and regulatory oversight is
essential for safeguarding netizens while interacting with cyberspace against
the insidious effects of deepfakes and GenAI-enabled m/disinformation
campaigns.
</p></li>
</ul>

<h3>Title: Learning to Simulate: Generative Metamodeling via Quantile Regression. (arXiv:2311.17797v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17797">http://arxiv.org/abs/2311.17797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17797]] Learning to Simulate: Generative Metamodeling via Quantile Regression(http://arxiv.org/abs/2311.17797)</code></li>
<li>Summary: <p>Stochastic simulation models, while effective in capturing the dynamics of
complex systems, are often too slow to run for real-time decision-making.
Metamodeling techniques are widely used to learn the relationship between a
summary statistic of the outputs (e.g., the mean or quantile) and the inputs of
the simulator, so that it can be used in real time. However, this methodology
requires the knowledge of an appropriate summary statistic in advance, making
it inflexible for many practical situations. In this paper, we propose a new
metamodeling concept, called generative metamodeling, which aims to construct a
"fast simulator of the simulator". This technique can generate random outputs
substantially faster than the original simulation model, while retaining an
approximately equal conditional distribution given the same inputs. Once
constructed, a generative metamodel can instantaneously generate a large amount
of random outputs as soon as the inputs are specified, thereby facilitating the
immediate computation of any summary statistic for real-time decision-making.
Furthermore, we propose a new algorithm -- quantile-regression-based generative
metamodeling (QRGMM) -- and study its convergence and rate of convergence.
Extensive numerical experiments are conducted to investigate the empirical
performance of QRGMM, compare it with other state-of-the-art generative
algorithms, and demonstrate its usefulness in practical real-time
decision-making.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Compositional Chain-of-Thought Prompting for Large Multimodal Models. (arXiv:2311.17076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17076">http://arxiv.org/abs/2311.17076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17076]] Compositional Chain-of-Thought Prompting for Large Multimodal Models(http://arxiv.org/abs/2311.17076)</code></li>
<li>Summary: <p>The combination of strong visual backbones and Large Language Model (LLM)
reasoning has led to Large Multimodal Models (LMMs) becoming the current
standard for a wide range of vision and language (VL) tasks. However, recent
research has shown that even the most advanced LMMs still struggle to capture
aspects of compositional visual reasoning, such as attributes and relationships
between objects. One solution is to utilize scene graphs (SGs)--a formalization
of objects and their relations and attributes that has been extensively used as
a bridge between the visual and textual domains. Yet, scene graph data requires
scene graph annotations, which are expensive to collect and thus not easily
scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic
forgetting of the pretraining objective. To overcome this, inspired by
chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a
novel zero-shot Chain-of-Thought prompting method that utilizes SG
representations in order to extract compositional knowledge from an LMM.
Specifically, we first generate an SG using the LMM, and then use that SG in
the prompt to produce a response. Through extensive experiments, we find that
the proposed CCoT approach not only improves LMM performance on several vision
and language VL compositional benchmarks but also improves the performance of
several popular LMMs on general multimodal benchmarks, without the need for
fine-tuning or annotated ground-truth SGs.
</p></li>
</ul>

<h3>Title: SEED-Bench-2: Benchmarking Multimodal Large Language Models. (arXiv:2311.17092v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17092">http://arxiv.org/abs/2311.17092</a></li>
<li>Code URL: https://github.com/ailab-cvc/seed-bench</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17092]] SEED-Bench-2: Benchmarking Multimodal Large Language Models(http://arxiv.org/abs/2311.17092)</code></li>
<li>Summary: <p>Multimodal large language models (MLLMs), building upon the foundation of
powerful large language models (LLMs), have recently demonstrated exceptional
capabilities in generating not only texts but also images given interleaved
multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However,
existing MLLM benchmarks remain limited to assessing only models' comprehension
ability of single image-text inputs, failing to keep up with the strides made
in MLLMs. A comprehensive benchmark is imperative for investigating the
progress and uncovering the limitations of current MLLMs. In this work, we
categorize the capabilities of MLLMs into hierarchical levels from $L_0$ to
$L_4$ based on the modalities they can accept and generate, and propose
SEED-Bench-2, a comprehensive benchmark that evaluates the
\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2
comprises 24K multiple-choice questions with accurate human annotations, which
spans 27 dimensions, including the evaluation of both text and image
generation. Multiple-choice questions with groundtruth options derived from
human annotation enables an objective and efficient assessment of model
performance, eliminating the need for human or GPT intervention during
evaluation. We further evaluate the performance of 23 prominent open-source
MLLMs and summarize valuable observations. By revealing the limitations of
existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to
provide insights that will motivate future research towards the goal of General
Artificial Intelligence. Dataset and evaluation code are available at
\href{https://github.com/AILab-CVC/SEED-Bench}
</p></li>
</ul>

<h3>Title: Large Model Based Referring Camouflaged Object Detection. (arXiv:2311.17122v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17122">http://arxiv.org/abs/2311.17122</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17122]] Large Model Based Referring Camouflaged Object Detection(http://arxiv.org/abs/2311.17122)</code></li>
<li>Summary: <p>Referring camouflaged object detection (Ref-COD) is a recently-proposed
problem aiming to segment out specified camouflaged objects matched with a
textual or visual reference. This task involves two major challenges: the COD
domain-specific perception and multimodal reference-image alignment. Our
motivation is to make full use of the semantic intelligence and intrinsic
knowledge of recent Multimodal Large Language Models (MLLMs) to decompose this
complex task in a human-like way. As language is highly condensed and
inductive, linguistic expression is the main media of human knowledge learning,
and the transmission of knowledge information follows a multi-level progression
from simplicity to complexity. In this paper, we propose a large-model-based
Multi-Level Knowledge-Guided multimodal method for Ref-COD termed MLKG, where
multi-level knowledge descriptions from MLLM are organized to guide the large
vision model of segmentation to perceive the camouflage-targets and
camouflage-scene progressively and meanwhile deeply align the textual
references with camouflaged photos. To our knowledge, our contributions mainly
include: (1) This is the first time that the MLLM knowledge is studied for
Ref-COD and COD. (2) We, for the first time, propose decomposing Ref-COD into
two main perspectives of perceiving the target and scene by integrating MLLM
knowledge, and contribute a multi-level knowledge-guided method. (3) Our method
achieves the state-of-the-art on the Ref-COD benchmark outperforming numerous
strong competitors. Moreover, thanks to the injected rich knowledge, it
demonstrates zero-shot generalization ability on uni-modal COD datasets. We
will release our code soon.
</p></li>
</ul>

<h3>Title: Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis. (arXiv:2311.17126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17126">http://arxiv.org/abs/2311.17126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17126]] Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis(http://arxiv.org/abs/2311.17126)</code></li>
<li>Summary: <p>Recent advancements in text-to-image (T2I) generative models have shown
remarkable capabilities in producing diverse and imaginative visuals based on
text prompts. Despite the advancement, these diffusion models sometimes
struggle to translate the semantic content from the text into images entirely.
While conditioning on the layout has shown to be effective in improving the
compositional ability of T2I diffusion models, they typically require manual
layout input. In this work, we introduce a novel approach to improving T2I
diffusion models using Large Language Models (LLMs) as layout generators. Our
method leverages the Chain-of-Thought prompting of LLMs to interpret text and
generate spatially reasonable object layouts. The generated layout is then used
to enhance the generated images' composition and spatial accuracy. Moreover, we
propose an efficient adapter based on a cross-attention mechanism, which
explicitly integrates the layout information into the stable diffusion models.
Our experiments demonstrate significant improvements in image quality and
layout accuracy, showcasing the potential of LLMs in augmenting generative
image models.
</p></li>
</ul>

<h3>Title: VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models. (arXiv:2311.17404v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17404">http://arxiv.org/abs/2311.17404</a></li>
<li>Code URL: https://github.com/lscpku/vitatecs</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17404]] VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models(http://arxiv.org/abs/2311.17404)</code></li>
<li>Summary: <p>The ability to perceive how objects change over time is a crucial ingredient
in human intelligence. However, current benchmarks cannot faithfully reflect
the temporal understanding abilities of video-language models (VidLMs) due to
the existence of static visual shortcuts. To remedy this issue, we present
VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal
Concept underStanding. Specifically, we first introduce a fine-grained taxonomy
of temporal concepts in natural language in order to diagnose the capability of
VidLMs to comprehend different temporal aspects. Furthermore, to disentangle
the correlation between static and temporal information, we generate
counterfactual video descriptions that differ from the original one only in the
specified temporal aspect. We employ a semi-automatic data collection framework
using large language models and human-in-the-loop annotation to obtain
high-quality counterfactual descriptions efficiently. Evaluation of
representative video-language understanding models confirms their deficiency in
temporal understanding, revealing the need for greater emphasis on the temporal
elements in video-language research.
</p></li>
</ul>

<h3>Title: VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following. (arXiv:2311.17647v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17647">http://arxiv.org/abs/2311.17647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17647]] VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following(http://arxiv.org/abs/2311.17647)</code></li>
<li>Summary: <p>We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to
evaluate the visual instruction following capability of Multimodal Large
Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs
by embedding the instructions into the visual scenes, demanding strong visual
interpretative skills for instruction following. We adapt VIM to various
benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM
bench, and probe diverse MLLMs across three distinct in-context learning
settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a
significant performance disparity between the open-source MLLMs and GPT-4V,
implying that their proficiency in visual instruction comprehension is not up
to par. Our results highlight a promising direction for the enhancement of
MLLMs capabilities on instruction following. We aim VIM to serve as a useful
norm for advancing the state of the art and driving further progress in the
field.
</p></li>
</ul>

<h3>Title: ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?. (arXiv:2311.17107v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17107">http://arxiv.org/abs/2311.17107</a></li>
<li>Code URL: https://github.com/rlacombe/climatex</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17107]] ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?(http://arxiv.org/abs/2311.17107)</code></li>
<li>Summary: <p>Evaluating the accuracy of outputs generated by Large Language Models (LLMs)
is especially important in the climate science and policy domain. We introduce
the Expert Confidence in Climate Statements (ClimateX) dataset, a novel,
curated, expert-labeled dataset consisting of 8094 climate statements collected
from the latest Intergovernmental Panel on Climate Change (IPCC) reports,
labeled with their associated confidence levels. Using this dataset, we show
that recent LLMs can classify human expert confidence in climate-related
statements, especially in a few-shot learning setting, but with limited (up to
47%) accuracy. Overall, models exhibit consistent and significant
over-confidence on low and medium confidence statements. We highlight
implications of our results for climate communication, LLMs evaluation
strategies, and the use of LLMs in information retrieval systems.
</p></li>
</ul>

<h3>Title: Quantifying the redundancy between prosody and text. (arXiv:2311.17233v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17233">http://arxiv.org/abs/2311.17233</a></li>
<li>Code URL: https://github.com/lu-wo/quantifying-redundancy</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17233]] Quantifying the redundancy between prosody and text(http://arxiv.org/abs/2311.17233)</code></li>
<li>Summary: <p>Prosody -- the suprasegmental component of speech, including pitch, loudness,
and tempo -- carries critical aspects of meaning. However, the relationship
between the information conveyed by prosody vs. by the words themselves remains
poorly understood. We use large language models (LLMs) to estimate how much
information is redundant between prosody and the words themselves. Using a
large spoken corpus of English audiobooks, we extract prosodic features aligned
to individual words and test how well they can be predicted from LLM
embeddings, compared to non-contextual word embeddings. We find a high degree
of redundancy between the information carried by the words and prosodic
information across several prosodic features, including intensity, duration,
pauses, and pitch contours. Furthermore, a word's prosodic information is
redundant with both the word itself and the context preceding as well as
following it. Still, we observe that prosodic features can not be fully
predicted from text, suggesting that prosody carries information above and
beyond the words. Along with this paper, we release a general-purpose data
processing pipeline for quantifying the relationship between linguistic
information and extra-linguistic features.
</p></li>
</ul>

<h3>Title: Universal Self-Consistency for Large Language Model Generation. (arXiv:2311.17311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17311">http://arxiv.org/abs/2311.17311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17311]] Universal Self-Consistency for Large Language Model Generation(http://arxiv.org/abs/2311.17311)</code></li>
<li>Summary: <p>Self-consistency with chain-of-thought prompting (CoT) has demonstrated
remarkable performance gains on various challenging tasks, by utilizing
multiple reasoning paths sampled from large language models (LLMs). However,
self-consistency relies on the answer extraction process to aggregate multiple
solutions, which is not applicable to free-form answers. In this work, we
propose Universal Self-Consistency (USC), which leverages LLMs themselves to
select the most consistent answer among multiple candidates. We evaluate USC on
a variety of benchmarks, including mathematical reasoning, code generation,
long-context summarization, and open-ended question answering. On open-ended
generation tasks where the original self-consistency method is not applicable,
USC effectively utilizes multiple samples and improves the performance. For
mathematical reasoning, USC matches the standard self-consistency performance
without requiring the answer formats to be similar. Finally, without access to
execution results, USC also matches the execution-based voting performance on
code generation.
</p></li>
</ul>

<h3>Title: Biomedical knowledge graph-enhanced prompt generation for large language models. (arXiv:2311.17330v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17330">http://arxiv.org/abs/2311.17330</a></li>
<li>Code URL: https://github.com/BaranziniLab/KG_RAG</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17330]] Biomedical knowledge graph-enhanced prompt generation for large language models(http://arxiv.org/abs/2311.17330)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have been driving progress in AI at an
unprecedented rate, yet still face challenges in knowledge-intensive domains
like biomedicine. Solutions such as pre-training and domain-specific
fine-tuning add substantial computational overhead, and the latter require
domain-expertise. External knowledge infusion is task-specific and requires
model training. Here, we introduce a task-agnostic Knowledge Graph-based
Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive
biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to
generate meaningful biomedical text rooted in established knowledge. KG-RAG
consistently enhanced the performance of LLMs across various prompt types,
including one-hop and two-hop prompts, drug repurposing queries, biomedical
true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG
provides a remarkable 71% boost in the performance of the Llama-2 model on the
challenging MCQ dataset, demonstrating the framework's capacity to empower
open-source models with fewer parameters for domain-specific questions.
Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as
GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ
data. Our approach was also able to address drug repurposing questions,
returning meaningful repurposing suggestions. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM, respectively,
in an optimized fashion, thus enhancing the adaptability of general-purpose
LLMs to tackle domain-specific questions in a unified framework.
</p></li>
</ul>

<h3>Title: Are Large Language Models Good Fact Checkers: A Preliminary Study. (arXiv:2311.17355v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17355">http://arxiv.org/abs/2311.17355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17355]] Are Large Language Models Good Fact Checkers: A Preliminary Study(http://arxiv.org/abs/2311.17355)</code></li>
<li>Summary: <p>Recently, Large Language Models (LLMs) have drawn significant attention due
to their outstanding reasoning capabilities and extensive knowledge repository,
positioning them as superior in handling various natural language processing
tasks compared to other language models. In this paper, we present a
preliminary investigation into the potential of LLMs in fact-checking. This
study aims to comprehensively evaluate various LLMs in tackling specific
fact-checking subtasks, systematically evaluating their capabilities, and
conducting a comparative analysis of their performance against pre-trained and
state-of-the-art low-parameter models. Experiments demonstrate that LLMs
achieve competitive performance compared to other small models in most
scenarios. However, they encounter challenges in effectively handling Chinese
fact verification and the entirety of the fact-checking pipeline due to
language inconsistencies and hallucinations. These findings underscore the need
for further exploration and research to enhance the proficiency of LLMs as
reliable fact-checkers, unveiling the potential capability of LLMs and the
possible challenges in fact-checking tasks.
</p></li>
</ul>

<h3>Title: CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs. (arXiv:2311.17376v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17376">http://arxiv.org/abs/2311.17376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17376]] CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs(http://arxiv.org/abs/2311.17376)</code></li>
<li>Summary: <p>Instruction-based multitasking has played a critical role in the success of
large language models (LLMs) in multi-turn dialog applications. While publicly
available LLMs have shown promising performance, when exposed to complex
instructions with multiple constraints, they lag against state-of-the-art
models like ChatGPT. In this work, we hypothesize that the availability of
large-scale complex demonstrations is crucial in bridging this gap. Focusing on
dialog applications, we propose a novel framework, CESAR, that unifies a large
number of dialog tasks in the same format and allows programmatic induction of
complex instructions without any manual effort.
</p>
<p>We apply CESAR on InstructDial, a benchmark for instruction-based dialog
tasks. We further enhance InstructDial with new datasets and tasks and utilize
CESAR to induce complex tasks with compositional instructions. This results in
a new benchmark called InstructDial++, which includes 63 datasets with 86 basic
tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the
scalability of CESAR in providing rich instructions. Models trained on
InstructDial++ can follow compositional prompts, such as prompts that ask for
multiple stylistic constraints.
</p></li>
</ul>

<h3>Title: Unveiling the Implicit Toxicity in Large Language Models. (arXiv:2311.17391v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17391">http://arxiv.org/abs/2311.17391</a></li>
<li>Code URL: https://github.com/thu-coai/implicit-toxicity</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17391]] Unveiling the Implicit Toxicity in Large Language Models(http://arxiv.org/abs/2311.17391)</code></li>
<li>Summary: <p>The open-endedness of large language models (LLMs) combined with their
impressive capabilities may lead to new safety issues when being exploited for
malicious use. While recent studies primarily focus on probing toxic outputs
that can be easily detected with existing toxicity classifiers, we show that
LLMs can generate diverse implicit toxic outputs that are exceptionally
difficult to detect via simply zero-shot prompting. Moreover, we propose a
reinforcement learning (RL) based attacking method to further induce the
implicit toxicity in LLMs. Specifically, we optimize the language model with a
reward that prefers implicit toxic outputs to explicit toxic and non-toxic
ones. Experiments on five widely-adopted toxicity classifiers demonstrate that
the attack success rate can be significantly improved through RL fine-tuning.
For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate
of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose
a significant threat in generating undetectable implicit toxic outputs. We
further show that fine-tuning toxicity classifiers on the annotated examples
from our attacking method can effectively enhance their ability to detect
LLM-generated implicit toxic language. The code is publicly available at
https://github.com/thu-coai/Implicit-Toxicity.
</p></li>
</ul>

<h3>Title: CLOMO: Counterfactual Logical Modification with Large Language Models. (arXiv:2311.17438v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17438">http://arxiv.org/abs/2311.17438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17438]] CLOMO: Counterfactual Logical Modification with Large Language Models(http://arxiv.org/abs/2311.17438)</code></li>
<li>Summary: <p>In this study, we delve into the realm of counterfactual reasoning
capabilities of large language models (LLMs). Our primary objective is to
cultivate the counterfactual thought processes within LLMs and rigorously
assess these processes for their validity. Specifically, we introduce a novel
task, Counterfactual Logical Modification (CLOMO), and a high-quality
human-annotated benchmark. In this task, LLMs must adeptly alter a given
argumentative text to uphold a predetermined logical relationship. To
effectively evaluate a generation model's counterfactual capabilities, we
propose an innovative evaluation metric, the LogicAware Counterfactual Score to
directly evaluate the natural language output of LLMs instead of modeling the
task as a multiple-choice problem. Analysis shows that the proposed automatic
metric aligns well with human preference. Our experimental results show that
while LLMs demonstrate a notable capacity for logical counterfactual thinking,
there remains a discernible gap between their current abilities and human
performance.
</p></li>
</ul>

<h3>Title: Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model. (arXiv:2311.17487v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17487">http://arxiv.org/abs/2311.17487</a></li>
<li>Code URL: https://github.com/miulab/taiwan-llm</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17487]] Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model(http://arxiv.org/abs/2311.17487)</code></li>
<li>Summary: <p>In the realm of language models, the nuanced linguistic and cultural
intricacies of Traditional Chinese, as spoken in Taiwan, have been largely
overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model
that specifically caters to the Traditional Chinese language, with a focus on
the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and
instruction-finetuning datasets, we have developed a model that not only
understands the complexities of Traditional Chinese but also embodies the
cultural context of Taiwan. Taiwan LLM represents the first of its kind, a
model that is not only linguistically accurate but also culturally resonant
with its user base. Our evaluations demonstrate that Taiwan LLM achieves
superior performance in understanding and generating Traditional Chinese text,
outperforming existing models that are predominantly trained on Simplified
Chinese or English. The open-source release of Taiwan LLM invites collaboration
and further innovation, ensuring that the linguistic diversity of Chinese
speakers is embraced and well-served. The model, datasets, and further
resources are made publicly available to foster ongoing research and
development in this field.
</p></li>
</ul>

<h3>Title: Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models. (arXiv:2311.17502v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17502">http://arxiv.org/abs/2311.17502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17502]] Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models(http://arxiv.org/abs/2311.17502)</code></li>
<li>Summary: <p>Community Question Answering (CQA) becomes increasingly prevalent in recent
years. However, there are a large number of answers, which is difficult for
users to select the relevant answers. Therefore, answer selection is a very
significant subtask of CQA. In this paper, we first propose the Question-Answer
cross attention networks (QAN) with pre-trained models for answer selection and
utilize large language model (LLM) to perform answer selection with knowledge
augmentation. Specifically, we apply the BERT model as the encoder layer to do
pre-training for question subjects, question bodies and answers, respectively,
then the cross attention mechanism selects the most relevant answer for
different questions. Experiments show that the QAN model achieves
state-of-the-art performance on two datasets, SemEval2015 and SemEval2017.
Moreover, we use the LLM to generate external knowledge from questions and
correct answers to achieve knowledge augmentation for the answer selection task
by LLM, while optimizing the prompt of LLM in different aspects. The results
show that the introduction of external knowledge can improve the correct answer
selection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM
can also select the correct answer on more questions by optimized prompt.
</p></li>
</ul>

<h3>Title: TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models. (arXiv:2311.17667v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17667">http://arxiv.org/abs/2311.17667</a></li>
<li>Code URL: https://github.com/zchuz/timebench</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17667]] TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models(http://arxiv.org/abs/2311.17667)</code></li>
<li>Summary: <p>Understanding time is a pivotal aspect of human cognition, crucial in the
broader framework of grasping the intricacies of the world. Previous studies
typically focus on specific aspects of time, lacking a comprehensive temporal
reasoning benchmark. To address this issue, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena, which provides a thorough evaluation
for investigating the temporal reasoning capabilities of large language models.
We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and
Mistral, incorporating chain-of-thought prompting. Our experimental results
indicate a significant performance gap between the state-of-the-art LLMs and
humans, highlighting that there is still a considerable distance to cover in
temporal reasoning. We aspire for TimeBench to serve as a comprehensive
benchmark, fostering research in temporal reasoning for LLMs. Our resource is
available at https://github.com/zchuz/TimeBench
</p></li>
</ul>

<h3>Title: AviationGPT: A Large Language Model for the Aviation Domain. (arXiv:2311.17686v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17686">http://arxiv.org/abs/2311.17686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17686]] AviationGPT: A Large Language Model for the Aviation Domain(http://arxiv.org/abs/2311.17686)</code></li>
<li>Summary: <p>The advent of ChatGPT and GPT-4 has captivated the world with large language
models (LLMs), demonstrating exceptional performance in question-answering,
summarization, and content generation. The aviation industry is characterized
by an abundance of complex, unstructured text data, replete with technical
jargon and specialized terminology. Moreover, labeled data for model building
are scarce in this domain, resulting in low usage of aviation text data. The
emergence of LLMs presents an opportunity to transform this situation, but
there is a lack of LLMs specifically designed for the aviation domain. To
address this gap, we propose AviationGPT, which is built on open-source LLaMA-2
and Mistral architectures and continuously trained on a wealth of carefully
curated aviation datasets. Experimental results reveal that AviationGPT offers
users multiple advantages, including the versatility to tackle diverse natural
language processing (NLP) problems (e.g., question-answering, summarization,
document writing, information extraction, report querying, data cleaning, and
interactive data exploration). It also provides accurate and contextually
relevant responses within the aviation domain and significantly improves
performance (e.g., over a 40% performance gain in tested cases). With
AviationGPT, the aviation industry is better equipped to address more complex
research problems and enhance the efficiency and safety of National Airspace
System (NAS) operations.
</p></li>
</ul>

<h3>Title: How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation. (arXiv:2311.17696v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17696">http://arxiv.org/abs/2311.17696</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17696]] How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation(http://arxiv.org/abs/2311.17696)</code></li>
<li>Summary: <p>Artificial intelligence is transforming education through data-driven,
personalized learning solutions. This paper introduces AI Tutor, an innovative
web application that provides personalized tutoring in any subject using
state-of-the-art Large Language Model (LLM). AI Tutor ingests course materials
to construct an adaptive knowledge base tailored to the course. When students
pose questions, it retrieves the most relevant information and generates
detailed, conversational responses citing supporting evidence. The system is
powered by advanced large language models and Retrieval-Augmented Generation
(RAG) techniques for accurate, natural question answering. We present a
fully-functional web interface and video demonstration that showcase AI Tutor's
versatility across diverse subjects and its ability to produce pedagogically
cogent responses. While an initial prototype, this work represents a pioneering
step toward AI-enabled tutoring systems that can democratize access to
high-quality, customized educational support.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: I-MedSAM: Implicit Medical Image Segmentation with Segment Anything. (arXiv:2311.17081v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17081">http://arxiv.org/abs/2311.17081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17081]] I-MedSAM: Implicit Medical Image Segmentation with Segment Anything(http://arxiv.org/abs/2311.17081)</code></li>
<li>Summary: <p>With the development of Deep Neural Networks (DNNs), many efforts have been
made to handle medical image segmentation. Traditional methods such as nnUNet
train specific segmentation models on the individual datasets. Plenty of recent
methods have been proposed to adapt the foundational Segment Anything Model
(SAM) to medical image segmentation. However, they still focus on discrete
representations to generate pixel-wise predictions, which are spatially
inflexible and scale poorly to higher resolution. In contrast, implicit methods
learn continuous representations for segmentation, which is crucial for medical
image segmentation. In this paper, we propose I-MedSAM, which leverages the
benefits of both continuous representations and SAM, to obtain better
cross-domain ability and accurate boundary delineation. Since medical image
segmentation needs to predict detailed segmentation boundaries, we designed a
novel adapter to enhance the SAM features with high-frequency information
during Parameter Efficient Fine Tuning (PEFT). To convert the SAM features and
coordinates into continuous segmentation output, we utilize Implicit Neural
Representation (INR) to learn an implicit segmentation decoder. We also propose
an uncertainty-guided sampling strategy for efficient learning of INR.
Extensive evaluations on 2D medical image segmentation tasks have shown that
our proposed method with only 1.6M trainable parameters outperforms existing
methods including discrete and continuous methods. The code will be released.
</p></li>
</ul>

<h3>Title: Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model. (arXiv:2311.17112v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17112">http://arxiv.org/abs/2311.17112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17112]] Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model(http://arxiv.org/abs/2311.17112)</code></li>
<li>Summary: <p>Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleash
the potential of large foundation models in novel scenarios with limited
training data. In the computer vision community, PEFT has shown effectiveness
in image classification, but little research has studied its ability for image
segmentation. Fine-tuning segmentation models usually require a heavier
adjustment of parameters to align the proper projection directions in the
parameter space for new scenarios. This raises a challenge to existing PEFT
algorithms, as they often inject a limited number of individual parameters into
each block, which prevents substantial adjustment of the projection direction
of the parameter space due to the limitation of Hidden Markov Chain along
blocks. In this paper, we equip PEFT with a cross-block orchestration mechanism
to enable the adaptation of the Segment Anything Model (SAM) to various
downstream scenarios. We introduce a novel inter-block communication module,
which integrates a learnable relation matrix to facilitate communication among
different coefficient sets of each PEFT block's parameter space. Moreover, we
propose an intra-block enhancement module, which introduces a linear projection
head whose weights are generated from a hyper-complex layer, further enhancing
the impact of the adjustment of projection directions on the entire parameter
space. Extensive experiments on diverse benchmarks demonstrate that our
proposed approach consistently improves the segmentation performance
significantly on novel scenarios with only around 1K additional parameters.
</p></li>
</ul>

<h3>Title: Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation. (arXiv:2311.17325v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17325">http://arxiv.org/abs/2311.17325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17325]] Alternate Diverse Teaching for Semi-supervised Medical Image Segmentation(http://arxiv.org/abs/2311.17325)</code></li>
<li>Summary: <p>Semi-supervised medical image segmentation studies have shown promise in
training models with limited labeled data. However, current dominant
teacher-student based approaches can suffer from the confirmation bias. To
address this challenge, we propose AD-MT, an alternate diverse teaching
approach in a teacher-student framework. It involves a single student model and
two non-trainable teacher models that are momentum-updated periodically and
randomly in an alternate fashion. To mitigate the confirmation bias from the
diverse supervision, the core of AD-MT lies in two proposed modules: the Random
Periodic Alternate (RPA) Updating Module and the Conflict-Combating Module
(CCM). The RPA schedules the alternating diverse updating process with
complementary data batches, distinct data augmentation, and random switching
periods to encourage diverse reasoning from different teaching perspectives.
The CCM employs an entropy-based ensembling strategy to encourage the model to
learn from both the consistent and conflicting predictions between the
teachers. Experimental results demonstrate the effectiveness and superiority of
our AD-MT on the 2D and 3D medical segmentation benchmarks across various
semi-supervised settings.
</p></li>
</ul>

<h3>Title: Continual Learning for Image Segmentation with Dynamic Query. (arXiv:2311.17450v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17450">http://arxiv.org/abs/2311.17450</a></li>
<li>Code URL: https://github.com/weijiawu/cisdq</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17450]] Continual Learning for Image Segmentation with Dynamic Query(http://arxiv.org/abs/2311.17450)</code></li>
<li>Summary: <p>Image segmentation based on continual learning exhibits a critical drop of
performance, mainly due to catastrophic forgetting and background shift, as
they are required to incorporate new classes continually. In this paper, we
propose a simple, yet effective Continual Image Segmentation method with
incremental Dynamic Query (CISDQ), which decouples the representation learning
of both old and new knowledge with lightweight query embedding. CISDQ mainly
includes three contributions: 1) We define dynamic queries with adaptive
background class to exploit past knowledge and learn future classes naturally.
2) CISDQ proposes a class/instance-aware Query Guided Knowledge Distillation
strategy to overcome catastrophic forgetting by capturing the inter-class
diversity and intra-class identity. 3) Apart from semantic segmentation, CISDQ
introduce the continual learning for instance segmentation in which
instance-wise labeling and supervision are considered. Extensive experiments on
three datasets for two tasks (i.e., continual semantic and instance
segmentation are conducted to demonstrate that CISDQ achieves the
state-of-the-art performance, specifically, obtaining 4.4% and 2.9% mIoU
improvements for the ADE 100-10 (6 steps) setting and ADE 100-5 (11 steps)
setting.
</p></li>
</ul>

<h3>Title: Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation. (arXiv:2311.17491v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17491">http://arxiv.org/abs/2311.17491</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17491]] Spherical Frustum Sparse Convolution Network for LiDAR Point Cloud Semantic Segmentation(http://arxiv.org/abs/2311.17491)</code></li>
<li>Summary: <p>LiDAR point cloud semantic segmentation enables the robots to obtain
fine-grained semantic information of the surrounding environment. Recently,
many works project the point cloud onto the 2D image and adopt the 2D
Convolutional Neural Networks (CNNs) or vision transformer for LiDAR point
cloud semantic segmentation. However, since more than one point can be
projected onto the same 2D position but only one point can be preserved, the
previous 2D image-based segmentation methods suffer from inevitable quantized
information loss. To avoid quantized information loss, in this paper, we
propose a novel spherical frustum structure. The points projected onto the same
2D position are preserved in the spherical frustums. Moreover, we propose a
memory-efficient hash-based representation of spherical frustums. Through the
hash-based representation, we propose the Spherical Frustum sparse Convolution
(SFC) and Frustum Fast Point Sampling (F2PS) to convolve and sample the points
stored in spherical frustums respectively. Finally, we present the Spherical
Frustum sparse Convolution Network (SFCNet) to adopt 2D CNNs for LiDAR point
cloud semantic segmentation without quantized information loss. Extensive
experiments on the SemanticKITTI and nuScenes datasets demonstrate that our
SFCNet outperforms the 2D image-based semantic segmentation methods based on
conventional spherical projection. The source code will be released later.
</p></li>
</ul>

<h3>Title: SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation. (arXiv:2311.17707v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17707">http://arxiv.org/abs/2311.17707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17707]] SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation(http://arxiv.org/abs/2311.17707)</code></li>
<li>Summary: <p>We introduce SAMPro3D for zero-shot 3D indoor scene segmentation. Given the
3D point cloud and multiple posed 2D frames of 3D scenes, our approach segments
3D scenes by applying the pretrained Segment Anything Model (SAM) to 2D frames.
Our key idea involves locating 3D points in scenes as natural 3D prompts to
align their projected pixel prompts across frames, ensuring frame-consistency
in both pixel prompts and their SAM-predicted masks. Moreover, we suggest
filtering out low-quality 3D prompts based on feedback from all 2D frames, for
enhancing segmentation quality. We also propose to consolidate different 3D
prompts if they are segmenting the same object, bringing a more comprehensive
segmentation. Notably, our method does not require any additional training on
domain-specific data, enabling us to preserve the zero-shot power of SAM.
Extensive qualitative and quantitative results show that our method
consistently achieves higher quality and more diverse segmentation than
previous zero-shot or fully supervised approaches, and in many cases even
surpasses human-level annotations. The project page can be accessed at
https://mutianxu.github.io/sampro3d/.
</p></li>
</ul>

<h3>Title: One-Shot Open Affordance Learning with Foundation Models. (arXiv:2311.17776v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.17776">http://arxiv.org/abs/2311.17776</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.17776]] One-Shot Open Affordance Learning with Foundation Models(http://arxiv.org/abs/2311.17776)</code></li>
<li>Summary: <p>We introduce One-shot Open Affordance Learning (OOAL), where a model is
trained with just one example per base object category, but is expected to
identify novel objects and affordances. While vision-language models excel at
recognizing novel objects and scenes, they often struggle to understand finer
levels of granularity such as affordances. To handle this issue, we conduct a
comprehensive analysis of existing foundation models, to explore their inherent
understanding of affordances and assess the potential for data-limited
affordance learning. We then propose a vision-language framework with simple
and effective designs that boost the alignment between visual features and
affordance text embeddings. Experiments on two affordance segmentation
benchmarks show that the proposed method outperforms state-of-the-art models
with less than 1% of the full training data, and exhibits reasonable
generalization capability on unseen objects and affordances.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
