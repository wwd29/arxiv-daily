<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: AccelAT: A Framework for Accelerating the Adversarial Training of Deep Neural Networks through Accuracy Gradient. (arXiv:2210.06888v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06888">http://arxiv.org/abs/2210.06888</a></li>
<li>Code URL: <a href="https://github.com/nikfam/accelat">https://github.com/nikfam/accelat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06888] AccelAT: A Framework for Accelerating the Adversarial Training of Deep Neural Networks through Accuracy Gradient](http://arxiv.org/abs/2210.06888)</code></li>
<li>Summary: <p>Adversarial training is exploited to develop a robust Deep Neural Network
(DNN) model against the malicious altered data. These attacks may have
catastrophic effects on DNN models but are indistinguishable for a human being.
For example, an external attack can modify an image adding noises invisible for
a human eye, but a DNN model misclassified the image. A key objective for
developing robust DNN models is to use a learning algorithm that is fast but
can also give model that is robust against different types of adversarial
attacks. Especially for adversarial training, enormously long training times
are needed for obtaining high accuracy under many different types of
adversarial samples generated using different adversarial attack techniques.
</p></li>
</ul>

<p>This paper aims at accelerating the adversarial training to enable fast
development of robust DNN models against adversarial attacks. The general
method for improving the training performance is the hyperparameters
fine-tuning, where the learning rate is one of the most crucial
hyperparameters. By modifying its shape (the value over time) and value during
the training, we can obtain a model robust to adversarial attacks faster than
standard training.
</p>
<p>First, we conduct experiments on two different datasets (CIFAR10, CIFAR100),
exploring various techniques. Then, this analysis is leveraged to develop a
novel fast training methodology, AccelAT, which automatically adjusts the
learning rate for different epochs based on the accuracy gradient. The
experiments show comparable results with the related works, and in several
experiments, the adversarial training of DNNs using our AccelAT framework is
conducted up to 2 times faster than the existing techniques. Thus, our findings
boost the speed of adversarial training in an era in which security and
performance are fundamental optimization objectives in DNN-based applications.
</p>

<h3>Title: Blockchain for Unmanned Underwater Drones: Research Issues, Challenges, Trends and Future Directions. (arXiv:2210.06540v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06540">http://arxiv.org/abs/2210.06540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06540] Blockchain for Unmanned Underwater Drones: Research Issues, Challenges, Trends and Future Directions](http://arxiv.org/abs/2210.06540)</code></li>
<li>Summary: <p>Underwater drones have found a place in oceanography, oceanic research,
bathymetric surveys, military, surveillance, monitoring, undersea exploration,
mining, commercial diving, photography and several other activities. Drones
housed with several sensors and complex propulsion systems help oceanographic
scientists and undersea explorers to map the seabed, study waves, view dead
zones, analyze fish counts, predict tidal wave behaviors, aid in finding
shipwrecks, building windfarms, examine oil platforms located in deep seas and
inspect nuclear reactors in the ship vessels. While drones can be explicitly
programmed for specific missions, data security and privacy are crucial issues
of serious concern. Blockchain has emerged as a key enabling technology,
amongst other disruptive technological enablers, to address security, data
sharing, storage, process tracking, collaboration and resource management. This
study presents a comprehensive review on the utilization of Blockchain in
different underwater applications, discussing use cases and detailing benefits.
Potential challenges of underwater applications addressed by Blockchain have
been detailed. This work identifies knowledge gaps between theoretical research
and real-time Blockchain integration in realistic underwater drone
applications. The key limitations for effective integration of Blockchain in
real-time integration in UUD applications, along with directions for future
research have been presented.
</p></li>
</ul>

<h3>Title: BLADERUNNER: Rapid Countermeasure for Synthetic (AI-Generated) StyleGAN Faces. (arXiv:2210.06587v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06587">http://arxiv.org/abs/2210.06587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06587] BLADERUNNER: Rapid Countermeasure for Synthetic (AI-Generated) StyleGAN Faces](http://arxiv.org/abs/2210.06587)</code></li>
<li>Summary: <p>StyleGAN is the open-sourced TensorFlow implementation made by NVIDIA. It has
revolutionized high quality facial image generation. However, this
democratization of Artificial Intelligence / Machine Learning (AI/ML)
algorithms has enabled hostile threat actors to establish cyber personas or
sock-puppet accounts in social media platforms. These ultra-realistic synthetic
faces. This report surveys the relevance of AI/ML with respect to Cyber &amp;
Information Operations. The proliferation of AI/ML algorithms has led to a rise
in DeepFakes and inauthentic social media accounts. Threats are analyzed within
the Strategic and Operational Environments. Existing methods of identifying
synthetic faces exists, but they rely on human beings to visually scrutinize
each photo for inconsistencies. However, through use of the DLIB 68-landmark
pre-trained file, it is possible to analyze and detect synthetic faces by
exploiting repetitive behaviors in StyleGAN images. Project Blade Runner
encompasses two scripts necessary to counter StyleGAN images. Through
PapersPlease.py acting as the analyzer, it is possible to derive
indicators-of-attack (IOA) from scraped image samples. These IOAs can be fed
back into among_us.py acting as the detector to identify synthetic faces from
live operational samples. The opensource copy of Blade Runner may lack
additional unit tests and some functionality, but the open-source copy is a
redacted version, far leaner, better optimized, and a proof-of-concept for the
information security community. The desired end-state will be to incrementally
add automation to stay on-par with its closed-source predecessor.
</p></li>
</ul>

<h3>Title: Forensic-Ready Risk Management Concepts. (arXiv:2210.06840v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06840">http://arxiv.org/abs/2210.06840</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06840] Forensic-Ready Risk Management Concepts](http://arxiv.org/abs/2210.06840)</code></li>
<li>Summary: <p>Currently, numerous approaches exist supporting the implementation of
forensic readiness and, indirectly, forensic-ready software systems. However,
the terminology used in the approaches and their focus tends to vary. To
facilitate the design of forensic-ready software systems, the clarity of the
underlying concepts needs to be established so that their requirements can be
unambiguously formulated and assessed. This is especially important when
considering forensic readiness as an add-on to information security. In this
paper, the concepts relevant to forensic readiness are derived and aligned
based on six existing approaches. The results then serve as a stepping stone
for enhancing Information Systems Security Risk Management (ISSRM) with
forensic readiness.
</p></li>
</ul>

<h3>Title: POSE: Practical Off-chain Smart Contract Execution. (arXiv:2210.07110v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.07110">http://arxiv.org/abs/2210.07110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.07110] POSE: Practical Off-chain Smart Contract Execution](http://arxiv.org/abs/2210.07110)</code></li>
<li>Summary: <p>Smart contracts enable users to execute payments depending on complex program
logic. Ethereum is the most notable example of a blockchain that supports smart
contracts leveraged for countless applications including games, auctions and
financial products. Unfortunately, the traditional method of running contract
code on-chain is very expensive, for instance, on the Ethereum platform, fees
have dramatically increased, rendering the system unsuitable for complex
applications. A prominent solution to address this problem is to execute code
off-chain and only use the blockchain as a trust anchor. While there has been
significant progress in developing off-chain systems over the last years,
current off-chain solutions suffer from various drawbacks including costly
blockchain interactions, lack of data privacy, huge capital costs from locked
collateral, or supporting only a restricted set of applications.
</p></li>
</ul>

<p>In this paper, we present POSE -- a practical off-chain protocol for smart
contracts that addresses the aforementioned shortcomings of existing solutions.
POSE leverages a pool of Trusted Execution Environments (TEEs) to execute the
computation efficiently and to swiftly recover from accidental or malicious
failures. We show that POSE provides strong security guarantees even if a large
subset of parties is corrupted. We evaluate our proof-of-concept implementation
with respect to its efficiency and effectiveness.
</p>

<h2>privacy</h2>
<h3>Title: Mitigating Unintended Memorization in Language Models via Alternating Teaching. (arXiv:2210.06772v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06772">http://arxiv.org/abs/2210.06772</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06772] Mitigating Unintended Memorization in Language Models via Alternating Teaching](http://arxiv.org/abs/2210.06772)</code></li>
<li>Summary: <p>Recent research has shown that language models have a tendency to memorize
rare or unique sequences in the training corpora which can thus leak sensitive
attributes of user data. We employ a teacher-student framework and propose a
novel approach called alternating teaching to mitigate unintended memorization
in sequential modeling. In our method, multiple teachers are trained on
disjoint training sets whose privacy one wishes to protect, and teachers'
predictions supervise the training of a student model in an alternating manner
at each time step. Experiments on LibriSpeech datasets show that the proposed
method achieves superior privacy-preserving results than other counterparts. In
comparison with no prevention for unintended memorization, the overall utility
loss is small when training records are sufficient.
</p></li>
</ul>

<h3>Title: A Tagging Solution to Discover IoT Devices in Apartments. (arXiv:2210.06676v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06676">http://arxiv.org/abs/2210.06676</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06676] A Tagging Solution to Discover IoT Devices in Apartments](http://arxiv.org/abs/2210.06676)</code></li>
<li>Summary: <p>The number of IoT devices in smart homes is increasing. This broad adoption
facilitates users' lives, but it also brings problems. One such issue is that
some IoT devices may invade users' privacy. Some reasons for this invasion can
stem from obscure data collection practices or hidden devices. Specific IoT
devices can exist out of sight and still collect user data to send to third
parties via the Internet. Owners can easily forget the location or even the
existence of these devices, especially if the owner is a landlord who manages
several properties. The landlord-owner scenario creates multi-user problems as
designers build machines for single users. We developed tags that use wireless
protocols, buzzers, and LED lighting to lead users to solve the issue of device
discovery in shared spaces and accommodate multi-user scenarios. They are
attached to IoT devices inside a unit during their installation to be later
discovered by a tenant. These tags have similar functionalities as the popular
Tile models or Airtag, but our tags have different features based on our
privacy use case. Our tags do not require pairing; multiple users can interact
with them through our Android application. Although researchers developed
several other tools, such as thermal cameras or virtual reality (VR), for
discovering devices in environments, they have not used wireless protocols as a
solution. We measured specific performance metrics of our tags to analyze their
feasibility for this problem. We also conducted a user study to measure the
participants' comfort levels while finding objects with our tags attached. Our
results indicate that wireless tags can be viable for device tracking in
residential properties.
</p></li>
</ul>

<h3>Title: PUPoW: A framework for designing blockchains with practically-useful-proof-of-work &amp; vanitycoin. (arXiv:2210.06738v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06738">http://arxiv.org/abs/2210.06738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06738] PUPoW: A framework for designing blockchains with practically-useful-proof-of-work &amp; vanitycoin](http://arxiv.org/abs/2210.06738)</code></li>
<li>Summary: <p>Bitcoin is the first of its kind, a truly decentralized and anonymous
cryptocurrency. To realize it, it has developed blockchain technology using the
concept of `Proof of Work' (PoW). The miners, nodes responsible for writing
transaction databases, solve a cryptographic puzzle to claim the right to write
to the database. Though bitcoin and many other relevant cryptocurrencies, such
as ether use revolutionary ideas, the main criticism involves computing
resources and energy consumption to solve puzzles that have otherwise no use.
There are attempts to use the PoW to do something useful, commonly referred to
as Proof-of-Useful-Work (PoUW). In this paper, we attempt to (i) make PoUW more
usable -- describe how a central problem setter can crowdsource their work as
PoUW and (ii) in the true spirit of blockchains, decentralize the role of
problem setter, whom we call puzzlers. We propose a formal framework to do so,
namely PUPoW. PUPoW has an inbuilt provision of payments from the puzzler to
the miner who solves its puzzle. Additionally, miners have the option to not
rely on a continuous feed of the puzzles and instead use original PoW puzzles.
</p></li>
</ul>

<p>We also propose a way to use PUPOW for solving TOR vanity URL generation and
bitcoin vanity address generation problems. We call this PUPoW blockchain
solving vanity address generation problems as VanityCoin. Both problems require
generating public keys from private keys such that resultant addresses are of
interest. Such key pairs are found only by a brute-force search. However, there
are privacy concerns that miners would know the private keys of the puzzlers.
We resolve this by splitting the private keys, and the miners would know only
one part of it. In summary, we are proposing how PoW can be made practically
helpful, and we believe such an approach is needed for PoW blockchains to
survive.
</p>

<h3>Title: PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs. (arXiv:2210.06746v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06746">http://arxiv.org/abs/2210.06746</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06746] PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs](http://arxiv.org/abs/2210.06746)</code></li>
<li>Summary: <p>Privacy policies disclose how an organization collects and handles personal
information. Recent work has made progress in leveraging natural language
processing (NLP) to automate privacy policy analysis and extract collection
statements from different sentences, considered in isolation from each other.
In this paper, we view and analyze, for the first time, the entire text of a
privacy policy in an integrated way. In terms of methodology: (1) we define
PoliGraph, a type of knowledge graph that captures different relations between
different parts of the text in a privacy policy; and (2) we develop an
NLP-based tool, PoliGraph-er, to automatically extract PoliGraph from the text.
In addition, (3) we revisit the notion of ontologies, previously defined in
heuristic ways, to capture subsumption relations between terms. We make a clear
distinction between local and global ontologies to capture the context of
individual privacy policies, application domains, and privacy laws. Using a
public dataset for evaluation, we show that PoliGraph-er identifies 61% more
collection statements than prior state-of-the-art, with over 90% precision. In
terms of applications, PoliGraph enables automated analysis of a corpus of
privacy policies and allows us to: (1) reveal common patterns in the texts
across different privacy policies, and (2) assess the correctness of the terms
as defined within a privacy policy. We also apply PoliGraph to: (3) detect
contradictions in a privacy policy-we show false positives by prior work, and
(4) analyze the consistency of privacy policies and network traffic, where we
identify significantly more clear disclosures than prior work.
</p></li>
</ul>

<h3>Title: Federated Learning for Tabular Data: Exploring Potential Risk to Privacy. (arXiv:2210.06856v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06856">http://arxiv.org/abs/2210.06856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06856] Federated Learning for Tabular Data: Exploring Potential Risk to Privacy](http://arxiv.org/abs/2210.06856)</code></li>
<li>Summary: <p>Federated Learning (FL) has emerged as a potentially powerful
privacy-preserving machine learning methodology, since it avoids exchanging
data between participants, but instead exchanges model parameters. FL has
traditionally been applied to image, voice and similar data, but recently it
has started to draw attention from domains including financial services where
the data is predominantly tabular. However, the work on tabular data has not
yet considered potential attacks, in particular attacks using Generative
Adversarial Networks (GANs), which have been successfully applied to FL for
non-tabular data. This paper is the first to explore leakage of private data in
Federated Learning systems that process tabular data. We design a Generative
Adversarial Networks (GANs)-based attack model which can be deployed on a
malicious client to reconstruct data and its properties from other
participants. As a side-effect of considering tabular data, we are able to
statistically assess the efficacy of the attack (without relying on human
observation such as done for FL for images). We implement our attack model in a
recently developed generic FL software framework for tabular data processing.
The experimental results demonstrate the effectiveness of the proposed attack
model, thus suggesting that further research is required to counter GAN-based
privacy attacks.
</p></li>
</ul>

<h3>Title: Personalized Federated Hypernetworks for Privacy Preservation in Multi-Task Reinforcement Learning. (arXiv:2210.06820v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06820">http://arxiv.org/abs/2210.06820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06820] Personalized Federated Hypernetworks for Privacy Preservation in Multi-Task Reinforcement Learning](http://arxiv.org/abs/2210.06820)</code></li>
<li>Summary: <p>Multi-Agent Reinforcement Learning currently focuses on implementations where
all data and training can be centralized to one machine. But what if local
agents are split across multiple tasks, and need to keep data private between
each? We develop the first application of Personalized Federated Hypernetworks
(PFH) to Reinforcement Learning (RL). We then present a novel application of
PFH to few-shot transfer, and demonstrate significant initial increases in
learning. PFH has never been demonstrated beyond supervised learning
benchmarks, so we apply PFH to an important domain: RL price-setting for energy
demand response. We consider a general case across where agents are split
across multiple microgrids, wherein energy consumption data must be kept
private within each microgrid. Together, our work explores how the fields of
personalized federated learning and RL can come together to make learning
efficient across multiple tasks while keeping data secure.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: The Inventory is Dark and Full of Misinformation: Understanding the Abuse of Ad Inventory Pooling in the Ad-Tech Supply Chain. (arXiv:2210.06654v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06654">http://arxiv.org/abs/2210.06654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06654] The Inventory is Dark and Full of Misinformation: Understanding the Abuse of Ad Inventory Pooling in the Ad-Tech Supply Chain](http://arxiv.org/abs/2210.06654)</code></li>
<li>Summary: <p>Ad-tech enables publishers to programmatically sell their ad inventory to
millions of demand partners through a complex supply chain. Bogus or low
quality publishers can exploit the opaque nature of the ad-tech to deceptively
monetize their ad inventory. In this paper, we investigate for the first time
how misinformation sites subvert the ad-tech transparency standards and pool
their ad inventory with unrelated sites to circumvent brand safety protections.
We find that a few major ad exchanges are disproportionately responsible for
the dark pools that are exploited by misinformation websites. We further find
evidence that dark pooling allows misinformation sites to deceptively sell
their ad inventory to reputable brands. We conclude with a discussion of
potential countermeasures such as better vetting of ad exchange partners,
adoption of new ad-tech transparency standards that enable end-to-end
validation of the ad-tech supply chain, as well as widespread deployment of
independent audits like ours.
</p></li>
</ul>

<h3>Title: SoK: How `Not' to Architect Your Next-Generation TEE Malware?. (arXiv:2210.06792v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06792">http://arxiv.org/abs/2210.06792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06792] SoK: How</code>Not' to Architect Your Next-Generation TEE Malware?](http://arxiv.org/abs/2210.06792)`</li>
<li>Summary: <p>Besides Intel's SGX technology, there are long-running discussions on how
trusted computing technologies can be used to cloak malware. Past research
showed example methods of malicious activities utilising Flicker, Trusted
Platform Module, and recently integrating with enclaves. There is, however, an
ambiguity over the core SGX ecosystem helps to cloak malware, or whether the
additional engineering work outside SGX's ecosystem forcefully attaches
(overfits) malware-behaviour into the enclave. We examine what malware aims to
do in real-world scenarios and state-of-art techniques in malware evasion. The
rising disadvantages of maintaining the malware and protecting it from
anti-malware mechanisms make SGX enclaves a bad choice for achieving a
successful malware campaign. We systematise twelve points outlining how an
overfit-malware using SGX weakens malware's existing abilities. By making a
comparison with a non-SGX malware (i.e., malware in the wild in our paper), we
conclude that the use of hardware enclaves does not increase the preexisting
attack surface, provides no new infection point, and does not contribute any
new methods to the stealthiness of malware.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06516">http://arxiv.org/abs/2210.06516</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06516] How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?](http://arxiv.org/abs/2210.06516)</code></li>
<li>Summary: <p>Given the volume of data needed to train modern machine learning models,
external suppliers are increasingly used. However, incorporating external data
poses data poisoning risks, wherein attackers manipulate their data to degrade
model utility or integrity. Most poisoning defenses presume access to a set of
clean data (or base set). While this assumption has been taken for granted,
given the fast-growing research on stealthy poisoning attacks, a question
arises: can defenders really identify a clean subset within a contaminated
dataset to support defenses?
</p></li>
</ul>

<p>This paper starts by examining the impact of poisoned samples on defenses
when they are mistakenly mixed into the base set. We analyze five defenses and
find that their performance deteriorates dramatically with less than 1%
poisoned points in the base set. These findings suggest that sifting out a base
set with high precision is key to these defenses' performance. Motivated by
these observations, we study how precise existing automated tools and human
inspection are at identifying clean data in the presence of data poisoning.
Unfortunately, neither effort achieves the precision needed. Worse yet, many of
the outcomes are worse than random selection.
</p>
<p>In addition to uncovering the challenge, we propose a practical
countermeasure, Meta-Sift. Our method is based on the insight that existing
attacks' poisoned samples shifts from clean data distributions. Hence, training
on the clean portion of a dataset and testing on the corrupted portion will
result in high prediction loss. Leveraging the insight, we formulate a bilevel
optimization to identify clean data and further introduce a suite of techniques
to improve efficiency and precision. Our evaluation shows that Meta-Sift can
sift a clean base set with 100% precision under a wide range of poisoning
attacks. The selected base set is large enough to give rise to successful
defenses.
</p>

<h2>attack</h2>
<h3>Title: Adversarial Attack Against Image-Based Localization Neural Networks. (arXiv:2210.06589v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06589">http://arxiv.org/abs/2210.06589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06589] Adversarial Attack Against Image-Based Localization Neural Networks](http://arxiv.org/abs/2210.06589)</code></li>
<li>Summary: <p>In this paper, we present a proof of concept for adversarially attacking the
image-based localization module of an autonomous vehicle. This attack aims to
cause the vehicle to perform a wrong navigational decisions and prevent it from
reaching a desired predefined destination in a simulated urban environment. A
database of rendered images allowed us to train a deep neural network that
performs a localization task and implement, develop and assess the adversarial
pattern. Our tests show that using this adversarial attack we can prevent the
vehicle from turning at a given intersection. This is done by manipulating the
vehicle's navigational module to falsely estimate its current position and thus
fail to initialize the turning procedure until the vehicle misses the last
opportunity to perform a safe turn in a given intersection.
</p></li>
</ul>

<h3>Title: A Stream Learning Approach for Real-Time Identification of False Data Injection Attacks in Cyber-Physical Power Systems. (arXiv:2210.06729v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06729">http://arxiv.org/abs/2210.06729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06729] A Stream Learning Approach for Real-Time Identification of False Data Injection Attacks in Cyber-Physical Power Systems](http://arxiv.org/abs/2210.06729)</code></li>
<li>Summary: <p>This paper presents a novel data-driven framework to aid in system state
estimation when the power system is under unobservable false data injection
attacks. The proposed framework dynamically detects and classifies false data
injection attacks. Then, it retrieves the control signal using the acquired
information. This process is accomplished in three main modules, with novel
designs, for detection, classification, and control signal retrieval. The
detection module monitors historical changes in phasor measurements and
captures any deviation pattern caused by an attack on a complex plane. This
approach can help to reveal characteristics of the attacks including the
direction, magnitude, and ratio of the injected false data. Using this
information, the signal retrieval module can easily recover the original
control signal and remove the injected false data. Further information
regarding the attack type can be obtained through the classifier module. The
proposed ensemble learner is compatible with harsh learning conditions
including the lack of labeled data, concept drift, concept evolution, recurring
classes, and independence from external updates. The proposed novel classifier
can dynamically learn from data and classify attacks under all these harsh
learning conditions. The introduced framework is evaluated w.r.t. real-world
data captured from the Central New York Power System. The obtained results
indicate the efficacy and stability of the proposed framework.
</p></li>
</ul>

<h3>Title: Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition. (arXiv:2210.06871v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06871">http://arxiv.org/abs/2210.06871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06871] Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition](http://arxiv.org/abs/2210.06871)</code></li>
<li>Summary: <p>Deep learning models have shown their vulnerability when dealing with
adversarial attacks. Existing attacks almost perform on low-level instances,
such as pixels and super-pixels, and rarely exploit semantic clues. For face
recognition attacks, existing methods typically generate the l_p-norm
perturbations on pixels, however, resulting in low attack transferability and
high vulnerability to denoising defense models. In this work, instead of
performing perturbations on the low-level pixels, we propose to generate
attacks through perturbing on the high-level semantics to improve attack
transferability. Specifically, a unified flexible framework, Adversarial
Attributes (Adv-Attribute), is designed to generate inconspicuous and
transferable attacks on face recognition, which crafts the adversarial noise
and adds it into different attributes based on the guidance of the difference
in face recognition features from the target. Moreover, the importance-aware
attribute selection and the multi-objective optimization strategy are
introduced to further ensure the balance of stealthiness and attacking
strength. Extensive experiments on the FFHQ and CelebA-HQ datasets show that
the proposed Adv-Attribute method achieves the state-of-the-art attacking
success rates while maintaining better visual effects against recent attack
methods.
</p></li>
</ul>

<h3>Title: Understanding Impacts of Task Similarity on Backdoor Attack and Detection. (arXiv:2210.06509v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06509">http://arxiv.org/abs/2210.06509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06509] Understanding Impacts of Task Similarity on Backdoor Attack and Detection](http://arxiv.org/abs/2210.06509)</code></li>
<li>Summary: <p>With extensive studies on backdoor attack and detection, still fundamental
questions are left unanswered regarding the limits in the adversary's
capability to attack and the defender's capability to detect. We believe that
answers to these questions can be found through an in-depth understanding of
the relations between the primary task that a benign model is supposed to
accomplish and the backdoor task that a backdoored model actually performs. For
this purpose, we leverage similarity metrics in multi-task learning to formally
define the backdoor distance (similarity) between the primary task and the
backdoor task, and analyze existing stealthy backdoor attacks, revealing that
most of them fail to effectively reduce the backdoor distance and even for
those that do, still much room is left to further improve their stealthiness.
So we further design a new method, called TSA attack, to automatically generate
a backdoor model under a given distance constraint, and demonstrate that our
new attack indeed outperforms existing attacks, making a step closer to
understanding the attacker's limits. Most importantly, we provide both
theoretic results and experimental evidence on various datasets for the
positive correlation between the backdoor distance and backdoor detectability,
demonstrating that indeed our task similarity analysis help us better
understand backdoor risks and has the potential to identify more effective
mitigations.
</p></li>
</ul>

<h3>Title: Feature Reconstruction Attacks and Countermeasures of DNN training in Vertical Federated Learning. (arXiv:2210.06771v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06771">http://arxiv.org/abs/2210.06771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06771] Feature Reconstruction Attacks and Countermeasures of DNN training in Vertical Federated Learning](http://arxiv.org/abs/2210.06771)</code></li>
<li>Summary: <p>Federated learning (FL) has increasingly been deployed, in its vertical form,
among organizations to facilitate secure collaborative training over siloed
data. In vertical FL (VFL), participants hold disjoint features of the same set
of sample instances. Among them, only one has labels. This participant, known
as the active party, initiates the training and interacts with the other
participants, known as the passive parties. Despite the increasing adoption of
VFL, it remains largely unknown if and how the active party can extract feature
data from the passive party, especially when training deep neural network (DNN)
models.
</p></li>
</ul>

<p>This paper makes the first attempt to study the feature security problem of
DNN training in VFL. We consider a DNN model partitioned between active and
passive parties, where the latter only holds a subset of the input layer and
exhibits some categorical features of binary values. Using a reduction from the
Exact Cover problem, we prove that reconstructing those binary features is
NP-hard. Through analysis, we demonstrate that, unless the feature dimension is
exceedingly large, it remains feasible, both theoretically and practically, to
launch a reconstruction attack with an efficient search-based algorithm that
prevails over current feature protection techniques. To address this problem,
we develop a novel feature protection scheme against the reconstruction attack
that effectively misleads the search to some pre-specified random values. With
an extensive set of experiments, we show that our protection scheme sustains
the feature reconstruction attack in various VFL applications at no expense of
accuracy loss.
</p>

<h3>Title: Observed Adversaries in Deep Reinforcement Learning. (arXiv:2210.06787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06787">http://arxiv.org/abs/2210.06787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06787] Observed Adversaries in Deep Reinforcement Learning](http://arxiv.org/abs/2210.06787)</code></li>
<li>Summary: <p>In this work, we point out the problem of observed adversaries for deep
policies. Specifically, recent work has shown that deep reinforcement learning
is susceptible to adversarial attacks where an observed adversary acts under
environmental constraints to invoke natural but adversarial observations. This
setting is particularly relevant for HRI since HRI-related robots are expected
to perform their tasks around and with other agents. In this work, we
demonstrate that this effect persists even with low-dimensional observations.
We further show that these adversarial attacks transfer across victims, which
potentially allows malicious attackers to train an adversary without access to
the target victim.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust Action Segmentation from Timestamp Supervision. (arXiv:2210.06501v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06501">http://arxiv.org/abs/2210.06501</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06501] Robust Action Segmentation from Timestamp Supervision](http://arxiv.org/abs/2210.06501)</code></li>
<li>Summary: <p>Action segmentation is the task of predicting an action label for each frame
of an untrimmed video. As obtaining annotations to train an approach for action
segmentation in a fully supervised way is expensive, various approaches have
been proposed to train action segmentation models using different forms of weak
supervision, e.g., action transcripts, action sets, or more recently
timestamps. Timestamp supervision is a promising type of weak supervision as
obtaining one timestamp per action is less expensive than annotating all
frames, but it provides more information than other forms of weak supervision.
However, previous works assume that every action instance is annotated with a
timestamp, which is a restrictive assumption since it assumes that annotators
do not miss any action. In this work, we relax this restrictive assumption and
take missing annotations for some action instances into account. We show that
our approach is more robust to missing annotations compared to other approaches
and various baselines.
</p></li>
</ul>

<h3>Title: Fairness via Adversarial Attribute Neighbourhood Robust Learning. (arXiv:2210.06630v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06630">http://arxiv.org/abs/2210.06630</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06630] Fairness via Adversarial Attribute Neighbourhood Robust Learning](http://arxiv.org/abs/2210.06630)</code></li>
<li>Summary: <p>Improving fairness between privileged and less-privileged sensitive attribute
groups (e.g, {race, gender}) has attracted lots of attention. To enhance the
model performs uniformly well in different sensitive attributes, we propose a
principled \underline{R}obust \underline{A}dversarial \underline{A}ttribute
\underline{N}eighbourhood (RAAN) loss to debias the classification head and
promote a fairer representation distribution across different sensitive
attribute groups. The key idea of RAAN is to mitigate the differences of biased
representations between different sensitive attribute groups by assigning each
sample an adversarial robust weight, which is defined on the representations of
adversarial attribute neighbors, i.e, the samples from different protected
groups. To provide efficient optimization algorithms, we cast the RAAN into a
sum of coupled compositional functions and propose a stochastic adaptive
(Adam-style) and non-adaptive (SGD-style) algorithm framework SCRAAN with
provable theoretical guarantee. Extensive empirical studies on fairness-related
benchmark datasets verify the effectiveness of the proposed method.
</p></li>
</ul>

<h3>Title: Application-Driven AI Paradigm for Hand-Held Action Detection. (arXiv:2210.06682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06682">http://arxiv.org/abs/2210.06682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06682] Application-Driven AI Paradigm for Hand-Held Action Detection](http://arxiv.org/abs/2210.06682)</code></li>
<li>Summary: <p>In practical applications especially with safety requirement, some hand-held
actions need to be monitored closely, including smoking cigarettes, dialing,
eating, etc. Taking smoking cigarettes as example, existing smoke detection
algorithms usually detect the cigarette or cigarette with hand as the target
object only, which leads to low accuracy. In this paper, we propose an
application-driven AI paradigm for hand-held action detection based on
hierarchical object detection. It is a coarse-to-fine hierarchical detection
framework composed of two modules. The first one is a coarse detection module
with the human pose consisting of the whole hand, cigarette and head as target
object. The followed second one is a fine detection module with the fingers
holding cigarette, mouth area and the whole cigarette as target. Some
experiments are done with the dataset collected from real-world scenarios, and
the results show that the proposed framework achieve higher detection rate with
good adaptation and robustness in complex environments.
</p></li>
</ul>

<h3>Title: COLLIDER: A Robust Training Framework for Backdoor Data. (arXiv:2210.06704v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06704">http://arxiv.org/abs/2210.06704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06704] COLLIDER: A Robust Training Framework for Backdoor Data](http://arxiv.org/abs/2210.06704)</code></li>
<li>Summary: <p>Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An
adversary poisons some of the training data in such attacks by installing a
trigger. The goal is to make the trained DNN output the attacker's desired
class whenever the trigger is activated while performing as usual for clean
data. Various approaches have recently been proposed to detect malicious
backdoored DNNs. However, a robust, end-to-end training approach, like
adversarial training, is yet to be discovered for backdoor poisoned data. In
this paper, we take the first step toward such methods by developing a robust
training framework, COLLIDER, that selects the most prominent samples by
exploiting the underlying geometric structures of the data. Specifically, we
effectively filter out candidate poisoned data at each training epoch by
solving a geometrical coreset selection objective. We first argue how clean
data samples exhibit (1) gradients similar to the clean majority of data and
(2) low local intrinsic dimensionality (LID). Based on these criteria, we
define a novel coreset selection objective to find such samples, which are used
for training a DNN. We show the effectiveness of the proposed method for robust
training of DNNs on various poisoned datasets, reducing the backdoor success
rate significantly.
</p></li>
</ul>

<h3>Title: H2RBox: Horizonal Box Annotation is All You Need for Oriented Object Detection. (arXiv:2210.06742v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06742">http://arxiv.org/abs/2210.06742</a></li>
<li>Code URL: <a href="https://github.com/yangxue0827/h2rbox-mmrotate">https://github.com/yangxue0827/h2rbox-mmrotate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06742] H2RBox: Horizonal Box Annotation is All You Need for Oriented Object Detection](http://arxiv.org/abs/2210.06742)</code></li>
<li>Summary: <p>Oriented object detection emerges in many applications from aerial images to
autonomous driving, while many existing detection benchmarks are annotated with
horizontal bounding box only which is also less costive than fine-grained
rotated box, leading to a gap between the readily available training corpus and
the rising demand for oriented object detection. This paper proposes a simple
yet effective oriented object detection approach called H2RBox merely using
horizontal box annotation for weakly-supervised training, which closes the
above gap and shows competitive performance even against those trained with
rotated boxes. The cores of our method are weakly- and self-supervised
learning, which predicts the angle of the object by learning the consistency of
two different views. To our best knowledge, H2RBox is the first horizontal box
annotation-based oriented object detector. Compared to an alternative i.e.
horizontal box-supervised instance segmentation with our post adaption to
oriented object detection, our approach is not susceptible to the prediction
quality of mask and can perform more robustly in complex scenes containing a
large number of dense objects and outliers. Experimental results show that
H2RBox has significant performance and speed advantages over horizontal
box-supervised instance segmentation methods, as well as lower memory
requirements. While compared to rotated box-supervised oriented object
detectors, our method shows very close performance and speed, and even
surpasses them in some cases. The source code is available at
https://github.com/yangxue0827/h2rbox-mmrotate.
</p></li>
</ul>

<h3>Title: Large-Scale Open-Set Classification Protocols for ImageNet. (arXiv:2210.06789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06789">http://arxiv.org/abs/2210.06789</a></li>
<li>Code URL: <a href="https://github.com/aiml-ifi/openset-imagenet">https://github.com/aiml-ifi/openset-imagenet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06789] Large-Scale Open-Set Classification Protocols for ImageNet](http://arxiv.org/abs/2210.06789)</code></li>
<li>Summary: <p>Open-Set Classification (OSC) intends to adapt closed-set classification
models to real-world scenarios, where the classifier must correctly label
samples of known classes while rejecting previously unseen unknown samples.
Only recently, research started to investigate on algorithms that are able to
handle these unknown samples correctly. Some of these approaches address OSC by
including into the training set negative samples that a classifier learns to
reject, expecting that these data increase the robustness of the classifier on
unknown classes. Most of these approaches are evaluated on small-scale and
low-resolution image datasets like MNIST, SVHN or CIFAR, which makes it
difficult to assess their applicability to the real world, and to compare them
among each other. We propose three open-set protocols that provide rich
datasets of natural images with different levels of similarity between known
and unknown classes. The protocols consist of subsets of ImageNet classes
selected to provide training and testing data closer to real-world scenarios.
Additionally, we propose a new validation metric that can be employed to assess
whether the training of deep learning models addresses both the classification
of known samples and the rejection of unknown samples. We use the protocols to
compare the performance of two baseline open-set algorithms to the standard
SoftMax baseline and find that the algorithms work well on negative samples
that have been seen during training, and partially on out-of-distribution
detection tasks, but drop performance in the presence of samples from
previously unseen unknown classes.
</p></li>
</ul>

<h3>Title: Hierarchical and Progressive Image Matting. (arXiv:2210.06906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06906">http://arxiv.org/abs/2210.06906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06906] Hierarchical and Progressive Image Matting](http://arxiv.org/abs/2210.06906)</code></li>
<li>Summary: <p>Most matting researches resort to advanced semantics to achieve high-quality
alpha mattes, and direct low-level features combination is usually explored to
complement alpha details. However, we argue that appearance-agnostic
integration can only provide biased foreground details and alpha mattes require
different-level feature aggregation for better pixel-wise opacity perception.
In this paper, we propose an end-to-end Hierarchical and Progressive Attention
Matting Network (HAttMatting++), which can better predict the opacity of the
foreground from single RGB images without additional input. Specifically, we
utilize channel-wise attention to distill pyramidal features and employ spatial
attention at different levels to filter appearance cues. This progressive
attention mechanism can estimate alpha mattes from adaptive semantics and
semantics-indicated boundaries. We also introduce a hybrid loss function fusing
Structural SIMilarity (SSIM), Mean Square Error (MSE), Adversarial loss, and
sentry supervision to guide the network to further improve the overall
foreground structure. Besides, we construct a large-scale and challenging image
matting dataset comprised of 59, 600 training images and 1000 test images (a
total of 646 distinct foreground alpha mattes), which can further improve the
robustness of our hierarchical and progressive aggregation model. Extensive
experiments demonstrate that the proposed HAttMatting++ can capture
sophisticated foreground structures and achieve state-of-the-art performance
with single RGB images as input.
</p></li>
</ul>

<h3>Title: Wider and Higher: Intensive Integration and Global Foreground Perception for Image Matting. (arXiv:2210.06919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06919">http://arxiv.org/abs/2210.06919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06919] Wider and Higher: Intensive Integration and Global Foreground Perception for Image Matting](http://arxiv.org/abs/2210.06919)</code></li>
<li>Summary: <p>This paper reviews recent deep-learning-based matting research and conceives
our wider and higher motivation for image matting. Many approaches achieve
alpha mattes with complex encoders to extract robust semantics, then resort to
the U-net-like decoder to concatenate or fuse encoder features. However, image
matting is essentially a pixel-wise regression, and the ideal situation is to
perceive the maximum opacity correspondence from the input image. In this
paper, we argue that the high-resolution feature representation, perception and
communication are more crucial for matting accuracy. Therefore, we propose an
Intensive Integration and Global Foreground Perception network (I2GFP) to
integrate wider and higher feature streams. Wider means we combine intensive
features in each decoder stage, while higher suggests we retain high-resolution
intermediate features and perceive large-scale foreground appearance. Our
motivation sacrifices model depth for a significant performance promotion. We
perform extensive experiments to prove the proposed I2GFP model, and
state-of-the-art results can be achieved on different public datasets.
</p></li>
</ul>

<h3>Title: SageMix: Saliency-Guided Mixup for Point Clouds. (arXiv:2210.06944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06944">http://arxiv.org/abs/2210.06944</a></li>
<li>Code URL: <a href="https://github.com/mlvlab/SageMix">https://github.com/mlvlab/SageMix</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06944] SageMix: Saliency-Guided Mixup for Point Clouds](http://arxiv.org/abs/2210.06944)</code></li>
<li>Summary: <p>Data augmentation is key to improving the generalization ability of deep
learning models. Mixup is a simple and widely-used data augmentation technique
that has proven effective in alleviating the problems of overfitting and data
scarcity. Also, recent studies of saliency-aware Mixup in the image domain show
that preserving discriminative parts is beneficial to improving the
generalization performance. However, these Mixup-based data augmentations are
underexplored in 3D vision, especially in point clouds. In this paper, we
propose SageMix, a saliency-guided Mixup for point clouds to preserve salient
local structures. Specifically, we extract salient regions from two point
clouds and smoothly combine them into one continuous shape. With a simple
sequential sampling by re-weighted saliency scores, SageMix preserves the local
structure of salient regions. Extensive experiments demonstrate that the
proposed method consistently outperforms existing Mixup methods in various
benchmark point cloud datasets. With PointNet++, our method achieves an
accuracy gain of 2.6% and 4.0% over standard training in 3D Warehouse dataset
(MN40) and ScanObjectNN, respectively. In addition to generalization
performance, SageMix improves robustness and uncertainty calibration. Moreover,
when adopting our method to various tasks including part segmentation and
standard 2D image classification, our method achieves competitive performance.
</p></li>
</ul>

<h3>Title: Denoising Masked AutoEncoders are Certifiable Robust Vision Learners. (arXiv:2210.06983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06983">http://arxiv.org/abs/2210.06983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06983] Denoising Masked AutoEncoders are Certifiable Robust Vision Learners](http://arxiv.org/abs/2210.06983)</code></li>
<li>Summary: <p>In this paper, we propose a new self-supervised method, which is called
Denoising Masked AutoEncoders (DMAE), for learning certified robust classifiers
of images. In DMAE, we corrupt each image by adding Gaussian noises to each
pixel value and randomly masking several patches. A Transformer-based
encoder-decoder model is then trained to reconstruct the original image from
the corrupted one. In this learning paradigm, the encoder will learn to capture
relevant semantics for the downstream tasks, which is also robust to Gaussian
additive noises. We show that the pre-trained encoder can naturally be used as
the base classifier in Gaussian smoothed models, where we can analytically
compute the certified radius for any data point. Although the proposed method
is simple, it yields significant performance improvement in downstream
classification tasks. We show that the DMAE ViT-Base model, which just uses
1/10 parameters of the model developed in recent work <a href="http://export.arxiv.org/abs/2206.10550">arXiv:2206.10550</a>,
achieves competitive or better certified accuracy in various settings. The DMAE
ViT-Large model significantly surpasses all previous results, establishing a
new state-of-the-art on ImageNet dataset. We further demonstrate that the
pre-trained model has good transferability to the CIFAR-10 dataset, suggesting
its wide adaptability. Models and code are available at
https://github.com/quanlin-wu/dmae.
</p></li>
</ul>

<h3>Title: Learning with Style: Continual Semantic Segmentation Across Tasks and Domains. (arXiv:2210.07016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.07016">http://arxiv.org/abs/2210.07016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.07016] Learning with Style: Continual Semantic Segmentation Across Tasks and Domains](http://arxiv.org/abs/2210.07016)</code></li>
<li>Summary: <p>Deep learning models dealing with image understanding in real-world settings
must be able to adapt to a wide variety of tasks across different domains.
Domain adaptation and class incremental learning deal with domain and task
variability separately, whereas their unified solution is still an open
problem. We tackle both facets of the problem together, taking into account the
semantic shift within both input and label spaces. We start by formally
introducing continual learning under task and domain shift. Then, we address
the proposed setup by using style transfer techniques to extend knowledge
across domains when learning incremental tasks and a robust distillation
framework to effectively recollect task knowledge under incremental domain
shift. The devised framework (LwS, Learning with Style) is able to generalize
incrementally acquired task knowledge across all the domains encountered,
proving to be robust against catastrophic forgetting. Extensive experimental
evaluation on multiple autonomous driving datasets shows how the proposed
method outperforms existing approaches, which prove to be ill-equipped to deal
with continual semantic segmentation under both task and domain shift.
</p></li>
</ul>

<h3>Title: On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations. (arXiv:2210.06916v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06916">http://arxiv.org/abs/2210.06916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06916] On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations](http://arxiv.org/abs/2210.06916)</code></li>
<li>Summary: <p>Current Explainable AI (ExAI) methods, especially in the NLP field, are
conducted on various datasets by employing different metrics to evaluate
several aspects. The lack of a common evaluation framework is hindering the
progress tracking of such methods and their wider adoption. In this work,
inspired by offline information retrieval, we propose different metrics and
techniques to evaluate the explainability of SA models from two angles. First,
we evaluate the strength of the extracted "rationales" in faithfully explaining
the predicted outcome. Second, we measure the agreement between ExAI methods
and human judgment on a homegrown dataset1 to reflect on the rationales
plausibility. Our conducted experiments comprise four dimensions: (1) the
underlying architectures of SA models, (2) the approach followed by the ExAI
method, (3) the reasoning difficulty, and (4) the homogeneity of the
ground-truth rationales. We empirically demonstrate that anchors explanations
are more aligned with the human judgment and can be more confident in
extracting supporting rationales. As can be foreseen, the reasoning complexity
of sentiment is shown to thwart ExAI methods from extracting supporting
evidence. Moreover, a remarkable discrepancy is discerned between the results
of different explainability methods on the various architectures suggesting the
need for consolidation to observe enhanced performance. Predominantly,
transformers are shown to exhibit better explainability than convolutional and
recurrent architectures. Our work paves the way towards designing more
interpretable NLP models and enabling a common evaluation ground for their
relative strengths and robustness.
</p></li>
</ul>

<h3>Title: Incorporating Context into Subword Vocabularies. (arXiv:2210.07095v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.07095">http://arxiv.org/abs/2210.07095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.07095] Incorporating Context into Subword Vocabularies](http://arxiv.org/abs/2210.07095)</code></li>
<li>Summary: <p>Most current popular subword tokenizers are trained based on word frequency
statistics over a corpus, without considering information about co-occurrence
or context. Nevertheless, the resulting vocabularies are used in language
models' highly contextualized settings. We present SaGe, a tokenizer that
tailors subwords for their downstream use by baking in the contextualized
signal at the vocabulary creation phase. We show that SaGe does a better job
than current widespread tokenizers in keeping token contexts cohesive, while
not incurring a large price in terms of encoding efficiency or domain
robustness. SaGe improves performance on English GLUE classification tasks as
well as on NER, and on Inference and NER in Turkish, demonstrating its
robustness to language properties such as morphological exponence and
agglutination.
</p></li>
</ul>

<h3>Title: A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models. (arXiv:2210.07111v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.07111">http://arxiv.org/abs/2210.07111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.07111] A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models](http://arxiv.org/abs/2210.07111)</code></li>
<li>Summary: <p>Recent work on tokenizer-free multilingual pretrained models show promising
results in improving cross-lingual transfer and reducing engineering overhead
(Clark et al., 2022; Xue et al., 2022). However, these works mainly focus on
reporting accuracy on a limited set of tasks and data settings, placing less
emphasis on other important factors when tuning and deploying the models in
practice, such as memory usage, inference speed, and fine-tuning data
robustness. We attempt to fill this gap by performing a comprehensive empirical
comparison of multilingual tokenizer-free and subword-based models considering
these various dimensions. Surprisingly, we find that subword-based models might
still be the most practical choice in many settings, achieving better
performance for lower inference latency and memory usage. Based on these
results, we encourage future work in tokenizer-free methods to consider these
factors when designing and evaluating new models.
</p></li>
</ul>

<h3>Title: FASTER-CE: Fast, Sparse, Transparent, and Robust Counterfactual Explanations. (arXiv:2210.06578v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06578">http://arxiv.org/abs/2210.06578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06578] FASTER-CE: Fast, Sparse, Transparent, and Robust Counterfactual Explanations](http://arxiv.org/abs/2210.06578)</code></li>
<li>Summary: <p>Counterfactual explanations have substantially increased in popularity in the
past few years as a useful human-centric way of understanding individual
black-box model predictions. While several properties desired of high-quality
counterfactuals have been identified in the literature, three crucial concerns:
the speed of explanation generation, robustness/sensitivity and succinctness of
explanations (sparsity) have been relatively unexplored. In this paper, we
present FASTER-CE: a novel set of algorithms to generate fast, sparse, and
robust counterfactual explanations. The key idea is to efficiently find
promising search directions for counterfactuals in a latent space that is
specified via an autoencoder. These directions are determined based on
gradients with respect to each of the original input features as well as of the
target, as estimated in the latent space. The ability to quickly examine
combinations of the most promising gradient directions as well as to
incorporate additional user-defined constraints allows us to generate multiple
counterfactual explanations that are sparse, realistic, and robust to input
manipulations. Through experiments on three datasets of varied complexities, we
show that FASTER-CE is not only much faster than other state of the art methods
for generating multiple explanations but also is significantly superior when
considering a larger set of desirable (and often conflicting) properties.
Specifically we present results across multiple performance metrics: sparsity,
proximity, validity, speed of generation, and the robustness of explanations,
to highlight the capabilities of the FASTER-CE family.
</p></li>
</ul>

<h3>Title: Outlier-Robust Group Inference via Gradient Space Clustering. (arXiv:2210.06759v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06759">http://arxiv.org/abs/2210.06759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06759] Outlier-Robust Group Inference via Gradient Space Clustering](http://arxiv.org/abs/2210.06759)</code></li>
<li>Summary: <p>Traditional machine learning models focus on achieving good performance on
the overall training distribution, but they often underperform on minority
groups. Existing methods can improve the worst-group performance, but they can
have several limitations: (i) they require group annotations, which are often
expensive and sometimes infeasible to obtain, and/or (ii) they are sensitive to
outliers. Most related works fail to solve these two issues simultaneously as
they focus on conflicting perspectives of minority groups and outliers. We
address the problem of learning group annotations in the presence of outliers
by clustering the data in the space of gradients of the model parameters. We
show that data in the gradient space has a simpler structure while preserving
information about minority groups and outliers, making it suitable for standard
clustering methods like DBSCAN. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art both in terms of group
identification and downstream worst-group performance.
</p></li>
</ul>

<h3>Title: Improving Out-of-Distribution Generalization by Adversarial Training with Structured Priors. (arXiv:2210.06807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06807">http://arxiv.org/abs/2210.06807</a></li>
<li>Code URL: <a href="https://github.com/novaglow646/nips22-mat-and-ldat-for-ood">https://github.com/novaglow646/nips22-mat-and-ldat-for-ood</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06807] Improving Out-of-Distribution Generalization by Adversarial Training with Structured Priors](http://arxiv.org/abs/2210.06807)</code></li>
<li>Summary: <p>Deep models often fail to generalize well in test domains when the data
distribution differs from that in the training domain. Among numerous
approaches to address this Out-of-Distribution (OOD) generalization problem,
there has been a growing surge of interest in exploiting Adversarial Training
(AT) to improve OOD performance. Recent works have revealed that the robust
model obtained by conducting sample-wise AT also retains transferability to
biased test domains. In this paper, we empirically show that sample-wise AT has
limited improvement on OOD performance. Specifically, we find that AT can only
maintain performance at smaller scales of perturbation while Universal AT (UAT)
is more robust to larger-scale perturbations. This provides us with clues that
adversarial perturbations with universal (low dimensional) structures can
enhance the robustness against large data distribution shifts that are common
in OOD scenarios. Inspired by this, we propose two AT variants with low-rank
structures to train OOD-robust models. Extensive experiments on DomainBed
benchmark show that our proposed approaches outperform Empirical Risk
Minimization (ERM) and sample-wise AT. Our code is available at
https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD.
</p></li>
</ul>

<h3>Title: GA-SAM: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization. (arXiv:2210.06895v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06895">http://arxiv.org/abs/2210.06895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06895] GA-SAM: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization](http://arxiv.org/abs/2210.06895)</code></li>
<li>Summary: <p>Recently, Sharpness-Aware Minimization (SAM) algorithm has shown
state-of-the-art generalization abilities in vision tasks. It demonstrates that
flat minima tend to imply better generalization abilities. However, it has some
difficulty implying SAM to some natural language tasks, especially to models
with drastic gradient changes, such as RNNs. In this work, we analyze the
relation between the flatness of the local minimum and its generalization
ability from a novel and straightforward theoretical perspective. We propose
that the shift of the training and test distributions can be equivalently seen
as a virtual parameter corruption or perturbation, which can explain why flat
minima that are robust against parameter corruptions or perturbations have
better generalization performances. On its basis, we propose a
Gradient-Strength based Adaptive Sharpness-Aware Minimization (GA-SAM)
algorithm to help to learn algorithms find flat minima that generalize better.
Results in various language benchmarks validate the effectiveness of the
proposed GA-SAM algorithm on natural language tasks.
</p></li>
</ul>

<h3>Title: Delta-Closure Structure for Studying Data Distribution. (arXiv:2210.06926v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06926">http://arxiv.org/abs/2210.06926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06926] Delta-Closure Structure for Studying Data Distribution](http://arxiv.org/abs/2210.06926)</code></li>
<li>Summary: <p>In this paper, we revisit pattern mining and study the distribution
underlying a binary dataset thanks to the closure structure which is based on
passkeys, i.e., minimum generators in equivalence classes robust to noise. We
introduce $\Delta$-closedness, a generalization of the closure operator, where
$\Delta$ measures how a closed set differs from its upper neighbors in the
partial order induced by closure. A $\Delta$-class of equivalence includes
minimum and maximum elements and allows us to characterize the distribution
underlying the data. Moreover, the set of $\Delta$-classes of equivalence can
be partitioned into the so-called $\Delta$-closure structure. In particular, a
$\Delta$-class of equivalence with a high level demonstrates correlations among
many attributes, which are supported by more observations when $\Delta$ is
large. In the experiments, we study the $\Delta$-closure structure of several
real-world datasets and show that this structure is very stable for large
$\Delta$ and does not substantially depend on the data sampling used for the
analysis.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Generalized Inter-class Loss for Gait Recognition. (arXiv:2210.06779v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06779">http://arxiv.org/abs/2210.06779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06779] Generalized Inter-class Loss for Gait Recognition](http://arxiv.org/abs/2210.06779)</code></li>
<li>Summary: <p>Gait recognition is a unique biometric technique that can be performed at a
long distance non-cooperatively and has broad applications in public safety and
intelligent traffic systems. Previous gait works focus more on minimizing the
intra-class variance while ignoring the significance in constraining
inter-class variance. To this end, we propose a generalized inter-class loss
which resolves the inter-class variance from both sample-level feature
distribution and class-level feature distribution. Instead of equal penalty
strength on pair scores, the proposed loss optimizes sample-level inter-class
feature distribution by dynamically adjusting the pairwise weight. Further, in
class-level distribution, generalized inter-class loss adds a constraint on the
uniformity of inter-class feature distribution, which forces the feature
representations to approximate a hypersphere and keep maximal inter-class
variance. In addition, the proposed method automatically adjusts the margin
between classes which enables the inter-class feature distribution to be more
flexible. The proposed method can be generalized to different gait recognition
networks and achieves significant improvements. We conduct a series of
experiments on CASIA-B and OUMVLP, and the experimental results show that the
proposed loss can significantly improve the performance and achieves the
state-of-the-art performances.
</p></li>
</ul>

<h3>Title: Behavioral graph fraud detection in E-commerce. (arXiv:2210.06968v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06968">http://arxiv.org/abs/2210.06968</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06968] Behavioral graph fraud detection in E-commerce](http://arxiv.org/abs/2210.06968)</code></li>
<li>Summary: <p>In e-commerce industry, graph neural network methods are the new trends for
transaction risk modeling.The power of graph algorithms lie in the capability
to catch transaction linking network information, which is very hard to be
captured by other algorithms.However, in most existing approaches, transaction
or user connections are defined by hard link strategies on shared properties,
such as same credit card, same device, same ip address, same shipping address,
etc. Those types of strategies will result in sparse linkages by entities with
strong identification characteristics (ie. device) and over-linkages by
entities that could be widely shared (ie. ip address), making it more difficult
to learn useful information from graph. To address aforementioned problems, we
present a novel behavioral biometric based method to establish transaction
linkings based on user behavioral similarities, then train an unsupervised GNN
to extract embedding features for downstream fraud prediction tasks. To our
knowledge, this is the first time similarity based soft link has been used in
graph embedding applications. To speed up similarity calculation, we apply an
in-house GPU based HDBSCAN clustering method to remove highly concentrated and
isolated nodes before graph construction. Our experiments show that embedding
features learned from similarity based behavioral graph have achieved
significant performance increase to the baseline fraud detection model in
various business scenarios. In new guest buyer transaction scenario, this
segment is a challenge for traditional method, we can make precision increase
from 0.82 to 0.86 at the same recall of 0.27, which means we can decrease false
positive rate using this method.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Dimensionality of datasets in object detection networks. (arXiv:2210.07049v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.07049">http://arxiv.org/abs/2210.07049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.07049] Dimensionality of datasets in object detection networks](http://arxiv.org/abs/2210.07049)</code></li>
<li>Summary: <p>In recent years, convolutional neural networks (CNNs) are used in a large
number of tasks in computer vision. One of them is object detection for
autonomous driving. Although CNNs are used widely in many areas, what happens
inside the network is still unexplained on many levels. Our goal is to
determine the effect of Intrinsic dimension (i.e. minimum number of parameters
required to represent data) in different layers on the accuracy of object
detection network for augmented data sets. Our investigation determines that
there is difference between the representation of normal and augmented data
during feature extraction.
</p></li>
</ul>

<h3>Title: Developing a general-purpose clinical language inference model from a large corpus of clinical notes. (arXiv:2210.06566v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06566">http://arxiv.org/abs/2210.06566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06566] Developing a general-purpose clinical language inference model from a large corpus of clinical notes](http://arxiv.org/abs/2210.06566)</code></li>
<li>Summary: <p>Several biomedical language models have already been developed for clinical
language inference. However, these models typically utilize general
vocabularies and are trained on relatively small clinical corpora. We sought to
evaluate the impact of using a domain-specific vocabulary and a large clinical
training corpus on the performance of these language models in clinical
language inference. We trained a Bidirectional Encoder Decoder from
Transformers (BERT) model using a diverse, deidentified corpus of 75 million
deidentified clinical notes authored at the University of California, San
Francisco (UCSF). We evaluated this model on several clinical language
inference benchmark tasks: clinical and temporal concept recognition, relation
extraction and medical language inference. We also evaluated our model on two
tasks using discharge summaries from UCSF: diagnostic code assignment and
therapeutic class inference. Our model performs at par with the best publicly
available biomedical language models of comparable sizes on the public
benchmark tasks, and is significantly better than these models in a
within-system evaluation on the two tasks using UCSF data. The use of in-domain
vocabulary appears to improve the encoding of longer documents. The use of
large clinical corpora appears to enhance document encoding and inferential
accuracy. However, further research is needed to improve abbreviation
resolution, and numerical, temporal, and implicitly causal inference.
</p></li>
</ul>

<h3>Title: Iterative Document-level Information Extraction via Imitation Learning. (arXiv:2210.06600v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06600">http://arxiv.org/abs/2210.06600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06600] Iterative Document-level Information Extraction via Imitation Learning](http://arxiv.org/abs/2210.06600)</code></li>
<li>Summary: <p>We present a novel iterative extraction (IterX) model for extracting complex
relations, or templates, i.e., N-tuples representing a mapping from named slots
to spans of text contained within a document. Documents may support zero or
more instances of a template of any particular type, leading to the tasks of
identifying the templates in a document, and extracting each template's slot
values. Our imitation learning approach relieves the need to use predefined
template orders to train an extractor and leads to state-of-the-art results on
two established benchmarks -- 4-ary relation extraction on SciREX and template
extraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular
task.
</p></li>
</ul>

<h3>Title: Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis. (arXiv:2210.06629v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06629">http://arxiv.org/abs/2210.06629</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06629] Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis](http://arxiv.org/abs/2210.06629)</code></li>
<li>Summary: <p>Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis
task which involves four elements from user-generated texts: aspect term,
aspect category, opinion term, and sentiment polarity. Most computational
approaches focus on some of the ABSA sub-tasks such as tuple (aspect term,
sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)
extraction using either pipeline or joint modeling approaches. Recently,
generative approaches have been proposed to extract all four elements as (one
or more) quadruplets from text as a single task. In this work, we take a step
further and propose a unified framework for solving ABSA, and the associated
sub-tasks to improve the performance in few-shot scenarios. To this end, we
fine-tune a T5 model with instructional prompts in a multi-task learning
fashion covering all the sub-tasks, as well as the entire quadruple prediction
task. In experiments with multiple benchmark data sets, we show that the
proposed multi-task prompting approach brings performance boost (by absolute
$6.75$ F1) in the few-shot learning setting.
</p></li>
</ul>

<h3>Title: An Empirical Study on Finding Spans. (arXiv:2210.06824v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06824">http://arxiv.org/abs/2210.06824</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06824] An Empirical Study on Finding Spans](http://arxiv.org/abs/2210.06824)</code></li>
<li>Summary: <p>We present an empirical study on methods for span finding, the selection of
consecutive tokens in text for some downstream tasks. We focus on approaches
that can be employed in training end-to-end information extraction systems. We
recognize there is no silver bullet that can simply solve all downstream tasks
well without considering task properties and provide our observations to help
with design choices in the future: 1) tagging method usually yields a higher
precision while span enumeration and boundary prediction prefer a higher
recall; 2) span type information can benefit boundary prediction approach; 3)
additional contextualization does not help span finding in most cases.
</p></li>
</ul>

<h3>Title: Ensemble Creation via Anchored Regularization for Unsupervised Aspect Extraction. (arXiv:2210.06829v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06829">http://arxiv.org/abs/2210.06829</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06829] Ensemble Creation via Anchored Regularization for Unsupervised Aspect Extraction](http://arxiv.org/abs/2210.06829)</code></li>
<li>Summary: <p>Aspect Based Sentiment Analysis is the most granular form of sentiment
analysis that can be performed on the documents / sentences. Besides delivering
the most insights at a finer grain, it also poses equally daunting challenges.
One of them being the shortage of labelled data. To bring in value right out of
the box for the text data being generated at a very fast pace in today's world,
unsupervised aspect-based sentiment analysis allows us to generate insights
without investing time or money in generating labels. From topic modelling
approaches to recent deep learning-based aspect extraction models, this domain
has seen a lot of development. One of the models that we improve upon is ABAE
that reconstructs the sentences as a linear combination of aspect terms present
in it, In this research we explore how we can use information from another
unsupervised model to regularize ABAE, leading to better performance. We
contrast it with baseline rule based ensemble and show that the ensemble
methods work better than the individual models and the regularization based
ensemble performs better than the rule-based one.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Anomaly Detection via Federated Learning. (arXiv:2210.06614v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06614">http://arxiv.org/abs/2210.06614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06614] Anomaly Detection via Federated Learning](http://arxiv.org/abs/2210.06614)</code></li>
<li>Summary: <p>Machine learning has helped advance the field of anomaly detection by
incorporating classifiers and autoencoders to decipher between normal and
anomalous behavior. Additionally, federated learning has provided a way for a
global model to be trained with multiple clients' data without requiring the
client to directly share their data. This paper proposes a novel anomaly
detector via federated learning to detect malicious network activity on a
client's server. In our experiments, we use an autoencoder with a classifier in
a federated learning framework to determine if the network activity is benign
or malicious. By using our novel min-max scalar and sampling technique, called
FedSam, we determined federated learning allows the global model to learn from
each client's data and, in turn, provide a means for each client to improve
their intrusion detection system's defense against cyber-attacks.
</p></li>
</ul>

<h3>Title: Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation. (arXiv:2210.06894v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06894">http://arxiv.org/abs/2210.06894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06894] Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation](http://arxiv.org/abs/2210.06894)</code></li>
<li>Summary: <p>Despite the potential of federated learning, it is known to be vulnerable to
backdoor attacks. Many robust federated aggregation methods are proposed to
reduce the potential backdoor risk. However, they are mainly validated in the
CV field. In this paper, we find that NLP backdoors are hard to defend against
than CV, and we provide a theoretical analysis that the malicious update
detection error probabilities are determined by the relative backdoor
strengths. NLP attacks tend to have small relative backdoor strengths, which
may result in the failure of robust federated aggregation methods for NLP
attacks. Inspired by the theoretical results, we can choose some dimensions
with higher backdoor strengths to settle this issue. We propose a novel
federated aggregation algorithm, Dim-Krum, for NLP tasks, and experimental
results validate its effectiveness.
</p></li>
</ul>

<h3>Title: Find Your Friends: Personalized Federated Learning with the Right Collaborators. (arXiv:2210.06597v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06597">http://arxiv.org/abs/2210.06597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06597] Find Your Friends: Personalized Federated Learning with the Right Collaborators](http://arxiv.org/abs/2210.06597)</code></li>
<li>Summary: <p>In the traditional federated learning setting, a central server coordinates a
network of clients to train one global model. However, the global model may
serve many clients poorly due to data heterogeneity. Moreover, there may not
exist a trusted central party that can coordinate the clients to ensure that
each of them can benefit from others. To address these concerns, we present a
novel decentralized framework, FedeRiCo, where each client can learn as much or
as little from other clients as is optimal for its local data distribution.
Based on expectation-maximization, FedeRiCo estimates the utilities of other
participants' models on each client's data so that everyone can select the
right collaborators for learning. As a result, our algorithm outperforms other
federated, personalized, and/or decentralized approaches on several benchmark
datasets, being the only approach that consistently performs better than
training with local data only.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models. (arXiv:2210.06475v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06475">http://arxiv.org/abs/2210.06475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06475] Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models](http://arxiv.org/abs/2210.06475)</code></li>
<li>Summary: <p>We introduce equi-tuning, a novel fine-tuning method that transforms
(potentially non-equivariant) pretrained models into group equivariant models
while incurring minimum $L_2$ loss between the feature representations of the
pretrained and the equivariant models. Large pretrained models can be
equi-tuned for different groups to satisfy the needs of various downstream
tasks. Equi-tuned models benefit from both group equivariance as an inductive
bias and semantic priors from pretrained models. We provide applications of
equi-tuning on three different tasks: image classification, compositional
generalization in language, and fairness in natural language generation (NLG).
We also provide a novel group-theoretic definition for fairness in NLG. The
effectiveness of this definition is shown by testing it against a standard
empirical method of fairness in NLG. We provide experimental results for
equi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and
Densenet for image classification; RNNs, GRUs, and LSTMs for compositional
generalization; and GPT2 for fairness in NLG. We test these models on benchmark
datasets across all considered tasks to show the generality and effectiveness
of the proposed method.
</p></li>
</ul>

<h3>Title: Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06799">http://arxiv.org/abs/2210.06799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06799] Benchmarking Long-tail Generalization with Likelihood Splits](http://arxiv.org/abs/2210.06799)</code></li>
<li>Summary: <p>In order to reliably process natural language, NLP systems must generalize to
the long tail of rare utterances. We propose a method to create challenging
benchmarks that require generalizing to the tail of the distribution by
re-splitting existing datasets. We create 'Likelihood splits' where examples
that are assigned lower likelihood by a pre-trained language model (LM) are
placed in the test set, and more likely examples are in the training set. This
simple approach can be customized to construct meaningful train-test splits for
a wide range of tasks. Likelihood splits are more challenging than random
splits: relative error rates of state-of-the-art models on our splits increase
by 59% for semantic parsing on Spider, 77% for natural language inference on
SNLI, and 38% for yes/no question answering on BoolQ compared with the
corresponding random splits. Moreover, Likelihood splits create fairer
benchmarks than adversarial filtering; when the LM used to create the splits is
used as the task model, our splits do not adversely penalize the LM.
</p></li>
</ul>

<h3>Title: Rethinking Annotation: Can Language Learners Contribute?. (arXiv:2210.06828v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06828">http://arxiv.org/abs/2210.06828</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06828] Rethinking Annotation: Can Language Learners Contribute?](http://arxiv.org/abs/2210.06828)</code></li>
<li>Summary: <p>Researchers have traditionally recruited native speakers to provide
annotations for the widely used benchmark datasets. But there are languages for
which recruiting native speakers is difficult, and it would help to get
learners of those languages to annotate the data. In this paper, we investigate
whether language learners can contribute annotations to the benchmark datasets.
In a carefully controlled annotation experiment, we recruit 36 language
learners, provide two types of additional resources (dictionaries and
machine-translated sentences), and perform mini-tests to measure their language
proficiency. We target three languages, English, Korean, and Indonesian, and
four NLP tasks, sentiment analysis, natural language inference, named entity
recognition, and machine reading comprehension. We find that language learners,
especially those with intermediate or advanced language proficiency, are able
to provide fairly accurate labels with the help of additional resources.
Moreover, we show that data annotation improves learners' language proficiency
in terms of vocabulary and grammar. The implication of our findings is that
broadening the annotation task to include language learners can open up the
opportunity to build benchmark datasets for languages for which it is difficult
to recruit native speakers.
</p></li>
</ul>

<h3>Title: Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities. (arXiv:2210.06640v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06640">http://arxiv.org/abs/2210.06640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06640] Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities](http://arxiv.org/abs/2210.06640)</code></li>
<li>Summary: <p>Although deep learning has made great progress in recent years, the exploding
economic and environmental costs of training neural networks are becoming
unsustainable. To address this problem, there has been a great deal of research
on <em>algorithmically-efficient deep learning</em>, which seeks to reduce training
costs not at the hardware or implementation level, but through changes in the
semantics of the training program. In this paper, we present a structured and
comprehensive overview of the research in this field. First, we formalize the
<em>algorithmic speedup</em> problem, then we use fundamental building blocks of
algorithmically efficient training to develop a taxonomy. Our taxonomy
highlights commonalities of seemingly disparate methods and reveals current
research gaps. Next, we present evaluation best practices to enable
comprehensive, fair, and reliable comparisons of speedup techniques. To further
aid research and applications, we discuss common bottlenecks in the training
pipeline (illustrated via experiments) and offer taxonomic mitigation
strategies for them. Finally, we highlight some unsolved research challenges
and present promising future directions.
</p></li>
</ul>

<h3>Title: Walk a Mile in Their Shoes: a New Fairness Criterion for Machine Learning. (arXiv:2210.06680v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06680">http://arxiv.org/abs/2210.06680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06680] Walk a Mile in Their Shoes: a New Fairness Criterion for Machine Learning](http://arxiv.org/abs/2210.06680)</code></li>
<li>Summary: <p>The old empathetic adage, ``Walk a mile in their shoes,'' asks that one
imagine the difficulties others may face. This suggests a new ML counterfactual
fairness criterion, based on a \textit{group} level: How would members of a
nonprotected group fare if their group were subject to conditions in some
protected group? Instead of asking what sentence would a particular Caucasian
convict receive if he were Black, take that notion to entire groups; e.g. how
would the average sentence for all White convicts change if they were Black,
but with their same White characteristics, e.g. same number of prior
convictions? We frame the problem and study it empirically, for different
datasets. Our approach also is a solution to the problem of covariate
correlation with sensitive attributes.
</p></li>
</ul>

<h3>Title: Equal Improvability: A New Fairness Notion Considering the Long-term Impact. (arXiv:2210.06732v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06732">http://arxiv.org/abs/2210.06732</a></li>
<li>Code URL: <a href="https://github.com/guldoganozgur/ei_fairness">https://github.com/guldoganozgur/ei_fairness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06732] Equal Improvability: A New Fairness Notion Considering the Long-term Impact](http://arxiv.org/abs/2210.06732)</code></li>
<li>Summary: <p>Devising a fair classifier that does not discriminate against different
groups is an important problem in machine learning. Although researchers have
proposed various ways of defining group fairness, most of them only focused on
the immediate fairness, ignoring the long-term impact of a fair classifier
under the dynamic scenario where each individual can improve its feature over
time. Such dynamic scenarios happen in real world, e.g., college admission and
credit loaning, where each rejected sample makes effort to change its features
to get accepted afterwards. In this dynamic setting, the long-term fairness
should equalize the samples' feature distribution across different groups after
the rejected samples make some effort to improve. In order to promote long-term
fairness, we propose a new fairness notion called Equal Improvability (EI),
which equalizes the potential acceptance rate of the rejected samples across
different groups assuming a bounded level of effort will be spent by each
rejected sample. We analyze the properties of EI and its connections with
existing fairness notions. To find a classifier that satisfies the EI
requirement, we propose and study three different approaches that solve
EI-regularized optimization problems. Through experiments on both synthetic and
real datasets, we demonstrate that the proposed EI-regularized algorithms
encourage us to find a fair classifier in terms of EI. Finally, we provide
experimental results on dynamic scenarios which highlight the advantages of our
EI metric in achieving the long-term fairness. Codes are available in a GitHub
repository, see https://github.com/guldoganozgur/ei_fairness.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: That's the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data. (arXiv:2210.06565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06565">http://arxiv.org/abs/2210.06565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06565] That's the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data](http://arxiv.org/abs/2210.06565)</code></li>
<li>Summary: <p>Pretraining multimodal models on Electronic Health Records (EHRs) provides a
means of learning representations that can transfer to downstream tasks with
minimal supervision. Recent multimodal models induce soft local alignments
between image regions and sentences. This is of particular interest in the
medical domain, where alignments might highlight regions in an image relevant
to specific phenomena described in free-text. While past work has suggested
that attention "heatmaps" can be interpreted in this manner, there has been
little evaluation of such alignments. We compare alignments from a
state-of-the-art multimodal (image and text) model for EHR with human
annotations that link image regions to sentences. Our main finding is that the
text has an often weak or unintuitive influence on attention; alignments do not
consistently reflect basic anatomical information. Moreover, synthetic
modifications -- such as substituting "left" for "right" -- do not
substantially influence highlights. Simple techniques such as allowing the
model to opt out of attending to the image and few-shot finetuning show promise
in terms of their ability to improve alignments with very little or no
supervision.
</p></li>
</ul>

<h3>Title: NoMorelization: Building Normalizer-Free Models from a Sample's Perspective. (arXiv:2210.06932v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06932">http://arxiv.org/abs/2210.06932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06932] NoMorelization: Building Normalizer-Free Models from a Sample's Perspective](http://arxiv.org/abs/2210.06932)</code></li>
<li>Summary: <p>The normalizing layer has become one of the basic configurations of deep
learning models, but it still suffers from computational inefficiency,
interpretability difficulties, and low generality. After gaining a deeper
understanding of the recent normalization and normalizer-free research works
from a sample's perspective, we reveal the fact that the problem lies in the
sampling noise and the inappropriate prior assumption. In this paper, we
propose a simple and effective alternative to normalization, which is called
"NoMorelization". NoMorelization is composed of two trainable scalars and a
zero-centered noise injector. Experimental results demonstrate that
NoMorelization is a general component for deep learning and is suitable for
different model paradigms (e.g., convolution-based and attention-based models)
to tackle different tasks (e.g., discriminative and generative tasks). Compared
with existing mainstream normalizers (e.g., BN, LN, and IN) and
state-of-the-art normalizer-free methods, NoMorelization shows the best
speed-accuracy trade-off.
</p></li>
</ul>

<h3>Title: On the Explainability of Natural Language Processing Deep Models. (arXiv:2210.06929v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06929">http://arxiv.org/abs/2210.06929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06929] On the Explainability of Natural Language Processing Deep Models](http://arxiv.org/abs/2210.06929)</code></li>
<li>Summary: <p>While there has been a recent explosion of work on ExplainableAI ExAI on deep
models that operate on imagery and tabular data, textual datasets present new
challenges to the ExAI community. Such challenges can be attributed to the lack
of input structure in textual data, the use of word embeddings that add to the
opacity of the models and the difficulty of the visualization of the inner
workings of deep models when they are trained on textual data.
</p></li>
</ul>

<p>Lately, methods have been developed to address the aforementioned challenges
and present satisfactory explanations on Natural Language Processing (NLP)
models. However, such methods are yet to be studied in a comprehensive
framework where common challenges are properly stated and rigorous evaluation
practices and metrics are proposed. Motivated to democratize ExAI methods in
the NLP field, we present in this work a survey that studies model-agnostic as
well as model-specific explainability methods on NLP models. Such methods can
either develop inherently interpretable NLP models or operate on pre-trained
models in a post-hoc manner. We make this distinction and we further decompose
the methods into three categories according to what they explain: (1) word
embeddings (input-level), (2) inner workings of NLP models (processing-level)
and (3) models' decisions (output-level). We also detail the different
evaluation approaches interpretability methods in the NLP field. Finally, we
present a case-study on the well-known neural machine translation in an
appendix and we propose promising future research directions for ExAI in the
NLP field.
</p>

<h3>Title: Interpreting Neural Policies with Disentangled Tree Representations. (arXiv:2210.06650v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06650">http://arxiv.org/abs/2210.06650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06650] Interpreting Neural Policies with Disentangled Tree Representations](http://arxiv.org/abs/2210.06650)</code></li>
<li>Summary: <p>Compact neural networks used in policy learning and closed-loop end-to-end
control learn representations from data that encapsulate agent dynamics and
potentially the agent-environment's factors of variation. A formal and
quantitative understanding and interpretation of these explanatory factors in
neural representations is difficult to achieve due to the complex and
intertwined correspondence of neural activities with emergent behaviors. In
this paper, we design a new algorithm that programmatically extracts tree
representations from compact neural policies, in the form of a set of logic
programs grounded by the world state. To assess how well networks uncover the
dynamics of the task and their factors of variation, we introduce
interpretability metrics that measure the disentanglement of learned neural
dynamics from a concentration of decisions, mutual information, and modularity
perspectives. Moreover, our method allows us to quantify how accurate the
extracted decision paths (explanations) are and computes cross-neuron logic
conflict. We demonstrate the effectiveness of our approach with several types
of compact network architectures on a series of end-to-end learning to control
tasks.
</p></li>
</ul>

<h3>Title: Global Explainability of GNNs via Logic Combination of Learned Concepts. (arXiv:2210.07147v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.07147">http://arxiv.org/abs/2210.07147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.07147] Global Explainability of GNNs via Logic Combination of Learned Concepts](http://arxiv.org/abs/2210.07147)</code></li>
<li>Summary: <p>While instance-level explanation of GNN is a well-studied problem with plenty
of approaches being developed, providing a global explanation for the behaviour
of a GNN is much less explored, despite its potential in interpretability and
debugging. Existing solutions either simply list local explanations for a given
class, or generate a synthetic prototypical graph with maximal score for a
given class, completely missing any combinatorial aspect that the GNN could
have learned. In this work, we propose GLGExplainer (Global Logic-based GNN
Explainer), the first Global Explainer capable of generating explanations as
arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a
fully differentiable architecture that takes local explanations as inputs and
combines them into a logic formula over graphical concepts, represented as
clusters of local explanations. Contrary to existing solutions, GLGExplainer
provides accurate and human-interpretable global explanations that are
perfectly aligned with ground-truth explanations (on synthetic data) or match
existing domain knowledge (on real-world data). Extracted formulas are faithful
to the model predictions, to the point of providing insights into some
occasionally incorrect rules learned by the model, making GLGExplainer a
promising diagnostic tool for learned GNNs.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
