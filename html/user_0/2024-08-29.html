<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-29</h1>
<h3>Title: Multi-Slice Spatial Transcriptomics Data Integration Analysis with STG3Net</h3>
<ul>
<li><strong>Authors: </strong>Donghai Fang, Fangfang Zhu, Wenwen Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15246">https://arxiv.org/abs/2408.15246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15246">https://arxiv.org/pdf/2408.15246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15246]] Multi-Slice Spatial Transcriptomics Data Integration Analysis with STG3Net(https://arxiv.org/abs/2408.15246)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>With the rapid development of the latest Spatially Resolved Transcriptomics (SRT) technology, which allows for the mapping of gene expression within tissue sections, the integrative analysis of multiple SRT data has become increasingly important. However, batch effects between multiple slices pose significant challenges in analyzing SRT data. To address these challenges, we have developed a plug-and-play batch correction method called Global Nearest Neighbor (G2N) anchor pairs selection. G2N effectively mitigates batch effects by selecting representative anchor pairs across slices. Building upon G2N, we propose STG3Net, which cleverly combines masked graph convolutional autoencoders as backbone modules. These autoencoders, integrated with generative adversarial learning, enable STG3Net to achieve robust multi-slice spatial domain identification and batch correction. We comprehensively evaluate the feasibility of STG3Net on three multiple SRT datasets from different platforms, considering accuracy, consistency, and the F1LISI metric (a measure of batch effect correction efficiency). Compared to existing methods, STG3Net achieves the best overall performance while preserving the biological variability and connectivity between slices. Source code and all public datasets used in this paper are available at this https URL and this https URL.</li>
</ul>

<h3>Title: Pedestrian Motion Prediction Using Transformer-based Behavior Clustering and Data-Driven Reachability Analysis</h3>
<ul>
<li><strong>Authors: </strong>Kleio Fragkedaki, Frank J. Jiang, Karl H. Johansson, Jonas Mårtensson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15250">https://arxiv.org/abs/2408.15250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15250">https://arxiv.org/pdf/2408.15250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15250]] Pedestrian Motion Prediction Using Transformer-based Behavior Clustering and Data-Driven Reachability Analysis(https://arxiv.org/abs/2408.15250)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this work, we present a transformer-based framework for predicting future pedestrian states based on clustered historical trajectory data. In previous studies, researchers propose enhancing pedestrian trajectory predictions by using manually crafted labels to categorize pedestrian behaviors and intentions. However, these approaches often only capture a limited range of pedestrian behaviors and introduce human bias into the predictions. To alleviate the dependency on manually crafted labels, we utilize a transformer encoder coupled with hierarchical density-based clustering to automatically identify diverse behavior patterns, and use these clusters in data-driven reachability analysis. By using a transformer-based approach, we seek to enhance the representation of pedestrian trajectories and uncover characteristics or features that are subsequently used to group trajectories into different "behavior" clusters. We show that these behavior clusters can be used with data-driven reachability analysis, yielding an end-to-end data-driven approach to predicting the future motion of pedestrians. We train and evaluate our approach on a real pedestrian dataset, showcasing its effectiveness in forecasting pedestrian movements.</li>
</ul>

<h3>Title: vFusedSeg3D: 3rd Place Solution for 2024 Waymo Open Dataset Challenge in Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Osama Amjad, Ammad Nadeem</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15254">https://arxiv.org/abs/2408.15254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15254">https://arxiv.org/pdf/2408.15254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15254]] vFusedSeg3D: 3rd Place Solution for 2024 Waymo Open Dataset Challenge in Semantic Segmentation(https://arxiv.org/abs/2408.15254)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this technical study, we introduce VFusedSeg3D, an innovative multi-modal fusion system created by the VisionRD team that combines camera and LiDAR data to significantly enhance the accuracy of 3D perception. VFusedSeg3D uses the rich semantic content of the camera pictures and the accurate depth sensing of LiDAR to generate a strong and comprehensive environmental understanding, addressing the constraints inherent in each modality. Through a carefully thought-out network architecture that aligns and merges these information at different stages, our novel feature fusion technique combines geometric features from LiDAR point clouds with semantic features from camera images. With the use of multi-modality techniques, performance has significantly improved, yielding a state-of-the-art mIoU of 72.46% on the validation set as opposed to the prior 70.51%.VFusedSeg3D sets a new benchmark in 3D segmentation accuracy. making it an ideal solution for applications requiring precise environmental perception.</li>
</ul>

<h3>Title: Transformer-based Neuro-Animator for Qualitative Simulation of Soft Body Movement</h3>
<ul>
<li><strong>Authors: </strong>Somnuk Phon-Amnuaisuk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15258">https://arxiv.org/abs/2408.15258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15258">https://arxiv.org/pdf/2408.15258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15258]] Transformer-based Neuro-Animator for Qualitative Simulation of Soft Body Movement(https://arxiv.org/abs/2408.15258)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The human mind effortlessly simulates the movements of objects governed by the laws of physics, such as a fluttering, or a waving flag under wind force, without understanding the underlying physics. This suggests that human cognition can predict the unfolding of physical events using an intuitive prediction process. This process might result from memory recall, yielding a qualitatively believable mental image, though it may not be exactly according to real-world physics. Drawing inspiration from the intriguing human ability to qualitatively visualize and describe dynamic events from past experiences without explicitly engaging in mathematical computations, this paper investigates the application of recent transformer architectures as a neuro-animator model. The visual transformer model is trained to predict flag motions at the \emph{t+1} time step, given information of previous motions from \emph{t-n} $\cdots$ \emph{t} time steps. The results show that the visual transformer-based architecture successfully learns temporal embedding of flag motions and produces reasonable quality simulations of flag waving under different wind forces.</li>
</ul>

<h3>Title: Multitask Fine-Tuning and Generative Adversarial Learning for Improved Auxiliary Classification</h3>
<ul>
<li><strong>Authors: </strong>Christopher Sun, Abishek Satish</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15265">https://arxiv.org/abs/2408.15265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15265">https://arxiv.org/pdf/2408.15265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15265]] Multitask Fine-Tuning and Generative Adversarial Learning for Improved Auxiliary Classification(https://arxiv.org/abs/2408.15265)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this study, we implement a novel BERT architecture for multitask fine-tuning on three downstream tasks: sentiment classification, paraphrase detection, and semantic textual similarity prediction. Our model, Multitask BERT, incorporates layer sharing and a triplet architecture, custom sentence pair tokenization, loss pairing, and gradient surgery. Such optimizations yield a 0.516 sentiment classification accuracy, 0.886 paraphase detection accuracy, and 0.864 semantic textual similarity correlation on test data. We also apply generative adversarial learning to BERT, constructing a conditional generator model that maps from latent space to create fake embeddings in $\mathbb{R}^{768}$. These fake embeddings are concatenated with real BERT embeddings and passed into a discriminator model for auxiliary classification. Using this framework, which we refer to as AC-GAN-BERT, we conduct semi-supervised sensitivity analyses to investigate the effect of increasing amounts of unlabeled training data on AC-GAN-BERT's test accuracy. Overall, aside from implementing a high-performing multitask classification system, our novelty lies in the application of adversarial learning to construct a generator that mimics BERT. We find that the conditional generator successfully produces rich embeddings with clear spatial correlation with class labels, demonstrating avoidance of mode collapse. Our findings validate the GAN-BERT approach and point to future directions of generator-aided knowledge distillation.</li>
</ul>

<h3>Title: A Survey of Deep Learning for Group-level Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiaohua Huang, Jinke Xu, Wenming Zheng, Qirong Mao, Abhinav Dhall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15276">https://arxiv.org/abs/2408.15276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15276">https://arxiv.org/pdf/2408.15276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15276]] A Survey of Deep Learning for Group-level Emotion Recognition(https://arxiv.org/abs/2408.15276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the advancement of artificial intelligence (AI) technology, group-level emotion recognition (GER) has emerged as an important area in analyzing human behavior. Early GER methods are primarily relied on handcrafted features. However, with the proliferation of Deep Learning (DL) techniques and their remarkable success in diverse tasks, neural networks have garnered increasing interest in GER. Unlike individual's emotion, group emotions exhibit diversity and dynamics. Presently, several DL approaches have been proposed to effectively leverage the rich information inherent in group-level image and enhance GER performance significantly. In this survey, we present a comprehensive review of DL techniques applied to GER, proposing a new taxonomy for the field cover all aspects of GER based on DL. The survey overviews datasets, the deep GER pipeline, and performance comparisons of the state-of-the-art methods past decade. Moreover, it summarizes and discuss the fundamental approaches and advanced developments for each aspect. Furthermore, we identify outstanding challenges and suggest potential avenues for the design of robust GER systems. To the best of our knowledge, thus survey represents the first comprehensive review of deep GER methods, serving as a pivotal references for future GER research endeavors.</li>
</ul>

<h3>Title: NeR-VCP: A Video Content Protection Method Based on Implicit Neural Representation</h3>
<ul>
<li><strong>Authors: </strong>Yangping Lin, Yan Ke, Ke Niu, Jia Liu, Xiaoyuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15281">https://arxiv.org/abs/2408.15281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15281">https://arxiv.org/pdf/2408.15281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15281]] NeR-VCP: A Video Content Protection Method Based on Implicit Neural Representation(https://arxiv.org/abs/2408.15281)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>With the popularity of video applications, the security of video content has emerged as a pressing issue that demands urgent attention. Most video content protection methods mainly rely on encryption technology, which needs to be manually designed or implemented in an experience-based manner. To address this problem, we propose an automatic encryption technique for video content protection based on implicit neural representation. We design a key-controllable module, which serves as a key for encryption and decryption. NeR-VCP first pre-distributes the key-controllable module trained by the sender to the recipients, and then uses Implicit Neural Representation (INR) with a (pre-distributed) key-controllable module to encrypt plain video as an implicit neural network, and the legal recipients uses a pre-distributed key-controllable module to decrypt this cipher neural network (the corresponding implicit neural network). Under the guidance of the key-controllable design, our method can improve the security of video content and provide a novel video encryption scheme. Moreover, using model compression techniques, this method can achieve video content protection while effectively mitigating the amount of encrypted data transferred. We experimentally find that it has superior performance in terms of visual representation, imperceptibility to illegal users, and security from a cryptographic viewpoint.</li>
</ul>

<h3>Title: 3D Photon Counting CT Image Super-Resolution Using Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Chuang Niu, Christopher Wiedeman, Mengzhou Li, Jonathan S Maltz, Ge Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15283">https://arxiv.org/abs/2408.15283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15283">https://arxiv.org/pdf/2408.15283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15283]] 3D Photon Counting CT Image Super-Resolution Using Conditional Diffusion Model(https://arxiv.org/abs/2408.15283)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This study aims to improve photon counting CT (PCCT) image resolution using denoising diffusion probabilistic models (DDPM). Although DDPMs have shown superior performance when applied to various computer vision tasks, their effectiveness has yet to be translated to high dimensional CT super-resolution. To train DDPMs in a conditional sampling manner, we first leverage CatSim to simulate realistic lower resolution PCCT images from high-resolution CT scans. Since maximizing DDPM performance is time-consuming for both inference and training, especially on high-dimensional PCCT data, we explore both 2D and 3D networks for conditional DDPM and apply methods to accelerate training. In particular, we decompose the 3D task into efficient 2D DDPMs and design a joint 2D inference in the reverse diffusion process that synergizes 2D results of all three dimensions to make the final 3D prediction. Experimental results show that our DDPM achieves improved results versus baseline reference models in recovering high-frequency structures, suggesting that a framework based on realistic simulation and DDPM shows promise for improving PCCT resolution.</li>
</ul>

<h3>Title: CrossInspector: A Static Analysis Approach for Cross-Contract Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiao Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15292">https://arxiv.org/abs/2408.15292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15292">https://arxiv.org/pdf/2408.15292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15292]] CrossInspector: A Static Analysis Approach for Cross-Contract Vulnerability Detection(https://arxiv.org/abs/2408.15292)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the development of blockchain technology, the detection of smart contract vulnerabilities is increasingly emphasized. However, when detecting vulnerabilities in inter-contract interactions (i.e., cross-contract vulnerabilities) using smart contract bytecode, existing tools often produce many false positives and false negatives due to insufficient recovery of semantic information and inadequate consideration of contract dependencies. We present CrossInspector, a novel framework for detecting cross-contract vulnerabilities at the bytecode level through static analysis. CrossInspector utilizes a trained Transformer model to recover semantic information and considers control flow, data flow, and dependencies related to smart contract state variables to construct a state dependency graph for fine-grained inter-procedural analysis. Additionally, CrossInspector incorporates a pruning method and two parallel optimization mechanisms to accelerate the vulnerability detection process. Experiments on our manually constructed dataset demonstrate that CrossInspector outperforms the state-of-the-art tools in both precision (97\%) and recall (96.75\%), while also significantly reducing the overall time from 16.34 seconds to 7.83 seconds, almost on par with the fastest tool that utilizes bytecode for detection. Additionally, we ran CrossInspector on a randomly selected set of 300 real-world smart contracts and identified 11 cross-contract vulnerabilities that were missed by prior tools.</li>
</ul>

<h3>Title: Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies</h3>
<ul>
<li><strong>Authors: </strong>Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15294">https://arxiv.org/abs/2408.15294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15294">https://arxiv.org/pdf/2408.15294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15294]] Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies(https://arxiv.org/abs/2408.15294)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Developing novel predictive models with complex biomedical information is challenging due to various idiosyncrasies related to heterogeneity, standardization or sparseness of the data. We previously introduced a person-centric ontology to organize information about individual patients, and a representation learning framework to extract person-centric knowledge graphs (PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a systematic approach to examine the results of GNN models trained with both structured and unstructured information from the MIMIC-III dataset. Through ablation studies on different clinical, demographic, and social data, we show the robustness of this approach in identifying predictive features in PKGs for the task of readmission prediction.</li>
</ul>

<h3>Title: GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Maxim Zhelnin, Viktor Moskvoretskii, Egor Shvetsov, Egor Venediktov, Mariya Krylova, Aleksandr Zuev, Evgeny Burnaev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15300">https://arxiv.org/abs/2408.15300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15300">https://arxiv.org/pdf/2408.15300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15300]] GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs(https://arxiv.org/abs/2408.15300)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parameter Efficient Fine-Tuning (PEFT) methods have gained popularity and democratized the usage of Large Language Models (LLMs). Recent studies have shown that a small subset of weights significantly impacts performance. Based on this observation, we introduce a novel PEFT method, called Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW). Our method updates only salient columns, while injecting Gaussian noise into non-salient ones. To identify these columns, we developeda generalized sensitivity metric that extends and unifies metrics from previous studies. Experiments with LLaMA models demonstrate that GIFT-SW outperforms full fine-tuning and modern PEFT methods under the same computational budget. Moreover, GIFT-SW offers practical advantages to recover performance of models subjected to mixed-precision quantization with keeping salient weights in full precision.</li>
</ul>

<h3>Title: The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Minghai Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15301">https://arxiv.org/abs/2408.15301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15301">https://arxiv.org/pdf/2408.15301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15301]] The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical Study(https://arxiv.org/abs/2408.15301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>We have observed a distinctive quantization-related behavior in the LLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and LLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying large language models (LLMs) efficiently. Among various bit widths and representations for weights and activations, the 8-bit integer weight and 8-bit integer activation (W8A8) configuration is particularly popular due to its widespread hardware support. However, the impact of W8A8 post-training quantization on model accuracy remains contentious. While several studies have suggested calibrating either weights or activations to mitigate accuracy degradation, a comprehensive solution has yet to be identified. In this paper, we empirically investigate multiple LLMs featured on an open LLM leaderboard, discovering that the LLaMA3-70B model series have a unique accuracy degradation behavior with W8A8 per-channel post-training quantization. In contrast, other model series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and Falcon demonstrate robust performance with W8A8, sometimes surpassing their FP16 counterparts. Contrary to previous assertions attributing degradation to the large dynamic range of activations, our findings indicate that the weight distribution of the LLaMA3-70B is the primary factor behind the vulnerability. By meticulously analyzing the distinct characteristics of weight distributions across Transformer blocks, we propose a mixed strategy with less than 3% of the layers enabling finer W8A8 quantization granularity, while the remaining 97% of layers retain the per-channel configuration. As a result, the average accuracy of LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of LLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires neither calibration nor fine-tuning.</li>
</ul>

<h3>Title: Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sakhinana Sagar Srinivas, Chidaksh Ravuru, Geethan Sannidhi, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15305">https://arxiv.org/abs/2408.15305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15305">https://arxiv.org/pdf/2408.15305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15305]] Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis(https://arxiv.org/abs/2408.15305)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>Semiconductors, crucial to modern electronics, are generally under-researched in foundational models. It highlights the need for research to enhance the semiconductor device technology portfolio and aid in high-end device fabrication. In this paper, we introduce sLAVA, a small-scale vision-language assistant tailored for semiconductor manufacturing, with a focus on electron microscopy image analysis. It addresses challenges of data scarcity and acquiring high-quality, expert-annotated data. We employ a teacher-student paradigm, using a foundational vision language model like GPT-4 as a teacher to create instruction-following multimodal data for customizing the student model, sLAVA, for electron microscopic image analysis tasks on consumer hardware with limited budgets. Our approach allows enterprises to further fine-tune the proposed framework with their proprietary data securely within their own infrastructure, protecting intellectual property. Rigorous experiments validate that our framework surpasses traditional methods, handles data shifts, and enables high-throughput screening.</li>
</ul>

<h3>Title: Optimization Solution Functions as Deterministic Policies for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Vanshaj Khattar, Ming Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15368">https://arxiv.org/abs/2408.15368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15368">https://arxiv.org/pdf/2408.15368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15368]] Optimization Solution Functions as Deterministic Policies for Offline Reinforcement Learning(https://arxiv.org/abs/2408.15368)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) is a promising approach for many control applications but faces challenges such as limited data coverage and value function overestimation. In this paper, we propose an implicit actor-critic (iAC) framework that employs optimization solution functions as a deterministic policy (actor) and a monotone function over the optimal value of optimization as a critic. By encoding optimality in the actor policy, we show that the learned policies are robust to the suboptimality of the learned actor parameters via the exponentially decaying sensitivity (EDS) property. We obtain performance guarantees for the proposed iAC framework and show its benefits over general function approximation schemes. Finally, we validate the proposed framework on two real-world applications and show a significant improvement over state-of-the-art (SOTA) offline RL methods.</li>
</ul>

<h3>Title: AutoPatch: Automated Generation of Hotpatches for Real-Time Embedded Devices</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Salehi, Karthik Pattabiraman</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15372">https://arxiv.org/abs/2408.15372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15372">https://arxiv.org/pdf/2408.15372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15372]] AutoPatch: Automated Generation of Hotpatches for Real-Time Embedded Devices(https://arxiv.org/abs/2408.15372)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Real-time embedded devices like medical or industrial devices are increasingly targeted by cyber-attacks. Prompt patching is crucial to mitigate the serious consequences of such attacks on these devices. Hotpatching is an approach to apply a patch to mission-critical embedded devices without rebooting them. However, existing hotpatching approaches require developers to manually write the hotpatch for target systems, which is time-consuming and error-prone. To address these issues, we propose AutoPatch, a new hotpatching technique that automatically generates functionally equivalent hotpatches via static analysis of the official patches. AutoPatch introduces a new software triggering approach that supports diverse embedded devices, and preserves the functionality of the official patch. In contrast to prior work, AutoPatch does not rely on hardware support for triggering patches, or on executing patches in specialized virtual machines. We implemented AutoPatch using the LLVM compiler, and evaluated its efficiency, effectiveness and generality using 62 real CVEs on four embedded devices with different specifications and architectures running popular RTOSes. We found that AutoPatch can fix more than 90% of CVEs, and resolve the vulnerability successfully. The results revealed an average total delay of less than 12.7 $\mu s$ for fixing the vulnerabilities, representing a performance improvement of 50% over RapidPatch, a state-of-the-art approach. Further, our memory overhead, on average, was slightly lower than theirs (23%). Finally, AutoPatch was able to generate hotpatches for all four devices without any modifications.</li>
</ul>

<h3>Title: Handling Geometric Domain Shifts in Semantic Segmentation of Surgical RGB and Hyperspectral Images</h3>
<ul>
<li><strong>Authors: </strong>Silvia Seidlitz, Jan Sellner, Alexander Studier-Fischer, Alessandro Motta, Berkin Özdemir, Beat P. Müller-Stich, Felix Nickel, Lena Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15373">https://arxiv.org/abs/2408.15373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15373">https://arxiv.org/pdf/2408.15373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15373]] Handling Geometric Domain Shifts in Semantic Segmentation of Surgical RGB and Hyperspectral Images(https://arxiv.org/abs/2408.15373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Robust semantic segmentation of intraoperative image data holds promise for enabling automatic surgical scene understanding and autonomous robotic surgery. While model development and validation are primarily conducted on idealistic scenes, geometric domain shifts, such as occlusions of the situs, are common in real-world open surgeries. To close this gap, we (1) present the first analysis of state-of-the-art (SOA) semantic segmentation models when faced with geometric out-of-distribution (OOD) data, and (2) propose an augmentation technique called "Organ Transplantation", to enhance generalizability. Our comprehensive validation on six different OOD datasets, comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs, each annotated with 19 classes, reveals a large performance drop in SOA organ segmentation models on geometric OOD data. This performance decline is observed not only in conventional RGB data (with a dice similarity coefficient (DSC) drop of 46 %) but also in HSI data (with a DSC drop of 45 %), despite the richer spectral information content. The performance decline increases with the spatial granularity of the input data. Our augmentation technique improves SOA model performance by up to 67 % for RGB data and 90 % for HSI data, achieving performance at the level of in-distribution performance on real OOD test data. Given the simplicity and effectiveness of our augmentation method, it is a valuable tool for addressing geometric domain shifts in surgical scene segmentation, regardless of the underlying model. Our code and pre-trained models are publicly available at this https URL.</li>
</ul>

<h3>Title: DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model Transformer for Multimodal Aspect-based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Adamu Lawan, Juhua Pu, Haruna Yunusa, Muhammad Lawan, Aliyu Umar, Adamu Sani Yahya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15379">https://arxiv.org/abs/2408.15379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15379">https://arxiv.org/pdf/2408.15379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15379]] DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model DualKanbaFormer: Kolmogorov-Arnold Networks and State Space Model Transformer for Multimodal Aspect-based Sentiment Analysis(https://arxiv.org/abs/2408.15379)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal aspect-based sentiment analysis (MABSA) enhances sentiment detection by combining text with other data types like images. However, despite setting significant benchmarks, attention mechanisms exhibit limitations in efficiently modelling long-range dependencies between aspect and opinion targets within the text. They also face challenges in capturing global-context dependencies for visual representations. To this end, we propose Kolmogorov-Arnold Networks (KANs) and Selective State Space model (Mamba) transformer (DualKanbaFormer), a novel architecture to address the above issues. We leverage the power of Mamba to capture global context dependencies, Multi-head Attention (MHA) to capture local context dependencies, and KANs to capture non-linear modelling patterns for both textual representations (textual KanbaFormer) and visual representations (visual KanbaFormer). Furthermore, we fuse the textual KanbaFormer and visual KanbaFomer with a gated fusion layer to capture the inter-modality dynamics. According to extensive experimental results, our model outperforms some state-of-the-art (SOTA) studies on two public datasets.</li>
</ul>

<h3>Title: Multi-Feature Aggregation in Diffusion Models for Enhanced Face Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Marcelo dos Santos, Rayson Laroca, Rafael O. Ribeiro, João C. Neves, David Menotti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15386">https://arxiv.org/abs/2408.15386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15386">https://arxiv.org/pdf/2408.15386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15386]] Multi-Feature Aggregation in Diffusion Models for Enhanced Face Super-Resolution(https://arxiv.org/abs/2408.15386)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Super-resolution algorithms often struggle with images from surveillance environments due to adverse conditions such as unknown degradation, variations in pose, irregular illumination, and occlusions. However, acquiring multiple images, even of low quality, is possible with surveillance cameras. In this work, we develop an algorithm based on diffusion models that utilize a low-resolution image combined with features extracted from multiple low-quality images to generate a super-resolved image while minimizing distortions in the individual's identity. Unlike other algorithms, our approach recovers facial features without explicitly providing attribute information or without the need to calculate a gradient of a function during the reconstruction process. To the best of our knowledge, this is the first time multi-features combined with low-resolution images are used as conditioners to generate more reliable super-resolution images using stochastic differential equations. The FFHQ dataset was employed for training, resulting in state-of-the-art performance in facial recognition and verification metrics when evaluated on the CelebA and Quis-Campi datasets. Our code is publicly available at this https URL</li>
</ul>

<h3>Title: Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome Dataset</h3>
<ul>
<li><strong>Authors: </strong>Diego Dimer Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15398">https://arxiv.org/abs/2408.15398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15398">https://arxiv.org/pdf/2408.15398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15398]] Evaluating Pre-Training Bias on Severe Acute Respiratory Syndrome Dataset(https://arxiv.org/abs/2408.15398)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) is a growing field of computer science that has found many practical applications in several domains, including Health. However, as data grows in size and availability, and the number of models that aim to aid or replace human decisions, it raises the concern that these models can be susceptible to bias, which can lead to harm to specific individuals by basing its decisions on protected attributes such as gender, religion, sexual orientation, ethnicity, and others. Visualization techniques might generate insights and help summarize large datasets, enabling data scientists to understand the data better before training a model by evaluating pre-training metrics applied to the datasets before training, which might contribute to identifying potential harm before any effort is put into training and deploying the models. This work uses the severe acute respiratory syndrome dataset from OpenDataSUS to visualize three pre-training bias metrics and their distribution across different regions in Brazil. A random forest model is trained in each region and applied to the others. The aim is to compare the bias for the different regions, focusing on their protected attributes and comparing the model's performance with the metric values.</li>
</ul>

<h3>Title: Awes, Laws, and Flaws From Today's LLM Research</h3>
<ul>
<li><strong>Authors: </strong>Adrian de Wynter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15409">https://arxiv.org/abs/2408.15409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15409">https://arxiv.org/pdf/2408.15409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15409]] Awes, Laws, and Flaws From Today's LLM Research(https://arxiv.org/abs/2408.15409)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We perform a critical examination of the scientific methodology behind contemporary large language model (LLM) research. For this we assess over 2,000 research works based on criteria typical of what is considered good research (e.g. presence of statistical tests and reproducibility) and cross-validate it with arguments that are at the centre of controversy (e.g., claims of emergent behaviour, the use of LLMs as evaluators). We find multiple trends, such as declines in claims of emergent behaviour and the presence of ethics disclaimers; and the rise of LLMs as evaluators. This paper underscores the need for more scrutiny and rigour by and from this field. Critical reading and familiarity with the literature are crucial to live up to the fundamentals of a responsible scientific method that is ethical, reproducible, systematic, and open to criticism.</li>
</ul>

<h3>Title: Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations</h3>
<ul>
<li><strong>Authors: </strong>Yize Zhao, Tina Behnia, Vala Vakilian, Christos Thrampoulidis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15417">https://arxiv.org/abs/2408.15417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15417">https://arxiv.org/pdf/2408.15417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15417]] Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations(https://arxiv.org/abs/2408.15417)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Next-token prediction (NTP) over large text corpora has become the go-to paradigm to train large language models. Yet, it remains unclear how NTP influences the mapping of linguistic patterns to geometric properties of the resulting model representations. We frame training of large language models as soft-label classification over sparse probabilistic label vectors, coupled with an analytical approximation that allows unrestricted generation of context embeddings. This approach links NTP training to rank-constrained, nuclear-norm regularized optimization in the logit domain, offering a framework for analyzing the geometry of word and context embeddings. In large embedding spaces, we find that NTP implicitly favors learning logits with a sparse plus low-rank structure. While the sparse component captures the co-occurrence frequency of context-word pairs, the orthogonal low-rank component, which becomes dominant as training progresses, depends solely on the sparsity pattern of the co-occurrence matrix. Consequently, when projected onto an appropriate subspace, representations of contexts that are followed by the same set of next-tokens collapse, a phenomenon we term subspace-collapse. We validate our findings on synthetic and small-scale real language datasets. Finally, we outline potential research directions aimed at deepening the understanding of NTP's influence on the learning of linguistic patterns and regularities.</li>
</ul>

<h3>Title: Understanding GNNs for Boolean Satisfiability through Approximation Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Jan Hůla, David Mojžíšek, Mikoláš Janota</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15418">https://arxiv.org/abs/2408.15418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15418">https://arxiv.org/pdf/2408.15418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15418]] Understanding GNNs for Boolean Satisfiability through Approximation Algorithms(https://arxiv.org/abs/2408.15418)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The paper deals with the interpretability of Graph Neural Networks in the context of Boolean Satisfiability. The goal is to demystify the internal workings of these models and provide insightful perspectives into their decision-making processes. This is done by uncovering connections to two approximation algorithms studied in the domain of Boolean Satisfiability: Belief Propagation and Semidefinite Programming Relaxations. Revealing these connections has empowered us to introduce a suite of impactful enhancements. The first significant enhancement is a curriculum training procedure, which incrementally increases the problem complexity in the training set, together with increasing the number of message passing iterations of the Graph Neural Network. We show that the curriculum, together with several other optimizations, reduces the training time by more than an order of magnitude compared to the baseline without the curriculum. Furthermore, we apply decimation and sampling of initial embeddings, which significantly increase the percentage of solved problems.</li>
</ul>

<h3>Title: Showing the Receipts: Understanding the Modern Ransomware Ecosystem</h3>
<ul>
<li><strong>Authors: </strong>Jack Cable, Ian W. Gray, Damon McCoy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15420">https://arxiv.org/abs/2408.15420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15420">https://arxiv.org/pdf/2408.15420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15420]] Showing the Receipts: Understanding the Modern Ransomware Ecosystem(https://arxiv.org/abs/2408.15420)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Ransomware attacks continue to wreak havoc across the globe, with public reports of total ransomware payments topping billions of dollars annually. While the use of cryptocurrency presents an avenue to understand the tactics of ransomware actors, to date published research has been constrained by relatively limited public datasets of ransomware payments. We present novel techniques to identify ransomware payments with low false positives, classifying nearly \$700 million in previously-unreported ransomware payments. We publish the largest public dataset of over \$900 million in ransomware payments -- several times larger than any existing public dataset. We then leverage this expanded dataset to present an analysis focused on understanding the activities of ransomware groups over time. This provides unique insights into ransomware behavior and a corpus for future study of ransomware cybercriminal activity.</li>
</ul>

<h3>Title: Avoiding Generative Model Writer's Block With Embedding Nudging</h3>
<ul>
<li><strong>Authors: </strong>Ali Zand, Milad Nasr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15450">https://arxiv.org/abs/2408.15450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15450">https://arxiv.org/pdf/2408.15450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15450]] Avoiding Generative Model Writer's Block With Embedding Nudging(https://arxiv.org/abs/2408.15450)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative image models, since introduction, have become a global phenomenon. From new arts becoming possible to new vectors of abuse, many new capabilities have become available. One of the challenging issues with generative models is controlling the generation process specially to prevent specific generations classes or instances . There are several reasons why one may want to control the output of generative models, ranging from privacy and safety concerns to application limitations or user preferences To address memorization and privacy challenges, there has been considerable research dedicated to filtering prompts or filtering the outputs of these models. What all these solutions have in common is that at the end of the day they stop the model from producing anything, hence limiting the usability of the model. In this paper, we propose a method for addressing this usability issue by making it possible to steer away from unwanted concepts (when detected in model's output) and still generating outputs. In particular we focus on the latent diffusion image generative models and how one can prevent them to generate particular images while generating similar images with limited overhead. We focus on mitigating issues like image memorization, demonstrating our technique's effectiveness through qualitative and quantitative evaluations. Our method successfully prevents the generation of memorized training images while maintaining comparable image quality and relevance to the unmodified model.</li>
</ul>

<h3>Title: Certified Causal Defense with Generalizable Robustness</h3>
<ul>
<li><strong>Authors: </strong>Yiran Qiao, Yu Yin, Chen Chen, Jing Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15451">https://arxiv.org/abs/2408.15451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15451">https://arxiv.org/pdf/2408.15451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15451]] Certified Causal Defense with Generalizable Robustness(https://arxiv.org/abs/2408.15451)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>While machine learning models have proven effective across various scenarios, it is widely acknowledged that many models are vulnerable to adversarial attacks. Recently, there have emerged numerous efforts in adversarial defense. Among them, certified defense is well known for its theoretical guarantees against arbitrary adversarial perturbations on input within a certain range (e.g., $l_2$ ball). However, most existing works in this line struggle to generalize their certified robustness in other data domains with distribution shifts. This issue is rooted in the difficulty of eliminating the negative impact of spurious correlations on robustness in different domains. To address this problem, in this work, we propose a novel certified defense framework GLEAN, which incorporates a causal perspective into the generalization problem in certified defense. More specifically, our framework integrates a certifiable causal factor learning component to disentangle the causal relations and spurious correlations between input and label, and thereby exclude the negative effect of spurious correlations on defense. On top of that, we design a causally certified defense strategy to handle adversarial attacks on latent causal factors. In this way, our framework is not only robust against malicious noises on data in the training distribution but also can generalize its robustness across domains with distribution shifts. Extensive experiments on benchmark datasets validate the superiority of our framework in certified robustness generalization in different data domains. Code is available in the supplementary materials.</li>
</ul>

<h3>Title: Hand1000: Generating Realistic Hands from Text with Only 1,000 Images</h3>
<ul>
<li><strong>Authors: </strong>Haozhuo Zhang, Bin Zhu, Yu Cao, Yanbin Hao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15461">https://arxiv.org/abs/2408.15461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15461">https://arxiv.org/pdf/2408.15461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15461]] Hand1000: Generating Realistic Hands from Text with Only 1,000 Images(https://arxiv.org/abs/2408.15461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image generation models have achieved remarkable advancements in recent years, aiming to produce realistic images from textual descriptions. However, these models often struggle with generating anatomically accurate representations of human hands. The resulting images frequently exhibit issues such as incorrect numbers of fingers, unnatural twisting or interlacing of fingers, or blurred and indistinct hands. These issues stem from the inherent complexity of hand structures and the difficulty in aligning textual descriptions with precise visual depictions of hands. To address these challenges, we propose a novel approach named Hand1000 that enables the generation of realistic hand images with target gesture using only 1,000 training samples. The training of Hand1000 is divided into three stages with the first stage aiming to enhance the model's understanding of hand anatomy by using a pre-trained hand gesture recognition model to extract gesture representation. The second stage further optimizes text embedding by incorporating the extracted hand gesture representation, to improve alignment between the textual descriptions and the generated hand images. The third stage utilizes the optimized embedding to fine-tune the Stable Diffusion model to generate realistic hand images. In addition, we construct the first publicly available dataset specifically designed for text-to-hand image generation. Based on the existing hand gesture recognition dataset, we adopt advanced image captioning models and LLaMA3 to generate high-quality textual descriptions enriched with detailed gesture information. Extensive experiments demonstrate that Hand1000 significantly outperforms existing models in producing anatomically correct hand images while faithfully representing other details in the text, such as faces, clothing, and colors.</li>
</ul>

<h3>Title: Legilimens: Practical and Unified Content Moderation for Large Language Model Services</h3>
<ul>
<li><strong>Authors: </strong>Jialin Wu, Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Jiayang Xu, Xinfeng Li, Wenyuan Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15488">https://arxiv.org/abs/2408.15488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15488">https://arxiv.org/pdf/2408.15488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15488]] Legilimens: Practical and Unified Content Moderation for Large Language Model Services(https://arxiv.org/abs/2408.15488)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Given the societal impact of unsafe content generated by large language models (LLMs), ensuring that LLM services comply with safety standards is a crucial concern for LLM service providers. Common content moderation methods are limited by an effectiveness-and-efficiency dilemma, where simple models are fragile while sophisticated models consume excessive computational resources. In this paper, we reveal for the first time that effective and efficient content moderation can be achieved by extracting conceptual features from chat-oriented LLMs, despite their initial fine-tuning for conversation rather than content moderation. We propose a practical and unified content moderation framework for LLM services, named Legilimens, which features both effectiveness and efficiency. Our red-team model-based data augmentation enhances the robustness of Legilimens against state-of-the-art jailbreaking. Additionally, we develop a framework to theoretically analyze the cost-effectiveness of Legilimens compared to other methods. We have conducted extensive experiments on five host LLMs, seventeen datasets, and nine jailbreaking methods to verify the effectiveness, efficiency, and robustness of Legilimens against normal and adaptive adversaries. A comparison of Legilimens with both commercial and academic baselines demonstrates the superior performance of Legilimens. Furthermore, we confirm that Legilimens can be applied to few-shot scenarios and extended to multi-label classification tasks.</li>
</ul>

<h3>Title: Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression</h3>
<ul>
<li><strong>Authors: </strong>Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15491">https://arxiv.org/abs/2408.15491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15491">https://arxiv.org/pdf/2408.15491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15491]] Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression(https://arxiv.org/abs/2408.15491)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have garnered widespread attention due to their remarkable performance across various tasks. However, to mitigate the issue of hallucinations, LLMs often incorporate retrieval-augmented pipeline to provide them with rich external knowledge and context. Nevertheless, challenges stem from inaccurate and coarse-grained context retrieved from the retriever. Supplying irrelevant context to the LLMs can result in poorer responses, increased inference latency, and higher costs. This paper introduces a method called Instruction-Aware Contextual Compression, which filters out less informative content, thereby accelerating and enhancing the use of LLMs. The experimental results demonstrate that Instruction-Aware Contextual Compression notably reduces memory consumption and minimizes generation latency while maintaining performance levels comparable to those achieved with the use of the full context. Specifically, we achieved a 50% reduction in context-related costs, resulting in a 5% reduction in inference memory usage and a 2.2-fold increase in inference speed, with only a minor drop of 0.047 in Rouge-1. These findings suggest that our method strikes an effective balance between efficiency and performance.</li>
</ul>

<h3>Title: ReMamba: Equip Mamba with Effective Long-Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang, Xunliang Cai, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15496">https://arxiv.org/abs/2408.15496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15496">https://arxiv.org/pdf/2408.15496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15496]] ReMamba: Equip Mamba with Effective Long-Sequence Modeling(https://arxiv.org/abs/2408.15496)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While the Mamba architecture demonstrates superior inference efficiency and competitive performance on short-context natural language processing (NLP) tasks, empirical evidence suggests its capacity to comprehend long contexts is limited compared to transformer-based models. In this study, we investigate the long-context efficiency issues of the Mamba models and propose ReMamba, which enhances Mamba's ability to comprehend long contexts. ReMamba incorporates selective compression and adaptation techniques within a two-stage re-forward process, incurring minimal additional inference costs overhead. Experimental results on the LongBench and L-Eval benchmarks demonstrate ReMamba's efficacy, improving over the baselines by 3.2 and 1.6 points, respectively, and attaining performance almost on par with same-size transformer models.</li>
</ul>

<h3>Title: MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifu Yuan, Zhenrui Zheng, Zibin Dong, Jianye Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15501">https://arxiv.org/abs/2408.15501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15501">https://arxiv.org/pdf/2408.15501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15501]] MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning(https://arxiv.org/abs/2408.15501)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multi-objective Reinforcement Learning (MORL) seeks to develop policies that simultaneously optimize multiple conflicting objectives, but it requires extensive online interactions. Offline MORL provides a promising solution by training on pre-collected datasets to generalize to any preference upon deployment. However, real-world offline datasets are often conservatively and narrowly distributed, failing to comprehensively cover preferences, leading to the emergence of out-of-distribution (OOD) preference areas. Existing offline MORL algorithms exhibit poor generalization to OOD preferences, resulting in policies that do not align with preferences. Leveraging the excellent expressive and generalization capabilities of diffusion models, we propose MODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs a preference-conditioned diffusion model as a planner to generate trajectories that align with various preferences and derive action for decision-making. To achieve accurate generation, MODULI introduces two return normalization methods under diverse preferences for refining guidance. To further enhance generalization to OOD preferences, MODULI proposes a novel sliding guidance mechanism, which involves training an additional slider adapter to capture the direction of preference changes. Incorporating the slider, it transitions from in-distribution (ID) preferences to generating OOD preferences, patching, and extending the incomplete Pareto front. Extensive experiments on the D4MORL benchmark demonstrate that our algorithm outperforms state-of-the-art Offline MORL baselines, exhibiting excellent generalization to OOD preferences.</li>
</ul>

<h3>Title: RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Haisheng Su, Feixiang Song, Cong Ma, Panpan Cai, Wei Wu, Cewu Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15503">https://arxiv.org/abs/2408.15503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15503">https://arxiv.org/pdf/2408.15503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15503]] RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed Autonomous Driving(https://arxiv.org/abs/2408.15503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust object detection and tracking under arbitrary sight of view is challenging yet essential for the development of Autonomous Vehicle technology. With the growing demand of unmanned function vehicles, near-field scene understanding becomes an important research topic in the areas of low-speed autonomous driving. Due to the complexity of driving conditions and diversity of near obstacles such as blind spots and high occlusion, the perception capability of near-field environment is still inferior than its farther counterpart. To further enhance the intelligent ability of unmanned vehicles, in this paper, we construct a multimodal data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view for ego vehicle, either global view or local view. Meanwhile, a large-scale multi-sensor dataset is built, named RoboSense, to facilitate near-field scene understanding. RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\times$ and $18\times$ as many annotations of near-field obstacles within 5$m$ as the previous single-vehicle datasets such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future development of related research, where the detailed data analysis as well as benchmarks are also provided accordingly.</li>
</ul>

<h3>Title: Measuring the Reliability of Causal Probing Methods: Tradeoffs, Limitations, and the Plight of Nullifying Interventions</h3>
<ul>
<li><strong>Authors: </strong>Marc Canby, Adam Davies, Chirag Rastogi, Julia Hockenmaier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15510">https://arxiv.org/abs/2408.15510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15510">https://arxiv.org/pdf/2408.15510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15510]] Measuring the Reliability of Causal Probing Methods: Tradeoffs, Limitations, and the Plight of Nullifying Interventions(https://arxiv.org/abs/2408.15510)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Causal probing is an approach to interpreting foundation models, such as large language models, by training probes to recognize latent properties of interest from embeddings, intervening on probes to modify this representation, and analyzing the resulting changes in the model's behavior. While some recent works have cast doubt on the theoretical basis of several leading causal probing intervention methods, it has been unclear how to systematically and empirically evaluate their effectiveness in practice. To address this problem, we propose a general empirical analysis framework to evaluate the reliability of causal probing interventions, formally defining and quantifying two key causal probing desiderata: completeness (fully transforming the representation of the target property) and selectivity (minimally impacting other properties). Our formalism allows us to make the first direct comparisons between different families of causal probing methods (e.g., linear vs. nonlinear or counterfactual vs. nullifying interventions). We conduct extensive experiments across several leading methods, finding that (1) there is an inherent tradeoff between these criteria, and no method is able to consistently satisfy both at once; and (2) across the board, nullifying interventions are always far less complete than counterfactual interventions, indicating that nullifying methods may not be an effective approach to causal probing.</li>
</ul>

<h3>Title: A Simple Baseline with Single-encoder for Referring Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Seonghoon Yu, Ilchae Jung, Byeongju Han, Taeoh Kim, Yunho Kim, Dongyoon Wee, Jeany Son</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15521">https://arxiv.org/abs/2408.15521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15521">https://arxiv.org/pdf/2408.15521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15521]] A Simple Baseline with Single-encoder for Referring Image Segmentation(https://arxiv.org/abs/2408.15521)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Referring image segmentation (RIS) requires dense vision-language interactions between visual pixels and textual words to segment objects based on a given description. However, commonly adapted dual-encoders in RIS, e.g., Swin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal dual-encoder), lack dense multi-modal interactions during pre-training, leading to a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods often rely on multi-modal fusion modules that interact two encoders, but this approach leads to high computational costs. In this paper, we present a novel RIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of shared self-attention across all framework components. This enables seamless interactions of two modalities from input to final prediction, producing granularly aligned multi-modal features. Furthermore, we propose lightweight yet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which contribute to the high efficiency of our model. Our simple baseline with a single encoder achieves outstanding performances on the RIS benchmark datasets while maintaining computational efficiency, compared to the most recent SoTA methods based on dual-encoders.</li>
</ul>

<h3>Title: LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation</h3>
<ul>
<li><strong>Authors: </strong>Haichuan Hu, Yuhan Sun, Qunjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15533">https://arxiv.org/abs/2408.15533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15533">https://arxiv.org/pdf/2408.15533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15533]] LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation(https://arxiv.org/abs/2408.15533)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has become a primary technique for mitigating hallucinations in large language models (LLMs). However, incomplete knowledge extraction and insufficient understanding can still mislead LLMs to produce irrelevant or even contradictory responses, which means hallucinations persist in RAG. In this paper, we propose LRP4RAG, a method based on the Layer-wise Relevance Propagation (LRP) algorithm for detecting hallucinations in RAG. Specifically, we first utilize LRP to compute the relevance between the input and output of the RAG generator. We then apply further extraction and resampling to the relevance matrix. The processed relevance data are input into multiple classifiers to determine whether the output contains hallucinations. To the best of our knowledge, this is the first time that LRP has been used for detecting RAG hallucinations, and extensive experiments demonstrate that LRP4RAG outperforms existing baselines.</li>
</ul>

<h3>Title: Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Jie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15542">https://arxiv.org/abs/2408.15542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15542">https://arxiv.org/pdf/2408.15542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15542]] Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input(https://arxiv.org/abs/2408.15542)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Rapid advancements have been made in extending Large Language Models (LLMs) to Large Multi-modal Models (LMMs). However, extending input modality of LLMs to video data remains a challenging endeavor, especially for long videos. Due to insufficient access to large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively processing long videos. In this paper, we introduce Kangaroo, a powerful Video LMM aimed at addressing these challenges. Confronted with issue of inadequate training data, we develop a data curation system to build a large-scale dataset with high-quality annotations for vision-language pre-training and instruction tuning. In addition, we design a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos. Evaluation results demonstrate that, with 8B parameters, Kangaroo achieves state-of-the-art performance across a variety of video understanding benchmarks while exhibiting competitive results on others. Particularly, on benchmarks specialized for long videos, Kangaroo excels some larger models with over 10B parameters and proprietary models.</li>
</ul>

<h3>Title: SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</h3>
<ul>
<li><strong>Authors: </strong>Sihang Li, Jian Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15545">https://arxiv.org/abs/2408.15545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15545">https://arxiv.org/pdf/2408.15545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15545]] SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding(https://arxiv.org/abs/2408.15545)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks. To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks. Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.</li>
</ul>

<h3>Title: ConsistencyTrack: A Robust Multi-Object Tracker with a Generation Strategy of Consistency Model</h3>
<ul>
<li><strong>Authors: </strong>Lifan Jiang, Zhihui Wang, Siqi Yin, Guangxiao Ma, Peng Zhang, Boxi Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15548">https://arxiv.org/abs/2408.15548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15548">https://arxiv.org/pdf/2408.15548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15548]] ConsistencyTrack: A Robust Multi-Object Tracker with a Generation Strategy of Consistency Model(https://arxiv.org/abs/2408.15548)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) is a critical technology in computer vision, designed to detect multiple targets in video sequences and assign each target a unique ID per frame. Existed MOT methods excel at accurately tracking multiple objects in real-time across various scenarios. However, these methods still face challenges such as poor noise resistance and frequent ID switches. In this research, we propose a novel ConsistencyTrack, joint detection and tracking(JDT) framework that formulates detection and association as a denoising diffusion process on perturbed bounding boxes. This progressive denoising strategy significantly improves the model's noise resistance. During the training phase, paired object boxes within two adjacent frames are diffused from ground-truth boxes to a random distribution, and then the model learns to detect and track by reversing this process. In inference, the model refines randomly generated boxes into detection and tracking results through minimal denoising steps. ConsistencyTrack also introduces an innovative target association strategy to address target occlusion. Experiments on the MOT17 and DanceTrack datasets demonstrate that ConsistencyTrack outperforms other compared methods, especially better than DiffusionTrack in inference speed and other performance metrics. Our code is available at this https URL.</li>
</ul>

<h3>Title: WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback</h3>
<ul>
<li><strong>Authors: </strong>Taiwei Shi, Zhuoer Wang, Longqi Yang, Ying-Chun Lin, Zexue He, Mengting Wan, Pei Zhou, Sujay Jauhar, Xiaofeng Xu, Xia Song, Jennifer Neville</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15549">https://arxiv.org/abs/2408.15549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15549">https://arxiv.org/pdf/2408.15549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15549]] WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback(https://arxiv.org/abs/2408.15549)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages real-time, in-situ user interactions to create preference datasets that more accurately reflect authentic human values. WildFeedback operates through a three-step process: feedback signal identification, preference data construction, and user-guided evaluation. We applied this framework to a large corpus of user-LLM conversations, resulting in a rich preference dataset that reflects genuine user preferences. This dataset captures the nuances of user preferences by identifying and classifying feedback signals within natural conversations, thereby enabling the construction of more representative and context-sensitive alignment data. Our extensive experiments demonstrate that LLMs fine-tuned on WildFeedback exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed user-guided evaluation. By incorporating real-time feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users. In summary, WildFeedback offers a robust, scalable solution for aligning LLMs with true human values, setting a new standard for the development and evaluation of user-centric language models.</li>
</ul>

<h3>Title: Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15556">https://arxiv.org/abs/2408.15556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15556">https://arxiv.org/pdf/2408.15556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15556]] Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models(https://arxiv.org/abs/2408.15556)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have experienced significant advancements recently, but still struggle to recognize and interpret intricate details in high-resolution (HR) images effectively. While state-of-the-art (SOTA) MLLMs claim to process images at 4K resolution, existing MLLM benchmarks only support up to 2K, leaving the capabilities of SOTA models on true HR images largely untested. Furthermore, existing methods for enhancing HR image perception in MLLMs rely on computationally expensive visual instruction tuning. To address these limitations, we introduce HR-Bench, the first deliberately designed benchmark to rigorously evaluate MLLM performance on 4K&8K images. Through extensive experiments, we demonstrate that while downsampling HR images leads to vision information loss, leveraging complementary modalities, e.g., text, can effectively compensate for this loss. Building upon this insight, we propose Divide, Conquer and Combine (DC$^2$), a novel training-free framework for enhancing MLLM perception of HR images. DC$^2$ follows a three-staged approach: 1) Divide: recursively partitioning the HR image into patches and merging similar patches to minimize computational overhead, 2) Conquer: leveraging the MLLM to generate accurate textual descriptions for each image patch, and 3) Combine: utilizing the generated text descriptions to enhance the MLLM's understanding of the overall HR image. Extensive experiments show that: 1) the SOTA MLLM achieves 63% accuracy, which is markedly lower than the 87% accuracy achieved by humans on HR-Bench; 2) our DC$^2$ brings consistent and significant improvements (a relative increase of +6% on HR-Bench and +8% on general multimodal benchmarks). The benchmark and code will be released to facilitate the multimodal R&D community.</li>
</ul>

<h3>Title: Generalization Capabilities of Neural Cellular Automata for Medical Image Segmentation: A Robust and Lightweight Approach</h3>
<ul>
<li><strong>Authors: </strong>Steven Korevaar, Ruwan Tennakoon, Alireza Bab-Hadiashar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15557">https://arxiv.org/abs/2408.15557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15557">https://arxiv.org/pdf/2408.15557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15557]] Generalization Capabilities of Neural Cellular Automata for Medical Image Segmentation: A Robust and Lightweight Approach(https://arxiv.org/abs/2408.15557)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In the field of medical imaging, the U-Net architecture, along with its variants, has established itself as a cornerstone for image segmentation tasks, particularly due to its strong performance when trained on limited datasets. Despite its impressive performance on identically distributed (in-domain) data, U-Nets exhibit a significant decline in performance when tested on data that deviates from the training distribution, out-of-distribution (out-of-domain) data. Current methodologies predominantly address this issue by employing generalization techniques that hinge on various forms of regularization, which have demonstrated moderate success in specific scenarios. This paper, however, ventures into uncharted territory by investigating the implications of utilizing models that are smaller by three orders of magnitude (i.e., x1000) compared to a conventional U-Net. A reduction of this size in U-net parameters typically adversely affects both in-domain and out-of-domain performance, possibly due to a significantly reduced receptive field. To circumvent this issue, we explore the concept of Neural Cellular Automata (NCA), which, despite its simpler model structure, can attain larger receptive fields through recursive processes. Experimental results on two distinct datasets reveal that NCA outperforms traditional methods in terms of generalization, while still maintaining a commendable IID performance.</li>
</ul>

<h3>Title: Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation</h3>
<ul>
<li><strong>Authors: </strong>Lujun Gui, Bin Xiao, Lei Su, Weipeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15562">https://arxiv.org/abs/2408.15562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15562">https://arxiv.org/pdf/2408.15562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15562]] Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation(https://arxiv.org/abs/2408.15562)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Lossless speculative decoding accelerates target large language model (LLM) inference by employing a lightweight draft model for generating tree-structured candidates, which are subsequently verified in parallel by the target LLM. Currently, effective approaches leverage feature-level rather than token-level autoregression within the draft model to facilitate more straightforward predictions and enhanced knowledge distillation. In this paper, we reassess these approaches and propose FSPAD (Feature Sampling and Partial Alignment Distillation for Lossless Speculative Decoding), which introduces two straightforward and effective components within the existing framework to boost lossless speculative decoding. Firstly, FSPAD utilizes token embeddings to sample features of the target LLM in high-dimensional space before feeding them into the draft model, due to the inherent uncertainty of the features preventing the draft model from obtaining the specific token output by the target LLM. Secondly, FSPAD introduces partial alignment distillation to weaken the draft model's connection between features and logits, aiming to reduce the conflict between feature alignment and logit confidence during training. Our experiments include both greedy and non-greedy decoding on the largest and smallest models from the Vicuna and LLaMA3-Instruct series, as well as tasks in multi-turn conversation, translation, summarization, question answering, mathematical reasoning, and retrieval-augmented generation. The results show that FSPAD outperforms the state-of-the-art method across all the aforementioned tasks and target LLMs.</li>
</ul>

<h3>Title: SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15565">https://arxiv.org/abs/2408.15565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15565">https://arxiv.org/pdf/2408.15565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15565]] SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models(https://arxiv.org/abs/2408.15565)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There is a growing trend of teaching large language models (LLMs) to solve mathematical problems through coding. Existing studies primarily focus on prompting powerful, closed-source models to generate seed training data followed by in-domain data augmentation, equipping LLMs with considerable capabilities for code-aided mathematical reasoning. However, continually training these models on augmented data derived from a few datasets such as GSM8K may impair their generalization abilities and restrict their effectiveness to a narrow range of question types. Conversely, the potential of improving such LLMs by leveraging large-scale, expert-written, diverse math question-answer pairs remains unexplored. To utilize these resources and tackle unique challenges such as code response assessment, we propose a novel paradigm that uses a code-based critic model to guide steps including question-code data construction, quality control, and complementary evaluation. We also explore different alignment algorithms with self-generated instruction/preference data to foster continuous improvement. Experiments across both in-domain (up to +5.7%) and out-of-domain (+4.4%) benchmarks in English and Chinese demonstrate the effectiveness of the proposed paradigm.</li>
</ul>

<h3>Title: Temporal Attention for Cross-View Sequential Image Localization</h3>
<ul>
<li><strong>Authors: </strong>Dong Yuan, Frederic Maire, Feras Dayoub</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15569">https://arxiv.org/abs/2408.15569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15569">https://arxiv.org/pdf/2408.15569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15569]] Temporal Attention for Cross-View Sequential Image Localization(https://arxiv.org/abs/2408.15569)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to enhancing cross-view localization, focusing on the fine-grained, sequential localization of street-view images within a single known satellite image patch, a significant departure from traditional one-to-one image retrieval methods. By expanding to sequential image fine-grained localization, our model, equipped with a novel Temporal Attention Module (TAM), leverages contextual information to significantly improve sequential image localization accuracy. Our method shows substantial reductions in both mean and median localization errors on the Cross-View Image Sequence (CVIS) dataset, outperforming current state-of-the-art single-image localization techniques. Additionally, by adapting the KITTI-CVL dataset into sequential image sets, we not only offer a more realistic dataset for future research but also demonstrate our model's robust generalization capabilities across varying times and areas, evidenced by a 75.3% reduction in mean distance error in cross-view sequential image localization.</li>
</ul>

<h3>Title: VFLIP: A Backdoor Defense for Vertical Federated Learning via Identification and Purification</h3>
<ul>
<li><strong>Authors: </strong>Yungi Cho, Woorim Han, Miseon Yu, Ho Bae, Yunheung Paek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15591">https://arxiv.org/abs/2408.15591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15591">https://arxiv.org/pdf/2408.15591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15591]] VFLIP: A Backdoor Defense for Vertical Federated Learning via Identification and Purification(https://arxiv.org/abs/2408.15591)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Vertical Federated Learning (VFL) focuses on handling vertically partitioned data over FL participants. Recent studies have discovered a significant vulnerability in VFL to backdoor attacks which specifically target the distinct characteristics of VFL. Therefore, these attacks may neutralize existing defense mechanisms designed primarily for Horizontal Federated Learning (HFL) and deep neural networks. In this paper, we present the first backdoor defense, called VFLIP, specialized for VFL. VFLIP employs the identification and purification techniques that operate at the inference stage, consequently improving the robustness against backdoor attacks to a great extent. VFLIP first identifies backdoor-triggered embeddings by adopting a participant-wise anomaly detection approach. Subsequently, VFLIP conducts purification which removes the embeddings identified as malicious and reconstructs all the embeddings based on the remaining embeddings. We conduct extensive experiments on CIFAR10, CINIC10, Imagenette, NUS-WIDE, and BankMarketing to demonstrate that VFLIP can effectively mitigate backdoor attacks in VFL. this https URL</li>
</ul>

<h3>Title: Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Minjong Yoo, Sangwoo Cho, Honguk Woo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15593">https://arxiv.org/abs/2408.15593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15593">https://arxiv.org/pdf/2408.15593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15593]] Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning(https://arxiv.org/abs/2408.15593)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) with diverse offline datasets can have the advantage of leveraging the relation of multiple tasks and the common skills learned across those tasks, hence allowing us to deal with real-world complex problems efficiently in a data-driven way. In offline RL where only offline data is used and online interaction with the environment is restricted, it is yet difficult to achieve the optimal policy for multiple tasks, especially when the data quality varies for the tasks. In this paper, we present a skill-based multi-task RL technique on heterogeneous datasets that are generated by behavior policies of different quality. To learn the shareable knowledge across those datasets effectively, we employ a task decomposition method for which common skills are jointly learned and used as guidance to reformulate a task in shared and achievable subtasks. In this joint learning, we use Wasserstein auto-encoder (WAE) to represent both skills and tasks on the same latent space and use the quality-weighted loss as a regularization term to induce tasks to be decomposed into subtasks that are more consistent with high-quality skills than others. To improve the performance of offline RL agents learned on the latent space, we also augment datasets with imaginary trajectories relevant to high-quality skills for each task. Through experiments, we show that our multi-task offline RL approach is robust to the mixed configurations of different-quality datasets and it outperforms other state-of-the-art algorithms for several robotic manipulation tasks and drone navigation tasks.</li>
</ul>

<h3>Title: Exploring Selective Layer Fine-Tuning in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuchang Sun, Yuexiang Xie, Bolin Ding, Yaliang Li, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15600">https://arxiv.org/abs/2408.15600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15600">https://arxiv.org/pdf/2408.15600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15600]] Exploring Selective Layer Fine-Tuning in Federated Learning(https://arxiv.org/abs/2408.15600)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has emerged as a promising paradigm for fine-tuning foundation models using distributed data in a privacy-preserving manner. Under limited computational resources, clients often find it more practical to fine-tune a selected subset of layers, rather than the entire model, based on their task-specific data. In this study, we provide a thorough theoretical exploration of selective layer fine-tuning in FL, emphasizing a flexible approach that allows the clients to adjust their selected layers according to their local data and resources. We theoretically demonstrate that the layer selection strategy has a significant impact on model convergence in two critical aspects: the importance of selected layers and the heterogeneous choices across clients. Drawing from these insights, we further propose a strategic layer selection method that utilizes local gradients and regulates layer selections across clients. The extensive experiments on both image and text datasets demonstrate the effectiveness of the proposed strategy compared with several baselines, highlighting its advances in identifying critical layers that adapt to the client heterogeneity and training dynamics in FL.</li>
</ul>

<h3>Title: Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications</h3>
<ul>
<li><strong>Authors: </strong>Korbinian Kuhn, Verena Kersken, Gottfried Zimmermann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15616">https://arxiv.org/abs/2408.15616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15616">https://arxiv.org/pdf/2408.15616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15616]] Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications(https://arxiv.org/abs/2408.15616)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.</li>
</ul>

<h3>Title: Convergent Differential Privacy Analysis for General Federated Learning: the f-DP Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yan Sun, Li Shen, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15621">https://arxiv.org/abs/2408.15621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15621">https://arxiv.org/pdf/2408.15621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15621]] Convergent Differential Privacy Analysis for General Federated Learning: the f-DP Perspective(https://arxiv.org/abs/2408.15621)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is an efficient collaborative training paradigm extensively developed with a focus on local privacy protection, and differential privacy (DP) is a classical approach to capture and ensure the reliability of local privacy. The powerful cooperation of FL and DP provides a promising learning framework for large-scale private clients, juggling both privacy securing and trustworthy learning. As the predominant algorithm of DP, the noisy perturbation has been widely studied and incorporated into various federated algorithms, theoretically proven to offer significant privacy protections. However, existing analyses in noisy FL-DP mostly rely on the composition theorem and cannot tightly quantify the privacy leakage challenges, which is nearly tight for small numbers of communication rounds but yields an arbitrarily loose and divergent bound under the large communication rounds. This implies a counterintuitive judgment, suggesting that FL may not provide adequate privacy protection during long-term training. To further investigate the convergent privacy and reliability of the FL-DP framework, in this paper, we comprehensively evaluate the worst privacy of two classical methods under the non-convex and smooth objectives based on the f-DP analysis, i.e. Noisy-FedAvg and Noisy-FedProx methods. With the aid of the shifted-interpolation technique, we successfully prove that the worst privacy of the Noisy-FedAvg method achieves a tight convergent lower bound. Moreover, in the Noisy-FedProx method, with the regularization of the proxy term, the worst privacy has a stable constant lower bound. Our analysis further provides a solid theoretical foundation for the reliability of privacy protection in FL-DP. Meanwhile, our conclusions can also be losslessly converted to other classical DP analytical frameworks, e.g. $(\epsilon,\delta)$-DP and R$\acute{\text{e}}$nyi-DP (RDP).</li>
</ul>

<h3>Title: CSAD: Unsupervised Component Segmentation for Logical Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsuan Hsieh, Shang-Hong Lai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15628">https://arxiv.org/abs/2408.15628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15628">https://arxiv.org/pdf/2408.15628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15628]] CSAD: Unsupervised Component Segmentation for Logical Anomaly Detection(https://arxiv.org/abs/2408.15628)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>To improve logical anomaly detection, some previous works have integrated segmentation techniques with conventional anomaly detection methods. Although these methods are effective, they frequently lead to unsatisfactory segmentation results and require manual annotations. To address these drawbacks, we develop an unsupervised component segmentation technique that leverages foundation models to autonomously generate training labels for a lightweight segmentation network without human labeling. Integrating this new segmentation technique with our proposed Patch Histogram module and the Local-Global Student-Teacher (LGST) module, we achieve a detection AUROC of 95.3% in the MVTec LOCO AD dataset, which surpasses previous SOTA methods. Furthermore, our proposed method provides lower latency and higher throughput than most existing approaches.</li>
</ul>

<h3>Title: Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Sondos Mohamed, Walter Zimmer, Ross Greer, Ahmed Alaaeldin Ghita, Modesto Castrillón-Santana, Mohan Trivedi, Alois Knoll, Salvatore Mario Carta, Mirko Marras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15637">https://arxiv.org/abs/2408.15637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15637">https://arxiv.org/pdf/2408.15637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15637]] Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection(https://arxiv.org/abs/2408.15637)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately detecting 3D objects from monocular images in dynamic roadside scenarios remains a challenging problem due to varying camera perspectives and unpredictable scene conditions. This paper introduces a two-stage training strategy to address these challenges. Our approach initially trains a model on the large-scale synthetic dataset, RoadSense3D, which offers a diverse range of scenarios for robust feature learning. Subsequently, we fine-tune the model on a combination of real-world datasets to enhance its adaptability to practical conditions. Experimental results of the Cube R-CNN model on challenging public benchmarks show a remarkable improvement in detection performance, with a mean average precision rising from 0.26 to 12.76 on the TUM Traffic A9 Highway dataset and from 2.09 to 6.60 on the DAIR-V2X-I dataset when performing transfer learning. Code, data, and qualitative video results are available on the project website: this https URL.</li>
</ul>

<h3>Title: GANs Conditioning Methods: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Anis Bourou, Auguste Genovesio, Valérie Mezger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15640">https://arxiv.org/abs/2408.15640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15640">https://arxiv.org/pdf/2408.15640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15640]] GANs Conditioning Methods: A Survey(https://arxiv.org/abs/2408.15640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, Generative Adversarial Networks (GANs) have seen significant advancements, leading to their widespread adoption across various fields. The original GAN architecture enables the generation of images without any specific control over the content, making it an unconditional generation process. However, many practical applications require precise control over the generated output, which has led to the development of conditional GANs (cGANs) that incorporate explicit conditioning to guide the generation process. cGANs extend the original framework by incorporating additional information (conditions), enabling the generation of samples that adhere to that specific criteria. Various conditioning methods have been proposed, each differing in how they integrate the conditioning information into both the generator and the discriminator networks. In this work, we review the conditioning methods proposed for GANs, exploring the characteristics of each method and highlighting their unique mechanisms and theoretical foundations. Furthermore, we conduct a comparative analysis of these methods, evaluating their performance on various image datasets. Through these analyses, we aim to provide insights into the strengths and limitations of various conditioning techniques, guiding future research and application in generative modeling.</li>
</ul>

<h3>Title: RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via Rotation-Invariant Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhaoxuan Wang, Xu Han, Hongxin Liu, Xianzhi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15643">https://arxiv.org/abs/2408.15643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15643">https://arxiv.org/pdf/2408.15643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15643]] RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via Rotation-Invariant Analysis(https://arxiv.org/abs/2408.15643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rotation robustness property has drawn much attention to point cloud analysis, whereas it still poses a critical challenge in 3D object detection. When subjected to arbitrary rotation, most existing detectors fail to produce expected outputs due to the poor rotation robustness. In this paper, we present RIDE, a pioneering exploration of Rotation-Invariance for the 3D LiDAR-point-based object DEtector, with the key idea of designing rotation-invariant features from LiDAR scenes and then effectively incorporating them into existing 3D detectors. Specifically, we design a bi-feature extractor that extracts (i) object-aware features though sensitive to rotation but preserve geometry well, and (ii) rotation-invariant features, which lose geometric information to a certain extent but are robust to rotation. These two kinds of features complement each other to decode 3D proposals that are robust to arbitrary rotations. Particularly, our RIDE is compatible and easy to plug into the existing one-stage and two-stage 3D detectors, and boosts both detection performance and rotation robustness. Extensive experiments on the standard benchmarks showcase that the mean average precision (mAP) and rotation robustness can be significantly boosted by integrating with our RIDE, with +5.6% mAP and 53% rotation robustness improvement on KITTI, +5.1% and 28% improvement correspondingly on nuScenes. The code will be available soon.</li>
</ul>

<h3>Title: Red Team Redemption: A Structured Comparison of Open-Source Tools for Adversary Emulation</h3>
<ul>
<li><strong>Authors: </strong>Max Landauer, Klaus Mayer, Florian Skopik, Markus Wurzenberger, Manuel Kern</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15645">https://arxiv.org/abs/2408.15645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15645">https://arxiv.org/pdf/2408.15645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15645]] Red Team Redemption: A Structured Comparison of Open-Source Tools for Adversary Emulation(https://arxiv.org/abs/2408.15645)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Red teams simulate adversaries and conduct sophisticated attacks against defenders without informing them about used tactics in advance. These interactive cyber exercises are highly beneficial to assess and improve the security posture of organizations, detect vulnerabilities, and train employees. Unfortunately, they are also time-consuming and expensive, which often limits their scale or prevents them entirely. To address this situation, adversary emulation tools partially automate attacker behavior and enable fast, continuous, and repeatable security testing even when involved personnel lacks red teaming experience. Currently, a wide range of tools designed for specific use-cases and requirements exist. To obtain an overview of these solutions, we conduct a review and structured comparison of nine open-source adversary emulation tools. To this end, we assemble a questionnaire with 80 questions addressing relevant aspects, including setup, support, documentation, usability, and technical features. In addition, we conduct a user study with domain experts to investigate the importance of these aspects for distinct user roles. Based on the evaluation and user feedback, we rank the tools and find MITRE Caldera, Metasploit, and Atomic Red Team on top.</li>
</ul>

<h3>Title: Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings</h3>
<ul>
<li><strong>Authors: </strong>Lingyu Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15650">https://arxiv.org/abs/2408.15650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15650">https://arxiv.org/pdf/2408.15650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15650]] Harnessing the Intrinsic Knowledge of Pretrained Language Models for Challenging Text Classification Settings(https://arxiv.org/abs/2408.15650)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Text classification is crucial for applications such as sentiment analysis and toxic text filtering, but it still faces challenges due to the complexity and ambiguity of natural language. Recent advancements in deep learning, particularly transformer architectures and large-scale pretraining, have achieved inspiring success in NLP fields. Building on these advancements, this thesis explores three challenging settings in text classification by leveraging the intrinsic knowledge of pretrained language models (PLMs). Firstly, to address the challenge of selecting misleading yet incorrect distractors for cloze questions, we develop models that utilize features based on contextualized word representations from PLMs, achieving performance that rivals or surpasses human accuracy. Secondly, to enhance model generalization to unseen labels, we create small finetuning datasets with domain-independent task label descriptions, improving model performance and robustness. Lastly, we tackle the sensitivity of large language models to in-context learning prompts by selecting effective demonstrations, focusing on misclassified examples and resolving model ambiguity regarding test example labels.</li>
</ul>

<h3>Title: TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Junbao Zhou, Jilin Mei, Pengze Wu, Liang Chen, Fangzhou Zhao, Xijun Zhao, Yu Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15657">https://arxiv.org/abs/2408.15657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15657">https://arxiv.org/pdf/2408.15657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15657]] TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic Segmentation(https://arxiv.org/abs/2408.15657)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In autonomous driving, 3D LiDAR plays a crucial role in understanding the vehicle's surroundings. However, the newly emerged, unannotated objects presents few-shot learning problem for semantic segmentation. This paper addresses the limitations of current few-shot semantic segmentation by exploiting the temporal continuity of LiDAR data. Employing a tracking model to generate pseudo-ground-truths from a sequence of LiDAR frames, our method significantly augments the dataset, enhancing the model's ability to learn on novel classes. However, this approach introduces a data imbalance biased to novel data that presents a new challenge of catastrophic forgetting. To mitigate this, we incorporate LoRA, a technique that reduces the number of trainable parameters, thereby preserving the model's performance on base classes while improving its adaptability to novel classes. This work represents a significant step forward in few-shot 3D LiDAR semantic segmentation for autonomous driving. Our code is available at this https URL.</li>
</ul>

<h3>Title: Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas</h3>
<ul>
<li><strong>Authors: </strong>Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15660">https://arxiv.org/abs/2408.15660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15660">https://arxiv.org/pdf/2408.15660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15660]] Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas(https://arxiv.org/abs/2408.15660)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have become the State-of-the-Art for text-to-image generation, and increasing research effort has been dedicated to adapting the inference process of pretrained diffusion models to achieve zero-shot capabilities. An example is the generation of panorama images, which has been tackled in recent works by combining independent diffusion paths over overlapping latent features, which is referred to as joint diffusion, obtaining perceptually aligned panoramas. However, these methods often yield semantically incoherent outputs and trade-off diversity for uniformity. To overcome this limitation, we propose the Merge-Attend-Diffuse operator, which can be plugged into different types of pretrained diffusion models used in a joint diffusion setting to improve the perceptual and semantical coherence of the generated panorama images. Specifically, we merge the diffusion paths, reprogramming self- and cross-attention to operate on the aggregated latent space. Extensive quantitative and qualitative experimental analysis, together with a user study, demonstrate that our method maintains compatibility with the input prompt and visual quality of the generated images while increasing their semantic coherence. We release the code at this https URL.</li>
</ul>

<h3>Title: StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements</h3>
<ul>
<li><strong>Authors: </strong>Jillian Fisher, Skyler Hallinan, Ximing Lu, Mitchell Gordon, Zaid Harchaoui, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15666">https://arxiv.org/abs/2408.15666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15666">https://arxiv.org/pdf/2408.15666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15666]] StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements(https://arxiv.org/abs/2408.15666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is an important but challenging task. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall. To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite an input specifically along various stylistic axes (e.g., formality and length) while maintaining low computational cost. StyleRemix outperforms state-of-the-art baselines and much larger LLMs in a variety of domains as assessed by both automatic and human evaluation. Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions</li>
</ul>

<h3>Title: Towards reliable respiratory disease diagnosis based on cough sounds and vision transformers</h3>
<ul>
<li><strong>Authors: </strong>Qian Wang, Zhaoyang Bu, Jiaxuan Mao, Wenyu Zhu, Jingya Zhao, Wei Du, Guochao Shi, Min Zhou, Si Chen, Jieming Qu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15667">https://arxiv.org/abs/2408.15667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15667">https://arxiv.org/pdf/2408.15667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15667]] Towards reliable respiratory disease diagnosis based on cough sounds and vision transformers(https://arxiv.org/abs/2408.15667)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in deep learning techniques have sparked performance boosts in various real-world applications including disease diagnosis based on multi-modal medical data. Cough sound data-based respiratory disease (e.g., COVID-19 and Chronic Obstructive Pulmonary Disease) diagnosis has also attracted much attention. However, existing works usually utilise traditional machine learning or deep models of moderate scales. On the other hand, the developed approaches are trained and evaluated on small-scale data due to the difficulty of curating and annotating clinical data on scale. To address these issues in prior works, we create a unified framework to evaluate various deep models from lightweight Convolutional Neural Networks (e.g., ResNet18) to modern vision transformers and compare their performance in respiratory disease classification. Based on the observations from such an extensive empirical study, we propose a novel approach to cough-based disease classification based on both self-supervised and supervised learning on a large-scale cough data set. Experimental results demonstrate our proposed approach outperforms prior arts consistently on two benchmark datasets for COVID-19 diagnosis and a proprietary dataset for COPD/non-COPD classification with an AUROC of 92.5%.</li>
</ul>

<h3>Title: DEAR: Depth-Enhanced Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sadegh Rahmaniboldaji, Filip Rybansky, Quoc Vuong, Frank Guerin, Andrew Gilbert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15679">https://arxiv.org/abs/2408.15679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15679">https://arxiv.org/pdf/2408.15679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15679]] DEAR: Depth-Enhanced Action Recognition(https://arxiv.org/abs/2408.15679)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Detecting actions in videos, particularly within cluttered scenes, poses significant challenges due to the limitations of 2D frame analysis from a camera perspective. Unlike human vision, which benefits from 3D understanding, recognizing actions in such environments can be difficult. This research introduces a novel approach integrating 3D features and depth maps alongside RGB features to enhance action recognition accuracy. Our method involves processing estimated depth maps through a separate branch from the RGB feature encoder and fusing the features to understand the scene and actions comprehensively. Using the Side4Video framework and VideoMamba, which employ CLIP and VisionMamba for spatial feature extraction, our approach outperformed our implementation of the Side4Video network on the Something-Something V2 dataset. Our code is available at: this https URL</li>
</ul>

<h3>Title: TempoFormer: A Transformer for Temporally-aware Representations in Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Talia Tseriotou, Adam Tsakalidis, Maria Liakata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15689">https://arxiv.org/abs/2408.15689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15689">https://arxiv.org/pdf/2408.15689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15689]] TempoFormer: A Transformer for Temporally-aware Representations in Change Detection(https://arxiv.org/abs/2408.15689)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Dynamic representation learning plays a pivotal role in understanding the evolution of linguistic content over time. On this front both context and time dynamics as well as their interplay are of prime importance. Current approaches model context via pre-trained representations, which are typically temporally agnostic. Previous work on modeling context and temporal dynamics has used recurrent methods, which are slow and prone to overfitting. Here we introduce TempoFormer, the fist task-agnostic transformer-based and temporally-aware model for dynamic representation learning. Our approach is jointly trained on inter and intra context dynamics and introduces a novel temporal variation of rotary positional embeddings. The architecture is flexible and can be used as the temporal representation foundation of other models or applied to different transformer-based architectures. We show new SOTA performance on three different real-time change detection tasks.</li>
</ul>

<h3>Title: Synthetic Forehead-creases Biometric Generation for Reliable User Verification</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Tandon, Geetanjali Sharma, Gaurav Jaswal, Aditya Nigam, Raghavendra Ramachandra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15693">https://arxiv.org/abs/2408.15693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15693">https://arxiv.org/pdf/2408.15693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15693]] Synthetic Forehead-creases Biometric Generation for Reliable User Verification(https://arxiv.org/abs/2408.15693)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, biometric, diffusion</a></li>
<li><strong>Abstract: </strong>Recent studies have emphasized the potential of forehead-crease patterns as an alternative for face, iris, and periocular recognition, presenting contactless and convenient solutions, particularly in situations where faces are covered by surgical masks. However, collecting forehead data presents challenges, including cost and time constraints, as developing and optimizing forehead verification methods requires a substantial number of high-quality images. To tackle these challenges, the generation of synthetic biometric data has gained traction due to its ability to protect privacy while enabling effective training of deep learning-based biometric verification methods. In this paper, we present a new framework to synthesize forehead-crease image data while maintaining important features, such as uniqueness and realism. The proposed framework consists of two main modules: a Subject-Specific Generation Module (SSGM), based on an image-to-image Brownian Bridge Diffusion Model (BBDM), which learns a one-to-many mapping between image pairs to generate identity-aware synthetic forehead creases corresponding to real subjects, and a Subject-Agnostic Generation Module (SAGM), which samples new synthetic identities with assistance from the SSGM. We evaluate the diversity and realism of the generated forehead-crease images primarily using the Fréchet Inception Distance (FID) and the Structural Similarity Index Measure (SSIM). In addition, we assess the utility of synthetically generated forehead-crease images using a forehead-crease verification system (FHCVS). The results indicate an improvement in the verification accuracy of the FHCVS by utilizing synthetic data.</li>
</ul>

<h3>Title: Protecting Privacy in Federated Time Series Analysis: A Pragmatic Technology Review for Application Developers</h3>
<ul>
<li><strong>Authors: </strong>Daniel Bachlechner, Ruben Hetfleisch, Stephan Krenn, Thomas Lorünser, Michael Rader</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15694">https://arxiv.org/abs/2408.15694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15694">https://arxiv.org/pdf/2408.15694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15694]] Protecting Privacy in Federated Time Series Analysis: A Pragmatic Technology Review for Application Developers(https://arxiv.org/abs/2408.15694)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>The federated analysis of sensitive time series has huge potential in various domains, such as healthcare or manufacturing. Yet, to fully unlock this potential, requirements imposed by various stakeholders must be fulfilled, regarding, e.g., efficiency or trust assumptions. While many of these requirements can be addressed by deploying advanced secure computation paradigms such as fully homomorphic encryption, certain aspects require an integration with additional privacy-preserving technologies. In this work, we perform a qualitative requirements elicitation based on selected real-world use cases. We match the derived requirements categories against the features and guarantees provided by available technologies. For each technology, we additionally perform a maturity assessment, including the state of standardization and availability on the market. Furthermore, we provide a decision tree supporting application developers in identifying the most promising technologies available matching their needs. Finally, existing gaps are identified, highlighting research potential to advance the field.</li>
</ul>

<h3>Title: Evaluating Model Robustness Using Adaptive Sparse L0 Regularization</h3>
<ul>
<li><strong>Authors: </strong>Weiyou Liu, Zhenyang Li, Weitong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15702">https://arxiv.org/abs/2408.15702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15702">https://arxiv.org/pdf/2408.15702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15702]] Evaluating Model Robustness Using Adaptive Sparse L0 Regularization(https://arxiv.org/abs/2408.15702)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks have demonstrated remarkable success in various domains but remain susceptible to adversarial examples, which are slightly altered inputs designed to induce misclassification. While adversarial attacks typically optimize under Lp norm constraints, attacks based on the L0 norm, prioritising input sparsity, are less studied due to their complex and non convex nature. These sparse adversarial examples challenge existing defenses by altering a minimal subset of features, potentially uncovering more subtle DNN weaknesses. However, the current L0 norm attack methodologies face a trade off between accuracy and efficiency either precise but computationally intense or expedient but imprecise. This paper proposes a novel, scalable, and effective approach to generate adversarial examples based on the L0 norm, aimed at refining the robustness evaluation of DNNs against such perturbations.</li>
</ul>

<h3>Title: Towards Realistic Example-based Modeling via 3D Gaussian Stitching</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Gao, Ziyi Yang, Bingchen Gong, Xiaoguang Han, Sipeng Yang, Xiaogang Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15708">https://arxiv.org/abs/2408.15708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15708">https://arxiv.org/pdf/2408.15708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15708]] Towards Realistic Example-based Modeling via 3D Gaussian Stitching(https://arxiv.org/abs/2408.15708)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Using parts of existing models to rebuild new models, commonly termed as example-based modeling, is a classical methodology in the realm of computer graphics. Previous works mostly focus on shape composition, making them very hard to use for realistic composition of 3D objects captured from real-world scenes. This leads to combining multiple NeRFs into a single 3D scene to achieve seamless appearance blending. However, the current SeamlessNeRF method struggles to achieve interactive editing and harmonious stitching for real-world scenes due to its gradient-based strategy and grid-based representation. To this end, we present an example-based modeling method that combines multiple Gaussian fields in a point-based representation using sample-guided synthesis. Specifically, as for composition, we create a GUI to segment and transform multiple fields in real time, easily obtaining a semantically meaningful composition of models represented by 3D Gaussian Splatting (3DGS). For texture blending, due to the discrete and irregular nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF is not supported. Thus, a novel sampling-based cloning method is proposed to harmonize the blending while preserving the original rich texture and content. Our workflow consists of three steps: 1) real-time segmentation and transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis to identify boundary points in the intersecting area between the source and target models, and 3) two-phase optimization of the target model using sampling-based cloning and gradient constraints. Extensive experimental results validate that our approach significantly outperforms previous works in terms of realistic synthesis, demonstrating its practicality. More demos are available at this https URL.</li>
</ul>

<h3>Title: Autoregressive model path dependence near Ising criticality</h3>
<ul>
<li><strong>Authors: </strong>Yi Hong Teoh, Roger G. Melko</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15715">https://arxiv.org/abs/2408.15715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15715">https://arxiv.org/pdf/2408.15715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15715]] Autoregressive model path dependence near Ising criticality(https://arxiv.org/abs/2408.15715)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive models are a class of generative model that probabilistically predict the next output of a sequence based on previous inputs. The autoregressive sequence is by definition one-dimensional (1D), which is natural for language tasks and hence an important component of modern architectures like recurrent neural networks (RNNs) and transformers. However, when language models are used to predict outputs on physical systems that are not intrinsically 1D, the question arises of which choice of autoregressive sequence -- if any -- is optimal. In this paper, we study the reconstruction of critical correlations in the two-dimensional (2D) Ising model, using RNNs and transformers trained on binary spin data obtained near the thermal phase transition. We compare the training performance for a number of different 1D autoregressive sequences imposed on finite-size 2D lattices. We find that paths with long 1D segments are more efficient at training the autoregressive models compared to space-filling curves that better preserve the 2D locality. Our results illustrate the potential importance in choosing the optimal autoregressive sequence ordering when training modern language models for tasks in physics.</li>
</ul>

<h3>Title: Defending Text-to-image Diffusion Models: Surprising Efficacy of Textual Perturbations Against Backdoor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Oscar Chew, Po-Yi Lu, Jayden Lin, Hsuan-Tien Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15721">https://arxiv.org/abs/2408.15721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15721">https://arxiv.org/pdf/2408.15721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15721]] Defending Text-to-image Diffusion Models: Surprising Efficacy of Textual Perturbations Against Backdoor Attacks(https://arxiv.org/abs/2408.15721)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have been widely adopted in real-world applications due to their ability to generate realistic images from textual descriptions. However, recent studies have shown that these methods are vulnerable to backdoor attacks. Despite the significant threat posed by backdoor attacks on text-to-image diffusion models, countermeasures remain under-explored. In this paper, we address this research gap by demonstrating that state-of-the-art backdoor attacks against text-to-image diffusion models can be effectively mitigated by a surprisingly simple defense strategy - textual perturbation. Experiments show that textual perturbations are effective in defending against state-of-the-art backdoor attacks with minimal sacrifice to generation quality. We analyze the efficacy of textual perturbation from two angles: text embedding space and cross-attention maps. They further explain how backdoor attacks have compromised text-to-image diffusion models, providing insights for studying future attack and defense strategies. Our code is available at this https URL.</li>
</ul>

<h3>Title: LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of Relational Knowledge in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Max Ploner, Jacek Wiland, Sebastian Pohl, Alan Akbik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15729">https://arxiv.org/abs/2408.15729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15729">https://arxiv.org/pdf/2408.15729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15729]] LM-PUB-QUIZ: A Comprehensive Framework for Zero-Shot Evaluation of Relational Knowledge in Language Models(https://arxiv.org/abs/2408.15729)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Knowledge probing evaluates the extent to which a language model (LM) has acquired relational knowledge during its pre-training phase. It provides a cost-effective means of comparing LMs of different sizes and training setups and is useful for monitoring knowledge gained or lost during continual learning (CL). In prior work, we presented an improved knowledge probe called BEAR (Wiland et al., 2024), which enables the comparison of LMs trained with different pre-training objectives (causal and masked LMs) and addresses issues of skewed distributions in previous probes to deliver a more unbiased reading of LM knowledge. With this paper, we present LM-PUB- QUIZ, a Python framework and leaderboard built around the BEAR probing mechanism that enables researchers and practitioners to apply it in their work. It provides options for standalone evaluation and direct integration into the widely-used training pipeline of the Hugging Face TRANSFORMERS library. Further, it provides a fine-grained analysis of different knowledge types to assist users in better understanding the knowledge in each evaluated LM. We publicly release LM-PUB-QUIZ as an open-source project.</li>
</ul>

<h3>Title: TCNFormer: Temporal Convolutional Network Former for Short-Term Wind Speed Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Abid Hasan Zim, Aquib Iqbal, Asad Malik, Zhicheng Dong, Hanzhou Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15737">https://arxiv.org/abs/2408.15737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15737">https://arxiv.org/pdf/2408.15737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15737]] TCNFormer: Temporal Convolutional Network Former for Short-Term Wind Speed Forecasting(https://arxiv.org/abs/2408.15737)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Global environmental challenges and rising energy demands have led to extensive exploration of wind energy technologies. Accurate wind speed forecasting (WSF) is crucial for optimizing wind energy capture and ensuring system stability. However, predicting wind speed remains challenging due to its inherent randomness, fluctuation, and unpredictability. This study proposes the Temporal Convolutional Network Former (TCNFormer) for short-term (12-hour) wind speed forecasting. The TCNFormer integrates the Temporal Convolutional Network (TCN) and transformer encoder to capture the spatio-temporal features of wind speed. The transformer encoder consists of two distinct attention mechanisms: causal temporal multi-head self-attention (CT-MSA) and temporal external attention (TEA). CT-MSA ensures that the output of a step derives only from previous steps, i.e., causality. Locality is also introduced to improve efficiency. TEA explores potential relationships between different sample sequences in wind speed data. This study utilizes wind speed data from the NASA Prediction of Worldwide Energy Resources (NASA POWER) of Patenga Sea Beach, Chittagong, Bangladesh (latitude 22.2352° N, longitude 91.7914° E) over a year (six seasons). The findings indicate that the TCNFormer outperforms state-of-the-art models in prediction accuracy. The proposed TCNFormer presents a promising method for spatio-temporal WSF and may achieve desirable performance in real-world applications of wind power systems.</li>
</ul>

<h3>Title: Segmentation-guided Layer-wise Image Vectorization with Gradient Fills</h3>
<ul>
<li><strong>Authors: </strong>Hengyu Zhou, Hui Zhang, Bin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15741">https://arxiv.org/abs/2408.15741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15741">https://arxiv.org/pdf/2408.15741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15741]] Segmentation-guided Layer-wise Image Vectorization with Gradient Fills(https://arxiv.org/abs/2408.15741)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The widespread use of vector graphics creates a significant demand for vectorization methods. While recent learning-based techniques have shown their capability to create vector images of clear topology, filling these primitives with gradients remains a challenge. In this paper, we propose a segmentation-guided vectorization framework to convert raster images into concise vector graphics with radial gradient fills. With the guidance of an embedded gradient-aware segmentation subroutine, our approach progressively appends gradient-filled Bézier paths to the output, where primitive parameters are initiated with our newly designed initialization technique and are optimized to minimize our novel loss function. We build our method on a differentiable renderer with traditional segmentation algorithms to develop it as a model-free tool for raster-to-vector conversion. It is tested on various inputs to demonstrate its feasibility, independent of datasets, to synthesize vector graphics with improved visual quality and layer-wise topology compared to prior work.</li>
</ul>

<h3>Title: Form and meaning co-determine the realization of tone in Taiwan Mandarin spontaneous speech: the case of Tone 3 sandhi</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Lu, Yu-Ying Chuang, R. Harald Baayen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15747">https://arxiv.org/abs/2408.15747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15747">https://arxiv.org/pdf/2408.15747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15747]] Form and meaning co-determine the realization of tone in Taiwan Mandarin spontaneous speech: the case of Tone 3 sandhi(https://arxiv.org/abs/2408.15747)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>In Standard Chinese, Tone 3 (the dipping tone) becomes Tone 2 (rising tone) when followed by another Tone 3. Previous studies have noted that this sandhi process may be incomplete, in the sense that the assimilated Tone 3 is still distinct from a true Tone 2. While Mandarin Tone 3 sandhi is widely studied using carefully controlled laboratory speech (Xu, 1997) and more formal registers of Beijing Mandarin (Yuan and Chen, 2014), less is known about its realization in spontaneous speech, and about the effect of contextual factors on tonal realization. The present study investigates the pitch contours of two-character words with T2-T3 and T3-T3 tone patterns in spontaneous Taiwan Mandarin conversations. Our analysis makes use of the Generative Additive Mixed Model (GAMM, Wood, 2017) to examine fundamental frequency (f0) contours as a function of normalized time. We consider various factors known to influence pitch contours, including gender, speaking rate, speaker, neighboring tones, word position, bigram probability, and also novel predictors, word and word sense (Chuang et al., 2024). Our analyses revealed that in spontaneous Taiwan Mandarin, T3-T3 words become indistinguishable from T2-T3 words, indicating complete sandhi, once the strong effect of word (or word sense) is taken into account. For our data, the shape of f0 contours is not co-determined by word frequency. In contrast, the effect of word meaning on f0 contours is robust, as strong as the effect of adjacent tones, and is present for both T2-T3 and T3-T3 words.</li>
</ul>

<h3>Title: Harmonized Speculative Sampling</h3>
<ul>
<li><strong>Authors: </strong>Lefan Zhang, Xiaodan Wang, Yanhua Huang, Ruiwen Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15766">https://arxiv.org/abs/2408.15766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15766">https://arxiv.org/pdf/2408.15766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15766]] Harmonized Speculative Sampling(https://arxiv.org/abs/2408.15766)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative sampling has proven to be an effective solution to accelerate decoding from large language models, where the acceptance rate significantly determines the performance. Most previous works on improving the acceptance rate focus on aligned training and efficient decoding, implicitly paying less attention to the linkage of training and decoding. In this work, we first investigate the linkage of training and decoding for speculative sampling and then propose a solution named HArmonized Speculative Sampling (HASS). HASS improves the acceptance rate without extra inference overhead by harmonizing training and decoding on their objectives and contexts. Experiments on three LLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup ratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.</li>
</ul>

<h3>Title: Started Off Local, Now We're in the Cloud: Forensic Examination of the Amazon Echo Show 15 Smart Display</h3>
<ul>
<li><strong>Authors: </strong>Jona Crasselt, Gaston Pugliese</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15768">https://arxiv.org/abs/2408.15768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15768">https://arxiv.org/pdf/2408.15768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15768]] Started Off Local, Now We're in the Cloud: Forensic Examination of the Amazon Echo Show 15 Smart Display(https://arxiv.org/abs/2408.15768)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Amazon Echo is one of the most popular product families of smart speakers and displays. Considering their growing presence in modern households as well as the digital traces associated with residents' interactions with these devices, analyses of Echo products are likely to become more common for forensic investigators at "smart home" crime scenes. With this in mind, we present the first forensic examination of the Echo Show 15, Amazon's largest smart display running on Fire OS and the first Echo device with Visual ID, a face recognition feature. We unveil a non-invasive method for accessing the unencrypted file system of the Echo Show 15 based on an undocumented pinout for the eMMC interface which we discovered on the main logic board. On the device, we identify various local usage artifacts, such as searched products, streamed movies, visited websites, metadata of photos and videos as well as logged events of Visual ID about movements and users detected by the built-in camera. Furthermore, we utilize an insecurely stored token on the Echo Show 15 to obtain access to remote user artifacts in Amazon's cloud, including Alexa voice requests, calendars, contacts, conversations, photos, and videos. In this regard, we also identify new Amazon APIs through network traffic analysis of two companion apps, namely Alexa and Photos. Overall, in terms of practical relevance, our findings demonstrate a non-destructive way of data acquisition for Echo Show 15 devices as well as how to lift the scope of forensic traces from local artifacts on the device to remote artifacts stored in the cloud.</li>
</ul>

<h3>Title: A Survey on Evaluation of Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Huang, Jingyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15769">https://arxiv.org/abs/2408.15769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15769">https://arxiv.org/pdf/2408.15769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15769]] A Survey on Evaluation of Multimodal Large Language Models(https://arxiv.org/abs/2408.15769)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) "what to evaluate" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) "how to evaluate" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.</li>
</ul>

<h3>Title: A Survey on Facial Expression Recognition of Static and Dynamic Emotions</h3>
<ul>
<li><strong>Authors: </strong>Yan Wang, Shaoqi Yan, Yang Liu, Wei Song, Jing Liu, Yang Chang, Xinji Mai, Xiping Hu, Wenqiang Zhang, Zhongxue Gan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15777">https://arxiv.org/abs/2408.15777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15777">https://arxiv.org/pdf/2408.15777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15777]] A Survey on Facial Expression Recognition of Static and Dynamic Emotions(https://arxiv.org/abs/2408.15777)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial expression recognition (FER) aims to analyze emotional states from static images and dynamic sequences, which is pivotal in enhancing anthropomorphic communication among humans, robots, and digital avatars by leveraging AI technologies. As the FER field evolves from controlled laboratory environments to more complex in-the-wild scenarios, advanced methods have been rapidly developed and new challenges and apporaches are encounted, which are not well addressed in existing reviews of FER. This paper offers a comprehensive survey of both image-based static FER (SFER) and video-based dynamic FER (DFER) methods, analyzing from model-oriented development to challenge-focused categorization. We begin with a critical comparison of recent reviews, an introduction to common datasets and evaluation criteria, and an in-depth workflow on FER to establish a robust research foundation. We then systematically review representative approaches addressing eight main challenges in SFER (such as expression disturbance, uncertainties, compound emotions, and cross-domain inconsistency) as well as seven main challenges in DFER (such as key frame sampling, expression intensity variations, and cross-modal alignment). Additionally, we analyze recent advancements, benchmark performances, major applications, and ethical considerations. Finally, we propose five promising future directions and development trends to guide ongoing research. The project page for this paper can be found at this https URL.</li>
</ul>

<h3>Title: Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions</h3>
<ul>
<li><strong>Authors: </strong>Huachuan Qiu, Zhenzhong Lan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15787">https://arxiv.org/abs/2408.15787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15787">https://arxiv.org/pdf/2408.15787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15787]] Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions(https://arxiv.org/abs/2408.15787)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Virtual counselors powered by large language models (LLMs) aim to create interactive support systems that effectively assist clients struggling with mental health challenges. To replicate counselor-client conversations, researchers have built an online mental health platform that allows professional counselors to provide clients with text-based counseling services for about an hour per session. Notwithstanding its effectiveness, challenges exist as human annotation is time-consuming, cost-intensive, privacy-protected, and not scalable. To address this issue and investigate the applicability of LLMs in psychological counseling conversation simulation, we propose a framework that employs two LLMs via role-playing for simulating counselor-client interactions. Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. We implement both the counselor and the client by zero-shot prompting the GPT-4 model. In order to assess the effectiveness of LLMs in simulating counselor-client interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the synthetic data from various perspectives. We begin by assessing the client's performance through automatic evaluations. Next, we analyze and compare the disparities between dialogues generated by the LLM and those generated by professional counselors. Furthermore, we conduct extensive experiments to thoroughly examine the performance of our LLM-based counselor trained with synthetic interactive dialogues by benchmarking against state-of-the-art models for mental health.</li>
</ul>

<h3>Title: Efficient LLM Scheduling by Learning to Rank</h3>
<ul>
<li><strong>Authors: </strong>Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15792">https://arxiv.org/abs/2408.15792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15792">https://arxiv.org/pdf/2408.15792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15792]] Efficient LLM Scheduling by Learning to Rank(https://arxiv.org/abs/2408.15792)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori. Consequently, most LLM serving systems employ a simple First-come-first-serve (FCFS) scheduling strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput and service quality. In this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank. The ranking information offers valuable guidance for scheduling requests. Building on this insight, we develop a novel scheduler for LLM inference and serving that can approximate the shortest-job-first (SJF) schedule better than existing approaches. We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation. Our code is available at this https URL</li>
</ul>

<h3>Title: Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization</h3>
<ul>
<li><strong>Authors: </strong>Léo Hemamou, Mehdi Debiane</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15801">https://arxiv.org/abs/2408.15801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15801">https://arxiv.org/pdf/2408.15801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15801]] Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization(https://arxiv.org/abs/2408.15801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In an era where digital text is proliferating at an unprecedented rate, efficient summarization tools are becoming indispensable. While Large Language Models (LLMs) have been successfully applied in various NLP tasks, their role in extractive text summarization remains underexplored. This paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and ChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of abstractive methods, which often suffer from issues like factual inaccuracies and hallucinations, EYEGLAXS focuses on extractive summarization to ensure factual and grammatical integrity. Utilizing state-of-the-art techniques such as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS addresses the computational and resource challenges typically associated with LLMs. The system sets new performance benchmarks on well-known datasets like PubMed and ArXiv. Furthermore, we extend our research through additional analyses that explore the adaptability of LLMs in handling different sequence lengths and their efficiency in training on smaller datasets. These contributions not only set a new standard in the field but also open up promising avenues for future research in extractive text summarization.</li>
</ul>

<h3>Title: Visual Prompt Engineering for Medical Vision Language Models in Radiology</h3>
<ul>
<li><strong>Authors: </strong>Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Paul F. Jäger, Klaus Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15802">https://arxiv.org/abs/2408.15802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15802">https://arxiv.org/pdf/2408.15802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15802]] Visual Prompt Engineering for Medical Vision Language Models in Radiology(https://arxiv.org/abs/2408.15802)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Medical image classification in radiology faces significant challenges, particularly in generalizing to unseen pathologies. In contrast, CLIP offers a promising solution by leveraging multimodal learning to improve zero-shot classification performance. However, in the medical domain, lesions can be small and might not be well represented in the embedding space. Therefore, in this paper, we explore the potential of visual prompt engineering to enhance the capabilities of Vision Language Models (VLMs) in radiology. Leveraging BiomedCLIP, trained on extensive biomedical image-text pairs, we investigate the impact of embedding visual markers directly within radiological images to guide the model's attention to critical regions. Our evaluation on the JSRT dataset, focusing on lung nodule malignancy classification, demonstrates that incorporating visual prompts $\unicode{x2013}$ such as arrows, circles, and contours $\unicode{x2013}$ significantly improves classification metrics including AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides attention maps, showcasing enhanced model interpretability and focus on clinically relevant areas. These findings underscore the efficacy of visual prompt engineering as a straightforward yet powerful approach to advance VLM performance in medical image analysis.</li>
</ul>

<h3>Title: Object Detection for Vehicle Dashcams using Transformers</h3>
<ul>
<li><strong>Authors: </strong>Osama Mustafa, Khizer Ali, Anam Bibi, Imran Siddiqi, Momina Moetesum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15809">https://arxiv.org/abs/2408.15809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15809">https://arxiv.org/pdf/2408.15809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15809]] Object Detection for Vehicle Dashcams using Transformers(https://arxiv.org/abs/2408.15809)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The use of intelligent automation is growing significantly in the automotive industry, as it assists drivers and fleet management companies, thus increasing their productivity. Dash cams are now been used for this purpose which enables the instant identification and understanding of multiple objects and occurrences in the surroundings. In this paper, we propose a novel approach for object detection in dashcams using transformers. Our system is based on the state-of-the-art DEtection TRansformer (DETR), which has demonstrated strong performance in a variety of conditions, including different weather and illumination scenarios. The use of transformers allows for the consideration of contextual information in decisionmaking, improving the accuracy of object detection. To validate our approach, we have trained our DETR model on a dataset that represents real-world conditions. Our results show that the use of intelligent automation through transformers can significantly enhance the capabilities of dashcam systems. The model achieves an mAP of 0.95 on detection.</li>
</ul>

<h3>Title: Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Laura Bragagnolo, Matteo Terreran, Davide Allegro, Stefano Ghidoni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15810">https://arxiv.org/abs/2408.15810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15810">https://arxiv.org/pdf/2408.15810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15810]] Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation(https://arxiv.org/abs/2408.15810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust 3D human pose estimation is crucial to ensure safe and effective human-robot collaboration. Accurate human perception,however, is particularly challenging in these scenarios due to strong occlusions and limited camera viewpoints. Current 3D human pose estimation approaches are rather vulnerable in such conditions. In this work we present a novel approach for robust 3D human pose estimation in the context of human-robot collaboration. Instead of relying on noisy 2D features triangulation, we perform multi-view fusion on 3D skeletons provided by absolute monocular methods. Accurate 3D pose estimation is then obtained via reprojection error optimization, introducing limbs length symmetry constraints. We evaluate our approach on the public dataset Human3.6M and on a novel version Human3.6M-Occluded, derived adding synthetic occlusions on the camera views with the purpose of testing pose estimation algorithms under severe occlusions. We further validate our method on real human-robot collaboration workcells, in which we strongly surpass current 3D human pose estimation methods. Our approach outperforms state-of-the-art multi-view human pose estimation techniques and demonstrates superior capabilities in handling challenging scenarios with strong occlusions, representing a reliable and effective solution for real human-robot collaboration setups.</li>
</ul>

<h3>Title: DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled Queries</h3>
<ul>
<li><strong>Authors: </strong>Yu Yang, Jianbiao Mei, Liang Liu, Siliang Du, Yilin Xiao, Jongwon Ra, Yong Liu, Xiao Xu, Huifeng Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15813">https://arxiv.org/abs/2408.15813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15813">https://arxiv.org/pdf/2408.15813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15813]] DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled Queries(https://arxiv.org/abs/2408.15813)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR panoptic segmentation, which jointly performs instance and semantic segmentation for things and stuff classes, plays a fundamental role in LiDAR perception tasks. While most existing methods explicitly separate these two segmentation tasks and utilize different branches (i.e., semantic and instance branches), some recent methods have embraced the query-based paradigm to unify LiDAR panoptic segmentation. However, the distinct spatial distribution and inherent characteristics of objects(things) and their surroundings(stuff) in 3D scenes lead to challenges, including the mutual competition of things/stuff and the ambiguity of classification/segmentation. In this paper, we propose decoupling things/stuff queries according to their intrinsic properties for individual decoding and disentangling classification/segmentation to mitigate ambiguity. To this end, we propose a novel framework dubbed DQFormer to implement semantic and instance segmentation in a unified workflow. Specifically, we design a decoupled query generator to propose informative queries with semantics by localizing things/stuff positions and fusing multi-level BEV embeddings. Moreover, a query-oriented mask decoder is introduced to decode corresponding segmentation masks by performing masked cross-attention between queries and mask embeddings. Finally, the decoded masks are combined with the semantics of the queries to produce panoptic results. Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the superiority of our DQFormer framework.</li>
</ul>

<h3>Title: Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification</h3>
<ul>
<li><strong>Authors: </strong>Abu Adnan Sadi, Mohammad Ashrafuzzaman Khan, Lubaba Binte Saber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15827">https://arxiv.org/abs/2408.15827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15827">https://arxiv.org/pdf/2408.15827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15827]] Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification(https://arxiv.org/abs/2408.15827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>As the field of artificial intelligence progresses, assistive technologies are becoming more widely used across all industries. The healthcare industry is no different, with numerous studies being done to develop assistive tools for healthcare professionals. Automatic diagnostic systems are one such beneficial tool that can assist with a variety of tasks, including collecting patient information, analyzing test results, and diagnosing patients. However, the idea of developing systems that can provide a differential diagnosis has been largely overlooked in most of these research studies. In this study, we propose a transformer-based approach for providing differential diagnoses based on a patient's age, sex, medical history, and symptoms. We use the DDXPlus dataset, which provides differential diagnosis information for patients based on 49 disease types. Firstly, we propose a method to process the tabular patient data from the dataset and engineer them into patient reports to make them suitable for our research. In addition, we introduce two data modification modules to diversify the training data and consequently improve the robustness of the models. We approach the task as a multi-label classification problem and conduct extensive experiments using four transformer models. All the models displayed promising results by achieving over 97% F1 score on the held-out test set. Moreover, we design additional behavioral tests to get a broader understanding of the models. In particular, for one of our test cases, we prepared a custom test set of 100 samples with the assistance of a doctor. The results on the custom set showed that our proposed data modification modules improved the model's generalization capabilities. We hope our findings will provide future researchers with valuable insights and inspire them to develop reliable systems for automatic differential diagnosis.</li>
</ul>

<h3>Title: SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Liu, Lintao Wang, Xiaogan Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15829">https://arxiv.org/abs/2408.15829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15829">https://arxiv.org/pdf/2408.15829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15829]] SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization(https://arxiv.org/abs/2408.15829)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an attractive summarization approach by integrating various types of information to create extremely concise yet informative summaries for individual modalities. Existing methods overlook the issue that multimodal data often contains more topic irrelevant information, which can mislead the model into producing inaccurate summaries especially for extremely short ones. In this paper, we propose SITransformer, a \textbf{S}hared \textbf{I}nformation-guided \textbf{T}ransformer for extreme multimodal summarization. It has a shared information guided pipeline which involves a cross-modal shared information extractor and a cross-modal interaction module. The extractor formulates semantically shared salient information from different modalities by devising a novel filtering process consisting of a differentiable top-k selector and a shared-information guided gating unit. As a result, the common, salient, and relevant contents across modalities are identified. Next, a transformer with cross-modal attentions is developed for intra- and inter-modality learning with the shared information guidance to produce the extreme summary. Comprehensive experiments demonstrate that SITransformer significantly enhances the summarization quality for both video and text summaries for XMSMO. Our code will be publicly available at this https URL.</li>
</ul>

<h3>Title: Network transferability of adversarial patches in real-time object detection</h3>
<ul>
<li><strong>Authors: </strong>Jens Bayer, Stefan Becker, David Münch, Michael Arens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15833">https://arxiv.org/abs/2408.15833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15833">https://arxiv.org/pdf/2408.15833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15833]] Network transferability of adversarial patches in real-time object detection(https://arxiv.org/abs/2408.15833)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial patches in computer vision can be used, to fool deep neural networks and manipulate their decision-making process. One of the most prominent examples of adversarial patches are evasion attacks for object detectors. By covering parts of objects of interest, these patches suppress the detections and thus make the target object 'invisible' to the object detector. Since these patches are usually optimized on a specific network with a specific train dataset, the transferability across multiple networks and datasets is not given. This paper addresses these issues and investigates the transferability across numerous object detector architectures. Our extensive evaluation across various models on two distinct datasets indicates that patches optimized with larger models provide better network transferability than patches that are optimized with smaller models.</li>
</ul>

<h3>Title: On the (In)security of optimized Stern-like signature schemes</h3>
<ul>
<li><strong>Authors: </strong>André Chailloux, Simona Etinski</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15843">https://arxiv.org/abs/2408.15843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15843">https://arxiv.org/pdf/2408.15843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15843]] On the (In)security of optimized Stern-like signature schemes(https://arxiv.org/abs/2408.15843)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Stern's signature scheme is a historically important code-based signature scheme. A crucial optimization of this scheme is to generate pseudo-random vectors and a permutation instead of random ones, and most proposals that are based on Stern's signature use this optimization. However, its security has not been properly analyzed, especially when we use deterministic commitments. In this article, we study the security of this optimization. We first show that for some parameters, there is an attack that exploits this optimization and breaks the scheme in time $O(2^{\frac{\lambda}{2}})$ while the claimed security is $\lambda$ bits. This impacts in particular the recent Quasy-cyclic Stern signature scheme [BGMS22]. Our second result shows that there is an efficient fix to this attack. By adding a string $salt \in \{0,1\}^{2\lambda}$ to the scheme, and changing slightly how the pseudo-random strings are generated, we prove not only that our attack doesn't work but that for any attack, the scheme preserves $\lambda$ bits of security, and this fix increases the total signature size by only $2\lambda$ bits. We apply this construction to other optimizations on Stern's signature scheme, such as the use of Lee's metric or the use of hash trees, and we show how these optimizations improve the signature length of Stern's signature scheme.</li>
</ul>

<h3>Title: Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction</h3>
<ul>
<li><strong>Authors: </strong>Xueqing Zhang. Di Fu, Naihao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15844">https://arxiv.org/abs/2408.15844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15844">https://arxiv.org/pdf/2408.15844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15844]] Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction(https://arxiv.org/abs/2408.15844)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Video key frame extraction is important in various fields, such as video summary, retrieval, and compression. Therefore, we suggest a video key frame extraction algorithm based on shot segmentation using Von Neumann entropy. The segmentation of shots is achieved through the computation of Von Neumann entropy of the similarity matrix among frames within the video sequence. The initial frame of each shot is selected as key frames, which combines the temporal sequence information of frames. The experimental results show the extracted key frames can fully and accurately represent the original video content while minimizing the number of repeated frames.</li>
</ul>

<h3>Title: What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Yaseen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15857">https://arxiv.org/abs/2408.15857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15857">https://arxiv.org/pdf/2408.15857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15857]] What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector(https://arxiv.org/abs/2408.15857)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.</li>
</ul>

<h3>Title: Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Weilin Lin, Li Liu, Jianze Li, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15861">https://arxiv.org/abs/2408.15861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15861">https://arxiv.org/pdf/2408.15861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15861]] Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation(https://arxiv.org/abs/2408.15861)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, data-free</a></li>
<li><strong>Abstract: </strong>Backdoor attacks present a serious security threat to deep neuron networks (DNNs). Although numerous effective defense techniques have been proposed in recent years, they inevitably rely on the availability of either clean or poisoned data. In contrast, data-free defense techniques have evolved slowly and still lag significantly in performance. To address this issue, different from the traditional approach of pruning followed by fine-tuning, we propose a novel data-free defense method named Optimal Transport-based Backdoor Repairing (OTBR) in this work. This method, based on our findings on neuron weight changes (NWCs) of random unlearning, uses optimal transport (OT)-based model fusion to combine the advantages of both pruned and backdoored models. Specifically, we first demonstrate our findings that the NWCs of random unlearning are positively correlated with those of poison unlearning. Based on this observation, we propose a random-unlearning NWC pruning technique to eliminate the backdoor effect and obtain a backdoor-free pruned model. Then, motivated by the OT-based model fusion, we propose the pruned-to-backdoored OT-based fusion technique, which fuses pruned and backdoored models to combine the advantages of both, resulting in a model that demonstrates high clean accuracy and a low attack success rate. To our knowledge, this is the first work to apply OT and model fusion techniques to backdoor defense. Extensive experiments show that our method successfully defends against all seven backdoor attacks across three benchmark datasets, outperforming both state-of-the-art (SOTA) data-free and data-dependent methods. The code implementation and Appendix are provided in the Supplementary Material.</li>
</ul>

<h3>Title: GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Yongjie Fu, Yunlong Li, Xuan Di</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15868">https://arxiv.org/abs/2408.15868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15868">https://arxiv.org/pdf/2408.15868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15868]] GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model(https://arxiv.org/abs/2408.15868)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autonomous driving training requires a diverse range of datasets encompassing various traffic conditions, weather scenarios, and road types. Traditional data augmentation methods often struggle to generate datasets that represent rare occurrences. To address this challenge, we propose GenDDS, a novel approach for generating driving scenarios generation by leveraging the capabilities of Stable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology involves the use of descriptive prompts to guide the synthesis process, aimed at producing realistic and diverse driving scenarios. With the power of the latest computer vision techniques, such as ControlNet and Hotshot-XL, we have built a complete pipeline for video generation together with SDXL. We employ the KITTI dataset, which includes real-world driving videos, to train the model. Through a series of experiments, we demonstrate that our model can generate high-quality driving videos that closely replicate the complexity and variability of real-world driving scenarios. This research contributes to the development of sophisticated training data for autonomous driving systems and opens new avenues for creating virtual environments for simulation and validation purposes.</li>
</ul>

<h3>Title: Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)</h3>
<ul>
<li><strong>Authors: </strong>Philipp Röchner, Henrique O. Marques, Ricardo J. G. B. Campello, Arthur Zimek, Franz Rothlauf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15874">https://arxiv.org/abs/2408.15874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15874">https://arxiv.org/pdf/2408.15874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15874]] Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)(https://arxiv.org/abs/2408.15874)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Outlier detection algorithms typically assign an outlier score to each observation in a dataset, indicating the degree to which an observation is an outlier. However, these scores are often not comparable across algorithms and can be difficult for humans to interpret. Statistical scaling addresses this problem by transforming outlier scores into outlier probabilities without using ground-truth labels, thereby improving interpretability and comparability across algorithms. However, the quality of this transformation can be different for outliers and inliers. Missing outliers in scenarios where they are of particular interest - such as healthcare, finance, or engineering - can be costly or dangerous. Thus, ensuring good probabilities for outliers is essential. This paper argues that statistical scaling, as commonly used in the literature, does not produce equally good probabilities for outliers as for inliers. Therefore, we propose robust statistical scaling, which uses robust estimators to improve the probabilities for outliers. We evaluate several variants of our method against other outlier score transformations for real-world datasets and outlier detection algorithms, where it can improve the probabilities for outliers.</li>
</ul>

<h3>Title: Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15876">https://arxiv.org/abs/2408.15876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15876">https://arxiv.org/pdf/2408.15876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15876]] Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation(https://arxiv.org/abs/2408.15876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2) pipeline to explore the training-free paradigm for audio and language-referenced video object segmentation, namely AVS and RVOS tasks. The intuitive solution leverages GroundingDINO to identify the target object from a single frame and SAM 2 to segment the identified object throughout the video, which is less robust to spatiotemporal variations due to a lack of video context exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt. Within GPT-PS, two task-specific Chain-of-Thought prompts are designed to unleash GPT's temporal-spatial reasoning capacity by guiding GPT to make selections based on a comprehensive understanding of video and reference information. Furthermore, we propose a Language-Binded Reference Unification (LBRU) module to convert audio signals into language-formatted references, thereby unifying the formats of AVS and RVOS tasks in the same pipeline. Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2 pipeline achieves performances comparable to or even better than fully-supervised fine-tuning methods. The code is available at: this https URL.</li>
</ul>

<h3>Title: Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Amar Amouri, Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Ismail Butun, Imad Mahgoub</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15886">https://arxiv.org/abs/2408.15886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15886">https://arxiv.org/pdf/2408.15886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15886]] Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks(https://arxiv.org/abs/2408.15886)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, interpretability</a></li>
<li><strong>Abstract: </strong>In recent years, the evolution of machine learning techniques has significantly impacted the field of intrusion detection, particularly within the context of the Internet of Things (IoT). As IoT networks expand, the need for robust security measures to counteract potential threats has become increasingly critical. This paper introduces a hybrid Intrusion Detection System (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs) with the XGBoost algorithm. Our proposed IDS leverages the unique capabilities of KANs, which utilize learnable activation functions to model complex relationships within data, alongside the powerful ensemble learning techniques of XGBoost, known for its high performance in classification tasks. This hybrid approach not only enhances the detection accuracy but also improves the interpretability of the model, making it suitable for dynamic and intricate IoT environments. Experimental evaluations demonstrate that our hybrid IDS achieves an impressive detection accuracy exceeding 99% in distinguishing between benign and malicious activities. Additionally, we were able to achieve F1 scores, precision, and recall that exceeded 98%. Furthermore, we conduct a comparative analysis against traditional Multi-Layer Perceptron (MLP) networks, assessing performance metrics such as Precision, Recall, and F1-score. The results underscore the efficacy of integrating KANs with XGBoost, highlighting the potential of this innovative approach to significantly strengthen the security framework of IoT networks.</li>
</ul>

<h3>Title: Disentangled Diffusion Autoencoder for Harmonization of Multi-site Neuroimaging Data</h3>
<ul>
<li><strong>Authors: </strong>Ayodeji Ijishakin, Ana Lawry Aguila, Elizabeth Levitis, Ahmed Abdulaal, Andre Altmann, James Cole</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15890">https://arxiv.org/abs/2408.15890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15890">https://arxiv.org/pdf/2408.15890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15890]] Disentangled Diffusion Autoencoder for Harmonization of Multi-site Neuroimaging Data(https://arxiv.org/abs/2408.15890)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Combining neuroimaging datasets from multiple sites and scanners can help increase statistical power and thus provide greater insight into subtle neuroanatomical effects. However, site-specific effects pose a challenge by potentially obscuring the biological signal and introducing unwanted variance. Existing harmonization techniques, which use statistical models to remove such effects, have been shown to incompletely remove site effects while also failing to preserve biological variability. More recently, generative models using GANs or autoencoder-based approaches, have been proposed for site adjustment. However, such methods are known for instability during training or blurry image generation. In recent years, diffusion models have become increasingly popular for their ability to generate high-quality synthetic images. In this work, we introduce the disentangled diffusion autoencoder (DDAE), a novel diffusion model designed for controlling specific aspects of an image. We apply the DDAE to the task of harmonizing MR images by generating high-quality site-adjusted images that preserve biological variability. We use data from 7 different sites and demonstrate the DDAE's superiority in generating high-resolution, harmonized 2D MR images over previous approaches. As far as we are aware, this work marks the first diffusion-based model for site adjustment of neuroimaging data.</li>
</ul>

<h3>Title: Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Vallejo Vera, Hunter Driggers</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15895">https://arxiv.org/abs/2408.15895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15895">https://arxiv.org/pdf/2408.15895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15895]] Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models(https://arxiv.org/abs/2408.15895)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human coders are biased. We test similar biases in Large Language Models (LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to judge political statements. Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained. We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties. The implications of our findings are discussed in the conclusion.</li>
</ul>

<h3>Title: Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation</h3>
<ul>
<li><strong>Authors: </strong>Reid Graves, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15898">https://arxiv.org/abs/2408.15898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15898">https://arxiv.org/pdf/2408.15898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15898]] Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation(https://arxiv.org/abs/2408.15898)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The design of aerodynamic shapes, such as airfoils, has traditionally required significant computational resources and relied on predefined design parameters, which limit the potential for novel shape synthesis. In this work, we introduce a data-driven methodology for airfoil generation using a diffusion model. Trained on a dataset of preexisting airfoils, our model can generate an arbitrary number of new airfoils from random vectors, which can be conditioned on specific aerodynamic performance metrics such as lift and drag, or geometric criteria. Our results demonstrate that the diffusion model effectively produces airfoil shapes with realistic aerodynamic properties, offering substantial improvements in efficiency, flexibility, and the potential for discovering innovative airfoil designs. This approach significantly expands the design space, facilitating the synthesis of high-performance aerodynamic shapes that transcend the limitations of traditional methods.</li>
</ul>

<h3>Title: Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Nikolas Gritsch, Qizhen Zhang, Acyr Locatelli, Sara Hooker, Ahmet Üstün</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15901">https://arxiv.org/abs/2408.15901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15901">https://arxiv.org/pdf/2408.15901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15901]] Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts(https://arxiv.org/abs/2408.15901)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on "upcycling" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.</li>
</ul>

<h3>Title: LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments</h3>
<ul>
<li><strong>Authors: </strong>Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Singh Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, Bo Ai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15903">https://arxiv.org/abs/2408.15903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15903">https://arxiv.org/pdf/2408.15903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15903]] LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments(https://arxiv.org/abs/2408.15903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits.</li>
</ul>

<h3>Title: MetaGFN: Exploring Distant Modes with Adapted Metadynamics for Continuous GFlowNets</h3>
<ul>
<li><strong>Authors: </strong>Dominic Phillips, Flaviu Cipcigan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15905">https://arxiv.org/abs/2408.15905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15905">https://arxiv.org/pdf/2408.15905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15905]] MetaGFN: Exploring Distant Modes with Adapted Metadynamics for Continuous GFlowNets(https://arxiv.org/abs/2408.15905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) are a class of generative models that sample objects in proportion to a specified reward function through a learned policy. They can be trained either on-policy or off-policy, needing a balance between exploration and exploitation for fast convergence to a target distribution. While exploration strategies for discrete GFlowNets have been studied, exploration in the continuous case remains to be investigated, despite the potential for novel exploration algorithms due to the local connectedness of continuous domains. Here, we introduce Adapted Metadynamics, a variant of metadynamics that can be applied to arbitrary black-box reward functions on continuous domains. We use Adapted Metadynamics as an exploration strategy for continuous GFlowNets. We show three continuous domains where the resulting algorithm, MetaGFN, accelerates convergence to the target distribution and discovers more distant reward modes than previous off-policy exploration strategies used for GFlowNets.</li>
</ul>

<h3>Title: Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuncheng Yang, Yulei Qin, Tong Wu, Zihan Xu, Gang Li, Pengcheng Guo, Hang Shao, Yucheng Shi, Ke Li, Xing Sun, Jie Yang, Yun Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15915">https://arxiv.org/abs/2408.15915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15915">https://arxiv.org/pdf/2408.15915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15915]] Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models(https://arxiv.org/abs/2408.15915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Codes and models will be released later.</li>
</ul>

<h3>Title: DiffAge3D: Diffusion-based 3D-aware Face Aging</h3>
<ul>
<li><strong>Authors: </strong>Junaid Wahid, Fangneng Zhan, Pramod Rao, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15922">https://arxiv.org/abs/2408.15922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15922">https://arxiv.org/pdf/2408.15922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15922]] DiffAge3D: Diffusion-based 3D-aware Face Aging(https://arxiv.org/abs/2408.15922)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Face aging is the process of converting an individual's appearance to a younger or older version of themselves. Existing face aging techniques have been limited to 2D settings, which often weaken their applications as there is a growing demand for 3D face modeling. Moreover, existing aging methods struggle to perform faithful aging, maintain identity, and retain the fine details of the input images. Given these limitations and the need for a 3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging framework that not only performs faithful aging and identity preservation but also operates in a 3D setting. Our aging framework allows to model the aging and camera pose separately by only taking a single image with a target age. Our framework includes a robust 3D-aware aging dataset generation pipeline by utilizing a pre-trained 3D GAN and the rich text embedding capabilities within CLIP model. Notably, we do not employ any inversion bottleneck in dataset generation. Instead, we randomly generate training samples from the latent space of 3D GAN, allowing us to manipulate the rich latent space of GAN to generate ages even with large gaps. With the generated dataset, we train a viewpoint-aware diffusion-based aging model to control the camera pose and facial age. Through quantitative and qualitative evaluations, we demonstrate that DiffAge3D outperforms existing methods, particularly in multiview-consistent aging and fine details preservation.</li>
</ul>

<h3>Title: InstanSeg: an embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation</h3>
<ul>
<li><strong>Authors: </strong>Thibaut Goldsborough, Ben Philps, Alan O'Callaghan, Fiona Inglis, Leo Leplat, Andrew Filby, Hakan Bilen, Peter Bankhead</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15954">https://arxiv.org/abs/2408.15954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15954">https://arxiv.org/pdf/2408.15954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15954]] InstanSeg: an embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation(https://arxiv.org/abs/2408.15954)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cell and nucleus segmentation are fundamental tasks for quantitative bioimage analysis. Despite progress in recent years, biologists and other domain experts still require novel algorithms to handle increasingly large and complex real-world datasets. These algorithms must not only achieve state-of-the-art accuracy, but also be optimized for efficiency, portability and user-friendliness. Here, we introduce InstanSeg: a novel embedding-based instance segmentation pipeline designed to identify cells and nuclei in microscopy images. Using six public cell segmentation datasets, we demonstrate that InstanSeg can significantly improve accuracy when compared to the most widely used alternative methods, while reducing the processing time by at least 60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript and supports GPU acceleration on a range of hardware. We provide an open-source implementation of InstanSeg in Python, in addition to a user-friendly, interactive QuPath extension for inference written in Java. Our code and pre-trained models are available at this https URL .</li>
</ul>

<h3>Title: Fall Detection for Smart Living using YOLOv5</h3>
<ul>
<li><strong>Authors: </strong>Gracile Astlin Pereira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15955">https://arxiv.org/abs/2408.15955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15955">https://arxiv.org/pdf/2408.15955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15955]] Fall Detection for Smart Living using YOLOv5(https://arxiv.org/abs/2408.15955)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work introduces a fall detection system using the YOLOv5mu model, which achieved a mean average precision (mAP) of 0.995, demonstrating exceptional accuracy in identifying fall events within smart home environments. Enhanced by advanced data augmentation techniques, the model demonstrates significant robustness and adaptability across various conditions. The integration of YOLOv5mu offers precise, real-time fall detection, which is crucial for improving safety and emergency response for residents. Future research will focus on refining the system by incorporating contextual data and exploring multi-sensor approaches to enhance its performance and practical applicability in diverse environments.</li>
</ul>

<h3>Title: More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Jinfeng Xu, Yixue Hao, Long Hu, Min Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15966">https://arxiv.org/abs/2408.15966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15966">https://arxiv.org/pdf/2408.15966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15966]] More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding(https://arxiv.org/abs/2408.15966)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: this https URL.</li>
</ul>

<h3>Title: Ain't How You Deploy: An Analysis of BGP Security Policies Performance Against Various Attack Scenarios with Differing Deployment Strategies</h3>
<ul>
<li><strong>Authors: </strong>Seth Barrett, Calvin Idom, German Zavala Villafuerte, Andrew Byers, Berk Gulmezoglu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15970">https://arxiv.org/abs/2408.15970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15970">https://arxiv.org/pdf/2408.15970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15970]] Ain't How You Deploy: An Analysis of BGP Security Policies Performance Against Various Attack Scenarios with Differing Deployment Strategies(https://arxiv.org/abs/2408.15970)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper investigates the performance of various Border Gateway Protocol (BGP) security policies against multiple attack scenarios using different deployment strategies. Through extensive simulations, we evaluate the effectiveness of defensive mechanisms such as Root Origin Validation (ROV), Autonomous System Provider Authorization (ASPA), and PeerROV across distinct AS deployment types. Our findings reveal critical insights into the strengths and limitations of current BGP security measures, providing guidance for future policy development and implementation.</li>
</ul>

<h3>Title: BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Wei Wang, Dan Zhang, Tao Feng, Boyan Wang, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15971">https://arxiv.org/abs/2408.15971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15971">https://arxiv.org/pdf/2408.15971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15971]] BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems(https://arxiv.org/abs/2408.15971)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, e.g., building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the collaboration capabilities of language models. Many benchmarks are proposed to evaluate their collaborative abilities. However, these benchmarks lack fine-grained evaluations of LLM collaborative capabilities. Additionally, multi-agent collaborative and competitive scenarios are ignored in existing works. To address these two problems, we propose a benchmark, called BattleAgentBench, which defines seven sub-stages of three varying difficulty levels and conducts a fine-grained evaluation of language models in terms of single-agent scenario navigation capabilities, paired-agent task execution abilities, and multi-agent collaboration and competition capabilities. We conducted extensive evaluations on leading four closed-source and seven open-source models. Experimental results indicate that API-based models perform excellently on simple tasks but open-source small models struggle with simple tasks. Regarding difficult tasks that require collaborative and competitive abilities, although API-based models have demonstrated some collaborative capabilities, there is still enormous room for improvement.</li>
</ul>

<h3>Title: Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15991">https://arxiv.org/abs/2408.15991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15991">https://arxiv.org/pdf/2408.15991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15991]] Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation(https://arxiv.org/abs/2408.15991)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accelerating the sampling speed of diffusion models remains a significant challenge. Recent score distillation methods distill a heavy teacher model into an one-step student generator, which is optimized by calculating the difference between the two score functions on the samples generated by the student model. However, there is a score mismatch issue in the early stage of the distillation process, because existing methods mainly focus on using the endpoint of pre-trained diffusion models as teacher models, overlooking the importance of the convergence trajectory between the student generator and the teacher model. To address this issue, we extend the score distillation process by introducing the entire convergence trajectory of teacher models and propose Distribution Backtracking Distillation (DisBack) for distilling student generators. DisBask is composed of two stages: Degradation Recording and Distribution Backtracking. Degradation Recording is designed to obtain the convergence trajectory of teacher models, which records the degradation path from the trained teacher model to the untrained initial student generator. The degradation path implicitly represents the intermediate distributions of teacher models. Then Distribution Backtracking trains a student generator to backtrack the intermediate distributions for approximating the convergence trajectory of teacher models. Extensive experiments show that DisBack achieves faster and better convergence than the existing distillation method and accomplishes comparable generation performance. Notably, DisBack is easy to implement and can be generalized to existing distillation methods to boost performance. Our code is publicly available on this https URL.</li>
</ul>

<h3>Title: ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution</h3>
<ul>
<li><strong>Authors: </strong>Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck, Matthew Lyle Olson, Tung Nguyen, Vasudev Lal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15993">https://arxiv.org/abs/2408.15993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15993">https://arxiv.org/pdf/2408.15993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15993]] ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution(https://arxiv.org/abs/2408.15993)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Detecting and attributing temperature increases due to climate change is crucial for understanding global warming and guiding adaptation strategies. The complexity of distinguishing human-induced climate signals from natural variability has challenged traditional detection and attribution (D&A) approaches, which seek to identify specific "fingerprints" in climate response variables. Deep learning offers potential for discerning these complex patterns in expansive spatial datasets. However, lack of standard protocols has hindered consistent comparisons across studies. We introduce ClimDetect, a standardized dataset of over 816k daily climate snapshots, designed to enhance model accuracy in identifying climate change signals. ClimDetect integrates various input and target variables used in past research, ensuring comparability and consistency. We also explore the application of vision transformers (ViT) to climate data, a novel and modernizing approach in this context. Our open-access data and code serve as a benchmark for advancing climate science through improved model evaluations. ClimDetect is publicly accessible via Huggingface dataet respository at: this https URL.</li>
</ul>

<h3>Title: Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Jiaqi Ma, Guoli Wang, Qian Zhang, Huan Zhang, Lefei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15994">https://arxiv.org/abs/2408.15994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15994">https://arxiv.org/pdf/2408.15994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15994]] Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration(https://arxiv.org/abs/2408.15994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The limitations of task-specific and general image restoration methods for specific degradation have prompted the development of all-in-one image restoration techniques. However, the diversity of patterns among multiple degradation, along with the significant uncertainties in mapping between degraded images of different severities and their corresponding undistorted versions, pose significant challenges to the all-in-one restoration tasks. To address these challenges, we propose Perceive-IR, an all-in-one image restorer designed to achieve fine-grained quality control that enables restored images to more closely resemble their undistorted counterparts, regardless of the type or severity of degradation. Specifically, Perceive-IR contains two stages: (1) prompt learning stage and (2) restoration stage. In the prompt learning stage, we leverage prompt learning to acquire a fine-grained quality perceiver capable of distinguishing three-tier quality levels by constraining the prompt-image similarity in the CLIP perception space. Subsequently, this quality perceiver and difficulty-adaptive perceptual loss are integrated as a quality-aware learning strategy to realize fine-grained quality control in restoration stage. For the restoration stage, a semantic guidance module (SGM) and compact feature extraction (CFE) are proposed to further promote the restoration process by utilizing the robust semantic information from the pre-trained large scale vision models and distinguishing degradation-specific features. Extensive experiments demonstrate that our Perceive-IR outperforms state-of-the-art methods in all-in-one image restoration tasks and exhibit superior generalization ability when dealing with unseen tasks.</li>
</ul>

<h3>Title: TEDRA: Text-based Editing of Dynamic and Photoreal Actors</h3>
<ul>
<li><strong>Authors: </strong>Basavaraj Sunagad, Heming Zhu, Mohit Mendiratta, Adam Kortylewski, Christian Theobalt, Marc Habermann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15995">https://arxiv.org/abs/2408.15995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15995">https://arxiv.org/pdf/2408.15995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15995]] TEDRA: Text-based Editing of Dynamic and Photoreal Actors(https://arxiv.org/abs/2408.15995)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Over the past years, significant progress has been made in creating photorealistic and drivable 3D avatars solely from videos of real humans. However, a core remaining challenge is the fine-grained and user-friendly editing of clothing styles by means of textual descriptions. To this end, we present TEDRA, the first method allowing text-based edits of an avatar, which maintains the avatar's high fidelity, space-time coherency, as well as dynamics, and enables skeletal pose and view control. We begin by training a model to create a controllable and high-fidelity digital replica of the real actor. Next, we personalize a pretrained generative diffusion model by fine-tuning it on various frames of the real character captured from different camera angles, ensuring the digital representation faithfully captures the dynamics and movements of the real person. This two-stage process lays the foundation for our approach to dynamic human avatar editing. Utilizing this personalized diffusion model, we modify the dynamic avatar based on a provided text prompt using our Personalized Normal Aligned Score Distillation Sampling (PNA-SDS) within a model-based guidance framework. Additionally, we propose a time step annealing strategy to ensure high-quality edits. Our results demonstrate a clear improvement over prior work in functionality and visual quality.</li>
</ul>

<h3>Title: Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Sijia Peng, Yun Xiong, Yangyong Zhu, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15997">https://arxiv.org/abs/2408.15997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15997">https://arxiv.org/pdf/2408.15997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15997]] Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need(https://arxiv.org/abs/2408.15997)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting requires balancing short-term and long-term dependencies for accurate predictions. Existing methods mainly focus on long-term dependency modeling, neglecting the complexities of short-term dynamics, which may hinder performance. Transformers are superior in modeling long-term dependencies but are criticized for their quadratic computational cost. Mamba provides a near-linear alternative but is reported less effective in time series longterm forecasting due to potential information loss. Current architectures fall short in offering both high efficiency and strong performance for long-term dependency modeling. To address these challenges, we introduce Mixture of Universals (MoU), a versatile model to capture both short-term and long-term dependencies for enhancing performance in time series forecasting. MoU is composed of two novel designs: Mixture of Feature Extractors (MoF), an adaptive method designed to improve time series patch representations for short-term dependency, and Mixture of Architectures (MoA), which hierarchically integrates Mamba, FeedForward, Convolution, and Self-Attention architectures in a specialized order to model long-term dependency from a hybrid perspective. The proposed approach achieves state-of-the-art performance while maintaining relatively low computational costs. Extensive experiments on seven real-world datasets demonstrate the superiority of MoU. Code is available at this https URL.</li>
</ul>

<h3>Title: Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders</h3>
<ul>
<li><strong>Authors: </strong>Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.15998">https://arxiv.org/abs/2408.15998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.15998">https://arxiv.org/pdf/2408.15998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.15998]] Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders(https://arxiv.org/abs/2408.15998)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
