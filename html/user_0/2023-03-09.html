<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Multi-block MEV. (arXiv:2303.04430v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04430">http://arxiv.org/abs/2303.04430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04430] Multi-block MEV](http://arxiv.org/abs/2303.04430) #secure</code></li>
<li>Summary: <p>Multi-block MEV denotes the practice of securing k-consecutive blocks in the
attempt at extracting surplus value by manipulating transaction ordering.
Following the implementation of proposer/builder separation (PBS) on Ethereum,
savvy builders can secure consecutive block space by implementing targeted
bidding strategies through relays. We collect data on all bids submitted by
builders through relays, finding that 20.60% of blocks issued since the Merge
share single builder entities for consecutive sequences. Our results indicate
that single builder entities are implementing super linear bidding strategies
above market, to secure consecutive block space. By comparing the results to a
simulation of naive builder behavior, we show that MMEV extraction may be
common amongst leading builder entities.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Graph Neural Networks Enhanced Smart Contract Vulnerability Detection of Educational Blockchain. (arXiv:2303.04477v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04477">http://arxiv.org/abs/2303.04477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04477] Graph Neural Networks Enhanced Smart Contract Vulnerability Detection of Educational Blockchain](http://arxiv.org/abs/2303.04477) #security</code></li>
<li>Summary: <p>With the development of blockchain technology, more and more attention has
been paid to the intersection of blockchain and education, and various
educational evaluation systems and E-learning systems are developed based on
blockchain technology. Among them, Ethereum smart contract is favored by
developers for its ``event-triggered" mechanism for building education
intelligent trading systems and intelligent learning platforms. However, due to
the immutability of blockchain, published smart contracts cannot be modified,
so problematic contracts cannot be fixed by modifying the code in the
educational blockchain. In recent years, security incidents due to smart
contract vulnerabilities have caused huge property losses, so the detection of
smart contract vulnerabilities in educational blockchain has become a great
challenge. To solve this problem, this paper proposes a graph neural network
(GNN) based vulnerability detection for smart contracts in educational
blockchains. Firstly, the bytecodes are decompiled to get the opcode. Secondly,
the basic blocks are divided, and the edges between the basic blocks according
to the opcode execution logic are added. Then, the control flow graphs (CFG)
are built. Finally, we designed a GNN-based model for vulnerability detection.
The experimental results show that the proposed method is effective for the
vulnerability detection of smart contracts. Compared with the traditional
approaches, it can get good results with fewer layers of the GCN model, which
shows that the contract bytecode and GCN model are efficient in vulnerability
detection.
</p></li>
</ul>

<h3>Title: Automatic verification of transparency protocols (extended version). (arXiv:2303.04500v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04500">http://arxiv.org/abs/2303.04500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04500] Automatic verification of transparency protocols (extended version)](http://arxiv.org/abs/2303.04500) #security</code></li>
<li>Summary: <p>We introduce new features in ProVerif, an automatic tool for verifying
security protocols, and a methodology for using them. This methodology and
these features are aimed at protocols which involve sophisticated data types
that have strong properties, such as Merkle trees, which allow compact proofs
of data presence and tree extension. Such data types are widely used in
protocols in systems that use distributed ledgers and/or blockchains.
</p></li>
</ul>

<p>With our methodology, it is possible to describe the data type quite
abstractly, using ProVerif axioms, and prove the correctness of the protocol
using those axioms as assumptions. Then, in separate steps, one can define one
or more concrete implementations of the data type, and again use ProVerif to
show that the implementations satisfy the assumptions that were coded as
axioms. This helps make compositional proofs, splitting the proof burden into
several manageable pieces.
</p>
<p>To enable this methodology, we introduce new capabilities in ProVerif, by
extending the class of lemmas and axioms that it can reason with. Specifically,
we allow user-defined predicates, attacker predicates and message predicates to
appear in lemmas and axioms. We show the soundness of the implementation of
this idea with respect to the semantics.
</p>
<p>We illustrate the methodology and features by providing the first formal
verification of two transparency protocols which precisely models the Merkle
tree data structure. The two protocols are transparent decryption and
certificate transparency. Transparent decryption is a way of ensuring that
decryption operations are visible by people who are affected by them. This can
be used to support privacy: it can mean that a subject is alerted to the fact
that information about them has been decrypted. Certificate transparency is an
Internet security standard for monitoring and auditing the issuance of digital
certificates.
</p>

<h3>Title: Keystroke Dynamics: Concepts, Techniques, and Applications. (arXiv:2303.04605v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04605">http://arxiv.org/abs/2303.04605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04605] Keystroke Dynamics: Concepts, Techniques, and Applications](http://arxiv.org/abs/2303.04605) #security</code></li>
<li>Summary: <p>Reliably identifying and authenticating users remains integral to computer
system security. Various novel authentication tenchniques such as biometric
authentication systems have been devised in recent years. This paper surveys
keystroke-based authentication systems and their applications such as
continuous authentication. Keystroke dynamics promises to be non-intrusive and
cost-effective as no addition hardware is required other than a keyboard. This
survey can be a reference for researchers working on keystroke dynamics.
</p></li>
</ul>

<h3>Title: Arion: Arithmetization-Oriented Permutation and Hashing from Generalized Triangular Dynamical Systems. (arXiv:2303.04639v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04639">http://arxiv.org/abs/2303.04639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04639] Arion: Arithmetization-Oriented Permutation and Hashing from Generalized Triangular Dynamical Systems](http://arxiv.org/abs/2303.04639) #security</code></li>
<li>Summary: <p>In this paper we propose the (keyed) permutation Arion and the hash function
ArionHash over $\mathbb{F}_p$ for odd and particularly large primes. The design
of Arion is based on the newly introduced Generalized Triangular Dynamical
System (GTDS), which provides a new algebraic framework for constructing
(keyed) permutation using polynomials over a finite field. At round level Arion
is the first design which is instantiated using the new GTDS. We provide
extensive security analysis of our construction including algebraic
cryptanalysis (e.g. interpolation and Groebner basis attacks) that are
particularly decisive in assessing the security of permutations and hash
functions over $\mathbb{F}_p$. From a application perspective, ArionHash is
aimed for efficient implementation in zkSNARK protocols and Zero-Knowledge
proof systems. For this purpose, we exploit that CCZ-equivalence of graphs can
lead to a more efficient implementation of Arithmetization-Oriented primitives.
We compare the efficiency of ArionHash in R1CS and Plonk settings with other
hash functions such as Poseidon, Anemoi and Griffin. For demonstrating the
practical efficiency of ArionHash we implemented it with the zkSNARK libraries
libsnark and Dusk Network Plonk. Our result shows that ArionHash is
significantly faster than Poseidon - a hash function designed for
zero-knowledge proof systems. We also found that an aggressive version of
ArionHash is considerably faster than Anemoi and Griffin in a practical zkSNARK
setting.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: CUDA: Convolution-based Unlearnable Datasets. (arXiv:2303.04278v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04278">http://arxiv.org/abs/2303.04278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04278] CUDA: Convolution-based Unlearnable Datasets](http://arxiv.org/abs/2303.04278) #privacy</code></li>
<li>Summary: <p>Large-scale training of modern deep learning models heavily relies on
publicly available data on the web. This potentially unauthorized usage of
online data leads to concerns regarding data privacy. Recent works aim to make
unlearnable data for deep learning models by adding small, specially designed
noises to tackle this issue. However, these methods are vulnerable to
adversarial training (AT) and/or are computationally heavy. In this work, we
propose a novel, model-free, Convolution-based Unlearnable DAtaset (CUDA)
generation technique. CUDA is generated using controlled class-wise
convolutions with filters that are randomly generated via a private key. CUDA
encourages the network to learn the relation between filters and labels rather
than informative features for classifying the clean data. We develop some
theoretical analysis demonstrating that CUDA can successfully poison Gaussian
mixture data by reducing the clean data performance of the optimal Bayes
classifier. We also empirically demonstrate the effectiveness of CUDA with
various datasets (CIFAR-10, CIFAR-100, ImageNet-100, and Tiny-ImageNet), and
architectures (ResNet-18, VGG-16, Wide ResNet-34-10, DenseNet-121, DeIT,
EfficientNetV2-S, and MobileNetV2). Our experiments show that CUDA is robust to
various data augmentations and training approaches such as smoothing, AT with
different budgets, transfer learning, and fine-tuning. For instance, training a
ResNet-18 on ImageNet-100 CUDA achieves only 8.96$\%$, 40.08$\%$, and 20.58$\%$
clean test accuracies with empirical risk minimization (ERM), $L_{\infty}$ AT,
and $L_{2}$ AT, respectively. Here, ERM on the clean training data achieves a
clean test accuracy of 80.66$\%$. CUDA exhibits unlearnability effect with ERM
even when only a fraction of the training dataset is perturbed. Furthermore, we
also show that CUDA is robust to adaptive defenses designed specifically to
break it.
</p></li>
</ul>

<h3>Title: Privacy-preserving and Uncertainty-aware Federated Trajectory Prediction for Connected Autonomous Vehicles. (arXiv:2303.04340v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04340">http://arxiv.org/abs/2303.04340</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04340] Privacy-preserving and Uncertainty-aware Federated Trajectory Prediction for Connected Autonomous Vehicles](http://arxiv.org/abs/2303.04340) #privacy</code></li>
<li>Summary: <p>Deep learning is the method of choice for trajectory prediction for
autonomous vehicles. Unfortunately, its data-hungry nature implicitly requires
the availability of sufficiently rich and high-quality centralized datasets,
which easily leads to privacy leakage. Besides, uncertainty-awareness becomes
increasingly important for safety-crucial cyber physical systems whose
prediction module heavily relies on machine learning tools. In this paper, we
relax the data collection requirement and enhance uncertainty-awareness by
using Federated Learning on Connected Autonomous Vehicles with an
uncertainty-aware global objective. We name our algorithm as FLTP. We further
introduce ALFLTP which boosts FLTP via using active learning techniques in
adaptatively selecting participating clients. We consider both negative
log-likelihood (NLL) and aleatoric uncertainty (AU) as client selection
metrics. Experiments on Argoverse dataset show that FLTP significantly
outperforms the model trained on local data. In addition, ALFLTP-AU converges
faster in training regression loss and performs better in terms of NLL, minADE
and MR than FLTP in most rounds, and has more stable round-wise performance
than ALFLTP-NLL.
</p></li>
</ul>

<h3>Title: Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04360">http://arxiv.org/abs/2303.04360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04360] Does Synthetic Data Generation of LLMs Help Clinical Text Mining?](http://arxiv.org/abs/2303.04360) #privacy</code></li>
<li>Summary: <p>Recent advancements in large language models (LLMs) have led to the
development of highly potent models like OpenAI's ChatGPT. These models have
exhibited exceptional performance in a variety of tasks, such as question
answering, essay composition, and code generation. However, their effectiveness
in the healthcare sector remains uncertain. In this study, we seek to
investigate the potential of ChatGPT to aid in clinical text mining by
examining its ability to extract structured information from unstructured
healthcare texts, with a focus on biological named entity recognition and
relation extraction. However, our preliminary results indicate that employing
ChatGPT directly for these tasks resulted in poor performance and raised
privacy concerns associated with uploading patients' information to the ChatGPT
API. To overcome these limitations, we propose a new training paradigm that
involves generating a vast quantity of high-quality synthetic data with labels
utilizing ChatGPT and fine-tuning a local model for the downstream task. Our
method has resulted in significant improvements in the performance of
downstream tasks, improving the F1-score from 23.37% to 63.99% for the named
entity recognition task and from 75.86% to 83.59% for the relation extraction
task. Furthermore, generating data using ChatGPT can significantly reduce the
time and effort required for data collection and labeling, as well as mitigate
data privacy concerns. In summary, the proposed framework presents a promising
solution to enhance the applicability of LLM models to clinical text mining.
</p></li>
</ul>

<h3>Title: PRIMO: Private Regression in Multiple Outcomes. (arXiv:2303.04195v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04195">http://arxiv.org/abs/2303.04195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04195] PRIMO: Private Regression in Multiple Outcomes](http://arxiv.org/abs/2303.04195) #privacy</code></li>
<li>Summary: <p>We introduce a new differentially private regression setting we call Private
Regression in Multiple Outcomes (PRIMO), inspired the common situation where a
data analyst wants to perform a set of $l$ regressions while preserving
privacy, where the covariates $X$ are shared across all $l$ regressions, and
each regression $i \in [l]$ has a different vector of outcomes $y_i$. While
naively applying private linear regression techniques $l$ times leads to a
$\sqrt{l}$ multiplicative increase in error over the standard linear regression
setting, in Subsection $4.1$ we modify techniques based on sufficient
statistics perturbation (SSP) to yield greatly improved dependence on $l$. In
Subsection $4.2$ we prove an equivalence to the problem of privately releasing
the answers to a special class of low-sensitivity queries we call inner product
queries. Via this equivalence, we adapt the geometric projection-based methods
from prior work on private query release to the PRIMO setting. Under the
assumption the labels $Y$ are public, the projection gives improved results
over the Gaussian mechanism when $n < l\sqrt{d}$, with no asymptotic dependence
on $l$ in the error. In Subsection $4.3$ we study the complexity of our
projection algorithm, and analyze a faster sub-sampling based variant in
Subsection $4.4$. Finally in Section $5$ we apply our algorithms to the task of
private genomic risk prediction for multiple phenotypes using data from the
1000 Genomes project. We find that for moderately large values of $l$ our
techniques drastically improve the accuracy relative to both the naive baseline
that uses existing private regression methods and our modified SSP algorithm
that doesn't use the projection.
</p></li>
</ul>

<h3>Title: adaPARL: Adaptive Privacy-Aware Reinforcement Learning for Sequential-Decision Making Human-in-the-Loop Systems. (arXiv:2303.04257v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04257">http://arxiv.org/abs/2303.04257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04257] adaPARL: Adaptive Privacy-Aware Reinforcement Learning for Sequential-Decision Making Human-in-the-Loop Systems](http://arxiv.org/abs/2303.04257) #privacy</code></li>
<li>Summary: <p>Reinforcement learning (RL) presents numerous benefits compared to rule-based
approaches in various applications. Privacy concerns have grown with the
widespread use of RL trained with privacy-sensitive data in IoT devices,
especially for human-in-the-loop systems. On the one hand, RL methods enhance
the user experience by trying to adapt to the highly dynamic nature of humans.
On the other hand, trained policies can leak the user's private information.
Recent attention has been drawn to designing privacy-aware RL algorithms while
maintaining an acceptable system utility. A central challenge in designing
privacy-aware RL, especially for human-in-the-loop systems, is that humans have
intrinsic variability and their preferences and behavior evolve. The effect of
one privacy leak mitigation can be different for the same human or across
different humans over time. Hence, we can not design one fixed model for
privacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive
approach for privacy-aware RL, especially for human-in-the-loop IoT systems.
adaPARL provides a personalized privacy-utility trade-off depending on human
behavior and preference. We validate the proposed adaPARL on two IoT
applications, namely (i) Human-in-the-Loop Smart Home and (ii)
Human-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on
these two applications validate the generality of adaPARL and its ability to
provide a personalized privacy-utility trade-off. On average, for the first
application, adaPARL improves the utility by $57\%$ over the baseline and by
$43\%$ over randomization. adaPARL also reduces the privacy leak by $23\%$ on
average. For the second application, adaPARL decreases the privacy leak to
$44\%$ before the utility drops by $15\%$.
</p></li>
</ul>

<h3>Title: Amplitude-Varying Perturbation for Balancing Privacy and Utility in Federated Learning. (arXiv:2303.04274v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04274">http://arxiv.org/abs/2303.04274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04274] Amplitude-Varying Perturbation for Balancing Privacy and Utility in Federated Learning](http://arxiv.org/abs/2303.04274) #privacy</code></li>
<li>Summary: <p>While preserving the privacy of federated learning (FL), differential privacy
(DP) inevitably degrades the utility (i.e., accuracy) of FL due to model
perturbations caused by DP noise added to model updates. Existing studies have
considered exclusively noise with persistent root-mean-square amplitude and
overlooked an opportunity of adjusting the amplitudes to alleviate the adverse
effects of the noise. This paper presents a new DP perturbation mechanism with
a time-varying noise amplitude to protect the privacy of FL and retain the
capability of adjusting the learning performance. Specifically, we propose a
geometric series form for the noise amplitude and reveal analytically the
dependence of the series on the number of global aggregations and the
$(\epsilon,\delta)$-DP requirement. We derive an online refinement of the
series to prevent FL from premature convergence resulting from excessive
perturbation noise. Another important aspect is an upper bound developed for
the loss function of a multi-layer perceptron (MLP) trained by FL running the
new DP mechanism. Accordingly, the optimal number of global aggregations is
obtained, balancing the learning and privacy. Extensive experiments are
conducted using MLP, supporting vector machine, and convolutional neural
network models on four public datasets. The contribution of the new DP
mechanism to the convergence and accuracy of privacy-preserving FL is
corroborated, compared to the state-of-the-art Gaussian noise mechanism with a
persistent noise amplitude.
</p></li>
</ul>

<h3>Title: Differential Privacy Meets Neural Network Pruning. (arXiv:2303.04612v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04612">http://arxiv.org/abs/2303.04612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04612] Differential Privacy Meets Neural Network Pruning](http://arxiv.org/abs/2303.04612) #privacy</code></li>
<li>Summary: <p>A major challenge in applying differential privacy to training deep neural
network models is scalability.The widely-used training algorithm,
differentially private stochastic gradient descent (DP-SGD), struggles with
training moderately-sized neural network models for a value of epsilon
corresponding to a high level of privacy protection. In this paper, we explore
the idea of dimensionality reduction inspired by neural network pruning to
improve the scalability of DP-SGD. We study the interplay between neural
network pruning and differential privacy, through the two modes of parameter
updates. We call the first mode, parameter freezing, where we pre-prune the
network and only update the remaining parameters using DP-SGD. We call the
second mode, parameter selection, where we select which parameters to update at
each step of training and update only those selected using DP-SGD. In these
modes, we use public data for freezing or selecting parameters to avoid privacy
loss incurring in these steps. Naturally, the closeness between the private and
public data plays an important role in the success of this paradigm. Our
experimental results demonstrate how decreasing the parameter space improves
differentially private training. Moreover, by studying two popular forms of
pruning which do not rely on gradients and do not incur an additional privacy
loss, we show that random selection performs on par with magnitude-based
selection when it comes to DP-SGD training.
</p></li>
</ul>

<h3>Title: Considerations on the Theory of Training Models with Differential Privacy. (arXiv:2303.04676v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04676">http://arxiv.org/abs/2303.04676</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04676] Considerations on the Theory of Training Models with Differential Privacy](http://arxiv.org/abs/2303.04676) #privacy</code></li>
<li>Summary: <p>In federated learning collaborative learning takes place by a set of clients
who each want to remain in control of how their local training data is used, in
particular, how can each client's local training data remain private?
Differential privacy is one method to limit privacy leakage. We provide a
general overview of its framework and provable properties, adopt the more
recent hypothesis based definition called Gaussian DP or $f$-DP, and discuss
Differentially Private Stochastic Gradient Descent (DP-SGD). We stay at a meta
level and attempt intuitive explanations and insights \textit{in this book
chapter}.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Immune Defense: A Novel Adversarial Defense Mechanism for Preventing the Generation of Adversarial Examples. (arXiv:2303.04502v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04502">http://arxiv.org/abs/2303.04502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04502] Immune Defense: A Novel Adversarial Defense Mechanism for Preventing the Generation of Adversarial Examples](http://arxiv.org/abs/2303.04502) #defense</code></li>
<li>Summary: <p>The vulnerability of Deep Neural Networks (DNNs) to adversarial examples has
been confirmed. Existing adversarial defenses primarily aim at preventing
adversarial examples from attacking DNNs successfully, rather than preventing
their generation. If the generation of adversarial examples is unregulated,
images within reach are no longer secure and pose a threat to non-robust DNNs.
Although gradient obfuscation attempts to address this issue, it has been shown
to be circumventable. Therefore, we propose a novel adversarial defense
mechanism, which is referred to as immune defense and is the example-based
pre-defense. This mechanism applies carefully designed quasi-imperceptible
perturbations to the raw images to prevent the generation of adversarial
examples for the raw images, and thereby protecting both images and DNNs. These
perturbed images are referred to as Immune Examples (IEs). In the white-box
immune defense, we provide a gradient-based and an optimization-based approach,
respectively. Additionally, the more complex black-box immune defense is taken
into consideration. We propose Masked Gradient Sign Descent (MGSD) to reduce
approximation error and stabilize the update to improve the transferability of
IEs and thereby ensure their effectiveness against black-box adversarial
attacks. The experimental results demonstrate that the optimization-based
approach has superior performance and better visual quality in white-box immune
defense. In contrast, the gradient-based approach has stronger transferability
and the proposed MGSD significantly improve the transferability of baselines.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (arXiv:2303.04238v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04238">http://arxiv.org/abs/2303.04238</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04238] Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors](http://arxiv.org/abs/2303.04238) #attack</code></li>
<li>Summary: <p>Adversarial attacks on deep-learning models have been receiving increased
attention in recent years. Work in this area has mostly focused on
gradient-based techniques, so-called white-box attacks, wherein the attacker
has access to the targeted model's internal parameters; such an assumption is
usually unrealistic in the real world. Some attacks additionally use the entire
pixel space to fool a given model, which is neither practical nor physical
(i.e., real-world). On the contrary, we propose herein a gradient-free method
that uses the learned image manifold of a pretrained generative adversarial
network (GAN) to generate naturalistic physical adversarial patches for object
detectors. We show that our proposed method works both digitally and
physically.
</p></li>
</ul>

<h3>Title: SALSA PICANTE: a machine learning attack on LWE with binary secrets. (arXiv:2303.04178v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04178">http://arxiv.org/abs/2303.04178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04178] SALSA PICANTE: a machine learning attack on LWE with binary secrets](http://arxiv.org/abs/2303.04178) #attack</code></li>
<li>Summary: <p>The Learning With Errors (LWE) problem is one of the major hard problems in
post-quantum cryptography. For example, 1) the only Key Exchange Mechanism KEM
standardized by NIST [14] is based on LWE; and 2) current publicly available
Homomorphic Encryption (HE) libraries are based on LWE. NIST KEM schemes use
random secrets, but homomorphic encryption schemes use binary or ternary
secrets, for efficiency reasons. In particular, sparse binary secrets have been
proposed, but not standardized [2], for HE.
</p></li>
</ul>

<p>Prior work SALSA [49] demonstrated a new machine learning attack on sparse
binary secrets for the LWE problem in small dimensions (up to n = 128) and low
Hamming weights (up to h = 4). However, this attack assumed access to millions
of LWE samples, and was not scaled to higher Hamming weights or dimensions.
</p>
<p>Our attack, PICANTE, reduces the number of samples required to just m = 4n
samples. Moreover, it can recover secrets with much larger dimensions (up to
350) and Hamming weights (roughly n/10, or h = 33 for n = 300). To achieve
this, we introduce a preprocessing step which allows us to generate the
training data from a linear number of samples and changes the distribution of
the training data to improve transformer training. We also improve the
distinguisher/secret recovery methods of SALSA and introduce a novel
cross-attention recovery mechanism which allows us to read-off the secret
directly from the trained models.
</p>

<h2>robust</h2>
<h3>Title: Robustness-preserving Lifelong Learning via Dataset Condensation. (arXiv:2303.04183v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04183">http://arxiv.org/abs/2303.04183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04183] Robustness-preserving Lifelong Learning via Dataset Condensation](http://arxiv.org/abs/2303.04183) #robust</code></li>
<li>Summary: <p>Lifelong learning (LL) aims to improve a predictive model as the data source
evolves continuously. Most work in this learning paradigm has focused on
resolving the problem of 'catastrophic forgetting,' which refers to a notorious
dilemma between improving model accuracy over new data and retaining accuracy
over previous data. Yet, it is also known that machine learning (ML) models can
be vulnerable in the sense that tiny, adversarial input perturbations can
deceive the models into producing erroneous predictions. This motivates the
research objective of this paper - specification of a new LL framework that can
salvage model robustness (against adversarial attacks) from catastrophic
forgetting. Specifically, we propose a new memory-replay LL strategy that
leverages modern bi-level optimization techniques to determine the 'coreset' of
the current data (i.e., a small amount of data to be memorized) for ease of
preserving adversarial robustness over time. We term the resulting LL framework
'Data-Efficient Robustness-Preserving LL' (DERPLL). The effectiveness of DERPLL
is evaluated for class-incremental image classification using ResNet-18 over
the CIFAR-10 dataset. Experimental results show that DERPLL outperforms the
conventional coreset-guided LL baseline and achieves a substantial improvement
in both standard accuracy and robust accuracy.
</p></li>
</ul>

<h3>Title: A Computer Vision Enabled damage detection model with improved YOLOv5 based on Transformer Prediction Head. (arXiv:2303.04275v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04275">http://arxiv.org/abs/2303.04275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04275] A Computer Vision Enabled damage detection model with improved YOLOv5 based on Transformer Prediction Head](http://arxiv.org/abs/2303.04275) #robust</code></li>
<li>Summary: <p>Objective:Computer vision-based up-to-date accurate damage classification and
localization are of decisive importance for infrastructure monitoring, safety,
and the serviceability of civil infrastructure. Current state-of-the-art deep
learning (DL)-based damage detection models, however, often lack superior
feature extraction capability in complex and noisy environments, limiting the
development of accurate and reliable object distinction. Method: To this end,
we present DenseSPH-YOLOv5, a real-time DL-based high-performance damage
detection model where DenseNet blocks have been integrated with the backbone to
improve in preserving and reusing critical feature information. Additionally,
convolutional block attention modules (CBAM) have been implemented to improve
attention performance mechanisms for strong and discriminating deep spatial
feature extraction that results in superior detection under various challenging
environments. Moreover, additional feature fusion layers and a Swin-Transformer
Prediction Head (SPH) have been added leveraging advanced self-attention
mechanism for more efficient detection of multiscale object sizes and
simultaneously reducing the computational complexity. Results: Evaluating the
model performance in large-scale Road Damage Dataset (RDD-2018), at a detection
rate of 62.4 FPS, DenseSPH-YOLOv5 obtains a mean average precision (mAP) value
of 85.25 %, F1-score of 81.18 %, and precision (P) value of 89.51 %
outperforming current state-of-the-art models. Significance: The present
research provides an effective and efficient damage localization model
addressing the shortcoming of existing DL-based damage detection models by
providing highly accurate localized bounding box prediction. Current work
constitutes a step towards an accurate and robust automated damage detection
system in real-time in-field applications.
</p></li>
</ul>

<h3>Title: Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics. (arXiv:2303.04302v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04302">http://arxiv.org/abs/2303.04302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04302] Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics](http://arxiv.org/abs/2303.04302) #robust</code></li>
<li>Summary: <p>One of the main paths towards the reduction of traffic accidents is the
increase in vehicle safety through driver assistance systems or even systems
with a complete level of autonomy. In these types of systems, tasks such as
obstacle detection and segmentation, especially the Deep Learning-based ones,
play a fundamental role in scene understanding for correct and safe navigation.
Besides that, the wide variety of sensors in vehicles nowadays provides a rich
set of alternatives for improvement in the robustness of perception in
challenging situations, such as navigation under lighting and weather adverse
conditions. Despite the current focus given to the subject, the literature
lacks studies on radar-based and radar-camera fusion-based perception. Hence,
this work aims to carry out a study on the current scenario of camera and
radar-based perception for ADAS and autonomous vehicles. Concepts and
characteristics related to both sensors, as well as to their fusion, are
presented. Additionally, we give an overview of the Deep Learning-based
detection and segmentation tasks, and the main datasets, metrics, challenges,
and open questions in vehicle perception.
</p></li>
</ul>

<h3>Title: A Threefold Review on Deep Semantic Segmentation: Efficiency-oriented, Temporal and Depth-aware design. (arXiv:2303.04315v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04315">http://arxiv.org/abs/2303.04315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04315] A Threefold Review on Deep Semantic Segmentation: Efficiency-oriented, Temporal and Depth-aware design](http://arxiv.org/abs/2303.04315) #robust</code></li>
<li>Summary: <p>Semantic image and video segmentation stand among the most important tasks in
computer vision nowadays, since they provide a complete and meaningful
representation of the environment by means of a dense classification of the
pixels in a given scene. Recently, Deep Learning, and more precisely
Convolutional Neural Networks, have boosted semantic segmentation to a new
level in terms of performance and generalization capabilities. However,
designing Deep Semantic Segmentation models is a complex task, as it may
involve application-dependent aspects. Particularly, when considering
autonomous driving applications, the robustness-efficiency trade-off, as well
as intrinsic limitations - computational/memory bounds and data-scarcity - and
constraints - real-time inference - should be taken into consideration. In this
respect, the use of additional data modalities, such as depth perception for
reasoning on the geometry of a scene, and temporal cues from videos to explore
redundancy and consistency, are promising directions yet not explored to their
full potential in the literature. In this paper, we conduct a survey on the
most relevant and recent advances in Deep Semantic Segmentation in the context
of vision for autonomous vehicles, from three different perspectives:
efficiency-oriented model development for real-time operation, RGB-Depth data
integration (RGB-D semantic segmentation), and the use of temporal information
from videos in temporally-aware models. Our main objective is to provide a
comprehensive discussion on the main methods, advantages, limitations, results
and challenges faced from each perspective, so that the reader can not only get
started, but also be up to date in respect to recent advances in this exciting
and challenging research field.
</p></li>
</ul>

<h3>Title: SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking. (arXiv:2303.04378v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04378">http://arxiv.org/abs/2303.04378</a></li>
<li>Code URL: <a href="https://github.com/vision4robotics/sgdvit">https://github.com/vision4robotics/sgdvit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04378] SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking](http://arxiv.org/abs/2303.04378) #robust</code></li>
<li>Summary: <p>Vision-based object tracking has boosted extensive autonomous applications
for unmanned aerial vehicles (UAVs). However, the dynamic changes in flight
maneuver and viewpoint encountered in UAV tracking pose significant
difficulties, e.g. , aspect ratio change, and scale variation. The conventional
cross-correlation operation, while commonly used, has limitations in
effectively capturing perceptual similarity and incorporates extraneous
background information. To mitigate these limitations, this work presents a
novel saliency-guided dynamic vision Transformer (SGDViT) for UAV tracking. The
proposed method designs a new task-specific object saliency mining network to
refine the cross-correlation operation and effectively discriminate foreground
and background information. Additionally, a saliency adaptation embedding
operation dynamically generates tokens based on initial saliency, thereby
reducing the computational complexity of the Transformer architecture. Finally,
a lightweight saliency filtering Transformer further refines saliency
information and increases the focus on appearance information. The efficacy and
robustness of the proposed approach have been thoroughly assessed through
experiments on three widely-used UAV tracking benchmarks and real-world
scenarios, with results demonstrating its superiority. The source code and demo
videos are available at https://github.com/vision4robotics/SGDViT.
</p></li>
</ul>

<h3>Title: A Light Weight Model for Active Speaker Detection. (arXiv:2303.04439v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04439">http://arxiv.org/abs/2303.04439</a></li>
<li>Code URL: <a href="https://github.com/junhua-liao/light-asd">https://github.com/junhua-liao/light-asd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04439] A Light Weight Model for Active Speaker Detection](http://arxiv.org/abs/2303.04439) #robust</code></li>
<li>Summary: <p>Active speaker detection is a challenging task in audio-visual scenario
understanding, which aims to detect who is speaking in one or more speakers
scenarios. This task has received extensive attention as it is crucial in
applications such as speaker diarization, speaker tracking, and automatic video
editing. The existing studies try to improve performance by inputting multiple
candidate information and designing complex models. Although these methods
achieved outstanding performance, their high consumption of memory and
computational power make them difficult to be applied in resource-limited
scenarios. Therefore, we construct a lightweight active speaker detection
architecture by reducing input candidates, splitting 2D and 3D convolutions for
audio-visual feature extraction, and applying gated recurrent unit (GRU) with
low computational complexity for cross-modal modeling. Experimental results on
the AVA-ActiveSpeaker dataset show that our framework achieves competitive mAP
performance (94.1% vs. 94.2%), while the resource costs are significantly lower
than the state-of-the-art method, especially in model parameters (1.0M vs.
22.5M, about 23x) and FLOPs (0.6G vs. 2.6G, about 4x). In addition, our
framework also performs well on the Columbia dataset showing good robustness.
The code and model weights are available at
https://github.com/Junhua-Liao/Light-ASD.
</p></li>
</ul>

<h3>Title: DANet: Density Adaptive Convolutional Network with Interactive Attention for 3D Point Clouds. (arXiv:2303.04473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04473">http://arxiv.org/abs/2303.04473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04473] DANet: Density Adaptive Convolutional Network with Interactive Attention for 3D Point Clouds](http://arxiv.org/abs/2303.04473) #robust</code></li>
<li>Summary: <p>Local features and contextual dependencies are crucial for 3D point cloud
analysis. Many works have been devoted to designing better local convolutional
kernels that exploit the contextual dependencies. However, current point
convolutions lack robustness to varying point cloud density. Moreover,
contextual modeling is dominated by non-local or self-attention models which
are computationally expensive. To solve these problems, we propose density
adaptive convolution, coined DAConv. The key idea is to adaptively learn the
convolutional weights from geometric connections obtained from the point
density and position. To extract precise context dependencies with fewer
computations, we propose an interactive attention module (IAM) that embeds
spatial information into channel attention along different spatial directions.
DAConv and IAM are integrated in a hierarchical network architecture to achieve
local density and contextual direction-aware learning for point cloud analysis.
Experiments show that DAConv is significantly more robust to point density
compared to existing methods and extensive comparisons on challenging 3D point
cloud datasets show that our network achieves state-of-the-art classification
results of 93.6% on ModelNet40, competitive semantic segmentation results of
68.71% mIoU on S3DIS and part segmentation results of 86.7% mIoU on ShapeNet.
</p></li>
</ul>

<h3>Title: Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search. (arXiv:2303.04497v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04497">http://arxiv.org/abs/2303.04497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04497] Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search](http://arxiv.org/abs/2303.04497) #robust</code></li>
<li>Summary: <p>Text-based Person Search (TPS), is targeted on retrieving pedestrians to
match text descriptions instead of query images. Recent Vision-Language
Pre-training (VLP) models can bring transferable knowledge to downstream TPS
tasks, resulting in more efficient performance gains. However, existing TPS
methods improved by VLP only utilize pre-trained visual encoders, neglecting
the corresponding textual representation and breaking the significant modality
alignment learned from large-scale pre-training. In this paper, we explore the
full utilization of textual potential from VLP in TPS tasks. We build on the
proposed VLP-TPS baseline model, which is the first TPS model with both
pre-trained modalities. We propose the Multi-Integrity Description Constraints
(MIDC) to enhance the robustness of the textual modality by incorporating
different components of fine-grained corpus during training. Inspired by the
prompt approach for zero-shot classification with VLP models, we propose the
Dynamic Attribute Prompt (DAP) to provide a unified corpus of fine-grained
attributes as language hints for the image modality. Extensive experiments show
that our proposed TPS framework achieves state-of-the-art performance,
exceeding the previous best method by a margin.
</p></li>
</ul>

<h3>Title: Continuity-Aware Latent Interframe Information Mining for Reliable UAV Tracking. (arXiv:2303.04525v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04525">http://arxiv.org/abs/2303.04525</a></li>
<li>Code URL: <a href="https://github.com/vision4robotics/climrt">https://github.com/vision4robotics/climrt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04525] Continuity-Aware Latent Interframe Information Mining for Reliable UAV Tracking](http://arxiv.org/abs/2303.04525) #robust</code></li>
<li>Summary: <p>Unmanned aerial vehicle (UAV) tracking is crucial for autonomous navigation
and has broad applications in robotic automation fields. However, reliable UAV
tracking remains a challenging task due to various difficulties like frequent
occlusion and aspect ratio change. Additionally, most of the existing work
mainly focuses on explicit information to improve tracking performance,
ignoring potential interframe connections. To address the above issues, this
work proposes a novel framework with continuity-aware latent interframe
information mining for reliable UAV tracking, i.e., ClimRT. Specifically, a new
efficient continuity-aware latent interframe information mining network
(ClimNet) is proposed for UAV tracking, which can generate highly-effective
latent frame between two adjacent frames. Besides, a novel location-continuity
Transformer (LCT) is designed to fully explore continuity-aware
spatial-temporal information, thereby markedly enhancing UAV tracking.
Extensive qualitative and quantitative experiments on three authoritative
aerial benchmarks strongly validate the robustness and reliability of ClimRT in
UAV tracking performance. Furthermore, real-world tests on the aerial platform
validate its practicability and effectiveness. The code and demo materials are
released at https://github.com/vision4robotics/ClimRT.
</p></li>
</ul>

<h3>Title: Robustness Evaluation in Hand Pose Estimation Models using Metamorphic Testing. (arXiv:2303.04566v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04566">http://arxiv.org/abs/2303.04566</a></li>
<li>Code URL: <a href="https://github.com/mpuu00001/robustness-evaluation-in-hand-pose-estimation">https://github.com/mpuu00001/robustness-evaluation-in-hand-pose-estimation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04566] Robustness Evaluation in Hand Pose Estimation Models using Metamorphic Testing](http://arxiv.org/abs/2303.04566) #robust</code></li>
<li>Summary: <p>Hand pose estimation (HPE) is a task that predicts and describes the hand
poses from images or video frames. When HPE models estimate hand poses captured
in a laboratory or under controlled environments, they normally deliver good
performance. However, the real-world environment is complex, and various
uncertainties may happen, which could degrade the performance of HPE models.
For example, the hands could be occluded, the visibility of hands could be
reduced by imperfect exposure rate, and the contour of hands prone to be
blurred during fast hand movements. In this work, we adopt metamorphic testing
to evaluate the robustness of HPE models and provide suggestions on the choice
of HPE models for different applications. The robustness evaluation was
conducted on four state-of-the-art models, namely MediaPipe hands, OpenPose,
BodyHands, and NSRM hand. We found that on average more than 80\% of the hands
could not be identified by BodyHands, and at least 50\% of hands could not be
identified by MediaPipe hands when diagonal motion blur is introduced, while an
average of more than 50\% of strongly underexposed hands could not be correctly
estimated by NSRM hand. Similarly, applying occlusions on only four hand joints
will also largely degrade the performance of these models. The experimental
results show that occlusions, illumination variations, and motion blur are the
main obstacles to the performance of existing HPE models. These findings may
pave the way for researchers to improve the performance and robustness of hand
pose estimation models and their applications.
</p></li>
</ul>

<h3>Title: POEM: Proof of Entropy Minima. (arXiv:2303.04305v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04305">http://arxiv.org/abs/2303.04305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04305] POEM: Proof of Entropy Minima](http://arxiv.org/abs/2303.04305) #robust</code></li>
<li>Summary: <p>Nakamoto consensus has been incredibly influential in enabling robust
blockchain systems, and one of its components is the so-called heaviest chain
rule (HCR). Within this rule, the calculation of the weight of the chain tip is
performed by adding the difficulty threshold value to the previous total
difficulty. Current difficulty based weighting systems do not take the
intrinsic block weight into account. This paper studies a new mechanism based
on entropy differences, named proof of entropy minima (POEM), which
incorporates the intrinsic block weight in a manner that significantly reduces
the orphan rate of the blockchain while simultaneously accelerating
finalization. Finally, POEM helps to understand blockchain as a static
time-independent sequence of committed events.
</p></li>
</ul>

<h3>Title: DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data. (arXiv:2303.04201v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04201">http://arxiv.org/abs/2303.04201</a></li>
<li>Code URL: <a href="https://github.com/shantanu48114860/dr-vidal-amia-22">https://github.com/shantanu48114860/dr-vidal-amia-22</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04201] DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data](http://arxiv.org/abs/2303.04201) #robust</code></li>
<li>Summary: <p>Determining causal effects of interventions onto outcomes from real-world,
observational (non-randomized) data, e.g., treatment repurposing using
electronic health records, is challenging due to underlying bias. Causal deep
learning has improved over traditional techniques for estimating individualized
treatment effects (ITE). We present the Doubly Robust Variational
Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative
framework that combines two joint models of treatment and outcome, ensuring an
unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL
integrates: (i) a variational autoencoder (VAE) to factorize confounders into
latent variables according to causal assumptions; (ii) an information-theoretic
generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a
doubly robust block incorporating treatment propensities for outcome
predictions. On synthetic and real-world datasets (Infant Health and
Development Program, Twin Birth Registry, and National Supported Work Program),
DR-VIDAL achieves better performance than other non-generative and generative
methods. In conclusion, DR-VIDAL uniquely fuses causal assumptions, VAE,
Info-GAN, and doubly robustness into a comprehensive, performant framework.
Code is available at: https://github.com/Shantanu48114860/DR-VIDAL-AMIA-22
under MIT license.
</p></li>
</ul>

<h3>Title: Soft Actor-Critic Algorithm with Truly Inequality Constraint. (arXiv:2303.04356v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04356">http://arxiv.org/abs/2303.04356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04356] Soft Actor-Critic Algorithm with Truly Inequality Constraint](http://arxiv.org/abs/2303.04356) #robust</code></li>
<li>Summary: <p>Soft actor-critic (SAC) in reinforcement learning is expected to be one of
the next-generation robot control schemes. Its ability to maximize policy
entropy would make a robotic controller robust to noise and perturbation, which
is useful for real-world robot applications. However, the priority of
maximizing the policy entropy is automatically tuned in the current
implementation, the rule of which can be interpreted as one for equality
constraint, binding the policy entropy into its specified target value. The
current SAC is therefore no longer maximize the policy entropy, contrary to our
expectation. To resolve this issue in SAC, this paper improves its
implementation with a slack variable for appropriately handling the inequality
constraint to maximize the policy entropy. In Mujoco and Pybullet simulators,
the modified SAC achieved the higher robustness and the more stable learning
than before while regularizing the norm of action. In addition, a real-robot
variable impedance task was demonstrated for showing the applicability of the
modified SAC to real-world robot control.
</p></li>
</ul>

<h3>Title: A robust method for reliability updating with equality information using sequential adaptive importance sampling. (arXiv:2303.04545v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04545">http://arxiv.org/abs/2303.04545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04545] A robust method for reliability updating with equality information using sequential adaptive importance sampling](http://arxiv.org/abs/2303.04545) #robust</code></li>
<li>Summary: <p>Reliability updating refers to a problem that integrates Bayesian updating
technique with structural reliability analysis and cannot be directly solved by
structural reliability methods (SRMs) when it involves equality information.
The state-of-the-art approaches transform equality information into inequality
information by introducing an auxiliary standard normal parameter. These
methods, however, encounter the loss of computational efficiency due to the
difficulty in finding the maximum of the likelihood function, the large
coefficient of variation (COV) associated with the posterior failure
probability and the inapplicability to dynamic updating problems where new
information is constantly available. To overcome these limitations, this paper
proposes an innovative method called RU-SAIS (reliability updating using
sequential adaptive importance sampling), which combines elements of sequential
importance sampling and K-means clustering to construct a series of important
sampling densities (ISDs) using Gaussian mixture. The last ISD of the sequence
is further adaptively modified through application of the cross entropy method.
The performance of RU-SAIS is demonstrated by three examples. Results show that
RU-SAIS achieves a more accurate and robust estimator of the posterior failure
probability than the existing methods such as subset simulation.
</p></li>
</ul>

<h3>Title: Robust Multimodal Fusion for Human Activity Recognition. (arXiv:2303.04636v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04636">http://arxiv.org/abs/2303.04636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04636] Robust Multimodal Fusion for Human Activity Recognition](http://arxiv.org/abs/2303.04636) #robust</code></li>
<li>Summary: <p>The proliferation of IoT and mobile devices equipped with heterogeneous
sensors has enabled new applications that rely on the fusion of time-series
data generated by multiple sensors with different modalities. While there are
promising deep neural network architectures for multimodal fusion, their
performance falls apart quickly in the presence of consecutive missing data and
noise across multiple modalities/sensors, the issues that are prevalent in
real-world settings. We propose Centaur, a multimodal fusion model for human
activity recognition (HAR) that is robust to these data quality issues. Centaur
combines a data cleaning module, which is a denoising autoencoder with
convolutional layers, and a multimodal fusion module, which is a deep
convolutional neural network with the self-attention mechanism to capture
cross-sensor correlation. We train Centaur using a stochastic data corruption
scheme and evaluate it on three datasets that contain data generated by
multiple inertial measurement units. Centaur's data cleaning module outperforms
2 state-of-the-art autoencoder-based models and its multimodal fusion module
outperforms 4 strong baselines. Compared to 2 related robust fusion
architectures, Centaur is more robust, achieving 11.59-17.52% higher accuracy
in HAR, especially in the presence of consecutive missing data in multiple
sensor channels.
</p></li>
</ul>

<h3>Title: Ewald-based Long-Range Message Passing for Molecular Graphs. (arXiv:2303.04791v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04791">http://arxiv.org/abs/2303.04791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04791] Ewald-based Long-Range Message Passing for Molecular Graphs](http://arxiv.org/abs/2303.04791) #robust</code></li>
<li>Summary: <p>Neural architectures that learn potential energy surfaces from molecular data
have undergone fast improvement in recent years. A key driver of this success
is the Message Passing Neural Network (MPNN) paradigm. Its favorable scaling
with system size partly relies upon a spatial distance limit on messages. While
this focus on locality is a useful inductive bias, it also impedes the learning
of long-range interactions such as electrostatics and van der Waals forces. To
address this drawback, we propose Ewald message passing: a nonlocal Fourier
space scheme which limits interactions via a cutoff on frequency instead of
distance, and is theoretically well-founded in the Ewald summation method. It
can serve as an augmentation on top of existing MPNN architectures as it is
computationally cheap and agnostic to other architectural details. We test the
approach with four baseline models and two datasets containing diverse periodic
(OC20) and aperiodic structures (OE62). We observe robust improvements in
energy mean absolute errors across all models and datasets, averaging 10% on
OC20 and 16% on OE62. Our analysis shows an outsize impact of these
improvements on structures with high long-range contributions to the ground
truth energy.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h3>Title: On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04729">http://arxiv.org/abs/2303.04729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04729] On the Risks of Stealing the Decoding Algorithms of Language Models](http://arxiv.org/abs/2303.04729) #steal</code></li>
<li>Summary: <p>A key component of generating text from modern language models (LM) is the
selection and tuning of decoding algorithms. These algorithms determine how to
generate text from the internal probability distribution generated by the LM.
The process of choosing a decoding algorithm and tuning its hyperparameters
takes significant time, manual effort, and computation, and it also requires
extensive human evaluation. Therefore, the identity and hyperparameters of such
decoding algorithms are considered to be extremely valuable to their owners. In
this work, we show, for the first time, that an adversary with typical API
access to an LM can steal the type and hyperparameters of its decoding
algorithms at very low monetary costs. Our attack is effective against popular
LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the
feasibility of stealing such information with only a few dollars, e.g.,
$\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
</p></li>
</ul>

<h2>extraction</h2>
<h3>Title: End-to-end Face-swapping via Adaptive Latent Representation Learning. (arXiv:2303.04186v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04186">http://arxiv.org/abs/2303.04186</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04186] End-to-end Face-swapping via Adaptive Latent Representation Learning](http://arxiv.org/abs/2303.04186) #extraction</code></li>
<li>Summary: <p>Taking full advantage of the excellent performance of StyleGAN, style
transfer-based face swapping methods have been extensively investigated
recently. However, these studies require separate face segmentation and
blending modules for successful face swapping, and the fixed selection of the
manipulated latent code in these works is reckless, thus degrading face
swapping quality, generalizability, and practicability. This paper proposes a
novel and end-to-end integrated framework for high resolution and attribute
preservation face swapping via Adaptive Latent Representation Learning.
Specifically, we first design a multi-task dual-space face encoder by sharing
the underlying feature extraction network to simultaneously complete the facial
region perception and face encoding. This encoder enables us to control the
face pose and attribute individually, thus enhancing the face swapping quality.
Next, we propose an adaptive latent codes swapping module to adaptively learn
the mapping between the facial attributes and the latent codes and select
effective latent codes for improved retention of facial attributes. Finally,
the initial face swapping image generated by StyleGAN2 is blended with the
facial region mask generated by our encoder to address the background blur
problem. Our framework integrating facial perceiving and blending into the
end-to-end training and testing process can achieve high realistic
face-swapping on wild faces without segmentation masks. Experimental results
demonstrate the superior performance of our approach over state-of-the-art
methods.
</p></li>
</ul>

<h3>Title: Comparing PSDNet, pretrained networks, and traditional feature extraction for predicting the particle size distribution of granular materials from photographs. (arXiv:2303.04265v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04265">http://arxiv.org/abs/2303.04265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04265] Comparing PSDNet, pretrained networks, and traditional feature extraction for predicting the particle size distribution of granular materials from photographs](http://arxiv.org/abs/2303.04265) #extraction</code></li>
<li>Summary: <p>This study aims to evaluate PSDNet, a series of convolutional neural networks
(ConvNets) trained with photographs to predict the particle size distribution
of granular materials. Nine traditional feature extraction methods and 15
pretrained ConvNets were also evaluated and compared. A dataset including 9600
photographs of 15 different granular materials was used. The influence of image
size and color band was verified by using six image sizes between 32 and 160
pixels, and both grayscale and color images as PSDNet inputs. In addition to
random training, validation, and testing datasets, a material removal method
was also used to evaluate the performances of each image analysis method. With
this method, each material was successively removed from the training and
validation datasets and used as the testing dataset. Results show that a
combination of all PSDNet color and grayscale features can lead to a root mean
square error (RMSE) on the percentages passing as low as 1.8 % with a random
testing dataset and 9.1% with the material removal method. For the random
datasets, a combination of all traditional features, and the features extracted
from InceptionResNetV2 led to RMSE on the percentages passing of 2.3 and 1.7 %,
respectively.
</p></li>
</ul>

<h3>Title: PSDNet: Determination of Particle Size Distributions Using Synthetic Soil Images and Convolutional Neural Networks. (arXiv:2303.04269v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04269">http://arxiv.org/abs/2303.04269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04269] PSDNet: Determination of Particle Size Distributions Using Synthetic Soil Images and Convolutional Neural Networks](http://arxiv.org/abs/2303.04269) #extraction</code></li>
<li>Summary: <p>This project aimed to determine the grain size distribution of granular
materials from images using convolutional neural networks. The application of
ConvNet and pretrained ConvNet models, including AlexNet, SqueezeNet,
GoogLeNet, InceptionV3, DenseNet201, MobileNetV2, ResNet18, ResNet50,
ResNet101, Xception, InceptionResNetV2, ShuffleNet, and NASNetMobile was
studied. Synthetic images of granular materials created with the discrete
element code YADE were used. All the models were trained and verified with
grayscale and color band datasets with image sizes ranging from 32 to 160
pixels. The proposed ConvNet model predicts the percentages of mass retained on
the finest sieve, coarsest sieve, and all sieves with root-mean-square errors
of 1.8 %, 3.3 %, and 2.8 %, respectively, and a coefficient of determination of
0.99. For pretrained networks, root-mean-square errors of 2.4 % and 2.8 % were
obtained for the finest sieve with feature extraction and transfer learning
models, respectively.
</p></li>
</ul>

<h3>Title: Corner Detection Based on Multi-directional Gabor Filters with Multi-scales. (arXiv:2303.04334v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04334">http://arxiv.org/abs/2303.04334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04334] Corner Detection Based on Multi-directional Gabor Filters with Multi-scales](http://arxiv.org/abs/2303.04334) #extraction</code></li>
<li>Summary: <p>Gabor wavelet is an essential tool for image analysis and computer vision
tasks. Local structure tensors with multiple scales are widely used in local
feature extraction. Our research indicates that the current corner detection
method based on Gabor wavelets can not effectively apply to complex scenes. In
this work, the capability of the Gabor function to discriminate the intensity
changes of step edges, L-shaped corners, Y-shaped or T-shaped corners, X-shaped
corners, and star-shaped corners are investigated. The properties of Gabor
wavelets to suppress affine image transformation are investigated and obtained.
Many properties for edges and corners were discovered, which prompted us to
propose a new corner extraction method. To fully use the structural information
from the tuned Gabor filters, a novel multi-directional structure tensor is
constructed for corner detection, and a multi-scale corner measurement function
is proposed to remove false candidate corners. Furthermore, we compare the
proposed method with twelve current state-of-the-art methods, which exhibit
optimal performance and practical application to 3D reconstruction with good
application potential.
</p></li>
</ul>

<h3>Title: FCN+: Global Receptive Convolution Makes FCN Great Again. (arXiv:2303.04589v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04589">http://arxiv.org/abs/2303.04589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04589] FCN+: Global Receptive Convolution Makes FCN Great Again](http://arxiv.org/abs/2303.04589) #extraction</code></li>
<li>Summary: <p>Fully convolutional network (FCN) is a seminal work for semantic
segmentation. However, due to its limited receptive field, FCN cannot
effectively capture global context information which is vital for semantic
segmentation. As a result, it is beaten by state-of-the-art methods which
leverage different filter sizes for larger receptive fields. However, such a
strategy usually introduces more parameters and increases the computational
cost. In this paper, we propose a novel global receptive convolution (GRC) to
effectively increase the receptive field of FCN for context information
extraction, which results in an improved FCN termed FCN+. The GRC provides
global receptive field for convolution without introducing any extra learnable
parameters. The motivation of GRC is that different channels of a convolutional
filter can have different grid sampling locations across the whole input
feature map. Specifically, the GRC first divides the channels of the filter
into two groups. The grid sampling locations of the first group are shifted to
different spatial coordinates across the whole feature map, according to their
channel indexes. This can help the convolutional filter capture the global
context information. The grid sampling location of the second group remains
unchanged to keep the original location information. Convolving using these two
groups, the GRC can integrate the global context into the original location
information of each pixel for better dense prediction results. With the GRC
built in, FCN+ can achieve comparable performance to state-of-the-art methods
for semantic segmentation tasks, as verified on PASCAL VOC 2012, Cityscapes,
and ADE20K.
</p></li>
</ul>

<h3>Title: Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing. (arXiv:2303.04794v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04794">http://arxiv.org/abs/2303.04794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04794] Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing](http://arxiv.org/abs/2303.04794) #extraction</code></li>
<li>Summary: <p>Recent work has utilised knowledge-aware approaches to natural language
understanding, question answering, recommendation systems, and other tasks.
These approaches rely on well-constructed and large-scale knowledge graphs that
can be useful for many downstream applications and empower knowledge-aware
models with commonsense reasoning. Such knowledge graphs are constructed
through knowledge acquisition tasks such as relation extraction and knowledge
graph completion. This work seeks to utilise and build on the growing body of
work that uses findings from the field of natural language processing (NLP) to
extract knowledge from text and build knowledge graphs. The focus of this
research project is on how we can use transformer-based approaches to extract
and contextualise event information, matching it to existing ontologies, to
build a comprehensive knowledge of graph-based event representations.
Specifically, sub-event extraction is used as a way of creating sub-event-aware
event representations. These event representations are then further enriched
through fine-grained location extraction and contextualised through the
alignment of historically relevant quotes.
</p></li>
</ul>

<h3>Title: MEV in fixed gas price blockchains: Terra Classic as a case of study. (arXiv:2303.04242v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04242">http://arxiv.org/abs/2303.04242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04242] MEV in fixed gas price blockchains: Terra Classic as a case of study](http://arxiv.org/abs/2303.04242) #extraction</code></li>
<li>Summary: <p>Maximum extractable value (MEV) has been extensively studied. In most papers,
the researchers have worked with the Ethereum blockchain almost exclusively.
Even though, Ethereum and other blockchains have dynamic gas prices this is not
the case for all blockchains; many of them have fixed gas prices. Extending the
research to other blockchains with fixed gas price could broaden the scope of
the existing studies on MEV. To our knowledge, there is not a vast
understanding of MEV in fixed gas price blockchains. Therefore, we propose to
study Terra Classic as an example to understand how MEV activities affect
blockchains with fixed gas price. We first analysed the data from Terra Classic
before the UST de-peg event in May 2022 and described the nature of the
exploited arbitrage opportunities. We found more than 188K successful
arbitrages, and most of them used UST as the initial token. The capital to
perform the arbitrage was less than 1K UST in 50% of the cases, and 80% of the
arbitrages had less than four swaps. Then, we explored the characteristics that
attribute to higher MEV. We found that searchers who use more complex
mechanisms, i.e. different contracts and accounts, made higher profits.
Finally, we concluded that the most profitable searchers used a strategy of
running bots in a multi-instance environment, i.e. running bots with different
virtual machines. We measured the importance of the geographic distribution of
the virtual machines that run the bots. We found that having good geographic
coverage makes the difference between winning or losing the arbitrage
opportunities. That is because, unlike MEV extraction in Ethereum, bots in
fixed gas price blockchains are not battling a gas war; they are fighting in a
latency war.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning via Variational Bayesian Inference: Personalization, Sparsity and Clustering. (arXiv:2303.04345v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04345">http://arxiv.org/abs/2303.04345</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04345] Federated Learning via Variational Bayesian Inference: Personalization, Sparsity and Clustering](http://arxiv.org/abs/2303.04345) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a promising framework that models distributed
machine learning while protecting the privacy of clients. However, FL suffers
performance degradation from heterogeneous and limited data. To alleviate the
degradation, we present a novel personalized Bayesian FL approach named
pFedBayes. By using the trained global distribution from the server as the
prior distribution of each client, each client adjusts its own distribution by
minimizing the sum of the reconstruction error over its personalized data and
the KL divergence with the downloaded global distribution. Then, we propose a
sparse personalized Bayesian FL approach named sFedBayes. To overcome the
extreme heterogeneity in non-i.i.d. data, we propose a clustered Bayesian FL
model named cFedbayes by learning different prior distributions for different
clients. Theoretical analysis gives the generalization error bound of three
approaches and shows that the generalization error convergence rates of the
proposed approaches achieve minimax optimality up to a logarithmic factor.
Moreover, the analysis presents that cFedbayes has a tighter generalization
error rate than pFedBayes. Numerous experiments are provided to demonstrate
that the proposed approaches have better performance than other advanced
personalized methods on private models in the presence of heterogeneous and
limited data.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Causal Dependence Plots for Interpretable Machine Learning. (arXiv:2303.04209v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04209">http://arxiv.org/abs/2303.04209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04209] Causal Dependence Plots for Interpretable Machine Learning](http://arxiv.org/abs/2303.04209) #fair</code></li>
<li>Summary: <p>Explaining artificial intelligence or machine learning models is an
increasingly important problem. For humans to stay in the loop and control such
systems, we must be able to understand how they interact with the world. This
work proposes using known or assumed causal structure in the input variables to
produce simple and practical explanations of supervised learning models. Our
explanations -- which we name Causal Dependence Plots or CDP -- visualize how
the model output depends on changes in a given predictor \emph{along with any
consequent causal changes in other predictors}. Since this causal dependence
captures how humans often think about input-output dependence, CDPs can be
powerful tools in the explainable AI or interpretable ML toolkit and contribute
to applications including scientific machine learning and algorithmic fairness.
CDP can also be used for model-agnostic or black-box explanations.
</p></li>
</ul>

<h3>Title: HappyMap: A Generalized Multi-calibration Method. (arXiv:2303.04379v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04379">http://arxiv.org/abs/2303.04379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04379] HappyMap: A Generalized Multi-calibration Method](http://arxiv.org/abs/2303.04379) #fair</code></li>
<li>Summary: <p>Multi-calibration is a powerful and evolving concept originating in the field
of algorithmic fairness. For a predictor $f$ that estimates the outcome $y$
given covariates $x$, and for a function class $\mathcal{C}$, multi-calibration
requires that the predictor $f(x)$ and outcome $y$ are indistinguishable under
the class of auditors in $\mathcal{C}$. Fairness is captured by incorporating
demographic subgroups into the class of functions~$\mathcal{C}$. Recent work
has shown that, by enriching the class $\mathcal{C}$ to incorporate appropriate
propensity re-weighting functions, multi-calibration also yields
target-independent learning, wherein a model trained on a source domain
performs well on unseen, future, target domains(approximately) captured by the
re-weightings.
</p></li>
</ul>

<p>Formally, multi-calibration with respect to $\mathcal{C}$ bounds
$\big|\mathbb{E}_{(x,y)\sim \mathcal{D}}[c(f(x),x)\cdot(f(x)-y)]\big|$ for all
$c \in \mathcal{C}$. In this work, we view the term $(f(x)-y)$ as just one
specific mapping, and explore the power of an enriched class of mappings. We
propose \textit{HappyMap}, a generalization of multi-calibration, which yields
a wide range of new applications, including a new fairness notion for
uncertainty quantification (conformal prediction), a novel technique for
conformal prediction under covariate shift, and a different approach to
analyzing missing data, while also yielding a unified understanding of several
existing seemingly disparate algorithmic fairness notions and
target-independent learning approaches.
</p>
<p>We give a single \textit{HappyMap} meta-algorithm that captures all these
results, together with a sufficiency condition for its success.
</p>

<h2>interpretability</h2>
<h3>Title: Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?. (arXiv:2303.04204v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04204">http://arxiv.org/abs/2303.04204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04204] Deep hybrid model with satellite imagery: how to combine demand modeling and computer vision for behavior analysis?](http://arxiv.org/abs/2303.04204) #interpretability</code></li>
<li>Summary: <p>Classical demand modeling analyzes travel behavior using only low-dimensional
numeric data (i.e. sociodemographics and travel attributes) but not
high-dimensional urban imagery. However, travel behavior depends on the factors
represented by both numeric data and urban imagery, thus necessitating a
synergetic framework to combine them. This study creates a theoretical
framework of deep hybrid models with a crossing structure consisting of a
mixing operator and a behavioral predictor, thus integrating the numeric and
imagery data into a latent space. Empirically, this framework is applied to
analyze travel mode choice using the MyDailyTravel Survey from Chicago as the
numeric inputs and the satellite images as the imagery inputs. We found that
deep hybrid models outperform both the traditional demand models and the recent
deep learning in predicting the aggregate and disaggregate travel behavior with
our supervision-as-mixing design. The latent space in deep hybrid models can be
interpreted, because it reveals meaningful spatial and social patterns. The
deep hybrid models can also generate new urban images that do not exist in
reality and interpret them with economic theory, such as computing substitution
patterns and social welfare changes. Overall, the deep hybrid models
demonstrate the complementarity between the low-dimensional numeric and
high-dimensional imagery data and between the traditional demand modeling and
recent deep learning. It generalizes the latent classes and variables in
classical hybrid demand models to a latent space, and leverages the
computational power of deep learning for imagery while retaining the economic
interpretability on the microeconomics foundation.
</p></li>
</ul>

<h3>Title: Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods. (arXiv:2303.04437v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04437">http://arxiv.org/abs/2303.04437</a></li>
<li>Code URL: <a href="https://github.com/ferryjul/hybridcorels">https://github.com/ferryjul/hybridcorels</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04437] Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods](http://arxiv.org/abs/2303.04437) #interpretability</code></li>
<li>Summary: <p>A hybrid model involves the cooperation of an interpretable model and a
complex black box. At inference, any input of the hybrid model is assigned to
either its interpretable or complex component based on a gating mechanism. The
advantages of such models over classical ones are two-fold: 1) They grant users
precise control over the level of transparency of the system and 2) They can
potentially perform better than a standalone black box since redirecting some
of the inputs to an interpretable model implicitly acts as regularization.
Still, despite their high potential, hybrid models remain under-studied in the
interpretability/explainability literature. In this paper, we remedy this fact
by presenting a thorough investigation of such models from three perspectives:
Theory, Taxonomy, and Methods. First, we explore the theory behind the
generalization of hybrid models from the Probably-Approximately-Correct (PAC)
perspective. A consequence of our PAC guarantee is the existence of a sweet
spot for the optimal transparency of the system. When such a sweet spot is
attained, a hybrid model can potentially perform better than a standalone black
box. Secondly, we provide a general taxonomy for the different ways of training
hybrid models: the Post-Black-Box and Pre-Black-Box paradigms. These approaches
differ in the order in which the interpretable and complex components are
trained. We show where the state-of-the-art hybrid models Hybrid-Rule-Set and
Companion-Rule-List fall in this taxonomy. Thirdly, we implement the two
paradigms in a single method: HybridCORELS, which extends the CORELS algorithm
to hybrid modeling. By leveraging CORELS, HybridCORELS provides a certificate
of optimality of its interpretable component and precise control over
transparency. We finally show empirically that HybridCORELS is competitive with
existing hybrid models, and performs just as well as a standalone black box (or
even better) while being partly transparent.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: SemEval-2023 Task 10: Explainable Detection of Online Sexism. (arXiv:2303.04222v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04222">http://arxiv.org/abs/2303.04222</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04222] SemEval-2023 Task 10: Explainable Detection of Online Sexism](http://arxiv.org/abs/2303.04222) #explainability</code></li>
<li>Summary: <p>Online sexism is a widespread and harmful phenomenon. Automated tools can
assist the detection of sexism at scale. Binary detection, however, disregards
the diversity of sexist content, and fails to provide clear explanations for
why something is sexist. To address this issue, we introduce SemEval Task 10 on
the Explainable Detection of Online Sexism (EDOS). We make three main
contributions: i) a novel hierarchical taxonomy of sexist content, which
includes granular vectors of sexism to aid explainability; ii) a new dataset of
20,000 social media comments with fine-grained labels, along with larger
unlabelled datasets for model adaptation; and iii) baseline models as well as
an analysis of the methods, results and errors for participant submissions to
our task.
</p></li>
</ul>

<h3>Title: "How to make them stay?" -- Diverse Counterfactual Explanations of Employee Attrition. (arXiv:2303.04579v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04579">http://arxiv.org/abs/2303.04579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04579] "How to make them stay?" -- Diverse Counterfactual Explanations of Employee Attrition](http://arxiv.org/abs/2303.04579) #explainability</code></li>
<li>Summary: <p>Employee attrition is an important and complex problem that can directly
affect an organisation's competitiveness and performance. Explaining the
reasons why employees leave an organisation is a key human resource management
challenge due to the high costs and time required to attract and keep talented
employees. Businesses therefore aim to increase employee retention rates to
minimise their costs and maximise their performance. Machine learning (ML) has
been applied in various aspects of human resource management including
attrition prediction to provide businesses with insights on proactive measures
on how to prevent talented employees from quitting. Among these ML methods, the
best performance has been reported by ensemble or deep neural networks, which
by nature constitute black box techniques and thus cannot be easily
interpreted. To enable the understanding of these models' reasoning several
explainability frameworks have been proposed. Counterfactual explanation
methods have attracted considerable attention in recent years since they can be
used to explain and recommend actions to be performed to obtain the desired
outcome. However current counterfactual explanations methods focus on
optimising the changes to be made on individual cases to achieve the desired
outcome. In the attrition problem it is important to be able to foresee what
would be the effect of an organisation's action to a group of employees where
the goal is to prevent them from leaving the company. Therefore, in this paper
we propose the use of counterfactual explanations focusing on multiple
attrition cases from historical data, to identify the optimum interventions
that an organisation needs to make to its practices/policies to prevent or
minimise attrition probability for these cases.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation. (arXiv:2303.04248v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04248">http://arxiv.org/abs/2303.04248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04248] TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation](http://arxiv.org/abs/2303.04248) #diffusion</code></li>
<li>Summary: <p>Denoising Diffusion models have demonstrated their proficiency for generative
sampling. However, generating good samples often requires many iterations.
Consequently, techniques such as binary time-distillation (BTD) have been
proposed to reduce the number of network calls for a fixed architecture. In
this paper, we introduce TRAnsitive Closure Time-distillation (TRACT), a new
method that extends BTD. For single step diffusion,TRACT improves FID by up to
2.4x on the same architecture, and achieves new single-step Denoising Diffusion
Implicit Models (DDIM) state-of-the-art FID (7.4 for ImageNet64, 3.8 for
CIFAR10). Finally we tease apart the method through extended ablations. The
PyTorch implementation will be released soon.
</p></li>
</ul>

<h3>Title: Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. (arXiv:2303.04671v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04671">http://arxiv.org/abs/2303.04671</a></li>
<li>Code URL: <a href="https://github.com/microsoft/visual-chatgpt">https://github.com/microsoft/visual-chatgpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04671] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models](http://arxiv.org/abs/2303.04671) #diffusion</code></li>
<li>Summary: <p>ChatGPT is attracting a cross-field interest as it provides a language
interface with remarkable conversational competency and reasoning capabilities
across many domains. However, since ChatGPT is trained with languages, it is
currently not capable of processing or generating images from the visual world.
At the same time, Visual Foundation Models, such as Visual Transformers or
Stable Diffusion, although showing great visual understanding and generation
capabilities, they are only experts on specific tasks with one-round fixed
inputs and outputs. To this end, We build a system called \textbf{Visual
ChatGPT}, incorporating different Visual Foundation Models, to enable the user
to interact with ChatGPT by 1) sending and receiving not only languages but
also images 2) providing complex visual questions or visual editing
instructions that require the collaboration of multiple AI models with
multi-steps. 3) providing feedback and asking for corrected results. We design
a series of prompts to inject the visual model information into ChatGPT,
considering models of multiple inputs/outputs and models that require visual
feedback. Experiments show that Visual ChatGPT opens the door to investigating
the visual roles of ChatGPT with the help of Visual Foundation Models. Our
system is publicly available at
\url{https://github.com/microsoft/visual-chatgpt}.
</p></li>
</ul>

<h3>Title: Video-P2P: Video Editing with Cross-attention Control. (arXiv:2303.04761v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04761">http://arxiv.org/abs/2303.04761</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04761] Video-P2P: Video Editing with Cross-attention Control](http://arxiv.org/abs/2303.04761) #diffusion</code></li>
<li>Summary: <p>This paper presents Video-P2P, a novel framework for real-world video editing
with cross-attention control. While attention control has proven effective for
image editing with pre-trained image generation models, there are currently no
large-scale video generation models publicly available. Video-P2P addresses
this limitation by adapting an image generation diffusion model to complete
various video editing tasks. Specifically, we propose to first tune a
Text-to-Set (T2S) model to complete an approximate inversion and then optimize
a shared unconditional embedding to achieve accurate video inversion with a
small memory cost. For attention control, we introduce a novel
decoupled-guidance strategy, which uses different guidance strategies for the
source and target prompts. The optimized unconditional embedding for the source
prompt improves reconstruction ability, while an initialized unconditional
embedding for the target prompt enhances editability. Incorporating the
attention maps of these two branches enables detailed editing. These technical
designs enable various text-driven editing applications, including word swap,
prompt refinement, and attention re-weighting. Video-P2P works well on
real-world videos for generating new characters while optimally preserving
their original poses and scenes. It significantly outperforms previous
approaches.
</p></li>
</ul>

<h3>Title: Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04772">http://arxiv.org/abs/2303.04772</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04772] Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation](http://arxiv.org/abs/2303.04772) #diffusion</code></li>
<li>Summary: <p>Score-based diffusion models (SBDM) have recently emerged as state-of-the-art
approaches for image generation. Existing SBDMs are typically formulated in a
finite-dimensional setting, where images are considered as tensors of a finite
size. This papers develops SBDMs in the infinite-dimensional setting, that is,
we model the training data as functions supported on a rectangular domain.
Besides the quest for generating images at ever higher resolution our primary
motivation is to create a well-posed infinite-dimensional learning problem so
that we can discretize it consistently on multiple resolution levels. We
thereby hope to obtain diffusion models that generalize across different
resolution levels and improve the efficiency of the training process. We
demonstrate how to overcome two shortcomings of current SBDM approaches in the
infinite-dimensional setting. First, we modify the forward process to ensure
that the latent distribution is well-defined in the infinite-dimensional
setting using the notion of trace class operators. Second, we illustrate that
approximating the score function with an operator network, in our case Fourier
neural operators (FNOs), is beneficial for multilevel training. After deriving
the forward and reverse process in the infinite-dimensional setting, we show
their well-posedness, derive adequate discretizations, and investigate the role
of the latent distributions. We provide first promising numerical results on
two datasets, MNIST and material structures. In particular, we show that
multilevel training is feasible within this framework.
</p></li>
</ul>

<h3>Title: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models. (arXiv:2303.04803v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04803">http://arxiv.org/abs/2303.04803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04803] Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.04803) #diffusion</code></li>
<li>Summary: <p>We present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation,
which unifies pre-trained text-image diffusion and discriminative models to
perform open-vocabulary panoptic segmentation. Text-to-image diffusion models
have shown the remarkable capability of generating high-quality images with
diverse open-vocabulary language descriptions. This demonstrates that their
internal representation space is highly correlated with open concepts in the
real world. Text-image discriminative models like CLIP, on the other hand, are
good at classifying images into open-vocabulary labels. We propose to leverage
the frozen representation of both these models to perform panoptic segmentation
of any category in the wild. Our approach outperforms the previous state of the
art by significant margins on both open-vocabulary panoptic and semantic
segmentation tasks. In particular, with COCO training only, our method achieves
23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute
improvement over the previous state-of-the-art. Project page is available at
\url{https://jerryxu.net/ODISE}.
</p></li>
</ul>

<h3>Title: Diffusing Gaussian Mixtures for Generating Categorical Data. (arXiv:2303.04635v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.04635">http://arxiv.org/abs/2303.04635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.04635] Diffusing Gaussian Mixtures for Generating Categorical Data](http://arxiv.org/abs/2303.04635) #diffusion</code></li>
<li>Summary: <p>Learning a categorical distribution comes with its own set of challenges. A
successful approach taken by state-of-the-art works is to cast the problem in a
continuous domain to take advantage of the impressive performance of the
generative models for continuous data. Amongst them are the recently emerging
diffusion probabilistic models, which have the observed advantage of generating
high-quality samples. Recent advances for categorical generative models have
focused on log likelihood improvements. In this work, we propose a generative
model for categorical data based on diffusion models with a focus on
high-quality sample generation, and propose sampled-based evaluation methods.
The efficacy of our method stems from performing diffusion in the continuous
domain while having its parameterization informed by the structure of the
categorical nature of the target distribution. Our method of evaluation
highlights the capabilities and limitations of different generative models for
generating categorical data, and includes experiments on synthetic and
real-world protein datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
