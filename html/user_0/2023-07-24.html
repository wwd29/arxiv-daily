<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. (arXiv:2307.11565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11565">http://arxiv.org/abs/2307.11565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11565] FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks](http://arxiv.org/abs/2307.11565) #secure</code></li>
<li>Summary: <p>Deep neural networks have been widely used in many critical applications,
such as autonomous vehicles and medical diagnosis. However, their security is
threatened by backdoor attack, which is achieved by adding artificial patterns
to specific training data. Existing defense strategies primarily focus on using
reverse engineering to reproduce the backdoor trigger generated by attackers
and subsequently repair the DNN model by adding the trigger into inputs and
fine-tuning the model with ground-truth labels. However, once the trigger
generated by the attackers is complex and invisible, the defender can not
successfully reproduce the trigger. Consequently, the DNN model will not be
repaired since the trigger is not effectively removed.
</p></li>
</ul>

<p>In this work, we propose Feature Map Testing~(FMT). Different from existing
defense strategies, which focus on reproducing backdoor triggers, FMT tries to
detect the backdoor feature maps, which are trained to extract backdoor
information from the inputs. After detecting these backdoor feature maps, FMT
will erase them and then fine-tune the model with a secure subset of training
data. Our experiments demonstrate that, compared to existing defense
strategies, FMT can effectively reduce the Attack Success Rate (ASR) even
against the most complex and invisible attack triggers. Second, unlike
conventional defense methods that tend to exhibit low Robust Accuracy (i.e.,
the model's accuracy on the poisoned data), FMT achieves higher RA, indicating
its superiority in maintaining model performance while mitigating the effects
of backdoor attacks~(e.g., FMT obtains 87.40\% RA in CIFAR10). Third, compared
to existing feature map pruning techniques, FMT can cover more backdoor feature
maps~(e.g., FMT removes 83.33\% of backdoor feature maps from the model in the
CIFAR10 \&amp; BadNet scenario).
</p>

<h2>security</h2>
<h3>Title: Attention Consistency Refined Masked Frequency Forgery Representation for Generalizing Face Forgery Detection. (arXiv:2307.11438v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11438">http://arxiv.org/abs/2307.11438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11438] Attention Consistency Refined Masked Frequency Forgery Representation for Generalizing Face Forgery Detection](http://arxiv.org/abs/2307.11438) #security</code></li>
<li>Summary: <p>Due to the successful development of deep image generation technology, visual
data forgery detection would play a more important role in social and economic
security. Existing forgery detection methods suffer from unsatisfactory
generalization ability to determine the authenticity in the unseen domain. In
this paper, we propose a novel Attention Consistency Refined masked frequency
forgery representation model toward generalizing face forgery detection
algorithm (ACMF). Most forgery technologies always bring in high-frequency
aware cues, which make it easy to distinguish source authenticity but difficult
to generalize to unseen artifact types. The masked frequency forgery
representation module is designed to explore robust forgery cues by randomly
discarding high-frequency information. In addition, we find that the forgery
attention map inconsistency through the detection network could affect the
generalizability. Thus, the forgery attention consistency is introduced to
force detectors to focus on similar attention regions for better generalization
ability. Experiment results on several public face forgery datasets
(FaceForensic++, DFD, Celeb-DF, and WDF datasets) demonstrate the superior
performance of the proposed method compared with the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Formal-Guided Fuzz Testing: Targeting Security Assurance from Specification to Implementation for 5G and Beyond. (arXiv:2307.11247v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11247">http://arxiv.org/abs/2307.11247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11247] Formal-Guided Fuzz Testing: Targeting Security Assurance from Specification to Implementation for 5G and Beyond](http://arxiv.org/abs/2307.11247) #security</code></li>
<li>Summary: <p>Softwarization and virtualization in 5G and beyond necessitate thorough
testing to ensure the security of critical infrastructure and networks,
requiring the identification of vulnerabilities and unintended emergent
behaviors from protocol designs to their software stack implementation. To
provide an efficient and comprehensive solution, we propose a novel and
first-of-its-kind approach that connects the strengths and coverage of formal
and fuzzing methods to efficiently detect vulnerabilities across protocol logic
and implementation stacks in a hierarchical manner. We design and implement
formal verification to detect attack traces in critical protocols, which are
used to guide subsequent fuzz testing and incorporate feedback from fuzz
testing to broaden the scope of formal verification. This innovative approach
significantly improves efficiency and enables the auto-discovery of
vulnerabilities and unintended emergent behaviors from the 3GPP protocols to
software stacks. Following this approach, we discover one identifier leakage
model, one DoS attack model, and two eavesdrop attack models due to the absence
of rudimentary MITM protection within the protocol, despite the existence of a
Transport Layer Security (TLS) solution to this issue for over a decade. More
remarkably, guided by the identified formal analysis and attack models, we
exploit 61 vulnerabilities using fuzz testing demonstrated on srsRAN platforms.
These identified vulnerabilities contribute to fortifying protocol-level
assumptions and refining the search space. Compared to state-of-the-art fuzz
testing, our united formal and fuzzing methodology enables auto-assurance by
systematically discovering vulnerabilities. It significantly reduces
computational complexity, transforming the non-practical exponential growth in
computational cost into linear growth.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: ParGANDA: Making Synthetic Pedestrians A Reality For Object Detection. (arXiv:2307.11360v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11360">http://arxiv.org/abs/2307.11360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11360] ParGANDA: Making Synthetic Pedestrians A Reality For Object Detection](http://arxiv.org/abs/2307.11360) #privacy</code></li>
<li>Summary: <p>Object detection is the key technique to a number of Computer Vision
applications, but it often requires large amounts of annotated data to achieve
decent results. Moreover, for pedestrian detection specifically, the collected
data might contain some personally identifiable information (PII), which is
highly restricted in many countries. This label intensive and privacy
concerning task has recently led to an increasing interest in training the
detection models using synthetically generated pedestrian datasets collected
with a photo-realistic video game engine. The engine is able to generate
unlimited amounts of data with precise and consistent annotations, which gives
potential for significant gains in the real-world applications. However, the
use of synthetic data for training introduces a synthetic-to-real domain shift
aggravating the final performance. To close the gap between the real and
synthetic data, we propose to use a Generative Adversarial Network (GAN), which
performsparameterized unpaired image-to-image translation to generate more
realistic images. The key benefit of using the GAN is its intrinsic preference
of low-level changes to geometric ones, which means annotations of a given
synthetic image remain accurate even after domain translation is performed thus
eliminating the need for labeling real data. We extensively experimented with
the proposed method using MOTSynth dataset to train and MOT17 and MOT20
detection datasets to test, with experimental results demonstrating the
effectiveness of this method. Our approach not only produces visually plausible
samples but also does not require any labels of the real domain thus making it
applicable to the variety of downstream tasks.
</p></li>
</ul>

<h3>Title: Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11469">http://arxiv.org/abs/2307.11469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11469] Distribution Shift Matters for Knowledge Distillation with Webly Collected Images](http://arxiv.org/abs/2307.11469) #privacy</code></li>
<li>Summary: <p>Knowledge distillation aims to learn a lightweight student network from a
pre-trained teacher network. In practice, existing knowledge distillation
methods are usually infeasible when the original training data is unavailable
due to some privacy issues and data management considerations. Therefore,
data-free knowledge distillation approaches proposed to collect training
instances from the Internet. However, most of them have ignored the common
distribution shift between the instances from original training data and webly
collected data, affecting the reliability of the trained student network. To
solve this problem, we propose a novel method dubbed ``Knowledge Distillation
between Different Distributions" (KD$^{3}$), which consists of three
components. Specifically, we first dynamically select useful training instances
from the webly collected data according to the combined predictions of teacher
network and student network. Subsequently, we align both the weighted features
and classifier parameters of the two networks for knowledge memorization.
Meanwhile, we also build a new contrastive learning block called
MixDistribution to generate perturbed data with a new distribution for instance
alignment, so that the student network can further learn a
distribution-invariant representation. Intensive experiments on various
benchmark datasets demonstrate that our proposed KD$^{3}$ can outperform the
state-of-the-art data-free knowledge distillation approaches.
</p></li>
</ul>

<h3>Title: The importance of feature preprocessing for differentially private linear optimization. (arXiv:2307.11106v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11106">http://arxiv.org/abs/2307.11106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11106] The importance of feature preprocessing for differentially private linear optimization](http://arxiv.org/abs/2307.11106) #privacy</code></li>
<li>Summary: <p>Training machine learning models with differential privacy (DP) has received
increasing interest in recent years. One of the most popular algorithms for
training differentially private models is differentially private stochastic
gradient descent (DPSGD) and its variants, where at each step gradients are
clipped and combined with some noise. Given the increasing usage of DPSGD, we
ask the question: is DPSGD alone sufficient to find a good minimizer for every
dataset under privacy constraints? As a first step towards answering this
question, we show that even for the simple case of linear classification,
unlike non-private optimization, (private) feature preprocessing is vital for
differentially private optimization. In detail, we first show theoretically
that there exists an example where without feature preprocessing, DPSGD incurs
a privacy error proportional to the maximum norm of features over all samples.
We then propose an algorithm called DPSGD-F, which combines DPSGD with feature
preprocessing and prove that for classification tasks, it incurs a privacy
error proportional to the diameter of the features $\max_{x, x' \in D} \|x -
x'\|_2$. We then demonstrate the practicality of our algorithm on image
classification benchmarks.
</p></li>
</ul>

<h3>Title: Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11280">http://arxiv.org/abs/2307.11280</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11280] Epsilon*: Privacy Metric for Machine Learning Models](http://arxiv.org/abs/2307.11280) #privacy</code></li>
<li>Summary: <p>We introduce Epsilon<em>, a new privacy metric for measuring the privacy risk of
a single model instance prior to, during, or after deployment of privacy
mitigation strategies. The metric does not require access to the training data
sampling or model training algorithm. Epsilon</em> is a function of true positive
and false positive rates in a hypothesis test used by an adversary in a
membership inference attack. We distinguish between quantifying the privacy
loss of a trained model instance and quantifying the privacy loss of the
training mechanism which produces this model instance. Existing approaches in
the privacy auditing literature provide lower bounds for the latter, while our
metric provides a lower bound for the former by relying on an
(${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained
model instance. We establish a relationship between these lower bounds and show
how to implement Epsilon<em> to avoid numerical and noise amplification
instability. We further show in experiments on benchmark public data sets that
Epsilon</em> is sensitive to privacy risk mitigation by training with differential
privacy (DP), where the value of Epsilon<em> is reduced by up to 800% compared to
the Epsilon</em> values of non-DP trained baseline models. This metric allows
privacy auditors to be independent of model owners, and enables all
decision-makers to visualize the privacy-utility landscape to make informed
decisions regarding the trade-offs between model privacy and utility.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields. (arXiv:2307.11526v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11526">http://arxiv.org/abs/2307.11526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11526] CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields](http://arxiv.org/abs/2307.11526) #protect</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Making Pre-trained Language Models both Task-solvers and Self-calibrators. (arXiv:2307.11316v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11316">http://arxiv.org/abs/2307.11316</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11316] Making Pre-trained Language Models both Task-solvers and Self-calibrators](http://arxiv.org/abs/2307.11316) #defense</code></li>
<li>Summary: <p>Pre-trained language models (PLMs) serve as backbones for various real-world
systems. For high-stake applications, it's equally essential to have reasonable
confidence estimations in predictions. While the vanilla confidence scores of
PLMs can already be effectively utilized, PLMs consistently become
overconfident in their wrong predictions, which is not desirable in practice.
Previous work shows that introducing an extra calibration task can mitigate
this issue. The basic idea involves acquiring additional data to train models
in predicting the confidence of their initial predictions. However, it only
demonstrates the feasibility of this kind of method, assuming that there are
abundant extra available samples for the introduced calibration task. In this
work, we consider the practical scenario that we need to effectively utilize
training samples to make PLMs both task-solvers and self-calibrators. Three
challenges are presented, including limited training samples, data imbalance,
and distribution shifts. We first conduct pilot experiments to quantify various
decisive factors in the calibration task. Based on the empirical analysis
results, we propose a training algorithm LM-TOAST to tackle the challenges.
Experimental results show that LM-TOAST can effectively utilize the training
data to make PLMs have reasonable confidence estimations while maintaining the
original task performance. Further, we consider three downstream applications,
namely selective classification, adversarial defense, and model cascading, to
show the practical usefulness of LM-TOAST. The code will be made public at
\url{https://github.com/Yangyi-Chen/LM-TOAST}.
</p></li>
</ul>

<h3>Title: Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11672">http://arxiv.org/abs/2307.11672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11672] Fast Adaptive Test-Time Defense with Robust Features](http://arxiv.org/abs/2307.11672) #defense</code></li>
<li>Summary: <p>Adaptive test-time defenses are used to improve the robustness of deep neural
networks to adversarial examples. However, existing methods significantly
increase the inference time due to additional optimization on the model
parameters or the input at test time. In this work, we propose a novel adaptive
test-time defense strategy that is easy to integrate with any existing (robust)
training procedure without additional test-time computation. Based on the
notion of robustness of features that we present, the key idea is to project
the trained models to the most robust feature space, thereby reducing the
vulnerability to adversarial attacks in non-robust directions. We theoretically
show that the top eigenspace of the feature matrix are more robust for a
generalized additive model and support our argument for a large width neural
network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive
experiments on CIFAR-10 and CIFAR-100 datasets for several robustness
benchmarks, including the state-of-the-art methods in RobustBench, and observe
that the proposed method outperforms existing adaptive test-time defenses at
much lower computation costs.
</p></li>
</ul>

<h3>Title: Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11730] Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense](http://arxiv.org/abs/2307.11730) #defense</code></li>
<li>Summary: <p>The rise of Decentralized Federated Learning (DFL) has enabled the training
of machine learning models across federated participants, fostering
decentralized model aggregation and reducing dependence on a server. However,
this approach introduces unique communication security challenges that have yet
to be thoroughly addressed in the literature. These challenges primarily
originate from the decentralized nature of the aggregation process, the varied
roles and responsibilities of the participants, and the absence of a central
authority to oversee and mitigate threats. Addressing these challenges, this
paper first delineates a comprehensive threat model, highlighting the potential
risks of DFL communications. In response to these identified risks, this work
introduces a security module designed for DFL platforms to counter
communication-based attacks. The module combines security techniques such as
symmetric and asymmetric encryption with Moving Target Defense (MTD)
techniques, including random neighbor selection and IP/port switching. The
security module is implemented in a DFL platform called Fedstellar, allowing
the deployment and monitoring of the federation. A DFL scenario has been
deployed, involving eight physical devices implementing three security
configurations: (i) a baseline with no security, (ii) an encrypted
configuration, and (iii) a configuration integrating both encryption and MTD
techniques. The effectiveness of the security module is validated through
experiments with the MNIST dataset and eclipse attacks. The results indicated
an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2%
+-3.5%) and network traffic (230 MB +-15 MB) under the most secure
configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Improving Transferability of Adversarial Examples via Bayesian Attacks. (arXiv:2307.11334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11334">http://arxiv.org/abs/2307.11334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11334] Improving Transferability of Adversarial Examples via Bayesian Attacks](http://arxiv.org/abs/2307.11334) #attack</code></li>
<li>Summary: <p>This paper presents a substantial extension of our work published at ICLR.
Our ICLR work advocated for enhancing transferability in adversarial examples
by incorporating a Bayesian formulation into model parameters, which
effectively emulates the ensemble of infinitely many deep neural networks,
while, in this paper, we introduce a novel extension by incorporating the
Bayesian formulation into the model input as well, enabling the joint
diversification of both the model input and model parameters. Our empirical
findings demonstrate that: 1) the combination of Bayesian formulations for both
the model input and model parameters yields significant improvements in
transferability; 2) by introducing advanced approximations of the posterior
distribution over the model input, adversarial transferability achieves further
enhancement, surpassing all state-of-the-arts when attacking without model
fine-tuning. Moreover, we propose a principled approach to fine-tune model
parameters in such an extended Bayesian formulation. The derived optimization
objective inherently encourages flat minima in the parameter space and input
space. Extensive experiments demonstrate that our method achieves a new
state-of-the-art on transfer-based attacks, improving the average success rate
on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with
our ICLR basic Bayesian method. We will make our code publicly available.
</p></li>
</ul>

<h3>Title: OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11729">http://arxiv.org/abs/2307.11729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11729] OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples](http://arxiv.org/abs/2307.11729) #attack</code></li>
<li>Summary: <p>Large Language Models (LLMs) have achieved human-level fluency in text
generation, making it difficult to distinguish between human-written and
LLM-generated texts. This poses a growing risk of misuse of LLMs and demands
the development of detectors to identify LLM-generated texts. However, existing
detectors degrade detection accuracy by simply paraphrasing LLM-generated
texts. Furthermore, the effectiveness of these detectors in real-life
situations, such as when students use LLMs for writing homework assignments
(e.g., essays) and quickly learn how to evade these detectors, has not been
explored. In this paper, we propose OUTFOX, a novel framework that improves the
robustness of LLM-generated-text detectors by allowing both the detector and
the attacker to consider each other's output and apply this to the domain of
student essays. In our framework, the attacker uses the detector's prediction
labels as examples for in-context learning and adversarially generates essays
that are harder to detect. While the detector uses the adversarially generated
essays as examples for in-context learning to learn to detect essays from a
strong attacker. Our experiments show that our proposed detector learned
in-context from the attacker improves the detection performance on the attacked
dataset by up to +41.3 point F1-score. While our proposed attacker can
drastically degrade the performance of the detector by up to -57.0 point
F1-score compared to the paraphrasing method.
</p></li>
</ul>

<h3>Title: RCVaR: an Economic Approach to Estimate Cyberattacks Costs using Data from Industry Reports. (arXiv:2307.11140v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11140">http://arxiv.org/abs/2307.11140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11140] RCVaR: an Economic Approach to Estimate Cyberattacks Costs using Data from Industry Reports](http://arxiv.org/abs/2307.11140) #attack</code></li>
<li>Summary: <p>Digitization increases business opportunities and the risk of companies being
victims of devastating cyberattacks. Therefore, managing risk exposure and
cybersecurity strategies is essential for digitized companies that want to
survive in competitive markets. However, understanding company-specific risks
and quantifying their associated costs is not trivial. Current approaches fail
to provide individualized and quantitative monetary estimations of
cybersecurity impacts. Due to limited resources and technical expertise, SMEs
and even large companies are affected and struggle to quantify their
cyberattack exposure. Therefore, novel approaches must be placed to support the
understanding of the financial loss due to cyberattacks. This article
introduces the Real Cyber Value at Risk (RCVaR), an economical approach for
estimating cybersecurity costs using real-world information from public
cybersecurity reports. RCVaR identifies the most significant cyber risk factors
from various sources and combines their quantitative results to estimate
specific cyberattacks costs for companies. Furthermore, RCVaR extends current
methods to achieve cost and risk estimations based on historical real-world
data instead of only probability-based simulations. The evaluation of the
approach on unseen data shows the accuracy and efficiency of the RCVaR in
predicting and managing cyber risks. Thus, it shows that the RCVaR is a
valuable addition to cybersecurity planning and risk management processes.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: CSSL-RHA: Contrastive Self-Supervised Learning for Robust Handwriting Authentication. (arXiv:2307.11100v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11100">http://arxiv.org/abs/2307.11100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11100] CSSL-RHA: Contrastive Self-Supervised Learning for Robust Handwriting Authentication](http://arxiv.org/abs/2307.11100) #robust</code></li>
<li>Summary: <p>Handwriting authentication is a valuable tool used in various fields, such as
fraud prevention and cultural heritage protection. However, it remains a
challenging task due to the complex features, severe damage, and lack of
supervision. In this paper, we propose a novel Contrastive Self-Supervised
Learning framework for Robust Handwriting Authentication (CSSL-RHA) to address
these issues. It can dynamically learn complex yet important features and
accurately predict writer identities. Specifically, to remove the negative
effects of imperfections and redundancy, we design an information-theoretic
filter for pre-processing and propose a novel adaptive matching scheme to
represent images as patches of local regions dominated by more important
features. Through online optimization at inference time, the most informative
patch embeddings are identified as the "most important" elements. Furthermore,
we employ contrastive self-supervised training with a momentum-based paradigm
to learn more general statistical structures of handwritten data without
supervision. We conduct extensive experiments on five benchmark datasets and
our manually annotated dataset EN-HA, which demonstrate the superiority of our
CSSL-RHA compared to baselines. Additionally, we show that our proposed model
can still effectively achieve authentication even under abnormal circumstances,
such as data falsification and corruption.
</p></li>
</ul>

<h3>Title: Flatness-Aware Minimization for Domain Generalization. (arXiv:2307.11108v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11108">http://arxiv.org/abs/2307.11108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11108] Flatness-Aware Minimization for Domain Generalization](http://arxiv.org/abs/2307.11108) #robust</code></li>
<li>Summary: <p>Domain generalization (DG) seeks to learn robust models that generalize well
under unknown distribution shifts. As a critical aspect of DG, optimizer
selection has not been explored in depth. Currently, most DG methods follow the
widely used benchmark, DomainBed, and utilize Adam as the default optimizer for
all datasets. However, we reveal that Adam is not necessarily the optimal
choice for the majority of current DG methods and datasets. Based on the
perspective of loss landscape flatness, we propose a novel approach,
Flatness-Aware Minimization for Domain Generalization (FAD), which can
efficiently optimize both zeroth-order and first-order flatness simultaneously
for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD)
generalization error and convergence. Our experimental results demonstrate the
superiority of FAD on various DG datasets. Additionally, we confirm that FAD is
capable of discovering flatter optima in comparison to other zeroth-order and
first-order flatness-aware optimization methods.
</p></li>
</ul>

<h3>Title: SimCol3D -- 3D Reconstruction during Colonoscopy Challenge. (arXiv:2307.11261v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11261">http://arxiv.org/abs/2307.11261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11261] SimCol3D -- 3D Reconstruction during Colonoscopy Challenge](http://arxiv.org/abs/2307.11261) #robust</code></li>
<li>Summary: <p>Colorectal cancer is one of the most common cancers in the world. While
colonoscopy is an effective screening technique, navigating an endoscope
through the colon to detect polyps is challenging. A 3D map of the observed
surfaces could enhance the identification of unscreened colon tissue and serve
as a training platform. However, reconstructing the colon from video footage
remains unsolved due to numerous factors such as self-occlusion, reflective
surfaces, lack of texture, and tissue deformation that limit feature-based
methods. Learning-based approaches hold promise as robust alternatives, but
necessitate extensive datasets. By establishing a benchmark, the 2022 EndoVis
sub-challenge SimCol3D aimed to facilitate data-driven depth and pose
prediction during colonoscopy. The challenge was hosted as part of MICCAI 2022
in Singapore. Six teams from around the world and representatives from academia
and industry participated in the three sub-challenges: synthetic depth
prediction, synthetic pose prediction, and real pose prediction. This paper
describes the challenge, the submitted methods, and their results. We show that
depth prediction in virtual colonoscopy is robustly solvable, while pose
estimation remains an open research question.
</p></li>
</ul>

<h3>Title: HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework. (arXiv:2307.11323v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11323">http://arxiv.org/abs/2307.11323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11323] HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework](http://arxiv.org/abs/2307.11323) #robust</code></li>
<li>Summary: <p>In the field of autonomous driving, 3D object detection is a very important
perception module. Although the current SOTA algorithm combines Camera and
Lidar sensors, limited by the high price of Lidar, the current mainstream
landing schemes are pure Camera sensors or Camera+Radar sensors. In this study,
we propose a new detection algorithm called HVDetFusion, which is a multi-modal
detection algorithm that not only supports pure camera data as input for
detection, but also can perform fusion input of radar data and camera data. The
camera stream does not depend on the input of Radar data, thus addressing the
downside of previous methods. In the pure camera stream, we modify the
framework of Bevdet4D for better perception and more efficient inference, and
this stream has the whole 3D detection output. Further, to incorporate the
benefits of Radar signals, we use the prior information of different object
positions to filter the false positive information of the original radar data,
according to the positioning information and radial velocity information
recorded by the radar sensors to supplement and fuse the BEV features generated
by the original camera data, and the effect is further improved in the process
of fusion training. Finally, HVDetFusion achieves the new state-of-the-art
67.4\% NDS on the challenging nuScenes test set among all camera-radar 3D
object detectors. The code is available at
https://github.com/HVXLab/HVDetFusion
</p></li>
</ul>

<h3>Title: Character Time-series Matching For Robust License Plate Recognition. (arXiv:2307.11336v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11336">http://arxiv.org/abs/2307.11336</a></li>
<li>Code URL: <a href="https://github.com/chequanghuy/Character-Time-series-Matching">https://github.com/chequanghuy/Character-Time-series-Matching</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11336] Character Time-series Matching For Robust License Plate Recognition](http://arxiv.org/abs/2307.11336) #robust</code></li>
<li>Summary: <p>Automatic License Plate Recognition (ALPR) is becoming a popular study area
and is applied in many fields such as transportation or smart city. However,
there are still several limitations when applying many current methods to
practical problems due to the variation in real-world situations such as light
changes, unclear License Plate (LP) characters, and image quality. Almost
recent ALPR algorithms process on a single frame, which reduces accuracy in
case of worse image quality. This paper presents methods to improve license
plate recognition accuracy by tracking the license plate in multiple frames.
First, the Adaptive License Plate Rotation algorithm is applied to correctly
align the detected license plate. Second, we propose a method called Character
Time-series Matching to recognize license plate characters from many
consequence frames. The proposed method archives high performance in the
UFPR-ALPR dataset which is \boldmath$96.7\%$ accuracy in real-time on RTX A5000
GPU card. We also deploy the algorithm for the Vietnamese ALPR system. The
accuracy for license plate detection and character recognition are 0.881 and
0.979 $mAP^{test}$@.5 respectively. The source code is available at
https://github.com/chequanghuy/Character-Time-series-Matching.git
</p></li>
</ul>

<h3>Title: Robust Visual Question Answering: Datasets, Methods, and Future Challenges. (arXiv:2307.11471v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11471">http://arxiv.org/abs/2307.11471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11471] Robust Visual Question Answering: Datasets, Methods, and Future Challenges](http://arxiv.org/abs/2307.11471) #robust</code></li>
<li>Summary: <p>Visual question answering requires a system to provide an accurate natural
language answer given an image and a natural language question. However, it is
widely recognized that previous generic VQA methods often exhibit a tendency to
memorize biases present in the training data rather than learning proper
behaviors, such as grounding images before predicting answers. Therefore, these
methods usually achieve high in-distribution but poor out-of-distribution
performance. In recent years, various datasets and debiasing methods have been
proposed to evaluate and enhance the VQA robustness, respectively. This paper
provides the first comprehensive survey focused on this emerging fashion.
Specifically, we first provide an overview of the development process of
datasets from in-distribution and out-of-distribution perspectives. Then, we
examine the evaluation metrics employed by these datasets. Thirdly, we propose
a typology that presents the development process, similarities and differences,
robustness comparison, and technical features of existing debiasing methods.
Furthermore, we analyze and discuss the robustness of representative
vision-and-language pre-training models on VQA. Finally, through a thorough
review of the available literature and experimental analysis, we discuss the
key areas for future research from various viewpoints.
</p></li>
</ul>

<h3>Title: Improving Viewpoint Robustness for Visual Recognition via Adversarial Training. (arXiv:2307.11528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11528">http://arxiv.org/abs/2307.11528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11528] Improving Viewpoint Robustness for Visual Recognition via Adversarial Training](http://arxiv.org/abs/2307.11528) #robust</code></li>
<li>Summary: <p>Viewpoint invariance remains challenging for visual recognition in the 3D
world, as altering the viewing directions can significantly impact predictions
for the same object. While substantial efforts have been dedicated to making
neural networks invariant to 2D image translations and rotations, viewpoint
invariance is rarely investigated. Motivated by the success of adversarial
training in enhancing model robustness, we propose Viewpoint-Invariant
Adversarial Training (VIAT) to improve the viewpoint robustness of image
classifiers. Regarding viewpoint transformation as an attack, we formulate VIAT
as a minimax optimization problem, where the inner maximization characterizes
diverse adversarial viewpoints by learning a Gaussian mixture distribution
based on the proposed attack method GMVFool. The outer minimization obtains a
viewpoint-invariant classifier by minimizing the expected loss over the
worst-case viewpoint distributions that can share the same one for different
objects within the same category. Based on GMVFool, we contribute a large-scale
dataset called ImageNet-V+ to benchmark viewpoint robustness. Experimental
results show that VIAT significantly improves the viewpoint robustness of
various image classifiers based on the diversity of adversarial viewpoints
generated by GMVFool. Furthermore, we propose ViewRS, a certified viewpoint
robustness method that provides a certified radius and accuracy to demonstrate
the effectiveness of VIAT from the theoretical perspective.
</p></li>
</ul>

<h3>Title: Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text. (arXiv:2307.11380v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11380">http://arxiv.org/abs/2307.11380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11380] Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text](http://arxiv.org/abs/2307.11380) #robust</code></li>
<li>Summary: <p>The remarkable capabilities of large-scale language models, such as ChatGPT,
in text generation have incited awe and spurred researchers to devise detectors
to mitigate potential risks, including misinformation, phishing, and academic
dishonesty. Despite this, most previous studies, including HC3, have been
predominantly geared towards creating detectors that differentiate between
purely ChatGPT-generated texts and human-authored texts. This approach,
however, fails to work on discerning texts generated through human-machine
collaboration, such as ChatGPT-polished texts. Addressing this gap, we
introduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts),
facilitating the construction of more robust detectors. It diverges from extant
corpora by comprising pairs of human-written and ChatGPT-polished abstracts
instead of purely ChatGPT-generated texts. Additionally, we propose the "Polish
Ratio" method, an innovative measure of ChatGPT's involvement in text
generation based on editing distance. It provides a mechanism to measure the
degree of human originality in the resulting text. Our experimental results
show our proposed model has better robustness on the HPPT dataset and two
existing datasets (HC3 and CDB). Furthermore, the "Polish Ratio" we proposed
offers a more comprehensive explanation by quantifying the degree of ChatGPT
involvement, which indicates that a Polish Ratio value greater than 0.2
signifies ChatGPT involvement and a value exceeding 0.6 implies that ChatGPT
generates most of the text.
</p></li>
</ul>

<h3>Title: WM-NET: Robust Deep 3D Watermarking with Limited Data. (arXiv:2307.11628v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11628">http://arxiv.org/abs/2307.11628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11628] WM-NET: Robust Deep 3D Watermarking with Limited Data](http://arxiv.org/abs/2307.11628) #robust</code></li>
<li>Summary: <p>The goal of 3D mesh watermarking is to embed the message in 3D meshes that
can withstand various attacks imperceptibly and reconstruct the message
accurately from watermarked meshes. Traditional methods are less robust against
attacks. Recent DNN-based methods either introduce excessive distortions or
fail to embed the watermark without the help of texture information. However,
embedding the watermark in textures is insecure because replacing the texture
image can completely remove the watermark. In this paper, we propose a robust
deep 3D mesh watermarking WM-NET, which leverages attention-based convolutions
in watermarking tasks to embed binary messages in vertex distributions without
texture assistance. Furthermore, our WM-NET exploits the property that
simplified meshes inherit similar relations from the original ones, where the
relation is the offset vector directed from one vertex to its neighbor. By
doing so, our method can be trained on simplified meshes(limited data) but
remains effective on large-sized meshes (size adaptable) and unseen categories
of meshes (geometry adaptable). Extensive experiments demonstrate our method
brings 50% fewer distortions and 10% higher bit accuracy compared to previous
work. Our watermark WM-NET is robust against various mesh attacks, e.g. Gauss,
rotation, translation, scaling, and cropping.
</p></li>
</ul>

<h3>Title: Using simulation to calibrate real data acquisition in veterinary medicine. (arXiv:2307.11695v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11695">http://arxiv.org/abs/2307.11695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11695] Using simulation to calibrate real data acquisition in veterinary medicine](http://arxiv.org/abs/2307.11695) #robust</code></li>
<li>Summary: <p>This paper explores the innovative use of simulation environments to enhance
data acquisition and diagnostics in veterinary medicine, focusing specifically
on gait analysis in dogs. The study harnesses the power of Blender and the
Blenderproc library to generate synthetic datasets that reflect diverse
anatomical, environmental, and behavioral conditions. The generated data,
represented in graph form and standardized for optimal analysis, is utilized to
train machine learning algorithms for identifying normal and abnormal gaits.
Two distinct datasets with varying degrees of camera angle granularity are
created to further investigate the influence of camera perspective on model
accuracy. Preliminary results suggest that this simulation-based approach holds
promise for advancing veterinary diagnostics by enabling more precise data
acquisition and more effective machine learning models. By integrating
synthetic and real-world patient data, the study lays a robust foundation for
improving overall effectiveness and efficiency in veterinary medicine.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Towards General Game Representations: Decomposing Games Pixels into Content and Style. (arXiv:2307.11141v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11141">http://arxiv.org/abs/2307.11141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11141] Towards General Game Representations: Decomposing Games Pixels into Content and Style](http://arxiv.org/abs/2307.11141) #extraction</code></li>
<li>Summary: <p>On-screen game footage contains rich contextual information that players
process when playing and experiencing a game. Learning pixel representations of
games can benefit artificial intelligence across several downstream tasks
including game-playing agents, procedural content generation, and player
modelling. The generalizability of these methods, however, remains a challenge,
as learned representations should ideally be shared across games with similar
game mechanics. This could allow, for instance, game-playing agents trained on
one game to perform well in similar games with no re-training. This paper
explores how generalizable pre-trained computer vision encoders can be for such
tasks, by decomposing the latent space into content embeddings and style
embeddings. The goal is to minimize the domain gap between games of the same
genre when it comes to game content critical for downstream tasks, and ignore
differences in graphical style. We employ a pre-trained Vision Transformer
encoder and a decomposition technique based on game genres to obtain separate
content and style embeddings. Our findings show that the decomposed embeddings
achieve style invariance across multiple games while still maintaining strong
content extraction capabilities. We argue that the proposed decomposition of
content and style offers better generalization capacities across game
environments independently of the downstream task.
</p></li>
</ul>

<h3>Title: UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models. (arXiv:2307.11227v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11227">http://arxiv.org/abs/2307.11227</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11227] UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models](http://arxiv.org/abs/2307.11227) #extraction</code></li>
<li>Summary: <p>In this study, we investigate the task of data pre-selection, which aims to
select instances for labeling from an unlabeled dataset through a single pass,
thereby optimizing performance for undefined downstream tasks with a limited
annotation budget. Previous approaches to data pre-selection relied solely on
visual features extracted from foundation models, such as CLIP and BLIP-2, but
largely ignored the powerfulness of text features. In this work, we argue that,
with proper design, the joint feature space of both vision and text can yield a
better representation for data pre-selection. To this end, we introduce UP-DP,
a simple yet effective unsupervised prompt learning approach that adapts
vision-language models, like BLIP-2, for data pre-selection. Specifically, with
the BLIP-2 parameters frozen, we train text prompts to extract the joint
features with improved representation, ensuring a diverse cluster structure
that covers the entire dataset. We extensively compare our method with the
state-of-the-art using seven benchmark datasets in different settings,
achieving up to a performance gain of 20%. Interestingly, the prompts learned
from one dataset demonstrate significant generalizability and can be applied
directly to enhance the feature extraction of BLIP-2 from other datasets. To
the best of our knowledge, UP-DP is the first work to incorporate unsupervised
prompt learning in a vision-language model for data pre-selection.
</p></li>
</ul>

<h3>Title: Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11643">http://arxiv.org/abs/2307.11643</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11643] Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models](http://arxiv.org/abs/2307.11643) #extraction</code></li>
<li>Summary: <p>As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model's
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: MAS: Towards Resource-Efficient Federated Multiple-Task Learning. (arXiv:2307.11285v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11285">http://arxiv.org/abs/2307.11285</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11285] MAS: Towards Resource-Efficient Federated Multiple-Task Learning](http://arxiv.org/abs/2307.11285) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous FL tasks could overload resource-constrained devices. In
this work, we propose the first FL system to effectively coordinate and train
multiple simultaneous FL tasks. We first formalize the problem of training
simultaneous FL tasks. Then, we present our new approach, MAS (Merge and
Split), to optimize the performance of training multiple simultaneous FL tasks.
MAS starts by merging FL tasks into an all-in-one FL task with a multi-task
architecture. After training for a few rounds, MAS splits the all-in-one FL
task into two or more FL tasks by using the affinities among tasks measured
during the all-in-one training. It then continues training each split of FL
tasks based on model parameters from the all-in-one training. Extensive
experiments demonstrate that MAS outperforms other methods while reducing
training time by 2x and reducing energy consumption by 40%. We hope this work
will inspire the community to further study and optimize training simultaneous
FL tasks.
</p></li>
</ul>

<h3>Title: A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11254">http://arxiv.org/abs/2307.11254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11254] A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing](http://arxiv.org/abs/2307.11254) #federate</code></li>
<li>Summary: <p>Language models (LMs) like BERT and GPT have revolutionized natural language
processing (NLP). However, privacy-sensitive domains, particularly the medical
field, face challenges to train LMs due to limited data access and privacy
constraints imposed by regulations like the Health Insurance Portability and
Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR).
Federated learning (FL) offers a decentralized solution that enables
collaborative learning while ensuring the preservation of data privacy. In this
study, we systematically evaluate FL in medicine across $2$ biomedical NLP
tasks using $6$ LMs encompassing $8$ corpora. Our results showed that: 1) FL
models consistently outperform LMs trained on individual client's data and
sometimes match the model trained with polled data; 2) With the fixed number of
total data, LMs trained using FL with more clients exhibit inferior
performance, but pre-trained transformer-based models exhibited greater
resilience. 3) LMs trained using FL perform nearly on par with the model
trained with pooled data when clients' data are IID distributed while
exhibiting visible gaps with non-IID data. Our code is available at:
https://github.com/PL97/FedNLP
</p></li>
</ul>

<h3>Title: Differentially Private Heavy Hitter Detection using Federated Analytics. (arXiv:2307.11749v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11749">http://arxiv.org/abs/2307.11749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11749] Differentially Private Heavy Hitter Detection using Federated Analytics](http://arxiv.org/abs/2307.11749) #federate</code></li>
<li>Summary: <p>In this work, we study practical heuristics to improve the performance of
prefix-tree based algorithms for differentially private heavy hitter detection.
Our model assumes each user has multiple data points and the goal is to learn
as many of the most frequent data points as possible across all users' data
with aggregate and local differential privacy. We propose an adaptive
hyperparameter tuning algorithm that improves the performance of the algorithm
while satisfying computational, communication and privacy constraints. We
explore the impact of different data-selection schemes as well as the impact of
introducing deny lists during multiple runs of the algorithm. We test these
improvements using extensive experimentation on the Reddit
dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
</p></li>
</ul>

<h3>Title: Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition. (arXiv:2307.11333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11333">http://arxiv.org/abs/2307.11333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11333] Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition](http://arxiv.org/abs/2307.11333) #federate</code></li>
<li>Summary: <p>In this paper, we present an information-theoretic perspective to group
fairness trade-offs in federated learning (FL) with respect to sensitive
attributes, such as gender, race, etc. Existing works mostly focus on either
\emph{global fairness} (overall disparity of the model across all clients) or
\emph{local fairness} (disparity of the model at each individual client),
without always considering their trade-offs. There is a lack of understanding
of the interplay between global and local fairness in FL, and if and when one
implies the other. To address this gap, we leverage a body of work in
information theory called partial information decomposition (PID) which first
identifies three sources of unfairness in FL, namely, \emph{Unique Disparity},
\emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical
examples, we demonstrate how these three disparities contribute to global and
local fairness. This decomposition helps us derive fundamental limits and
trade-offs between global or local fairness, particularly under data
heterogeneity, as well as, derive conditions under which one implies the other.
We also present experimental results on benchmark datasets to support our
theoretical findings. This work offers a more nuanced understanding of the
sources of disparity in FL that can inform the use of local disparity
mitigation techniques, and their convergence and effectiveness when deployed in
practice.
</p></li>
</ul>

<h3>Title: Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning. (arXiv:2307.11532v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11532">http://arxiv.org/abs/2307.11532</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11532] Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning](http://arxiv.org/abs/2307.11532) #federate</code></li>
<li>Summary: <p>To alleviate the shortage of computing power faced by clients in training
deep neural networks (DNNs) using federated learning (FL), we leverage the edge
computing and split learning to propose a model-splitting allowed FL (SFL)
framework, with the aim to minimize the training latency without loss of test
accuracy. Under the synchronized global update setting, the latency to complete
a round of global training is determined by the maximum latency for the clients
to complete a local training session. Therefore, the training latency
minimization problem (TLMP) is modelled as a minimizing-maximum problem. To
solve this mixed integer nonlinear programming problem, we first propose a
regression method to fit the quantitative-relationship between the cut-layer
and other parameters of an AI-model, and thus, transform the TLMP into a
continuous problem. Considering that the two subproblems involved in the TLMP,
namely, the cut-layer selection problem for the clients and the computing
resource allocation problem for the parameter-server are relative independence,
an alternate-optimization-based algorithm with polynomial time complexity is
developed to obtain a high-quality solution to the TLMP. Extensive experiments
are performed on a popular DNN-model EfficientNetV2 using dataset MNIST, and
the results verify the validity and improved performance of the proposed SFL
framework.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11413">http://arxiv.org/abs/2307.11413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11413] A Video-based Detector for Suspicious Activity in Examination with OpenPose](http://arxiv.org/abs/2307.11413) #fair</code></li>
<li>Summary: <p>Examinations are a crucial part of the learning process, and academic
institutions invest significant resources into maintaining their integrity by
preventing cheating from students or facilitators. However, cheating has become
rampant in examination setups, compromising their integrity. The traditional
method of relying on invigilators to monitor every student is impractical and
ineffective. To address this issue, there is a need to continuously record exam
sessions to monitor students for suspicious activities. However, these
recordings are often too lengthy for invigilators to analyze effectively, and
fatigue may cause them to miss significant details. To widen the coverage,
invigilators could use fixed overhead or wearable cameras. This paper
introduces a framework that uses automation to analyze videos and detect
suspicious activities during examinations efficiently and effectively. We
utilized the OpenPose framework and Convolutional Neural Network (CNN) to
identify students exchanging objects during exams. This detection system is
vital in preventing cheating and promoting academic integrity, fairness, and
quality education for institutions.
</p></li>
</ul>

<h3>Title: FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification. (arXiv:2307.11654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11654">http://arxiv.org/abs/2307.11654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11654] FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification](http://arxiv.org/abs/2307.11654) #fair</code></li>
<li>Summary: <p>Skin diseases affect millions of people worldwide, across all ethnicities.
Increasing diagnosis accessibility requires fair and accurate segmentation and
classification of dermatology images. However, the scarcity of annotated
medical images, especially for rare diseases and underrepresented skin tones,
poses a challenge to the development of fair and accurate models. In this
study, we introduce a Fair, Efficient, and Diverse Diffusion-based framework
for skin lesion segmentation and malignancy classification. FEDD leverages
semantically meaningful feature embeddings learned through a denoising
diffusion probabilistic backbone and processes them via linear probes to
achieve state-of-the-art performance on Diverse Dermatology Images (DDI). We
achieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07
while using only 5%, 10%, 15%, and 20% labeled samples, respectively.
Additionally, FEDD trained on 10% of DDI demonstrates malignancy classification
accuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high
efficiency in data-constrained scenarios while providing fair performance for
diverse skin tones and rare malignancy conditions. Our newly annotated DDI
segmentation masks and training code can be found on
https://github.com/hectorcarrion/fedd.
</p></li>
</ul>

<h3>Title: FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation. (arXiv:2307.11214v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11214">http://arxiv.org/abs/2307.11214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11214] FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation](http://arxiv.org/abs/2307.11214) #fair</code></li>
<li>Summary: <p>Generating realistic human flows across regions is essential for our
understanding of urban structures and population activity patterns, enabling
important applications in the fields of urban planning and management. However,
a notable shortcoming of most existing mobility generation methodologies is
neglect of prediction fairness, which can result in underestimation of mobility
flows across regions with vulnerable population groups, potentially resulting
in inequitable resource distribution and infrastructure development. To
overcome this limitation, our study presents a novel, fairness-aware deep
learning model, FairMobi-Net, for inter-region human flow prediction. The
FairMobi-Net model uniquely incorporates fairness loss into the loss function
and employs a hybrid approach, merging binary classification and numerical
regression techniques for human flow prediction. We validate the FairMobi-Net
model using comprehensive human mobility datasets from four U.S. cities,
predicting human flow at the census-tract level. Our findings reveal that the
FairMobi-Net model outperforms state-of-the-art models (such as the DeepGravity
model) in producing more accurate and equitable human flow predictions across a
variety of region pairs, regardless of regional income differences. The model
maintains a high degree of accuracy consistently across diverse regions,
addressing the previous fairness concern. Further analysis of feature
importance elucidates the impact of physical distances and road network
structures on human flows across regions. With fairness as its touchstone, the
model and results provide researchers and practitioners across the fields of
urban sciences, transportation engineering, and computing with an effective
tool for accurate generation of human mobility flows across regions.
</p></li>
</ul>

<h3>Title: Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework. (arXiv:2307.11379v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11379">http://arxiv.org/abs/2307.11379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11379] Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework](http://arxiv.org/abs/2307.11379) #fair</code></li>
<li>Summary: <p>Machine learning is widely used to make decisions with societal impact such
as bank loan approving, criminal sentencing, and resume filtering. How to
ensure its fairness while maintaining utility is a challenging but crucial
issue. Fairness is a complex and context-dependent concept with over 70
different measurement metrics. Since existing regulations are often vague in
terms of which metric to use and different organizations may prefer different
fairness metrics, it is important to have means of improving fairness
comprehensively. Existing mitigation techniques often target at one specific
fairness metric and have limitations in improving multiple notions of fairness
simultaneously. In this work, we propose CFU (Comprehensive Fairness-Utility),
a reinforcement learning-based framework, to efficiently improve the
fairness-utility trade-off in machine learning classifiers. A comprehensive
measurement that can simultaneously consider multiple fairness notions as well
as utility is established, and new metrics are proposed based on an in-depth
analysis of the relationship between different fairness metrics. The reward
function of CFU is constructed with comprehensive measurement and new metrics.
We conduct extensive experiments to evaluate CFU on 6 tasks, 3 machine learning
models, and 15 fairness-utility measurements. The results demonstrate that CFU
can improve the classifier on multiple fairness metrics without sacrificing its
utility. It outperforms all state-of-the-art techniques and has witnessed a
37.5% improvement on average.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Advancing Visual Grounding with Scene Knowledge: Benchmark and Method. (arXiv:2307.11558v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11558">http://arxiv.org/abs/2307.11558</a></li>
<li>Code URL: <a href="https://github.com/zhjohnchan/sk-vg">https://github.com/zhjohnchan/sk-vg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11558] Advancing Visual Grounding with Scene Knowledge: Benchmark and Method](http://arxiv.org/abs/2307.11558) #interpretability</code></li>
<li>Summary: <p>Visual grounding (VG) aims to establish fine-grained alignment between vision
and language. Ideally, it can be a testbed for vision-and-language models to
evaluate their understanding of the images and texts and their reasoning
abilities over their joint space. However, most existing VG datasets are
constructed using simple description texts, which do not require sufficient
reasoning over the images and texts. This has been demonstrated in a recent
study~\cite{luo2022goes}, where a simple LSTM-based text encoder without
pretraining can achieve state-of-the-art performance on mainstream VG datasets.
Therefore, in this paper, we propose a novel benchmark of \underline{S}cene
\underline{K}nowledge-guided \underline{V}isual \underline{G}rounding (SK-VG),
where the image content and referring expressions are not sufficient to ground
the target objects, forcing the models to have a reasoning ability on the
long-form scene knowledge. To perform this task, we propose two approaches to
accept the triple-type input, where the former embeds knowledge into the image
features before the image-query interaction; the latter leverages linguistic
structure to assist in computing the image-text matching. We conduct extensive
experiments to analyze the above methods and show that the proposed approaches
achieve promising results but still leave room for improvement, including
performance and interpretability. The dataset and code are available at
\url{https://github.com/zhjohnchan/SK-VG}.
</p></li>
</ul>

<h3>Title: Interpretable Graph Networks Formulate Universal Algebra Conjectures. (arXiv:2307.11688v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11688">http://arxiv.org/abs/2307.11688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11688] Interpretable Graph Networks Formulate Universal Algebra Conjectures](http://arxiv.org/abs/2307.11688) #interpretability</code></li>
<li>Summary: <p>The rise of Artificial Intelligence (AI) recently empowered researchers to
investigate hard mathematical problems which eluded traditional approaches for
decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields
laying the foundations of modern mathematics -- is still completely unexplored.
This work proposes the first use of AI to investigate UA's conjectures with an
equivalent equational and topological characterization. While topological
representations would enable the analysis of such properties using graph neural
networks, the limited transparency and brittle explainability of these models
hinder their straightforward use to empirically validate existing conjectures
or to formulate new ones. To bridge these gaps, we propose a general algorithm
generating AI-ready datasets based on UA's conjectures, and introduce a novel
neural layer to build fully interpretable graph networks. The results of our
experiments demonstrate that interpretable graph networks: (i) enhance
interpretability without sacrificing task accuracy, (ii) strongly generalize
when predicting universal algebra's properties, (iii) generate simple
explanations that empirically validate existing conjectures, and (iv) identify
subgraphs suggesting the formulation of novel conjectures.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?. (arXiv:2307.11636v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11636">http://arxiv.org/abs/2307.11636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11636] OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?](http://arxiv.org/abs/2307.11636) #explainability</code></li>
<li>Summary: <p>This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale
dataset for humour generation and understanding. Humour is an abstract,
subjective, and context-dependent cognitive construct involving several
cognitive factors, making it a challenging task to generate and interpret.
Hence, humour generation and understanding can serve as a new task for
evaluating the ability of deep-learning methods to process abstract and
subjective information. Due to the scarcity of data, humour-related generation
tasks such as captioning remain under-explored. To address this gap,
OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to
train a generalizable humour captioning model. Contrary to existing captioning
datasets, OxfordTVG-HIC features a wide range of emotional and semantic
diversity resulting in out-of-context examples that are particularly conducive
to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive
content. We also show how OxfordTVG-HIC can be leveraged for evaluating the
humour of a generated text. Through explainability analysis of the trained
models, we identify the visual and linguistic cues influential for evoking
humour prediction (and generation). We observe qualitatively that these cues
are aligned with the benign violation theory of humour in cognitive psychology.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Diffusion Sampling with Momentum for Mitigating Divergence Artifacts. (arXiv:2307.11118v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11118">http://arxiv.org/abs/2307.11118</a></li>
<li>Code URL: <a href="https://github.com/sWizad/momentum-diffusion">https://github.com/sWizad/momentum-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11118] Diffusion Sampling with Momentum for Mitigating Divergence Artifacts](http://arxiv.org/abs/2307.11118) #diffusion</code></li>
<li>Summary: <p>Despite the remarkable success of diffusion models in image generation, slow
sampling remains a persistent issue. To accelerate the sampling process, prior
studies have reformulated diffusion sampling as an ODE/SDE and introduced
higher-order numerical methods. However, these methods often produce divergence
artifacts, especially with a low number of sampling steps, which limits the
achievable acceleration. In this paper, we investigate the potential causes of
these artifacts and suggest that the small stability regions of these methods
could be the principal cause. To address this issue, we propose two novel
techniques. The first technique involves the incorporation of Heavy Ball (HB)
momentum, a well-known technique for improving optimization, into existing
diffusion numerical methods to expand their stability regions. We also prove
that the resulting methods have first-order convergence. The second technique,
called Generalized Heavy Ball (GHVB), constructs a new high-order method that
offers a variable trade-off between accuracy and artifact suppression.
Experimental results show that our techniques are highly effective in reducing
artifacts and improving image quality, surpassing state-of-the-art diffusion
solvers on both pixel-based and latent-based diffusion models for low-step
sampling. Our research provides novel insights into the design of numerical
methods for future diffusion work.
</p></li>
</ul>

<h3>Title: DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport. (arXiv:2307.11308v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11308">http://arxiv.org/abs/2307.11308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11308] DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport](http://arxiv.org/abs/2307.11308) #diffusion</code></li>
<li>Summary: <p>Sampling from diffusion probabilistic models (DPMs) can be viewed as a
piecewise distribution transformation, which generally requires hundreds or
thousands of steps of the inverse diffusion trajectory to get a high-quality
image. Recent progress in designing fast samplers for DPMs achieves a trade-off
between sampling speed and sample quality by knowledge distillation or
adjusting the variance schedule or the denoising equation. However, it can't be
optimal in both aspects and often suffer from mode mixture in short steps. To
tackle this problem, we innovatively regard inverse diffusion as an optimal
transport (OT) problem between latents at different stages and propose the
DPM-OT, a unified learning framework for fast DPMs with a direct expressway
represented by OT map, which can generate high-quality samples within around 10
function evaluations. By calculating the semi-discrete optimal transport map
between the data latents and the white noise, we obtain an expressway from the
prior distribution to the data distribution, while significantly alleviating
the problem of mode mixture. In addition, we give the error bound of the
proposed method, which theoretically guarantees the stability of the algorithm.
Extensive experiments validate the effectiveness and advantages of DPM-OT in
terms of speed and quality (FID and mode mixture), thus representing an
efficient solution for generative modeling. Source codes are available at
https://github.com/cognaclee/DPM-OT
</p></li>
</ul>

<h3>Title: Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning. (arXiv:2307.11410v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11410">http://arxiv.org/abs/2307.11410</a></li>
<li>Code URL: <a href="https://github.com/OPPO-Mente-Lab/Subject-Diffusion">https://github.com/OPPO-Mente-Lab/Subject-Diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11410] Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning](http://arxiv.org/abs/2307.11410) #diffusion</code></li>
<li>Summary: <p>Recent progress in personalized image generation using diffusion models has
been significant. However, development in the area of open-domain and
non-fine-tuning personalized image generation is proceeding rather slowly. In
this paper, we propose Subject-Diffusion, a novel open-domain personalized
image generation model that, in addition to not requiring test-time
fine-tuning, also only requires a single reference image to support
personalized generation of single- or multi-subject in any domain. Firstly, we
construct an automatic data labeling tool and use the LAION-Aesthetics dataset
to construct a large-scale dataset consisting of 76M images and their
corresponding subject detection bounding boxes, segmentation masks and text
descriptions. Secondly, we design a new unified framework that combines text
and image semantics by incorporating coarse location and fine-grained reference
image control to maximize subject fidelity and generalization. Furthermore, we
also adopt an attention control mechanism to support multi-subject generation.
Extensive qualitative and quantitative results demonstrate that our method
outperforms other SOTA frameworks in single, multiple, and human customized
image generation. Please refer to our
\href{https://oppo-mente-lab.github.io/subject_diffusion/}{project page}
</p></li>
</ul>

<h3>Title: QDC: Quantum Diffusion Convolution Kernels on Graphs. (arXiv:2307.11234v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11234">http://arxiv.org/abs/2307.11234</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11234] QDC: Quantum Diffusion Convolution Kernels on Graphs](http://arxiv.org/abs/2307.11234) #diffusion</code></li>
<li>Summary: <p>Graph convolutional neural networks (GCNs) operate by aggregating messages
over local neighborhoods given the prediction task under interest. Many GCNs
can be understood as a form of generalized diffusion of input features on the
graph, and significant work has been dedicated to improving predictive accuracy
by altering the ways of message passing. In this work, we propose a new
convolution kernel that effectively rewires the graph according to the
occupation correlations of the vertices by trading on the generalized diffusion
paradigm for the propagation of a quantum particle over the graph. We term this
new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In
addition, we introduce a multiscale variant that combines messages from the QDC
operator and the traditional combinatorial Laplacian. To understand our method,
we explore the spectral dependence of homophily and the importance of quantum
dynamics in the construction of a bandpass filter. Through these studies, as
well as experiments on a range of datasets, we observe that QDC improves
predictive performance on the widely used benchmark datasets when compared to
similar methods.
</p></li>
</ul>

<h3>Title: Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11494">http://arxiv.org/abs/2307.11494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11494] Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting](http://arxiv.org/abs/2307.11494) #diffusion</code></li>
<li>Summary: <p>Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Comparison between transformers and convolutional models for fine-grained classification of insects. (arXiv:2307.11112v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11112">http://arxiv.org/abs/2307.11112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11112] Comparison between transformers and convolutional models for fine-grained classification of insects](http://arxiv.org/abs/2307.11112) #transformer</code></li>
<li>Summary: <p>Fine-grained classification is challenging due to the difficulty of finding
discriminatory features. This problem is exacerbated when applied to
identifying species within the same taxonomical class. This is because species
are often sharing morphological characteristics that make them difficult to
differentiate. We consider the taxonomical class of Insecta. The identification
of insects is essential in biodiversity monitoring as they are one of the
inhabitants at the base of many ecosystems. Citizen science is doing brilliant
work of collecting images of insects in the wild giving the possibility to
experts to create improved distribution maps in all countries. We have billions
of images that need to be automatically classified and deep neural network
algorithms are one of the main techniques explored for fine-grained tasks. At
the SOTA, the field of deep learning algorithms is extremely fruitful, so how
to identify the algorithm to use? We focus on Odonata and Coleoptera orders,
and we propose an initial comparative study to analyse the two best-known layer
structures for computer vision: transformer and convolutional layers. We
compare the performance of T2TViT, a fully transformer-base, EfficientNet, a
fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of
the three models in identical conditions evaluating the performance per
species, per morph together with sex, the inference time, and the overall
performance with unbalanced datasets of images from smartphones. Although we
observe high performances with all three families of models, our analysis shows
that the hybrid model outperforms the fully convolutional-base and fully
transformer-base models on accuracy performance and the fully transformer-base
model outperforms the others on inference speed and, these prove the
transformer to be robust to the shortage of samples and to be faster at
inference time.
</p></li>
</ul>

<h3>Title: Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition. (arXiv:2307.11404v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11404">http://arxiv.org/abs/2307.11404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11404] Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition](http://arxiv.org/abs/2307.11404) #transformer</code></li>
<li>Summary: <p>Most research on facial expression recognition (FER) is conducted in highly
controlled environments, but its performance is often unacceptable when applied
to real-world situations. This is because when unexpected objects occlude the
face, the FER network faces difficulties extracting facial features and
accurately predicting facial expressions. Therefore, occluded FER (OFER) is a
challenging problem. Previous studies on occlusion-aware FER have typically
required fully annotated facial images for training. However, collecting facial
images with various occlusions and expression annotations is time-consuming and
expensive. Latent-OFER, the proposed method, can detect occlusions, restore
occluded parts of the face as if they were unoccluded, and recognize them,
improving FER accuracy. This approach involves three steps: First, the vision
transformer (ViT)-based occlusion patch detector masks the occluded position by
training only latent vectors from the unoccluded patches using the support
vector data description algorithm. Second, the hybrid reconstruction network
generates the masking position as a complete image using the ViT and
convolutional neural network (CNN). Last, the expression-relevant latent vector
extractor retrieves and uses expression-related information from all latent
vectors by applying a CNN-based class activation map. This mechanism has a
significant advantage in preventing performance degradation from occlusion by
unseen objects. The experimental results on several databases demonstrate the
superiority of the proposed method over state-of-the-art methods.
</p></li>
</ul>

<h3>Title: YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation. (arXiv:2307.11550v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11550">http://arxiv.org/abs/2307.11550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11550] YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation](http://arxiv.org/abs/2307.11550) #transformer</code></li>
<li>Summary: <p>6D object pose estimation is a crucial prerequisite for autonomous robot
manipulation applications. The state-of-the-art models for pose estimation are
convolutional neural network (CNN)-based. Lately, Transformers, an architecture
originally proposed for natural language processing, is achieving
state-of-the-art results in many computer vision tasks as well. Equipped with
the multi-head self-attention mechanism, Transformers enable simple
single-stage end-to-end architectures for learning object detection and 6D
object pose estimation jointly. In this work, we propose YOLOPose (short form
for You Only Look Once Pose estimation), a Transformer-based multi-object 6D
pose estimation method based on keypoint regression and an improved variant of
the YOLOPose model. In contrast to the standard heatmaps for predicting
keypoints in an image, we directly regress the keypoints. Additionally, we
employ a learnable orientation estimation module to predict the orientation
from the keypoints. Along with a separate translation estimation module, our
model is end-to-end differentiable. Our method is suitable for real-time
applications and achieves results comparable to state-of-the-art methods. We
analyze the role of object queries in our architecture and reveal that the
object queries specialize in detecting objects in specific image regions.
Furthermore, we quantify the accuracy trade-off of using datasets of smaller
sizes to train our model.
</p></li>
</ul>

<h3>Title: SACReg: Scene-Agnostic Coordinate Regression for Visual Localization. (arXiv:2307.11702v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11702">http://arxiv.org/abs/2307.11702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11702] SACReg: Scene-Agnostic Coordinate Regression for Visual Localization](http://arxiv.org/abs/2307.11702) #transformer</code></li>
<li>Summary: <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every
pixel of a given image, has recently shown promising potential. However,
existing methods remain mostly scene-specific or limited to small scenes and
thus hardly scale to realistic datasets. In this paper, we propose a new
paradigm where a single generic SCR model is trained once to be then deployed
to new test scenes, regardless of their scale and without further finetuning.
For a given query image, it collects inputs from off-the-shelf image retrieval
techniques and Structure-from-Motion databases: a list of relevant database
images with sparse pointwise 2D-3D annotations. The model is based on the
transformer architecture and can take a variable number of images and sparse
2D-3D annotations as input. It is trained on a few diverse datasets and
significantly outperforms other scene regression approaches on several
benchmarks, including scene-specific models, for visual localization. In
particular, we set a new state of the art on the Cambridge localization
benchmark, even outperforming feature-matching-based approaches.
</p></li>
</ul>

<h3>Title: UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition. (arXiv:2307.11170v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11170">http://arxiv.org/abs/2307.11170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11170] UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition](http://arxiv.org/abs/2307.11170) #transformer</code></li>
<li>Summary: <p>Pre-trained transformer language models (LMs) have in recent years become the
dominant paradigm in applied NLP. These models have achieved state-of-the-art
performance on tasks such as information extraction, question answering,
sentiment analysis, document classification and many others. In the biomedical
domain, significant progress has been made in adapting this paradigm to NLP
tasks that require the integration of domain-specific knowledge as well as
statistical modelling of language. In particular, research in this area has
focused on the question of how best to construct LMs that take into account not
only the patterns of token distribution in medical text, but also the wealth of
structured information contained in terminology resources such as the UMLS.
This work contributes a data-centric paradigm for enriching the language
representations of biomedical transformer-encoder LMs by extracting text
sequences from the UMLS. This allows for graph-based learning objectives to be
combined with masked-language pre-training. Preliminary results from
experiments in the extension of pre-trained LMs as well as training from
scratch show that this framework improves downstream performance on multiple
biomedical and clinical Named Entity Recognition (NER) tasks.
</p></li>
</ul>

<h3>Title: What can a Single Attention Layer Learn? A Study Through the Random Features Lens. (arXiv:2307.11353v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11353">http://arxiv.org/abs/2307.11353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11353] What can a Single Attention Layer Learn? A Study Through the Random Features Lens](http://arxiv.org/abs/2307.11353) #transformer</code></li>
<li>Summary: <p>Attention layers -- which map a sequence of inputs to a sequence of outputs
-- are core building blocks of the Transformer architecture which has achieved
significant breakthroughs in modern artificial intelligence. This paper
presents a rigorous theoretical study on the learning and generalization of a
single multi-head attention layer, with a sequence of key vectors and a
separate query vector as input. We consider the random feature setting where
the attention layer has a large number of heads, with randomly sampled frozen
query and key matrices, and trainable value matrices. We show that such a
random-feature attention layer can express a broad class of target functions
that are permutation invariant to the key vectors. We further provide
quantitative excess risk bounds for learning these target functions from finite
samples, using random feature attention with finitely many heads.
</p></li>
</ul>

<p>Our results feature several implications unique to the attention structure
compared with existing random features theory for neural networks, such as (1)
Advantages in the sample complexity over standard two-layer random-feature
networks; (2) Concrete and natural classes of functions that can be learned
efficiently by a random-feature attention layer; and (3) The effect of the
sampling distribution of the query-key weight matrix (the product of the query
and key matrix), where Gaussian random weights with a non-zero mean result in
better sample complexities over the zero-mean counterpart for learning certain
natural target functions. Experiments on simulated data corroborate our
theoretical findings and further illustrate the interplay between the sample
size and the complexity of the target function.
</p>

<h3>Title: A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11465">http://arxiv.org/abs/2307.11465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11465] A Deep Learning Approach for Overall Survival Analysis with Missing Values](http://arxiv.org/abs/2307.11465) #transformer</code></li>
<li>Summary: <p>One of the most challenging fields where Artificial Intelligence (AI) can be
applied is lung cancer research, specifically non-small cell lung cancer
(NSCLC). In particular, overall survival (OS) is a vital indicator of patient
status, helping to identify subgroups with diverse survival probabilities,
enabling tailored treatment and improved OS rates. In this analysis, there are
two challenges to take into account. First, few studies effectively exploit the
information available from each patient, leveraging both uncensored (i.e.,
dead) and censored (i.e., survivors) patients, considering also the death
times. Second, the handling of incomplete data is a common issue in the medical
field. This problem is typically tackled through the use of imputation methods.
Our objective is to present an AI model able to overcome these limits,
effectively learning from both censored and uncensored patients and their
available features, for the prediction of OS for NSCLC patients. We present a
novel approach to survival analysis in the context of NSCLC, which exploits the
strengths of the transformer architecture accounting for only available
features without requiring any imputation strategy. By making use of ad-hoc
losses for OS, it accounts for both censored and uncensored patients,
considering risks over time. We evaluated the results over a period of 6 years
using different time granularities obtaining a Ct-index, a time-dependent
variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1
year and 2 years, respectively, outperforming all state-of-the-art methods
regardless of the imputation method used.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: LatentAugment: Data Augmentation via Guided Manipulation of GAN's Latent Space. (arXiv:2307.11375v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11375">http://arxiv.org/abs/2307.11375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11375] LatentAugment: Data Augmentation via Guided Manipulation of GAN's Latent Space](http://arxiv.org/abs/2307.11375) #generative</code></li>
<li>Summary: <p>Data Augmentation (DA) is a technique to increase the quantity and diversity
of the training data, and by that alleviate overfitting and improve
generalisation. However, standard DA produces synthetic data for augmentation
with limited diversity. Generative Adversarial Networks (GANs) may unlock
additional information in a dataset by generating synthetic samples having the
appearance of real images. However, these models struggle to simultaneously
address three key requirements: fidelity and high-quality samples; diversity
and mode coverage; and fast sampling. Indeed, GANs generate high-quality
samples rapidly, but have poor mode coverage, limiting their adoption in DA
applications. We propose LatentAugment, a DA strategy that overcomes the low
diversity of GANs, opening up for use in DA applications. Without external
supervision, LatentAugment modifies latent vectors and moves them into latent
space regions to maximise the synthetic images' diversity and fidelity. It is
also agnostic to the dataset and the downstream task. A wide set of experiments
shows that LatentAugment improves the generalisation of a deep model
translating from MRI-to-CT beating both standard DA as well GAN-based sampling.
Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic
samples show superior mode coverage and diversity. Code is available at:
https://github.com/ltronchin/LatentAugment.
</p></li>
</ul>

<h3>Title: Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11661">http://arxiv.org/abs/2307.11661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11661] Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts](http://arxiv.org/abs/2307.11661) #generative</code></li>
<li>Summary: <p>Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.
</p></li>
</ul>

<h3>Title: PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations. (arXiv:2307.11289v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11289">http://arxiv.org/abs/2307.11289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11289] PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations](http://arxiv.org/abs/2307.11289) #generative</code></li>
<li>Summary: <p>We present a new category of physics-informed neural networks called physics
informed variational embedding generative adversarial network (PI-VEGAN), that
effectively tackles the forward, inverse, and mixed problems of stochastic
differential equations. In these scenarios, the governing equations are known,
but only a limited number of sensor measurements of the system parameters are
available. We integrate the governing physical laws into PI-VEGAN with
automatic differentiation, while introducing a variational encoder for
approximating the latent variables of the actual distribution of the
measurements. These latent variables are integrated into the generator to
facilitate accurate learning of the characteristics of the stochastic partial
equations. Our model consists of three components, namely the encoder,
generator, and discriminator, each of which is updated alternatively employing
the stochastic gradient descent algorithm. We evaluate the effectiveness of
PI-VEGAN in addressing forward, inverse, and mixed problems that require the
concurrent calculation of system parameters and solutions. Numerical results
demonstrate that the proposed method achieves satisfactory stability and
accuracy in comparison with the previous physics-informed generative
adversarial network (PI-WGAN).
</p></li>
</ul>

<h3>Title: Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11714">http://arxiv.org/abs/2307.11714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11714] Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses](http://arxiv.org/abs/2307.11714) #generative</code></li>
<li>Summary: <p>Optimal Transport has sparked vivid interest in recent years, in particular
thanks to the Wasserstein distance, which provides a geometrically sensible and
intuitive way of comparing probability measures. For computational reasons, the
Sliced Wasserstein (SW) distance was introduced as an alternative to the
Wasserstein distance, and has seen uses for training generative Neural Networks
(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed
practically in such a setting, there is to our knowledge no theoretical
guarantee for this observation. Leveraging recent works on convergence of SGD
on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to
bridge that knowledge gap, and provide a realistic context under which
fixed-step SGD trajectories for the SW loss on NN parameters converge. More
precisely, we show that the trajectories approach the set of (sub)-gradient
flow equations as the step decreases. Under stricter assumptions, we show a
much stronger convergence result for noised and projected SGD schemes, namely
that the long-run limits of the trajectories approach a set of generalised
critical points of the loss function.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Generating Image-Specific Text Improves Fine-grained Image Classification. (arXiv:2307.11315v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11315">http://arxiv.org/abs/2307.11315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11315] Generating Image-Specific Text Improves Fine-grained Image Classification](http://arxiv.org/abs/2307.11315) #large language model</code></li>
<li>Summary: <p>Recent vision-language models outperform vision-only models on many image
classification tasks. However, because of the absence of paired text/image
descriptions, it remains difficult to fine-tune these models for fine-grained
image classification. In this work, we propose a method, GIST, for generating
image-specific fine-grained text descriptions from image-only datasets, and
show that these text descriptions can be used to improve classification. Key
parts of our method include 1. prompting a pretrained large language model with
domain-specific prompts to generate diverse fine-grained text descriptions for
each class and 2. using a pretrained vision-language model to match each image
to label-preserving text descriptions that capture relevant visual features in
the image. We demonstrate the utility of GIST by fine-tuning vision-language
models on the image-and-generated-text pairs to learn an aligned
vision-language representation space for improved classification. We evaluate
our learned representation space in full-shot and few-shot scenarios across
four diverse fine-grained classification datasets, each from a different
domain. Our method achieves an average improvement of $4.1\%$ in accuracy over
CLIP linear probes and an average of $1.1\%$ improvement in accuracy over the
previous state-of-the-art image-text classification method on the full-shot
datasets. Our method achieves similar improvements across few-shot regimes.
Code is available at https://github.com/emu1729/GIST.
</p></li>
</ul>

<h3>Title: Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering. (arXiv:2307.11278v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11278">http://arxiv.org/abs/2307.11278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11278] Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering](http://arxiv.org/abs/2307.11278) #large language model</code></li>
<li>Summary: <p>Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance at
least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively.
We provide code, datasets, and checkpoints
\footnote{\url{https://github.com/abdoelsayed2016/GRG}}
</p></li>
</ul>

<h3>Title: CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study. (arXiv:2307.11346v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11346">http://arxiv.org/abs/2307.11346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11346] CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study](http://arxiv.org/abs/2307.11346) #large language model</code></li>
<li>Summary: <p>Participant recruitment based on unstructured medical texts such as clinical
notes and radiology reports has been a challenging yet important task for the
cohort establishment in clinical research. Recently, Large Language Models
(LLMs) such as ChatGPT have achieved tremendous success in various downstream
tasks thanks to their promising performance in language understanding,
inference, and generation. It is then natural to test their feasibility in
solving the cohort recruitment task, which involves the classification of a
given paragraph of medical text into disease label(s). However, when applied to
knowledge-intensive problem settings such as medical text classification, where
the LLMs are expected to understand the decision made by human experts and
accurately identify the implied disease labels, the LLMs show a mediocre
performance. A possible explanation is that, by only using the medical text,
the LLMs neglect to use the rich context of additional information that
languages afford. To this end, we propose to use a knowledge graph as auxiliary
information to guide the LLMs in making predictions. Moreover, to further boost
the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample
selection strategy enhanced by reinforcement learning, which selects a set of
CoT samples given each individual medical report. Experimental results and
various ablation studies show that our few-shot learning method achieves
satisfactory performance compared with fine-tuning strategies and gains superb
advantages when the available data is limited. The code and sample dataset of
the proposed CohortGPT model is available at:
https://anonymous.4open.science/r/CohortGPT-4872/
</p></li>
</ul>

<h3>Title: Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11288">http://arxiv.org/abs/2307.11288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11288] Kernelized Offline Contextual Dueling Bandits](http://arxiv.org/abs/2307.11288) #large language model</code></li>
<li>Summary: <p>Preference-based feedback is important for many applications where direct
evaluation of a reward function is not feasible. A notable recent example
arises in reinforcement learning from human feedback on large language models.
For many of these applications, the cost of acquiring the human feedback can be
substantial or even prohibitive. In this work, we take advantage of the fact
that often the agent can choose contexts at which to obtain human feedback in
order to most efficiently identify a good policy, and introduce the offline
contextual dueling bandit setting. We give an upper-confidence-bound style
algorithm for this setting and prove a regret bound. We also give empirical
confirmation that this method outperforms a similar strategy that uses
uniformly sampled contexts.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention. (arXiv:2307.11253v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11253">http://arxiv.org/abs/2307.11253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11253] Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention](http://arxiv.org/abs/2307.11253) #segmentation</code></li>
<li>Summary: <p>Deep learning has shown excellent performance in analysing medical images.
However, datasets are difficult to obtain due privacy issues, standardization
problems, and lack of annotations. We address these problems by producing
realistic synthetic images using a combination of 3D technologies and
generative adversarial networks. We propose CUT-seg, a joint training where a
segmentation model and a generative model are jointly trained to produce
realistic images while learning to segment polyps. We take advantage of recent
one-sided translation models because they use significantly less memory,
allowing us to add a segmentation model in the training loop. CUT-seg performs
better, is computationally less expensive, and requires less real images than
other memory-intensive image translation approaches that require two stage
training. Promising results are achieved on five real polyp segmentation
datasets using only one real image and zero real annotations. As a part of this
study we release Synth-Colon, an entirely synthetic dataset that includes 20000
realistic colon images and additional details about depth and 3D geometry:
https://enric1994.github.io/synth-colon
</p></li>
</ul>

<h3>Title: MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction. (arXiv:2307.11466v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11466">http://arxiv.org/abs/2307.11466</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11466] MatSpectNet: Material Segmentation Network with Domain-Aware and Physically-Constrained Hyperspectral Reconstruction](http://arxiv.org/abs/2307.11466) #segmentation</code></li>
<li>Summary: <p>Achieving accurate material segmentation for 3-channel RGB images is
challenging due to the considerable variation in a material's appearance.
Hyperspectral images, which are sets of spectral measurements sampled at
multiple wavelengths, theoretically offer distinct information for material
identification, as variations in intensity of electromagnetic radiation
reflected by a surface depend on the material composition of a scene. However,
existing hyperspectral datasets are impoverished regarding the number of images
and material categories for the dense material segmentation task, and
collecting and annotating hyperspectral images with a spectral camera is
prohibitively expensive. To address this, we propose a new model, the
MatSpectNet to segment materials with recovered hyperspectral images from RGB
images. The network leverages the principles of colour perception in modern
cameras to constrain the reconstructed hyperspectral images and employs the
domain adaptation method to generalise the hyperspectral reconstruction
capability from a spectral recovery dataset to material segmentation datasets.
The reconstructed hyperspectral images are further filtered using learned
response curves and enhanced with human perception. The performance of
MatSpectNet is evaluated on the LMD dataset as well as the OpenSurfaces
dataset. Our experiments demonstrate that MatSpectNet attains a 1.60% increase
in average pixel accuracy and a 3.42% improvement in mean class accuracy
compared with the most recent publication. The project code is attached to the
supplementary material and will be published on GitHub.
</p></li>
</ul>

<h3>Title: SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection. (arXiv:2307.11477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11477">http://arxiv.org/abs/2307.11477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11477] SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection](http://arxiv.org/abs/2307.11477) #segmentation</code></li>
<li>Summary: <p>Recently, the pure camera-based Bird's-Eye-View (BEV) perception provides a
feasible solution for economical autonomous driving. However, the existing
BEV-based multi-view 3D detectors generally transform all image features into
BEV features, without considering the problem that the large proportion of
background information may submerge the object information. In this paper, we
propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out
background information according to the semantic segmentation of image features
and transform image features into semantic-aware BEV features. Accordingly, we
propose BEV-Paste, an effective data augmentation strategy that closely matches
with semantic-aware BEV feature. In addition, we design a Multi-Scale
Cross-Task (MSCT) head, which combines task-specific and cross-task information
to predict depth distribution and semantic segmentation more accurately,
further improving the quality of semantic-aware BEV feature. Finally, we
integrate the above modules into a novel multi-view 3D object detection
framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves
state-of-the-art performance. Code has been available at
https://github.com/mengtan00/SA-BEV.git.
</p></li>
</ul>

<h3>Title: CORE: Cooperative Reconstruction for Multi-Agent Perception. (arXiv:2307.11514v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11514">http://arxiv.org/abs/2307.11514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11514] CORE: Cooperative Reconstruction for Multi-Agent Perception](http://arxiv.org/abs/2307.11514) #segmentation</code></li>
<li>Summary: <p>This paper presents CORE, a conceptually simple, effective and
communication-efficient model for multi-agent cooperative perception. It
addresses the task from a novel perspective of cooperative reconstruction,
based on two key insights: 1) cooperating agents together provide a more
holistic observation of the environment, and 2) the holistic observation can
serve as valuable supervision to explicitly guide the model learning how to
reconstruct the ideal observation based on collaboration. CORE instantiates the
idea with three major components: a compressor for each agent to create more
compact feature representation for efficient broadcasting, a lightweight
attentive collaboration component for cross-agent message aggregation, and a
reconstruction module to reconstruct the observation based on aggregated
feature representations. This learning-to-reconstruct idea is task-agnostic,
and offers clear and reasonable supervision to inspire more effective
collaboration, eventually promoting perception tasks. We validate CORE on
OPV2V, a large-scale multi-agent percetion dataset, in two tasks, i.e., 3D
object detection and semantic segmentation. Results demonstrate that the model
achieves state-of-the-art performance on both tasks, and is more
communication-efficient.
</p></li>
</ul>

<h3>Title: Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation. (arXiv:2307.11545v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11545">http://arxiv.org/abs/2307.11545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11545] Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation](http://arxiv.org/abs/2307.11545) #segmentation</code></li>
<li>Summary: <p>Parameter Efficient Tuning (PET) has gained attention for reducing the number
of parameters while maintaining performance and providing better hardware
resource savings, but few studies investigate dense prediction tasks and
interaction between modalities. In this paper, we do an investigation of
efficient tuning problems on referring image segmentation. We propose a novel
adapter called Bridger to facilitate cross-modal information exchange and
inject task-specific information into the pre-trained model. We also design a
lightweight decoder for image segmentation. Our approach achieves comparable or
superior performance with only 1.61\% to 3.38\% backbone parameter updates,
evaluated on challenging benchmarks. The code is available at
\url{https://github.com/kkakkkka/ETRIS}.
</p></li>
</ul>

<h3>Title: Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation. (arXiv:2307.11604v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.11604">http://arxiv.org/abs/2307.11604</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.11604] Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised Medical Image Segmentation](http://arxiv.org/abs/2307.11604) #segmentation</code></li>
<li>Summary: <p>Medical imaging has witnessed remarkable progress but usually requires a
large amount of high-quality annotated data which is time-consuming and costly
to obtain. To alleviate this burden, semi-supervised learning has garnered
attention as a potential solution. In this paper, we present Meta-Learning for
Bootstrapping Medical Image Segmentation (MLB-Seg), a novel method for tackling
the challenge of semi-supervised medical image segmentation. Specifically, our
approach first involves training a segmentation model on a small set of clean
labeled images to generate initial labels for unlabeled data. To further
optimize this bootstrapping process, we introduce a per-pixel weight mapping
system that dynamically assigns weights to both the initialized labels and the
model's own predictions. These weights are determined using a meta-process that
prioritizes pixels with loss gradient directions closer to those of clean data,
which is based on a small set of precisely annotated images. To facilitate the
meta-learning process, we additionally introduce a consistency-based Pseudo
Label Enhancement (PLE) scheme that improves the quality of the model's own
predictions by ensembling predictions from various augmented versions of the
same input. In order to improve the quality of the weight maps obtained through
multiple augmentations of a single input, we introduce a mean teacher into the
PLE scheme. This method helps to reduce noise in the weight maps and stabilize
its generation process. Our extensive experimental results on public atrial and
prostate segmentation datasets demonstrate that our proposed method achieves
state-of-the-art results under semi-supervision. Our code is available at
https://github.com/aijinrjinr/MLB-Seg.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
