<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: The offline digital currency puzzle solved by a local blockchain. (arXiv:2305.02290v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02290">http://arxiv.org/abs/2305.02290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02290] The offline digital currency puzzle solved by a local blockchain](http://arxiv.org/abs/2305.02290) #secure</code></li>
<li>Summary: <p>A major drawback in deploying central bank digital currencies (CDBC) is the
offline puzzle, which requires that a CBDC must keep the provision given by
cash, and, simultaneously, avoid double-spending, counterfeiting, and other
issues. The puzzle is solved by minting the coins in serials, which are stored
on a local blockchain (e.g. smartphone). The local blockchain is secured by
keys embedded in the hardware and can be continuously mined by the wallet to
enhance security. The coins can be either minted as hot coins, which can be
retrieved in case of loss, or minted as cold coins, like physical cash.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Illicit item detection in X-ray images for security applications. (arXiv:2305.01936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01936">http://arxiv.org/abs/2305.01936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01936] Illicit item detection in X-ray images for security applications](http://arxiv.org/abs/2305.01936) #security</code></li>
<li>Summary: <p>Automated detection of contraband items in X-ray images can significantly
increase public safety, by enhancing the productivity and alleviating the
mental load of security officers in airports, subways, customs/post offices,
etc. The large volume and high throughput of passengers, mailed parcels, etc.,
during rush hours make it a Big Data analysis task. Modern computer vision
algorithms relying on Deep Neural Networks (DNNs) have proven capable of
undertaking this task even under resource-constrained and embedded execution
scenarios, e.g., as is the case with fast, single-stage, anchor-based object
detectors. This paper proposes a two-fold improvement of such algorithms for
the X-ray analysis domain, introducing two complementary novelties. Firstly,
more efficient anchors are obtained by hierarchical clustering the sizes of the
ground-truth training set bounding boxes; thus, the resulting anchors follow a
natural hierarchy aligned with the semantic structure of the data. Secondly,
the default Non-Maximum Suppression (NMS) algorithm at the end of the object
detection pipeline is modified to better handle occluded object detection and
to reduce the number of false predictions, by inserting the Efficient
Intersection over Union (E-IoU) metric into the Weighted Cluster NMS method.
E-IoU provides more discriminative geometrical correlations between the
candidate bounding boxes/Regions-of-Interest (RoIs). The proposed method is
implemented on a common single-stage object detector (YOLOv5) and its
experimental evaluation on a relevant public dataset indicates significant
accuracy gains over both the baseline and competing approaches. This highlights
the potential of Big Data analysis in enhancing public safety.
</p></li>
</ul>

<h3>Title: Deep Learning-Based Multiband Signal Fusion for 3-D SAR Super-Resolution. (arXiv:2305.02017v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02017">http://arxiv.org/abs/2305.02017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02017] Deep Learning-Based Multiband Signal Fusion for 3-D SAR Super-Resolution](http://arxiv.org/abs/2305.02017) #security</code></li>
<li>Summary: <p>Three-dimensional (3-D) synthetic aperture radar (SAR) is widely used in many
security and industrial applications requiring high-resolution imaging of
concealed or occluded objects. The ability to resolve intricate 3-D targets is
essential to the performance of such applications and depends directly on
system bandwidth. However, because high-bandwidth systems face several
prohibitive hurdles, an alternative solution is to operate multiple radars at
distinct frequency bands and fuse the multiband signals. Current multiband
signal fusion methods assume a simple target model and a small number of point
reflectors, which is invalid for realistic security screening and industrial
imaging scenarios wherein the target model effectively consists of a large
number of reflectors. To the best of our knowledge, this study presents the
first use of deep learning for multiband signal fusion. The proposed network,
called kR-Net, employs a hybrid, dual-domain complex-valued convolutional
neural network (CV-CNN) to fuse multiband signals and impute the missing
samples in the frequency gaps between subbands. By exploiting the relationships
in both the wavenumber domain and wavenumber spectral domain, the proposed
framework overcomes the drawbacks of existing multiband imaging techniques for
realistic scenarios at a fraction of the computation time of existing multiband
fusion algorithms. Our method achieves high-resolution imaging of intricate
targets previously impossible using conventional techniques and enables finer
resolution capacity for concealed weapon detection and occluded object
classification using multiband signaling without requiring more advanced
hardware. Furthermore, a fully integrated multiband imaging system is developed
using commercially available millimeter-wave (mmWave) radars for efficient
multiband imaging.
</p></li>
</ul>

<h3>Title: GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions. (arXiv:2305.02143v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02143">http://arxiv.org/abs/2305.02143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02143] GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions](http://arxiv.org/abs/2305.02143) #security</code></li>
<li>Summary: <p>In recent years, the increasing availability of personal data has raised
concerns regarding privacy and security. One of the critical processes to
address these concerns is data anonymization, which aims to protect individual
privacy and prevent the release of sensitive information. This research focuses
on the importance of face anonymization. Therefore, we introduce
GANonymization, a novel face anonymization framework with facial
expression-preserving abilities. Our approach is based on a high-level
representation of a face which is synthesized into an anonymized version based
on a generative adversarial network (GAN). The effectiveness of the approach
was assessed by evaluating its performance in removing identifiable facial
attributes to increase the anonymity of the given individual face.
Additionally, the performance of preserving facial expressions was evaluated on
several affect recognition datasets and outperformed the state-of-the-art
method in most categories. Finally, our approach was analyzed for its ability
to remove various facial traits, such as jewelry, hair color, and multiple
others. Here, it demonstrated reliable performance in removing these
attributes. Our results suggest that GANonymization is a promising approach for
anonymizing faces while preserving facial expressions.
</p></li>
</ul>

<h3>Title: AutoLock: Automatic Design of Logic Locking with Evolutionary Computation. (arXiv:2305.01840v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01840">http://arxiv.org/abs/2305.01840</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01840] AutoLock: Automatic Design of Logic Locking with Evolutionary Computation](http://arxiv.org/abs/2305.01840) #security</code></li>
<li>Summary: <p>Logic locking protects the integrity of hardware designs throughout the
integrated circuit supply chain. However, recent machine learning (ML)-based
attacks have challenged its fundamental security, initiating the requirement
for the design of learning-resilient locking policies. A promising ML-resilient
locking mechanism hides within multiplexer-based locking. Nevertheless, recent
attacks have successfully breached these state-of-the-art locking schemes,
making it ever more complex to manually design policies that are resilient to
all existing attacks. In this project, for the first time, we propose the
automatic design exploration of logic locking with evolutionary computation
(EC) -- a set of versatile black-box optimization heuristics inspired by
evolutionary mechanisms. The project will evaluate the performance of
EC-designed logic locking against various types of attacks, starting with the
latest ML-based link prediction. Additionally, the project will provide
guidelines and best practices for using EC-based logic locking in practical
applications.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: A Systematic Study on Object Recognition Using Millimeter-wave Radar. (arXiv:2305.02085v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02085">http://arxiv.org/abs/2305.02085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02085] A Systematic Study on Object Recognition Using Millimeter-wave Radar](http://arxiv.org/abs/2305.02085) #privacy</code></li>
<li>Summary: <p>Due to its light and weather-independent sensing, millimeter-wave (MMW) radar
is essential in smart environments. Intelligent vehicle systems and
industry-grade MMW radars have integrated such capabilities. Industry-grade MMW
radars are expensive and hard to get for community-purpose smart environment
applications. However, commercially available MMW radars have hidden
underpinning challenges that need to be investigated for tasks like recognizing
objects and activities, real-time person tracking, object localization, etc.
Image and video data are straightforward to gather, understand, and annotate
for such jobs. Image and video data are light and weather-dependent,
susceptible to the occlusion effect, and present privacy problems. To eliminate
dependence and ensure privacy, commercial MMW radars should be tested. MMW
radar's practicality and performance in varied operating settings must be
addressed before promoting it. To address the problems, we collected a dataset
using Texas Instruments' Automotive mmWave Radar (AWR2944) and reported the
best experimental settings for object recognition performance using different
deep learning algorithms. Our extensive data gathering technique allows us to
systematically explore and identify object identification task problems under
cross-ambience conditions. We investigated several solutions and published
detailed experimental data.
</p></li>
</ul>

<h3>Title: Data Privacy with Homomorphic Encryption in Neural Networks Training and Inference. (arXiv:2305.02225v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02225">http://arxiv.org/abs/2305.02225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02225] Data Privacy with Homomorphic Encryption in Neural Networks Training and Inference](http://arxiv.org/abs/2305.02225) #privacy</code></li>
<li>Summary: <p>The use of Neural Networks (NNs) for sensitive data processing is becoming
increasingly popular, raising concerns about data privacy and security.
Homomorphic Encryption (HE) has the potential to be used as a solution to
preserve data privacy in NN. This study provides a comprehensive analysis on
the use of HE for NN training and classification, focusing on the techniques
and strategies used to enhance data privacy and security. The current
state-of-the-art in HE for NNs is analysed, and the challenges and limitations
that need to be addressed to make it a reliable and efficient approach for
privacy preservation are identified. Also, the different categories of HE
schemes and their suitability for NNs are discussed, as well as the techniques
used to optimize the accuracy and efficiency of encrypted models. The review
reveals that HE has the potential to provide strong data privacy guarantees for
NNs, but several challenges need to be addressed, such as limited support for
advanced NN operations, scalability issues, and performance trade-offs.
</p></li>
</ul>

<h3>Title: A Survey on Dataset Distillation: Approaches, Applications and Future Directions. (arXiv:2305.01975v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01975">http://arxiv.org/abs/2305.01975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01975] A Survey on Dataset Distillation: Approaches, Applications and Future Directions](http://arxiv.org/abs/2305.01975) #privacy</code></li>
<li>Summary: <p>Dataset distillation is attracting more attention in machine learning as
training sets continue to grow and the cost of training state-of-the-art models
becomes increasingly high. By synthesizing datasets with high information
density, dataset distillation offers a range of potential applications,
including support for continual learning, neural architecture search, and
privacy protection. Despite recent advances, we lack a holistic understanding
of the approaches and applications. Our survey aims to bridge this gap by first
proposing a taxonomy of dataset distillation, characterizing existing
approaches, and then systematically reviewing the data modalities, and related
applications. In addition, we summarize the challenges and discuss future
directions for this field of research.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning. (arXiv:2305.02022v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02022">http://arxiv.org/abs/2305.02022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02022] LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning](http://arxiv.org/abs/2305.02022) #attack</code></li>
<li>Summary: <p>Targeted model poisoning attacks pose a significant threat to federated
learning systems. Recent studies show that edge-case targeted attacks, which
target a small fraction of the input space are nearly impossible to counter
using existing fixed defense strategies. In this paper, we strive to design a
learned-defense strategy against such attacks, using a small defense dataset.
The defense dataset can be collected by the central authority of the federated
learning task, and should contain a mix of poisoned and clean examples. The
proposed framework, LearnDefend, estimates the probability of a client update
being malicious. The examples in defense dataset need not be pre-marked as
poisoned or clean. We also learn a poisoned data detector model which can be
used to mark each example in the defense dataset as clean or poisoned. We
estimate the poisoned data detector and the client importance models in a
coupled optimization approach. Our experiments demonstrate that LearnDefend is
capable of defending against state-of-the-art attacks where existing fixed
defense strategies fail. We also show that LearnDefend is robust to size and
noise in the marking of clean examples in the defense dataset.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Out-of-distribution detection algorithms for robust insect classification. (arXiv:2305.01823v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01823">http://arxiv.org/abs/2305.01823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01823] Out-of-distribution detection algorithms for robust insect classification](http://arxiv.org/abs/2305.01823) #robust</code></li>
<li>Summary: <p>Deep learning-based approaches have produced models with good insect
classification accuracy; Most of these models are conducive for application in
controlled environmental conditions. One of the primary emphasis of researchers
is to implement identification and classification models in the real
agriculture fields, which is challenging because input images that are wildly
out of the distribution (e.g., images like vehicles, animals, humans, or a
blurred image of an insect or insect class that is not yet trained on) can
produce an incorrect insect classification. Out-of-distribution (OOD) detection
algorithms provide an exciting avenue to overcome these challenge as it ensures
that a model abstains from making incorrect classification prediction of
non-insect and/or untrained insect class images. We generate and evaluate the
performance of state-of-the-art OOD algorithms on insect detection classifiers.
These algorithms represent a diversity of methods for addressing an OOD
problem. Specifically, we focus on extrusive algorithms, i.e., algorithms that
wrap around a well-trained classifier without the need for additional
co-training. We compared three OOD detection algorithms: (i) Maximum Softmax
Probability, which uses the softmax value as a confidence score, (ii)
Mahalanobis distance-based algorithm, which uses a generative classification
approach; and (iii) Energy-Based algorithm that maps the input data to a scalar
value, called energy. We performed an extensive series of evaluations of these
OOD algorithms across three performance axes: (a) \textit{Base model accuracy}:
How does the accuracy of the classifier impact OOD performance? (b) How does
the \textit{level of dissimilarity to the domain} impact OOD performance? and
(c) \textit{Data imbalance}: How sensitive is OOD performance to the imbalance
in per-class sample size?
</p></li>
</ul>

<h3>Title: Class adaptive threshold and negative class guided noisy annotation robust Facial Expression Recognition. (arXiv:2305.01884v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01884">http://arxiv.org/abs/2305.01884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01884] Class adaptive threshold and negative class guided noisy annotation robust Facial Expression Recognition](http://arxiv.org/abs/2305.01884) #robust</code></li>
<li>Summary: <p>The hindering problem in facial expression recognition (FER) is the presence
of inaccurate annotations referred to as noisy annotations in the datasets.
These noisy annotations are present in the datasets inherently because the
labeling is subjective to the annotator, clarity of the image, etc. Recent
works use sample selection methods to solve this noisy annotation problem in
FER. In our work, we use a dynamic adaptive threshold to separate confident
samples from non-confident ones so that our learning won't be hampered due to
non-confident samples. Instead of discarding the non-confident samples, we
impose consistency in the negative classes of those non-confident samples to
guide the model to learn better in the positive class. Since FER datasets
usually come with 7 or 8 classes, we can correctly guess a negative class by
85% probability even by choosing randomly. By learning "which class a sample
doesn't belong to", the model can learn "which class it belongs to" in a better
manner. We demonstrate proposed framework's effectiveness using quantitative as
well as qualitative results. Our method performs better than the baseline by a
margin of 4% to 28% on RAFDB and 3.3% to 31.4% on FERPlus for various levels of
synthetic noisy labels in the aforementioned datasets.
</p></li>
</ul>

<h3>Title: Real-Time Radiance Fields for Single-Image Portrait View Synthesis. (arXiv:2305.02310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02310">http://arxiv.org/abs/2305.02310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02310] Real-Time Radiance Fields for Single-Image Portrait View Synthesis](http://arxiv.org/abs/2305.02310) #robust</code></li>
<li>Summary: <p>We present a one-shot method to infer and render a photorealistic 3D
representation from a single unposed image (e.g., face portrait) in real-time.
Given a single RGB input, our image encoder directly predicts a canonical
triplane representation of a neural radiance field for 3D-aware novel view
synthesis via volume rendering. Our method is fast (24 fps) on consumer
hardware, and produces higher quality results than strong GAN-inversion
baselines that require test-time optimization. To train our triplane encoder
pipeline, we use only synthetic data, showing how to distill the knowledge from
a pretrained 3D GAN into a feedforward encoder. Technical contributions include
a Vision Transformer-based triplane encoder, a camera data augmentation
strategy, and a well-designed loss function for synthetic data training. We
benchmark against the state-of-the-art methods, demonstrating significant
improvements in robustness and image quality in challenging real-world
settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ),
but our algorithm can also be applied in the future to other categories with a
3D-aware image generator.
</p></li>
</ul>

<h3>Title: Robust Natural Language Watermarking through Invariant Features. (arXiv:2305.01904v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01904">http://arxiv.org/abs/2305.01904</a></li>
<li>Code URL: <a href="https://github.com/bangawayoo/nlp-watermarking">https://github.com/bangawayoo/nlp-watermarking</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01904] Robust Natural Language Watermarking through Invariant Features](http://arxiv.org/abs/2305.01904) #robust</code></li>
<li>Summary: <p>Recent years have witnessed a proliferation of valuable original natural
language contents found in subscription-based media outlets, web novel
platforms, and outputs of large language models. Without proper security
measures, however, these contents are susceptible to illegal piracy and
potential misuse. This calls for a secure watermarking system to guarantee
copyright protection through leakage tracing or ownership identification. To
effectively combat piracy and protect copyrights, a watermarking framework
should be able not only to embed adequate bits of information but also extract
the watermarks in a robust manner despite possible corruption. In this work, we
explore ways to advance both payload and robustness by following a well-known
proposition from image watermarking and identify features in natural language
that are invariant to minor corruption. Through a systematic analysis of the
possible sources of errors, we further propose a corruption-resistant infill
model. Our full method improves upon the previous work on robustness by +16.8%
point on average on four datasets, three corruption types, and two corruption
ratios. Code available at https://github.com/bangawayoo/nlp-watermarking.
</p></li>
</ul>

<h3>Title: A Curriculum View of Robust Loss Functions. (arXiv:2305.02139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02139">http://arxiv.org/abs/2305.02139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02139] A Curriculum View of Robust Loss Functions](http://arxiv.org/abs/2305.02139) #robust</code></li>
<li>Summary: <p>Robust loss functions are designed to combat the adverse impacts of label
noise, whose robustness is typically supported by theoretical bounds agnostic
to the training dynamics. However, these bounds may fail to characterize the
empirical performance as it remains unclear why robust loss functions can
underfit. We show that most loss functions can be rewritten into a form with
the same class-score margin and different sample-weighting functions. The
resulting curriculum view provides a straightforward analysis of the training
dynamics, which helps attribute underfitting to diminished average sample
weights and noise robustness to larger weights for clean samples. We show that
simple fixes to the curriculums can make underfitting robust loss functions
competitive with the state-of-the-art, and training schedules can substantially
affect the noise robustness even with robust loss functions. Code is available
at \url{github}.
</p></li>
</ul>

<h3>Title: The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02239">http://arxiv.org/abs/2305.02239</a></li>
<li>Code URL: <a href="https://github.com/lingyugao/labeldesctraining">https://github.com/lingyugao/labeldesctraining</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02239] The Benefits of Label-Description Training for Zero-Shot Text Classification](http://arxiv.org/abs/2305.02239) #robust</code></li>
<li>Summary: <p>Large language models have improved zero-shot text classification by allowing
the transfer of semantic knowledge from the training data in order to classify
among specific label sets in downstream tasks. We propose a simple way to
further improve zero-shot accuracies with minimal effort. We curate small
finetuning datasets intended to describe the labels for a task. Unlike typical
finetuning data, which has texts annotated with labels, our data simply
describes the labels in language, e.g., using a few related terms,
dictionary/encyclopedia entries, and short templates. Across a range of topic
and sentiment datasets, our method is more accurate than zero-shot by 15-17%
absolute. It is also more robust to choices required for zero-shot
classification, such as patterns for prompting the model to classify and
mappings from labels to tokens in the model's vocabulary. Furthermore, since
our data merely describes the labels but does not use input texts, finetuning
on it yields a model that performs strongly on multiple text domains for a
given label set, even improving over few-shot out-of-domain classification in
multiple settings.
</p></li>
</ul>

<h3>Title: Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles. (arXiv:2305.01754v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01754">http://arxiv.org/abs/2305.01754</a></li>
<li>Code URL: <a href="https://github.com/learningmatter-mit/uq_singlenn">https://github.com/learningmatter-mit/uq_singlenn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01754] Single-model uncertainty quantification in neural network potentials does not consistently outperform model ensembles](http://arxiv.org/abs/2305.01754) #robust</code></li>
<li>Summary: <p>Neural networks (NNs) often assign high confidence to their predictions, even
for points far out-of-distribution, making uncertainty quantification (UQ) a
challenge. When they are employed to model interatomic potentials in materials
systems, this problem leads to unphysical structures that disrupt simulations,
or to biased statistics and dynamics that do not reflect the true physics.
Differentiable UQ techniques can find new informative data and drive active
learning loops for robust potentials. However, a variety of UQ techniques,
including newly developed ones, exist for atomistic simulations and there are
no clear guidelines for which are most effective or suitable for a given case.
In this work, we examine multiple UQ schemes for improving the robustness of NN
interatomic potentials (NNIPs) through active learning. In particular, we
compare incumbent ensemble-based methods against strategies that use single,
deterministic NNs: mean-variance estimation, deep evidential regression, and
Gaussian mixture models. We explore three datasets ranging from in-domain
interpolative learning to more extrapolative out-of-domain generalization
challenges: rMD17, ammonia inversion, and bulk silica glass. Performance is
measured across multiple metrics relating model error to uncertainty. Our
experiments show that none of the methods consistently outperformed each other
across the various metrics. Ensembling remained better at generalization and
for NNIP robustness; MVE only proved effective for in-domain interpolation,
while GMM was better out-of-domain; and evidential regression, despite its
promise, was not the preferable alternative in any of the cases. More broadly,
cost-effective, single deterministic models cannot yet consistently match or
outperform ensembling for uncertainty quantification in NNIPs.
</p></li>
</ul>

<h3>Title: MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction. (arXiv:2305.01912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01912">http://arxiv.org/abs/2305.01912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01912] MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction](http://arxiv.org/abs/2305.01912) #robust</code></li>
<li>Summary: <p>How to effectively represent molecules is a long-standing challenge for
molecular property prediction and drug discovery. This paper studies this
problem and proposes to incorporate chemical domain knowledge, specifically
related to chemical reactions, for learning effective molecular
representations. However, the inherent cross-modality property between chemical
reactions and molecules presents a significant challenge to address. To this
end, we introduce a novel method, namely MolKD, which Distills cross-modal
Knowledge in chemical reactions to assist Molecular property prediction.
Specifically, the reaction-to-molecule distillation model within MolKD
transfers cross-modal knowledge from a pre-trained teacher network learning
with one modality (i.e., reactions) into a student network learning with
another modality (i.e., molecules). Moreover, MolKD learns effective molecular
representations by incorporating reaction yields to measure transformation
efficiency of the reactant-product pair when pre-training on reactions.
Extensive experiments demonstrate that MolKD significantly outperforms various
competitive baseline models, e.g., 2.1% absolute AUC-ROC gain on Tox21. Further
investigations demonstrate that pre-trained molecular representations in MolKD
can distinguish chemically reasonable molecular similarities, which enables
molecular property prediction with high robustness and interpretability.
</p></li>
</ul>

<h3>Title: Rethinking Graph Lottery Tickets: Graph Sparsity Matters. (arXiv:2305.02190v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02190">http://arxiv.org/abs/2305.02190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02190] Rethinking Graph Lottery Tickets: Graph Sparsity Matters](http://arxiv.org/abs/2305.02190) #robust</code></li>
<li>Summary: <p>Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket
(i.e., a properly pruned sub-network together with original weight
initialization) that can achieve competitive performance to the original dense
network. A recent work, called UGS, extended LTH to prune graph neural networks
(GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes
the graph adjacency matrix and the model weights using the same masking
mechanism, but since the roles of the graph adjacency matrix and the weight
matrices are very different, we find that their sparsifications lead to
different performance characteristics. Specifically, we find that the
performance of a sparsified GNN degrades significantly when the graph sparsity
goes beyond a certain extent. Therefore, we propose two techniques to improve
GNN performance when the graph sparsity is high. First, UGS prunes the
adjacency matrix using a loss formulation which, however, does not properly
involve all elements of the adjacency matrix; in contrast, we add a new
auxiliary loss head to better guide the edge pruning by involving the entire
adjacency matrix. Second, by regarding unfavorable graph sparsification as
adversarial data perturbations, we formulate the pruning process as a min-max
optimization problem to gain the robustness of lottery tickets when the graph
sparsity is high. We further investigate the question: Can the "retrainable"
winning ticket of a GNN be also effective for graph transferring learning? We
call it the transferable graph lottery ticket (GLT) hypothesis. Extensive
experiments were conducted which demonstrate the superiority of our proposed
sparsification method over UGS, and which empirically verified our transferable
GLT hypothesis.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Localization using Multi-Focal Spatial Attention for Masked Face Recognition. (arXiv:2305.01905v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01905">http://arxiv.org/abs/2305.01905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01905] Localization using Multi-Focal Spatial Attention for Masked Face Recognition](http://arxiv.org/abs/2305.01905) #biometric</code></li>
<li>Summary: <p>Since the beginning of world-wide COVID-19 pandemic, facial masks have been
recommended to limit the spread of the disease. However, these masks hide
certain facial attributes. Hence, it has become difficult for existing face
recognition systems to perform identity verification on masked faces. In this
context, it is necessary to develop masked Face Recognition (MFR) for
contactless biometric recognition systems. Thus, in this paper, we propose
Complementary Attention Learning and Multi-Focal Spatial Attention that
precisely removes masked region by training complementary spatial attention to
focus on two distinct regions: masked regions and backgrounds. In our method,
standard spatial attention and networks focus on unmasked regions, and extract
mask-invariant features while minimizing the loss of the conventional Face
Recognition (FR) performance. For conventional FR, we evaluate the performance
on the IJB-C, Age-DB, CALFW, and CPLFW datasets. We evaluate the MFR
performance on the ICCV2021-MFR/Insightface track, and demonstrate the improved
performance on the both MFR and FR datasets. Additionally, we empirically
verify that spatial attention of proposed method is more precisely activated in
unmasked regions.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: High-Resolution Synthetic RGB-D Datasets for Monocular Depth Estimation. (arXiv:2305.01732v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01732">http://arxiv.org/abs/2305.01732</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01732] High-Resolution Synthetic RGB-D Datasets for Monocular Depth Estimation](http://arxiv.org/abs/2305.01732) #extraction</code></li>
<li>Summary: <p>Accurate depth maps are essential in various applications, such as autonomous
driving, scene reconstruction, point-cloud creation, etc. However,
monocular-depth estimation (MDE) algorithms often fail to provide enough
texture &amp; sharpness, and also are inconsistent for homogeneous scenes. These
algorithms mostly use CNN or vision transformer-based architectures requiring
large datasets for supervised training. But, MDE algorithms trained on
available depth datasets do not generalize well and hence fail to perform
accurately in diverse real-world scenes. Moreover, the ground-truth depth maps
are either lower resolution or sparse leading to relatively inconsistent depth
maps. In general, acquiring a high-resolution ground truth dataset with
pixel-level precision for accurate depth prediction is an expensive, and
time-consuming challenge.
</p></li>
</ul>

<p>In this paper, we generate a high-resolution synthetic depth dataset (HRSD)
of dimension 1920 X 1080 from Grand Theft Auto (GTA-V), which contains 100,000
color images and corresponding dense ground truth depth maps. The generated
datasets are diverse and have scenes from indoors to outdoors, from homogeneous
surfaces to textures. For experiments and analysis, we train the DPT algorithm,
a state-of-the-art transformer-based MDE algorithm on the proposed synthetic
dataset, which significantly increases the accuracy of depth maps on different
scenes by 9 %. Since the synthetic datasets are of higher resolution, we
propose adding a feature extraction module in the transformer encoder and
incorporating an attention-based loss, further improving the accuracy by 15 %.
</p>

<h3>Title: LineFormer: Rethinking Line Chart Data Extraction as Instance Segmentation. (arXiv:2305.01837v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01837">http://arxiv.org/abs/2305.01837</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01837] LineFormer: Rethinking Line Chart Data Extraction as Instance Segmentation](http://arxiv.org/abs/2305.01837) #extraction</code></li>
<li>Summary: <p>Data extraction from line-chart images is an essential component of the
automated document understanding process, as line charts are a ubiquitous data
visualization format. However, the amount of visual and structural variations
in multi-line graphs makes them particularly challenging for automated parsing.
Existing works, however, are not robust to all these variations, either taking
an all-chart unified approach or relying on auxiliary information such as
legends for line data extraction. In this work, we propose LineFormer, a robust
approach to line data extraction using instance segmentation. We achieve
state-of-the-art performance on several benchmark synthetic and real chart
datasets. Our implementation is available at
https://github.com/TheJaeLal/LineFormer .
</p></li>
</ul>

<h3>Title: Evolving Dictionary Representation for Few-shot Class-incremental Learning. (arXiv:2305.01885v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01885">http://arxiv.org/abs/2305.01885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01885] Evolving Dictionary Representation for Few-shot Class-incremental Learning](http://arxiv.org/abs/2305.01885) #extraction</code></li>
<li>Summary: <p>New objects are continuously emerging in the dynamically changing world and a
real-world artificial intelligence system should be capable of continual and
effectual adaptation to new emerging classes without forgetting old ones. In
view of this, in this paper we tackle a challenging and practical continual
learning scenario named few-shot class-incremental learning (FSCIL), in which
labeled data are given for classes in a base session but very limited labeled
instances are available for new incremental classes. To address this problem,
we propose a novel and succinct approach by introducing deep dictionary
learning which is a hybrid learning architecture that combines dictionary
learning and visual representation learning to provide a better space for
characterizing different classes. We simultaneously optimize the dictionary and
the feature extraction backbone in the base session, while only finetune the
dictionary in the incremental session for adaptation to novel classes, which
can alleviate the forgetting on base classes compared to finetuning the entire
model. To further facilitate future adaptation, we also incorporate multiple
pseudo classes into the base session training so that certain space projected
by dictionary can be reserved for future new concepts. The extensive
experimental results on CIFAR100, miniImageNet and CUB200 validate the
effectiveness of our approach compared to other SOTA methods.
</p></li>
</ul>

<h3>Title: Improved Static Hand Gesture Classification on Deep Convolutional Neural Networks using Novel Sterile Training Technique. (arXiv:2305.02039v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02039">http://arxiv.org/abs/2305.02039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02039] Improved Static Hand Gesture Classification on Deep Convolutional Neural Networks using Novel Sterile Training Technique](http://arxiv.org/abs/2305.02039) #extraction</code></li>
<li>Summary: <p>In this paper, we investigate novel data collection and training techniques
towards improving classification accuracy of non-moving (static) hand gestures
using a convolutional neural network (CNN) and
frequency-modulated-continuous-wave (FMCW) millimeter-wave (mmWave) radars.
Recently, non-contact hand pose and static gesture recognition have received
considerable attention in many applications ranging from human-computer
interaction (HCI), augmented/virtual reality (AR/VR), and even therapeutic
range of motion for medical applications. While most current solutions rely on
optical or depth cameras, these methods require ideal lighting and temperature
conditions. mmWave radar devices have recently emerged as a promising
alternative offering low-cost system-on-chip sensors whose output signals
contain precise spatial information even in non-ideal imaging conditions.
Additionally, deep convolutional neural networks have been employed extensively
in image recognition by learning both feature extraction and classification
simultaneously. However, little work has been done towards static gesture
recognition using mmWave radars and CNNs due to the difficulty involved in
extracting meaningful features from the radar return signal, and the results
are inferior compared with dynamic gesture classification. This article
presents an efficient data collection approach and a novel technique for deep
CNN training by introducing ``sterile'' images which aid in distinguishing
distinct features among the static gestures and subsequently improve the
classification accuracy. Applying the proposed data collection and training
methods yields an increase in classification rate of static hand gestures from
$85\%$ to $93\%$ and $90\%$ to $95\%$ for range and range-angle profiles,
respectively.
</p></li>
</ul>

<h3>Title: Rethinking the Encoding of Satellite Image Time Series. (arXiv:2305.02086v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02086">http://arxiv.org/abs/2305.02086</a></li>
<li>Code URL: <a href="https://github.com/TotalVariation/Exchanger4SITS">https://github.com/TotalVariation/Exchanger4SITS</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02086] Rethinking the Encoding of Satellite Image Time Series](http://arxiv.org/abs/2305.02086) #extraction</code></li>
<li>Summary: <p>Representation learning of Satellite Image Time Series (SITS) presents its
unique challenges, such as prohibitive computation burden caused by high
spatiotemporal resolutions, irregular acquisition times, and complex
spatiotemporal interactions, leading to highly-specialized neural network
architectures for SITS analysis. Despite the promising results achieved by some
pioneering work, we argue that satisfactory representation learning paradigms
have not yet been established for SITS analysis, causing an isolated island
where transferring successful paradigms or the latest advances from Computer
Vision (CV) to SITS is arduous. In this paper, we develop a unique perspective
of SITS processing as a direct set prediction problem, inspired by the recent
trend in adopting query-based transformer decoders to streamline the object
detection or image segmentation pipeline, and further propose to decompose the
representation learning process of SITS into three explicit steps:
collect--update--distribute, which is computationally efficient and suits for
irregularly-sampled and asynchronous temporal observations. Facilitated by the
unique reformulation and effective feature extraction framework proposed, our
models pre-trained on pixel-set format input and then fine-tuned on downstream
dense prediction tasks by simply appending a commonly-used segmentation network
have attained new state-of-the-art (SoTA) results on PASTIS dataset compared to
bespoke neural architectures such as U-TAE. Furthermore, the clear separation,
conceptually and practically, between temporal and spatial components in the
panoptic segmentation pipeline of SITS allows us to leverage the recent
advances in CV, such as Mask2Former, a universal segmentation architecture,
resulting in a noticeable 8.8 points increase in PQ compared to the best score
reported so far.
</p></li>
</ul>

<h3>Title: Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01876">http://arxiv.org/abs/2305.01876</a></li>
<li>Code URL: <a href="https://github.com/siyuyuan/kpce">https://github.com/siyuyuan/kpce</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01876] Causality-aware Concept Extraction based on Knowledge-guided Prompting](http://arxiv.org/abs/2305.01876) #extraction</code></li>
<li>Summary: <p>Concepts benefit natural language understanding but are far from complete in
existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs)
have been widely used in text-based concept extraction (CE). However, PLMs tend
to mine the co-occurrence associations from massive corpus as pre-trained
knowledge rather than the real causal effect between tokens.As a result, the
pre-trained knowledge confounds PLMs to extract biased concepts based on
spurious co-occurrence correlations, inevitably resulting in low precision. In
this paper, through the lens of a Structural Causal Model (SCM), we propose
equipping the PLM-based extractor with a knowledge-guided prompt as an
intervention to alleviate concept bias. The prompt adopts the topic of the
given entity from the existing knowledge in KGs to mitigate the spurious
co-occurrence correlations between entities and biased concepts. Our extensive
experiments on representative multilingual KG datasets justify that our
proposed prompt can effectively alleviate concept bias and improve the
performance of PLM-based CE models.The code has been released on
https://github.com/siyuyuan/KPCE.
</p></li>
</ul>

<h3>Title: Generative Meta-Learning for Zero-Shot Relation Triplet Extraction. (arXiv:2305.01920v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01920">http://arxiv.org/abs/2305.01920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01920] Generative Meta-Learning for Zero-Shot Relation Triplet Extraction](http://arxiv.org/abs/2305.01920) #extraction</code></li>
<li>Summary: <p>The zero-shot relation triplet extraction (ZeroRTE) task aims to extract
relation triplets from a piece of text with unseen relation types. The seminal
work adopts the pre-trained generative model to generate synthetic samples for
new relations. However, current generative models lack the optimization process
of model generalization on different tasks during training, and thus have
limited generalization capability. For this reason, we propose a novel
generative meta-learning framework which exploits the `learning-to-learn'
ability of meta-learning to boost the generalization capability of generative
models. Specifically, we first design a task-aware generative model which can
learn the general knowledge by forcing the optimization process to be conducted
across multiple tasks. Based on it, we then present three generative
meta-learning approaches designated for three typical meta-learning categories.
Extensive experimental results demonstrate that our framework achieves a new
state-of-the-art performance for the ZeroRTE task.
</p></li>
</ul>

<h3>Title: Natural language processing on customer note data. (arXiv:2305.02029v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02029">http://arxiv.org/abs/2305.02029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02029] Natural language processing on customer note data](http://arxiv.org/abs/2305.02029) #extraction</code></li>
<li>Summary: <p>Automatic analysis of customer data for businesses is an area that is of
interest to companies. Business to business data is studied rarely in academia
due to the sensitive nature of such information. Applying natural language
processing can speed up the analysis of prohibitively large sets of data. This
paper addresses this subject and applies sentiment analysis, topic modelling
and keyword extraction to a B2B data set. We show that accurate sentiment can
be extracted from the notes automatically and the notes can be sorted by
relevance into different topics. We see that without clear separation topics
can lack relevance to a business context.
</p></li>
</ul>

<h3>Title: GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02105">http://arxiv.org/abs/2305.02105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02105] GPT-RE: In-context Learning for Relation Extraction using Large Language Models](http://arxiv.org/abs/2305.02105) #extraction</code></li>
<li>Summary: <p>In spite of the potential for ground-breaking achievements offered by large
language models (LLMs) (e.g., GPT-3), they still lag significantly behind
fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).
This is due to the two major shortcomings of LLMs in RE: (1) low relevance
regarding entity and relation in retrieved demonstrations for in-context
learning; and (2) the strong inclination to wrongly classify NULL examples into
other pre-defined labels.
</p></li>
</ul>

<p>In this paper, we propose GPT-RE to bridge the gap between LLMs and
fully-supervised baselines. GPT-RE successfully addresses the aforementioned
issues by (1) incorporating task-specific entity representations in
demonstration retrieval; and (2) enriching the demonstrations with gold
label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE
datasets, and observe that GPT-RE achieves improvements over not only existing
GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE
achieves SOTA performances on the Semeval and SciERC datasets, and competitive
performances on the TACRED and ACE05 datasets.
</p>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Scalable Data Point Valuation in Decentralized Learning. (arXiv:2305.01657v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01657">http://arxiv.org/abs/2305.01657</a></li>
<li>Code URL: <a href="https://github.com/kpandl/Scalable-Data-Point-Valuation-in-Decentralized-Learning">https://github.com/kpandl/Scalable-Data-Point-Valuation-in-Decentralized-Learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01657] Scalable Data Point Valuation in Decentralized Learning](http://arxiv.org/abs/2305.01657) #federate</code></li>
<li>Summary: <p>Existing research on data valuation in federated and swarm learning focuses
on valuing client contributions and works best when data across clients is
independent and identically distributed (IID). In practice, data is rarely
distributed IID. We develop an approach called DDVal for decentralized data
valuation, capable of valuing individual data points in federated and swarm
learning. DDVal is based on sharing deep features and approximating Shapley
values through a k-nearest neighbor approximation method. This allows for novel
applications, for example, to simultaneously reward institutions and
individuals for providing data to a decentralized machine learning task. The
valuation of data points through DDVal allows to also draw hierarchical
conclusions on the contribution of institutions, and we empirically show that
the accuracy of DDVal in estimating institutional contributions is higher than
existing Shapley value approximation methods for federated learning.
Specifically, it reaches a cosine similarity in approximating Shapley values of
99.969 % in both, IID and non-IID data distributions across institutions,
compared with 99.301 % and 97.250 % for the best state of the art methods.
DDVal scales with the number of data points instead of the number of clients,
and has a loglinear complexity. This scales more favorably than existing
approaches with an exponential complexity. We show that DDVal is especially
efficient in data distribution scenarios with many clients that have few data
points - for example, more than 16 clients with 8,000 data points each. By
integrating DDVal into a decentralized system, we show that it is not only
suitable for centralized federated learning, but also decentralized swarm
learning, which aligns well with the research on emerging internet technologies
such as web3 to reward users for providing data to algorithms.
</p></li>
</ul>

<h3>Title: LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning. (arXiv:2305.02219v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02219">http://arxiv.org/abs/2305.02219</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02219] LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning](http://arxiv.org/abs/2305.02219) #federate</code></li>
<li>Summary: <p>We propose LESS-VFL, a communication-efficient feature selection method for
distributed systems with vertically partitioned data. We consider a system of a
server and several parties with local datasets that share a sample ID space but
have different feature sets. The parties wish to collaboratively train a model
for a prediction task. As part of the training, the parties wish to remove
unimportant features in the system to improve generalization, efficiency, and
explainability. In LESS-VFL, after a short pre-training period, the server
optimizes its part of the global model to determine the relevant outputs from
party models. This information is shared with the parties to then allow local
feature selection without communication. We analytically prove that LESS-VFL
removes spurious features from model training. We provide extensive empirical
evidence that LESS-VFL can achieve high accuracy and remove spurious features
at a fraction of the communication cost of other feature selection approaches.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness in AI Systems: Mitigating gender bias from language-vision models. (arXiv:2305.01888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01888">http://arxiv.org/abs/2305.01888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01888] Fairness in AI Systems: Mitigating gender bias from language-vision models](http://arxiv.org/abs/2305.01888) #fair</code></li>
<li>Summary: <p>Our society is plagued by several biases, including racial biases, caste
biases, and gender bias. As a matter of fact, several years ago, most of these
notions were unheard of. These biases passed through generations along with
amplification have lead to scenarios where these have taken the role of
expected norms by certain groups in the society. One notable example is of
gender bias. Whether we talk about the political world, lifestyle or corporate
world, some generic differences are observed regarding the involvement of both
the groups. This differential distribution, being a part of the society at
large, exhibits its presence in the recorded data as well. Machine learning is
almost entirely dependent on the availability of data; and the idea of learning
from data and making predictions assumes that data defines the expected
behavior at large. Hence, with biased data the resulting models are corrupted
with those inherent biases too; and with the current popularity of ML in
products, this can result in a huge obstacle in the path of equality and
justice. This work studies and attempts to alleviate gender bias issues from
language vision models particularly the task of image captioning. We study the
extent of the impact of gender bias in existing datasets and propose a
methodology to mitigate its impact in caption based language vision models.
</p></li>
</ul>

<h3>Title: Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01901">http://arxiv.org/abs/2305.01901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01901] Few-shot Event Detection: An Empirical Study and a Unified View](http://arxiv.org/abs/2305.01901) #fair</code></li>
<li>Summary: <p>Few-shot event detection (ED) has been widely studied, while this brings
noticeable discrepancies, e.g., various motivations, tasks, and experimental
settings, that hinder the understanding of models for future progress. This
paper presents a thorough empirical study, a unified view of ED models, and a
better unified baseline. For fair evaluation, we choose two practical settings:
low-resource setting to assess generalization ability and class-transfer
setting for transferability. We compare ten representative methods on three
datasets, which are roughly grouped into prompt-based and prototype-based
models for detailed analysis. To investigate the superior performance of
prototype-based methods, we break down the design and build a unified
framework. Based on that, we not only propose a simple yet effective method
(e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable
research insights for future research.
</p></li>
</ul>

<h3>Title: Explaining Language Models' Predictions with High-Impact Concepts. (arXiv:2305.02160v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02160">http://arxiv.org/abs/2305.02160</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02160] Explaining Language Models' Predictions with High-Impact Concepts](http://arxiv.org/abs/2305.02160) #fair</code></li>
<li>Summary: <p>The emergence of large-scale pretrained language models has posed
unprecedented challenges in deriving explanations of why the model has made
some predictions. Stemmed from the compositional nature of languages, spurious
correlations have further undermined the trustworthiness of NLP systems,
leading to unreliable model explanations that are merely correlated with the
output predictions. To encourage fairness and transparency, there exists an
urgent demand for reliable explanations that allow users to consistently
understand the model's behavior. In this work, we propose a complete framework
for extending concept-based interpretability methods to NLP. Specifically, we
propose a post-hoc interpretability method for extracting predictive high-level
features (concepts) from the pretrained model's hidden layer activations. We
optimize for features whose existence causes the output predictions to change
substantially, \ie generates a high impact. Moreover, we devise several
evaluation metrics that can be universally applied. Extensive experiments on
real and synthetic tasks demonstrate that our method achieves superior results
on {predictive impact}, usability, and faithfulness compared to the baselines.
</p></li>
</ul>

<h3>Title: Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy. (arXiv:2305.01783v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01783">http://arxiv.org/abs/2305.01783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01783] Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy](http://arxiv.org/abs/2305.01783) #fair</code></li>
<li>Summary: <p>Poverty maps derived from satellite imagery are increasingly used to inform
high-stakes policy decisions, such as the allocation of humanitarian aid and
the distribution of government resources. Such poverty maps are typically
constructed by training machine learning algorithms on a relatively modest
amount of ``ground truth" data from surveys, and then predicting poverty levels
in areas where imagery exists but surveys do not. Using survey and satellite
data from ten countries, this paper investigates disparities in representation,
systematic biases in prediction errors, and fairness concerns in
satellite-based poverty mapping across urban and rural lines, and shows how
these phenomena affect the validity of policies based on predicted maps. Our
findings highlight the importance of careful error and bias analysis before
using satellite-based poverty maps in real-world policy decisions.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste. (arXiv:2305.02307v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02307">http://arxiv.org/abs/2305.02307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02307] Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste](http://arxiv.org/abs/2305.02307) #interpretability</code></li>
<li>Summary: <p>Existing fashion datasets do not consider the multi-facts that cause a
consumer to like or dislike a fashion image. Even two consumers like a same
fashion image, they could like this image for total different reasons. In this
paper, we study the reason why a consumer like a certain fashion image. Towards
this goal, we introduce an interpretability dataset, Fashionpedia-taste,
consist of rich annotation to explain why a subject like or dislike a fashion
image from the following 3 perspectives: 1) localized attributes; 2) human
attention; 3) caption. Furthermore, subjects are asked to provide their
personal attributes and preference on fashion, such as personality and
preferred fashion brands. Our dataset makes it possible for researchers to
build computational models to fully understand and interpret human fashion
taste from different humanistic perspectives and modalities.
</p></li>
</ul>

<h3>Title: Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02317">http://arxiv.org/abs/2305.02317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02317] Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings](http://arxiv.org/abs/2305.02317) #interpretability</code></li>
<li>Summary: <p>Recent advances in large language models elicit reasoning in a chain of
thought that allows models to decompose problems in a human-like fashion.
Though this paradigm improves multi-step reasoning ability in language models,
it is limited by being unimodal and applied mainly to question-answering tasks.
We claim that incorporating visual augmentation into reasoning is essential,
especially for complex, imaginative tasks. Consequently, we introduce VCoT, a
novel method that leverages chain of thought prompting with vision-language
grounding to recursively bridge the logical gaps within sequential data. Our
method uses visual guidance to generate synthetic multimodal infillings that
add consistent and novel information to reduce the logical gaps for downstream
tasks that can benefit from temporal reasoning, as well as provide
interpretability into models' multi-step reasoning. We apply VCoT to the Visual
Storytelling and WikiHow summarization datasets and demonstrate through human
evaluation that VCoT offers novel and consistent synthetic data augmentation
beating chain of thought baselines, which can be used to enhance downstream
performance.
</p></li>
</ul>

<h3>Title: Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis. (arXiv:2305.01710v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01710">http://arxiv.org/abs/2305.01710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01710] Stars Are All You Need: A Distantly Supervised Pyramid Network for Document-Level End-to-End Sentiment Analysis](http://arxiv.org/abs/2305.01710) #interpretability</code></li>
<li>Summary: <p>In this paper, we propose document-level end-to-end sentiment analysis to
efficiently understand aspect and review sentiment expressed in online reviews
in a unified manner. In particular, we assume that star rating labels are a
"coarse-grained synthesis" of aspect ratings across in the review. We propose a
Distantly Supervised Pyramid Network (DSPN) to efficiently perform
Aspect-Category Detection, Aspect-Category Sentiment Analysis, and Rating
Prediction using only document star rating labels for training. By performing
these three related sentiment subtasks in an end-to-end manner, DSPN can
extract aspects mentioned in the review, identify the corresponding sentiments,
and predict the star rating labels. We evaluate DSPN on multi-aspect review
datasets in English and Chinese and find that with only star rating labels for
supervision, DSPN can perform comparably well to a variety of benchmark models.
We also demonstrate the interpretability of DSPN's outputs on reviews to show
the pyramid structure inherent in document level end-to-end sentiment analysis.
</p></li>
</ul>

<h3>Title: Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01713">http://arxiv.org/abs/2305.01713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01713] Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks](http://arxiv.org/abs/2305.01713) #interpretability</code></li>
<li>Summary: <p>Disentangling sentence representations over continuous spaces can be a
critical process in improving interpretability and semantic control by
localising explicit generative factors. Such process confers to neural-based
language models some of the advantages that are characteristic of symbolic
models, while keeping their flexibility. This work presents a methodology for
disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it
into a more separable semantic space with the support of a flow-based
invertible neural network (INN). Experimental results indicate that the INN can
transform the distributed hidden space into a better semantically disentangled
latent space, resulting in better interpretability and controllability, when
compared to recent state-of-the-art models.
</p></li>
</ul>

<h3>Title: Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features. (arXiv:2305.01807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01807">http://arxiv.org/abs/2305.01807</a></li>
<li>Code URL: <a href="https://github.com/pennbindlab/vnn_brain_age">https://github.com/pennbindlab/vnn_brain_age</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01807] Transferablility of coVariance Neural Networks and Application to Interpretable Brain Age Prediction using Anatomical Features](http://arxiv.org/abs/2305.01807) #interpretability</code></li>
<li>Summary: <p>Graph convolutional networks (GCN) leverage topology-driven graph
convolutional operations to combine information across the graph for inference
tasks. In our recent work, we have studied GCNs with covariance matrices as
graphs in the form of coVariance neural networks (VNNs) that draw similarities
with traditional PCA-driven data analysis approaches while offering significant
advantages over them. In this paper, we first focus on theoretically
characterizing the transferability of VNNs. The notion of transferability is
motivated from the intuitive expectation that learning models could generalize
to "compatible" datasets (possibly of different dimensionalities) with minimal
effort. VNNs inherit the scale-free data processing architecture from GCNs and
here, we show that VNNs exhibit transferability of performance over datasets
whose covariance matrices converge to a limit object. Multi-scale neuroimaging
datasets enable the study of the brain at multiple scales and hence, can
validate the theoretical results on the transferability of VNNs. To gauge the
advantages offered by VNNs in neuroimaging data analysis, we focus on the task
of "brain age" prediction using cortical thickness features. In clinical
neuroscience, there has been an increased interest in machine learning
algorithms which provide estimates of "brain age" that deviate from
chronological age. We leverage the architecture of VNNs to extend beyond the
coarse metric of brain age gap in Alzheimer's disease (AD) and make two
important observations: (i) VNNs can assign anatomical interpretability to
elevated brain age gap in AD, and (ii) the interpretability offered by VNNs is
contingent on their ability to exploit specific principal components of the
anatomical covariance matrix. We further leverage the transferability of VNNs
to cross validate the above observations across different datasets.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Multimodal Data Augmentation for Image Captioning using Diffusion Models. (arXiv:2305.01855v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01855">http://arxiv.org/abs/2305.01855</a></li>
<li>Code URL: <a href="https://github.com/xiaochr/multimodal-augmentation-image-captioning">https://github.com/xiaochr/multimodal-augmentation-image-captioning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01855] Multimodal Data Augmentation for Image Captioning using Diffusion Models](http://arxiv.org/abs/2305.01855) #diffusion</code></li>
<li>Summary: <p>Image captioning, an important vision-language task, often requires a
tremendous number of finely labeled image-caption pairs for learning the
underlying alignment between images and texts. In this paper, we proposed a
multimodal data augmentation method, leveraging a recent text-to-image model
called Stable Diffusion, to expand the training set via high-quality generation
of image-caption pairs. Extensive experiments on the MS COCO dataset
demonstrate the advantages of our approach over several benchmark methods, and
particularly a significant boost when having fewer training instances. In
addition, models trained on our augmented datasets also outperform prior
unpaired image captioning methods by a large margin. Finally, further
improvement regarding the training efficiency and effectiveness can be obtained
after intentionally filtering the generated data based on quality assessment.
</p></li>
</ul>

<h3>Title: DiffFacto Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion. (arXiv:2305.01921v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01921">http://arxiv.org/abs/2305.01921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01921] DiffFacto Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion](http://arxiv.org/abs/2305.01921) #diffusion</code></li>
<li>Summary: <p>While the community of 3D point cloud generation has witnessed a big growth
in recent years, there still lacks an effective way to enable intuitive user
control in the generation process, hence limiting the general utility of such
methods. Since an intuitive way of decomposing a shape is through its parts, we
propose to tackle the task of controllable part-based point cloud generation.
We introduce DiffFacto, a novel probabilistic generative model that learns the
distribution of shapes with part-level control. We propose a factorization that
models independent part style and part configuration distributions, and present
a novel cross diffusion network that enables us to generate coherent and
plausible shapes under our proposed factorization. Experiments show that our
method is able to generate novel shapes with multiple axes of control. It
achieves state-of-the-art part-level generation quality and generates plausible
and coherent shape, while enabling various downstream editing applications such
as shape interpolation, mixing and transformation editing. Code will be made
publicly available.
</p></li>
</ul>

<h3>Title: DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01735">http://arxiv.org/abs/2305.01735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01735] DiffuSum: Generation Enhanced Extractive Summarization with Diffusion](http://arxiv.org/abs/2305.01735) #diffusion</code></li>
<li>Summary: <p>Extractive summarization aims to form a summary by directly extracting
sentences from the source document. Existing works mostly formulate it as a
sequence labeling problem by making individual sentence label predictions. This
paper proposes DiffuSum, a novel paradigm for extractive summarization, by
directly generating the desired summary sentence representations with diffusion
models and extracting sentences based on sentence representation matching. In
addition, DiffuSum jointly optimizes a contrastive sentence encoder with a
matching loss for sentence representation alignment and a multi-class
contrastive loss for representation diversity. Experimental results show that
DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail
with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets
with different summary lengths also demonstrate the effectiveness of DiffuSum.
The strong performance of our framework shows the great potential of adapting
generative models for extractive summarization.
</p></li>
</ul>

<h3>Title: Multimodal Procedural Planning via Dual Text-Image Prompting. (arXiv:2305.01795v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01795">http://arxiv.org/abs/2305.01795</a></li>
<li>Code URL: <a href="https://github.com/yujielu10/mpp">https://github.com/yujielu10/mpp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01795] Multimodal Procedural Planning via Dual Text-Image Prompting](http://arxiv.org/abs/2305.01795) #diffusion</code></li>
<li>Summary: <p>Embodied agents have achieved prominent performance in following human
instructions to complete tasks. However, the potential of providing
instructions informed by texts and images to assist humans in completing tasks
remains underexplored. To uncover this capability, we present the multimodal
procedural planning (MPP) task, in which models are given a high-level goal and
generate plans of paired text-image steps, providing more complementary and
informative guidance than unimodal plans. The key challenges of MPP are to
ensure the informativeness, temporal coherence,and accuracy of plans across
modalities. To tackle this, we propose Text-Image Prompting (TIP), a
dual-modality prompting method that jointly leverages zero-shot reasoning
ability in large language models (LLMs) and compelling text-to-image generation
ability from diffusion-based models. TIP improves the interaction in the dual
modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs
to guide the textual-grounded image plan generation and leveraging the
descriptions of image plans to ground the textual plan reversely. To address
the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed
for MPP. Our results show compelling human preferences and automatic scores
against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms
of informativeness, temporal coherence, and plan accuracy. Our code and data:
https://github.com/YujieLu10/MPP.
</p></li>
</ul>

<h3>Title: Unpaired Downscaling of Fluid Flows with Diffusion Bridges. (arXiv:2305.01822v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01822">http://arxiv.org/abs/2305.01822</a></li>
<li>Code URL: <a href="https://github.com/clima/diffusion-bridge-downscaling">https://github.com/clima/diffusion-bridge-downscaling</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01822] Unpaired Downscaling of Fluid Flows with Diffusion Bridges](http://arxiv.org/abs/2305.01822) #diffusion</code></li>
<li>Summary: <p>We present a method to downscale idealized geophysical fluid simulations
using generative models based on diffusion maps. By analyzing the Fourier
spectra of images drawn from different data distributions, we show how one can
chain together two independent conditional diffusion models for use in domain
translation. The resulting transformation is a diffusion bridge between a low
resolution and a high resolution dataset and allows for new sample generation
of high-resolution images given specific low resolution features. The ability
to generate new samples allows for the computation of any statistic of
interest, without any additional calibration or training. Our unsupervised
setup is also designed to downscale images without access to paired training
data; this flexibility allows for the combination of multiple source and target
domains without additional training. We demonstrate that the method enhances
resolution and corrects context-dependent biases in geophysical fluid
simulations, including in extreme events. We anticipate that the same method
can be used to downscale the output of climate simulations, including
temperature and precipitation fields, without needing to train a new model for
each application and providing a significant computational cost savings.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: "Glitch in the Matrix!": A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization. (arXiv:2305.01979v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01979">http://arxiv.org/abs/2305.01979</a></li>
<li>Code URL: <a href="https://github.com/ControlNet/LAV-DF">https://github.com/ControlNet/LAV-DF</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01979] "Glitch in the Matrix!": A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization](http://arxiv.org/abs/2305.01979) #transformer</code></li>
<li>Summary: <p>Most deepfake detection methods focus on detecting spatial and/or
spatio-temporal changes in facial attributes. This is because available
benchmark datasets contain mostly visual-only modifications. However, a
sophisticated deepfake may include small segments of audio or audio-visual
manipulations that can completely change the meaning of the content. To
addresses this gap, we propose and benchmark a new dataset, Localized Audio
Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual
and audio-visual manipulations. The proposed baseline method, Boundary Aware
Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based
architecture which efficiently captures multimodal manipulations. We further
improve (i.e. BA-TFD+) the baseline method by replacing the backbone with a
Multiscale Vision Transformer and guide the training process with contrastive,
frame classification, boundary matching and multimodal boundary matching loss
functions. The quantitative analysis demonstrates the superiority of BA- TFD+
on temporal forgery localization and deepfake detection tasks using several
benchmark datasets including our newly proposed dataset. The dataset, models
and code are available at https://github.com/ControlNet/LAV-DF.
</p></li>
</ul>

<h3>Title: Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification. (arXiv:2305.02032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02032">http://arxiv.org/abs/2305.02032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02032] Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification](http://arxiv.org/abs/2305.02032) #transformer</code></li>
<li>Summary: <p>Classification of gigapixel Whole Slide Images (WSIs) is an important
prediction task in the emerging area of computational pathology. There has been
a surge of research in deep learning models for WSI classification with
clinical applications such as cancer detection or prediction of molecular
mutations from WSIs. Most methods require expensive and labor-intensive manual
annotations by expert pathologists. Weakly supervised Multiple Instance
Learning (MIL) methods have recently demonstrated excellent performance;
however, they still require large slide-level labeled training datasets that
need a careful inspection of each slide by an expert pathologist. In this work,
we propose a fully unsupervised WSI classification algorithm based on mutual
transformer learning. Instances from gigapixel WSI (i.e., image patches) are
transformed into a latent space and then inverse-transformed to the original
space. Using the transformation loss, pseudo-labels are generated and cleaned
using a transformer label-cleaner. The proposed transformer-based pseudo-label
generation and cleaning modules mutually train each other iteratively in an
unsupervised manner. A discriminative learning mechanism is introduced to
improve normal versus cancerous instance labeling. In addition to unsupervised
classification, we demonstrate the effectiveness of the proposed framework for
weak supervision for cancer subtype classification as downstream analysis.
Extensive experiments on four publicly available datasets show excellent
performance compared to the state-of-the-art methods. We intend to make the
source code of our algorithm publicly available soon.
</p></li>
</ul>

<h3>Title: A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution. (arXiv:2305.02074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02074">http://arxiv.org/abs/2305.02074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02074] A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution](http://arxiv.org/abs/2305.02074) #transformer</code></li>
<li>Summary: <p>In this paper, we develop a novel super-resolution algorithm for near-field
synthetic-aperture radar (SAR) under irregular scanning geometries. As
fifth-generation (5G) millimeter-wave (mmWave) devices are becoming
increasingly affordable and available, high-resolution SAR imaging is feasible
for end-user applications and non-laboratory environments. Emerging
applications such freehand imaging, wherein a handheld radar is scanned
throughout space by a user, unmanned aerial vehicle (UAV) imaging, and
automotive SAR face several unique challenges for high-resolution imaging.
First, recovering a SAR image requires knowledge of the array positions
throughout the scan. While recent work has introduced camera-based positioning
systems capable of adequately estimating the position, recovering the algorithm
efficiently is a requirement to enable edge and Internet of Things (IoT)
technologies. Efficient algorithms for non-cooperative near-field SAR sampling
have been explored in recent work, but suffer image defocusing under position
estimation error and can only produce medium-fidelity images. In this paper, we
introduce a mobile-friend vision transformer (ViT) architecture to address
position estimation error and perform SAR image super-resolution (SR) under
irregular sampling geometries. The proposed algorithm, Mobile-SRViT, is the
first to employ a ViT approach for SAR image enhancement and is validated in
simulation and via empirical studies.
</p></li>
</ul>

<h3>Title: Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02279">http://arxiv.org/abs/2305.02279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02279] Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models](http://arxiv.org/abs/2305.02279) #transformer</code></li>
<li>Summary: <p>During the continuous evolution of one organism's ancestry, its genes
accumulate extensive experiences and knowledge, enabling newborn descendants to
rapidly adapt to their specific environments. Motivated by this observation, we
propose a novel machine learning paradigm \textit{Learngene} to enable learning
models to incorporate three key characteristics of genes. (i) Accumulating: the
knowledge is accumulated during the continuous learning of an \textbf{ancestry
model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into
a much more compact information piece, \ie \textbf{learngene}. (iii):
Inheriting: the condensed \textbf{learngene} is inherited to make it easier for
\textbf{descendant models} to adapt to new environments. Since accumulating has
been studied in some well-developed paradigms like large-scale pre-training and
lifelong learning, we focus on condensing and inheriting, which induces three
key issues and we provide the preliminary solutions to these issues in this
paper: (i) \textit{Learngene} Form: the \textbf{learngene} is set to a few
integral layers that can preserve the most commonality. (ii) \textit{Learngene}
Condensing: we identify which layers among the ancestry model have the most
similarity as one pseudo descendant model. (iii) \textit{Learngene} Inheriting:
to construct distinct descendant models for specific downstream tasks, we stack
some randomly initialized layers to the \textbf{learngene} layers. Extensive
experiments of various settings, including using different network
architectures like Vision Transformer (ViT) and Convolutional Neural Networks
(CNNs) on different datasets, are carried out to confirm five advantages and
two characteristics of \textit{Learngene}.
</p></li>
</ul>

<h3>Title: DynamicStereo: Consistent Dynamic Depth from Stereo Videos. (arXiv:2305.02296v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02296">http://arxiv.org/abs/2305.02296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02296] DynamicStereo: Consistent Dynamic Depth from Stereo Videos](http://arxiv.org/abs/2305.02296) #transformer</code></li>
<li>Summary: <p>We consider the problem of reconstructing a dynamic scene observed from a
stereo camera. Most existing methods for depth from stereo treat different
stereo frames independently, leading to temporally inconsistent depth
predictions. Temporal consistency is especially important for immersive AR or
VR scenarios, where flickering greatly diminishes the user experience. We
propose DynamicStereo, a novel transformer-based architecture to estimate
disparity for stereo videos. The network learns to pool information from
neighboring frames to improve the temporal consistency of its predictions. Our
architecture is designed to process stereo videos efficiently through divided
attention layers. We also introduce Dynamic Replica, a new benchmark dataset
containing synthetic videos of people and animals in scanned environments,
which provides complementary training and evaluation data for dynamic stereo
closer to real applications than existing datasets. Training with this dataset
further improves the quality of predictions of our proposed DynamicStereo as
well as prior methods. Finally, it acts as a benchmark for consistent stereo
methods.
</p></li>
</ul>

<h3>Title: SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method. (arXiv:2305.01954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01954">http://arxiv.org/abs/2305.01954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01954] SeqAug: Sequential Feature Resampling as a modality agnostic augmentation method](http://arxiv.org/abs/2305.01954) #transformer</code></li>
<li>Summary: <p>Data augmentation is a prevalent technique for improving performance in
various machine learning applications. We propose SeqAug, a modality-agnostic
augmentation method that is tailored towards sequences of extracted features.
The core idea of SeqAug is to augment the sequence by resampling from the
underlying feature distribution. Resampling is performed by randomly selecting
feature dimensions and permuting them along the temporal axis. Experiments on
CMU-MOSEI verify that SeqAug is modality agnostic; it can be successfully
applied to a single modality or multiple modalities. We further verify its
compatibility with both recurrent and transformer architectures, and also
demonstrate comparable to state-of-the-art results.
</p></li>
</ul>

<h3>Title: Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02176">http://arxiv.org/abs/2305.02176</a></li>
<li>Code URL: <a href="https://github.com/fe1ixxu/stratified_mixture_of_experts">https://github.com/fe1ixxu/stratified_mixture_of_experts</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02176] Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity](http://arxiv.org/abs/2305.02176) #transformer</code></li>
<li>Summary: <p>Mixture-of-experts (MoE) models that employ sparse activation have
demonstrated effectiveness in significantly increasing the number of parameters
while maintaining low computational requirements per token. However, recent
studies have established that MoE models are inherently parameter-inefficient
as the improvement in performance diminishes with an increasing number of
experts. We hypothesize this parameter inefficiency is a result of all experts
having equal capacity, which may not adequately meet the varying complexity
requirements of different tokens or tasks, e.g., in a multilingual setting,
languages based on their resource levels might require different capacities. In
light of this, we propose Stratified Mixture of Experts(SMoE) models, which
feature a stratified structure and can assign dynamic capacity to different
tokens. We demonstrate the effectiveness of SMoE on two multilingual machine
translation benchmarks, where it outperforms multiple state-of-the-art MoE
models. On a diverse 15-language dataset, SMoE improves the translation quality
over vanilla MoE by +0.93 BLEU points on average. Additionally, SMoE is
parameter-efficient, matching vanilla MoE performance with around 50\% fewer
parameters.
</p></li>
</ul>

<h3>Title: Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages. (arXiv:2305.02215v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02215">http://arxiv.org/abs/2305.02215</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02215] Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages](http://arxiv.org/abs/2305.02215) #transformer</code></li>
<li>Summary: <p>The overwhelming success of transformers is a real conundrum stimulating a
compelling question: are these machines replicating some traditional linguistic
models or discovering radically new theories? In this paper, we propose a novel
standpoint to investigate this important question. Using typological
similarities among languages, we aim to layer-wise compare transformers for
different languages to observe whether these similarities emerge for particular
layers. For this investigation, we propose to use Centered kernel alignment to
measure similarity among weight matrices. We discovered that syntactic
typological similarity is consistent with the similarity among weights in the
middle layers. This finding confirms results obtained by syntactically probing
BERT and, thus, gives an important confirmation that BERT is replicating
traditional linguistic models.
</p></li>
</ul>

<h3>Title: A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems. (arXiv:2305.01883v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01883">http://arxiv.org/abs/2305.01883</a></li>
<li>Code URL: <a href="https://github.com/cm8908/cnn_transformer3">https://github.com/cm8908/cnn_transformer3</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01883] A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems](http://arxiv.org/abs/2305.01883) #transformer</code></li>
<li>Summary: <p>Transformer-based models show state-of-the-art performance even for
large-scale Traveling Salesman Problems (TSPs). However, they are based on
fully-connected attention models and suffer from large computational complexity
and GPU memory usage. We propose a lightweight CNN-Transformer model based on a
CNN embedding layer and partial self-attention. Our CNN-Transformer model is
able to better learn spatial features from input data using a CNN embedding
layer compared with the standard Transformer models. It also removes
considerable redundancy in fully connected attention models using the proposed
partial self-attention. Experiments show that the proposed model outperforms
other state-of-the-art Transformer-based models in terms of TSP solution
quality, GPU memory usage, and inference time. Our model consumes approximately
20% less GPU memory usage and has 45% faster inference time compared with other
state-of-the-art Transformer-based models. Our code is publicly available at
https://github.com/cm8908/CNN_Transformer3
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime. (arXiv:2305.02297v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02297">http://arxiv.org/abs/2305.02297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02297] Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime](http://arxiv.org/abs/2305.02297) #generative</code></li>
<li>Summary: <p>Large-scale visual language models are widely used as pre-trained models and
then adapted for various downstream tasks. While humans are known to
efficiently learn new tasks from a few examples, deep learning models struggle
with adaptation from few examples. In this work, we look into task adaptation
in the low-data regime, and provide a thorough study of the existing adaptation
methods for generative Visual Language Models. And we show important benefits
of self-labelling, i.e. using the model's own predictions to self-improve when
having access to a larger number of unlabelled images of the same distribution.
Our study demonstrates significant gains using our proposed task adaptation
pipeline across a wide range of visual language tasks such as visual
classification (ImageNet), visual captioning (COCO), detailed visual captioning
(Localised Narratives) and visual question answering (VQAv2).
</p></li>
</ul>

<h3>Title: AG3D: Learning to Generate 3D Avatars from 2D Image Collections. (arXiv:2305.02312v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02312">http://arxiv.org/abs/2305.02312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02312] AG3D: Learning to Generate 3D Avatars from 2D Image Collections](http://arxiv.org/abs/2305.02312) #generative</code></li>
<li>Summary: <p>While progress in 2D generative models of human appearance has been rapid,
many applications require 3D avatars that can be animated and rendered.
Unfortunately, most existing methods for learning generative models of 3D
humans with diverse shape and appearance require 3D training data, which is
limited and expensive to acquire. The key to progress is hence to learn
generative models of 3D avatars from abundant unstructured 2D image
collections. However, learning realistic and complete 3D appearance and
geometry in this under-constrained setting remains challenging, especially in
the presence of loose clothing such as dresses. In this paper, we propose a new
adversarial generative model of realistic 3D people from 2D images. Our method
captures shape and deformation of the body and loose clothing by adopting a
holistic 3D generator and integrating an efficient and flexible articulation
module. To improve realism, we train our model using multiple discriminators
while also integrating geometric cues in the form of predicted 2D normal maps.
We experimentally find that our method outperforms previous 3D- and
articulation-aware methods in terms of geometry and appearance. We validate the
effectiveness of our model and the importance of each component via systematic
ablation studies.
</p></li>
</ul>

<h3>Title: Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02164">http://arxiv.org/abs/2305.02164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02164] Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows](http://arxiv.org/abs/2305.02164) #generative</code></li>
<li>Summary: <p>Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric
generative modeling but has not been widely adopted due to its suboptimal
generative quality and lack of conditional modeling capabilities. In this work,
we make two major contributions to bridging this gap. First, based on a
pleasant observation that (under certain conditions) the SWF of joint
distributions coincides with those of conditional distributions, we propose
Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of
SWF that enables nonparametric conditional modeling. Second, we introduce
appropriate inductive biases of images into SWF with two techniques inspired by
local connectivity and multiscale representation in vision research, which
greatly improve the efficiency and quality of modeling images. With all the
improvements, we achieve generative performance comparable with many deep
parametric generative models on both conditional and unconditional tasks in a
purely nonparametric fashion, demonstrating its great potential.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01750">http://arxiv.org/abs/2305.01750</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01750] Few-shot In-context Learning for Knowledge Base Question Answering](http://arxiv.org/abs/2305.01750) #large language model</code></li>
<li>Summary: <p>Question answering over knowledge bases is considered a difficult problem due
to the challenge of generalizing to a wide variety of possible natural language
questions. Additionally, the heterogeneity of knowledge base schema items
between different knowledge bases often necessitates specialized training for
different knowledge base question-answering (KBQA) datasets. To handle
questions over diverse KBQA datasets with a unified training-free framework, we
propose KB-BINDER, which for the first time enables few-shot in-context
learning over KBQA tasks. Firstly, KB-BINDER leverages large language models
like Codex to generate logical forms as the draft for a specific question by
imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge
base to bind the generated draft to an executable one with BM25 score matching.
The experimental results on four public heterogeneous KBQA datasets show that
KB-BINDER can achieve a strong performance with only a few in-context
demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even
outperform the state-of-the-art trained models. On GrailQA and WebQSP, our
model is also on par with other fully-trained models. We believe KB-BINDER can
serve as an important baseline for future research. We plan to release all the
code and data.
</p></li>
</ul>

<h3>Title: SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01879">http://arxiv.org/abs/2305.01879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01879] SCOTT: Self-Consistent Chain-of-Thought Distillation](http://arxiv.org/abs/2305.01879) #large language model</code></li>
<li>Summary: <p>Large language models (LMs) beyond a certain scale, demonstrate the emergent
capability of generating free-text rationales for their predictions via
chain-of-thought (CoT) prompting. While CoT can yield dramatically improved
performance, such gains are only observed for sufficiently large LMs. Even more
concerning, there is little guarantee that the generated rationales are
consistent with LM's predictions or faithfully justify the decisions. In this
work, we propose a faithful knowledge distillation method to learn a small,
self-consistent CoT model from a teacher model that is orders of magnitude
larger. To form better supervision, we elicit rationales supporting the gold
answers from a large LM (teacher) by contrastive decoding, which encourages the
teacher to generate tokens that become more plausible only when the answer is
considered. To ensure faithful distillation, we use the teacher-generated
rationales to learn a student LM with a counterfactual reasoning objective,
which prevents the student from ignoring the rationales to make inconsistent
predictions. Experiments show that, while yielding comparable end-task
performance, our method can generate CoT rationales that are more faithful than
baselines do. Further analysis suggests that such a model respects the
rationales more when making decisions; thus, we can improve its performance
more by refining its rationales.
</p></li>
</ul>

<h3>Title: Can Large Language Models Be an Alternative to Human Evaluations?. (arXiv:2305.01937v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01937">http://arxiv.org/abs/2305.01937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01937] Can Large Language Models Be an Alternative to Human Evaluations?](http://arxiv.org/abs/2305.01937) #large language model</code></li>
<li>Summary: <p>Human evaluation is indispensable and inevitable for assessing the quality of
texts generated by machine learning models or written by humans. However, human
evaluation is very difficult to reproduce and its quality is notoriously
unstable, hindering fair comparisons among different natural language
processing (NLP) models and algorithms. Recently, large language models (LLMs)
have demonstrated exceptional performance on unseen tasks when only the task
instructions are provided. In this paper, we explore if such an ability of the
LLMs can be used as an alternative to human evaluation. We present the LLMs
with the exact same instructions, samples to be evaluated, and questions used
to conduct human evaluation, and then ask the LLMs to generate responses to
those questions; we dub this LLM evaluation. We use human evaluation and LLM
evaluation to evaluate the texts in two NLP tasks: open-ended story generation
and adversarial attacks. We show that the result of LLM evaluation is
consistent with the results obtained by expert human evaluation: the texts
rated higher by human experts are also rated higher by the LLMs. We also find
that the results of LLM evaluation are stable over different formatting of the
task instructions and the sampling algorithm used to generate the answer. We
are the first to show the potential of using LLMs to assess the quality of
texts and discuss the limitations and ethical considerations of LLM evaluation.
</p></li>
</ul>

<h3>Title: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02220">http://arxiv.org/abs/2305.02220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02220] Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat](http://arxiv.org/abs/2305.02220) #large language model</code></li>
<li>Summary: <p>This paper describes our submission to the MEDIQA-Chat 2023 shared task for
automatic clinical note generation from doctor-patient conversations. We report
results for two approaches: the first fine-tunes a pre-trained language model
(PLM) on the shared task data, and the second uses few-shot in-context learning
(ICL) with a large language model (LLM). Both achieve high performance as
measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and
first, respectively, of all submissions to the shared task. Expert human
scrutiny indicates that notes generated via the ICL-based approach with GPT-4
are preferred about as often as human-written notes, making it a promising path
toward automated note generation from doctor-patient conversations.
</p></li>
</ul>

<h3>Title: Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02301">http://arxiv.org/abs/2305.02301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02301] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](http://arxiv.org/abs/2305.02301) #large language model</code></li>
<li>Summary: <p>Deploying large language models (LLMs) is challenging because they are memory
inefficient and compute-intensive for practical applications. In reaction,
researchers train smaller task-specific models by either finetuning with human
labels or distilling using LLM-generated labels. However, finetuning and
distillation require large amounts of training data to achieve comparable
performance to LLMs. We introduce Distilling step-by-step, a new mechanism that
(a) trains smaller models that outperform LLMs, and (b) achieves so by
leveraging less training data needed by finetuning or distillation. Our method
extracts LLM rationales as additional supervision for small models within a
multi-task training framework. We present three findings across 4 NLP
benchmarks: First, compared to both finetuning and distillation, our mechanism
achieves better performance with much fewer labeled/unlabeled training
examples. Second, compared to LLMs, we achieve better performance using
substantially smaller model sizes. Third, we reduce both the model size and the
amount of data required to outperform LLMs; our 770M T5 model outperforms the
540B PaLM model using only 80% of available data on a benchmark task.
</p></li>
</ul>

<h3>Title: CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02309">http://arxiv.org/abs/2305.02309</a></li>
<li>Code URL: <a href="https://github.com/salesforce/codegen2">https://github.com/salesforce/codegen2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02309] CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](http://arxiv.org/abs/2305.02309) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable abilities in
representation learning for program synthesis and understanding tasks. The
quality of the learned representations appears to be dictated by the neural
scaling laws as a function of the number of model parameters and observations,
while imposing upper bounds on the model performance by the amount of available
data and compute, which is costly.
</p></li>
</ul>

<p>In this study, we attempt to render the training of LLMs for program
synthesis more efficient by unifying four key components: (1) model
architectures, (2) learning methods, (3) infill sampling, and, (4) data
distributions. Specifically, for the model architecture, we attempt to unify
encoder and decoder-based models into a single prefix-LM. For learning methods,
(i) causal language modeling, (ii) span corruption, (iii) infilling are unified
into a simple learning algorithm. For infill sampling, we explore the claim of
a "free lunch" hypothesis. For data distributions, the effect of a mixture
distribution of programming and natural languages on model performance is
explored.
</p>
<p>We conduct a comprehensive series of empirical experiments on 1B LLMs, for
which failures and successes of this exploration are distilled into four
lessons. We will provide a final recipe for training and release CodeGen2
models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training
framework as open-source: https://github.com/salesforce/CodeGen2.
</p>

<h2>segmentation</h2>
<h3>Title: DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation. (arXiv:2305.01698v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01698">http://arxiv.org/abs/2305.01698</a></li>
<li>Code URL: <a href="https://github.com/melqkiades/deep-wetlands">https://github.com/melqkiades/deep-wetlands</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01698] DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation](http://arxiv.org/abs/2305.01698) #segmentation</code></li>
<li>Summary: <p>Remote sensing has significantly advanced water detection by applying
semantic segmentation techniques to satellite imagery. However, semantic
segmentation remains challenging due to the substantial amount of annotated
data required. This is particularly problematic in wetland detection, where
water extent varies over time and space, necessitating multiple annotations for
the same area. In this paper, we present DeepAqua, a self-supervised deep
learning model that leverages knowledge distillation to eliminate the need for
manual annotations during the training phase. DeepAqua utilizes the Normalized
Difference Water Index (NDWI) as a teacher model to train a Convolutional
Neural Network (CNN) for segmenting water from Synthetic Aperture Radar (SAR)
images. To train the student model, we exploit cases where optical- and
radar-based water masks coincide, enabling the detection of both open and
vegetated water surfaces. Our model represents a significant advancement in
computer vision techniques by effectively training semantic segmentation models
without any manually annotated data. This approach offers a practical solution
for monitoring wetland water extent changes without needing ground truth data,
making it highly adaptable and scalable for wetland conservation efforts.
</p></li>
</ul>

<h3>Title: Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01747">http://arxiv.org/abs/2305.01747</a></li>
<li>Code URL: <a href="https://github.com/moucheng2017/emssl">https://github.com/moucheng2017/emssl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01747] Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations](http://arxiv.org/abs/2305.01747) #segmentation</code></li>
<li>Summary: <p>We study pseudo labelling and its generalisation for semi-supervised
segmentation of medical images. Pseudo labelling has achieved great empirical
successes in semi-supervised learning, by utilising raw inferences on
unlabelled data as pseudo labels for self-training. In our paper, we build a
connection between pseudo labelling and the Expectation Maximization algorithm
which partially explains its empirical successes. We thereby realise that the
original pseudo labelling is an empirical estimation of its underlying full
formulation. Following this insight, we demonstrate the full generalisation of
pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then
provide a variational approach to learn to approximate Bayesian Pseudo Labels,
by learning a threshold to select good quality pseudo labels. In the rest of
the paper, we demonstrate the applications of Pseudo Labelling and its
generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of
medical images on: 1) 3D binary segmentation of lung vessels from CT volumes;
2) 2D multi class segmentation of brain tumours from MRI volumes; 3) 3D binary
segmentation of brain tumours from MRI volumes. We also show that pseudo labels
can enhance the robustness of the learnt representations.
</p></li>
</ul>

<h3>Title: AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation. (arXiv:2305.01836v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01836">http://arxiv.org/abs/2305.01836</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01836] AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation](http://arxiv.org/abs/2305.01836) #segmentation</code></li>
<li>Summary: <p>Segment Anything Model (SAM) has recently shown its powerful effectiveness in
visual segmentation tasks. However, there is less exploration concerning how
SAM works on audio-visual tasks, such as visual sound localization and
segmentation. In this work, we propose a simple yet effective audio-visual
localization and segmentation framework based on the Segment Anything Model,
namely AV-SAM, that can generate sounding object masks corresponding to the
audio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusion
across audio features and visual features from the pre-trained image encoder in
SAM to aggregate cross-modal representations. Then, the aggregated cross-modal
features are fed into the prompt encoder and mask decoder to generate the final
audio-visual segmentation masks. We conduct extensive experiments on
Flickr-SoundNet and AVSBench datasets. The results demonstrate that the
proposed AV-SAM can achieve competitive performance on sounding object
localization and segmentation.
</p></li>
</ul>

<h3>Title: Morphological Classification of Galaxies Using SpinalNet. (arXiv:2305.01873v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01873">http://arxiv.org/abs/2305.01873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01873] Morphological Classification of Galaxies Using SpinalNet](http://arxiv.org/abs/2305.01873) #segmentation</code></li>
<li>Summary: <p>Deep neural networks (DNNs) with a step-by-step introduction of inputs, which
is constructed by imitating the somatosensory system in human body, known as
SpinalNet have been implemented in this work on a Galaxy Zoo dataset. The input
segmentation in SpinalNet has enabled the intermediate layers to take some of
the inputs as well as output of preceding layers thereby reducing the amount of
the collected weights in the intermediate layers. As a result of these, the
authors of SpinalNet reported to have achieved in most of the DNNs they tested,
not only a remarkable cut in the error but also in the large reduction of the
computational costs. Having applied it to the Galaxy Zoo dataset, we are able
to classify the different classes and/or sub-classes of the galaxies. Thus, we
have obtained higher classification accuracies of 98.2, 95 and 82 percents
between elliptical and spirals, between these two and irregulars, and between
10 sub-classes of galaxies, respectively.
</p></li>
</ul>

<h3>Title: Distributional Instance Segmentation: Modeling Uncertainty and High Confidence Predictions with Latent-MaskRCNN. (arXiv:2305.01910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.01910">http://arxiv.org/abs/2305.01910</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.01910] Distributional Instance Segmentation: Modeling Uncertainty and High Confidence Predictions with Latent-MaskRCNN](http://arxiv.org/abs/2305.01910) #segmentation</code></li>
<li>Summary: <p>Object recognition and instance segmentation are fundamental skills in any
robotic or autonomous system. Existing state-of-the-art methods are often
unable to capture meaningful uncertainty in challenging or ambiguous scenes,
and as such can cause critical errors in high-performance applications. In this
paper, we explore a class of distributional instance segmentation models using
latent codes that can model uncertainty over plausible hypotheses of object
masks. For robotic picking applications, we propose a confidence mask method to
achieve the high precision necessary in industrial use cases. We show that our
method can significantly reduce critical errors in robotic systems, including
our newly released dataset of ambiguous scenes in a robotic application. On a
real-world apparel-picking robot, our method significantly reduces double pick
errors while maintaining high performance.
</p></li>
</ul>

<h3>Title: Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving. (arXiv:2305.02008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02008">http://arxiv.org/abs/2305.02008</a></li>
<li>Code URL: <a href="https://github.com/zenseact/zod">https://github.com/zenseact/zod</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02008] Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving](http://arxiv.org/abs/2305.02008) #segmentation</code></li>
<li>Summary: <p>Existing datasets for autonomous driving (AD) often lack diversity and
long-range capabilities, focusing instead on 360{\deg} perception and temporal
reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a
large-scale and diverse multimodal dataset collected over two years in various
European countries, covering an area 9x that of existing datasets. ZOD boasts
the highest range and resolution sensors among comparable datasets, coupled
with detailed keyframe annotations for 2D and 3D objects (up to 245m), road
instance/semantic segmentation, traffic sign recognition, and road
classification. We believe that this unique combination will facilitate
breakthroughs in long-range perception and multi-task learning. The dataset is
composed of Frames, Sequences, and Drives, designed to encompass both data
diversity and support for spatio-temporal learning, sensor fusion,
localization, and mapping. Frames consist of 100k curated camera images with
two seconds of other supporting sensor data, while the 1473 Sequences and 29
Drives include the entire sensor suite for 20 seconds and a few minutes,
respectively. ZOD is the only large-scale AD dataset released under a
permissive license, allowing for both research and commercial use. The dataset
is accompanied by an extensive development kit. Data and more information are
available online (https://zod.zenseact.com).
</p></li>
</ul>

<h3>Title: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model. (arXiv:2305.02034v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02034">http://arxiv.org/abs/2305.02034</a></li>
<li>Code URL: <a href="https://github.com/vitae-transformer/samrs">https://github.com/vitae-transformer/samrs</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02034] Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model](http://arxiv.org/abs/2305.02034) #segmentation</code></li>
<li>Summary: <p>The success of the Segment Anything Model (SAM) demonstrates the significance
of data-centric machine learning. However, due to the difficulties and high
costs associated with annotating Remote Sensing (RS) images, a large amount of
valuable RS data remains unlabeled, particularly at the pixel level. In this
study, we leverage SAM and existing RS object detection datasets to develop an
efficient pipeline for generating a large-scale RS segmentation dataset, dubbed
SAMRS. SAMRS surpasses existing high-resolution RS segmentation datasets in
size by several orders of magnitude, and provides object category, location,
and instance information that can be used for semantic segmentation, instance
segmentation, and object detection, either individually or in combination. We
also provide a comprehensive analysis of SAMRS from various aspects. We hope it
could facilitate research in RS segmentation, particularly in large model
pre-training.
</p></li>
</ul>

<h3>Title: CLUSTSEG: Clustering for Universal Segmentation. (arXiv:2305.02187v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.02187">http://arxiv.org/abs/2305.02187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.02187] CLUSTSEG: Clustering for Universal Segmentation](http://arxiv.org/abs/2305.02187) #segmentation</code></li>
<li>Summary: <p>We present CLUSTSEG, a general, transformer-based framework that tackles
different image segmentation tasks (i.e., superpixel, semantic, instance, and
panoptic) through a unified neural clustering scheme. Regarding queries as
cluster centers, CLUSTSEG is innovative in two aspects:1) cluster centers are
initialized in heterogeneous ways so as to pointedly address task-specific
demands (e.g., instance- or category-level distinctiveness), yet without
modifying the architecture; and 2) pixel-cluster assignment, formalized in a
cross-attention fashion, is alternated with cluster center update, yet without
learning additional parameters. These innovations closely link CLUSTSEG to EM
clustering and make it a transparent and powerful framework that yields
superior results across the above segmentation tasks.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
