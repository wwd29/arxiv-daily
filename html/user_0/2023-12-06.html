<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Fortress: Securing IoT Peripherals with Trusted Execution Environments. (arXiv:2312.02542v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02542">http://arxiv.org/abs/2312.02542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02542]] Fortress: Securing IoT Peripherals with Trusted Execution Environments(http://arxiv.org/abs/2312.02542)</code></li>
<li>Summary: <p>With the increasing popularity of Internet of Things (IoT) devices, securing
sensitive user data has emerged as a major challenge. These devices often
collect confidential information, such as audio and visual data, through
peripheral inputs like microphones and cameras. Such sensitive information is
then exposed to potential threats, either from malicious software with
high-level access rights or transmitted (sometimes inadvertently) to untrusted
cloud services. In this paper, we propose a generic design to enhance the
privacy in IoT-based systems by isolating peripheral I/O memory regions in a
secure kernel space of a trusted execution environment (TEE). Only a minimal
set of peripheral driver code, resident within the secure kernel, can access
this protected memory area.
</p>
<p>This design effectively restricts any unauthorised access by system software,
including the operating system and hypervisor. The sensitive peripheral data is
then securely transferred to a user-space TEE, where obfuscation mechanisms can
be applied before it is relayed to third parties, e.g., the cloud. To validate
our architectural approach, we provide a proof-of-concept implementation of our
design by securing an audio peripheral based on inter-IC sound (I2S), a serial
bus to interconnect audio devices. The experimental results show that our
design offers a robust security solution with an acceptable computational
overhead.
</p></li>
</ul>

<h3>Title: ESP2CS: Securing Internet of Vehicles through Blockchain-enabled Communications and Payments. (arXiv:2312.02589v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02589">http://arxiv.org/abs/2312.02589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02589]] ESP2CS: Securing Internet of Vehicles through Blockchain-enabled Communications and Payments(http://arxiv.org/abs/2312.02589)</code></li>
<li>Summary: <p>The burgeoning domain of the Internet of Vehicles (IoV), a subset of the
Internet of Things (IoT), promises to revolutionize transportation through
enhanced safety, efficiency, and environmental sustainability. By amalgamating
technologies like sensors and cloud computing, the IoV paves the way for
optimized traffic management, heightened vehicle safety, and the birth of novel
business paradigms. However, this growth is shadowed by significant security
concerns, especially in the communication and payment sectors. Addressing the
pressing need for secure Vehicle to Everything (V2X) communications and
payments amidst rising cyber threats, this research introduces the Ethereum
based Secure Payment and Communication Solution (ESP2CS). Utilizing Ethereum as
a middleware, ESP2CS ensures robust and secure V2X interactions. The solution
is complemented by an Android Auto application for vehicles, streamlining inter
vehicle communication, parking space detection, and transaction management.
Furthermore, dedicated Android applications are developed for parking space
renters and the parking IoT system. Preliminary evaluations underscore ESP2CS's
superior cost effectiveness, integrity and consistency over contemporary
solutions, with Ethereum bolstering both security and efficiency.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security. (arXiv:2312.02699v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02699">http://arxiv.org/abs/2312.02699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02699]] Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security(http://arxiv.org/abs/2312.02699)</code></li>
<li>Summary: <p>The auto-management of vehicle entrance and parking in any organization is a
complex challenge encompassing record-keeping, efficiency, and security
concerns. Manual methods for tracking vehicles and finding parking spaces are
slow and a waste of time. To solve the problem of auto management of vehicle
entrance and parking, we have utilized state-of-the-art deep learning models
and automated the process of vehicle entrance and parking into any
organization. To ensure security, our system integrated vehicle detection,
license number plate verification, and face detection and recognition models to
ensure that the person and vehicle are registered with the organization. We
have trained multiple deep-learning models for vehicle detection, license
number plate detection, face detection, and recognition, however, the YOLOv8n
model outperformed all the other models. Furthermore, License plate recognition
is facilitated by Google's Tesseract-OCR Engine. By integrating these
technologies, the system offers efficient vehicle detection, precise
identification, streamlined record keeping, and optimized parking slot
allocation in buildings, thereby enhancing convenience, accuracy, and security.
Future research opportunities lie in fine-tuning system performance for a wide
range of real-world applications.
</p></li>
</ul>

<h3>Title: Can a Tabula Recta provide security in the XXI century?. (arXiv:2312.02869v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02869">http://arxiv.org/abs/2312.02869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02869]] Can a Tabula Recta provide security in the XXI century?(http://arxiv.org/abs/2312.02869)</code></li>
<li>Summary: <p>In the not so unlikely scenario of total compromise of computers accessible
to a group of users, they might be tempted to resort to human-computable
paper-and-pencil cryptographic methods aided by a classic Tabula Recta, which
helps to perform addition and subtraction directly with letters. But do these
classic algorithms, or some new ones using the same simple tools, have any
chance against computer-aided cryptanalysis? In this paper I discuss how some
human-computable algorithms can indeed afford sufficient security in this
situation, drawing conclusions from computer-based statistical analysis. Three
kinds of algorithms are discussed: those that concentrate entropy from shared
text sources, stream ciphers based on arithmetic of non-binary spaces, and
hash-like algorithms that may be used to generate a password from a challenge
text.
</p></li>
</ul>

<h3>Title: LpiCT: A logic security analysis framework for protocols. (arXiv:2312.02171v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02171">http://arxiv.org/abs/2312.02171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02171]] LpiCT: A logic security analysis framework for protocols(http://arxiv.org/abs/2312.02171)</code></li>
<li>Summary: <p>The pi calculus is a basic theory of mobile communication based on the notion
of interaction, which, aimed at analyzing and modelling the behaviors of
communication process in communicating and mobile systems, is widely applied to
the security analysis of cryptographic protocol's design and implementation.
But the pi calculus does not provide perfect logic security analysis, so the
logic flaws in the design and the implementation of a cryptographic protocol
can not be discovered in time. The aim is to analyze whether there are logic
flaws in the design and the implementation of a cryptographic protocol, so as
to ensure the security of the cryptographic protocol when it is encoded into a
software and implemented. This paper introduces logic rules and proofs, binary
tree and the KMP algorithm, and proposes a new extension the pi calculus
theory, a logic security analysis framework and an algorithm. This paper
presents the logic security proof and analysis of TLS1.3 protocol's
interactional implementation process. Empirical results show that the new
extension theory, the logic security analysis framework and the algorithm can
effectively analyze whether there are logic flaws in the design and the
implementation of a cryptographic protocol. The security of cryptographic
protocols depends not only on cryptographic primitives, but also on the coding
of cryptographic protocols and the environment in which they are implemented.
The security analysis framework of cryptographic protocol implementation
proposed in this paper can ensure the security of protocol implementation.
</p></li>
</ul>

<h3>Title: UCCA: A Verified Architecture for Compartmentalization of Untrusted Code Sections in Resource-Constrained Devices. (arXiv:2312.02348v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02348">http://arxiv.org/abs/2312.02348</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02348]] UCCA: A Verified Architecture for Compartmentalization of Untrusted Code Sections in Resource-Constrained Devices(http://arxiv.org/abs/2312.02348)</code></li>
<li>Summary: <p>Micro-controller units (MCUs) implement the de facto interface between the
physical and digital worlds. As a consequence, they appear in a variety of
sensing/actuation applications, from smart personal spaces to complex
industrial control systems and safety-critical medical equipment. While many of
these devices perform safety- and time-critical tasks, they often lack support
for security features compatible with their importance to overall system
functions. This lack of architectural support leaves them vulnerable to
run-time attacks that can remotely alter their intended behavior, with
potentially catastrophic consequences. In particular, we note that MCU software
often includes untrusted third-party libraries (some of them closed-source)
that are blindly used within MCU programs, without proper isolation from the
rest of the system. In turn, a single vulnerability (or intentional backdoor)
in one such third-party software can often compromise the entire MCU software
state.
</p>
<p>In this paper, we tackle this problem by proposing, demonstrating security,
and formally verifying the implementation of UCCA: an Untrusted Code
Compartment Architecture. UCCA provides flexible hardware-enforced isolation of
untrusted code sections (e.g., third-party software modules) in
resource-constrained and time-critical MCUs. To demonstrate UCCA's
practicality, we implement an open-source version of the design on a real
resource-constrained MCU: the well-known TI MSP430. Our evaluation shows that
UCCA incurs little overhead and is affordable even to lowest-end MCUs,
requiring significantly less overhead and assumptions than prior related work.
</p></li>
</ul>

<h3>Title: Skipping Scheme for Gate-hiding Garbled Circuits. (arXiv:2312.02514v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02514">http://arxiv.org/abs/2312.02514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02514]] Skipping Scheme for Gate-hiding Garbled Circuits(http://arxiv.org/abs/2312.02514)</code></li>
<li>Summary: <p>In classic settings of garbled circuits, each gate type is leaked to improve
both space and speed optimization. Zahur et al. have shown in EUROCRYPT 2015
that a typical linear garbling scheme requires at least two $\lambda$-bit
elements per gate with a security parameter of $\lambda$, which limits their
efficiency. In contrast to typical garbled circuits, gate-hiding garbled
circuits have the potential to drastically reduce time costs, although they
have been underappreciated.
</p>
<p>We propose the first skipping scheme for gate-hiding garbled circuits to
enhance the efficiency of evaluation by observing prime implicants. Our scheme
introduces skip gates to eliminate the need to calculate the entire circuit,
enabling unnecessary execution paths to be avoided. We also introduce two
variants of our scheme that balance security with parallelism. A proof of
hybrid security that combines simulation-based and symmetry-based security in
semi-honest scenarios is presented to demonstrate its security under
gate-hiding conditions. Our scheme will inspire new directions to improve the
general garbling scheme and lead to more practical ones.
</p></li>
</ul>

<h3>Title: Understanding Ethereum Mempool Security under Asymmetric DoS by Symbolic Fuzzing. (arXiv:2312.02642v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02642">http://arxiv.org/abs/2312.02642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02642]] Understanding Ethereum Mempool Security under Asymmetric DoS by Symbolic Fuzzing(http://arxiv.org/abs/2312.02642)</code></li>
<li>Summary: <p>In blockchains, mempool controls transaction flow before consensus, denial of
whose service hurts the health and security of blockchain networks. This paper
presents MPFUZZ, the first mempool fuzzer to find asymmetric DoS bugs by
symbolically exploring mempool state space and optimistically estimating the
promisingness an intermediate state is in reaching bug oracles. Compared to the
baseline blockchain fuzzers, MPFUZZ achieves a &gt; 100x speedup in finding known
DETER exploits. Running MPFUZZ on six major Ethereum clients leads to the
discovering of new mempool vulnerabilities, which exhibit a wide variety of
sophisticated patterns including stealthy mempool eviction and mempool locking.
Rule-based mitigation schemes are proposed against newly discovered
vulnerabilities.
</p></li>
</ul>

<h3>Title: A Review of Password-less User Authentication Schemes. (arXiv:2312.02845v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02845">http://arxiv.org/abs/2312.02845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02845]] A Review of Password-less User Authentication Schemes(http://arxiv.org/abs/2312.02845)</code></li>
<li>Summary: <p>Since the demise of the password was predicted in 2004, different attempts in
industry and academia have been made to create an alternative for the use of
passwords in authentication, without compromising on security and user
experience. This review examines password-less authentication schemes that have
been proposed since after the death knell was placed on passwords in 2004. We
start with a brief discussion of the requirements of authentication systems and
then identify various password-less authentication proposals to date. We then
evaluate the truly password-less and practical schemes using a framework that
examines authentication credentials based on their impact on user experience,
overall security, and ease of deployment. The findings of this review observe a
difficulty in balancing security with a user experience compared to that of
passwords in new password-less schemes, providing the opportunity for new
applied research to leverage existing knowledge and combine technologies and
techniques in innovative ways that can address this imbalance.
</p></li>
</ul>

<h3>Title: Zero Trust for Cyber Resilience. (arXiv:2312.02882v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02882">http://arxiv.org/abs/2312.02882</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02882]] Zero Trust for Cyber Resilience(http://arxiv.org/abs/2312.02882)</code></li>
<li>Summary: <p>The increased connectivity and potential insider threats make traditional
network defense vulnerable. Instead of assuming that everything behind the
security perimeter is safe, the zero-trust security model verifies every
incoming request before granting access. This chapter draws attention to the
cyber resilience within the zero-trust model. We introduce the evolution from
traditional perimeter-based security to zero trust and discuss their
difference. Two key elements of the zero-trust engine are trust evaluation (TE)
and policy engine (PE). We introduce the design of the two components and
discuss how their interplay would contribute to cyber resilience. Dynamic game
theory and learning are applied as quantitative approaches to achieve automated
zero-trust cyber resilience. Several case studies and implementations are
introduced to illustrate the benefits of such a security model.
</p></li>
</ul>

<h3>Title: Rethinking Adversarial Training with Neural Tangent Kernel. (arXiv:2312.02236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02236">http://arxiv.org/abs/2312.02236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02236]] Rethinking Adversarial Training with Neural Tangent Kernel(http://arxiv.org/abs/2312.02236)</code></li>
<li>Summary: <p>Adversarial training (AT) is an important and attractive topic in deep
learning security, exhibiting mysteries and odd properties. Recent studies of
neural network training dynamics based on Neural Tangent Kernel (NTK) make it
possible to reacquaint AT and deeply analyze its properties. In this paper, we
perform an in-depth investigation of AT process and properties with NTK, such
as NTK evolution. We uncover three new findings that are missed in previous
works. First, we disclose the impact of data normalization on AT and the
importance of unbiased estimators in batch normalization layers. Second, we
experimentally explore the kernel dynamics and propose more time-saving AT
methods. Third, we study the spectrum feature inside the kernel to address the
catastrophic overfitting problem. To the best of our knowledge, it is the first
work leveraging the observations of kernel dynamics to improve existing AT
methods.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition. (arXiv:2312.02185v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02185">http://arxiv.org/abs/2312.02185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02185]] Virtual Fusion with Contrastive Learning for Single Sensor-based Activity Recognition(http://arxiv.org/abs/2312.02185)</code></li>
<li>Summary: <p>Various types of sensors can be used for Human Activity Recognition (HAR),
and each of them has different strengths and weaknesses. Sometimes a single
sensor cannot fully observe the user's motions from its perspective, which
causes wrong predictions. While sensor fusion provides more information for
HAR, it comes with many inherent drawbacks like user privacy and acceptance,
costly set-up, operation, and maintenance. To deal with this problem, we
propose Virtual Fusion - a new method that takes advantage of unlabeled data
from multiple time-synchronized sensors during training, but only needs one
sensor for inference. Contrastive learning is adopted to exploit the
correlation among sensors. Virtual Fusion gives significantly better accuracy
than training with the same single sensor, and in some cases, it even surpasses
actual fusion using multiple sensors at test time. We also extend this method
to a more general version called Actual Fusion within Virtual Fusion (AFVF),
which uses a subset of training sensors during inference. Our method achieves
state-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark
datasets. Implementation is available upon request.
</p></li>
</ul>

<h3>Title: FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation. (arXiv:2312.02327v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02327">http://arxiv.org/abs/2312.02327</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02327]] FLea: Improving federated learning on scarce and label-skewed data via privacy-preserving feature augmentation(http://arxiv.org/abs/2312.02327)</code></li>
<li>Summary: <p>Learning a global model by abstracting the knowledge, distributed across
multiple clients, without aggregating the raw data is the primary goal of
Federated Learning (FL). Typically, this works in rounds alternating between
parallel local training at several clients, followed by model aggregation at a
server. We found that existing FL methods under-perform when local datasets are
small and present severe label skew as these lead to over-fitting and local
model bias. This is a realistic setting in many real-world applications. To
address the problem, we propose \textit{FLea}, a unified framework that tackles
over-fitting and local bias by encouraging clients to exchange
privacy-protected features to aid local training. The features refer to
activations from an intermediate layer of the model, which are obfuscated
before being shared with other clients to protect sensitive information in the
data. \textit{FLea} leverages a novel way of combining local and shared
features as augmentations to enhance local model learning. Our extensive
experiments demonstrate that \textit{FLea} outperforms the start-of-the-art FL
methods, sharing only model parameters, by up to $17.6\%$, and FL methods that
share data augmentations by up to $6.3\%$, while reducing the privacy
vulnerability associated with shared data augmentations.
</p></li>
</ul>

<h3>Title: Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation. (arXiv:2312.02400v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02400">http://arxiv.org/abs/2312.02400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02400]] Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation(http://arxiv.org/abs/2312.02400)</code></li>
<li>Summary: <p>DP-SGD has emerged as a popular method to protect personally identifiable
information in deep learning applications. Unfortunately, DP-SGD's per-sample
gradient clipping and uniform noise addition during training can significantly
degrade model utility. To enhance the model's utility, researchers proposed
various adaptive DP-SGD methods. However, we examine and discover that these
techniques result in greater privacy leakage or lower accuracy than the
traditional DP-SGD method, or a lack of evaluation on a complex data set such
as CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our
method automates clipping threshold estimation based on the DL model's gradient
norm and scales the gradients of each training sample without losing gradient
information. This helps to improve the algorithm's utility while using a less
privacy budget. To further improve accuracy, we introduce automatic noise
multiplier decay mechanisms to decrease the noise multiplier after every epoch.
Finally, we develop closed-form mathematical expressions using tCDP accountant
for automatic noise multiplier and automatic clipping threshold estimation.
Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms
existing SOTA DP-SGD methods in privacy and accuracy on various benchmark
datasets. We also show that privacy can be improved by lowering the scale
factor and using learning rate schedulers without significantly reducing
accuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier,
improves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10,
CIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a
substantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37
for the corresponding data sets.
</p></li>
</ul>

<h3>Title: When PETs misbehave: A Contextual Integrity analysis. (arXiv:2312.02509v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02509">http://arxiv.org/abs/2312.02509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02509]] When PETs misbehave: A Contextual Integrity analysis(http://arxiv.org/abs/2312.02509)</code></li>
<li>Summary: <p>Privacy enhancing technologies, or PETs, have been hailed as a promising
means to protect privacy without compromising on the functionality of digital
services. At the same time, and partly because they may encode a narrow
conceptualization of privacy as confidentiality that is popular among
policymakers, engineers and the public, PETs risk being co-opted to promote
privacy-invasive practices. In this paper, we resort to the theory of
Contextual Integrity to explain how privacy technologies may be misused to
erode privacy. To illustrate, we consider three PETs and scenarios: anonymous
credentials for age verification, client-side scanning for illegal content
detection, and homomorphic encryption for machine learning model training.
Using the theory of Contextual Integrity, we reason about the notion of privacy
that these PETs encode, and show that CI enables us to identify and reason
about the limitations of PETs and their misuse, and which may ultimately lead
to privacy violations.
</p></li>
</ul>

<h3>Title: Privacy-Aware Data Acquisition under Data Similarity in Regression Markets. (arXiv:2312.02611v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02611">http://arxiv.org/abs/2312.02611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02611]] Privacy-Aware Data Acquisition under Data Similarity in Regression Markets(http://arxiv.org/abs/2312.02611)</code></li>
<li>Summary: <p>Data markets facilitate decentralized data exchange for applications such as
prediction, learning, or inference. The design of these markets is challenged
by varying privacy preferences as well as data similarity among data owners.
Related works have often overlooked how data similarity impacts pricing and
data value through statistical information leakage. We demonstrate that data
similarity and privacy preferences are integral to market design and propose a
query-response protocol using local differential privacy for a two-party data
acquisition mechanism. In our regression data market model, we analyze
strategic interactions between privacy-aware owners and the learner as a
Stackelberg game over the asked price and privacy factor. Finally, we
numerically evaluate how data similarity affects market participation and
traded data value.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems. (arXiv:2312.02490v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02490">http://arxiv.org/abs/2312.02490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02490]] Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems(http://arxiv.org/abs/2312.02490)</code></li>
<li>Summary: <p>Intrusion detection systems (IDSs) play a critical role in protecting
billions of IoT devices from malicious attacks. However, the IDSs for IoT
devices face inherent challenges of IoT systems, including the heterogeneity of
IoT data/devices, the high dimensionality of training data, and the imbalanced
data. Moreover, the deployment of IDSs on IoT systems is challenging, and
sometimes impossible, due to the limited resources such as memory/storage and
computing capability of typical IoT devices. To tackle these challenges, this
article proposes a novel deep neural network/architecture called Constrained
Twin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with
more separable/distinguishable and lower-dimensional representation data.
Additionally, in comparison to the state-of-the-art neural networks used in
IDSs, CTVAE requires less memory/storage and computing power, hence making it
more suitable for IoT IDS systems. Extensive experiments with the 11 most
popular IoT botnet datasets show that CTVAE can boost around 1% in terms of
accuracy and Fscore in detection attack compared to the state-of-the-art
machine learning and representation learning methods, whilst the running time
for attack detection is lower than 2E-6 seconds and the model size is lower
than 1 MB. We also further investigate various characteristics of CTVAE in the
latent space and in the reconstruction representation to demonstrate its
efficacy compared with current well-known methods.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers. (arXiv:2312.02220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02220">http://arxiv.org/abs/2312.02220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02220]] QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers(http://arxiv.org/abs/2312.02220)</code></li>
<li>Summary: <p>In recent years, there has been a significant trend in deep neural networks
(DNNs), particularly transformer-based models, of developing ever-larger and
more capable models. While they demonstrate state-of-the-art performance, their
growing scale requires increased computational resources (e.g., GPUs with
greater memory capacity). To address this problem, quantization techniques
(i.e., low-bit-precision representation and matrix multiplication) have been
proposed. Most quantization techniques employ a static strategy in which the
model parameters are quantized, either during training or inference, without
considering the test-time sample. In contrast, dynamic quantization techniques,
which have become increasingly popular, adapt during inference based on the
input provided, while maintaining full-precision performance. However, their
dynamic behavior and average-case performance assumption makes them vulnerable
to a novel threat vector -- adversarial attacks that target the model's
efficiency and availability. In this paper, we present QuantAttack, a novel
attack that targets the availability of quantized models, slowing down the
inference, and increasing memory usage and energy consumption. We show that
carefully crafted adversarial examples, which are designed to exhaust the
resources of the operating system, can trigger worst-case performance. In our
experiments, we demonstrate the effectiveness of our attack on vision
transformers on a wide range of tasks, both uni-modal and multi-modal. We also
examine the effect of different attack variants (e.g., a universal
perturbation) and the transferability between different models.
</p></li>
</ul>

<h3>Title: Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network. (arXiv:2312.02224v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02224">http://arxiv.org/abs/2312.02224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02224]] Tracing Hyperparameter Dependencies for Model Parsing via Learnable Graph Pooling Network(http://arxiv.org/abs/2312.02224)</code></li>
<li>Summary: <p>Model Parsing defines the research task of predicting hyperparameters of the
generative model (GM), given a generated image as input. Since a diverse set of
hyperparameters is jointly employed by the generative model, and dependencies
often exist among them, it is crucial to learn these hyperparameter
dependencies for the improved model parsing performance. To explore such
important dependencies, we propose a novel model parsing method called
Learnable Graph Pooling Network (LGPN). Specifically, we transform model
parsing into a graph node classification task, using graph nodes and edges to
represent hyperparameters and their dependencies, respectively. Furthermore,
LGPN incorporates a learnable pooling-unpooling mechanism tailored to model
parsing, which adaptively learns hyperparameter dependencies of GMs used to
generate the input image. We also extend our proposed method to CNN-generated
image detection and coordinate attacks detection. Empirically, we achieve
state-of-the-art results in model parsing and its extended applications,
showing the effectiveness of our method. Our source code are available.
</p></li>
</ul>

<h3>Title: Scaling Laws for Adversarial Attacks on Language Model Activations. (arXiv:2312.02780v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02780">http://arxiv.org/abs/2312.02780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02780]] Scaling Laws for Adversarial Attacks on Language Model Activations(http://arxiv.org/abs/2312.02780)</code></li>
<li>Summary: <p>We explore a class of adversarial attacks targeting the activations of
language models. By manipulating a relatively small subset of model
activations, $a$, we demonstrate the ability to control the exact prediction of
a significant number (in some cases up to 1000) of subsequent tokens $t$. We
empirically verify a scaling law where the maximum number of target tokens
$t_\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose
activations the attacker controls as $t_\mathrm{max} = \kappa a$. We find that
the number of bits of control in the input space needed to control a single bit
in the output space (what we call attack resistance $\chi$) is remarkably
constant between $\approx 16$ and $\approx 25$ over 2 orders of magnitude of
model sizes for different language models. Compared to attacks on tokens,
attacks on activations are predictably much stronger, however, we identify a
surprising regularity where one bit of input steered either via activations or
via tokens is able to exert control over a similar amount of output bits. This
gives support for the hypothesis that adversarial attacks are a consequence of
dimensionality mismatch between the input and output spaces. A practical
implication of the ease of attacking language model activations instead of
tokens is for multi-modal and selected retrieval models, where additional data
sources are added as activations directly, sidestepping the tokenized input.
This opens up a new, broad attack surface. By using language models as a
controllable test-bed to study adversarial attacks, we were able to experiment
with input-output dimensions that are inaccessible in computer vision,
especially where the output dimension dominates.
</p></li>
</ul>

<h3>Title: CVE representation to build attack positions graphs. (arXiv:2312.02585v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02585">http://arxiv.org/abs/2312.02585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02585]] CVE representation to build attack positions graphs(http://arxiv.org/abs/2312.02585)</code></li>
<li>Summary: <p>In cybersecurity, CVEs (Common Vulnerabilities and Exposures) are publicly
disclosed hardware or software vulnerabilities. These vulnerabilities are
documented and listed in the NVD database maintained by the NIST. Knowledge of
the CVEs impacting an information system provides a measure of its level of
security. This article points out that these vulnerabilities should be
described in greater detail to understand how they could be chained together in
a complete attack scenario. This article presents the first proposal for the
CAPG format, which is a method for representing a CVE vulnerability, a
corresponding exploit, and associated attack positions.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Singular Regularization with Information Bottleneck Improves Model's Adversarial Robustness. (arXiv:2312.02237v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02237">http://arxiv.org/abs/2312.02237</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02237]] Singular Regularization with Information Bottleneck Improves Model's Adversarial Robustness(http://arxiv.org/abs/2312.02237)</code></li>
<li>Summary: <p>Adversarial examples are one of the most severe threats to deep learning
models. Numerous works have been proposed to study and defend adversarial
examples. However, these works lack analysis of adversarial information or
perturbation, which cannot reveal the mystery of adversarial examples and lose
proper interpretation. In this paper, we aim to fill this gap by studying
adversarial information as unstructured noise, which does not have a clear
pattern. Specifically, we provide some empirical studies with singular value
decomposition, by decomposing images into several matrices, to analyze
adversarial information for different attacks. Based on the analysis, we
propose a new module to regularize adversarial information and combine
information bottleneck theory, which is proposed to theoretically restrict
intermediate representations. Therefore, our method is interpretable. Moreover,
the fashion of our design is a novel principle that is general and unified.
Equipped with our new module, we evaluate two popular model structures on two
mainstream datasets with various adversarial attacks. The results indicate that
the improvement in robust accuracy is significant. On the other hand, we prove
that our method is efficient with only a few additional parameters and able to
be explained under regional faithfulness analysis.
</p></li>
</ul>

<h3>Title: Calibrated Uncertainties for Neural Radiance Fields. (arXiv:2312.02350v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02350">http://arxiv.org/abs/2312.02350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02350]] Calibrated Uncertainties for Neural Radiance Fields(http://arxiv.org/abs/2312.02350)</code></li>
<li>Summary: <p>Neural Radiance Fields have achieved remarkable results for novel view
synthesis but still lack a crucial component: precise measurement of
uncertainty in their predictions. Probabilistic NeRF methods have tried to
address this, but their output probabilities are not typically accurately
calibrated, and therefore do not capture the true confidence levels of the
model. Calibration is a particularly challenging problem in the sparse-view
setting, where additional held-out data is unavailable for fitting a calibrator
that generalizes to the test distribution. In this paper, we introduce the
first method for obtaining calibrated uncertainties from NeRF models. Our
method is based on a robust and efficient metric to calculate per-pixel
uncertainties from the predictive posterior distribution. We propose two
techniques that eliminate the need for held-out data. The first, based on patch
sampling, involves training two NeRF models for each scene. The second is a
novel meta-calibrator that only requires the training of one NeRF model. Our
proposed approach for obtaining calibrated uncertainties achieves
state-of-the-art uncertainty in the sparse-view setting while maintaining image
quality. We further demonstrate our method's effectiveness in applications such
as view enhancement and next-best view selection.
</p></li>
</ul>

<h3>Title: Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks. (arXiv:2312.02366v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02366">http://arxiv.org/abs/2312.02366</a></li>
<li>Code URL: https://github.com/mohammedsb/dinov2formedical</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02366]] Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks(http://arxiv.org/abs/2312.02366)</code></li>
<li>Summary: <p>The integration of deep learning systems into the medical domain has been
hindered by the resource-intensive process of data annotation and the inability
of these systems to generalize to different data distributions. Foundation
models, which are models pre-trained on large datasets, have emerged as a
solution to reduce reliance on annotated data and enhance model
generalizability and robustness. DINOv2, an open-source foundation model
pre-trained with self-supervised learning on 142 million curated natural
images, excels in extracting general-purpose visual representations, exhibiting
promising capabilities across various vision tasks. Nevertheless, a critical
question remains unanswered regarding DINOv2's adaptability to radiological
imaging, and the clarity on whether its features are sufficiently general to
benefit radiology image analysis is yet to be established. Therefore, this
study comprehensively evaluates DINOv2 for radiology, conducting over 100
experiments across diverse modalities (X-ray, CT, and MRI). Tasks include
disease classification and organ segmentation on both 2D and 3D images,
evaluated under different settings like kNN, few-shot learning, linear-probing,
end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the
effectiveness and generalizability of the DINOv2 feature embeddings.
Comparative analyses with established medical image analysis models, U-Net and
TransUnet for segmentation, and CNN and ViT models pre-trained via supervised,
weakly supervised, and self-supervised learning for classification, reveal
DINOv2's superior performance in segmentation tasks and competitive results in
disease classification. The findings contribute insights to potential avenues
for optimizing pre-training strategies for medical imaging and enhancing the
broader understanding of DINOv2's role in bridging the gap between natural and
radiological image analysis.
</p></li>
</ul>

<h3>Title: Lenna: Language Enhanced Reasoning Detection Assistant. (arXiv:2312.02433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02433">http://arxiv.org/abs/2312.02433</a></li>
<li>Code URL: https://github.com/meituan-automl/lenna</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02433]] Lenna: Language Enhanced Reasoning Detection Assistant(http://arxiv.org/abs/2312.02433)</code></li>
<li>Summary: <p>With the fast-paced development of multimodal large language models (MLLMs),
we can now converse with AI systems in natural languages to understand images.
However, the reasoning power and world knowledge embedded in the large language
models have been much less investigated and exploited for image perception
tasks. In this paper, we propose Lenna, a language-enhanced reasoning detection
assistant, which utilizes the robust multimodal feature representation of
MLLMs, while preserving location information for detection. This is achieved by
incorporating an additional &lt;DET&gt; token in the MLLM vocabulary that is free of
explicit semantic context but serves as a prompt for the detector to identify
the corresponding position. To evaluate the reasoning capability of Lenna, we
construct a ReasonDet dataset to measure its performance on reasoning-based
detection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet
and comes with significantly low training costs. It also incurs minimal
transferring overhead when extended to other tasks. Our code and model will be
available at https://git.io/Lenna.
</p></li>
</ul>

<h3>Title: AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation. (arXiv:2312.02512v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02512">http://arxiv.org/abs/2312.02512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02512]] AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation(http://arxiv.org/abs/2312.02512)</code></li>
<li>Summary: <p>This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. The demo page is available on
https://choijeongsoo.github.io/av2av.
</p></li>
</ul>

<h3>Title: Towards Open-set Gesture Recognition via Feature Activation Enhancement and Orthogonal Prototype Learning. (arXiv:2312.02535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02535">http://arxiv.org/abs/2312.02535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02535]] Towards Open-set Gesture Recognition via Feature Activation Enhancement and Orthogonal Prototype Learning(http://arxiv.org/abs/2312.02535)</code></li>
<li>Summary: <p>Gesture recognition is a foundational task in human-machine interaction
(HMI). While there has been significant progress in gesture recognition based
on surface electromyography (sEMG), accurate recognition of predefined gestures
only within a closed set is still inadequate in practice. It is essential to
effectively discern and reject unknown gestures of disinterest in a robust
system. Numerous methods based on prototype learning (PL) have been proposed to
tackle this open set recognition (OSR) problem. However, they do not fully
explore the inherent distinctions between known and unknown classes. In this
paper, we propose a more effective PL method leveraging two novel and inherent
distinctions, feature activation level and projection inconsistency.
Specifically, the Feature Activation Enhancement Mechanism (FAEM) widens the
gap in feature activation values between known and unknown classes.
Furthermore, we introduce Orthogonal Prototype Learning (OPL) to construct
multiple perspectives. OPL acts to project a sample from orthogonal directions
to maximize the distinction between its two projections, where unknown samples
will be projected near the clusters of different known classes while known
samples still maintain intra-class similarity. Our proposed method
simultaneously achieves accurate closed-set classification for predefined
gestures and effective rejection for unknown gestures. Extensive experiments
demonstrate its efficacy and superiority in open-set gesture recognition based
on sEMG.
</p></li>
</ul>

<h3>Title: Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning. (arXiv:2312.02546v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02546">http://arxiv.org/abs/2312.02546</a></li>
<li>Code URL: https://github.com/tmllab/machine_vision_therapy</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02546]] Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning(http://arxiv.org/abs/2312.02546)</code></li>
<li>Summary: <p>Although vision models such as Contrastive Language-Image Pre-Training (CLIP)
show impressive generalization performance, their zero-shot robustness is still
limited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead
of undesirably providing human supervision as commonly done, it is possible to
take advantage of Multi-modal Large Language Models (MLLMs) that hold powerful
visual understanding abilities. However, MLLMs are shown to struggle with
vision problems due to the incompatibility of tasks, thus hindering their
utilization. In this paper, we propose to effectively leverage MLLMs to conduct
Machine Vision Therapy which aims to rectify the noisy predictions from vision
models. By fine-tuning with the denoised labels, the learning model performance
can be boosted in an unsupervised manner. To solve the incompatibility issue,
we propose a novel Denoising In-Context Learning (DICL) strategy to align
vision tasks with MLLMs. Concretely, by estimating a transition matrix that
captures the probability of one class being confused with another, an
instruction containing a correct exemplar and an erroneous one from the most
probable noisy class can be constructed. Such an instruction can help any MLLMs
with ICL ability to detect and rectify incorrect predictions of vision models.
Through extensive experiments on ImageNet, WILDS, DomainBed, and other OOD
datasets, we carefully validate the quantitative and qualitative effectiveness
of our method. Our code is available at
https://github.com/tmllab/Machine_Vision_Therapy.
</p></li>
</ul>

<h3>Title: Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics. (arXiv:2312.02673v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02673">http://arxiv.org/abs/2312.02673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02673]] Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics(http://arxiv.org/abs/2312.02673)</code></li>
<li>Summary: <p>A backdoor attack in deep learning inserts a hidden backdoor in the model to
trigger malicious behavior upon specific input patterns. Existing detection
approaches assume a metric space (for either the original inputs or their
latent representations) in which normal samples and malicious samples are
separable. We show that this assumption has a severe limitation by introducing
a novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures
the difference between normal samples and malicious samples.
</p>
<p>To overcome this limitation, we move beyond looking for a perfect metric
space that would work for different deep-learning models, and instead resort to
more robust topological constructs. We propose TED (Topological Evolution
Dynamics) as a model-agnostic basis for robust backdoor detection. The main
idea of TED is to view a deep-learning model as a dynamical system that evolves
inputs to outputs. In such a dynamical system, a benign input follows a natural
evolution trajectory similar to other benign inputs. In contrast, a malicious
sample displays a distinct trajectory, since it starts close to benign samples
but eventually shifts towards the neighborhood of attacker-specified target
samples to activate the backdoor.
</p>
<p>Extensive evaluations are conducted on vision and natural language datasets
across different network architectures. The results demonstrate that TED not
only achieves a high detection rate, but also significantly outperforms
existing state-of-the-art detection approaches, particularly in addressing the
sophisticated SSDT attack. The code to reproduce the results is made public on
GitHub.
</p></li>
</ul>

<h3>Title: (Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More. (arXiv:2312.02708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02708">http://arxiv.org/abs/2312.02708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02708]] (Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More(http://arxiv.org/abs/2312.02708)</code></li>
<li>Summary: <p>A machine learning model is traditionally considered robust if its prediction
remains (almost) constant under input perturbations with small norm. However,
real-world tasks like molecular property prediction or point cloud segmentation
have inherent equivariances, such as rotation or permutation equivariance. In
such tasks, even perturbations with large norm do not necessarily change an
input's semantic content. Furthermore, there are perturbations for which a
model's prediction explicitly needs to change. For the first time, we propose a
sound notion of adversarial robustness that accounts for task equivariance. We
then demonstrate that provable robustness can be achieved by (1) choosing a
model that matches the task's equivariances (2) certifying traditional
adversarial robustness. Certification methods are, however, unavailable for
many models, such as those with continuous equivariances. We close this gap by
developing the framework of equivariance-preserving randomized smoothing, which
enables architecture-agnostic certification. We additionally derive the first
architecture-specific graph edit distance certificates, i.e. sound robustness
guarantees for isomorphism equivariant tasks like node classification. Overall,
a sound notion of robustness is an important prerequisite for future work at
the intersection of robust and geometric machine learning.
</p></li>
</ul>

<h3>Title: Reconsideration on evaluation of machine learning models in continuous monitoring using wearables. (arXiv:2312.02300v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02300">http://arxiv.org/abs/2312.02300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02300]] Reconsideration on evaluation of machine learning models in continuous monitoring using wearables(http://arxiv.org/abs/2312.02300)</code></li>
<li>Summary: <p>This paper explores the challenges in evaluating machine learning (ML) models
for continuous health monitoring using wearable devices beyond conventional
metrics. We state the complexities posed by real-world variability, disease
dynamics, user-specific characteristics, and the prevalence of false
notifications, necessitating novel evaluation strategies. Drawing insights from
large-scale heart studies, the paper offers a comprehensive guideline for
robust ML model evaluation on continuous health monitoring.
</p></li>
</ul>

<h3>Title: Robust Clustering using Hyperdimensional Computing. (arXiv:2312.02407v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02407">http://arxiv.org/abs/2312.02407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02407]] Robust Clustering using Hyperdimensional Computing(http://arxiv.org/abs/2312.02407)</code></li>
<li>Summary: <p>This paper addresses the clustering of data in the hyperdimensional computing
(HDC) domain. In prior work, an HDC-based clustering framework, referred to as
HDCluster, has been proposed. However, the performance of the existing
HDCluster is not robust. The performance of HDCluster is degraded as the
hypervectors for the clusters are chosen at random during the initialization
step. To overcome this bottleneck, we assign the initial cluster hypervectors
by exploring the similarity of the encoded data, referred to as \textit{query}
hypervectors. Intra-cluster hypervectors have a higher similarity than
inter-cluster hypervectors. Harnessing the similarity results among query
hypervectors, this paper proposes four HDC-based clustering algorithms:
similarity-based k-means, equal bin-width histogram, equal bin-height
histogram, and similarity-based affinity propagation. Experimental results
illustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based
clustering algorithms can achieve better accuracy, more robust performance,
fewer iterations, and less execution time. Similarity-based affinity
propagation outperforms the other three HDC-based clustering algorithms on
eight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass
clustering, i.e., without any iterative update of the cluster hypervectors, our
proposed algorithms can provide more robust clustering accuracy than HDCluster.
(iii) Over eight datasets, five out of eight can achieve higher or comparable
accuracy when projected onto the hyperdimensional space. Traditional clustering
is more desirable than HDC when the number of clusters, $k$, is large.
</p></li>
</ul>

<h3>Title: Towards Causal Representations of Climate Model Data. (arXiv:2312.02858v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02858">http://arxiv.org/abs/2312.02858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02858]] Towards Causal Representations of Climate Model Data(http://arxiv.org/abs/2312.02858)</code></li>
<li>Summary: <p>Climate models, such as Earth system models (ESMs), are crucial for
simulating future climate change based on projected Shared Socioeconomic
Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated
and invaluable, machine learning-based emulators trained on existing simulation
data can project additional climate scenarios much faster and are
computationally efficient. However, they often lack generalizability and
interpretability. This work delves into the potential of causal representation
learning, specifically the \emph{Causal Discovery with Single-parent Decoding}
(CDSD) method, which could render climate model emulation efficient
\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,
focusing on emissions, temperature, and precipitation. Our findings shed light
on the challenges, limitations, and promise of using CDSD as a stepping stone
towards more interpretable and robust climate model emulation.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars. (arXiv:2312.02222v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02222">http://arxiv.org/abs/2312.02222</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02222]] InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars(http://arxiv.org/abs/2312.02222)</code></li>
<li>Summary: <p>While high fidelity and efficiency are central to the creation of digital
head avatars, recent methods relying on 2D or 3D generative models often
experience limitations such as shape distortion, expression inaccuracy, and
identity flickering. Additionally, existing one-shot inversion techniques fail
to fully leverage multiple input images for detailed feature extraction. We
propose a novel framework, \textbf{Incremental 3D GAN Inversion}, that enhances
avatar reconstruction performance using an algorithm designed to increase the
fidelity from multiple frames, resulting in improved reconstruction quality
proportional to frame count. Our method introduces a unique animatable 3D GAN
prior with two crucial modifications for enhanced expression controllability
alongside an innovative neural texture encoder that categorizes texture feature
spaces based on UV parameterization. Differentiating from traditional
techniques, our architecture emphasizes pixel-aligned image-to-image
translation, mitigating the need to learn correspondences between observation
and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent
networks for temporal data aggregation from multiple frames, boosting geometry
and texture detail reconstruction. The proposed paradigm demonstrates
state-of-the-art performance on one-shot and few-shot avatar animation tasks.
</p></li>
</ul>

<h3>Title: Generator Born from Classifier. (arXiv:2312.02470v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02470">http://arxiv.org/abs/2312.02470</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02470]] Generator Born from Classifier(http://arxiv.org/abs/2312.02470)</code></li>
<li>Summary: <p>In this paper, we make a bold attempt toward an ambitious task: given a
pre-trained classifier, we aim to reconstruct an image generator, without
relying on any data samples. From a black-box perspective, this challenge seems
intractable, since it inevitably involves identifying the inverse function for
a classifier, which is, by nature, an information extraction process. As such,
we resort to leveraging the knowledge encapsulated within the parameters of the
neural network. Grounded on the theory of Maximum-Margin Bias of gradient
descent, we propose a novel learning paradigm, in which the generator is
trained to ensure that the convergence conditions of the network parameters are
satisfied over the generated distribution of the samples. Empirical validation
from various image generation tasks substantiates the efficacy of our strategy.
</p></li>
</ul>

<h3>Title: LLMs Accelerate Annotation for Medical Information Extraction. (arXiv:2312.02296v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02296">http://arxiv.org/abs/2312.02296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02296]] LLMs Accelerate Annotation for Medical Information Extraction(http://arxiv.org/abs/2312.02296)</code></li>
<li>Summary: <p>The unstructured nature of clinical notes within electronic health records
often conceals vital patient-related information, making it challenging to
access or interpret. To uncover this hidden information, specialized Natural
Language Processing (NLP) models are required. However, training these models
necessitates large amounts of labeled data, a process that is both
time-consuming and costly when relying solely on human experts for annotation.
In this paper, we propose an approach that combines Large Language Models
(LLMs) with human expertise to create an efficient method for generating ground
truth labels for medical text annotation. By utilizing LLMs in conjunction with
human annotators, we significantly reduce the human annotation burden, enabling
the rapid creation of labeled datasets. We rigorously evaluate our method on a
medical information extraction task, demonstrating that our approach not only
substantially cuts down on human intervention but also maintains high accuracy.
The results highlight the potential of using LLMs to improve the utilization of
unstructured clinical data, allowing for the swift deployment of tailored NLP
solutions in healthcare.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Active Learning for Target Domain Generalisation. (arXiv:2312.02247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02247">http://arxiv.org/abs/2312.02247</a></li>
<li>Code URL: https://github.com/razvancaramalau/fedalv</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02247]] Federated Active Learning for Target Domain Generalisation(http://arxiv.org/abs/2312.02247)</code></li>
<li>Summary: <p>In this paper, we introduce Active Learning framework in Federated Learning
for Target Domain Generalisation, harnessing the strength from both learning
paradigms. Our framework, FEDALV, composed of Active Learning (AL) and
Federated Domain Generalisation (FDG), enables generalisation of an image
classification model trained from limited source domain client's data without
sharing images to an unseen target domain. To this end, our FDG, FEDA, consists
of two optimisation updates during training, one at the client and another at
the server level. For the client, the introduced losses aim to reduce feature
complexity and condition alignment, while in the server, the regularisation
limits free energy biases between source and target obtained by the global
model. The remaining component of FEDAL is AL with variable budgets, which
queries the server to retrieve and sample the most informative local data for
the targeted client. We performed multiple experiments on FDG w/ and w/o AL and
compared with both conventional FDG baselines and Federated Active Learning
baselines. Our extensive quantitative experiments demonstrate the superiority
of our method in accuracy and efficiency compared to the multiple contemporary
methods. FEDALV manages to obtain the performance of the full training target
accuracy while sampling as little as 5% of the source client's data.
</p></li>
</ul>

<h3>Title: Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts. (arXiv:2312.02567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02567">http://arxiv.org/abs/2312.02567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02567]] Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts(http://arxiv.org/abs/2312.02567)</code></li>
<li>Summary: <p>Federated learning facilitates the collaborative learning of a global model
across multiple distributed medical institutions without centralizing data.
Nevertheless, the expensive cost of annotation on local clients remains an
obstacle to effectively utilizing local data. To mitigate this issue, federated
active learning methods suggest leveraging local and global model predictions
to select a relatively small amount of informative local data for annotation.
However, existing methods mainly focus on all local data sampled from the same
domain, making them unreliable in realistic medical scenarios with domain
shifts among different clients. In this paper, we make the first attempt to
assess the informativeness of local data derived from diverse domains and
propose a novel methodology termed Federated Evidential Active Learning (FEAL)
to calibrate the data evaluation under domain shift. Specifically, we introduce
a Dirichlet prior distribution in both local and global models to treat the
prediction as a distribution over the probability simplex and capture both
aleatoric and epistemic uncertainties by using the Dirichlet-based evidential
model. Then we employ the epistemic uncertainty to calibrate the aleatoric
uncertainty. Afterward, we design a diversity relaxation strategy to reduce
data redundancy and maintain data diversity. Extensive experiments and analyses
are conducted to show the superiority of FEAL over the state-of-the-art active
learning methods and the efficiency of FEAL under the federated active learning
framework.
</p></li>
</ul>

<h3>Title: Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor. (arXiv:2312.02416v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02416">http://arxiv.org/abs/2312.02416</a></li>
<li>Code URL: https://github.com/J1nqianChen/FedKA</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02416]] Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor(http://arxiv.org/abs/2312.02416)</code></li>
<li>Summary: <p>Federated learning encounters a critical challenge of data heterogeneity,
adversely affecting the performance and convergence of the federated model.
Various approaches have been proposed to address this issue, yet their
effectiveness is still limited. Recent studies have revealed that the federated
model suffers severe forgetting in local training, leading to global forgetting
and performance degradation. Although the analysis provides valuable insights,
a comprehensive understanding of the vulnerable classes and their impact
factors is yet to be established. In this paper, we aim to bridge this gap by
systematically analyzing the forgetting degree of each class during local
training across different communication rounds. Our observations are: (1) Both
missing and non-dominant classes suffer similar severe forgetting during local
training, while dominant classes show improvement in performance. (2) When
dynamically reducing the sample size of a dominant class, catastrophic
forgetting occurs abruptly when the proportion of its samples is below a
certain threshold, indicating that the local model struggles to leverage a few
samples of a specific class effectively to prevent forgetting. Motivated by
these findings, we propose a novel and straightforward algorithm called
Federated Knowledge Anchor (FedKA). Assuming that all clients have a single
shared sample for each class, the knowledge anchor is constructed before each
local training stage by extracting shared samples for missing classes and
randomly selecting one sample per class for non-dominant classes. The knowledge
anchor is then utilized to correct the gradient of each mini-batch towards the
direction of preserving the knowledge of the missing and non-dominant classes.
Extensive experimental results demonstrate that our proposed FedKA achieves
fast and stable convergence, significantly improving accuracy on popular
benchmarks.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Inspecting Model Fairness in Ultrasound Segmentation Tasks. (arXiv:2312.02501v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02501">http://arxiv.org/abs/2312.02501</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02501]] Inspecting Model Fairness in Ultrasound Segmentation Tasks(http://arxiv.org/abs/2312.02501)</code></li>
<li>Summary: <p>With the rapid expansion of machine learning and deep learning (DL),
researchers are increasingly employing learning-based algorithms to alleviate
diagnostic challenges across diverse medical tasks and applications. While
advancements in diagnostic precision are notable, some researchers have
identified a concerning trend: their models exhibit biased performance across
subgroups characterized by different sensitive attributes. This bias not only
infringes upon the rights of patients but also has the potential to lead to
life-altering consequences. In this paper, we inspect a series of DL
segmentation models using two ultrasound datasets, aiming to assess the
presence of model unfairness in these specific tasks. Our findings reveal that
even state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound
segmentation tasks. These results serve as a crucial warning, underscoring the
necessity for careful model evaluation before their deployment in real-world
scenarios. Such assessments are imperative to ensure ethical considerations and
mitigate the risk of adverse impacts on patient outcomes.
</p></li>
</ul>

<h3>Title: FRAPP\'E: A Post-Processing Framework for Group Fairness Regularization. (arXiv:2312.02592v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02592">http://arxiv.org/abs/2312.02592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02592]] FRAPP\'E: A Post-Processing Framework for Group Fairness Regularization(http://arxiv.org/abs/2312.02592)</code></li>
<li>Summary: <p>Post-processing mitigation techniques for group fairness generally adjust the
decision threshold of a base model in order to improve fairness. Methods in
this family exhibit several advantages that make them appealing in practice:
post-processing requires no access to the model training pipeline, is agnostic
to the base model architecture, and offers a reduced computation cost compared
to in-processing. Despite these benefits, existing methods face other
challenges that limit their applicability: they require knowledge of the
sensitive attributes at inference time and are oftentimes outperformed by
in-processing. In this paper, we propose a general framework to transform any
in-processing method with a penalized objective into a post-processing
procedure. The resulting method is specifically designed to overcome the
aforementioned shortcomings of prior post-processing approaches. Furthermore,
we show theoretically and through extensive experiments on real-world data that
the resulting post-processing method matches or even surpasses the
fairness-error trade-off offered by the in-processing counterpart.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields. (arXiv:2312.02218v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02218">http://arxiv.org/abs/2312.02218</a></li>
<li>Code URL: https://github.com/azzarelli/waveplanes</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02218]] WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields(http://arxiv.org/abs/2312.02218)</code></li>
<li>Summary: <p>Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to
model moving scenes. However, they are resource intensive and challenging to
compress. To address this issue, this paper presents WavePlanes, a fast and
more compact explicit model. We propose a multi-scale space and space-time
feature plane representation using N-level 2-D wavelet coefficients. The
inverse discrete wavelet transform reconstructs N feature signals at varying
detail, which are linearly decoded to approximate the color and density of
volumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we
compress a Hash Map containing only non-zero coefficients and their locations
on each plane. This results in a compressed model size of ~12 MB. Compared with
state-of-the-art plane-based models, WavePlanes is up to 15x smaller, less
computationally demanding and achieves comparable results in as little as one
hour of training - without requiring custom CUDA code or high performance
computing resources. Additionally, we propose new feature fusion schemes that
work as well as previously proposed schemes while providing greater
interpretability. Our code is available at:
https://github.com/azzarelli/waveplanes/
</p></li>
</ul>

<h3>Title: Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition. (arXiv:2312.02226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02226">http://arxiv.org/abs/2312.02226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02226]] Generating Action-conditioned Prompts for Open-vocabulary Video Action Recognition(http://arxiv.org/abs/2312.02226)</code></li>
<li>Summary: <p>Exploring open-vocabulary video action recognition is a promising venture,
which aims to recognize previously unseen actions within any arbitrary set of
categories. Existing methods typically adapt pretrained image-text models to
the video domain, capitalizing on their inherent strengths in generalization. A
common thread among such methods is the augmentation of visual embeddings with
temporal information to improve the recognition of seen actions. Yet, they
compromise with standard less-informative action descriptions, thus faltering
when confronted with novel actions. Drawing inspiration from human cognitive
processes, we argue that augmenting text embeddings with human prior knowledge
is pivotal for open-vocabulary video action recognition. To realize this, we
innovatively blend video models with Large Language Models (LLMs) to devise
Action-conditioned Prompts. Specifically, we harness the knowledge in LLMs to
produce a set of descriptive sentences that contain distinctive features for
identifying given actions. Building upon this foundation, we further introduce
a multi-modal action knowledge alignment mechanism to align concepts in video
and textual knowledge encapsulated within the prompts. Extensive experiments on
various video benchmarks, including zero-shot, few-shot, and base-to-novel
generalization settings, demonstrate that our method not only sets new SOTA
performance but also possesses excellent interpretability.
</p></li>
</ul>

<h3>Title: Recursive Visual Programming. (arXiv:2312.02249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02249">http://arxiv.org/abs/2312.02249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02249]] Recursive Visual Programming(http://arxiv.org/abs/2312.02249)</code></li>
<li>Summary: <p>Visual Programming (VP) has emerged as a powerful framework for Visual
Question Answering (VQA). By generating and executing bespoke code for each
question, these methods demonstrate impressive compositional and reasoning
capabilities, especially in few-shot and zero-shot scenarios. However, existing
VP methods generate all code in a single function, resulting in code that is
suboptimal in terms of both accuracy and interpretability. Inspired by human
coding practices, we propose Recursive Visual Programming (RVP), which
simplifies generated routines, provides more efficient problem solving, and can
manage more complex data structures. RVP is inspired by human coding practices
and approaches VQA tasks with an iterative recursive code generation approach,
allowing decomposition of complicated problems into smaller parts. Notably, RVP
is capable of dynamic type assignment, i.e., as the system recursively
generates a new piece of code, it autonomously determines the appropriate
return type and crafts the requisite code to generate that output. We show
RVP's efficacy through extensive experiments on benchmarks including VSR, COVR,
GQA, and NextQA, underscoring the value of adopting human-like recursive and
modular programming techniques for solving VQA tasks through coding.
</p></li>
</ul>

<h3>Title: Semi-Supervised Health Index Monitoring with Feature Generation and Fusion. (arXiv:2312.02867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02867">http://arxiv.org/abs/2312.02867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02867]] Semi-Supervised Health Index Monitoring with Feature Generation and Fusion(http://arxiv.org/abs/2312.02867)</code></li>
<li>Summary: <p>The Health Index (HI) is crucial for evaluating system health, aiding tasks
like anomaly detection and predicting remaining useful life for systems
demanding high safety and reliability. Tight monitoring is crucial for
achieving high precision at a lower cost, with applications such as spray
coating. Obtaining HI labels in real-world applications is often
cost-prohibitive, requiring continuous, precise health measurements. Therefore,
it is more convenient to leverage run-to failure datasets that may provide
potential indications of machine wear condition, making it necessary to apply
semi-supervised tools for HI construction. In this study, we adapt the Deep
Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use
the DeepSAD embedding as a condition indicators to address interpretability
challenges and sensitivity to system-specific factors. Then, we introduce a
diversity loss to enrich condition indicators. We employ an alternating
projection algorithm with isotonic constraints to transform the DeepSAD
embedding into a normalized HI with an increasing trend. Validation on the PHME
2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates
meaningful HIs estimations. Our methodology is then applied to monitor wear
states of thermal spray coatings using high-frequency voltage. Our
contributions create opportunities for more accessible and reliable HI
estimation, particularly in cases where obtaining ground truth HI labels is
unfeasible.
</p></li>
</ul>

<h3>Title: Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction. (arXiv:2312.02872v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02872">http://arxiv.org/abs/2312.02872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02872]] Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction(http://arxiv.org/abs/2312.02872)</code></li>
<li>Summary: <p>In the context of autonomous driving, pedestrian crossing prediction is a key
component for improving road safety. Presently, the focus of these predictions
extends beyond achieving trustworthy results; it is shifting towards the
explainability and interpretability of these predictions. This research
introduces a novel neuro-symbolic approach that combines deep learning and
fuzzy logic for an explainable and interpretable pedestrian crossing
prediction. We have developed an explainable predictor (ExPedCross), which
utilizes a set of explainable features and employs a fuzzy inference system to
predict whether the pedestrian will cross or not. Our approach was evaluated on
both the PIE and JAAD datasets. The results offer experimental insights into
achieving explainability and interpretability in the pedestrian crossing
prediction task. Furthermore, the testing results yield a set of guidelines and
recommendations regarding the process of dataset selection, feature selection,
and explainability.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking. (arXiv:2312.02382v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02382">http://arxiv.org/abs/2312.02382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02382]] New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking(http://arxiv.org/abs/2312.02382)</code></li>
<li>Summary: <p>With the increasing use of large-language models (LLMs) like ChatGPT,
watermarking has emerged as a promising approach for tracing machine-generated
content. However, research on LLM watermarking often relies on simple
perplexity or diversity-based measures to assess the quality of watermarked
text, which can mask important limitations in watermarking. Here we introduce
two new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1)
evaluation by LLM-judger with specific guidelines; and 2) binary classification
on text embeddings to distinguish between watermarked and unwatermarked text.
We apply these methods to characterize the effectiveness of current
watermarking techniques. Our experiments, conducted across various datasets,
reveal that current watermarking methods are detectable by even simple
classifiers, challenging the notion of watermarking subtlety. We also found,
through the LLM judger, that watermarking impacts text quality, especially in
degrading the coherence and depth of the response. Our findings underscore the
trade-off between watermark robustness and text quality and highlight the
importance of having more informative metrics to assess watermarking quality.
</p></li>
</ul>

<h3>Title: Watermarking for Neural Radiation Fields by Invertible Neural Network. (arXiv:2312.02456v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02456">http://arxiv.org/abs/2312.02456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02456]] Watermarking for Neural Radiation Fields by Invertible Neural Network(http://arxiv.org/abs/2312.02456)</code></li>
<li>Summary: <p>To protect the copyright of the 3D scene represented by the neural radiation
field, the embedding and extraction of the neural radiation field watermark are
considered as a pair of inverse problems of image transformations. A scheme for
protecting the copyright of the neural radiation field is proposed using
invertible neural network watermarking, which utilizes watermarking techniques
for 2D images to achieve the protection of the 3D scene. The scheme embeds the
watermark in the training image of the neural radiation field through the
forward process in the invertible network and extracts the watermark from the
image rendered by the neural radiation field using the inverse process to
realize the copyright protection of both the neural radiation field and the 3D
scene. Since the rendering process of the neural radiation field can cause the
loss of watermark information, the scheme incorporates an image quality
enhancement module, which utilizes a neural network to recover the rendered
image and then extracts the watermark. The scheme embeds a watermark in each
training image to train the neural radiation field and enables the extraction
of watermark information from multiple viewpoints. Simulation experimental
results demonstrate the effectiveness of the method.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D. (arXiv:2312.02189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02189">http://arxiv.org/abs/2312.02189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02189]] StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D(http://arxiv.org/abs/2312.02189)</code></li>
<li>Summary: <p>In the realm of text-to-3D generation, utilizing 2D diffusion models through
score distillation sampling (SDS) frequently leads to issues such as blurred
appearances and multi-faced geometry, primarily due to the intrinsically noisy
nature of the SDS loss. Our analysis identifies the core of these challenges as
the interaction among noise levels in the 2D diffusion process, the
architecture of the diffusion network, and the 3D model representation. To
overcome these limitations, we present StableDreamer, a methodology
incorporating three advances. First, inspired by InstructNeRF2NeRF, we
formalize the equivalence of the SDS generative prior and a simple supervised
L2 reconstruction loss. This finding provides a novel tool to debug SDS, which
we use to show the impact of time-annealing noise levels on reducing
multi-faced geometries. Second, our analysis shows that while image-space
diffusion contributes to geometric precision, latent-space diffusion is crucial
for vivid color rendition. Based on this observation, StableDreamer introduces
a two-stage training strategy that effectively combines these aspects,
resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D
Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance
the overall quality, reduce memory usage during training, and accelerate
rendering speeds, and better capture semi-transparent objects. StableDreamer
reduces multi-face geometries, generates fine details, and converges stably.
</p></li>
</ul>

<h3>Title: Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D. (arXiv:2312.02190v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02190">http://arxiv.org/abs/2312.02190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02190]] Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D(http://arxiv.org/abs/2312.02190)</code></li>
<li>Summary: <p>Diffusion Handles is a novel approach to enabling 3D object edits on
diffusion images. We accomplish these edits using existing pre-trained
diffusion models, and 2D image depth estimation, without any fine-tuning or 3D
object retrieval. The edited results remain plausible, photo-real, and preserve
object identity. Diffusion Handles address a critically missing facet of
generative image based creative design, and significantly advance the
state-of-the-art in generative image editing. Our key insight is to lift
diffusion activations for an object to 3D using a proxy depth, 3D-transform the
depth and associated activations, and project them back to image space. The
diffusion process applied to the manipulated activations with identity control,
produces plausible edited images showing complex 3D occlusion and lighting
effects. We evaluate Diffusion Handles: quantitatively, on a large synthetic
data benchmark; and qualitatively by a user study, showing our output to be
more plausible, and better than prior art at both, 3D editing and identity
control.
</p></li>
</ul>

<h3>Title: Exploiting Diffusion Priors for All-in-One Image Restoration. (arXiv:2312.02197v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02197">http://arxiv.org/abs/2312.02197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02197]] Exploiting Diffusion Priors for All-in-One Image Restoration(http://arxiv.org/abs/2312.02197)</code></li>
<li>Summary: <p>All-in-one aims to solve various tasks of image restoration in a single
model. To this end, we present a feasible way of exploiting the image priors
captured by the pretrained diffusion model, through addressing the two
challenges, i.e., degradation modeling and diffusion guidance. The former aims
to simulate the process of the clean image degenerated by certain degradations,
and the latter aims at guiding the diffusion model to generate the
corresponding clean image. With the motivations, we propose a zero-shot
framework for all-in-one image restoration, termed ZeroAIR, which alternatively
performs the test-time degradation modeling (TDM) and the three-stage diffusion
guidance (TDG) at each timestep of the reverse sampling. To be specific, TDM
exploits the diffusion priors to learn a degradation model from a given
degraded image, and TDG divides the timesteps into three stages for taking full
advantage of the varying diffusion priors. Thanks to their degradation-agnostic
property, the all-in-one image restoration could be achieved in a zero-shot way
by ZeroAIR. Through extensive experiments, we show that our ZeroAIR achieves
comparable even better performance than those task-specific methods. The code
will be available on Github.
</p></li>
</ul>

<h3>Title: ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation. (arXiv:2312.02201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02201">http://arxiv.org/abs/2312.02201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02201]] ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation(http://arxiv.org/abs/2312.02201)</code></li>
<li>Summary: <p>We introduce "ImageDream," an innovative image-prompt, multi-view diffusion
model for 3D object generation. ImageDream stands out for its ability to
produce 3D models of higher quality compared to existing state-of-the-art,
image-conditioned methods. Our approach utilizes a canonical camera
coordination for the objects in images, improving visual geometry accuracy. The
model is designed with various levels of control at each block inside the
diffusion model based on the input image, where global control shapes the
overall object layout and local control fine-tunes the image details. The
effectiveness of ImageDream is demonstrated through extensive evaluations using
a standard prompt list. For more information, visit our project page at
https://Image-Dream.github.io.
</p></li>
</ul>

<h3>Title: Portrait Diffusion: Training-free Face Stylization with Chain-of-Painting. (arXiv:2312.02212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02212">http://arxiv.org/abs/2312.02212</a></li>
<li>Code URL: https://github.com/liujin112/portraitdiffusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02212]] Portrait Diffusion: Training-free Face Stylization with Chain-of-Painting(http://arxiv.org/abs/2312.02212)</code></li>
<li>Summary: <p>Face stylization refers to the transformation of a face into a specific
portrait style. However, current methods require the use of example-based
adaptation approaches to fine-tune pre-trained generative models so that they
demand lots of time and storage space and fail to achieve detailed style
transformation. This paper proposes a training-free face stylization framework,
named Portrait Diffusion. This framework leverages off-the-shelf text-to-image
diffusion models, eliminating the need for fine-tuning specific examples.
Specifically, the content and style images are first inverted into latent
codes. Then, during image reconstruction using the corresponding latent code,
the content and style features in the attention space are delicately blended
through a modified self-attention operation called Style Attention Control.
Additionally, a Chain-of-Painting method is proposed for the gradual redrawing
of unsatisfactory areas from rough adjustments to fine-tuning. Extensive
experiments validate the effectiveness of our Portrait Diffusion method and
demonstrate the superiority of Chain-of-Painting in achieving precise face
stylization. Code will be released at
\url{https://github.com/liujin112/PortraitDiffusion}.
</p></li>
</ul>

<h3>Title: Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction. (arXiv:2312.02221v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02221">http://arxiv.org/abs/2312.02221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02221]] Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction(http://arxiv.org/abs/2312.02221)</code></li>
<li>Summary: <p>We introduce multi-slice reasoning, a new notion for single-view 3D
reconstruction which challenges the current and prevailing belief that
multi-view synthesis is the most natural conduit between single-view and 3D.
Our key observation is that object slicing is more advantageous than altering
views to reveal occluded structures. Specifically, slicing is more
occlusion-revealing since it can peel through any occluders without
obstruction. In the limit, i.e., with infinitely many slices, it is guaranteed
to unveil all hidden object parts. We realize our idea by developing Slice3D, a
novel method for single-view 3D reconstruction which first predicts multi-slice
images from a single RGB image and then integrates the slices into a 3D model
using a coordinate-based transformer network for signed distance prediction.
The slice images can be regressed or generated, both through a U-Net based
network. For the former, we inject a learnable slice indicator code to
designate each decoded image into a spatial slice location, while the slice
generator is a denoising diffusion model operating on the entirety of slice
images stacked on the input channels. We conduct extensive evaluation against
state-of-the-art alternatives to demonstrate superiority of our method,
especially in recovering complex and severely occluded shape structures, amid
ambiguities. All Slice3D results were produced by networks trained on a single
Nvidia A40 GPU, with an inference time less than 20 seconds.
</p></li>
</ul>

<h3>Title: MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model. (arXiv:2312.02233v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02233">http://arxiv.org/abs/2312.02233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02233]] MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model(http://arxiv.org/abs/2312.02233)</code></li>
<li>Summary: <p>Despite the success of Large Language Models (LLMs) in general image tasks, a
gap persists in the medical field for a multimodal large model adept at
handling the nuanced diversity of medical images. Addressing this, we propose
MedXChat, a unified multimodal large model designed for seamless interactions
between medical assistants and users. MedXChat encompasses three key
functionalities: CXR(Chest X-ray)-to-Report generation, CXR-based visual
question-answering (VQA), and Text-to-CXR synthesis. Our contributions are as
follows. Firstly, our model showcases exceptional cross-task adaptability,
displaying adeptness across all three defined tasks and outperforming the
benchmark models on the MIMIC dataset in medical multimodal applications.
Secondly, we introduce an innovative Text-to-CXR synthesis approach that
utilizes instruction-following capabilities within the Stable Diffusion (SD)
architecture. This technique integrates smoothly with the existing model
framework, requiring no extra parameters, thereby maintaining the SD's
generative strength while also bestowing upon it the capacity to render
fine-grained medical images with high fidelity. Comprehensive experiments
validate MedXChat's synergistic enhancement across all tasks. Our instruction
data and model will be open-sourced.
</p></li>
</ul>

<h3>Title: X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model. (arXiv:2312.02238v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02238">http://arxiv.org/abs/2312.02238</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02238]] X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model(http://arxiv.org/abs/2312.02238)</code></li>
<li>Summary: <p>We introduce X-Adapter, a universal upgrader to enable the pretrained
plug-and-play modules (e.g., ControlNet, LoRA) to work directly with the
upgraded text-to-image diffusion model (e.g., SDXL) without further retraining.
We achieve this goal by training an additional network to control the frozen
upgraded model with the new text-image data pairs. In detail, X-Adapter keeps a
frozen copy of the old model to preserve the connectors of different plugins.
Additionally, X-Adapter adds trainable mapping layers that bridge the decoders
from models of different versions for feature remapping. The remapped features
will be used as guidance for the upgraded model. To enhance the guidance
ability of X-Adapter, we employ a null-text training strategy for the upgraded
model. After training, we also introduce a two-stage denoising strategy to
align the initial latents of X-Adapter and the upgraded model. Thanks to our
strategies, X-Adapter demonstrates universal compatibility with various plugins
and also enables plugins of different versions to work together, thereby
expanding the functionalities of diffusion community. To verify the
effectiveness of the proposed method, we conduct extensive experiments and the
results show that X-Adapter may facilitate wider application in the upgraded
foundational diffusion model.
</p></li>
</ul>

<h3>Title: Conditional Variational Diffusion Models. (arXiv:2312.02246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02246">http://arxiv.org/abs/2312.02246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02246]] Conditional Variational Diffusion Models(http://arxiv.org/abs/2312.02246)</code></li>
<li>Summary: <p>Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
</p></li>
</ul>

<h3>Title: Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images. (arXiv:2312.02253v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02253">http://arxiv.org/abs/2312.02253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02253]] Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images(http://arxiv.org/abs/2312.02253)</code></li>
<li>Summary: <p>Recent advances in generative deep learning have enabled the creation of
high-quality synthetic images in text-to-image generation. Prior work shows
that fine-tuning a pretrained diffusion model on ImageNet and generating
synthetic training images from the finetuned model can enhance an ImageNet
classifier's performance. However, performance degrades as synthetic images
outnumber real ones. In this paper, we explore whether generative fine-tuning
is essential for this improvement and whether it is possible to further scale
up training using more synthetic data. We present a new framework leveraging
off-the-shelf generative models to generate synthetic training images,
addressing multiple challenges: class name ambiguity, lack of diversity in
naive prompts, and domain shifts. Specifically, we leverage large language
models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we
propose contextualized diversification (CD) and stylized diversification (SD)
methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage
domain adaptation techniques with auxiliary batch normalization for synthetic
images. Our framework consistently enhances recognition model performance with
more synthetic data, up to 6x of original ImageNet size showcasing the
potential of synthetic data for improved recognition models and strong
out-of-domain generalization.
</p></li>
</ul>

<h3>Title: EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation. (arXiv:2312.02256v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02256">http://arxiv.org/abs/2312.02256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02256]] EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion Generation(http://arxiv.org/abs/2312.02256)</code></li>
<li>Summary: <p>We introduce Efficient Motion Diffusion Model (EMDM) for fast and
high-quality human motion generation. Although previous motion diffusion models
have shown impressive results, they struggle to achieve fast generation while
maintaining high-quality human motions. Motion latent diffusion has been
proposed for efficient motion generation. However, effectively learning a
latent space can be non-trivial in such a two-stage manner. Meanwhile,
accelerating motion sampling by increasing the step size, e.g., DDIM, typically
leads to a decline in motion quality due to the inapproximation of complex data
distributions when naively increasing the step size. In this paper, we propose
EMDM that allows for much fewer sample steps for fast motion generation by
modeling the complex denoising distribution during multiple sampling steps.
Specifically, we develop a Conditional Denoising Diffusion GAN to capture
multimodal data distributions conditioned on both control signals, i.e.,
textual description and denoising time step. By modeling the complex data
distribution, a larger sampling step size and fewer steps are achieved during
motion synthesis, significantly accelerating the generation process. To
effectively capture the human dynamics and reduce undesired artifacts, we
employ motion geometric loss during network training, which improves the motion
quality and training efficiency. As a result, EMDM achieves a remarkable
speed-up at the generation stage while maintaining high-quality motion
generation in terms of fidelity and diversity.
</p></li>
</ul>

<h3>Title: Towards Granularity-adjusted Pixel-level Semantic Annotation. (arXiv:2312.02420v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02420">http://arxiv.org/abs/2312.02420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02420]] Towards Granularity-adjusted Pixel-level Semantic Annotation(http://arxiv.org/abs/2312.02420)</code></li>
<li>Summary: <p>Recent advancements in computer vision predominantly rely on learning-based
systems, leveraging annotations as the driving force to develop specialized
models. However, annotating pixel-level information, particularly in semantic
segmentation, presents a challenging and labor-intensive task, prompting the
need for autonomous processes. In this work, we propose GranSAM which
distinguishes itself by providing semantic segmentation at the user-defined
granularity level on unlabeled data without the need for any manual
supervision, offering a unique contribution in the realm of semantic mask
annotation method. Specifically, we propose an approach to enable the Segment
Anything Model (SAM) with semantic recognition capability to generate
pixel-level annotations for images without any manual supervision. For this, we
accumulate semantic information from synthetic images generated by the Stable
Diffusion model or web crawled images and employ this data to learn a mapping
function between SAM mask embeddings and object class labels. As a result, SAM,
enabled with granularity-adjusted mask recognition, can be used for pixel-level
semantic annotation purposes. We conducted experiments on the PASCAL VOC 2012
and COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU,
respectively, compared to existing state-of-the-art methods when evaluated
under our problem setting.
</p></li>
</ul>

<h3>Title: Orthogonal Adaptation for Modular Customization of Diffusion Models. (arXiv:2312.02432v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02432">http://arxiv.org/abs/2312.02432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02432]] Orthogonal Adaptation for Modular Customization of Diffusion Models(http://arxiv.org/abs/2312.02432)</code></li>
<li>Summary: <p>Customization techniques for text-to-image models have paved the way for a
wide range of previously unattainable applications, enabling the generation of
specific concepts across diverse contexts and styles. While existing methods
facilitate high-fidelity customization for individual concepts or a limited,
pre-defined set of them, they fall short of achieving scalability, where a
single model can seamlessly render countless concepts. In this paper, we
address a new problem called Modular Customization, with the goal of
efficiently merging customized models that were fine-tuned independently for
individual concepts. This allows the merged model to jointly synthesize
concepts in one image without compromising fidelity or incurring any additional
computational costs.
</p>
<p>To address this problem, we introduce Orthogonal Adaptation, a method
designed to encourage the customized models, which do not have access to each
other during fine-tuning, to have orthogonal residual weights. This ensures
that during inference time, the customized models can be summed with minimal
interference.
</p>
<p>Our proposed method is both simple and versatile, applicable to nearly all
optimizable weights in the model architecture. Through an extensive set of
quantitative and qualitative evaluations, our method consistently outperforms
relevant baselines in terms of efficiency and identity preservation,
demonstrating a significant leap toward scalable customization of diffusion
models.
</p></li>
</ul>

<h3>Title: Retrieving Conditions from Reference Images for Diffusion Models. (arXiv:2312.02521v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02521">http://arxiv.org/abs/2312.02521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02521]] Retrieving Conditions from Reference Images for Diffusion Models(http://arxiv.org/abs/2312.02521)</code></li>
<li>Summary: <p>Recent diffusion-based subject driven generative methods have enabled image
generations with good fidelity for specific objects or human portraits.
However, to achieve better versatility for applications, we argue that not only
improved datasets and evaluations are desired, but also more careful methods to
retrieve only relevant information from conditional images are anticipated. To
this end, we propose an anime figures dataset RetriBooru-V1, with enhanced
identity and clothing labels. We state new tasks enabled by this dataset, and
introduce a new diversity metric to measure success in completing these tasks,
quantifying the flexibility of image generations. We establish an RAG-inspired
baseline method, designed to retrieve precise conditional information from
reference images. Then, we compare with current methods on existing task to
demonstrate the capability of the proposed method. Finally, we provide baseline
experiment results on new tasks, and conduct ablation studies on the possible
structural choices.
</p></li>
</ul>

<h3>Title: GeNIe: Generative Hard Negative Images Through Diffusion. (arXiv:2312.02548v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02548">http://arxiv.org/abs/2312.02548</a></li>
<li>Code URL: https://github.com/ucdvision/genie</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02548]] GeNIe: Generative Hard Negative Images Through Diffusion(http://arxiv.org/abs/2312.02548)</code></li>
<li>Summary: <p>Data augmentation is crucial in training deep models, preventing them from
overfitting to limited data. Common data augmentation methods are effective,
but recent advancements in generative AI, such as diffusion models for image
generation, enable more sophisticated augmentation techniques that produce data
resembling natural images. We recognize that augmented samples closer to the
ideal decision boundary of a classifier are particularly effective and
efficient in guiding the learning process. We introduce GeNIe which leverages a
diffusion model conditioned on a text prompt to merge contrasting data points
(an image from the source category and a text prompt from the target category)
to generate challenging samples for the target category. Inspired by recent
image editing methods, we limit the number of diffusion iterations and the
amount of noise. This ensures that the generated image retains low-level and
contextual features from the source image, potentially conflicting with the
target category. Our extensive experiments, in few-shot and also long-tail
distribution settings, demonstrate the effectiveness of our novel augmentation
method, especially benefiting categories with a limited number of examples.
</p></li>
</ul>

<h3>Title: Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent. (arXiv:2312.02568v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02568">http://arxiv.org/abs/2312.02568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02568]] Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent(http://arxiv.org/abs/2312.02568)</code></li>
<li>Summary: <p>This paper explores promptable NeRF generation (e.g., text prompt or single
image prompt) for direct conditioning and fast generation of NeRF parameters
for the underlying 3D scenes, thus undoing complex intermediate steps while
providing full 3D generation with conditional control. Unlike previous
diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,
Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single
forward pass, leveraging a pre-trained implicit latent space of NeRF
parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that
the NeRFs produced by our method serve as semantically informative
initializations, significantly accelerating the inference process of existing
prompt-to-NeRF methods. Specifically, we will show that our approach speeds up
the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the
image-to-NeRF method Zero-1-to-3 by 3 to 5 times.
</p></li>
</ul>

<h3>Title: Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models. (arXiv:2312.02615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02615">http://arxiv.org/abs/2312.02615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02615]] Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models(http://arxiv.org/abs/2312.02615)</code></li>
<li>Summary: <p>Novelty detection is a fundamental task of machine learning which aims to
detect abnormal ($\textit{i.e.}$ out-of-distribution (OOD)) samples. Since
diffusion models have recently emerged as the de facto standard generative
framework with surprising generation results, novelty detection via diffusion
models has also gained much attention. Recent methods have mainly utilized the
reconstruction property of in-distribution samples. However, they often suffer
from detecting OOD samples that share similar background information to the
in-distribution data. Based on our observation that diffusion models can
\emph{project} any sample to an in-distribution sample with similar background
information, we propose \emph{Projection Regret (PR)}, an efficient novelty
detection method that mitigates the bias of non-semantic information. To be
specific, PR computes the perceptual distance between the test image and its
diffusion-based projection to detect abnormality. Since the perceptual distance
often fails to capture semantic changes when the background information is
dominant, we cancel out the background bias by comparing it against recursive
projections. Extensive experiments demonstrate that PR outperforms the prior
art of generative-model-based novelty detection methods by a significant
margin.
</p></li>
</ul>

<h3>Title: DreaMo: Articulated 3D Reconstruction From A Single Casual Video. (arXiv:2312.02617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02617">http://arxiv.org/abs/2312.02617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02617]] DreaMo: Articulated 3D Reconstruction From A Single Casual Video(http://arxiv.org/abs/2312.02617)</code></li>
<li>Summary: <p>Articulated 3D reconstruction has valuable applications in various domains,
yet it remains costly and demands intensive work from domain experts. Recent
advancements in template-free learning methods show promising results with
monocular videos. Nevertheless, these approaches necessitate a comprehensive
coverage of all viewpoints of the subject in the input video, thus limiting
their applicability to casually captured videos from online sources. In this
work, we study articulated 3D shape reconstruction from a single and casually
captured internet video, where the subject's view coverage is incomplete. We
propose DreaMo that jointly performs shape reconstruction while solving the
challenging low-coverage regions with view-conditioned diffusion prior and
several tailored regularizations. In addition, we introduce a skeleton
generation strategy to create human-interpretable skeletons from the learned
neural bones and skinning weights. We conduct our study on a self-collected
internet video collection characterized by incomplete view coverage. DreaMo
shows promising quality in novel-view rendering, detailed articulated shape
reconstruction, and skeleton generation. Extensive qualitative and quantitative
studies validate the efficacy of each proposed component, and show existing
methods are unable to solve correct geometry due to the incomplete view
coverage.
</p></li>
</ul>

<h3>Title: Diffusion Noise Feature: Accurate and Fast Generated Image Detection. (arXiv:2312.02625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02625">http://arxiv.org/abs/2312.02625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02625]] Diffusion Noise Feature: Accurate and Fast Generated Image Detection(http://arxiv.org/abs/2312.02625)</code></li>
<li>Summary: <p>Generative models have reached an advanced stage where they can produce
remarkably realistic images. However, this remarkable generative capability
also introduces the risk of disseminating false or misleading information.
Notably, existing image detectors for generated images encounter challenges
such as low accuracy and limited generalization. This paper seeks to address
this issue by seeking a representation with strong generalization capabilities
to enhance the detection of generated images. Our investigation has revealed
that real and generated images display distinct latent Gaussian representations
when subjected to an inverse diffusion process within a pre-trained diffusion
model. Exploiting this disparity, we can amplify subtle artifacts in generated
images. Building upon this insight, we introduce a novel image representation
known as Diffusion Noise Feature (DNF). DNF is an ensemble representation that
estimates the noise generated during the inverse diffusion process. A simple
classifier, e.g., ResNet, trained on DNF achieves high accuracy, robustness,
and generalization capabilities for detecting generated images, even from
previously unseen classes or models. We conducted experiments using a widely
recognized and standard dataset, achieving state-of-the-art effects of
Detection.
</p></li>
</ul>

<h3>Title: TPA3D: Triplane Attention for Fast Text-to-3D Generation. (arXiv:2312.02647v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02647">http://arxiv.org/abs/2312.02647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02647]] TPA3D: Triplane Attention for Fast Text-to-3D Generation(http://arxiv.org/abs/2312.02647)</code></li>
<li>Summary: <p>Due to the lack of large-scale text-3D correspondence data, recent text-to-3D
generation works mainly rely on utilizing 2D diffusion models for synthesizing
3D data. Since diffusion-based methods typically require significant
optimization time for both training and inference, the use of GAN-based models
would still be desirable for fast 3D generation. In this work, we propose
Triplane Attention for text-guided 3D generation (TPA3D), an end-to-end
trainable GAN-based deep learning model for fast text-to-3D generation. With
only 3D shape data and their rendered 2D images observed during training, our
TPA3D is designed to retrieve detailed visual descriptions for synthesizing the
corresponding 3D mesh data. This is achieved by the proposed attention
mechanisms on the extracted sentence and word-level text features. In our
experiments, we show that TPA3D generates high-quality 3D textured shapes
aligned with fine-grained descriptions, while impressive computation efficiency
can be observed.
</p></li>
</ul>

<h3>Title: Analyzing and Improving the Training Dynamics of Diffusion Models. (arXiv:2312.02696v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02696">http://arxiv.org/abs/2312.02696</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02696]] Analyzing and Improving the Training Dynamics of Diffusion Models(http://arxiv.org/abs/2312.02696)</code></li>
<li>Summary: <p>Diffusion models currently dominate the field of data-driven image synthesis
with their unparalleled scaling to large datasets. In this paper, we identify
and rectify several causes for uneven and ineffective training in the popular
ADM diffusion model architecture, without altering its high-level structure.
Observing uncontrolled magnitude changes and imbalances in both the network
activations and weights over the course of training, we redesign the network
layers to preserve activation, weight, and update magnitudes on expectation. We
find that systematic application of this philosophy eliminates the observed
drifts and imbalances, resulting in considerably better networks at equal
computational complexity. Our modifications improve the previous record FID of
2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic
sampling.
</p>
<p>As an independent contribution, we present a method for setting the
exponential moving average (EMA) parameters post-hoc, i.e., after completing
the training run. This allows precise tuning of EMA length without the cost of
performing several training runs, and reveals its surprising interactions with
network architecture, training time, and guidance.
</p></li>
</ul>

<h3>Title: Neural Sign Actors: A diffusion model for 3D sign language production from text. (arXiv:2312.02702v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02702">http://arxiv.org/abs/2312.02702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02702]] Neural Sign Actors: A diffusion model for 3D sign language production from text(http://arxiv.org/abs/2312.02702)</code></li>
<li>Summary: <p>Sign Languages (SL) serve as the predominant mode of communication for the
Deaf and Hard of Hearing communities. The advent of deep learning has aided
numerous methods in SL recognition and translation, achieving remarkable
results. However, Sign Language Production (SLP) poses a challenge for the
computer vision community as the motions generated must be realistic and have
precise semantic meanings. Most SLP methods rely on 2D data, thus impeding
their ability to attain a necessary level of realism. In this work, we propose
a diffusion-based SLP model trained on a curated large-scale dataset of 4D
signing avatars and their corresponding text transcripts. The proposed method
can generate dynamic sequences of 3D avatars from an unconstrained domain of
discourse using a diffusion process formed on a novel and anatomically informed
graph neural network defined on the SMPL-X body skeleton. Through a series of
quantitative and qualitative experiments, we show that the proposed method
considerably outperforms previous methods of SLP. We believe that this work
presents an important and necessary step towards realistic neural sign avatars,
bridging the communication gap between Deaf and hearing communities. The code,
method and generated data will be made publicly available.
</p></li>
</ul>

<h3>Title: A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling. (arXiv:2312.02719v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02719">http://arxiv.org/abs/2312.02719</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02719]] A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling(http://arxiv.org/abs/2312.02719)</code></li>
<li>Summary: <p>Point cloud upsampling (PCU) enriches the representation of raw point clouds,
significantly improving the performance in downstream tasks such as
classification and reconstruction. Most of the existing point cloud upsampling
methods focus on sparse point cloud feature extraction and upsampling module
design. In a different way, we dive deeper into directly modelling the gradient
of data distribution from dense point clouds. In this paper, we proposed a
conditional denoising diffusion probability model (DDPM) for point cloud
upsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a
condition, and iteratively learns the transformation relationship between the
dense point cloud and the noise. Simultaneously, PUDM aligns with a dual
mapping paradigm to further improve the discernment of point features. In this
context, PUDM enables learning complex geometry details in the ground truth
through the dominant features, while avoiding an additional upsampling module
design. Furthermore, to generate high-quality arbitrary-scale point clouds
during inference, PUDM exploits the prior knowledge of the scale between sparse
point clouds and dense point clouds during training by parameterizing a rate
factor. Moreover, PUDM exhibits strong noise robustness in experimental
results. In the quantitative and qualitative evaluations on PU1K and PUGAN,
PUDM significantly outperformed existing methods in terms of Chamfer Distance
(CD) and Hausdorff Distance (HD), achieving state of the art (SOTA)
performance.
</p></li>
</ul>

<h3>Title: Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions. (arXiv:2312.02772v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02772">http://arxiv.org/abs/2312.02772</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02772]] Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions(http://arxiv.org/abs/2312.02772)</code></li>
<li>Summary: <p>Recently, significant progress has been made in text-based motion generation,
enabling the generation of diverse and high-quality human motions that conform
to textual descriptions. However, it remains challenging to generate
fine-grained or stylized motions due to the lack of datasets annotated with
detailed textual descriptions. By adopting a divide-and-conquer strategy, we
propose a new framework named Fine-Grained Human Motion Diffusion Model
(FG-MDM) for human motion generation. Specifically, we first parse previous
vague textual annotation into fine-grained description of different body parts
by leveraging a large language model (GPT-3.5). We then use these fine-grained
descriptions to guide a transformer-based diffusion model. FG-MDM can generate
fine-grained and stylized motions even outside of the distribution of the
training data. Our experimental results demonstrate the superiority of FG-MDM
over previous methods, especially the strong generalization capability. We will
release our fine-grained textual annotations for HumanML3D and KIT.
</p></li>
</ul>

<h3>Title: BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models. (arXiv:2312.02813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02813">http://arxiv.org/abs/2312.02813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02813]] BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models(http://arxiv.org/abs/2312.02813)</code></li>
<li>Summary: <p>Diffusion models have made tremendous progress in text-driven image and video
generation. Now text-to-image foundation models are widely applied to various
downstream image synthesis tasks, such as controllable image generation and
image editing, while downstream video synthesis tasks are less explored for
several reasons. First, it requires huge memory and compute overhead to train a
video generation foundation model. Even with video foundation models,
additional costly training is still required for downstream video synthesis
tasks. Second, although some works extend image diffusion models into videos in
a training-free manner, temporal consistency cannot be well kept. Finally,
these adaption methods are specifically designed for one task and fail to
generalize to different downstream video synthesis tasks. To mitigate these
issues, we propose a training-free general-purpose video synthesis framework,
coined as BIVDiff, via bridging specific image diffusion models and general
text-to-video foundation diffusion models. Specifically, we first use an image
diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video
generation, then perform Mixed Inversion on the generated video, and finally
input the inverted latents into the video diffusion model for temporal
smoothing. Decoupling image and video models enables flexible image model
selection for different purposes, which endows the framework with strong task
generalization and high efficiency. To validate the effectiveness and general
use of BIVDiff, we perform a wide range of video generation tasks, including
controllable video generation video editing, video inpainting and outpainting.
Our project page is available at https://bivdiff.github.io.
</p></li>
</ul>

<h3>Title: Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting. (arXiv:2312.02819v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02819">http://arxiv.org/abs/2312.02819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02819]] Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting(http://arxiv.org/abs/2312.02819)</code></li>
<li>Summary: <p>Weather forecasting requires not only accuracy but also the ability to
perform probabilistic prediction. However, deterministic weather forecasting
methods do not support probabilistic predictions, and conversely, probabilistic
models tend to be less accurate. To address these challenges, in this paper, we
introduce the \textbf{\textit{D}}eterministic \textbf{\textit{G}}uidance
\textbf{\textit{D}}iffusion \textbf{\textit{M}}odel (DGDM) for probabilistic
weather forecasting, integrating benefits of both deterministic and
probabilistic approaches. During the forward process, both the deterministic
and probabilistic models are trained end-to-end. In the reverse process,
weather forecasting leverages the predicted result from the deterministic
model, using as an intermediate starting point for the probabilistic model. By
fusing deterministic models with probabilistic models in this manner, DGDM is
capable of providing accurate forecasts while also offering probabilistic
predictions. To evaluate DGDM, we assess it on the global weather forecasting
dataset (WeatherBench) and the common video frame prediction benchmark (Moving
MNIST). We also introduce and evaluate the Pacific Northwest Windstorm
(PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in
high-resolution regional forecasting. As a result of our experiments, DGDM
achieves state-of-the-art results not only in global forecasting but also in
regional forecasting. The code is available at:
\url{https://github.com/DongGeun-Yoon/DGDM}.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Local Masking Meets Progressive Freezing: Crafting Efficient Vision Transformers for Self-Supervised Learning. (arXiv:2312.02194v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02194">http://arxiv.org/abs/2312.02194</a></li>
<li>Code URL: https://github.com/utkutpcgl/vitfreeze</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02194]] Local Masking Meets Progressive Freezing: Crafting Efficient Vision Transformers for Self-Supervised Learning(http://arxiv.org/abs/2312.02194)</code></li>
<li>Summary: <p>In this paper, we present an innovative approach to self-supervised learning
for Vision Transformers (ViTs), integrating local masked image modeling with
progressive layer freezing. This method focuses on enhancing the efficiency and
speed of initial layer training in ViTs. By systematically freezing specific
layers at strategic points during training, we reduce computational demands
while maintaining or improving learning capabilities. Our approach employs a
novel multi-scale reconstruction process that fosters efficient learning in
initial layers and enhances semantic comprehension across scales. The results
demonstrate a substantial reduction in training time (~12.5\%) with a minimal
impact on model accuracy (decrease in top-1 accuracy by 0.6\%). Our method
achieves top-1 and top-5 accuracies of 82.6\% and 96.2\%, respectively,
underscoring its potential in scenarios where computational resources and time
are critical. This work marks an advancement in the field of self-supervised
learning for computer vision. The implementation of our approach is available
at our project's GitHub repository: github.com/utkutpcgl/ViTFreeze.
</p></li>
</ul>

<h3>Title: USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery. (arXiv:2312.02199v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02199">http://arxiv.org/abs/2312.02199</a></li>
<li>Code URL: https://github.com/stanfordmlgroup/usat</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02199]] USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite Imagery(http://arxiv.org/abs/2312.02199)</code></li>
<li>Summary: <p>Large, self-supervised vision models have led to substantial advancements for
automatically interpreting natural images. Recent works have begun tailoring
these methods to remote sensing data which has rich structure with
multi-sensor, multi-spectral, and temporal information providing massive
amounts of self-labeled data that can be used for self-supervised pre-training.
In this work, we develop a new encoder architecture called USat that can input
multi-spectral data from multiple sensors for self-supervised pre-training.
USat is a vision transformer with modified patch projection layers and
positional encodings to model spectral bands with varying spatial scales from
multiple sensors. We integrate USat into a Masked Autoencoder (MAE)
self-supervised pre-training procedure and find that a pre-trained USat
outperforms state-of-the-art self-supervised MAE models trained on remote
sensing data on multiple remote sensing benchmark datasets (up to 8%) and leads
to improvements in low data regimes (up to 7%). Code and pre-trained weights
are available at https://github.com/stanfordmlgroup/USat .
</p></li>
</ul>

<h3>Title: VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding. (arXiv:2312.02310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02310">http://arxiv.org/abs/2312.02310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02310]] VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding(http://arxiv.org/abs/2312.02310)</code></li>
<li>Summary: <p>Recent advancements in language-model-based video understanding have been
progressing at a remarkable pace, spurred by the introduction of Large Language
Models (LLMs). However, the focus of prior research has been predominantly on
devising a projection layer that maps video features to tokens, an approach
that is both rudimentary and inefficient. In our study, we introduce a
cutting-edge framework, VaQuitA, designed to refine the synergy between video
and textual information. At the data level, instead of sampling frames
uniformly, we implement a sampling method guided by CLIP-score rankings, which
enables a more aligned selection of frames with the given question. At the
feature level, we integrate a trainable Video Perceiver alongside a
Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the
interplay between the input question and the video features. We also discover
that incorporating a simple prompt, "Please be critical", into the LLM input
can substantially enhance its video comprehension capabilities. Our
experimental results indicate that VaQuitA consistently sets a new benchmark
for zero-shot video question-answering tasks and is adept at producing
high-quality, multi-turn video dialogues with users.
</p></li>
</ul>

<h3>Title: Class-Discriminative Attention Maps for Vision Transformers. (arXiv:2312.02364v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02364">http://arxiv.org/abs/2312.02364</a></li>
<li>Code URL: https://github.com/lenbrocki/cdam</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02364]] Class-Discriminative Attention Maps for Vision Transformers(http://arxiv.org/abs/2312.02364)</code></li>
<li>Summary: <p>Interpretability methods are critical components for examining and exploring
deep neural networks (DNN), as well as increasing our understanding of and
trust in them. Vision transformers (ViT), which can be trained to
state-of-the-art performance with a self-supervised learning (SSL) training
method, provide built-in attention maps (AM). While AMs can provide
high-quality semantic segmentation of input images, they do not account for any
signal coming from a downstream classifier. We introduce class-discriminative
attention maps (CDAM), a novel post-hoc explanation method that is highly
sensitive to the target class. Our method essentially scales attention scores
by how relevant the corresponding tokens are for the predictions of a
classifier head. Alternative to classifier outputs, CDAM can also explain a
user-defined concept by targeting similarity measures in the latent space of
the ViT. This allows for explanations of arbitrary concepts, defined by the
user through a few sample images. We investigate the operating characteristics
of CDAM in comparison with relevance propagation (RP) and token ablation maps
(TAM), an alternative to pixel occlusion methods. CDAM is highly
class-discriminative and semantically relevant, while providing implicit
regularization of relevance scores.
</p>
<p>PyTorch implementation: \url{https://github.com/lenbrocki/CDAM}
</p>
<p>Web live demo: \url{https://cdam.informatism.com/}
</p></li>
</ul>

<h3>Title: MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR. (arXiv:2312.02409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02409">http://arxiv.org/abs/2312.02409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02409]] MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR(http://arxiv.org/abs/2312.02409)</code></li>
<li>Summary: <p>Motion prediction has been an essential component of autonomous driving
systems since it handles highly uncertain and complex scenarios involving
moving agents of different types. In this paper, we propose a Multi-Granular
TRansformer (MGTR) framework, an encoder-decoder network that exploits context
features in different granularities for different kinds of traffic agents. To
further enhance MGTR's capabilities, we leverage LiDAR point cloud data by
incorporating LiDAR semantic features from an off-the-shelf LiDAR feature
extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark
and show that the proposed method achieved state-of-the-art performance,
ranking 1st on its leaderboard
(https://waymo.com/open/challenges/2023/motion-prediction/).
</p></li>
</ul>

<h3>Title: Towards More Unified In-context Visual Understanding. (arXiv:2312.02520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02520">http://arxiv.org/abs/2312.02520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02520]] Towards More Unified In-context Visual Understanding(http://arxiv.org/abs/2312.02520)</code></li>
<li>Summary: <p>The rapid advancement of large language models (LLMs) has accelerated the
emergence of in-context learning (ICL) as a cutting-edge approach in the
natural language processing domain. Recently, ICL has been employed in visual
understanding tasks, such as semantic segmentation and image captioning,
yielding promising results. However, existing visual ICL framework can not
enable producing content across multiple modalities, which limits their
potential usage scenarios. To address this issue, we present a new ICL
framework for visual understanding with multi-modal output enabled. First, we
quantize and embed both text and visual prompt into a unified representational
space, structured as interleaved in-context sequences. Then a decoder-only
sparse transformer architecture is employed to perform generative modeling on
them, facilitating in-context learning. Thanks to this design, the model is
capable of handling in-context vision understanding tasks with multimodal
output in a unified pipeline. Experimental results demonstrate that our model
achieves competitive performance compared with specialized models and previous
ICL baselines. Overall, our research takes a further step toward unified
multimodal in-context learning.
</p></li>
</ul>

<h3>Title: DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding. (arXiv:2312.02549v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02549">http://arxiv.org/abs/2312.02549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02549]] DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding(http://arxiv.org/abs/2312.02549)</code></li>
<li>Summary: <p>Temporal Language Grounding seeks to localize video moments that semantically
correspond to a natural language query. Recent advances employ the attention
mechanism to learn the relations between video moments and the text query.
However, naive attention might not be able to appropriately capture such
relations, resulting in ineffective distributions where target video moments
are difficult to separate from the remaining ones. To resolve the issue, we
propose an energy-based model framework to explicitly learn moment-query
distributions. Moreover, we propose DemaFormer, a novel Transformer-based
architecture that utilizes exponential moving average with a learnable damping
factor to effectively encode moment-query inputs. Comprehensive experiments on
four public temporal language grounding datasets showcase the superiority of
our methods over the state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: UPOCR: Towards Unified Pixel-Level OCR Interface. (arXiv:2312.02694v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02694">http://arxiv.org/abs/2312.02694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02694]] UPOCR: Towards Unified Pixel-Level OCR Interface(http://arxiv.org/abs/2312.02694)</code></li>
<li>Summary: <p>In recent years, the optical character recognition (OCR) field has been
proliferating with plentiful cutting-edge approaches for a wide spectrum of
tasks. However, these approaches are task-specifically designed with divergent
paradigms, architectures, and training strategies, which significantly
increases the complexity of research and maintenance and hinders the fast
deployment in applications. To this end, we propose UPOCR, a
simple-yet-effective generalist model for Unified Pixel-level OCR interface.
Specifically, the UPOCR unifies the paradigm of diverse OCR tasks as
image-to-image transformation and the architecture as a vision Transformer
(ViT)-based encoder-decoder. Learnable task prompts are introduced to push the
general feature representations extracted by the encoder toward task-specific
spaces, endowing the decoder with task awareness. Moreover, the model training
is uniformly aimed at minimizing the discrepancy between the generated and
ground-truth images regardless of the inhomogeneity among tasks. Experiments
are conducted on three pixel-level OCR tasks including text removal, text
segmentation, and tampered text detection. Without bells and whistles, the
experimental results showcase that the proposed method can simultaneously
achieve state-of-the-art performance on three tasks with a unified single
model, which provides valuable strategies and insights for future research on
generalist OCR models. Code will be publicly available.
</p></li>
</ul>

<h3>Title: R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction. (arXiv:2312.02725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02725">http://arxiv.org/abs/2312.02725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02725]] R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction(http://arxiv.org/abs/2312.02725)</code></li>
<li>Summary: <p>Recently, vision transformers have performed well in various computer vision
tasks, including voxel 3D reconstruction. However, the windows of the vision
transformer are not multi-scale, and there is no connection between the
windows, which limits the accuracy of voxel 3D reconstruction . Therefore, we
propose a shifted windows attention voxel 3D reconstruction network. To the
best of our knowledge, this is the first work to apply shifted window attention
to voxel 3D reconstruction. Experimental results on ShapeNet verify our method
achieves SOTA accuracy in single-view reconstruction.
</p></li>
</ul>

<h3>Title: RotaTR: Detection Transformer for Dense and Rotated Object. (arXiv:2312.02821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02821">http://arxiv.org/abs/2312.02821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02821]] RotaTR: Detection Transformer for Dense and Rotated Object(http://arxiv.org/abs/2312.02821)</code></li>
<li>Summary: <p>Detecting the objects in dense and rotated scenes is a challenging task.
Recent works on this topic are mostly based on Faster RCNN or Retinanet. As
they are highly dependent on the pre-set dense anchors and the NMS operation,
the approach is indirect and suboptimal.The end-to-end DETR-based detectors
have achieved great success in horizontal object detection and many other areas
like segmentation, tracking, action recognition and etc.However, the DETR-based
detectors perform poorly on dense rotated target tasks and perform worse than
most modern CNN-based detectors. In this paper, we find the most significant
reason for the poor performance is that the original attention can not
accurately focus on the oriented targets. Accordingly, we propose Rotated
object detection TRansformer (RotaTR) as an extension of DETR to oriented
detection. Specifically, we design Rotation Sensitive deformable (RSDeform)
attention to enhance the DETR's ability to detect oriented targets. It is used
to build the feature alignment module and rotation-sensitive decoder for our
model. We test RotaTR on four challenging-oriented benchmarks. It shows a great
advantage in detecting dense and oriented objects compared to the original
DETR. It also achieves competitive results when compared to the
state-of-the-art.
</p></li>
</ul>

<h3>Title: Are Vision Transformers More Data Hungry Than Newborn Visual Systems?. (arXiv:2312.02843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02843">http://arxiv.org/abs/2312.02843</a></li>
<li>Code URL: https://github.com/buildingamind/vit-cot</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02843]] Are Vision Transformers More Data Hungry Than Newborn Visual Systems?(http://arxiv.org/abs/2312.02843)</code></li>
<li>Summary: <p>Vision transformers (ViTs) are top performing models on many computer vision
benchmarks and can accurately predict human behavior on object recognition
tasks. However, researchers question the value of using ViTs as models of
biological learning because ViTs are thought to be more data hungry than
brains, with ViTs requiring more training data to reach similar levels of
performance. To test this assumption, we directly compared the learning
abilities of ViTs and animals, by performing parallel controlled rearing
experiments on ViTs and newborn chicks. We first raised chicks in impoverished
visual environments containing a single object, then simulated the training
data available in those environments by building virtual animal chambers in a
video game engine. We recorded the first-person images acquired by agents
moving through the virtual chambers and used those images to train self
supervised ViTs that leverage time as a teaching signal, akin to biological
visual systems. When ViTs were trained through the eyes of newborn chicks, the
ViTs solved the same view invariant object recognition tasks as the chicks.
Thus, ViTs were not more data hungry than newborn visual systems: both learned
view invariant object representations in impoverished visual environments. The
flexible and generic attention based learning mechanism in ViTs combined with
the embodied data streams available to newborn animals appears sufficient to
drive the development of animal-like object recognition.
</p></li>
</ul>

<h3>Title: Fine-tuning pre-trained extractive QA models for clinical document parsing. (arXiv:2312.02314v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02314">http://arxiv.org/abs/2312.02314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02314]] Fine-tuning pre-trained extractive QA models for clinical document parsing(http://arxiv.org/abs/2312.02314)</code></li>
<li>Summary: <p>Electronic health records (EHRs) contain a vast amount of high-dimensional
multi-modal data that can accurately represent a patient's medical history.
Unfortunately, most of this data is either unstructured or semi-structured,
rendering it unsuitable for real-time and retrospective analyses. A remote
patient monitoring (RPM) program for Heart Failure (HF) patients needs to have
access to clinical markers like EF (Ejection Fraction) or LVEF (Left
Ventricular Ejection Fraction) in order to ascertain eligibility and
appropriateness for the program. This paper explains a system that can parse
echocardiogram reports and verify EF values. This system helps identify
eligible HF patients who can be enrolled in such a program. At the heart of
this system is a pre-trained extractive QA transformer model that is fine-tuned
on custom-labeled data. The methods used to prepare such a model for deployment
are illustrated by running experiments on a public clinical dataset like
MIMIC-IV-Note. The pipeline can be used to generalize solutions to similar
problems in a low-resource setting. We found that the system saved over 1500
hours for our clinicians over 12 months by automating the task at scale.
</p></li>
</ul>

<h3>Title: Empathy and Distress Detection using Ensembles of Transformer Models. (arXiv:2312.02578v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02578">http://arxiv.org/abs/2312.02578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02578]] Empathy and Distress Detection using Ensembles of Transformer Models(http://arxiv.org/abs/2312.02578)</code></li>
<li>Summary: <p>This paper presents our approach for the WASSA 2023 Empathy, Emotion and
Personality Shared Task. Empathy and distress are human feelings that are
implicitly expressed in natural discourses. Empathy and distress detection are
crucial challenges in Natural Language Processing that can aid our
understanding of conversations. The provided dataset consists of several
long-text examples in the English language, with each example associated with a
numeric score for empathy and distress. We experiment with several BERT-based
models as a part of our approach. We also try various ensemble methods. Our
final submission has a Pearson's r score of 0.346, placing us third in the
empathy and distress detection subtask.
</p></li>
</ul>

<h3>Title: Text Intimacy Analysis using Ensembles of Multilingual Transformers. (arXiv:2312.02590v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02590">http://arxiv.org/abs/2312.02590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02590]] Text Intimacy Analysis using Ensembles of Multilingual Transformers(http://arxiv.org/abs/2312.02590)</code></li>
<li>Summary: <p>Intimacy estimation of a given text has recently gained importance due to the
increase in direct interaction of NLP systems with humans. Intimacy is an
important aspect of natural language and has a substantial impact on our
everyday communication. Thus the level of intimacy can provide us with deeper
insights and richer semantics of conversations. In this paper, we present our
work on the SemEval shared task 9 on predicting the level of intimacy for the
given text. The dataset consists of tweets in ten languages, out of which only
six are available in the training dataset. We conduct several experiments and
show that an ensemble of multilingual models along with a language-specific
monolingual model has the best performance. We also evaluate other data
augmentation methods such as translation and present the results. Lastly, we
study the results thoroughly and present some noteworthy insights into this
problem.
</p></li>
</ul>

<h3>Title: FaultFormer: Transformer-based Prediction of Bearing Faults. (arXiv:2312.02380v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02380">http://arxiv.org/abs/2312.02380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02380]] FaultFormer: Transformer-based Prediction of Bearing Faults(http://arxiv.org/abs/2312.02380)</code></li>
<li>Summary: <p>The growth of deep learning in the past decade has motivated important
applications to smart manufacturing and machine health monitoring. In
particular, vibration data offers a rich and reliable source to provide
meaningful insights into machine health and predictive maintenance. In this
work, we present a Transformer based framework for analyzing vibration signals
to predict different types of bearing faults (FaultFormer). In particular, we
process signal data using data augmentations and extract their Fourier modes to
train a transformer encoder to achieve state of the art accuracies. The
attention mechanism as well as model outputs were analyzed to confirm the
transformer's ability to automatically extract features within signals and
learn both global and local relationships to make classifications. Lastly, two
pretraining strategies were proposed to pave the way for large, generalizable
transformers that could adapt to new data, situations, or machinery on the
production floor.
</p></li>
</ul>

<h3>Title: MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection. (arXiv:2312.02530v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02530">http://arxiv.org/abs/2312.02530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02530]] MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection(http://arxiv.org/abs/2312.02530)</code></li>
<li>Summary: <p>Detecting anomalies in real-world multivariate time series data is
challenging due to complex temporal dependencies and inter-variable
correlations. Recently, reconstruction-based deep models have been widely used
to solve the problem. However, these methods still suffer from an
over-generalization issue and fail to deliver consistently high performance. To
address this issue, we propose the MEMTO, a memory-guided Transformer using a
reconstruction-based approach. It is designed to incorporate a novel memory
module that can learn the degree to which each memory item should be updated in
response to the input data. To stabilize the training procedure, we use a
two-phase training paradigm which involves using K-means clustering for
initializing memory items. Additionally, we introduce a bi-dimensional
deviation-based detection criterion that calculates anomaly scores considering
both input space and latent space. We evaluate our proposed method on five
real-world datasets from diverse domains, and it achieves an average anomaly
detection F1-score of 95.74%, significantly outperforming the previous
state-of-the-art methods. We also conduct extensive experiments to empirically
validate the effectiveness of our proposed model's key components.
</p></li>
</ul>

<h3>Title: Structured World Representations in Maze-Solving Transformers. (arXiv:2312.02566v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02566">http://arxiv.org/abs/2312.02566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02566]] Structured World Representations in Maze-Solving Transformers(http://arxiv.org/abs/2312.02566)</code></li>
<li>Summary: <p>Transformer models underpin many recent advances in practical machine
learning applications, yet understanding their internal behavior continues to
elude researchers. Given the size and complexity of these models, forming a
comprehensive picture of their inner workings remains a significant challenge.
To this end, we set out to understand small transformer models in a more
tractable setting: that of solving mazes. In this work, we focus on the
abstractions formed by these models and find evidence for the consistent
emergence of structured internal representations of maze topology and valid
paths. We demonstrate this by showing that the residual stream of only a single
token can be linearly decoded to faithfully reconstruct the entire maze. We
also find that the learned embeddings of individual tokens have spatial
structure. Furthermore, we take steps towards deciphering the circuity of
path-following by identifying attention heads (dubbed $\textit{adjacency
heads}$), which are implicated in finding valid subsequent tokens.
</p></li>
</ul>

<h3>Title: MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition. (arXiv:2312.02829v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02829">http://arxiv.org/abs/2312.02829</a></li>
<li>Code URL: https://github.com/ibm/multiple-input-multiple-output-nets</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02829]] MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition(http://arxiv.org/abs/2312.02829)</code></li>
<li>Summary: <p>With the advent of deep learning, progressively larger neural networks have
been designed to solve complex tasks. We take advantage of these capacity-rich
models to lower the cost of inference by exploiting computation in
superposition. To reduce the computational burden per input, we propose
Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling
many inputs at once. MIMONets augment various deep neural network architectures
with variable binding mechanisms to represent an arbitrary number of inputs in
a compositional data structure via fixed-width distributed representations.
Accordingly, MIMONets adapt nonlinear neural transformations to process the
data structure holistically, leading to a speedup nearly proportional to the
number of superposed input items in the data structure. After processing in
superposition, an unbinding mechanism recovers each transformed input of
interest. MIMONets also provide a dynamic trade-off between accuracy and
throughput by an instantaneous on-demand switching between a set of
accuracy-throughput operating points, yet within a single set of fixed
parameters. We apply the concept of MIMONets to both CNN and Transformer
architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical
evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy
delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and
CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining
a high average accuracy within a [-1.07, -3.43]% delta on the long range arena
benchmark. Finally, we provide mathematical bounds on the interference between
superposition channels in MIMOFormer. Our code is available at
https://github.com/IBM/multiple-input-multiple-output-nets.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch. (arXiv:2312.02168v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02168">http://arxiv.org/abs/2312.02168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02168]] The SVHN Dataset Is Deceptive for Probabilistic Generative Models Due to a Distribution Mismatch(http://arxiv.org/abs/2312.02168)</code></li>
<li>Summary: <p>The Street View House Numbers (SVHN) dataset is a popular benchmark dataset
in deep learning. Originally designed for digit classification tasks, the SVHN
dataset has been widely used as a benchmark for various other tasks including
generative modeling. However, with this work, we aim to warn the community
about an issue of the SVHN dataset as a benchmark for generative modeling
tasks: we discover that the official split into training set and test set of
the SVHN dataset are not drawn from the same distribution. We empirically show
that this distribution mismatch has little impact on the classification task
(which may explain why this issue has not been detected before), but it
severely affects the evaluation of probabilistic generative models, such as
Variational Autoencoders and diffusion models. As a workaround, we propose to
mix and re-split the official training and test set when SVHN is used for tasks
other than classification. We publish a new split and the indices we used to
create it at https://jzenn.github.io/svhn-remix/ .
</p></li>
</ul>

<h3>Title: GenEM: Physics-Informed Generative Cryo-Electron Microscopy. (arXiv:2312.02235v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02235">http://arxiv.org/abs/2312.02235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02235]] GenEM: Physics-Informed Generative Cryo-Electron Microscopy(http://arxiv.org/abs/2312.02235)</code></li>
<li>Summary: <p>In the past decade, deep conditional generative models have revolutionized
the generation of realistic images, extending their application from
entertainment to scientific domains. Single-particle cryo-electron microscopy
(cryo-EM) is crucial in resolving near-atomic resolution 3D structures of
proteins, such as the SARS-COV-2 spike protein. To achieve high-resolution
reconstruction, AI models for particle picking and pose estimation have been
adopted. However, their performance is still limited as they lack high-quality
annotated datasets. To address this, we introduce physics-informed generative
cryo-electron microscopy (GenEM), which for the first time integrates
physical-based cryo-EM simulation with a generative unpaired noise translation
to generate physically correct synthetic cryo-EM datasets with realistic
noises. Initially, GenEM simulates the cryo-EM imaging process based on a
virtual specimen. To generate realistic noises, we leverage an unpaired noise
translation via contrastive learning with a novel mask-guided sampling scheme.
Extensive experiments show that GenEM is capable of generating realistic
cryo-EM images. The generated dataset can further enhance particle picking and
pose estimation models, eventually improving the reconstruction resolution. We
will release our code and annotated synthetic datasets.
</p></li>
</ul>

<h3>Title: PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation. (arXiv:2312.02284v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02284">http://arxiv.org/abs/2312.02284</a></li>
<li>Code URL: https://github.com/zhyever/PatchFusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02284]] PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation(http://arxiv.org/abs/2312.02284)</code></li>
<li>Summary: <p>Single image depth estimation is a foundational task in computer vision and
generative modeling. However, prevailing depth estimation models grapple with
accommodating the increasing resolutions commonplace in today's consumer
cameras and devices. Existing high-resolution strategies show promise, but they
often face limitations, ranging from error propagation to the loss of
high-frequency details. We present PatchFusion, a novel tile-based framework
with three key components to improve the current state of the art: (1) A
patch-wise fusion network that fuses a globally-consistent coarse prediction
with finer, inconsistent tiled predictions via high-level feature guidance, (2)
A Global-to-Local (G2L) module that adds vital context to the fusion network,
discarding the need for patch selection heuristics, and (3) A Consistency-Aware
Training (CAT) and Inference (CAI) approach, emphasizing patch overlap
consistency and thereby eradicating the necessity for post-processing.
Experiments on UnrealStereo4K, MVS-Synth, and Middleburry 2014 demonstrate that
our framework can generate high-resolution depth maps with intricate details.
PatchFusion is independent of the base model for depth estimation. Notably, our
framework built on top of SOTA ZoeDepth brings improvements for a total of
17.3% and 29.4% in terms of the root mean squared error (RMSE) on
UnrealStereo4K and MVS-Synth, respectively.
</p></li>
</ul>

<h3>Title: How Generative-AI can be Effectively used in Government Chatbots. (arXiv:2312.02181v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02181">http://arxiv.org/abs/2312.02181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02181]] How Generative-AI can be Effectively used in Government Chatbots(http://arxiv.org/abs/2312.02181)</code></li>
<li>Summary: <p>With the rapid development of artificial intelligence and breakthroughs in
machine learning and natural language processing, intelligent
question-answering robots have become widely used in government affairs. This
paper conducts a horizontal comparison between Guangdong Province's government
chatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the
strengths and weaknesses of existing government chatbots and AIGC technology.
The study finds significant differences between government chatbots and large
language models. China's government chatbots are still in an exploratory stage
and have a gap to close to achieve "intelligence." To explore the future
direction of government chatbots more deeply, this research proposes targeted
optimization paths to help generative AI be effectively applied in government
chatbot conversations.
</p></li>
</ul>

<h3>Title: An Evaluation Framework for Mapping News Headlines to Event Classes in a Knowledge Graph. (arXiv:2312.02334v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02334">http://arxiv.org/abs/2312.02334</a></li>
<li>Code URL: https://github.com/mbouadeus/news-headline-event-linking</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02334]] An Evaluation Framework for Mapping News Headlines to Event Classes in a Knowledge Graph(http://arxiv.org/abs/2312.02334)</code></li>
<li>Summary: <p>Mapping ongoing news headlines to event-related classes in a rich knowledge
base can be an important component in a knowledge-based event analysis and
forecasting solution. In this paper, we present a methodology for creating a
benchmark dataset of news headlines mapped to event classes in Wikidata, and
resources for the evaluation of methods that perform the mapping. We use the
dataset to study two classes of unsupervised methods for this task: 1)
adaptations of classic entity linking methods, and 2) methods that treat the
problem as a zero-shot text classification problem. For the first approach, we
evaluate off-the-shelf entity linking systems. For the second approach, we
explore a) pre-trained natural language inference (NLI) models, and b)
pre-trained large generative language models. We present the results of our
evaluation, lessons learned, and directions for future work. The dataset and
scripts for evaluation are made publicly available.
</p></li>
</ul>

<h3>Title: Visually Grounded Language Learning: a review of language games, datasets, tasks, and models. (arXiv:2312.02431v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02431">http://arxiv.org/abs/2312.02431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02431]] Visually Grounded Language Learning: a review of language games, datasets, tasks, and models(http://arxiv.org/abs/2312.02431)</code></li>
<li>Summary: <p>In recent years, several machine learning models have been proposed. They are
trained with a language modelling objective on large-scale text-only data. With
such pretraining, they can achieve impressive results on many Natural Language
Understanding and Generation tasks. However, many facets of meaning cannot be
learned by ``listening to the radio" only. In the literature, many
Vision+Language (V+L) tasks have been defined with the aim of creating models
that can ground symbols in the visual modality. In this work, we provide a
systematic literature review of several tasks and models proposed in the V+L
field. We rely on Wittgenstein's idea of `language games' to categorise such
tasks into 3 different families: 1) discriminative games, 2) generative games,
and 3) interactive games. Our analysis of the literature provides evidence that
future work should be focusing on interactive games where communication in
Natural Language is important to resolve ambiguities about object referents and
action plans and that physical embodiment is essential to understand the
semantics of situations and events. Overall, these represent key requirements
for developing grounded meanings in neural models.
</p></li>
</ul>

<h3>Title: MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks. (arXiv:2312.02496v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02496">http://arxiv.org/abs/2312.02496</a></li>
<li>Code URL: https://github.com/liangke23/knowledge_assisted_medical_dialogue_generation_mechanism</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02496]] MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks(http://arxiv.org/abs/2312.02496)</code></li>
<li>Summary: <p>Using natural language processing (NLP) technologies to develop medical
chatbots makes the diagnosis of the patient more convenient and efficient,
which is a typical application in healthcare AI. Because of its importance,
lots of research have been come out. Recently, the neural generative models
have shown their impressive ability as the core of chatbot, while it cannot
scale well when directly applied to medical conversation due to the lack of
medical-specific knowledge. To address the limitation, a scalable Medical
Knowledge Assisted mechanism, MKA, is proposed in this paper. The mechanism
aims to assist general neural generative models to achieve better performance
on the medical conversation task. The medical-specific knowledge graph is
designed within the mechanism, which contains 6 types of medical-related
information, including department, drug, check, symptom, disease, food.
Besides, the specific token concatenation policy is defined to effectively
inject medical information into the input data. Evaluation of our method is
carried out on two typical medical datasets, MedDG and MedDialog-CN. The
evaluation results demonstrate that models combined with our mechanism
outperform original methods in multiple automatic evaluation metrics. Besides,
MKA-Bert-GPT achieves state-of-the-art performance. The open-sourced codes are
public:
https://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanism
</p></li>
</ul>

<h3>Title: H-GAP: Humanoid Control with a Generalist Planner. (arXiv:2312.02682v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02682">http://arxiv.org/abs/2312.02682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02682]] H-GAP: Humanoid Control with a Generalist Planner(http://arxiv.org/abs/2312.02682)</code></li>
<li>Summary: <p>Humanoid control is an important research challenge offering avenues for
integration into human-centric infrastructures and enabling physics-driven
humanoid animations. The daunting challenges in this field stem from the
difficulty of optimizing in high-dimensional action spaces and the instability
introduced by the bipedal morphology of humanoids. However, the extensive
collection of human motion-captured data and the derived datasets of humanoid
trajectories, such as MoCapAct, paves the way to tackle these challenges. In
this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a
state-action trajectory generative model trained on humanoid trajectories
derived from human motion-captured data, capable of adeptly handling downstream
control tasks with Model Predictive Control (MPC). For 56 degrees of freedom
humanoid, we empirically demonstrate that H-GAP learns to represent and
generate a wide range of motor behaviours. Further, without any learning from
online interactions, it can also flexibly transfer these behaviors to solve
novel downstream control tasks via planning. Notably, H-GAP excels established
MPC baselines that have access to the ground truth dynamics model, and is
superior or comparable to offline RL methods trained for individual tasks.
Finally, we do a series of empirical studies on the scaling properties of
H-GAP, showing the potential for performance gains via additional data but not
computing. Code and videos are available at
https://ycxuyingchen.github.io/hgap/.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Large Language Models as Consistent Story Visualizers. (arXiv:2312.02252v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02252">http://arxiv.org/abs/2312.02252</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02252]] Large Language Models as Consistent Story Visualizers(http://arxiv.org/abs/2312.02252)</code></li>
<li>Summary: <p>Recent generative models have demonstrated impressive capabilities in
generating realistic and visually pleasing images grounded on textual prompts.
Nevertheless, a significant challenge remains in applying these models for the
more intricate task of story visualization. Since it requires resolving
pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,
and ensuring consistent characters and background synthesis across frames. Yet,
the emerging Large Language Model (LLM) showcases robust reasoning abilities to
navigate through ambiguous references and process extensive sequences.
Therefore, we introduce \textbf{StoryGPT-V}, which leverages the merits of the
latent diffusion (LDM) and LLM to produce images with consistent and
high-quality characters grounded on given story descriptions. First, we train a
character-aware LDM, which takes character-augmented semantic embedding as
input and includes the supervision of the cross-attention map using character
segmentation masks, aiming to enhance character generation accuracy and
faithfulness. In the second stage, we enable an alignment between the output of
LLM and the character-augmented embedding residing in the input space of the
first-stage model. This harnesses the reasoning ability of LLM to address
ambiguous references and the comprehension capability to memorize the context.
We conduct comprehensive experiments on two visual story visualization
benchmarks. Our model reports superior quantitative results and consistently
generates accurate characters of remarkable quality with low memory
consumption. Our code will be made publicly available.
</p></li>
</ul>

<h3>Title: EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model. (arXiv:2312.02483v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02483">http://arxiv.org/abs/2312.02483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02483]] EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model(http://arxiv.org/abs/2312.02483)</code></li>
<li>Summary: <p>Early weakly supervised video grounding (WSVG) methods often struggle with
incomplete boundary detection due to the absence of temporal boundary
annotations. To bridge the gap between video-level and boundary-level
annotation, explicit-supervision methods, i.e., generating pseudo-temporal
boundaries for training, have achieved great success. However, data
augmentations in these methods might disrupt critical temporal information,
yielding poor pseudo boundaries. In this paper, we propose a new perspective
that maintains the integrity of the original temporal content while introducing
more valuable information for expanding the incomplete boundaries. To this end,
we propose EtC (Expand then Clarify), first use the additional information to
expand the initial incomplete pseudo boundaries, and subsequently refine these
expanded ones to achieve precise boundaries. Motivated by video continuity,
i.e., visual similarity across adjacent frames, we use powerful multimodal
large language models (MLLMs) to annotate each frame within initial pseudo
boundaries, yielding more comprehensive descriptions for expanded boundaries.
To further clarify the noise of expanded boundaries, we combine mutual learning
with a tailored proposal-level contrastive objective to use a learnable
approach to harmonize a balance between incomplete yet clean (initial) and
comprehensive yet noisy (expanded) boundaries for more precise ones.
Experiments demonstrate the superiority of our method on two challenging WSVG
datasets.
</p></li>
</ul>

<h3>Title: Training Chain-of-Thought via Latent-Variable Inference. (arXiv:2312.02179v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02179">http://arxiv.org/abs/2312.02179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02179]] Training Chain-of-Thought via Latent-Variable Inference(http://arxiv.org/abs/2312.02179)</code></li>
<li>Summary: <p>Large language models (LLMs) solve problems more accurately and interpretably
when instructed to work out the answer step by step using a
``chain-of-thought'' (CoT) prompt. One can also improve LLMs' performance on a
specific task by supervised fine-tuning, i.e., by using gradient ascent on some
tunable parameters to maximize the average log-likelihood of correct answers
from a labeled training set. Naively combining CoT with supervised tuning
requires supervision not just of the correct answers, but also of detailed
rationales that lead to those answers; these rationales are expensive to
produce by hand. Instead, we propose a fine-tuning strategy that tries to
maximize the \emph{marginal} log-likelihood of generating a correct answer
using CoT prompting, approximately averaging over all possible rationales. The
core challenge is sampling from the posterior over rationales conditioned on
the correct answer; we address it using a simple Markov-chain Monte Carlo
(MCMC) expectation-maximization (EM) algorithm inspired by the self-taught
reasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent
contrastive divergence. This algorithm also admits a novel control-variate
technique that drives the variance of our gradient estimates to zero as the
model improves. Applying our technique to GSM8K and the tasks in BIG-Bench
Hard, we find that this MCMC-EM fine-tuning technique typically improves the
model's accuracy on held-out examples more than STaR or prompt-tuning with or
without CoT.
</p></li>
</ul>

<h3>Title: Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings. (arXiv:2312.02337v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02337">http://arxiv.org/abs/2312.02337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02337]] Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings(http://arxiv.org/abs/2312.02337)</code></li>
<li>Summary: <p>An essential part of monitoring machine learning models in production is
measuring input and output data drift. In this paper, we present a system for
measuring distributional shifts in natural language data and highlight and
investigate the potential advantage of using large language models (LLMs) for
this problem. Recent advancements in LLMs and their successful adoption in
different domains indicate their effectiveness in capturing semantic
relationships for solving various natural language processing problems. The
power of LLMs comes largely from the encodings (embeddings) generated in the
hidden layers of the corresponding neural network. First we propose a
clustering-based algorithm for measuring distributional shifts in text data by
exploiting such embeddings. Then we study the effectiveness of our approach
when applied to text embeddings generated by both LLMs and classical embedding
algorithms. Our experiments show that general-purpose LLM-based embeddings
provide a high sensitivity to data drift compared to other embedding methods.
We propose drift sensitivity as an important evaluation metric to consider when
comparing language models. Finally, we present insights and lessons learned
from deploying our framework as part of the Fiddler ML Monitoring platform over
a period of 18 months.
</p></li>
</ul>

<h3>Title: Efficient Online Data Mixing For Language Model Pre-Training. (arXiv:2312.02406v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02406">http://arxiv.org/abs/2312.02406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02406]] Efficient Online Data Mixing For Language Model Pre-Training(http://arxiv.org/abs/2312.02406)</code></li>
<li>Summary: <p>The data used to pretrain large language models has a decisive impact on a
model's downstream performance, which has led to a large body of work on data
selection methods that aim to automatically determine the most suitable data to
use for pretraining. Existing data selection methods suffer from slow and
computationally expensive processes, a problem amplified by the increasing size
of models and of pretraining datasets. Data mixing, on the other hand, reduces
the complexity of data selection by grouping data points together and
determining sampling probabilities across entire groups. However, data mixing
proportions are typically fixed before training and therefore cannot adapt to
changing training dynamics. To address these limitations, we develop an
efficient algorithm for Online Data Mixing (ODM) that combines elements from
both data selection and data mixing. Based on multi-armed bandit algorithms,
our online approach optimizes the data mixing proportions during training.
Remarkably, our method trains a model that reaches the final perplexity of the
next best method with 19\% fewer training iterations, and improves performance
on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible
wall-clock time during pretraining.
</p></li>
</ul>

<h3>Title: Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data. (arXiv:2312.02418v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02418">http://arxiv.org/abs/2312.02418</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02418]] Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data(http://arxiv.org/abs/2312.02418)</code></li>
<li>Summary: <p>Code datasets, often collected from diverse and uncontrolled sources such as
GitHub, potentially suffer from quality issues, thereby affecting the
performance and training efficiency of Large Language Models (LLMs) optimized
for code generation. Previous studies demonstrated the benefit of using
embedding spaces for data pruning, but they mainly focused on duplicate removal
or increasing variety, and in other modalities, such as images. Our work
focuses on using embeddings to identify and remove "low-quality" code data.
First, we explore features of "low-quality" code in embedding space, through
the use of synthetic corruptions. Armed with this knowledge, we devise novel
pruning metrics that operate in embedding space to identify and remove
low-quality entries in the Stack dataset. We demonstrate the benefits of this
synthetic corruption informed pruning (SCIP) approach on the well-established
HumanEval and MBPP benchmarks, outperforming existing embedding-based methods.
Importantly, we achieve up to a 3% performance improvement over no pruning,
thereby showing the promise of insights from synthetic corruptions for data
pruning.
</p></li>
</ul>

<h3>Title: MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following. (arXiv:2312.02436v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02436">http://arxiv.org/abs/2312.02436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02436]] MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following(http://arxiv.org/abs/2312.02436)</code></li>
<li>Summary: <p>In the realm of large language models (LLMs), enhancing instruction-following
capability often involves curating expansive training data. This is achieved
through two primary schemes: i) Scaling-Inputs: Amplifying (input, output)
pairs per task instruction, aiming for better instruction adherence. ii)
Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction,
output) pair (without requiring a separate input anymore). However, LLMs under
Scaling-Inputs tend to be overly sensitive to inputs, leading to
misinterpretation or non-compliance with instructions. Conversely, Scaling
Input-Free Tasks demands a substantial number of tasks but is less effective in
instruction following when dealing with instances in Scaling-Inputs. This work
introduces MUFFIN, a new scheme of instruction-following dataset curation.
Specifically, we automatically Scale Tasks per Input by diversifying these
tasks with various input facets. Experimental results across four zero-shot
benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes,
reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate
superior instruction-following capabilities compared to those trained on the
two aforementioned schemes.
</p></li>
</ul>

<h3>Title: ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference. (arXiv:2312.02554v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02554">http://arxiv.org/abs/2312.02554</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02554]] ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference(http://arxiv.org/abs/2312.02554)</code></li>
<li>Summary: <p>Language model alignment is a cutting-edge technique in large language model
training to align the model output to user's intent, e.g., being helpful and
harmless. Recent alignment framework consists of two steps: supervised
fine-tuning with demonstration data and preference learning with human
preference data. Previous preference learning methods, such as RLHF and DPO,
mainly focus on pair-wise preference data. However, in many real-world
scenarios where human feedbacks are intrinsically point-wise, these methods
will suffer from information loss or even fail. To fill this gap, in this
paper, we first develop a preference learning method called point-wise DPO to
tackle point-wise preference data. Further revelation on the connection between
supervised fine-tuning and point-wise preference learning enables us to develop
a unified framework for both human demonstration and point-wise preference
data, which sheds new light on the construction of preference dataset.
Extensive experiments on point-wise datasets with binary or continuous labels
demonstrate the superior performance and efficiency of our proposed methods. A
new dataset with high-quality demonstration samples on harmlessness is
constructed and made publicly available.
</p></li>
</ul>

<h3>Title: Impact of Tokenization on LLaMa Russian Adaptation. (arXiv:2312.02598v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02598">http://arxiv.org/abs/2312.02598</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02598]] Impact of Tokenization on LLaMa Russian Adaptation(http://arxiv.org/abs/2312.02598)</code></li>
<li>Summary: <p>Latest instruction-tuned large language models (LLM) show great results on
various tasks, however, they often face performance degradation for non-English
input. There is evidence that the reason lies in inefficient tokenization
caused by low language representation in pre-training data which hinders the
comprehension of non-English instructions, limiting the potential of target
language instruction-tuning. In this work we investigate the possibility of
addressing the issue with vocabulary substitution in the context of LLaMa
Russian language adaptation. We explore three variants of vocabulary adaptation
and test their performance on Saiga instruction-tuning and fine-tuning on
Russian Super Glue benchmark. The results of automatic evaluation show that
vocabulary substitution not only improves the model's quality in Russian but
also accelerates fine-tuning (35%) and inference (up to 60%) while reducing
memory consumption. Additional human evaluation of the instruction-tuned models
demonstrates that models with Russian-adapted vocabulary generate answers with
higher user preference than the original Saiga-LLaMa model.
</p></li>
</ul>

<h3>Title: Towards Measuring Representational Similarity of Large Language Models. (arXiv:2312.02730v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02730">http://arxiv.org/abs/2312.02730</a></li>
<li>Code URL: https://github.com/mklabunde/llm_repsim</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02730]] Towards Measuring Representational Similarity of Large Language Models(http://arxiv.org/abs/2312.02730)</code></li>
<li>Summary: <p>Understanding the similarity of the numerous released large language models
(LLMs) has many uses, e.g., simplifying model selection, detecting illegal
model reuse, and advancing our understanding of what makes LLMs perform well.
In this work, we measure the similarity of representations of a set of LLMs
with 7B parameters. Our results suggest that some LLMs are substantially
different from others. We identify challenges of using representational
similarity measures that suggest the need of careful study of similarity scores
to avoid false conclusions.
</p></li>
</ul>

<h3>Title: Large Language Models on Graphs: A Comprehensive Survey. (arXiv:2312.02783v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02783">http://arxiv.org/abs/2312.02783</a></li>
<li>Code URL: https://github.com/petergriffinjin/awesome-language-model-on-graphs</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02783]] Large Language Models on Graphs: A Comprehensive Survey(http://arxiv.org/abs/2312.02783)</code></li>
<li>Summary: <p>Large language models (LLMs), such as ChatGPT and LLaMA, are creating
significant advancements in natural language processing, due to their strong
text encoding/decoding ability and newly found emergent capability (e.g.,
reasoning). While LLMs are mainly designed to process pure texts, there are
many real-world scenarios where text data are associated with rich structure
information in the form of graphs (e.g., academic networks, and e-commerce
networks) or scenarios where graph data are paired with rich textual
information (e.g., molecules with descriptions). Besides, although LLMs have
shown their pure text-based reasoning ability, it is underexplored whether such
ability can be generalized to graph scenarios (i.e., graph-based reasoning). In
this paper, we provide a systematic review of scenarios and techniques related
to large language models on graphs. We first summarize potential scenarios of
adopting LLMs on graphs into three categories, namely pure graphs, text-rich
graphs, and text-paired graphs. We then discuss detailed techniques for
utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM
as Aligner, and compare the advantages and disadvantages of different schools
of models. Furthermore, we mention the real-world applications of such methods
and summarize open-source codes and benchmark datasets. Finally, we conclude
with potential future research directions in this fast-growing field. The
related source can be found at
https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.
</p></li>
</ul>

<h3>Title: Weakly Supervised Detection of Hallucinations in LLM Activations. (arXiv:2312.02798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02798">http://arxiv.org/abs/2312.02798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02798]] Weakly Supervised Detection of Hallucinations in LLM Activations(http://arxiv.org/abs/2312.02798)</code></li>
<li>Summary: <p>We propose an auditing method to identify whether a large language model
(LLM) encodes patterns such as hallucinations in its internal states, which may
propagate to downstream tasks. We introduce a weakly supervised auditing
technique using a subset scanning approach to detect anomalous patterns in LLM
activations from pre-trained models. Importantly, our method does not need
knowledge of the type of patterns a-priori. Instead, it relies on a reference
dataset devoid of anomalies during testing. Further, our approach enables the
identification of pivotal nodes responsible for encoding these patterns, which
may offer crucial insights for fine-tuning specific sub-networks for bias
mitigation. We introduce two new scanning methods to handle LLM activations for
anomalous sentences that may deviate from the expected distribution in either
direction. Our results confirm prior findings of BERT's limited internal
capacity for encoding hallucinations, while OPT appears capable of encoding
hallucination information internally. Importantly, our scanning approach,
without prior exposure to false statements, performs comparably to a fully
supervised out-of-distribution classifier.
</p></li>
</ul>

<h3>Title: JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization. (arXiv:2312.02213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02213">http://arxiv.org/abs/2312.02213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02213]] JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization(http://arxiv.org/abs/2312.02213)</code></li>
<li>Summary: <p>In this study, we introduce JarviX, a sophisticated data analytics framework.
JarviX is designed to employ Large Language Models (LLMs) to facilitate an
automated guide and execute high-precision data analyzes on tabular datasets.
This framework emphasizes the significance of varying column types,
capitalizing on state-of-the-art LLMs to generate concise data insight
summaries, propose relevant analysis inquiries, visualize data effectively, and
provide comprehensive explanations for results drawn from an extensive data
analysis pipeline. Moreover, JarviX incorporates an automated machine learning
(AutoML) pipeline for predictive modeling. This integration forms a
comprehensive and automated optimization cycle, which proves particularly
advantageous for optimizing machine configuration. The efficacy and
adaptability of JarviX are substantiated through a series of practical use case
studies.
</p></li>
</ul>

<h3>Title: ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU. (arXiv:2312.02515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02515">http://arxiv.org/abs/2312.02515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02515]] ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU(http://arxiv.org/abs/2312.02515)</code></li>
<li>Summary: <p>Transformer-based large language models (LLMs) have demonstrated outstanding
performance across diverse domains, particularly when fine-turned for specific
domains. Recent studies suggest that the resources required for fine-tuning
LLMs can be economized through parameter-efficient methods such as Low-Rank
Adaptation (LoRA). While LoRA effectively reduces computational burdens and
resource demands, it currently supports only a single-job fine-tuning setup.
</p>
<p>In this paper, we present ASPEN, a high-throughput framework for fine-tuning
LLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA
method, leveraging shared pre-trained model and adaptive scheduling. ASPEN is
compatible with transformer-based language models like LLaMA and ChatGLM, etc.
Experiments show that ASPEN saves 53% of GPU memory when training multiple
LLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about
17% compared to existing methods when training with various pre-trained models
on different GPUs. The adaptive scheduling algorithm reduces turnaround time by
24%, end-to-end training latency by 12%, prioritizing jobs and preventing
out-of-memory issues.
</p></li>
</ul>

<h3>Title: Toward autocorrection of chemical process flowsheets using large language models. (arXiv:2312.02873v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02873">http://arxiv.org/abs/2312.02873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02873]] Toward autocorrection of chemical process flowsheets using large language models(http://arxiv.org/abs/2312.02873)</code></li>
<li>Summary: <p>The process engineering domain widely uses Process Flow Diagrams (PFDs) and
Process and Instrumentation Diagrams (P&amp;IDs) to represent process flows and
equipment configurations. However, the P&amp;IDs and PFDs, hereafter called
flowsheets, can contain errors causing safety hazards, inefficient operation,
and unnecessary expenses. Correcting and verifying flowsheets is a tedious,
manual process. We propose a novel generative AI methodology for automatically
identifying errors in flowsheets and suggesting corrections to the user, i.e.,
autocorrecting flowsheets. Inspired by the breakthrough of Large Language
Models (LLMs) for grammatical autocorrection of human language, we investigate
LLMs for the autocorrection of flowsheets. The input to the model is a
potentially erroneous flowsheet and the output of the model are suggestions for
a corrected flowsheet. We train our autocorrection model on a synthetic dataset
in a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5
accuracy of 84% on an independent test dataset of synthetically generated
flowsheets. The results suggest that the model can learn to autocorrect the
synthetic flowsheets. We envision that flowsheet autocorrection will become a
useful tool for chemical engineers.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Uncertainty Quantification in Machine Learning Based Segmentation: A Post-Hoc Approach for Left Ventricle Volume Estimation in MRI. (arXiv:2312.02167v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02167">http://arxiv.org/abs/2312.02167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02167]] Uncertainty Quantification in Machine Learning Based Segmentation: A Post-Hoc Approach for Left Ventricle Volume Estimation in MRI(http://arxiv.org/abs/2312.02167)</code></li>
<li>Summary: <p>Recent studies have confirmed cardiovascular diseases remain responsible for
highest death toll amongst non-communicable diseases. Accurate left ventricular
(LV) volume estimation is critical for valid diagnosis and management of
various cardiovascular conditions, but poses significant challenge due to
inherent uncertainties associated with segmentation algorithms in magnetic
resonance imaging (MRI). Recent machine learning advancements, particularly
U-Net-like convolutional networks, have facilitated automated segmentation for
medical images, but struggles under certain pathologies and/or different
scanner vendors and imaging protocols. This study proposes a novel methodology
for post-hoc uncertainty estimation in LV volume prediction using It\^{o}
stochastic differential equations (SDEs) to model path-wise behavior for the
prediction error. The model describes the area of the left ventricle along the
heart's long axis. The method is agnostic to the underlying segmentation
algorithm, facilitating its use with various existing and future segmentation
technologies. The proposed approach provides a mechanism for quantifying
uncertainty, enabling medical professionals to intervene for unreliable
predictions. This is of utmost importance in critical applications such as
medical diagnosis, where prediction accuracy and reliability can directly
impact patient outcomes. The method is also robust to dataset changes, enabling
application for medical centers with limited access to labeled data. Our
findings highlight the proposed uncertainty estimation methodology's potential
to enhance automated segmentation robustness and generalizability, paving the
way for more reliable and accurate LV volume estimation in clinical settings as
well as opening new avenues for uncertainty quantification in biomedical image
segmentation, providing promising directions for future research.
</p></li>
</ul>

<h3>Title: TranSegPGD: Improving Transferability of Adversarial Examples on Semantic Segmentation. (arXiv:2312.02207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02207">http://arxiv.org/abs/2312.02207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02207]] TranSegPGD: Improving Transferability of Adversarial Examples on Semantic Segmentation(http://arxiv.org/abs/2312.02207)</code></li>
<li>Summary: <p>Transferability of adversarial examples on image classification has been
systematically explored, which generates adversarial examples in black-box
mode. However, the transferability of adversarial examples on semantic
segmentation has been largely overlooked. In this paper, we propose an
effective two-stage adversarial attack strategy to improve the transferability
of adversarial examples on semantic segmentation, dubbed TranSegPGD.
Specifically, at the first stage, every pixel in an input image is divided into
different branches based on its adversarial property. Different branches are
assigned different weights for optimization to improve the adversarial
performance of all pixels.We assign high weights to the loss of the
hard-to-attack pixels to misclassify all pixels. At the second stage, the
pixels are divided into different branches based on their transferable property
which is dependent on Kullback-Leibler divergence. Different branches are
assigned different weights for optimization to improve the transferability of
the adversarial examples. We assign high weights to the loss of the
high-transferability pixels to improve the transferability of adversarial
examples. Extensive experiments with various segmentation models are conducted
on PASCAL VOC 2012 and Cityscapes datasets to demonstrate the effectiveness of
the proposed method. The proposed adversarial attack method can achieve
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing. (arXiv:2312.02208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02208">http://arxiv.org/abs/2312.02208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02208]] A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing(http://arxiv.org/abs/2312.02208)</code></li>
<li>Summary: <p>Existing state-of-the-art 3D point clouds understanding methods only perform
well in a fully supervised manner. To the best of our knowledge, there exists
no unified framework which simultaneously solves the downstream high-level
understanding tasks, especially when labels are extremely limited. This work
presents a general and simple framework to tackle point clouds understanding
when labels are limited. We propose a novel unsupervised region expansion based
clustering method for generating clusters. More importantly, we innovatively
propose to learn to merge the over-divided clusters based on the local
low-level geometric property similarities and the learned high-level feature
similarities supervised by weak labels. Hence, the true weak labels guide
pseudo labels merging taking both geometric and semantic feature correlations
into consideration. Finally, the self-supervised reconstruction and data
augmentation optimization modules are proposed to guide the propagation of
labels among semantically similar points within a scene. Experimental Results
demonstrate that our framework has the best performance among the three most
important weakly supervised point clouds understanding tasks including semantic
segmentation, instance segmentation, and object detection even when limited
points are labeled, under the data-efficient settings for the large-scale 3D
semantic scene parsing. The developed techniques have postentials to be applied
to downstream tasks for better representations in robotic manipulation and
robotic autonomous navigation. Codes and models are publicly available at:
https://github.com/KangchengLiu.
</p></li>
</ul>

<h3>Title: PixelLM: Pixel Reasoning with Large Multimodal Model. (arXiv:2312.02228v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02228">http://arxiv.org/abs/2312.02228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02228]] PixelLM: Pixel Reasoning with Large Multimodal Model(http://arxiv.org/abs/2312.02228)</code></li>
<li>Summary: <p>While large multimodal models (LMMs) have achieved remarkable progress,
generating pixel-level masks for image reasoning tasks involving multiple
open-world targets remains a challenge. To bridge this gap, we introduce
PixelLM, an effective and efficient LMM for pixel-level reasoning and
understanding. Central to PixelLM is a novel, lightweight pixel decoder and a
comprehensive segmentation codebook. The decoder efficiently produces masks
from the hidden embeddings of the codebook tokens, which encode detailed
target-relevant information. With this design, PixelLM harmonizes with the
structure of popular LMMs and avoids the need for additional costly
segmentation models. Furthermore, we propose a target refinement loss to
enhance the model's ability to differentiate between multiple targets, leading
to substantially improved mask quality. To advance research in this area, we
construct MUSE, a high-quality multi-target reasoning segmentation benchmark.
PixelLM excels across various pixel-level image reasoning and understanding
tasks, outperforming well-established methods in multiple benchmarks, including
MUSE, single- and multi-referring segmentation. Comprehensive ablations confirm
the efficacy of each proposed component. All code, models, and datasets will be
publicly available.
</p></li>
</ul>

<h3>Title: Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality and Missing Modality Scenarios in Semantic Segmentation. (arXiv:2312.02240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02240">http://arxiv.org/abs/2312.02240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02240]] Contrastive Learning-Based Spectral Knowledge Distillation for Multi-Modality and Missing Modality Scenarios in Semantic Segmentation(http://arxiv.org/abs/2312.02240)</code></li>
<li>Summary: <p>Improving the performance of semantic segmentation models using multispectral
information is crucial, especially for environments with low-light and adverse
conditions. Multi-modal fusion techniques pursue either the learning of
cross-modality features to generate a fused image or engage in knowledge
distillation but address multimodal and missing modality scenarios as distinct
issues, which is not an optimal approach for multi-sensor models. To address
this, a novel multi-modal fusion approach called CSK-Net is proposed, which
uses a contrastive learning-based spectral knowledge distillation technique
along with an automatic mixed feature exchange mechanism for semantic
segmentation in optical (EO) and infrared (IR) images. The distillation scheme
extracts detailed textures from the optical images and distills them into the
optical branch of CSK-Net. The model encoder consists of shared convolution
weights with separate batch norm (BN) layers for both modalities, to capture
the multi-spectral information from different modalities of the same objects. A
Novel Gated Spectral Unit (GSU) and mixed feature exchange strategy are
proposed to increase the correlation of modality-shared information and
decrease the modality-specific information during the distillation process.
Comprehensive experiments show that CSK-Net surpasses state-of-the-art models
in multi-modal tasks and for missing modalities when exclusively utilizing IR
data for inference across three public benchmarking datasets. For missing
modality scenarios, the performance increase is achieved without additional
computational costs compared to the baseline segmentation models.
</p></li>
</ul>

<h3>Title: Geometrically-driven Aggregation for Zero-shot 3D Point Cloud Understanding. (arXiv:2312.02244v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02244">http://arxiv.org/abs/2312.02244</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02244]] Geometrically-driven Aggregation for Zero-shot 3D Point Cloud Understanding(http://arxiv.org/abs/2312.02244)</code></li>
<li>Summary: <p>Zero-shot 3D point cloud understanding can be achieved via 2D Vision-Language
Models (VLMs). Existing strategies directly map Vision-Language Models from 2D
pixels of rendered or captured views to 3D points, overlooking the inherent and
expressible point cloud geometric structure. Geometrically similar or close
regions can be exploited for bolstering point cloud understanding as they are
likely to share semantic information. To this end, we introduce the first
training-free aggregation technique that leverages the point cloud's 3D
geometric structure to improve the quality of the transferred Vision-Language
Models. Our approach operates iteratively, performing local-to-global
aggregation based on geometric and semantic point-level reasoning. We benchmark
our approach on three downstream tasks, including classification, part
segmentation, and semantic segmentation, with a variety of datasets
representing both synthetic/real-world, and indoor/outdoor scenarios. Our
approach achieves new state-of-the-art results in all benchmarks. We will
release the source code publicly.
</p></li>
</ul>

<h3>Title: Cable Slack Detection for Arresting Gear Application using Machine Vision. (arXiv:2312.02320v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02320">http://arxiv.org/abs/2312.02320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02320]] Cable Slack Detection for Arresting Gear Application using Machine Vision(http://arxiv.org/abs/2312.02320)</code></li>
<li>Summary: <p>The cable-based arrestment systems are integral to the launch and recovery of
aircraft onboard carriers and on expeditionary land-based installations. These
modern arrestment systems rely on various mechanisms to absorb energy from an
aircraft during an arrestment cycle to bring the aircraft to a full stop. One
of the primary components of this system is the cable interface to the engine.
The formation of slack in the cable at this interface can result in reduced
efficiency and drives maintenance efforts to remove the slack prior to
continued operations. In this paper, a machine vision based slack detection
system is presented. A situational awareness camera is utilized to collect
video data of the cable interface region, machine vision algorithms are applied
to reduce noise, remove background clutter, focus on regions of interest, and
detect changes in the image representative of slack formations. Some algorithms
employed in this system include bilateral image filters, least squares
polynomial fit, Canny Edge Detection, K-Means clustering, Gaussian
Mixture-based Background/Foreground Segmentation for background subtraction,
Hough Circle Transforms, and Hough line Transforms. The resulting detections
are filtered and highlighted to create an indication to the shipboard operator
of the presence of slack and a need for a maintenance action. A user interface
was designed to provide operators with an easy method to redefine regions of
interest and adjust the methods to specific locations. The algorithms were
validated on shipboard footage and were able to accurately identify slack with
minimal false positives.
</p></li>
</ul>

<h3>Title: SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints. (arXiv:2312.02464v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02464">http://arxiv.org/abs/2312.02464</a></li>
<li>Code URL: https://github.com/sstary/ssrs</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02464]] SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints(http://arxiv.org/abs/2312.02464)</code></li>
<li>Summary: <p>Semantic segmentation of remote sensing imagery plays a pivotal role in
extracting precise information for diverse down-stream applications. Recent
development of the Segment Anything Model (SAM), an advanced general-purpose
segmentation model, has revolutionized this field, presenting new avenues for
accurate and efficient segmentation. However, SAM is limited to generating
segmentation results without class information. Consequently, the utilization
of such a powerful general vision model for semantic segmentation in remote
sensing images has become a focal point of research. In this paper, we present
a streamlined framework aimed at leveraging the raw output of SAM by exploiting
two novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary
(SGB). More specifically, we propose a novel object loss and further introduce
a boundary loss as augmentative components to aid in model optimization in a
general semantic segmentation framework. Taking into account the content
characteristics of SGO, we introduce the concept of object consistency to
leverage segmented regions lacking semantic information. By imposing
constraints on the consistency of predicted values within objects, the object
loss aims to enhance semantic segmentation performance. Furthermore, the
boundary loss capitalizes on the distinctive features of SGB by directing the
model's attention to the boundary information of the object. Experimental
results on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban,
demonstrate the effectiveness of our proposed method. The source code for this
work will be accessible at https://github.com/sstary/SSRS.
</p></li>
</ul>

<h3>Title: Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline. (arXiv:2312.02528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02528">http://arxiv.org/abs/2312.02528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02528]] Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline(http://arxiv.org/abs/2312.02528)</code></li>
<li>Summary: <p>We conduct a comprehensive study on a new task named power battery detection
(PBD), which aims to localize the dense cathode and anode plates endpoints from
X-ray images to evaluate the quality of power batteries. Existing manufacturers
usually rely on human eye observation to complete PBD, which makes it difficult
to balance the accuracy and efficiency of detection. To address this issue and
drive more attention into this meaningful task, we first elaborately collect a
dataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from
thousands of power batteries of $5$ manufacturers, with $7$ different visual
interference. Then, we propose a novel segmentation-based solution for PBD,
termed multi-dimensional collaborative network (MDCNet). With the help of line
and counting predictors, the representation of the point segmentation branch
can be improved at both semantic and detail aspects. Besides, we design an
effective distance-adaptive mask generation strategy, which can alleviate the
visual challenge caused by the inconsistent distribution density of plates to
provide MDCNet with stable supervision. Without any bells and whistles, our
segmentation-based MDCNet consistently outperforms various other corner
detection, crowd counting and general/tiny object detection-based solutions,
making it a strong baseline that can help facilitate future research in PBD.
Finally, we share some potential difficulties and works for future researches.
The source code and datasets will be publicly available at
\href{<a href="http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0">this http URL</a>}{X-ray
PBD}.
</p></li>
</ul>

<h3>Title: Graph Information Bottleneck for Remote Sensing Segmentation. (arXiv:2312.02545v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02545">http://arxiv.org/abs/2312.02545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02545]] Graph Information Bottleneck for Remote Sensing Segmentation(http://arxiv.org/abs/2312.02545)</code></li>
<li>Summary: <p>Remote sensing segmentation has a wide range of applications in environmental
protection, and urban change detection, etc. Despite the success of deep
learning-based remote sensing segmentation methods (e.g., CNN and Transformer),
they are not flexible enough to model irregular objects. In addition, existing
graph contrastive learning methods usually adopt the way of maximizing mutual
information to keep the node representations consistent between different graph
views, which may cause the model to learn task-independent redundant
information. To tackle the above problems, this paper treats images as graph
structures and introduces a simple contrastive vision GNN (SC-ViG) architecture
for remote sensing segmentation. Specifically, we construct a node-masked and
edge-masked graph view to obtain an optimal graph structure representation,
which can adaptively learn whether to mask nodes and edges. Furthermore, this
paper innovatively introduces information bottleneck theory into graph
contrastive learning to maximize task-related information while minimizing
task-independent redundant information. Finally, we replace the convolutional
module in UNet with the SC-ViG module to complete the segmentation and
classification tasks of remote sensing images. Extensive experiments on
publicly available real datasets demonstrate that our method outperforms
state-of-the-art remote sensing image segmentation methods.
</p></li>
</ul>

<h3>Title: Panoptica -- instance-wise evaluation of 3D semantic and instance segmentation maps. (arXiv:2312.02608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02608">http://arxiv.org/abs/2312.02608</a></li>
<li>Code URL: https://github.com/brainlesion/panoptica</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02608]] Panoptica -- instance-wise evaluation of 3D semantic and instance segmentation maps(http://arxiv.org/abs/2312.02608)</code></li>
<li>Summary: <p>This paper introduces panoptica, a versatile and performance-optimized
package designed for computing instance-wise segmentation quality metrics from
2D and 3D segmentation maps. panoptica addresses the limitations of existing
metrics and provides a modular framework that complements the original
intersection over union-based panoptic quality with other metrics, such as the
distance metric Average Symmetric Surface Distance. The package is open-source,
implemented in Python, and accompanied by comprehensive documentation and
tutorials. panoptica employs a three-step metrics computation process to cover
diverse use cases. The efficacy of panoptica is demonstrated on various
real-world biomedical datasets, where an instance-wise evaluation is
instrumental for an accurate representation of the underlying clinical task.
Overall, we envision panoptica as a valuable tool facilitating in-depth
evaluation of segmentation methods.
</p></li>
</ul>

<h3>Title: A Unified Simulation Framework for Visual and Behavioral Fidelity in Crowd Analysis. (arXiv:2312.02613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02613">http://arxiv.org/abs/2312.02613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02613]] A Unified Simulation Framework for Visual and Behavioral Fidelity in Crowd Analysis(http://arxiv.org/abs/2312.02613)</code></li>
<li>Summary: <p>Simulation is a powerful tool to easily generate annotated data, and a highly
desirable feature, especially in those domains where learning models need large
training datasets. Machine learning and deep learning solutions, have proven to
be extremely data-hungry and sometimes, the available real-world data are not
sufficient to effectively model the given task. Despite the initial skepticism
of a portion of the scientific community, the potential of simulation has been
largely confirmed in many application areas, and the recent developments in
terms of rendering and virtualization engines, have shown a good ability also
in representing complex scenes. This includes environmental factors, such as
weather conditions and surface reflectance, as well as human-related events,
like human actions and behaviors. We present a human crowd simulator, called
UniCrowd, and its associated validation pipeline. We show how the simulator can
generate annotated data, suitable for computer vision tasks, in particular for
detection and segmentation, as well as the related applications, as crowd
counting, human pose estimation, trajectory analysis and prediction, and
anomaly detection.
</p></li>
</ul>

<h3>Title: Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs. (arXiv:2312.02638v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02638">http://arxiv.org/abs/2312.02638</a></li>
<li>Code URL: https://github.com/fpv-iplab/synchronization-is-all-you-need</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02638]] Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs(http://arxiv.org/abs/2312.02638)</code></li>
<li>Summary: <p>We consider the problem of transferring a temporal action segmentation system
initially designed for exocentric (fixed) cameras to an egocentric scenario,
where wearable cameras capture video data. The conventional supervised approach
requires the collection and labeling of a new set of egocentric videos to adapt
the model, which is costly and time-consuming. Instead, we propose a novel
methodology which performs the adaptation leveraging existing labeled
exocentric videos and a new set of unlabeled, synchronized
exocentric-egocentric video pairs, for which temporal action segmentation
annotations do not need to be collected. We implement the proposed methodology
with an approach based on knowledge distillation, which we investigate both at
the feature and model level. To evaluate our approach, we introduce a new
benchmark based on the Assembly101 dataset. Results demonstrate the feasibility
and effectiveness of the proposed method against classic unsupervised domain
adaptation and temporal sequence alignment approaches. Remarkably, without
bells and whistles, our best model performs on par with supervised approaches
trained on labeled egocentric data, without ever seeing a single egocentric
label, achieving a +15.99% (28.59% vs 12.60%) improvement in the edit score on
the Assembly101 dataset compared to a baseline model trained solely on
exocentric data.
</p></li>
</ul>

<h3>Title: Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark. (arXiv:2312.02672v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02672">http://arxiv.org/abs/2312.02672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02672]] Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark(http://arxiv.org/abs/2312.02672)</code></li>
<li>Summary: <p>In this study, we investigate the effectiveness of synthetic data in
enhancing hand-object interaction detection within the egocentric vision
domain. We introduce a simulator able to generate synthetic images of
hand-object interactions automatically labeled with hand-object contact states,
bounding boxes, and pixel-wise segmentation masks. Through comprehensive
experiments and comparative analyses on three egocentric datasets, VISOR,
EgoHOS, and ENIGMA-51, we demonstrate that the use of synthetic data and domain
adaptation techniques allows for comparable performance to conventional
supervised methods while requiring annotations on only a fraction of the real
data. When tested with in-domain synthetic data generated from 3D models of
real target environments and objects, our best models show consistent
performance improvements with respect to standard fully supervised approaches
based on labeled real data only. Our study also sets a new benchmark of domain
adaptation for egocentric hand-object interaction detection (HOI-Synth) and
provides baseline results to encourage the community to engage in this
challenging task. We release the generated data, code, and the simulator at the
following link: https://iplab.dmi.unict.it/HOI-Synth/.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
