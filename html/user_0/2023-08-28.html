<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Self-supervised learning for hotspot detection and isolation from thermal images. (arXiv:2308.13204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13204">http://arxiv.org/abs/2308.13204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13204]] Self-supervised learning for hotspot detection and isolation from thermal images(http://arxiv.org/abs/2308.13204)</code></li>
<li>Summary: <p>Hotspot detection using thermal imaging has recently become essential in
several industrial applications, such as security applications, health
applications, and equipment monitoring applications. Hotspot detection is of
utmost importance in industrial safety where equipment can develop anomalies.
Hotspots are early indicators of such anomalies. We address the problem of
hotspot detection in thermal images by proposing a self-supervised learning
approach. Self-supervised learning has shown potential as a competitive
alternative to their supervised learning counterparts but their application to
thermography has been limited. This has been due to lack of diverse data
availability, domain specific pre-trained models, standardized benchmarks, etc.
We propose a self-supervised representation learning approach followed by
fine-tuning that improves detection of hotspots by classification. The SimSiam
network based ensemble classifier decides whether an image contains hotspots or
not. Detection of hotspots is followed by precise hotspot isolation. By doing
so, we are able to provide a highly accurate and precise hotspot
identification, applicable to a wide range of applications. We created a novel
large thermal image dataset to address the issue of paucity of easily
accessible thermal images. Our experiments with the dataset created by us and a
publicly available segmentation dataset show the potential of our approach for
hotspot detection and its ability to isolate hotspots with high accuracy. We
achieve a Dice Coefficient of 0.736, the highest when compared with existing
hotspot identification techniques. Our experiments also show self-supervised
learning as a strong contender of supervised learning, providing competitive
metrics for hotspot detection, with the highest accuracy of our approach being
97%.
</p></li>
</ul>

<h3>Title: ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching. (arXiv:2308.13062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13062">http://arxiv.org/abs/2308.13062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13062]] ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching(http://arxiv.org/abs/2308.13062)</code></li>
<li>Summary: <p>Security critical software, e.g., OpenSSL, comes with numerous side-channel
leakages left unpatched due to a lack of resources or experts. The situation
will only worsen as the pace of code development accelerates, with developers
relying on Large Language Models (LLMs) to automatically generate code. In this
work, we explore the use of LLMs in generating patches for vulnerable code with
microarchitectural side-channel leakages. For this, we investigate the
generative abilities of powerful LLMs by carefully crafting prompts following a
zero-shot learning approach. All generated code is dynamically analyzed by
leakage detection tools, which are capable of pinpointing information leakage
at the instruction level leaked either from secret dependent accesses or
branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts
are used to generate candidate replacements for vulnerable code, which are then
analyzed for correctness and for leakage resilience. From a cost/performance
perspective, the GPT4-based configuration costs in API calls a mere few cents
per vulnerability fixed. Our results show that LLM-based patching is far more
cost-effective and thus provides a scalable solution. Finally, the framework we
propose will improve in time, especially as vulnerability detection tools and
LLMs mature.
</p></li>
</ul>

<h3>Title: A Large-Scale Study of IoT Security Weaknesses and Vulnerabilities in the Wild. (arXiv:2308.13141v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13141">http://arxiv.org/abs/2308.13141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13141]] A Large-Scale Study of IoT Security Weaknesses and Vulnerabilities in the Wild(http://arxiv.org/abs/2308.13141)</code></li>
<li>Summary: <p>Internet of Things (IoT) is defined as the connection between places and
physical objects (i.e., things) over the internet/network via smart computing
devices. We observed that IoT software developers share solutions to
programming questions as code examples on three Stack Exchange Q&amp;A sites: Stack
Overflow (SO), Arduino, and Raspberry Pi. Previous research studies found
vulnerabilities/weaknesses in C/C++ code examples shared in Stack Overflow.
However, the studies did not investigate C/C++ code examples related to IoT.
The studies investigated SO code examples only. In this paper, we conduct a
large-scale empirical study of all IoT C/C++ code examples shared in the three
Stack Exchange sites, i.e., SO, Arduino, and Raspberry Pi. From the 11,329
obtained code snippets from the three sites, we identify 29 distinct CWE
(Common Weakness Enumeration) types in 609 snippets. These CWE types can be
categorized into 8 general weakness categories, and we observe that evaluation,
memory, and initialization related weaknesses are the most common to be
introduced by users when posting programming solutions. Furthermore, we find
that 39.58% of the vulnerable code snippets contain instances of CWE types that
can be mapped to real-world occurrences of those CWE types (i.e. CVE
instances). The most number vulnerable IoT code examples was found in Arduino,
followed by SO, and Raspberry Pi. Memory type vulnerabilities are on the rise
in the sites. For example, from the 3595 mapped CVE instances, we find that
28.99% result in Denial of Service (DoS) errors, which is particularly harmful
for network reliant IoT devices such as smart cars. Our study results can guide
various IoT stakeholders to be aware of such vulnerable IoT code examples and
to inform IoT researchers during their development of tools that can help
prevent developers the sharing of such vulnerable code examples in the sites.
[Abridged].
</p></li>
</ul>

<h3>Title: Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation. (arXiv:2308.13269v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13269">http://arxiv.org/abs/2308.13269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13269]] Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation(http://arxiv.org/abs/2308.13269)</code></li>
<li>Summary: <p>As some recent information security legislation endowed users with
unconditional rights to be forgotten by any trained machine learning model,
personalized IoT service providers have to put unlearning functionality into
their consideration. The most straightforward method to unlearn users'
contribution is to retrain the model from the initial state, which is not
realistic in high throughput applications with frequent unlearning requests.
Though some machine unlearning frameworks have been proposed to speed up the
retraining process, they fail to match decentralized learning scenarios. In
this paper, we design a decentralized unlearning framework called HDUS, which
uses distilled seed models to construct erasable ensembles for all clients.
Moreover, the framework is compatible with heterogeneous on-device models,
representing stronger scalability in real-world applications. Extensive
experiments on three real-world datasets show that our HDUS achieves
state-of-the-art performance.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: EgoBlur: Responsible Innovation in Aria. (arXiv:2308.13093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13093">http://arxiv.org/abs/2308.13093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13093]] EgoBlur: Responsible Innovation in Aria(http://arxiv.org/abs/2308.13093)</code></li>
<li>Summary: <p>Project Aria pushes the frontiers of Egocentric AI with large-scale
real-world data collection using purposely designed glasses with privacy first
approach. To protect the privacy of bystanders being recorded by the glasses,
our research protocols are designed to ensure recorded video is processed by an
AI anonymization model that removes bystander faces and vehicle license plates.
Detected face and license plate regions are processed with a Gaussian blur such
that these personal identification information (PII) regions are obscured. This
process helps to ensure that anonymized versions of the video is retained for
research purposes. In Project Aria, we have developed a state-of-the-art
anonymization system EgoBlur. In this paper, we present extensive analysis of
EgoBlur on challenging datasets comparing its performance with other
state-of-the-art systems from industry and academia including extensive
Responsible AI analysis on recently released Casual Conversations V2 dataset.
</p></li>
</ul>

<h3>Title: Black-box Unsupervised Domain Adaptation with Bi-directional Atkinson-Shiffrin Memory. (arXiv:2308.13236v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13236">http://arxiv.org/abs/2308.13236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13236]] Black-box Unsupervised Domain Adaptation with Bi-directional Atkinson-Shiffrin Memory(http://arxiv.org/abs/2308.13236)</code></li>
<li>Summary: <p>Black-box unsupervised domain adaptation (UDA) learns with source predictions
of target data without accessing either source data or source models during
training, and it has clear superiority in data privacy and flexibility in
target network selection. However, the source predictions of target data are
often noisy and training with them is prone to learning collapses. We propose
BiMem, a bi-directional memorization mechanism that learns to remember useful
and representative information to correct noisy pseudo labels on the fly,
leading to robust black-box UDA that can generalize across different visual
recognition tasks. BiMem constructs three types of memory, including sensory
memory, short-term memory, and long-term memory, which interact in a
bi-directional manner for comprehensive and robust memorization of learnt
features. It includes a forward memorization flow that identifies and stores
useful features and a backward calibration flow that rectifies features' pseudo
labels progressively. Extensive experiments show that BiMem achieves superior
domain adaptation performance consistently across various visual recognition
tasks such as image classification, semantic segmentation and object detection.
</p></li>
</ul>

<h3>Title: Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs. (arXiv:2308.13513v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13513">http://arxiv.org/abs/2308.13513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13513]] Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs(http://arxiv.org/abs/2308.13513)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) are powerful tools for learning representations
on graphs, such as social networks. However, their vulnerability to privacy
inference attacks restricts their practicality, especially in high-stake
domains. To address this issue, privacy-preserving GNNs have been proposed,
focusing on preserving node and/or link privacy. This work takes a step back
and investigates how GNNs contribute to privacy leakage. Through theoretical
analysis and simulations, we identify message passing under structural bias as
the core component that allows GNNs to \textit{propagate} and \textit{amplify}
privacy leakage. Building upon these findings, we propose a principled
privacy-preserving GNN framework that effectively safeguards both node and link
privacy, referred to as dual-privacy preservation. The framework comprises
three major modules: a Sensitive Information Obfuscation Module that removes
sensitive information from node embeddings, a Dynamic Structure Debiasing
Module that dynamically corrects the structural bias, and an Adversarial
Learning Module that optimizes the privacy-utility trade-off. Experimental
results on four benchmark datasets validate the effectiveness of the proposed
model in protecting both node and link privacy while preserving high utility
for downstream tasks, such as node classification.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h2>robust</h2>
<h3>Title: EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior. (arXiv:2308.13223v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13223">http://arxiv.org/abs/2308.13223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13223]] EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior(http://arxiv.org/abs/2308.13223)</code></li>
<li>Summary: <p>While the image diffusion model has made significant strides in text-driven
3D content creation, it often falls short in accurately capturing the intended
meaning of the text prompt, particularly with respect to direction information.
This shortcoming gives rise to the Janus problem, where multi-faced 3D models
are produced with the guidance of such diffusion models. In this paper, we
present a robust pipeline for generating high-fidelity 3D content with
orthogonal-view image guidance. Specifically, we introduce a novel 2D diffusion
model that generates an image consisting of four orthogonal-view sub-images for
the given text prompt. The 3D content is then created with this diffusion
model, which enhances 3D consistency and provides strong structured semantic
priors. This addresses the infamous Janus problem and significantly promotes
generation efficiency. Additionally, we employ a progressive 3D synthesis
strategy that results in substantial improvement in the quality of the created
3D contents. Both quantitative and qualitative evaluations show that our method
demonstrates a significant improvement over previous text-to-3D techniques.
</p></li>
</ul>

<h3>Title: ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking. (arXiv:2308.13229v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13229">http://arxiv.org/abs/2308.13229</a></li>
<li>Code URL: https://github.com/chengche6230/rest</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13229]] ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking(http://arxiv.org/abs/2308.13229)</code></li>
<li>Summary: <p>Multi-Camera Multi-Object Tracking (MC-MOT) utilizes information from
multiple views to better handle problems with occlusion and crowded scenes.
Recently, the use of graph-based approaches to solve tracking problems has
become very popular. However, many current graph-based methods do not
effectively utilize information regarding spatial and temporal consistency.
Instead, they rely on single-camera trackers as input, which are prone to
fragmentation and ID switch errors. In this paper, we propose a novel
reconfigurable graph model that first associates all detected objects across
cameras spatially before reconfiguring it into a temporal graph for Temporal
Association. This two-stage association approach enables us to extract robust
spatial and temporal-aware features and address the problem with fragmented
tracklets. Furthermore, our model is designed for online tracking, making it
suitable for real-world applications. Experimental results show that the
proposed graph model is able to extract more discriminating features for object
tracking, and our model achieves state-of-the-art performance on several public
datasets.
</p></li>
</ul>

<h3>Title: Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map. (arXiv:2308.13245v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13245">http://arxiv.org/abs/2308.13245</a></li>
<li>Code URL: https://github.com/naughtyzz/3d_facial_shape_attribute_translation_ssgmap</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13245]] Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map(http://arxiv.org/abs/2308.13245)</code></li>
<li>Summary: <p>While impressive progress has recently been made in image-oriented facial
attribute translation, shape-oriented 3D facial attribute translation remains
an unsolved issue. This is primarily limited by the lack of 3D generative
models and ineffective usage of 3D facial data. We propose a learning framework
for 3D facial attribute translation to relieve these limitations. Firstly, we
customize a novel geometric map for 3D shape representation and embed it in an
end-to-end generative adversarial network. The geometric map represents 3D
shapes symmetrically on a square image grid, while preserving the neighboring
relationship of 3D vertices in a local least-square sense. This enables
effective learning for the latent representation of data with different
attributes. Secondly, we employ a unified and unpaired learning framework for
multi-domain attribute translation. It not only makes effective usage of data
correlation from multiple domains, but also mitigates the constraint for hardly
accessible paired data. Finally, we propose a hierarchical architecture for the
discriminator to guarantee robust results against both global and local
artifacts. We conduct extensive experiments to demonstrate the advantage of the
proposed framework over the state-of-the-art in generating high-fidelity facial
shapes. Given an input 3D facial shape, the proposed framework is able to
synthesize novel shapes of different attributes, which covers some downstream
applications, such as expression transfer, gender translation, and aging. Code
at https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap.
</p></li>
</ul>

<h3>Title: 3D Face Alignment Through Fusion of Head Pose Information and Features. (arXiv:2308.13327v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13327">http://arxiv.org/abs/2308.13327</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13327]] 3D Face Alignment Through Fusion of Head Pose Information and Features(http://arxiv.org/abs/2308.13327)</code></li>
<li>Summary: <p>The ability of humans to infer head poses from face shapes, and vice versa,
indicates a strong correlation between the two. Accordingly, recent studies on
face alignment have employed head pose information to predict facial landmarks
in computer vision tasks. In this study, we propose a novel method that employs
head pose information to improve face alignment performance by fusing said
information with the feature maps of a face alignment network, rather than
simply using it to initialize facial landmarks. Furthermore, the proposed
network structure performs robust face alignment through a dual-dimensional
network using multidimensional features represented by 2D feature maps and a 3D
heatmap. For effective dense face alignment, we also propose a prediction
method for facial geometric landmarks through training based on knowledge
distillation using predicted keypoints. We experimentally assessed the
correlation between the predicted facial landmarks and head pose information,
as well as variations in the accuracy of facial landmarks with respect to the
quality of head pose information. In addition, we demonstrated the
effectiveness of the proposed method through a competitive performance
comparison with state-of-the-art methods on the AFLW2000-3D, AFLW, and BIWI
datasets.
</p></li>
</ul>

<h3>Title: Exploiting Diverse Feature for Multimodal Sentiment Analysis. (arXiv:2308.13421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13421">http://arxiv.org/abs/2308.13421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13421]] Exploiting Diverse Feature for Multimodal Sentiment Analysis(http://arxiv.org/abs/2308.13421)</code></li>
<li>Summary: <p>In this paper, we present our solution to the MuSe-Personalisation
sub-challenge in the MuSe 2023 Multimodal Sentiment Analysis Challenge. The
task of MuSe-Personalisation aims to predict the continuous arousal and valence
values of a participant based on their audio-visual, language, and
physiological signal modalities data. Considering different people have
personal characteristics, the main challenge of this task is how to build
robustness feature presentation for sentiment prediction. To address this
issue, we propose exploiting diverse features. Specifically, we proposed a
series of feature extraction methods to build a robust representation and model
ensemble. We empirically evaluate the performance of the utilized method on the
officially provided dataset. \textbf{As a result, we achieved 3rd place in the
MuSe-Personalisation sub-challenge.} Specifically, we achieve the results of
0.8492 and 0.8439 for MuSe-Personalisation in terms of arousal and valence CCC.
</p></li>
</ul>

<h3>Title: Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons. (arXiv:2308.13198v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13198">http://arxiv.org/abs/2308.13198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13198]] Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons(http://arxiv.org/abs/2308.13198)</code></li>
<li>Summary: <p>Pre-trained language models (PLMs) contain vast amounts of factual knowledge,
but how the knowledge is stored in the parameters remains unclear. This paper
delves into the complex task of understanding how factual knowledge is stored
in multilingual PLMs, and introduces the Architecture-adapted Multilingual
Integrated Gradients method, which successfully localizes knowledge neurons
more precisely compared to current methods, and is more universal across
various architectures and languages. Moreover, we conduct an in-depth
exploration of knowledge neurons, leading to the following two important
discoveries: (1) The discovery of Language-Independent Knowledge Neurons, which
store factual knowledge in a form that transcends language. We design
cross-lingual knowledge editing experiments, demonstrating that the PLMs can
accomplish this task based on language-independent neurons; (2) The discovery
of Degenerate Knowledge Neurons, a novel type of neuron showing that different
knowledge neurons can store the same fact. Its property of functional overlap
endows the PLMs with a robust mastery of factual knowledge. We design
fact-checking experiments, proving that the degenerate knowledge neurons can
help the PLMs to detect wrong facts. Experiments corroborate these findings,
shedding light on the mechanisms of factual knowledge storage in multilingual
PLMs, and contribute valuable insights to the field. The source code will be
made publicly available for further research.
</p></li>
</ul>

<h3>Title: Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13259">http://arxiv.org/abs/2308.13259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13259]] Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering(http://arxiv.org/abs/2308.13259)</code></li>
<li>Summary: <p>Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown
impressive reasoning ability in various downstream tasks. Even so, suffering
from hallucinations and the inability to access external knowledge, LLMs often
come with incorrect or unfaithful intermediate reasoning steps, especially in
the context of answering knowledge-intensive tasks such as KBQA. To alleviate
this issue, we propose a framework called Knowledge-Driven Chain-of-Thought
(KD-CoT) to verify and modify reasoning traces in CoT via interaction with
external knowledge, and thus overcome the hallucinations and error propagation.
Concretely, we formulate the CoT rationale process of LLMs into a structured
multi-round QA format. In each round, LLMs interact with a QA system that
retrieves external knowledge and produce faithful reasoning traces based on
retrieved precise answers. The structured CoT reasoning of LLMs is facilitated
by our developed KBQA CoT collection, which serves as in-context learning
demonstrations and can also be utilized as feedback augmentation to train a
robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion
datasets demonstrate the effectiveness of proposed KD-CoT in task-solving
reasoning generation, which outperforms the vanilla CoT ICL with an absolute
success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented
retriever outperforms the state-of-the-art baselines for retrieving knowledge,
achieving significant improvement in Hit performance.
</p></li>
</ul>

<h3>Title: Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13068">http://arxiv.org/abs/2308.13068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13068]] Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology(http://arxiv.org/abs/2308.13068)</code></li>
<li>Summary: <p>Multivariate Time Series (MVTS) anomaly detection is a long-standing and
challenging research topic that has attracted tremendous research effort from
both industry and academia recently. However, a careful study of the literature
makes us realize that 1) the community is active but not as organized as other
sibling machine learning communities such as Computer Vision (CV) and Natural
Language Processing (NLP), and 2) most proposed solutions are evaluated using
either inappropriate or highly flawed protocols, with an apparent lack of
scientific foundation. So flawed is one very popular protocol, the so-called
\pa protocol, that a random guess can be shown to systematically outperform
\emph{all} algorithms developed so far. In this paper, we review and evaluate
many recent algorithms using more robust protocols and discuss how a normally
good protocol may have weaknesses in the context of MVTS anomaly detection and
how to mitigate them. We also share our concerns about benchmark datasets,
experiment design and evaluation methodology we observe in many works.
Furthermore, we propose a simple, yet challenging, baseline algorithm based on
Principal Components Analysis (PCA) that surprisingly outperforms many recent
Deep Learning (DL) based approaches on popular benchmark datasets. The main
objective of this work is to stimulate more effort towards important aspects of
the research such as data, experiment design, evaluation methodology and result
interpretability, instead of putting the highest weight on the design of
increasingly more complex and "fancier" algorithms.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: TriGait: Aligning and Fusing Skeleton and Silhouette Gait Data via a Tri-Branch Network. (arXiv:2308.13340v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13340">http://arxiv.org/abs/2308.13340</a></li>
<li>Code URL: https://github.com/feng-xueling/trigait</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13340]] TriGait: Aligning and Fusing Skeleton and Silhouette Gait Data via a Tri-Branch Network(http://arxiv.org/abs/2308.13340)</code></li>
<li>Summary: <p>Gait recognition is a promising biometric technology for identification due
to its non-invasiveness and long-distance. However, external variations such as
clothing changes and viewpoint differences pose significant challenges to gait
recognition. Silhouette-based methods preserve body shape but neglect internal
structure information, while skeleton-based methods preserve structure
information but omit appearance. To fully exploit the complementary nature of
the two modalities, a novel triple branch gait recognition framework, TriGait,
is proposed in this paper. It effectively integrates features from the skeleton
and silhouette data in a hybrid fusion manner, including a two-stream network
to extract static and motion features from appearance, a simple yet effective
module named JSA-TC to capture dependencies between all joints, and a third
branch for cross-modal learning by aligning and fusing low-level features of
two modalities. Experimental results demonstrate the superiority and
effectiveness of TriGait for gait recognition. The proposed method achieves a
mean rank-1 accuracy of 96.0% over all conditions on CASIA-B dataset and 94.3%
accuracy for CL, significantly outperforming all the state-of-the-art methods.
The source code will be available at https://github.com/feng-xueling/TriGait/.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Burnt area extraction from high-resolution satellite images based on anomaly detection. (arXiv:2308.13367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13367">http://arxiv.org/abs/2308.13367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13367]] Burnt area extraction from high-resolution satellite images based on anomaly detection(http://arxiv.org/abs/2308.13367)</code></li>
<li>Summary: <p>Wildfire detection using satellite images is a widely studied task in remote
sensing with many applications to fire delineation and mapping. Recently, deep
learning methods have become a scalable solution to automate this task,
especially in the field of unsupervised learning where no training data is
available. This is particularly important in the context of emergency risk
monitoring where fast and effective detection is needed, generally based on
high-resolution satellite data. Among various approaches, Anomaly Detection
(AD) appears to be highly potential thanks to its broad applications in
computer vision, medical imaging, as well as remote sensing. In this work, we
build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE),
a popular reconstruction-based AD method with discrete latent spaces, to
perform unsupervised burnt area extraction. We integrate VQ-VAE into an
end-to-end framework with an intensive post-processing step using dedicated
vegetation, water and brightness indexes. Our experiments conducted on
high-resolution SPOT-6/7 images provide promising results of the proposed
technique, showing its high potential in future research on unsupervised burnt
area extraction.
</p></li>
</ul>

<h3>Title: EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13399">http://arxiv.org/abs/2308.13399</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13399]] EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression(http://arxiv.org/abs/2308.13399)</code></li>
<li>Summary: <p>We propose an unsupervised method to extract keywords and keyphrases from
texts based on a pre-trained language model (LM) and Shannon's information
maximization. Specifically, our method extracts phrases having the highest
conditional entropy under the LM. The resulting set of keyphrases turns out to
solve a relevant information-theoretic problem: if provided as side
information, it leads to the expected minimal binary code length in compressing
the text using the LM and an entropy encoder. Alternately, the resulting set is
an approximation via a causal LM to the set of phrases that minimize the
entropy of the text when conditioned upon it. Empirically, the method provides
results comparable to the most commonly used methods in various keyphrase
extraction benchmark challenges.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13047">http://arxiv.org/abs/2308.13047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13047]] Federated Learning of Causal Effects from Incomplete Observational Data(http://arxiv.org/abs/2308.13047)</code></li>
<li>Summary: <p>Decentralized and incomplete data sources are prevalent in real-world
applications, posing a formidable challenge for causal inference. These sources
cannot be consolidated into a single entity owing to privacy constraints, and
the presence of missing values within them can potentially introduce bias to
the causal estimands. We introduce a new approach for federated causal
inference from incomplete data, enabling the estimation of causal effects from
multiple decentralized and incomplete data sources. Our approach disentangles
the loss function into multiple components, each corresponding to a specific
data source with missing values. Our approach accounts for the missing data
under the missing at random assumption, while also estimating higher-order
statistics of the causal estimands. Our method recovers the conditional
distribution of missing confounders given the observed confounders from the
decentralized data sources to identify causal effects. Our framework estimates
heterogeneous causal effects without the sharing of raw training data among
sources, which helps to mitigate privacy risks. The efficacy of our approach is
demonstrated through a collection of simulated and real-world instances,
illustrating its potential and practicality.
</p></li>
</ul>

<h3>Title: Federated Learning in IoT: a Survey from a Resource-Constrained Perspective. (arXiv:2308.13157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13157">http://arxiv.org/abs/2308.13157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13157]] Federated Learning in IoT: a Survey from a Resource-Constrained Perspective(http://arxiv.org/abs/2308.13157)</code></li>
<li>Summary: <p>The IoT ecosystem is able to leverage vast amounts of data for intelligent
decision-making. Federated Learning (FL), a decentralized machine learning
technique, is widely used to collect and train machine learning models from a
variety of distributed data sources. Both IoT and FL systems can be
complementary and used together. However, the resource-constrained nature of
IoT devices prevents the widescale deployment FL in the real world. This
research paper presents a comprehensive survey of the challenges and solutions
associated with implementing Federated Learning (FL) in resource-constrained
Internet of Things (IoT) environments, viewed from 2 levels, client and server.
We focus on solutions regarding limited client resources, presence of
heterogeneous client data, server capacity, and high communication costs, and
assess their effectiveness in various scenarios. Furthermore, we categorize the
solutions based on the location of their application, i.e., the IoT client, and
the FL server. In addition to a comprehensive review of existing research and
potential future directions, this paper also presents new evaluation metrics
that would allow researchers to evaluate their solutions on
resource-constrained IoT devices.
</p></li>
</ul>

<h3>Title: DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT. (arXiv:2308.13158v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13158">http://arxiv.org/abs/2308.13158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13158]] DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT(http://arxiv.org/abs/2308.13158)</code></li>
<li>Summary: <p>Federated learning (FL) aims to collaboratively train a global model while
ensuring client data privacy. However, FL faces challenges from the non-IID
data distribution among clients. Clustered FL (CFL) has emerged as a promising
solution, but most existing CFL frameworks adopt synchronous frameworks lacking
asynchrony. An asynchronous CFL framework called SDAGFL based on directed
acyclic graph distributed ledger techniques (DAG-DLT) was proposed, but its
complete decentralization leads to high communication and storage costs. We
propose DAG-ACFL, an asynchronous clustered FL framework based on directed
acyclic graph distributed ledger techniques (DAG-DLT). We first detail the
components of DAG-ACFL. A tip selection algorithm based on the cosine
similarity of model parameters is then designed to aggregate models from
clients with similar distributions. An adaptive tip selection algorithm
leveraging change-point detection dynamically determines the number of selected
tips. We evaluate the clustering and training performance of DAG-ACFL on
multiple datasets and analyze its communication and storage costs. Experiments
show the superiority of DAG-ACFL in asynchronous clustered FL. By combining
DAG-DLT with clustered FL, DAG-ACFL realizes robust, decentralized and private
model training with efficient performance.
</p></li>
</ul>

<h3>Title: Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13265">http://arxiv.org/abs/2308.13265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13265]] Heterogeneous Federated Learning via Personalized Generative Networks(http://arxiv.org/abs/2308.13265)</code></li>
<li>Summary: <p>Federated Learning (FL) allows several clients to construct a common global
machine-learning model without having to share their data. FL, however, faces
the challenge of statistical heterogeneity between the client's data, which
degrades performance and slows down the convergence toward the global model. In
this paper, we provide theoretical proof that minimizing heterogeneity between
clients facilitates the convergence of a global model for every single client.
This becomes particularly important under empirical concept shifts among
clients, rather than merely considering imbalanced classes, which have been
studied until now. Therefore, we propose a method for knowledge transfer
between clients where the server trains client-specific generators. Each
generator generates samples for the corresponding client to remove the conflict
with other clients' models. Experiments conducted on synthetic and real data,
along with a theoretical study, support the effectiveness of our method in
constructing a well-generalizable global model by reducing the conflict between
local models.
</p></li>
</ul>

<h3>Title: Federated Linear Bandit Learning via Over-the-Air Computation. (arXiv:2308.13298v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13298">http://arxiv.org/abs/2308.13298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13298]] Federated Linear Bandit Learning via Over-the-Air Computation(http://arxiv.org/abs/2308.13298)</code></li>
<li>Summary: <p>In this paper, we investigate federated contextual linear bandit learning
within a wireless system that comprises a server and multiple devices. Each
device interacts with the environment, selects an action based on the received
reward, and sends model updates to the server. The primary objective is to
minimize cumulative regret across all devices within a finite time horizon. To
reduce the communication overhead, devices communicate with the server via
over-the-air computation (AirComp) over noisy fading channels, where the
channel noise may distort the signals. In this context, we propose a customized
federated linear bandits scheme, where each device transmits an analog signal,
and the server receives a superposition of these signals distorted by channel
noise. A rigorous mathematical analysis is conducted to determine the regret
bound of the proposed scheme. Both theoretical analysis and numerical
experiments demonstrate the competitive performance of our proposed scheme in
terms of regret bounds in various settings.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning. (arXiv:2308.13411v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13411">http://arxiv.org/abs/2308.13411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13411]] Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning(http://arxiv.org/abs/2308.13411)</code></li>
<li>Summary: <p>Glaucoma is the number one cause of irreversible blindness globally. A major
challenge for accurate glaucoma detection and progression forecasting is the
bottleneck of limited labeled patients with the state-of-the-art (SOTA) 3D
retinal imaging data of optical coherence tomography (OCT). To address the data
scarcity issue, this paper proposes two solutions. First, we develop a novel
generalization-reinforced semi-supervised learning (SSL) model called pseudo
supervisor to optimally utilize unlabeled data. Compared with SOTA models, the
proposed pseudo supervisor optimizes the policy of predicting pseudo labels
with unlabeled samples to improve empirical generalization. Our pseudo
supervisor model is evaluated with two clinical tasks consisting of glaucoma
detection and progression forecasting. The progression forecasting task is
evaluated both unimodally and multimodally. Our pseudo supervisor model
demonstrates superior performance than SOTA SSL comparison models. Moreover,
our model also achieves the best results on the publicly available LAG fundus
dataset. Second, we introduce the Harvard Glaucoma Detection and Progression
(Harvard-GDP) Dataset, a multimodal multitask dataset that includes data from
1,000 patients with OCT imaging data, as well as labels for glaucoma detection
and progression. This is the largest glaucoma detection dataset with 3D OCT
imaging data and the first glaucoma progression forecasting dataset that is
publicly available. Detailed sex and racial analysis are provided, which can be
used by interested researchers for fairness learning studies. Our released
dataset is benchmarked with several SOTA supervised CNN and transformer deep
learning models. The dataset and code are made publicly available via
\url{https://ophai.hms.harvard.edu/datasets/harvard-gdp1000}.
</p></li>
</ul>

<h3>Title: Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness. (arXiv:2308.13242v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13242">http://arxiv.org/abs/2308.13242</a></li>
<li>Code URL: https://github.com/sruthigorantla/group-fair-pl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13242]] Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness(http://arxiv.org/abs/2308.13242)</code></li>
<li>Summary: <p>In learning-to-rank (LTR), optimizing only the relevance (or the expected
ranking utility) can cause representational harm to certain categories of
items. Moreover, if there is implicit bias in the relevance scores, LTR models
may fail to optimize for true relevance. Previous works have proposed efficient
algorithms to train stochastic ranking models that achieve fairness of exposure
to the groups ex-ante (or, in expectation), which may not guarantee
representation fairness to the groups ex-post, that is, after realizing a
ranking from the stochastic ranking model. Typically, ex-post fairness is
achieved by post-processing, but previous work does not train stochastic
ranking models that are aware of this post-processing.
</p>
<p>In this paper, we propose a novel objective that maximizes expected relevance
only over those rankings that satisfy given representation constraints to
ensure ex-post fairness. Building upon recent work on an efficient sampler for
ex-post group-fair rankings, we propose a group-fair Plackett-Luce model and
show that it can be efficiently optimized for our objective in the LTR
framework.
</p>
<p>Experiments on three real-world datasets show that our group-fair algorithm
guarantees fairness alongside usually having better relevance compared to the
LTR baselines. In addition, our algorithm also achieves better relevance than
post-processing baselines, which also ensures ex-post fairness. Further, when
implicit bias is injected into the training data, our algorithm typically
outperforms existing LTR baselines in relevance.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13453">http://arxiv.org/abs/2308.13453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13453]] Learning to Intervene on Concept Bottlenecks(http://arxiv.org/abs/2308.13453)</code></li>
<li>Summary: <p>While traditional deep learning models often lack interpretability, concept
bottleneck models (CBMs) provide inherent explanations via their concept
representations. Specifically, they allow users to perform interventional
interactions on these concepts by updating the concept values and thus
correcting the predictive output of the model. Traditionally, however, these
interventions are applied to the model only once and discarded afterward. To
rectify this, we present concept bottleneck memory models (CB2M), an extension
to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate
novel situations via a two-fold memory with which it can learn to detect
mistakes and to reapply previous interventions. In this way, a CB2M learns to
automatically improve model performance from a few initially obtained
interventions. If no prior human interventions are available, a CB2M can detect
potential mistakes of the CBM bottleneck and request targeted interventions. In
our experimental evaluations on challenging scenarios like handling
distribution shifts and confounded training data, we illustrate that CB2M are
able to successfully generalize interventions to unseen data and can indeed
identify wrongly inferred concepts. Overall, our results show that CB2M is a
great tool for users to provide interactive feedback on CBMs, e.g., by guiding
a user's interaction and requiring fewer interventions.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records. (arXiv:2308.13104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13104">http://arxiv.org/abs/2308.13104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13104]] Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records(http://arxiv.org/abs/2308.13104)</code></li>
<li>Summary: <p>Survival analysis plays a crucial role in many healthcare decisions, where
the risk prediction for the events of interest can support an informative
outlook for a patient's medical journey. Given the existence of data censoring,
an effective way of survival analysis is to enforce the pairwise temporal
concordance between censored and observed data, aiming to utilize the time
interval before censoring as partially observed time-to-event labels for
supervised learning. Although existing studies mostly employed ranking methods
to pursue an ordering objective, contrastive methods which learn a
discriminative embedding by having data contrast against each other, have not
been explored thoroughly for survival analysis. Therefore, in this paper, we
propose a novel Ontology-aware Temporality-based Contrastive Survival (OTCSurv)
analysis framework that utilizes survival durations from both censored and
observed data to define temporal distinctiveness and construct negative sample
pairs with adjustable hardness for contrastive learning. Specifically, we first
use an ontological encoder and a sequential self-attention encoder to represent
the longitudinal EHR data with rich contexts. Second, we design a temporal
contrastive loss to capture varying survival durations in a supervised setting
through a hardness-aware negative sampling mechanism. Last, we incorporate the
contrastive task into the time-to-event predictive task with multiple loss
components. We conduct extensive experiments using a large EHR dataset to
forecast the risk of hospitalized patients who are in danger of developing
acute kidney injury (AKI), a critical and urgent medical condition. The
effectiveness and explainability of the proposed model are validated through
comprehensive quantitative and qualitative studies.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions. (arXiv:2308.13142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13142">http://arxiv.org/abs/2308.13142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13142]] A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions(http://arxiv.org/abs/2308.13142)</code></li>
<li>Summary: <p>Recently, there has been significant progress in the development of large
models. Following the success of ChatGPT, numerous language models have been
introduced, demonstrating remarkable performance. Similar advancements have
also been observed in image generation models, such as Google's Imagen model,
OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive
capabilities in generating images. However, similar to large language models,
these models still encounter unresolved challenges. Fortunately, the
availability of open-source stable diffusion models and their underlying
mathematical principles has enabled the academic community to extensively
analyze the performance of current image generation models and make
improvements based on this stable diffusion framework. This survey aims to
examine the existing issues and the current solutions pertaining to image
generation models.
</p></li>
</ul>

<h3>Title: Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model. (arXiv:2308.13164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13164">http://arxiv.org/abs/2308.13164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13164]] Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model(http://arxiv.org/abs/2308.13164)</code></li>
<li>Summary: <p>In this paper, we rethink the low-light image enhancement task and propose a
physically explainable and generative diffusion model for low-light image
enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the
physical model and the generative network. Furthermore, we hope to supplement
and even deduce the information missing in the low-light image through the
generative network. Therefore, Diff-Retinex formulates the low-light image
enhancement problem into Retinex decomposition and conditional image
generation. In the Retinex decomposition, we integrate the superiority of
attention in Transformer and meticulously design a Retinex Transformer
decomposition network (TDN) to decompose the image into illumination and
reflectance maps. Then, we design multi-path generative diffusion networks to
reconstruct the normal-light Retinex probability distribution and solve the
various degradations in these components respectively, including dark
illumination, noise, color deviation, loss of scene contents, etc. Owing to
generative diffusion model, Diff-Retinex puts the restoration of low-light
subtle detail into practice. Extensive experiments conducted on real-world
low-light datasets qualitatively and quantitatively demonstrate the
effectiveness, superiority, and generalization of the proposed method.
</p></li>
</ul>

<h3>Title: Distribution-Aligned Diffusion for Human Mesh Recovery. (arXiv:2308.13369v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13369">http://arxiv.org/abs/2308.13369</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13369]] Distribution-Aligned Diffusion for Human Mesh Recovery(http://arxiv.org/abs/2308.13369)</code></li>
<li>Summary: <p>Recovering a 3D human mesh from a single RGB image is a challenging task due
to depth ambiguity and self-occlusion, resulting in a high degree of
uncertainty. Meanwhile, diffusion models have recently seen much success in
generating high-quality outputs by progressively denoising noisy inputs.
Inspired by their capability, we explore a diffusion-based approach for human
mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which
frames mesh recovery as a reverse diffusion process. We also propose a
Distribution Alignment Technique (DAT) that injects input-specific distribution
information into the diffusion process, and provides useful prior knowledge to
simplify the mesh recovery task. Our method achieves state-of-the-art
performance on three widely used datasets. Project page:
https://gongjia0208.github.io/HMDiff/.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Spherical Vision Transformer for 360-degree Video Saliency Prediction. (arXiv:2308.13004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13004">http://arxiv.org/abs/2308.13004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13004]] Spherical Vision Transformer for 360-degree Video Saliency Prediction(http://arxiv.org/abs/2308.13004)</code></li>
<li>Summary: <p>The growing interest in omnidirectional videos (ODVs) that capture the full
field-of-view (FOV) has gained 360-degree saliency prediction importance in
computer vision. However, predicting where humans look in 360-degree scenes
presents unique challenges, including spherical distortion, high resolution,
and limited labelled data. We propose a novel vision-transformer-based model
for omnidirectional videos named SalViT360 that leverages tangent image
representations. We introduce a spherical geometry-aware spatiotemporal
self-attention mechanism that is capable of effective omnidirectional video
understanding. Furthermore, we present a consistency-based unsupervised
regularization term for projection-based 360-degree dense-prediction models to
reduce artefacts in the predictions that occur after inverse projection. Our
approach is the first to employ tangent images for omnidirectional saliency
prediction, and our experimental results on three ODV saliency datasets
demonstrate its effectiveness compared to the state-of-the-art.
</p></li>
</ul>

<h3>Title: GEMTrans: A General, Echocardiography-based, Multi-Level Transformer Framework for Cardiovascular Diagnosis. (arXiv:2308.13217v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13217">http://arxiv.org/abs/2308.13217</a></li>
<li>Code URL: https://github.com/dsl-lab/gemtrans</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13217]] GEMTrans: A General, Echocardiography-based, Multi-Level Transformer Framework for Cardiovascular Diagnosis(http://arxiv.org/abs/2308.13217)</code></li>
<li>Summary: <p>Echocardiography (echo) is an ultrasound imaging modality that is widely used
for various cardiovascular diagnosis tasks. Due to inter-observer variability
in echo-based diagnosis, which arises from the variability in echo image
acquisition and the interpretation of echo images based on clinical experience,
vision-based machine learning (ML) methods have gained popularity to act as
secondary layers of verification. For such safety-critical applications, it is
essential for any proposed ML method to present a level of explainability along
with good accuracy. In addition, such methods must be able to process several
echo videos obtained from various heart views and the interactions among them
to properly produce predictions for a variety of cardiovascular measurements or
interpretation tasks. Prior work lacks explainability or is limited in scope by
focusing on a single cardiovascular task. To remedy this, we propose a General,
Echo-based, Multi-Level Transformer (GEMTrans) framework that provides
explainability, while simultaneously enabling multi-video training where the
inter-play among echo image patches in the same frame, all frames in the same
video, and inter-video relationships are captured based on a downstream task.
We show the flexibility of our framework by considering two critical tasks
including ejection fraction (EF) and aortic stenosis (AS) severity detection.
Our model achieves mean absolute errors of 4.15 and 4.84 for single and
dual-video EF estimation and an accuracy of 96.5 % for AS detection, while
providing informative task-specific attention maps and prototypical
explainability.
</p></li>
</ul>

<h3>Title: Bridging the Gap: Fine-to-Coarse Sketch Interpolation Network for High-Quality Animation Sketch Inbetweening. (arXiv:2308.13273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13273">http://arxiv.org/abs/2308.13273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13273]] Bridging the Gap: Fine-to-Coarse Sketch Interpolation Network for High-Quality Animation Sketch Inbetweening(http://arxiv.org/abs/2308.13273)</code></li>
<li>Summary: <p>The 2D animation workflow is typically initiated with the creation of
keyframes using sketch-based drawing. Subsequent inbetweens (i.e., intermediate
sketch frames) are crafted through manual interpolation for smooth animations,
which is a labor-intensive process. Thus, the prospect of automatic animation
sketch interpolation has become highly appealing. However, existing video
interpolation methods are generally hindered by two key issues for sketch
inbetweening: 1) limited texture and colour details in sketches, and 2)
exaggerated alterations between two sketch keyframes. To overcome these issues,
we propose a novel deep learning method, namely Fine-to-Coarse Sketch
Interpolation Network (FC-SIN). This approach incorporates multi-level guidance
that formulates region-level correspondence, sketch-level correspondence and
pixel-level dynamics. A multi-stream U-Transformer is then devised to
characterize sketch inbewteening patterns using these multi-level guides
through the integration of both self-attention and cross-attention mechanisms.
Additionally, to facilitate future research on animation sketch inbetweening,
we constructed a large-scale dataset - STD-12K, comprising 30 sketch animation
series in diverse artistic styles. Comprehensive experiments on this dataset
convincingly show that our proposed FC-SIN surpasses the state-of-the-art
interpolation methods. Our code and dataset will be publicly available.
</p></li>
</ul>

<h3>Title: ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis. (arXiv:2308.13324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13324">http://arxiv.org/abs/2308.13324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13324]] ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis(http://arxiv.org/abs/2308.13324)</code></li>
<li>Summary: <p>Whole slide image (WSI) analysis has become increasingly important in the
medical imaging community, enabling automated and objective diagnosis,
prognosis, and therapeutic-response prediction. However, in clinical practice,
the ever-evolving environment hamper the utility of WSI analysis models. In
this paper, we propose the FIRST continual learning framework for WSI analysis,
named ConSlide, to tackle the challenges of enormous image size, utilization of
hierarchical structure, and catastrophic forgetting by progressive model
updating on multiple sequential datasets. Our framework contains three key
components. The Hierarchical Interaction Transformer (HIT) is proposed to model
and utilize the hierarchical structural knowledge of WSI. The
Breakup-Reorganize (BuRo) rehearsal method is developed for WSI data replay
with efficient region storing buffer and WSI reorganizing operation. The
asynchronous updating mechanism is devised to encourage the network to learn
generic and specific knowledge respectively during the replay stage, based on a
nested cross-scale similarity learning (CSSL) module. We evaluated the proposed
ConSlide on four public WSI datasets from TCGA projects. It performs best over
other state-of-the-art methods with a fair WSI-based continual learning setting
and achieves a better trade-off of the overall performance and forgetting on
previous task
</p></li>
</ul>

<h3>Title: A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation. (arXiv:2308.13331v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13331">http://arxiv.org/abs/2308.13331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13331]] A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation(http://arxiv.org/abs/2308.13331)</code></li>
<li>Summary: <p>The task of semantic segmentation requires a model to assign semantic labels
to each pixel of an image. However, the performance of such models degrades
when deployed in an unseen domain with different data distributions compared to
the training domain. We present a new augmentation-driven approach to domain
generalization for semantic segmentation using a re-parameterized vision
transformer (ReVT) with weight averaging of multiple models after training. We
evaluate our approach on several benchmark datasets and achieve
state-of-the-art mIoU performance of 47.3% (prior art: 46.3%) for small models
and of 50.1% (prior art: 47.8%) for midsized models on commonly used benchmark
datasets. At the same time, our method requires fewer parameters and reaches a
higher frame rate than the best prior art. It is also easy to implement and,
unlike network ensembles, does not add any computational complexity during
inference.
</p></li>
</ul>

<h3>Title: CS-Mixer: A Cross-Scale Vision MLP Model with Spatial-Channel Mixing. (arXiv:2308.13363v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13363">http://arxiv.org/abs/2308.13363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13363]] CS-Mixer: A Cross-Scale Vision MLP Model with Spatial-Channel Mixing(http://arxiv.org/abs/2308.13363)</code></li>
<li>Summary: <p>Despite their simpler information fusion designs compared with Vision
Transformers and Convolutional Neural Networks, Vision MLP architectures have
demonstrated strong performance and high data efficiency in recent research.
However, existing works such as CycleMLP and Vision Permutator typically model
spatial information in equal-size spatial regions and do not consider
cross-scale spatial interactions. Further, their token mixers only model 1- or
2-axis correlations, avoiding 3-axis spatial-channel mixing due to its
computational demands. We therefore propose CS-Mixer, a hierarchical Vision MLP
that learns dynamic low-rank transformations for spatial-channel mixing through
cross-scale local and global aggregation. The proposed methodology achieves
competitive results on popular image recognition benchmarks without incurring
substantially more compute. Our largest model, CS-Mixer-L, reaches 83.2% top-1
accuracy on ImageNet-1k with 13.7 GFLOPs and 94 M parameters.
</p></li>
</ul>

<h3>Title: Prompting Visual-Language Models for Dynamic Facial Expression Recognition. (arXiv:2308.13382v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13382">http://arxiv.org/abs/2308.13382</a></li>
<li>Code URL: https://github.com/zengqunzhao/dfer-clip</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13382]] Prompting Visual-Language Models for Dynamic Facial Expression Recognition(http://arxiv.org/abs/2308.13382)</code></li>
<li>Summary: <p>This paper presents a novel visual-language model called DFER-CLIP, which is
based on the CLIP model and designed for in-the-wild Dynamic Facial Expression
Recognition (DFER). Specifically, the proposed DFER-CLIP consists of a visual
part and a textual part. For the visual part, based on the CLIP image encoder,
a temporal model consisting of several Transformer encoders is introduced for
extracting temporal facial expression features, and the final feature embedding
is obtained as a learnable "class" token. For the textual part, we use as
inputs textual descriptions of the facial behaviour that is related to the
classes (facial expressions) that we are interested in recognising -- those
descriptions are generated using large language models, like ChatGPT. This, in
contrast to works that use only the class names and more accurately captures
the relationship between them. Alongside the textual description, we introduce
a learnable token which helps the model learn relevant context information for
each expression during training. Extensive experiments demonstrate the
effectiveness of the proposed method and show that our DFER-CLIP also achieves
state-of-the-art results compared with the current supervised DFER methods on
the DFEW, FERV39k, and MAFW benchmarks. Code is publicly available at
https://github.com/zengqunzhao/DFER-CLIP.
</p></li>
</ul>

<h3>Title: Nougat: Neural Optical Understanding for Academic Documents. (arXiv:2308.13418v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13418">http://arxiv.org/abs/2308.13418</a></li>
<li>Code URL: https://github.com/facebookresearch/nougat</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13418]] Nougat: Neural Optical Understanding for Academic Documents(http://arxiv.org/abs/2308.13418)</code></li>
<li>Summary: <p>Scientific knowledge is predominantly stored in books and scientific
journals, often in the form of PDFs. However, the PDF format leads to a loss of
semantic information, particularly for mathematical expressions. We propose
Nougat (Neural Optical Understanding for Academic Documents), a Visual
Transformer model that performs an Optical Character Recognition (OCR) task for
processing scientific documents into a markup language, and demonstrate the
effectiveness of our model on a new dataset of scientific documents. The
proposed approach offers a promising solution to enhance the accessibility of
scientific knowledge in the digital age, by bridging the gap between
human-readable documents and machine-readable text. We release the models and
code to accelerate future work on scientific text recognition.
</p></li>
</ul>

<h3>Title: Unlocking Fine-Grained Details with Wavelet-based High-Frequency Enhancement in Transformers. (arXiv:2308.13442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13442">http://arxiv.org/abs/2308.13442</a></li>
<li>Code URL: https://github.com/mindflow-institue/waveformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13442]] Unlocking Fine-Grained Details with Wavelet-based High-Frequency Enhancement in Transformers(http://arxiv.org/abs/2308.13442)</code></li>
<li>Summary: <p>Medical image segmentation is a critical task that plays a vital role in
diagnosis, treatment planning, and disease monitoring. Accurate segmentation of
anatomical structures and abnormalities from medical images can aid in the
early detection and treatment of various diseases. In this paper, we address
the local feature deficiency of the Transformer model by carefully re-designing
the self-attention map to produce accurate dense prediction in medical images.
To this end, we first apply the wavelet transformation to decompose the input
feature map into low-frequency (LF) and high-frequency (HF) subbands. The LF
segment is associated with coarse-grained features while the HF components
preserve fine-grained features such as texture and edge information. Next, we
reformulate the self-attention operation using the efficient Transformer to
perform both spatial and context attention on top of the frequency
representation. Furthermore, to intensify the importance of the boundary
information, we impose an additional attention map by creating a Gaussian
pyramid on top of the HF components. Moreover, we propose a multi-scale context
enhancement block within skip connections to adaptively model inter-scale
dependencies to overcome the semantic gap among stages of the encoder and
decoder modules. Throughout comprehensive experiments, we demonstrate the
effectiveness of our strategy on multi-organ and skin lesion segmentation
benchmarks. The implementation code will be available upon acceptance.
\href{https://github.com/mindflow-institue/WaveFormer}{GitHub}.
</p></li>
</ul>

<h3>Title: Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers. (arXiv:2308.13494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13494">http://arxiv.org/abs/2308.13494</a></li>
<li>Code URL: https://github.com/WISION-Lab/eventful-transformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13494]] Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers(http://arxiv.org/abs/2308.13494)</code></li>
<li>Summary: <p>Vision Transformers achieve impressive accuracy across a range of visual
recognition tasks. Unfortunately, their accuracy frequently comes with high
computational costs. This is a particular issue in video recognition, where
models are often applied repeatedly across frames or temporal chunks. In this
work, we exploit temporal redundancy between subsequent inputs to reduce the
cost of Transformers for video processing. We describe a method for identifying
and re-processing only those tokens that have changed significantly over time.
Our proposed family of models, Eventful Transformers, can be converted from
existing Transformers (often without any re-training) and give adaptive control
over the compute cost at runtime. We evaluate our method on large-scale
datasets for video object detection (ImageNet VID) and action recognition
(EPIC-Kitchens 100). Our approach leads to significant computational savings
(on the order of 2-4x) with only minor reductions in accuracy.
</p></li>
</ul>

<h3>Title: MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13139">http://arxiv.org/abs/2308.13139</a></li>
<li>Code URL: https://github.com/huiyegit/matchxml</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13139]] MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification(http://arxiv.org/abs/2308.13139)</code></li>
<li>Summary: <p>The eXtreme Multi-label text Classification(XMC) refers to training a
classifier that assigns a text sample with relevant labels from an extremely
large-scale label set (e.g., millions of labels). We propose MatchXML, an
efficient text-label matching framework for XMC. We observe that the label
embeddings generated from the sparse Term Frequency-Inverse Document
Frequency(TF-IDF) features have several limitations. We thus propose label2vec
to effectively train the semantic dense label embeddings by the Skip-gram
model. The dense label embeddings are then used to build a Hierarchical Label
Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we
formulate the multi-label text classification as a text-label matching problem
in a bipartite graph. We then extract the dense text representations from the
fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also
extract the static dense sentence embeddings from a pre-trained Sentence
Transformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF
features, the fine-tuned dense text representations and static dense sentence
features. Experimental results demonstrate that MatchXML achieves
state-of-the-art accuracy on five out of six datasets. As for the speed,
MatchXML outperforms the competing methods on all the six datasets. Our source
code is publicly available at https://github.com/huiyegit/MatchXML.
</p></li>
</ul>

<h3>Title: Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13191">http://arxiv.org/abs/2308.13191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13191]] Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers(http://arxiv.org/abs/2308.13191)</code></li>
<li>Summary: <p>Although dominant in natural language processing, transformer-based models
remain challenged by the task of long-sequence processing, because the
computational cost of self-attention operations in transformers swells
quadratically with the input sequence length. To alleviate the complexity of
long-sequence processing, we propose a simple framework to enable the
offthe-shelf pre-trained transformers to process much longer sequences, while
the computation and memory costs remain growing linearly with the input
sequence lengths. More specifically, our method divides each long-sequence
input into a batch of chunks, then aligns the interchunk information during the
encoding steps, and finally selects the most representative hidden states from
the encoder for the decoding process. To extract inter-chunk semantic
information, we align the start and end token embeddings among chunks in each
encoding transformer block. To learn an effective hidden selection policy, we
design a dual updating scheme inspired by reinforcement learning, which regards
the decoders of transformers as environments, and the downstream performance
metrics as the rewards to evaluate the hidden selection actions. Our empirical
results on real-world long-text summarization and reading comprehension tasks
demonstrate effective improvements compared to prior longsequence processing
baselines.
</p></li>
</ul>

<h3>Title: Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13278">http://arxiv.org/abs/2308.13278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13278]] Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity(http://arxiv.org/abs/2308.13278)</code></li>
<li>Summary: <p>Quality-Diversity is a branch of stochastic optimization that is often
applied to problems from the Reinforcement Learning and control domains in
order to construct repertoires of well-performing policies/skills that exhibit
diversity with respect to a behavior space. Such archives are usually composed
of a finite number of reactive agents which are each associated to a unique
behavior descriptor, and instantiating behavior descriptors outside of that
coarsely discretized space is not straight-forward. While a few recent works
suggest solutions to that issue, the trajectory that is generated is not easily
customizable beyond the specification of a target behavior descriptor. We
propose to jointly solve those problems in environments where semantic
information about static scene elements is available by leveraging a Large
Language Model to augment the repertoire with natural language descriptions of
trajectories, and training a policy conditioned on those descriptions. Thus,
our method allows a user to not only specify an arbitrary target behavior
descriptor, but also provide the model with a high-level textual prompt to
shape the generated trajectory. We also propose an LLM-based approach to
evaluating the performance of such generative agents. Furthermore, we develop a
benchmark based on simulated robot navigation in a 2d maze that we use for
experimental validation.
</p></li>
</ul>

<h3>Title: TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. (arXiv:2308.13490v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13490">http://arxiv.org/abs/2308.13490</a></li>
<li>Code URL: https://github.com/google-research-datasets/tpu_graphs</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13490]] TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs(http://arxiv.org/abs/2308.13490)</code></li>
<li>Summary: <p>Precise hardware performance models play a crucial role in code
optimizations. They can assist compilers in making heuristic decisions or aid
autotuners in identifying the optimal configuration for a given program. For
example, the autotuner for XLA, a machine learning compiler, discovered 10-20%
speedup on state-of-the-art models serving substantial production traffic at
Google. Although there exist a few datasets for program performance prediction,
they target small sub-programs such as basic blocks or kernels. This paper
introduces TpuGraphs, a performance prediction dataset on full tensor programs,
represented as computational graphs, running on Tensor Processing Units (TPUs).
Each graph in the dataset represents the main computation of a machine learning
workload, e.g., a training epoch or an inference step. Each data sample
contains a computational graph, a compilation configuration, and the execution
time of the graph when compiled with the configuration. The graphs in the
dataset are collected from open-source machine learning programs, featuring
popular model architectures, e.g., ResNet, EfficientNet, Mask R-CNN, and
Transformer. TpuGraphs provides 25x more graphs than the largest graph property
prediction dataset (with comparable graph sizes), and 770x larger graphs on
average compared to existing performance prediction datasets on machine
learning programs. This graph-level prediction task on large graphs introduces
new challenges in learning, ranging from scalability, training efficiency, to
model quality.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13182">http://arxiv.org/abs/2308.13182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13182]] Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon(http://arxiv.org/abs/2308.13182)</code></li>
<li>Summary: <p>With the advent of digital scanners and deep learning, diagnostic operations
may move from a microscope to a desktop. Hematoxylin and Eosin (H&amp;E) staining
is one of the most frequently used stains for disease analysis, diagnosis, and
grading, but pathologists do need different immunohistochemical (IHC) stains to
analyze specific structures or cells. Obtaining all of these stains (H&amp;E and
different IHCs) on a single specimen is a tedious and time-consuming task.
Consequently, virtual staining has emerged as an essential research direction.
Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for
synthesizing IHC stains from H&amp;E images, and vice versa. Our method expressly
incorporates structural information in the form of edges (in addition to color
data) and employs attention modules exclusively in the decoder of the proposed
generator model. This integration enhances feature localization and preserves
contextual information during the generation process. In addition, a structural
loss is incorporated to ensure accurate structure alignment between the
generated and input markers. To demonstrate the efficacy of the proposed model,
experiments are conducted with two IHC markers emphasizing distinct structures
of glands in the colon: the nucleus of epithelial cells (CDX2) and the
cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently
used for the analysis of generative models, but they do not correlate
explicitly with higher-quality virtual staining results. Therefore, we propose
two new quantitative metrics that correlate directly with the virtual staining
specificity of IHC markers.
</p></li>
</ul>

<h3>Title: ARTIST: ARTificial Intelligence for Simplified Text. (arXiv:2308.13458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13458">http://arxiv.org/abs/2308.13458</a></li>
<li>Code URL: https://github.com/delftcrowd/artist</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13458]] ARTIST: ARTificial Intelligence for Simplified Text(http://arxiv.org/abs/2308.13458)</code></li>
<li>Summary: <p>Complex text is a major barrier for many citizens when accessing public
information and knowledge. While often done manually, Text Simplification is a
key Natural Language Processing task that aims for reducing the linguistic
complexity of a text while preserving the original meaning. Recent advances in
Generative Artificial Intelligence (AI) have enabled automatic text
simplification both on the lexical and syntactical levels. However, as
applications often focus on English, little is understood about the
effectiveness of Generative AI techniques on low-resource languages such as
Dutch. For this reason, we carry out empirical studies to understand the
benefits and limitations of applying generative technologies for text
simplification and provide the following outcomes: 1) the design and
implementation for a configurable text simplification pipeline that
orchestrates state-of-the-art generative text simplification models, domain and
reader adaptation, and visualisation modules; 2) insights and lessons learned,
showing the strengths of automatic text simplification while exposing the
challenges in handling cultural and commonsense knowledge. These outcomes
represent a first step in the exploration of Dutch text simplification and shed
light on future endeavours both for research and practice.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models. (arXiv:2308.13437v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13437">http://arxiv.org/abs/2308.13437</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13437]] Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models(http://arxiv.org/abs/2308.13437)</code></li>
<li>Summary: <p>Recently, Multimodal Large Language Models (MLLMs) that enable Large Language
Models (LLMs) to interpret images through visual instruction tuning have
achieved significant success. However, existing visual instruction tuning
methods only utilize image-language instruction data to align the language and
image modalities, lacking a more fine-grained cross-modal alignment. In this
paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which
extends the functionality of MLLMs by integrating an additional region-level
vision encoder. This integration promotes a more detailed comprehension of
images for the MLLM. In addition, to efficiently achieve a fine-grained
alignment between the vision modules and the LLM, we design multiple data
generation strategies to construct an image-region-language instruction
dataset. Finally, we present both quantitative experiments and qualitative
analysis that demonstrate the superiority of the proposed model. Code and data
will be released at https://github.com/THUNLP-MT/PVIT.
</p></li>
</ul>

<h3>Title: Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13032">http://arxiv.org/abs/2308.13032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13032]] Financial News Analytics Using Fine-Tuned Llama 2 GPT Model(http://arxiv.org/abs/2308.13032)</code></li>
<li>Summary: <p>The paper considers the possibility to fine-tune Llama 2 Large Language Model
(LLM) for the multitask analysis of financial news. For fine-tuning, the
PEFT/LoRA based approach was used. In the study, the model was fine-tuned for
the following tasks: analysing a text from financial market perspectives,
highlighting main points of a text, summarizing a text and extracting named
entities with appropriate sentiments. The obtained results show that the
fine-tuned Llama 2 model can perform a multitask financial news analysis with a
specified structure of response, part of response can be a structured text and
another part of data can have JSON format for further processing. Extracted
sentiments for named entities can be considered as predictive features in
supervised machine learning models with quantitative target variables.
</p></li>
</ul>

<h3>Title: OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13137">http://arxiv.org/abs/2308.13137</a></li>
<li>Code URL: https://github.com/opengvlab/omniquant</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13137]] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models(http://arxiv.org/abs/2308.13137)</code></li>
<li>Summary: <p>Large language models (LLMs) have revolutionized natural language processing
tasks. However, their practical deployment is hindered by their immense memory
and computation requirements. Although recent post-training quantization (PTQ)
methods are effective in reducing memory footprint and improving the
computational efficiency of LLM, they hand-craft quantization parameters, which
leads to low performance and fails to deal with extremely low-bit quantization.
To tackle this issue, we introduce an Omnidirectionally calibrated Quantization
(OmniQuant) technique for LLMs, which achieves good performance in diverse
quantization settings while maintaining the computational efficiency of PTQ by
efficiently optimizing various quantization parameters. OmniQuant comprises two
innovative components including Learnable Weight Clipping (LWC) and Learnable
Equivalent Transformation (LET). LWC modulates the extreme values of weights by
optimizing the clipping threshold. Meanwhile, LET tackles activation outliers
by shifting the challenge of quantization from activations to weights through a
learnable equivalent transformation. Operating within a differentiable
framework using block-wise error minimization, OmniQuant can optimize the
quantization process efficiently for both weight-only and weight-activation
quantization. For instance, the LLaMA-2 model family with the size of 7-70B can
be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using
128 samples. Extensive experiments validate OmniQuant's superior performance
across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16,
and W2A16. Additionally, OmniQuant demonstrates effectiveness in
instruction-tuned models and delivers notable improvements in inference speed
and memory reduction on real devices. Codes and models are available at
\url{https://github.com/OpenGVLab/OmniQuant}.
</p></li>
</ul>

<h3>Title: SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research. (arXiv:2308.13149v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13149">http://arxiv.org/abs/2308.13149</a></li>
<li>Code URL: https://github.com/opendfm/bai-scieval</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13149]] SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research(http://arxiv.org/abs/2308.13149)</code></li>
<li>Summary: <p>Recently, there has been growing interest in using Large Language Models
(LLMs) for scientific research. Numerous benchmarks have been proposed to
evaluate the ability of LLMs for scientific research. However, current
benchmarks are mostly based on pre-collected objective questions. This design
suffers from data leakage problem and lacks the evaluation of subjective Q/A
ability. In this paper, we propose SciEval, a comprehensive and
multi-disciplinary evaluation benchmark to address these issues. Based on
Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate
scientific research ability. In particular, we design a "dynamic" subset based
on scientific principles to prevent evaluation from potential data leakage.
Both objective and subjective questions are included in SciEval. These
characteristics make SciEval a more effective benchmark for scientific research
ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs
show that, although GPT-4 achieves SOTA performance compared to other LLMs,
there is still substantial room for improvement, especially for dynamic
questions. The data and codes are now publicly available.
</p></li>
</ul>

<h3>Title: LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models. (arXiv:2308.13207v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13207">http://arxiv.org/abs/2308.13207</a></li>
<li>Code URL: https://github.com/anmoln94/Team_LLM2KB_LM-KBC-2023</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13207]] LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models(http://arxiv.org/abs/2308.13207)</code></li>
<li>Summary: <p>The advent of Large Language Models (LLM) has revolutionized the field of
natural language processing, enabling significant progress in various
applications. One key area of interest is the construction of Knowledge Bases
(KB) using these powerful models. Knowledge bases serve as repositories of
structured information, facilitating information retrieval and inference tasks.
Our paper proposes LLM2KB, a system for constructing knowledge bases using
large language models, with a focus on the Llama 2 architecture and the
Wikipedia dataset. We perform parameter efficient instruction tuning for
Llama-2-13b-chat and StableBeluga-13B by training small injection models that
have only 0.05 % of the parameters of the base models using the Low Rank
Adaptation (LoRA) technique. These injection models have been trained with
prompts that are engineered to utilize Wikipedia page contexts of subject
entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer
relevant object entities for a given subject entity and relation. Our best
performing model achieved an average F1 score of 0.6185 across 21 relations in
the LM-KBC challenge held at the ISWC 2023 conference.
</p></li>
</ul>

<h3>Title: Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. (arXiv:2308.13387v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13387">http://arxiv.org/abs/2308.13387</a></li>
<li>Code URL: https://github.com/libr-ai/do-not-answer</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13387]] Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs(http://arxiv.org/abs/2308.13387)</code></li>
<li>Summary: <p>With the rapid evolution of large language models (LLMs), new and
hard-to-predict harmful capabilities are emerging. This requires developers to
be able to identify risks through the evaluation of "dangerous capabilities" in
order to responsibly deploy LLMs. In this work, we collect the first
open-source dataset to evaluate safeguards in LLMs, and deploy safer
open-source LLMs at a low cost. Our dataset is curated and filtered to consist
only of instructions that responsible language models should not follow. We
annotate and assess the responses of six popular LLMs to these instructions.
Based on our annotation, we proceed to train several BERT-like classifiers, and
find that these small classifiers can achieve results that are comparable with
GPT-4 on automatic safety evaluation. Warning: this paper contains example data
that may be offensive, harmful, or biased.
</p></li>
</ul>

<h3>Title: The Poison of Alignment. (arXiv:2308.13449v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13449">http://arxiv.org/abs/2308.13449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13449]] The Poison of Alignment(http://arxiv.org/abs/2308.13449)</code></li>
<li>Summary: <p>From the perspective of content safety issues, alignment has shown to limit
large language models' (LLMs) harmful content generation. This intentional
method of reinforcing models to not respond to certain user inputs seem to be
present in many modern open-source instruction tuning datasets such as
OpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned
model's performance affected by the presence of alignment in supervised
fine-tuning dataset. To be specific, we noticed that alignment acts as if it is
poisoning the instruction dataset. Experimentally, we demonstrate that aligned
answers significantly worsen the performance of the resulting fine-tuned
model's on various reasoning benchmarks such as Big Bench (BBH), Massive
Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning
Over Paragraphs (DROP), performing worse than the counterpart tuned without
alignment by 4-33%.
</p></li>
</ul>

<h3>Title: Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages. (arXiv:2308.13479v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13479">http://arxiv.org/abs/2308.13479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13479]] Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages(http://arxiv.org/abs/2308.13479)</code></li>
<li>Summary: <p>Large language models (LLMs) are increasingly capable and prevalent, and can
be used to produce creative content. The quality of content is influenced by
the prompt used, with more specific prompts that incorporate examples generally
producing better results. On from this, it could be seen that using
instructions written for crowdsourcing tasks (that are specific and include
examples to guide workers) could prove effective LLM prompts. To explore this,
we used a previous crowdsourcing pipeline that gave examples to people to help
them generate a collectively diverse corpus of motivational messages. We then
used this same pipeline to generate messages using GPT-4, and compared the
collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the
pipeline, and (3 &amp; 4) two baseline GPT-4 prompts. We found that the LLM prompts
using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages
than the two baseline prompts. We also discuss implications from messages
generated by both human writers and LLMs.
</p></li>
</ul>

<h3>Title: ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection. (arXiv:2308.13517v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13517">http://arxiv.org/abs/2308.13517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13517]] ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection(http://arxiv.org/abs/2308.13517)</code></li>
<li>Summary: <p>Open intent detection, a crucial aspect of natural language understanding,
involves the identification of previously unseen intents in user-generated
text. Despite the progress made in this field, challenges persist in handling
new combinations of language components, which is essential for compositional
generalization. In this paper, we present a case study exploring the use of
ChatGPT as a data augmentation technique to enhance compositional
generalization in open intent detection tasks. We begin by discussing the
limitations of existing benchmarks in evaluating this problem, highlighting the
need for constructing datasets for addressing compositional generalization in
open intent detection tasks. By incorporating synthetic data generated by
ChatGPT into the training process, we demonstrate that our approach can
effectively improve model performance. Rigorous evaluation of multiple
benchmarks reveals that our method outperforms existing techniques and
significantly enhances open intent detection capabilities. Our findings
underscore the potential of large language models like ChatGPT for data
augmentation in natural language understanding tasks.
</p></li>
</ul>

<h3>Title: Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13111">http://arxiv.org/abs/2308.13111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13111]] Bayesian low-rank adaptation for large language models(http://arxiv.org/abs/2308.13111)</code></li>
<li>Summary: <p>Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for
cost-efficient fine-tuning of large language models (LLMs), with low-rank
adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often
become overconfident especially on when fine-tuned on smaller datasets.
Bayesian methods, with their inherent ability to estimate uncertainty, serve as
potent tools to mitigate overconfidence and enhance calibration. In this work,
we introduce Laplace-LoRA, a straightforward yet effective Bayesian method,
which applies the Laplace approximation to the LoRA parameters and,
considerably boosts the calibration of fine-tuned LLMs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Enhancing Perception and Immersion in Pre-Captured Environments through Learning-Based Eye Height Adaptation. (arXiv:2308.13042v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13042">http://arxiv.org/abs/2308.13042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13042]] Enhancing Perception and Immersion in Pre-Captured Environments through Learning-Based Eye Height Adaptation(http://arxiv.org/abs/2308.13042)</code></li>
<li>Summary: <p>Pre-captured immersive environments using omnidirectional cameras provide a
wide range of virtual reality applications. Previous research has shown that
manipulating the eye height in egocentric virtual environments can
significantly affect distance perception and immersion. However, the influence
of eye height in pre-captured real environments has received less attention due
to the difficulty of altering the perspective after finishing the capture
process. To explore this influence, we first propose a pilot study that
captures real environments with multiple eye heights and asks participants to
judge the egocentric distances and immersion. If a significant influence is
confirmed, an effective image-based approach to adapt pre-captured real-world
environments to the user's eye height would be desirable. Motivated by the
study, we propose a learning-based approach for synthesizing novel views for
omnidirectional images with altered eye heights. This approach employs a
multitask architecture that learns depth and semantic segmentation in two
formats, and generates high-quality depth and semantic segmentation to
facilitate the inpainting stage. With the improved omnidirectional-aware
layered depth image, our approach synthesizes natural and realistic visuals for
eye height adaptation. Quantitative and qualitative evaluation shows favorable
results against state-of-the-art methods, and an extensive user study verifies
improved perception and immersion for pre-captured real-world environments.
</p></li>
</ul>

<h3>Title: Interactive segmentation in aerial images: a new benchmark and an open access web-based tool. (arXiv:2308.13174v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13174">http://arxiv.org/abs/2308.13174</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13174]] Interactive segmentation in aerial images: a new benchmark and an open access web-based tool(http://arxiv.org/abs/2308.13174)</code></li>
<li>Summary: <p>In recent years, deep learning has emerged as a powerful approach in remote
sensing applications, particularly in segmentation and classification
techniques that play a crucial role in extracting significant land features
from satellite and aerial imagery. However, only a limited number of papers
have discussed the use of deep learning for interactive segmentation in land
cover classification tasks. In this study, we aim to bridge the gap between
interactive segmentation and remote sensing image analysis by conducting a
benchmark study on various deep learning-based interactive segmentation models.
We assessed the performance of five state-of-the-art interactive segmentation
methods (SimpleClick, FocalClick, Iterative Click Loss (ICL), Reviving
Iterative Training with Mask Guidance for Interactive Segmentation (RITM), and
Segment Anything (SAM)) on two high-resolution aerial imagery datasets. To
enhance the segmentation results without requiring multiple models, we
introduced the Cascade-Forward Refinement (CFR) approach, an innovative
inference strategy for interactive segmentation. We evaluated these interactive
segmentation methods on various land cover types, object sizes, and band
combinations in remote sensing. Surprisingly, the popularly discussed method,
SAM, proved to be ineffective for remote sensing images. Conversely, the
point-based approach used in the SimpleClick models consistently outperformed
the other methods in all experiments. Building upon these findings, we
developed a dedicated online tool called RSISeg for interactive segmentation of
remote sensing data. RSISeg incorporates a well-performing interactive model,
fine-tuned with remote sensing data. Additionally, we integrated the SAM model
into this tool. Compared to existing interactive segmentation tools, RSISeg
offers strong interactivity, modifiability, and adaptability to remote sensing
data.
</p></li>
</ul>

<h3>Title: Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions. (arXiv:2308.13178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13178">http://arxiv.org/abs/2308.13178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13178]] Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions(http://arxiv.org/abs/2308.13178)</code></li>
<li>Summary: <p>Text segmentation tasks have a very wide range of application values, such as
image editing, style transfer, watermark removal, etc.However, existing public
datasets are of poor quality of pixel-level labels that have been shown to be
notoriously costly to acquire, both in terms of money and time. At the same
time, when pretraining is performed on synthetic datasets, the data
distribution of the synthetic datasets is far from the data distribution in the
real scene. These all pose a huge challenge to the current pixel-level text
segmentation algorithms.To alleviate the above problems, we propose a
self-supervised scene text segmentation algorithm with layered decoupling of
representations derived from the object-centric manner to segment images into
texts and background. In our method, we propose two novel designs which include
Region Query Module and Representation Consistency Constraints adapting to the
unique properties of text as complements to Auto Encoder, which improves the
network's sensitivity to texts.For this unique design, we treat the
polygon-level masks predicted by the text localization model as extra input
information, and neither utilize any pixel-level mask annotations for training
stage nor pretrain on synthetic datasets.Extensive experiments show the
effectiveness of the method proposed. On several public scene text datasets,
our method outperforms the state-of-the-art unsupervised segmentation
algorithms.
</p></li>
</ul>

<h3>Title: DPF-Net: Combining Explicit Shape Priors in Deformable Primitive Field for Unsupervised Structural Reconstruction of 3D Objects. (arXiv:2308.13225v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13225">http://arxiv.org/abs/2308.13225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13225]] DPF-Net: Combining Explicit Shape Priors in Deformable Primitive Field for Unsupervised Structural Reconstruction of 3D Objects(http://arxiv.org/abs/2308.13225)</code></li>
<li>Summary: <p>Unsupervised methods for reconstructing structures face significant
challenges in capturing the geometric details with consistent structures among
diverse shapes of the same category. To address this issue, we present a novel
unsupervised structural reconstruction method, named DPF-Net, based on a new
Deformable Primitive Field (DPF) representation, which allows for high-quality
shape reconstruction using parameterized geometric primitives. We design a
two-stage shape reconstruction pipeline which consists of a primitive
generation module and a primitive deformation module to approximate the target
shape of each part progressively. The primitive generation module estimates the
explicit orientation, position, and size parameters of parameterized geometric
primitives, while the primitive deformation module predicts a dense deformation
field based on a parameterized primitive field to recover shape details. The
strong shape prior encoded in parameterized geometric primitives enables our
DPF-Net to extract high-level structures and recover fine-grained shape details
consistently. The experimental results on three categories of objects in
diverse shapes demonstrate the effectiveness and generalization ability of our
DPF-Net on structural reconstruction and shape segmentation.
</p></li>
</ul>

<h3>Title: Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation. (arXiv:2308.13266v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13266">http://arxiv.org/abs/2308.13266</a></li>
<li>Code URL: https://github.com/yoxu515/mits</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13266]] Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation(http://arxiv.org/abs/2308.13266)</code></li>
<li>Summary: <p>Tracking any given object(s) spatially and temporally is a common purpose in
Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint
tracking and segmentation have been attempted in some studies but they often
lack full compatibility of both box and mask in initialization and prediction,
and mainly focus on single-object scenarios. To address these limitations, this
paper proposes a Multi-object Mask-box Integrated framework for unified
Tracking and Segmentation, dubbed MITS. Firstly, the unified identification
module is proposed to support both box and mask reference for initialization,
where detailed object information is inferred from boxes or directly retained
from masks. Additionally, a novel pinpoint box predictor is proposed for
accurate multi-object box prediction, facilitating target-oriented
representation learning. All target objects are processed simultaneously from
encoding to propagation and decoding, as a unified pipeline for VOT and VOS.
Experimental results show MITS achieves state-of-the-art performance on both
VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor
by around 6% on the GOT-10k test set, and significantly improves the
performance of box initialization on VOS benchmarks. The code is available at
https://github.com/yoxu515/MITS.
</p></li>
</ul>

<h3>Title: SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation. (arXiv:2308.13323v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13323">http://arxiv.org/abs/2308.13323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13323]] SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation(http://arxiv.org/abs/2308.13323)</code></li>
<li>Summary: <p>LiDAR-based semantic perception tasks are critical yet challenging for
autonomous driving. Due to the motion of objects and static/dynamic occlusion,
temporal information plays an essential role in reinforcing perception by
enhancing and completing single-frame knowledge. Previous approaches either
directly stack historical frames to the current frame or build a 4D
spatio-temporal neighborhood using KNN, which duplicates computation and
hinders realtime performance. Based on our observation that stacking all the
historical points would damage performance due to a large amount of redundant
and misleading information, we propose the Sparse Voxel-Adjacent Query Network
(SVQNet) for 4D LiDAR semantic segmentation. To take full advantage of the
historical frames high-efficiently, we shunt the historical points into two
groups with reference to the current points. One is the Voxel-Adjacent
Neighborhood carrying local enhancing knowledge. The other is the Historical
Context completing the global knowledge. Then we propose new modules to select
and extract the instructive features from the two groups. Our SVQNet achieves
state-of-the-art performance in LiDAR semantic segmentation of the
SemanticKITTI benchmark and the nuScenes dataset.
</p></li>
</ul>

<h3>Title: RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network. (arXiv:2308.13469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13469">http://arxiv.org/abs/2308.13469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13469]] RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual Transformation Network(http://arxiv.org/abs/2308.13469)</code></li>
<li>Summary: <p>Cross-domain few-shot segmentation (CD-FSS) aims to achieve semantic
segmentation in previously unseen domains with a limited number of annotated
samples. Although existing CD-FSS models focus on cross-domain feature
transformation, relying exclusively on inter-domain knowledge transfer may lead
to the loss of critical intra-domain information. To this end, we propose a
novel residual transformation network (RestNet) that facilitates knowledge
transfer while retaining the intra-domain support-query feature information.
Specifically, we propose a Semantic Enhanced Anchor Transform (SEAT) module
that maps features to a stable domain-agnostic space using advanced semantics.
Additionally, an Intra-domain Residual Enhancement (IRE) module is designed to
maintain the intra-domain representation of the original discriminant space in
the new space. We also propose a mask prediction strategy based on prototype
fusion to help the model gradually learn how to segment. Our RestNet can
transfer cross-domain knowledge from both inter-domain and intra-domain without
requiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray,
and FSS-1000 show that our RestNet achieves state-of-the-art performance. Our
code will be available soon.
</p></li>
</ul>

<h3>Title: Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation. (arXiv:2308.13505v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13505">http://arxiv.org/abs/2308.13505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13505]] Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation(http://arxiv.org/abs/2308.13505)</code></li>
<li>Summary: <p>Current prevailing Video Object Segmentation (VOS) methods usually perform
dense matching between the current and reference frames after extracting their
features. One on hand, the decoupled modeling restricts the targets information
propagation only at high-level feature space. On the other hand, the pixel-wise
matching leads to a lack of holistic understanding of the targets. To overcome
these issues, we propose a unified VOS framework, coined as JointFormer, for
joint modeling the three elements of feature, correspondence, and a compressed
memory. The core design is the Joint Block, utilizing the flexibility of
attention to simultaneously extract feature and propagate the targets
information to the current tokens and the compressed memory token. This scheme
allows to perform extensive information propagation and discriminative feature
learning. To incorporate the long-term temporal targets information, we also
devise a customized online updating mechanism for the compressed memory token,
which can prompt the information flow along the temporal dimension and thus
improve the global modeling capability. Under the design, our method achieves a
new state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) and
YouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existing
works by a large margin.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
