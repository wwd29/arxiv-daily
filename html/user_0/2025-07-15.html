<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-15</h1>
<h3>Title: Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Kimia Soroush, Nastaran Shirazi, Mohsen Raji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08829">https://arxiv.org/abs/2507.08829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08829">https://arxiv.org/pdf/2507.08829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08829]] Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI(https://arxiv.org/abs/2507.08829)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) are widely employed in safety-critical domains, where ensuring their reliability is essential. Triple Modular Redundancy (TMR) is an effective technique to enhance the reliability of DNNs in the presence of bit-flip faults. In order to handle the significant overhead of TMR, it is applied selectively on the parameters and components with the highest contribution at the model output. Hence, the accuracy of the selection criterion plays the key role on the efficiency of TMR. This paper presents an efficient TMR approach to enhance the reliability of DNNs against bit-flip faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can provide valuable insights about the importance of individual neurons and weights in the performance of the network, they can be applied as the selection metric in TMR techniques. The proposed method utilizes a low-cost, gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to calculate importance scores for DNN parameters. These scores are then used to enhance the reliability of the model, with the most critical weights being protected by TMR. The proposed approach is evaluated on two DNN models, VGG16 and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate that the method can protect the AlexNet model at a bit error rate of 10-4, achieving over 60% reliability improvement while maintaining the same overhead as state-of-the-art methods.</li>
</ul>

<h3>Title: View Invariant Learning for Vision-Language Navigation in Continuous Environments</h3>
<ul>
<li><strong>Authors: </strong>Josh Qixuan Sun, Xiaoying Xing, Huaiyuan Weng, Chul Min Yeum, Mark Crowley</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08831">https://arxiv.org/abs/2507.08831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08831">https://arxiv.org/pdf/2507.08831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08831]] View Invariant Learning for Vision-Language Navigation in Continuous Environments(https://arxiv.org/abs/2507.08831)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.</li>
</ul>

<h3>Title: LoRA Is Slower Than You Think</h3>
<ul>
<li><strong>Authors: </strong>Seokmin Ko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08833">https://arxiv.org/abs/2507.08833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08833">https://arxiv.org/pdf/2507.08833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08833]] LoRA Is Slower Than You Think(https://arxiv.org/abs/2507.08833)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is one of the most widely used techniques for fine-tuning large language models (LLMs). By introducing a small number of trainable low-rank weight matrices, LoRA substantially reduces the number of parameters that need to be updated, offering significant advantages in memory consumption and computational efficiency compared to full fine-tuning. However, we observed that LoRA does not consistently provide speed improvements across all model architectures and training setups. Motivated by this inconsistency, we conduct a comprehensive analysis of LoRA's performance and investigate the underlying factors limiting its speedup. Based on our findings, we propose several methods for more efficient fine-tuning of LLMs. We empirically evaluate these methods and compare them to LoRA, demonstrating that our approach achieves comparable or superior performance while delivering more consistent training speed improvements. Our work offers valuable insights and practical guidelines for practitioners seeking to optimize LLM fine-tuning under resource constraints.</li>
</ul>

<h3>Title: Physical Informed Neural Networks for modeling ocean pollutant</h3>
<ul>
<li><strong>Authors: </strong>Karishma Battina, Prathamesh Dinesh Joshi, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08834">https://arxiv.org/abs/2507.08834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08834">https://arxiv.org/pdf/2507.08834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08834]] Physical Informed Neural Networks for modeling ocean pollutant(https://arxiv.org/abs/2507.08834)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Traditional numerical methods often struggle with the complexity and scale of modeling pollutant transport across vast and dynamic oceanic domains. This paper introduces a Physics-Informed Neural Network (PINN) framework to simulate the dispersion of pollutants governed by the 2D advection-diffusion equation. The model achieves physically consistent predictions by embedding physical laws and fitting to noisy synthetic data, generated via a finite difference method (FDM), directly into the neural network training process. This approach addresses challenges such as non-linear dynamics and the enforcement of boundary and initial conditions. Synthetic data sets, augmented with varying noise levels, are used to capture real-world variability. The training incorporates a hybrid loss function including PDE residuals, boundary/initial condition conformity, and a weighted data fit term. The approach takes advantage of the Julia language scientific computing ecosystem for high-performance simulations, offering a scalable and flexible alternative to traditional solvers</li>
</ul>

<h3>Title: Representation learning with a transformer by contrastive learning for money laundering detection</h3>
<ul>
<li><strong>Authors: </strong>Harold Gu√©neau (SAMM), Alain Celisse (LPP, MODAL), Pascal Delange</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, q-fin.RM, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08835">https://arxiv.org/abs/2507.08835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08835">https://arxiv.org/pdf/2507.08835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08835]] Representation learning with a transformer by contrastive learning for money laundering detection(https://arxiv.org/abs/2507.08835)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The present work tackles the money laundering detection problem. A new procedure is introduced which exploits structured time series of both qualitative and quantitative data by means of a transformer neural network. The first step of this procedure aims at learning representations of time series through contrastive learning (without any labels). The second step leverages these representations to generate a money laundering scoring of all observations. A two-thresholds approach is then introduced, which ensures a controlled false-positive rate by means of the Benjamini-Hochberg (BH) procedure. Experiments confirm that the transformer is able to produce general representations that succeed in exploiting money laundering patterns with minimal supervision from domain experts. It also illustrates the higher ability of the new procedure for detecting nonfraudsters as well as fraudsters, while keeping the false positive rate under control. This greatly contrasts with rule-based procedures or the ones based on LSTM architectures.</li>
</ul>

<h3>Title: Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing</h3>
<ul>
<li><strong>Authors: </strong>Damien Fovet, Shashank Chamoli, Sarah Oury, Srishti Singhal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08836">https://arxiv.org/abs/2507.08836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08836">https://arxiv.org/pdf/2507.08836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08836]] Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing(https://arxiv.org/abs/2507.08836)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the performance of a compression method, called CompactifAI, developed by Multiverse Computing, applied to the large language model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in terms of energy consumption) and accuracy using respectively the frameworks Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed between the model compressed with CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our findings reveal that the compressed model using CompactifAI not only significantly reduced the computational resources but also maintained the model accuracy, making the model more efficient, scalable and cost-effective.</li>
</ul>

<h3>Title: wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaohang Tang, Rares Dolga, Sangwoong Yoon, Ilija Bogunovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08838">https://arxiv.org/abs/2507.08838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08838">https://arxiv.org/pdf/2507.08838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08838]] wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models(https://arxiv.org/abs/2507.08838)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Improving the reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL) remains an open problem. The intractability of dLLMs likelihood function necessitates approximating the current, old, and reference policy likelihoods at each policy optimization step. This reliance introduces additional computational overhead and lead to potentially large bias -- particularly when approximation errors occur in the denominator of policy ratios used for importance sampling. To mitigate these issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that reformulates the objective as a weighted likelihood, requiring only a single approximation for the current parametrized policy likelihood. Experiments on widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without supervised fine-tuning (SFT) or any supervised data, outperforms existing RL methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers additional computational gains, including reduced training time and fewer function evaluations (NFEs) per gradient step. These findings, combined with the simplicity of method's implementation and R1-Zero-like training (no SFT), position $\mathtt{wd1}$ as a more effective and efficient method for applying RL to dLLMs reasoning.</li>
</ul>

<h3>Title: Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer</h3>
<ul>
<li><strong>Authors: </strong>Xiaowei Yu, Jing Zhang, Tong Chen, Yan Zhuang, Minheng Chen, Chao Cao, Yanjun Lyu, Lu Zhang, Li Su, Tianming Liu, Dajiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08839">https://arxiv.org/abs/2507.08839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08839">https://arxiv.org/pdf/2507.08839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08839]] Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer(https://arxiv.org/abs/2507.08839)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Lewy Body Disease (LBD) is a common yet understudied form of dementia that imposes a significant burden on public health. It shares clinical similarities with Alzheimer's disease (AD), as both progress through stages of normal cognition, mild cognitive impairment, and dementia. A major obstacle in LBD diagnosis is data scarcity, which limits the effectiveness of deep learning. In contrast, AD datasets are more abundant, offering potential for knowledge transfer. However, LBD and AD data are typically collected from different sites using different machines and protocols, resulting in a distinct domain shift. To effectively leverage AD data while mitigating domain shift, we propose a Transferability Aware Transformer (TAT) that adapts knowledge from AD to enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived from structural MRI as training data. Built on the attention mechanism, TAT adaptively assigns greater weights to disease-transferable features while suppressing domain-specific ones, thereby reducing domain shift and improving diagnostic accuracy with limited LBD data. The experimental results demonstrate the effectiveness of TAT. To the best of our knowledge, this is the first study to explore domain adaptation from AD to LBD under conditions of data scarcity and domain shift, providing a promising framework for domain-adaptive diagnosis of rare diseases.</li>
</ul>

<h3>Title: Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing</h3>
<ul>
<li><strong>Authors: </strong>Zhufeng Lu, Chentao Jia, Ming Hu, Xiaofei Xie, Mingsong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08842">https://arxiv.org/abs/2507.08842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08842">https://arxiv.org/pdf/2507.08842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08842]] Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing(https://arxiv.org/abs/2507.08842)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>As a promising privacy-aware collaborative model training paradigm, Federated Learning (FL) is becoming popular in the design of distributed recommender systems. However, Federated Recommender Systems (FedRecs) greatly suffer from two major problems: i) extremely high communication overhead due to massive item embeddings involved in recommendation systems, and ii) intolerably low training efficiency caused by the entanglement of both heterogeneous network environments and client devices. Although existing methods attempt to employ various compression techniques to reduce communication overhead, due to the parameter errors introduced by model compression, they inevitably suffer from model performance degradation. To simultaneously address the above problems, this paper presents a communication-efficient FedRec framework named FedRAS, which adopts an action-sharing strategy to cluster the gradients of item embedding into a specific number of model updating actions for communication rather than directly compressing the item embeddings. In this way, the cloud server can use the limited actions from clients to update all the items. Since gradient values are significantly smaller than item embeddings, constraining the directions of gradients (i.e., the action space) introduces smaller errors compared to compressing the entire item embedding matrix into a reduced space. To accommodate heterogeneous devices and network environments, FedRAS incorporates an adaptive clustering mechanism that dynamically adjusts the number of actions. Comprehensive experiments on well-known datasets demonstrate that FedRAS can reduce the size of communication payloads by up to 96.88%, while not sacrificing recommendation performance within various heterogeneous scenarios. We have open-sourced FedRAS at this https URL.</li>
</ul>

<h3>Title: Can We Predict Your Next Move Without Breaking Your Privacy?</h3>
<ul>
<li><strong>Authors: </strong>Arpita Soni, Sahil Tripathi, Gautam Siddharth Kashyap, Manaswi Kulahara, Mohammad Anas Azeez, Zohaib Hasan Siddiqui, Nipun Joshi, Jiechao Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08843">https://arxiv.org/abs/2507.08843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08843">https://arxiv.org/pdf/2507.08843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08843]] Can We Predict Your Next Move Without Breaking Your Privacy?(https://arxiv.org/abs/2507.08843)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>We propose FLLL3M--Federated Learning with Large Language Models for Mobility Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP). By retaining user data locally and leveraging LLMs through an efficient outer product mechanism, FLLL3M ensures high accuracy with low resource demands. It achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71, 0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while reducing parameters by up to 45.6% and memory usage by 52.7%.</li>
</ul>

<h3>Title: Immutability Does Not Guarantee Trust: A Formal and Logical Refutation</h3>
<ul>
<li><strong>Authors: </strong>Craig S Wright</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08844">https://arxiv.org/abs/2507.08844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08844">https://arxiv.org/pdf/2507.08844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08844]] Immutability Does Not Guarantee Trust: A Formal and Logical Refutation(https://arxiv.org/abs/2507.08844)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>It is frequently claimed in blockchain discourse that immutability guarantees trust. This paper rigorously refutes that assertion. We define immutability as the cryptographic persistence of historical states in an append-only data structure and contrast it with trust, understood as a rational epistemic expectation under uncertainty. Employing predicate logic, automata-theoretic models, and epistemic game-theoretic analysis, we demonstrate that immutability neither entails nor implies correctness, fairness, or credibility. Through formal constructions and counterexamples--including predictive fraud schemes and the phenomenon of garbage permanence--we show that the belief conflates structural and epistemic domains. Immutability preserves all data equally, regardless of veracity. Therefore, the assertion that immutability guarantees trust collapses under the weight of formal scrutiny.</li>
</ul>

<h3>Title: Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives</h3>
<ul>
<li><strong>Authors: </strong>Victoria L. Lemieux, Rosa Gil, Faith Molosiwa, Qihong Zhou, Binming Li, Roberto Garcia, Luis De La Torre Cubillo, Zehua Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08853">https://arxiv.org/abs/2507.08853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08853">https://arxiv.org/pdf/2507.08853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08853]] Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives(https://arxiv.org/abs/2507.08853)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>As archives turn to artificial intelligence to manage growing volumes of digital records, privacy risks inherent in current AI data practices raise critical concerns about data sovereignty and ethical accountability. This paper explores how privacy-enhancing technologies (PETs) and Web3 architectures can support archives to preserve control over sensitive content while still being able to make it available for access by researchers. We present Clio-X, a decentralized, privacy-first Web3 digital solution designed to embed PETs into archival workflows and support AI-enabled reference and access. Drawing on a user evaluation of a medium-fidelity prototype, the study reveals both interest in the potential of the solution and significant barriers to adoption related to trust, system opacity, economic concerns, and governance. Using Rogers' Diffusion of Innovation theory, we analyze the sociotechnical dimensions of these barriers and propose a path forward centered on participatory design and decentralized governance through a Clio-X Decentralized Autonomous Organization. By integrating technical safeguards with community-based oversight, Clio-X offers a novel model to ethically deploy AI in cultural heritage contexts.</li>
</ul>

<h3>Title: RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianzhe Zhao, Jiaoyan Chen, Yanchi Ru, Haiping Zhu, Nan Hu, Jun Liu, Qika Lin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08862">https://arxiv.org/abs/2507.08862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08862">https://arxiv.org/pdf/2507.08862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08862]] RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation(https://arxiv.org/abs/2507.08862)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving external data to mitigate hallucinations and outdated knowledge issues. Benefiting from the strong ability in facilitating diverse data sources and supporting faithful reasoning, knowledge graphs (KGs) have been increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG) methods. Though RAG systems are widely applied in various applications, recent studies have also revealed its vulnerabilities to data poisoning attacks, where malicious information injected into external knowledge sources can mislead the system into producing incorrect or harmful responses. However, these studies focus exclusively on RAG systems using unstructured textual data sources, leaving the security risks of KG-RAG largely unexplored, despite the fact that KGs present unique vulnerabilities due to their structured and editable nature. In this work, we conduct the first systematic investigation of the security issue of KG-RAG methods through data poisoning attacks. To this end, we introduce a practical, stealthy attack setting that aligns with real-world implementation. We propose an attack strategy that first identifies adversarial target answers and then inserts perturbation triples to complete misleading inference chains in the KG, increasing the likelihood that KG-RAG methods retrieve and rely on these perturbations during generation. Through extensive experiments on two benchmarks and four recent KG-RAG methods, our attack strategy demonstrates strong effectiveness in degrading KG-RAG performance, even with minimal KG perturbations. In-depth analyses are also conducted to understand the safety threats within the internal stages of KG-RAG systems and to explore the robustness of LLMs against adversarial knowledge.</li>
</ul>

<h3>Title: Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System</h3>
<ul>
<li><strong>Authors: </strong>Poushali Sengupta, Sabita Maharjan, frank Eliassen, Yan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08864">https://arxiv.org/abs/2507.08864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08864">https://arxiv.org/pdf/2507.08864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08864]] Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System(https://arxiv.org/abs/2507.08864)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, fair</a></li>
<li><strong>Abstract: </strong>Location-based vehicular traffic management faces significant challenges in protecting sensitive geographical data while maintaining utility for traffic management and fairness across regions. Existing state-of-the-art solutions often fail to meet the required level of protection against linkage attacks and demographic biases, leading to privacy leakage and inequity in data analysis. In this paper, we propose a novel algorithm designed to address the challenges regarding the balance of privacy, utility, and fairness in location-based vehicular traffic management systems. In this context, utility means providing reliable and meaningful traffic information, while fairness ensures that all regions and individuals are treated equitably in data use and decision-making. Employing differential privacy techniques, we enhance data security by integrating query-based data access with iterative shuffling and calibrated noise injection, ensuring that sensitive geographical data remains protected. We ensure adherence to epsilon-differential privacy standards by implementing the Laplace mechanism. We implemented our algorithm on vehicular location-based data from Norway, demonstrating its ability to maintain data utility for traffic management and urban planning while ensuring fair representation of all geographical areas without being overrepresented or underrepresented. Additionally, we have created a heatmap of Norway based on our model, illustrating the privatized and fair representation of the traffic conditions across various cities. Our algorithm provides privacy in vehicular traffic</li>
</ul>

<h3>Title: Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale</h3>
<ul>
<li><strong>Authors: </strong>Javis AI Team: Amrendra Singh, Maulik Shah, Dharshan Sampath</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08865">https://arxiv.org/abs/2507.08865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08865">https://arxiv.org/pdf/2507.08865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08865]] Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale(https://arxiv.org/abs/2507.08865)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Extracting tables and key-value pairs from financial documents is essential for business workflows such as auditing, data analytics, and automated invoice processing. In this work, we introduce Spatial ModernBERT-a transformer-based model augmented with spatial embeddings-to accurately detect and extract tabular data and key-value fields from complex financial documents. We cast the extraction task as token classification across three heads: (1) Label Head, classifying each token as a label (e.g., PO Number, PO Date, Item Description, Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices; (3) Row Head, distinguishing the start of item rows and header rows. The model is pretrained on the PubTables-1M dataset, then fine-tuned on a financial document dataset, achieving robust performance through cross-entropy loss on each classification head. We propose a post-processing method to merge tokens using B-I-IB tagging, reconstruct the tabular layout, and extract key-value pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages both textual and spatial cues, facilitating highly accurate table and key-value extraction in real-world financial documents.</li>
</ul>

<h3>Title: Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Marina Ceccon, Giandomenico Cornacchia, Davide Dalle Pezze, Alessandro Fabris, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08866">https://arxiv.org/abs/2507.08866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08866">https://arxiv.org/pdf/2507.08866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08866]] Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond(https://arxiv.org/abs/2507.08866)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Undesirable biases encoded in the data are key drivers of algorithmic discrimination. Their importance is widely recognized in the algorithmic fairness literature, as well as legislation and standards on anti-discrimination in AI. Despite this recognition, data biases remain understudied, hindering the development of computational best practices for their detection and mitigation. In this work, we present three common data biases and study their individual and joint effect on algorithmic discrimination across a variety of datasets, models, and fairness measures. We find that underrepresentation of vulnerable populations in training sets is less conducive to discrimination than conventionally affirmed, while combinations of proxies and label bias can be far more critical. Consequently, we develop dedicated mechanisms to detect specific types of bias, and combine them into a preliminary construct we refer to as the Data Bias Profile (DBP). This initial formulation serves as a proof of concept for how different bias signals can be systematically documented. Through a case study with popular fairness datasets, we demonstrate the effectiveness of the DBP in predicting the risk of discriminatory outcomes and the utility of fairness-enhancing interventions. Overall, this article bridges algorithmic fairness research and anti-discrimination policy through a data-centric lens.</li>
</ul>

<h3>Title: GUIDE: Towards Scalable Advising for Research Ideas</h3>
<ul>
<li><strong>Authors: </strong>Yaowenqi Liu, BingXu Meng, Rui Pan, Jerry Huang, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08870">https://arxiv.org/abs/2507.08870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08870">https://arxiv.org/pdf/2507.08870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08870]] GUIDE: Towards Scalable Advising for Research Ideas(https://arxiv.org/abs/2507.08870)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The field of AI research is advancing at an unprecedented pace, enabling automated hypothesis generation and experimental design across diverse domains such as biology, mathematics, and artificial intelligence. Despite these advancements, there remains a significant gap in the availability of scalable advising systems capable of providing high-quality, well-reasoned feedback to refine proposed hypotheses and experimental designs. To address this challenge, we explore key factors that underlie the development of robust advising systems, including model size, context length, confidence estimation, and structured reasoning processes. Our findings reveal that a relatively small model, when equipped with a well-compressed literature database and a structured reasoning framework, can outperform powerful general-purpose language models such as Deepseek-R1 in terms of acceptance rates for self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to high-confidence predictions, our system achieves an acceptance rate exceeding 90% on the ICLR 2025 test set, underscoring its potential to significantly enhance the quality and efficiency of hypothesis generation and experimental design. The code is released at this https URL.</li>
</ul>

<h3>Title: Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination</h3>
<ul>
<li><strong>Authors: </strong>Xishun Liao, Haoxuan Ma, Yifan Liu, Yuxiang Wei, Brian Yueshuai He, Chris Stanford, Jiaqi Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08871">https://arxiv.org/abs/2507.08871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08871">https://arxiv.org/pdf/2507.08871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08871]] Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination(https://arxiv.org/abs/2507.08871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Travel demand models are critical tools for planning, policy, and mobility system design. Traditional activity-based models (ABMs), although grounded in behavioral theories, often rely on simplified rules and assumptions, and are costly to develop and difficult to adapt across different regions. This paper presents a learning-based travel demand modeling framework that synthesizes household-coordinated daily activity patterns based on a household's socio-demographic profiles. The whole framework integrates population synthesis, coordinated activity generation, location assignment, and large-scale microscopic traffic simulation into a unified system. It is fully generative, data-driven, scalable, and transferable to other regions. A full-pipeline implementation is conducted in Los Angeles with a 10 million population. Comprehensive validation shows that the model closely replicates real-world mobility patterns and matches the performance of legacy ABMs with significantly reduced modeling cost and greater scalability. With respect to the SCAG ABM benchmark, the origin-destination matrix achieves a cosine similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute percentage error (MAPE). When compared to real-world observations from Caltrans PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001 JSD and a 6.11% MAPE.</li>
</ul>

<h3>Title: ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling</h3>
<ul>
<li><strong>Authors: </strong>Hanlong Zhang, Jingsheng Yang, Hao Li, Yuhao He, Franck Gong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08877">https://arxiv.org/abs/2507.08877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08877">https://arxiv.org/pdf/2507.08877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08877]] ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling(https://arxiv.org/abs/2507.08877)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Function Calling is a crucial technique that enables Large Language Models (LLMs) to interact with external systems through APIs. However, the high latency associated with LLM-based Function Calling significantly impacts user experience. This paper presents a novel approach called Oriented Distillation for Inline Acceleration (ODIA) that leverages online user interaction data to accelerate Function Calling. By automatically identifying "simple queries" from production traffic and distilling knowledge from larger models to smaller ones, our method reduces response latency by 45% (expected) and 78% (median) while maintaining accuracy. We demonstrate the effectiveness of our approach through real-world deployment in a music application, where the smaller model successfully handles 60% of traffic with negligible accuracy loss. Our method requires minimal human intervention and continuously improves through automated data collection and model updating, making it a practical solution for production environments.</li>
</ul>

<h3>Title: Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Huang, Leming Shen, Zijing Ma, Yuanqing Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08878">https://arxiv.org/abs/2507.08878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08878">https://arxiv.org/pdf/2507.08878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08878]] Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models(https://arxiv.org/abs/2507.08878)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have showcased remarkable generalizability in language comprehension and hold significant potential to revolutionize human-computer interaction in smart homes. Existing LLM-based smart home assistants typically transmit user commands, along with user profiles and home configurations, to remote servers to obtain personalized services. However, users are increasingly concerned about the potential privacy leaks to the remote servers. To address this issue, we develop HomeLLaMA, an on-device assistant for privacy-preserving and personalized smart home serving with a tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to deliver satisfactory responses and enable user-friendly interactions. Once deployed, HomeLLaMA facilitates proactive interactions by continuously updating local SLMs and user profiles. To further enhance user experience while protecting their privacy, we develop PrivShield to offer an optional privacy-preserving LLM-based smart home serving for those users, who are unsatisfied with local responses and willing to send less-sensitive queries to remote servers. For evaluation, we build a comprehensive benchmark DevFinder to assess the service quality. Extensive experiments and user studies (M=100) demonstrate that HomeLLaMA can provide personalized services while significantly enhancing user privacy.</li>
</ul>

<h3>Title: SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems</h3>
<ul>
<li><strong>Authors: </strong>Wenliang Shan, Michael Fu, Rui Yang, Chakkrit (Kla)Tantithamthavorn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08898">https://arxiv.org/abs/2507.08898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08898">https://arxiv.org/pdf/2507.08898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08898]] SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems(https://arxiv.org/abs/2507.08898)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. SEALGuard advances the safety alignment of LLM systems by introducing an effective multilingual guardrail.</li>
</ul>

<h3>Title: CovertAuth: Joint Covert Communication and Authentication in MmWave Systems</h3>
<ul>
<li><strong>Authors: </strong>Yulin Teng, Keshuang Han, Pinchang Zhang, Xiaohong Jiang, Yulong Shen, Fu Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08904">https://arxiv.org/abs/2507.08904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08904">https://arxiv.org/pdf/2507.08904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08904]] CovertAuth: Joint Covert Communication and Authentication in MmWave Systems(https://arxiv.org/abs/2507.08904)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Beam alignment (BA) is a crucial process in millimeter-wave (mmWave) communications, enabling precise directional transmission and efficient link establishment. However, due to characteristics like omnidirectional exposure and the broadcast nature of the BA phase, it is particularly vulnerable to eavesdropping and identity impersonation attacks. To this end, this paper proposes a novel secure framework named CovertAuth, designed to enhance the security of the BA phase against such attacks. In particular, to combat eavesdropping attacks, the closed-form expressions of successful BA probability and covert transmission rate are first derived. Then, a covert communication problem aimed at jointly optimizing beam training budget and transmission power is formulated to maximize covert communication rate, subject to the covertness requirement. An alternating optimization algorithm combined with successive convex approximation is employed to iteratively achieve optimal results. To combat impersonation attacks, the mutual coupling effect of antenna array impairments is explored as a device feature to design a weighted-sum energy detector based physical layer authentication scheme. Moreover, theoretical models for authentication metrics like detection and false alarm probabilities are also provided to conduct performance analysis. Based on these models, an optimization problem is constructed to determine the optimal weight value that maximizes authentication accuracy. Finally, simulation results demonstrate that CovertAuth presents improved detection accuracy under the same covertness requirement compared to existing works.</li>
</ul>

<h3>Title: Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising</h3>
<ul>
<li><strong>Authors: </strong>Tomasz Szandala, Fatima Ezzeddine, Natalia Rusin, Silvia Giordano, Omran Ayoub</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08912">https://arxiv.org/abs/2507.08912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08912">https://arxiv.org/pdf/2507.08912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08912]] Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising(https://arxiv.org/abs/2507.08912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence-generated content has become increasingly popular, yet its malicious use, particularly the deepfakes, poses a serious threat to public trust and discourse. While deepfake detection methods achieve high predictive performance, they often exhibit biases across demographic attributes such as ethnicity and gender. In this work, we tackle the challenge of fair deepfake detection, aiming to mitigate these biases while maintaining robust detection capabilities. To this end, we propose a novel post-processing approach, referred to as Fairness-Oriented Final Layer Input Prioritising (Fair-FLIP), that reweights a trained model's final-layer inputs to reduce subgroup disparities, prioritising those with low variability while demoting highly variable ones. Experimental results comparing Fair-FLIP to both the baseline (without fairness-oriented de-biasing) and state-of-the-art approaches show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining baseline accuracy, with only a negligible reduction of 0.25%. Code is available on Github: this https URL</li>
</ul>

<h3>Title: Evaluating LLMs in Medicine: A Call for Rigor, Transparency</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Alwakeel, Aditya Nagori, Vijay Krishnamoorthy, Rishikesan Kamaleswaran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08916">https://arxiv.org/abs/2507.08916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08916">https://arxiv.org/pdf/2507.08916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08916]] Evaluating LLMs in Medicine: A Call for Rigor, Transparency(https://arxiv.org/abs/2507.08916)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, large language model</a></li>
<li><strong>Abstract: </strong>Objectives: To evaluate the current limitations of large language models (LLMs) in medical question answering, focusing on the quality of datasets used for their evaluation. Materials and Methods: Widely-used benchmark datasets, including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor, transparency, and relevance to clinical scenarios. Alternatives, such as challenge questions in medical journals, were also analyzed to identify their potential as unbiased evaluation tools. Results: Most existing datasets lack clinical realism, transparency, and robust validation processes. Publicly available challenge questions offer some benefits but are limited by their small size, narrow scope, and exposure to LLM training. These gaps highlight the need for secure, comprehensive, and representative datasets. Conclusion: A standardized framework is critical for evaluating LLMs in medicine. Collaborative efforts among institutions and policymakers are needed to ensure datasets and methodologies are rigorous, unbiased, and reflective of clinical complexities.</li>
</ul>

<h3>Title: Detecting Deepfake Talking Heads from Facial Biometric Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Justin D. Norman, Hany Farid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08917">https://arxiv.org/abs/2507.08917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08917">https://arxiv.org/pdf/2507.08917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08917]] Detecting Deepfake Talking Heads from Facial Biometric Anomalies(https://arxiv.org/abs/2507.08917)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>The combination of highly realistic voice cloning, along with visually compelling avatar, face-swap, or lip-sync deepfake video generation, makes it relatively easy to create a video of anyone saying anything. Today, such deepfake impersonations are often used to power frauds, scams, and political disinformation. We propose a novel forensic machine learning technique for the detection of deepfake video impersonations that leverages unnatural patterns in facial biometrics. We evaluate this technique across a large dataset of deepfake techniques and impersonations, as well as assess its reliability to video laundering and its generalization to previously unseen video deepfake generators.</li>
</ul>

<h3>Title: From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Seokhee Hong, Sunkyoung Kim, Guijin Son, Soyeon Kim, Yeonjung Hong, Jinsik Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08924">https://arxiv.org/abs/2507.08924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08924">https://arxiv.org/pdf/2507.08924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08924]] From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation(https://arxiv.org/abs/2507.08924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available.</li>
</ul>

<h3>Title: Beyond Scores: Proximal Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenghan Fang, Mateo D√≠az, Sam Buchanan, Jeremias Sulam</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08956">https://arxiv.org/abs/2507.08956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08956">https://arxiv.org/pdf/2507.08956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08956]] Beyond Scores: Proximal Diffusion Models(https://arxiv.org/abs/2507.08956)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have quickly become some of the most popular and powerful generative models for high-dimensional data. The key insight that enabled their development was the realization that access to the score -- the gradient of the log-density at different noise levels -- allows for sampling from data distributions by solving a reverse-time stochastic differential equation (SDE) via forward discretization, and that popular denoisers allow for unbiased estimators of this score. In this paper, we demonstrate that an alternative, backward discretization of these SDEs, using proximal maps in place of the score, leads to theoretical and practical benefits. We leverage recent results in proximal matching to learn proximal operators of the log-density and, with them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that $\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL divergence. Empirically, we show that two variants of ProxDM achieve significantly faster convergence within just a few sampling steps compared to conventional score-matching methods.</li>
</ul>

<h3>Title: Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Xinyu Wang, Yifan Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08959">https://arxiv.org/abs/2507.08959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08959">https://arxiv.org/pdf/2507.08959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08959]] Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign(https://arxiv.org/abs/2507.08959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In order to improve the accuracy of cross-platform advertisement recommendation, a graph neural network (GNN)- based advertisement recommendation method is analyzed. Through multi-dimensional modeling, user behavior data (e.g., click frequency, active duration) reveal temporal patterns of interest evolution, ad content (e.g., type, tag, duration) influences semantic preferences, and platform features (e.g., device type, usage context) shape the environment where interest transitions occur. These factors jointly enable the GNN to capture the latent pathways of user interest migration across platforms. The experimental results are based on the datasets of three platforms, and Platform B reaches 0.937 in AUC value, which is the best performance. Platform A and Platform C showed a slight decrease in precision and recall with uneven distribution of ad labels. By adjusting the hyperparameters such as learning rate, batch size and embedding dimension, the adaptability and robustness of the model in heterogeneous data are further improved.</li>
</ul>

<h3>Title: Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Rojas, Ye He, Chieh-Hsin Lai, Yuta Takida, Yuki Mitsufuji, Molei Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08965">https://arxiv.org/abs/2507.08965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08965">https://arxiv.org/pdf/2507.08965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08965]] Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models(https://arxiv.org/abs/2507.08965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) is a widely used technique for conditional generation and improving sample quality in continuous diffusion models, and recent works have extended it to discrete diffusion. This paper theoretically analyzes CFG in the context of masked discrete diffusion, focusing on the role of guidance schedules. Our analysis shows that high guidance early in sampling (when inputs are heavily masked) harms generation quality, while late-stage guidance has a larger effect. These findings provide a theoretical explanation for empirical observations in recent studies on guidance schedules. The analysis also reveals an imperfection of the current CFG implementations. These implementations can unintentionally cause imbalanced transitions, such as unmasking too rapidly during the early stages of generation, which degrades the quality of the resulting samples. To address this, we draw insight from the analysis and propose a novel classifier-free guidance mechanism empirically applicable to any discrete diffusion. Intuitively, our method smoothens the transport between the data distribution and the initial (masked/uniform) distribution, which results in improved sample quality. Remarkably, our method is achievable via a simple one-line code change. The efficacy of our method is empirically demonstrated with experiments on ImageNet (masked discrete diffusion) and QM9 (uniform discrete diffusion).</li>
</ul>

<h3>Title: Self-Improving Model Steering</h3>
<ul>
<li><strong>Authors: </strong>Rongyi Zhu, Yuhui Wang, Tanqiu Jiang, Jiacheng Liang, Ting Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08967">https://arxiv.org/abs/2507.08967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08967">https://arxiv.org/pdf/2507.08967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08967]] Self-Improving Model Steering(https://arxiv.org/abs/2507.08967)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model steering represents a powerful technique that dynamically aligns large language models (LLMs) with human preferences during inference. However, conventional model-steering methods rely heavily on externally annotated data, not only limiting their adaptability to varying contexts but also tethering their effectiveness to annotation quality. In this paper, we present SIMS, the first self-improving model-steering framework that operates without relying on external supervision. At its core, SIMS autonomously generates and refines contrastive samples through iterative self-improvement cycles, enabling adaptive, context-specific steering. Additionally, SIMS employs novel strategies, including prompt ranking and contrast sampling, to further enhance steering efficacy. Extensive evaluation across diverse LLMs and benchmarks demonstrates that SIMS substantially outperforms existing methods in steering effectiveness and adaptability, highlighting self-improving model steering as a promising direction for future research on inference-time LLM alignment.</li>
</ul>

<h3>Title: Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery</h3>
<ul>
<li><strong>Authors: </strong>Carson Dudley, Reiden Magdaleno, Christopher Harding, Marisa Eisenberg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08977">https://arxiv.org/abs/2507.08977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08977">https://arxiv.org/pdf/2507.08977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08977]] Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery(https://arxiv.org/abs/2507.08977)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Scientific modeling faces a core limitation: mechanistic models offer interpretability but collapse under real-world complexity, while machine learning models are flexible but require large labeled datasets, cannot infer unobservable quantities, and operate as black boxes. We introduce Simulation-Grounded Neural Networks (SGNNs), a general framework that uses mechanistic simulations as training data for neural networks. SGNNs are pretrained on synthetic corpora spanning diverse model structures, parameter regimes, stochasticity, and observational artifacts. We evaluated SGNNs across scientific disciplines and modeling tasks, and found that SGNNs achieved state-of-the-art results across settings: for prediction tasks, they nearly tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield prediction error by one third, and maintained accuracy in ecological forecasting where task specific models failed. For inference tasks, SGNNs also accurately classified the source of information spread in simulated social networks and enabled supervised learning for unobservable targets, such as estimating COVID-19 transmissibility more accurately than traditional methods even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution, a new form of mechanistic interpretability. Given real world input, SGNNs retrieve simulations based on what the model has learned to see as most similar, revealing which underlying dynamics the model believes are active. This provides process-level insight -- what the model thinks is happening -- not just which features mattered. SGNNs unify scientific theory with deep learning flexibility and unlock a new modeling paradigm -- transforming simulations from rigid, post hoc tools into flexible sources of supervision, enabling robust, interpretable inference even when ground truth is missing.</li>
</ul>

<h3>Title: Characterizing Security and Privacy Teaching Standards for Schools in the United States</h3>
<ul>
<li><strong>Authors: </strong>Katherine Limes, Nathan Malkin, Kelsey R. Fulton</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08978">https://arxiv.org/abs/2507.08978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08978">https://arxiv.org/pdf/2507.08978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08978]] Characterizing Security and Privacy Teaching Standards for Schools in the United States(https://arxiv.org/abs/2507.08978)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Increasingly, students begin learning aspects of security and privacy during their primary and secondary education (grades K-12 in the United States). Individual U.S. states and some national organizations publish teaching standards -- guidance that outlines expectations for what students should learn -- which often form the basis for course curricula. However, research has not yet examined what is covered by these standards and whether the topics align with what the broader security and privacy community thinks students should know. To shed light on these questions, we started by collecting computer science teaching standards from all U.S. states and eight national organizations. After manually examining a total of 11,954 standards, we labeled 3,778 of them as being related to security and privacy, further classifying these into 103 topics. Topics ranged from technical subjects like encryption, network security, and embedded systems to social subjects such as laws, ethics, and appropriate online behavior. Subsequently, we interviewed 11 security and privacy professionals to examine how the teaching standards align with their expectations. We found that, while the specific topics they mentioned mostly overlapped with those of existing standards, professionals placed a greater emphasis on threat modeling and security mindset.</li>
</ul>

<h3>Title: PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection</h3>
<ul>
<li><strong>Authors: </strong>Mahdiyar Molahasani, Azadeh Motamedi, Michael Greenspan, Il-Min Kim, Ali Etemad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08979">https://arxiv.org/abs/2507.08979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08979">https://arxiv.org/pdf/2507.08979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08979]] PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection(https://arxiv.org/abs/2507.08979)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text this http URL experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: this https URL.</li>
</ul>

<h3>Title: Learning Diffusion Models with Flexible Representation Guidance</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Wang, Cai Zhou, Sharut Gupta, Zongyu Lin, Stefanie Jegelka, Stephen Bates, Tommi Jaakkola</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08980">https://arxiv.org/abs/2507.08980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08980">https://arxiv.org/pdf/2507.08980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08980]] Learning Diffusion Models with Flexible Representation Guidance(https://arxiv.org/abs/2507.08980)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at this https URL.</li>
</ul>

<h3>Title: Video Inference for Human Mesh Recovery with Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Hanbyel Cho, Jaesung Ahn, Yooshin Cho, Junmo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08981">https://arxiv.org/abs/2507.08981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08981">https://arxiv.org/pdf/2507.08981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08981]] Video Inference for Human Mesh Recovery with Vision Transformer(https://arxiv.org/abs/2507.08981)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Human Mesh Recovery (HMR) from an image is a challenging problem because of the inherent ambiguity of the task. Existing HMR methods utilized either temporal information or kinematic relationships to achieve higher accuracy, but there is no method using both. Hence, we propose "Video Inference for Human Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic Feature Image is constructed using feature vectors obtained from video frames by an image encoder. When generating the feature image, we use a Channel Rearranging Matrix (CRM) so that similar kinematic features could be located spatially close together. The feature image is then further encoded using Vision Transformer, and the SMPL pose and shape parameters are finally inferred using a regression network. Extensive evaluation on the 3DPW and Human3.6M datasets indicates that our method achieves a competitive performance in HMR.</li>
</ul>

<h3>Title: Exploiting Leaderboards for Large-Scale Distribution of Malicious Models</h3>
<ul>
<li><strong>Authors: </strong>Anshuman Suri, Harsh Chaudhari, Yuefeng Peng, Ali Naseh, Amir Houmansadr, Alina Oprea</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08983">https://arxiv.org/abs/2507.08983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08983">https://arxiv.org/pdf/2507.08983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08983]] Exploiting Leaderboards for Large-Scale Distribution of Malicious Models(https://arxiv.org/abs/2507.08983)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>While poisoning attacks on machine learning models have been extensively studied, the mechanisms by which adversaries can distribute poisoned models at scale remain largely unexplored. In this paper, we shed light on how model leaderboards -- ranked platforms for model discovery and evaluation -- can serve as a powerful channel for adversaries for stealthy large-scale distribution of poisoned models. We present TrojanClimb, a general framework that enables injection of malicious behaviors while maintaining competitive leaderboard performance. We demonstrate its effectiveness across four diverse modalities: text-embedding, text-generation, text-to-speech and text-to-image, showing that adversaries can successfully achieve high leaderboard rankings while embedding arbitrary harmful functionalities, from backdoors to bias injection. Our findings reveal a significant vulnerability in the machine learning ecosystem, highlighting the urgent need to redesign leaderboard evaluation mechanisms to detect and filter malicious (e.g., poisoned) models, while exposing broader security implications for the machine learning community regarding the risks of adopting models from unverified sources.</li>
</ul>

<h3>Title: VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels</h3>
<ul>
<li><strong>Authors: </strong>Xiwei Xuan, Xiaoqi Wang, Wenbin He, Jorge Piazentin Ono, Liang Gou, Kwan-Liu Ma, Liu Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09008">https://arxiv.org/abs/2507.09008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09008">https://arxiv.org/pdf/2507.09008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09008]] VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels(https://arxiv.org/abs/2507.09008)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA's effectiveness from both quantitative and qualitative perspectives.</li>
</ul>

<h3>Title: Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography</h3>
<ul>
<li><strong>Authors: </strong>Zhengxiao He, Huayu Li, Geng Yuan, William D.S. Killgore, Stuart F. Quan, Chen X. Chen, Ao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09009">https://arxiv.org/abs/2507.09009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09009">https://arxiv.org/pdf/2507.09009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09009]] Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography(https://arxiv.org/abs/2507.09009)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Methods: We developed a self-supervised deep learning model that extracts meaningful patterns from multi-modal signals (Electroencephalography (EEG), Electrocardiography (ECG), and respiratory signals). The model was trained on data from 4,398 participants. Projection scores were derived by contrasting embeddings from individuals with and without CVD outcomes. External validation was conducted in an independent cohort with 1,093 participants. The source code is available on this https URL. Results: The projection scores revealed distinct and clinically meaningful patterns across modalities. ECG-derived features were predictive of both prevalent and incident cardiac conditions, particularly CVD mortality. EEG-derived features were predictive of incident hypertension and CVD mortality. Respiratory signals added complementary predictive value. Combining these projection scores with the Framingham Risk Score consistently improved predictive performance, achieving area under the curve values ranging from 0.607 to 0.965 across different outcomes. Findings were robustly replicated and validated in the external testing cohort. Conclusion: Our findings demonstrate that the proposed framework can generate individualized CVD risk scores directly from PSG data. The resulting projection scores have the potential to be integrated into clinical practice, enhancing risk assessment and supporting personalized care.</li>
</ul>

<h3>Title: Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery</h3>
<ul>
<li><strong>Authors: </strong>Ana Chkhaidze, Reshanne R. Reeder, Connor Gag, Anastasia Kiyonaga, Seana Coulson</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09011">https://arxiv.org/abs/2507.09011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09011">https://arxiv.org/pdf/2507.09011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09011]] Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery(https://arxiv.org/abs/2507.09011)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A rapidly alternating red and black display known as Ganzflicker induces visual hallucinations that reflect the generative capacity of the visual system. Recent proposals regarding the imagery spectrum, that is, differences in the visual system of individuals with absent imagery, typical imagery, and vivid imagery, suggest these differences should impact the complexity of other internally generated visual experiences. Here, we used tools from natural language processing to analyze free-text descriptions of hallucinations from over 4,000 participants, asking whether people with different imagery phenotypes see different things in their mind's eye during Ganzflicker-induced hallucinations. Strong imagers described complex, naturalistic content, while weak imagers reported simple geometric patterns. Embeddings from vision language models better captured these differences than text-only language models, and participants with stronger imagery used language with richer sensorimotor associations. These findings may reflect individual variation in coordination between early visual areas and higher-order regions relevant for the imagery spectrum.</li>
</ul>

<h3>Title: On Evaluating Performance of LLM Inference Serving Systems</h3>
<ul>
<li><strong>Authors: </strong>Amey Agrawal, Nitin Kedia, Anmol Agarwal, Jayashree Mohan, Nipun Kwatra, Souvik Kundu, Ramachandran Ramjee, Alexey Tumanov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09019">https://arxiv.org/abs/2507.09019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09019">https://arxiv.org/pdf/2507.09019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09019]] On Evaluating Performance of LLM Inference Serving Systems(https://arxiv.org/abs/2507.09019)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Language Model (LLM) inference systems has yielded significant efficiency improvements. However, our systematic analysis reveals that current evaluation methodologies frequently exhibit fundamental flaws, often manifesting as common evaluation anti-patterns that obscure true performance characteristics and impede scientific progress. Through a comprehensive examination of recent systems, we identify recurring anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup, and Metric Design. These anti-patterns are uniquely problematic for LLM inference due to its dual-phase nature combining distinct prefill and decode operations, its handling of highly heterogeneous workloads, and its strict temporal requirements for interactive use. We demonstrate how common anti-patterns -- such as inadequate baseline comparisons that conflate engineering effort with algorithmic novelty, workload selections that fail to represent production scenarios, and metric normalizations that hide substantial performance variability like generation stalls-lead to misleading conclusions. To address these challenges, we provide a comprehensive checklist derived from our analysis, establishing a framework for recognizing and avoiding these anti-patterns in favor of robust LLM inference evaluation. To demonstrate the practical application of our framework, we present a case study analyzing speculative decoding, a technique whose bursty, non-uniform token generation is easily misinterpreted when evaluated using approaches characteristic of these anti-patterns. Our work establishes a rigorous foundation for evaluation methodology, enabling meaningful comparisons, ensuring reproducible results, and ultimately accelerating genuine progress in LLM inference systems by moving beyond common anti-patterns to align evaluation with real-world requirements.</li>
</ul>

<h3>Title: SSH-Passkeys: Leveraging Web Authentication for Passwordless SSH</h3>
<ul>
<li><strong>Authors: </strong>Moe Kayali, Jonas Schmitt, Franziska Roesner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09022">https://arxiv.org/abs/2507.09022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09022">https://arxiv.org/pdf/2507.09022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09022]] SSH-Passkeys: Leveraging Web Authentication for Passwordless SSH(https://arxiv.org/abs/2507.09022)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>We propose a method for using Web Authentication APIs for SSH authentication, enabling passwordless remote server login with passkeys. These are credentials that are managed throughout the key lifecycle by an authenticator on behalf of the user and offer strong security guarantees. Passwords remain the dominant mode of SSH authentication, despite their well known flaws such as phishing and reuse. SSH's custom key-based authentication protocol can alleviate these issues but remains vulnerable to key theft. Additionally, it has poor usability, with even knowledgeable users leaking key material and failing to verify fingerprints. Hence, effective key management remains a critical open area in SSH security. In contrast, WebAuthn is a modern authentication standard designed to replace passwords, managing keys on behalf of the user. As a web API, this standard cannot integrate with SSH directly. We propose a framework to integrate WebAuthn with SSH servers, by using UNIX pluggable authentication modules (PAM). Our approach is backwards-compatible, supports stock SSH servers and requires no new software client-side. It offers protection for cryptographic material at rest, resistance to key leaks, phishing protection, privacy protection and attestation capability. None of these properties are offered by passwords nor traditional SSH keys. We validate these advantages with a structured, conceptual security analysis. We develop a prototype implementation and conduct a user study to quantify the security advantages of our proposal, testing our prototype with 40 SSH users. The study confirms the security problems of SSH-keys, including 20% of the cohort leaking their private keys. Our SSH-passkeys effectively address these problems: we find a 90% reduction in critical security errors, while reducing authentication time by 4x on average.</li>
</ul>

<h3>Title: Lizard: An Efficient Linearization Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chien Van Nguyen, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Viet Dac Lai, Haoliang Wang, Jayakumar Subramanian, Ryan A. Rossi, Trung Bui, Nikos Vlassis, Franck Dernoncourt, Thien Huu Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09025">https://arxiv.org/abs/2507.09025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09025">https://arxiv.org/pdf/2507.09025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09025]] Lizard: An Efficient Linearization Framework for Large Language Models(https://arxiv.org/abs/2507.09025)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.</li>
</ul>

<h3>Title: Model Parallelism With Subnetwork Data Parallelism</h3>
<ul>
<li><strong>Authors: </strong>Vaibhav Singh, Zafir Khalid, Edouard Oyallon, Eugene Belilovsky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09029">https://arxiv.org/abs/2507.09029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09029">https://arxiv.org/pdf/2507.09029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09029]] Model Parallelism With Subnetwork Data Parallelism(https://arxiv.org/abs/2507.09029)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Distributed pre-training of large models at scale often imposes heavy memory demands on individual nodes and incurs significant intra-node communication costs. We propose a novel alternative approach that reduces the memory requirements by training small, structured subnetworks of the model on separate workers. Unlike pipelining, our method avoids inter-node activation communication and maintains bandwidth requirements that are comparable to or lower than standard data parallel communication schemes based on all-reduce. We evaluate two subnetwork construction strategies guided by the principle of ensuring uniform representation of each parameter across the distributed training setup. Our results show that the stochastic block dropping technique consistently outperforms the width-wise subnetwork construction previously explored in federated learning. We empirically attribute this superior performance to stronger gradient alignment in subnetworks that retain blocks having skip connections. Preliminary experiments highlight the promise of our approach, achieving a 20-40% reduction in memory usage without any loss in performance.</li>
</ul>

<h3>Title: Confounder-Free Continual Learning via Recursive Feature Normalization</h3>
<ul>
<li><strong>Authors: </strong>Yash Shah, Camila Gonzalez, Mohammad H. Abbasi, Qingyu Zhao, Kilian M. Pohl, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09031">https://arxiv.org/abs/2507.09031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09031">https://arxiv.org/pdf/2507.09031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09031]] Confounder-Free Continual Learning via Recursive Feature Normalization(https://arxiv.org/abs/2507.09031)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Confounders are extraneous variables that affect both the input and the target, resulting in spurious correlations and biased predictions. There are recent advances in dealing with or removing confounders in traditional models, such as metadata normalization (MDN), where the distribution of the learned features is adjusted based on the study confounders. However, in the context of continual learning, where a model learns continuously from new data over time without forgetting, learning feature representations that are invariant to confounders remains a significant challenge. To remove their influence from intermediate feature representations, we introduce the Recursive MDN (R-MDN) layer, which can be integrated into any deep learning architecture, including vision transformers, and at any model stage. R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables. Our experiments demonstrate that R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time.</li>
</ul>

<h3>Title: BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Florian Kofler, Marcel Rosier, Mehdi Astaraki, Hendrik M√∂ller, Ilhem Isra Mekki, Josef A. Buchner, Anton Schmick, Arianna Pfiffer, Eva Oswald, Lucas Zimmer, Ezequiel de la Rosa, Sarthak Pati, Julian Canisius, Arianna Piffer, Ujjwal Baid, Mahyar Valizadeh, Akis Linardos, Jan C. Peeken, Surprosanna Shit, Felix Steinbauer, Daniel Rueckert, Rolf Heckemann, Spyridon Bakas, Jan Kirschke, Constantin von See, Ivan Ezhov, Marie Piraud, Benedikt Wiestler, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09036">https://arxiv.org/abs/2507.09036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09036">https://arxiv.org/pdf/2507.09036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09036]] BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis(https://arxiv.org/abs/2507.09036)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>BrainLesion Suite is a versatile toolkit for building modular brain lesion image analysis pipelines in Python. Following Pythonic principles, BrainLesion Suite is designed to provide a 'brainless' development experience, minimizing cognitive effort and streamlining the creation of complex workflows for clinical and scientific practice. At its core is an adaptable preprocessing module that performs co-registration, atlas registration, and optional skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion Suite leverages algorithms from the BraTS challenge to synthesize missing modalities, inpaint lesions, and generate pathology-specific tumor segmentations. BrainLesion Suite also enables quantifying segmentation model performance, with tools such as panoptica to compute lesion-wise metrics. Although BrainLesion Suite was originally developed for image analysis pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis, it can be adapted for other biomedical image analysis applications. The individual BrainLesion Suite packages and tutorials are accessible on GitHub.</li>
</ul>

<h3>Title: ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Bharadwaj Ravichandran, David Joy, Paul Elliott, Brian Hu, Jadie Adams, Christopher Funk, Emily Veenhuis, Anthony Hoogs, Arslan Basharat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09037">https://arxiv.org/abs/2507.09037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09037">https://arxiv.org/pdf/2507.09037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09037]] ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making(https://arxiv.org/abs/2507.09037)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.</li>
</ul>

<h3>Title: Behavioral Exploration: Learning to Explore via In-Context Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Andrew Wagenmaker, Zhiyuan Zhou, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09041">https://arxiv.org/abs/2507.09041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09041">https://arxiv.org/pdf/2507.09041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09041]] Behavioral Exploration: Learning to Explore via In-Context Adaptation(https://arxiv.org/abs/2507.09041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing autonomous agents that quickly explore an environment and adapt their behavior online is a canonical challenge in robotics and machine learning. While humans are able to achieve such fast online exploration and adaptation, often acquiring new information and skills in only a handful of interactions, existing algorithmic approaches tend to rely on random exploration and slow, gradient-based behavior updates. How can we endow autonomous agents with such capabilities on par with humans? Taking inspiration from recent progress on both in-context learning and large-scale behavioral cloning, in this work we propose behavioral exploration: training agents to internalize what it means to explore and adapt in-context over the space of ``expert'' behaviors. To achieve this, given access to a dataset of expert demonstrations, we train a long-context generative model to predict expert actions conditioned on a context of past observations and a measure of how ``exploratory'' the expert's behaviors are relative to this context. This enables the model to not only mimic the behavior of an expert, but also, by feeding its past history of interactions into its context, to select different expert behaviors than what have been previously selected, thereby allowing for fast online adaptation and targeted, ``expert-like'' exploration. We demonstrate the effectiveness of our method in both simulated locomotion and manipulation settings, as well as on real-world robotic manipulation tasks, illustrating its ability to learn adaptive, exploratory behavior.</li>
</ul>

<h3>Title: Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingxiang Qu, Wenhan Gao, Yi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09043">https://arxiv.org/abs/2507.09043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09043">https://arxiv.org/pdf/2507.09043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09043]] Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation(https://arxiv.org/abs/2507.09043)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gaussian-based Probabilistic Generative Models (GPGMs) generate data by reversing a stochastic process that progressively corrupts samples with Gaussian noise. While these models have achieved state-of-the-art performance across diverse domains, their practical deployment remains constrained by the high computational cost of long generative trajectories, which often involve hundreds to thousands of steps during training and sampling. In this work, we introduce a theoretically grounded and empirically validated framework that improves generation efficiency without sacrificing training granularity or inference fidelity. Our key insight is that for certain data modalities, the noising process causes data to rapidly lose its identity and converge toward a Gaussian distribution. We analytically identify a characteristic step at which the data has acquired sufficient Gaussianity, and then replace the remaining generation trajectory with a closed-form Gaussian approximation. Unlike existing acceleration techniques that coarsening the trajectories by skipping steps, our method preserves the full resolution of learning dynamics while avoiding redundant stochastic perturbations between `Gaussian-like' distributions. Empirical results across multiple data modalities demonstrate substantial improvements in both sample quality and computational efficiency.</li>
</ul>

<h3>Title: Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?</h3>
<ul>
<li><strong>Authors: </strong>Fang Chen, Alex Villa, Gongbo Liang, Xiaoyi Lu, Meng Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09052">https://arxiv.org/abs/2507.09052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09052">https://arxiv.org/pdf/2507.09052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09052]] Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?(https://arxiv.org/abs/2507.09052)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity of tail class images without compromising the fidelity and diversity of head class images. We achieve this by introducing two deceptively simple but highly effective contrastive loss functions. Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to increase the distance/dissimilarity among synthetic images, particularly for tail classes. To further enhance the diversity of tail classes, our second loss is an MSE loss that contrasts class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. Conditional-unconditional alignment has been shown to enhance the performance of long-tailed GAN. We are the first to adapt such alignment to diffusion models. We successfully leveraged contrastive learning for class-imbalanced diffusion models. Our contrastive learning framework is easy to implement and outperforms standard DDPM and alternative methods for class-imbalanced diffusion models across various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.</li>
</ul>

<h3>Title: Infinite Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09068">https://arxiv.org/abs/2507.09068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09068">https://arxiv.org/pdf/2507.09068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09068]] Infinite Video Understanding(https://arxiv.org/abs/2507.09068)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.</li>
</ul>

<h3>Title: Favicon Trojans: Executable Steganography Via Ico Alpha Channel Exploitation</h3>
<ul>
<li><strong>Authors: </strong>David Noever, Forrest McKee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09074">https://arxiv.org/abs/2507.09074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09074">https://arxiv.org/pdf/2507.09074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09074]] Favicon Trojans: Executable Steganography Via Ico Alpha Channel Exploitation(https://arxiv.org/abs/2507.09074)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>This paper presents a novel method of executable steganography using the alpha transparency layer of ICO image files to embed and deliver self-decompressing JavaScript payloads within web browsers. By targeting the least significant bit (LSB) of non-transparent alpha layer image values, the proposed method successfully conceals compressed JavaScript code inside a favicon image without affecting visual fidelity. Global web traffic loads 294 billion favicons daily and consume 0.9 petabytes of network bandwidth. A proof-of-concept implementation demonstrates that a 64x64 ICO image can embed up to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold compression. On page load, a browser fetches the favicon as part of standard behavior, allowing an embedded loader script to extract and execute the payload entirely in memory using native JavaScript APIs and canvas pixel access. This creates a two-stage covert channel requiring no additional network or user requests. Testing across multiple browsers in both desktop and mobile environments confirms successful and silent execution of the embedded script. We evaluate the threat model, relate it to polymorphic phishing attacks that evade favicon-based detection, and analyze evasion of content security policies and antivirus scanners. We map nine example MITRE ATT&CK Framework objectives to single line JavaScript to execute arbitrarily in ICO files. Existing steganalysis and sanitization defenses are discussed, highlighting limitations in detecting or neutralizing alpha-channel exploits. The results demonstrate a stealthy and reusable attack surface that blurs traditional boundaries between static images and executable content. Because modern browsers report silent errors when developers specifically fail to load ICO files, this attack surface offers an interesting example of required web behaviors that in turn compromise security.</li>
</ul>

<h3>Title: OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique</h3>
<ul>
<li><strong>Authors: </strong>Wasi Uddin Ahmad, Somshubra Majumdar, Aleksander Ficek, Sean Narenthiran, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Vahid Noroozi, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09075">https://arxiv.org/abs/2507.09075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09075">https://arxiv.org/pdf/2507.09075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09075]] OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique(https://arxiv.org/abs/2507.09075)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a dataset consists of 2.5M question-solution-critique triples (approx. 35K unique programming questions), making it nearly twice the size of the previous largest publicly available code reasoning dataset. In this work, we employ a two-stage supervised fine-tuning strategy. The first stage focuses on fine-tuning for code generation, while the second stage involves the joint training of models for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct models achieve performance in code generation that either exceeds or equals the best prior open-weight distilled models. Notably, the integration of our code generation and critique models leads to significant improvements in competitive coding performance. Furthermore, we present an extension of the LiveCodeBench benchmark to specifically support the C++ programming language, thereby facilitating more comprehensive LLM evaluation using this benchmark.</li>
</ul>

<h3>Title: Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation</h3>
<ul>
<li><strong>Authors: </strong>Jialong Mai, Xiaofen Xing, Yawei Li, Zhipeng Li, Jingyuan Xing, Xiangmin Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09076">https://arxiv.org/abs/2507.09076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09076">https://arxiv.org/pdf/2507.09076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09076]] Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation(https://arxiv.org/abs/2507.09076)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively "memorize" the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Yu, Mohd Yamani Idna Idris, Hua Wang, Pei Wang, Junyi Chen, Kun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09081">https://arxiv.org/abs/2507.09081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09081">https://arxiv.org/pdf/2507.09081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09081]] From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion(https://arxiv.org/abs/2507.09081)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Quantitative remote sensing inversion aims to estimate continuous surface variables-such as biomass, vegetation indices, and evapotranspiration-from satellite observations, supporting applications in ecosystem monitoring, carbon accounting, and land management. With the evolution of remote sensing systems and artificial intelligence, traditional physics-based paradigms are giving way to data-driven and foundation model (FM)-based approaches. This paper systematically reviews the methodological evolution of inversion techniques, from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods (e.g., deep learning, multimodal fusion), and further to foundation models (e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application scenarios, and limitations of each paradigm, with emphasis on recent FM advances in self-supervised pretraining, multi-modal integration, and cross-task adaptation. We also highlight persistent challenges in physical interpretability, domain generalization, limited supervision, and uncertainty quantification. Finally, we envision the development of next-generation foundation models for remote sensing inversion, emphasizing unified modeling capacity, cross-domain generalization, and physical interpretability.</li>
</ul>

<h3>Title: Taming generative video models for zero-shot optical flow extraction</h3>
<ul>
<li><strong>Authors: </strong>Seungwoo Kim, Khai Loong Aw, Klemen Kotar, Cristobal Eyzaguirre, Wanhee Lee, Yunong Liu, Jared Watrous, Stefan Stojanov, Juan Carlos Niebles, Jiajun Wu, Daniel L. K. Yamins</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09082">https://arxiv.org/abs/2507.09082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09082">https://arxiv.org/pdf/2507.09082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09082]] Taming generative video models for zero-shot optical flow extraction(https://arxiv.org/abs/2507.09082)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow.</li>
</ul>

<h3>Title: On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Md Hasan Shahriar, Md Mohaimin Al Barat, Harshavardhan Sundar, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09095">https://arxiv.org/abs/2507.09095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09095">https://arxiv.org/pdf/2507.09095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09095]] On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving(https://arxiv.org/abs/2507.09095)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, a novel attack that exploits network-induced delays to create subtle temporal misalignments across sensor streams, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals these sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense patch that can work alongside the existing perception model to monitor temporal alignment through cross-modal temporal consistency. AION leverages multimodal shared representation learning and dynamic time warping to determine the path of temporal alignment and calculate anomaly scores based on the alignment. Our thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with low false positives across datasets and model architectures, demonstrating it as a robust and generalized defense against the temporal misalignment attacks.</li>
</ul>

<h3>Title: S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe</h3>
<ul>
<li><strong>Authors: </strong>Yanan Cao, Omid Memarrast, Shiqin Cai, Sinduja Subramaniam, Evren Korpeoglu, Kannan Achan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09101">https://arxiv.org/abs/2507.09101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09101">https://arxiv.org/pdf/2507.09101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09101]] S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe(https://arxiv.org/abs/2507.09101)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In grocery e-commerce, customers often build ingredient baskets guided by dietary preferences but lack the expertise to create complete meals. Leveraging recipe knowledge to recommend complementary ingredients based on a partial basket is essential for improving the culinary experience. Traditional recipe completion methods typically predict a single missing ingredient using a leave-one-out strategy. However, they fall short in two key aspects: (i) they do not reflect real-world scenarios where multiple ingredients are often needed, and (ii) they overlook relationships among the missing ingredients themselves. To address these limitations, we reformulate basket completion as a set-to-set (S2S) recommendation problem, where an incomplete basket is input into a system that predicts a set of complementary ingredients. We introduce S2SRec2, a set-to-set ingredient recommendation framework based on a Set Transformer and trained in a multitask learning paradigm. S2SRec2 jointly learns to (i) retrieve missing ingredients from the representation of existing ones and (ii) assess basket completeness after prediction. These tasks are optimized together, enforcing accurate retrieval and coherent basket completion. Experiments on large-scale recipe datasets and qualitative analyses show that S2SRec2 significantly outperforms single-target baselines, offering a promising approach to enhance grocery shopping and inspire culinary creativity.</li>
</ul>

<h3>Title: Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Chen, Shanshan Zhao, Lunhao Duan, Changxing Ding, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09102">https://arxiv.org/abs/2507.09102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09102">https://arxiv.org/pdf/2507.09102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09102]] Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning(https://arxiv.org/abs/2507.09102)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based models, widely used in text-to-image generation, have proven effective in 2D representation learning. Recently, this framework has been extended to 3D self-supervised learning by constructing a conditional point generator for enhancing 3D representations. However, its performance remains constrained by the 3D diffusion model, which is trained on the available 3D datasets with limited size. We hypothesize that the robust capabilities of text-to-image diffusion models, particularly Stable Diffusion (SD), which is trained on large-scale datasets, can help overcome these limitations. To investigate this hypothesis, we propose PointSD, a framework that leverages the SD model for 3D self-supervised learning. By replacing the SD model's text encoder with a 3D encoder, we train a point-to-image diffusion model that allows point clouds to guide the denoising of rendered noisy images. With the trained point-to-image diffusion model, we use noise-free images as the input and point clouds as the condition to extract SD features. Next, we train a 3D backbone by aligning its features with these SD features, thereby facilitating direct semantic learning. Comprehensive experiments on downstream point cloud tasks and ablation studies demonstrate that the SD model can enhance point cloud self-supervised learning. Code is publicly available at this https URL.</li>
</ul>

<h3>Title: CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Maosong Cao, Alexander Lam, Songyang Zhang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09104">https://arxiv.org/abs/2507.09104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09104">https://arxiv.org/pdf/2507.09104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09104]] CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards(https://arxiv.org/abs/2507.09104)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards.</li>
</ul>

<h3>Title: Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production</h3>
<ul>
<li><strong>Authors: </strong>Maoxiao Ye, Xinfeng Ye, Mano Manoharan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09105">https://arxiv.org/abs/2507.09105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09105">https://arxiv.org/pdf/2507.09105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09105]] Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production(https://arxiv.org/abs/2507.09105)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Earlier Sign Language Production (SLP) models typically relied on autoregressive methods that generate output tokens one by one, which inherently provide temporal alignment. Although techniques like Teacher Forcing can prevent model collapse during training, they still cannot solve the problem of error accumulation during inference, since ground truth is unavailable at that stage. In contrast, more recent approaches based on diffusion models leverage step-by-step denoising to enable high-quality generation. However, the iterative nature of these models and the requirement to denoise entire sequences limit their applicability in real-time tasks like SLP. To address it, we apply a hybrid approach combining autoregressive and diffusion models to SLP for the first time, leveraging the strengths of both models in sequential dependency modeling and output refinement. To capture fine-grained body movements, we design a Multi-Scale Pose Representation module that separately extracts detailed features from distinct articulators and integrates them via a Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal Attention mechanism that utilizes joint-level confidence scores to dynamically guide the pose generation process, improving accuracy and robustness. Extensive experiments on the PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method in both generation quality and real-time streaming efficiency.</li>
</ul>

<h3>Title: RoHOI: Robustness Benchmark for Human-Object Interaction Detection</h3>
<ul>
<li><strong>Authors: </strong>Di Wen, Kunyu Peng, Kailun Yang, Yufan Chen, Ruiping Liu, Junwei Zheng, Alina Roitberg, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.RO, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09111">https://arxiv.org/abs/2507.09111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09111">https://arxiv.org/pdf/2507.09111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09111]] RoHOI: Robustness Benchmark for Human-Object Interaction Detection(https://arxiv.org/abs/2507.09111)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human-Object Interaction (HOI) detection is crucial for robot-human assistance, enabling context-aware support. However, models trained on clean datasets degrade in real-world conditions due to unforeseen corruptions, leading to inaccurate prediction. To address this, we introduce the first robustness benchmark for HOI detection, evaluating model resilience under diverse challenges. Despite advances, current models struggle with environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes 20 corruption types based on HICO-DET and V-COCO datasets and a new robustness-focused metric. We systematically analyze existing models in the related field, revealing significant performance drops under corruptions. To improve robustness, we propose a Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to guide the model to be optimized based on holistic and partial cues, dynamically adjusting the model's optimization to enhance robust feature learning. Extensive experiments show our approach outperforms state-of-the-art methods, setting a new standard for robust HOI detection. Benchmarks, datasets, and code will be made publicly available at this https URL.</li>
</ul>

<h3>Title: SnapMoGen: Human Motion Generation from Expressive Texts</h3>
<ul>
<li><strong>Authors: </strong>Chuan Guo, Inwoo Hwang, Jian Wang, Bing Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09122">https://arxiv.org/abs/2507.09122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09122">https://arxiv.org/pdf/2507.09122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09122]] SnapMoGen: Human Motion Generation from Expressive Texts(https://arxiv.org/abs/2507.09122)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Text-to-motion generation has experienced remarkable progress in recent years. However, current approaches remain limited to synthesizing motion from short or general text prompts, primarily due to dataset constraints. This limitation undermines fine-grained controllability and generalization to unseen prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset featuring high-quality motion capture data paired with accurate, expressive textual annotations. The dataset comprises 20K motion clips totaling 44 hours, accompanied by 122K detailed textual descriptions averaging 48 words per description (vs. 12 words of HumanML3D). Importantly, these motion clips preserve original temporal continuity as they were in long sequences, facilitating research in long-term motion generation and blending. We also improve upon previous generative masked modeling approaches. Our model, MoMask++, transforms motion into multi-scale token sequences that better exploit the token capacity, and learns to generate all tokens using a single generative masked transformer. MoMask++ achieves state-of-the-art performance on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the ability to process casual user prompts by employing an LLM to reformat inputs to align with the expressivity and narration style of SnapMoGen. Project webpage: this https URL</li>
</ul>

<h3>Title: Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning</h3>
<ul>
<li><strong>Authors: </strong>Chu-Yuan Wei, Shun-Yao Liu, Sheng-Da Zhuo, Chang-Dong Wang, Shu-Qiang Huang, Mohsen Guizani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09132">https://arxiv.org/abs/2507.09132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09132">https://arxiv.org/pdf/2507.09132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09132]] Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning(https://arxiv.org/abs/2507.09132)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based tasks (e.g., node classification or link prediction). Despite their triumphs, GNNs still face challenges such as long training and inference times, difficulty in capturing complex relationships, and insufficient feature extraction. To tackle these issues, graph pre-training and graph prompt methods have garnered increasing attention for their ability to leverage large-scale datasets for initial learning and task-specific adaptation, offering potential improvements in GNN performance. However, previous research has overlooked the potential of graph prompts in optimizing models, as well as the impact of both positive and negative graph prompts on model stability and efficiency. To bridge this gap, we propose a novel framework combining graph prompts with weight pruning, called GPAWP, which aims to enhance the performance and efficiency of graph prompts by using fewer of them. We evaluate the importance of graph prompts using an importance assessment function to determine positive and negative weights at different granularities. Through hierarchically structured pruning, we eliminate negative prompt labels, resulting in more parameter-efficient and competitively performing prompts. Extensive experiments on three benchmark datasets demonstrate the superiority of GPAWP, leading to a significant reduction in parameters in node classification tasks.</li>
</ul>

<h3>Title: CLIProv: A Contrastive Log-to-Intelligence Multimodal Approach for Threat Detection and Provenance Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jingwen Li, Ru Zhang, Jianyi Liu, Wanguo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09133">https://arxiv.org/abs/2507.09133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09133">https://arxiv.org/pdf/2507.09133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09133]] CLIProv: A Contrastive Log-to-Intelligence Multimodal Approach for Threat Detection and Provenance Analysis(https://arxiv.org/abs/2507.09133)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the increasing complexity of cyberattacks, the proactive and forward-looking nature of threat intelligence has become more crucial for threat detection and provenance analysis. However, translating high-level attack patterns described in Tactics, Techniques, and Procedures (TTP) intelligence into actionable security policies remains a significant challenge. This challenge arises from the semantic gap between high-level threat intelligence and low-level provenance log. To address this issue, this paper introduces CLIProv, a novel approach for detecting threat behaviors in a host system. CLIProv employs a multimodal framework that leverages contrastive learning to align the semantics of provenance logs with threat intelligence, effectively correlating system intrusion activities with attack patterns. Furthermore, CLIProv formulates threat detection as a semantic search problem, identifying attack behaviors by searching for threat intelligence that is most semantically similar to the log sequence. By leveraging attack pattern information in threat intelligence, CLIProv identifies TTPs and generates complete and concise attack scenarios. Experimental evaluations on standard datasets show that CLIProv effectively identifies attack behaviors in system provenance logs, offering valuable references for potential techniques. Compared to state-of-the-art methods, CLIProv achieves higher precision and significantly improved detection efficiency.</li>
</ul>

<h3>Title: POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution</h3>
<ul>
<li><strong>Authors: </strong>Nripsuta Ani Saxena, Shang-Ling Hsu, Mehul Shetty, Omar Alkhadra, Cyrus Shahabi, Abigail L. Horn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09137">https://arxiv.org/abs/2507.09137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09137">https://arxiv.org/pdf/2507.09137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09137]] POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution(https://arxiv.org/abs/2507.09137)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurately attributing user visits to specific Points of Interest (POIs) is a foundational task for mobility analytics, personalized services, marketing and urban planning. However, POI attribution remains challenging due to GPS inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and the high spatial density of POIs in urban environments, where multiple venues can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius in dense city centers). Relying on proximity is therefore often insufficient for determining which POI was actually visited. We introduce \textsf{POIFormer}, a novel Transformer-based framework for accurate and efficient POI attribution. Unlike prior approaches that rely on limited spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly models a rich set of signals, including spatial proximity, visit timing and duration, contextual features from POI semantics, and behavioral features from user mobility and aggregated crowd behavior patterns--using the Transformer's self-attention mechanism to jointly model complex interactions across these dimensions. By leveraging the Transformer to model a user's past and future visits (with the current visit masked) and incorporating crowd-level behavioral patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate, efficient attribution in large, noisy mobility datasets. Its architecture supports generalization across diverse data sources and geographic contexts while avoiding reliance on hard-to-access or unavailable data layers, making it practical for real-world deployment. Extensive experiments on real-world mobility datasets demonstrate significant improvements over existing baselines, particularly in challenging real-world settings characterized by spatial noise and dense POI clustering.</li>
</ul>

<h3>Title: PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment</h3>
<ul>
<li><strong>Authors: </strong>Dewen Zhang, Tahir Hussain, Wangpeng An, Hayaru Shouno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09139">https://arxiv.org/abs/2507.09139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09139">https://arxiv.org/pdf/2507.09139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09139]] PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment(https://arxiv.org/abs/2507.09139)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human pose estimation traditionally relies on architectures that encode keypoint priors, limiting their generalization to novel poses or unseen keypoints. Recent language-guided approaches like LocLLM reformulate keypoint localization as a vision-language task, enabling zero-shot generalization through textual descriptions. However, LocLLM's linear projector fails to capture complex spatial-textual interactions critical for high-precision localization. To address this, we propose PoseLLM, the first Large Language Model (LLM)-based pose estimation framework that replaces the linear projector with a nonlinear MLP vision-language connector. This lightweight two-layer MLP with GELU activation enables hierarchical cross-modal feature transformation, enhancing the fusion of visual patches and textual keypoint descriptions. Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO validation set, outperforming LocLLM by +0.4 AP, while maintaining strong zero-shot generalization on Human-Art and MPII. Our work demonstrates that a simple yet powerful nonlinear connector significantly boosts localization accuracy without sacrificing generalization, advancing the state-of-the-art in language-guided pose estimation. Code is available at this https URL.</li>
</ul>

<h3>Title: PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning</h3>
<ul>
<li><strong>Authors: </strong>Bhavinkumar Vinodbhai Kuwar, Bikrant Bikram Pratap Maurya, Priyanshu Gupta, Nitin Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09157">https://arxiv.org/abs/2507.09157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09157">https://arxiv.org/pdf/2507.09157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09157]] PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning(https://arxiv.org/abs/2507.09157)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Detecting deception in strategic dialogues is a complex and high-stakes task due to the subtlety of language and extreme class imbalance between deceptive and truthful communications. In this work, we revisit deception detection in the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We introduce a lightweight yet effective model combining frozen BERT embeddings, interpretable linguistic and game-specific features, and a Positive-Unlabeled (PU) learning objective. Unlike traditional binary classifiers, PU-Lie is tailored for situations where only a small portion of deceptive messages are labeled, and the majority are unlabeled. Our model achieves a new best macro F1 of 0.60 while reducing trainable parameters by over 650x. Through comprehensive evaluations and ablation studies across seven models, we demonstrate the value of PU learning, linguistic interpretability, and speaker-aware representations. Notably, we emphasize that in this problem setting, accurately detecting deception is more critical than identifying truthful messages. This priority guides our choice of PU learning, which explicitly models the rare but vital deceptive class.</li>
</ul>

<h3>Title: Stable Score Distillation</h3>
<ul>
<li><strong>Authors: </strong>Haiming Zhu, Yangyang Xu, Chenshu Xu, Tingrui Shen, Wenxi Liu, Yong Du, Jun Yu, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09168">https://arxiv.org/abs/2507.09168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09168">https://arxiv.org/pdf/2507.09168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09168]] Stable Score Distillation(https://arxiv.org/abs/2507.09168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing.</li>
</ul>

<h3>Title: Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations</h3>
<ul>
<li><strong>Authors: </strong>Mengjie Chen, Ming Zhang, Cunquan Qu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.MN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09173">https://arxiv.org/abs/2507.09173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09173">https://arxiv.org/pdf/2507.09173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09173]] Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations(https://arxiv.org/abs/2507.09173)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Drug-drug interactions (DDIs) represent a critical challenge in pharmacology, often leading to adverse drug reactions with significant implications for patient safety and healthcare outcomes. While graph-based methods have achieved strong predictive performance, most approaches treat drug pairs independently, overlooking the complex, context-dependent interactions unique to drug pairs. Additionally, these models struggle to integrate biological interaction networks and molecular-level structures to provide meaningful mechanistic insights. In this study, we propose MolecBioNet, a novel graph-based framework that integrates molecular and biomedical knowledge for robust and interpretable DDI prediction. By modeling drug pairs as unified entities, MolecBioNet captures both macro-level biological interactions and micro-level molecular influences, offering a comprehensive perspective on DDIs. The framework extracts local subgraphs from biomedical knowledge graphs and constructs hierarchical interaction graphs from molecular representations, leveraging classical graph neural network methods to learn multi-scale representations of drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces two domain-specific pooling strategies: context-aware subgraph pooling (CASPool), which emphasizes biologically relevant entities, and attention-guided influence pooling (AGIPool), which prioritizes influential molecular substructures. The framework further employs mutual information minimization regularization to enhance information diversity during embedding fusion. Experimental results demonstrate that MolecBioNet outperforms state-of-the-art methods in DDI prediction, while ablation studies and embedding visualizations further validate the advantages of unified drug pair modeling and multi-scale knowledge integration.</li>
</ul>

<h3>Title: RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking</h3>
<ul>
<li><strong>Authors: </strong>Shuo Yang, Zijian Yu, Zhenzhe Ying, Yuqin Dai, Guoqing Wang, Jun Lan, Jinfeng Xu, Jinze Li, Edith C.H. Ngai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09174">https://arxiv.org/abs/2507.09174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09174">https://arxiv.org/pdf/2507.09174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09174]] RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking(https://arxiv.org/abs/2507.09174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of multimodal misinformation presents significant challenges for automated fact-checking systems, especially when claims are ambiguous or lack sufficient context. We introduce RAMA, a novel retrieval-augmented multi-agent framework designed for verifying multimedia misinformation. RAMA incorporates three core innovations: (1) strategic query formulation that transforms multimodal claims into precise web search queries; (2) cross-verification evidence aggregation from diverse, authoritative sources; and (3) a multi-agent ensemble architecture that leverages the complementary strengths of multiple multimodal large language models and prompt variants. Extensive experiments demonstrate that RAMA achieves superior performance on benchmark datasets, particularly excelling in resolving ambiguous or improbable claims by grounding verification in retrieved factual evidence. Our findings underscore the necessity of integrating web-based evidence and multi-agent reasoning for trustworthy multimedia verification, paving the way for more reliable and scalable fact-checking solutions. RAMA will be publicly available at this https URL.</li>
</ul>

<h3>Title: Learning and Transferring Better with Depth Information in Visual Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zichun Xu, Yuntao Li, Zhaomin Wang, Lei Zhuang, Guocai Yang, Jingdong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09180">https://arxiv.org/abs/2507.09180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09180">https://arxiv.org/pdf/2507.09180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09180]] Learning and Transferring Better with Depth Information in Visual Reinforcement Learning(https://arxiv.org/abs/2507.09180)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Depth information is robust to scene appearance variations and inherently carries 3D spatial details. In this paper, a visual backbone based on the vision transformer is proposed to fuse RGB and depth modalities for enhancing generalization. Different modalities are first processed by separate CNN stems, and the combined convolutional features are delivered to the scalable vision transformer to obtain visual representations. Moreover, a contrastive unsupervised learning scheme is designed with masked and unmasked tokens to accelerate the sample efficiency during the reinforcement learning progress. For sim2real transfer, a flexible curriculum learning schedule is developed to deploy domain randomization over training processes.</li>
</ul>

<h3>Title: Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ameen Ali, Shahar Katz, Lior Wolf, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09185">https://arxiv.org/abs/2507.09185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09185">https://arxiv.org/pdf/2507.09185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09185]] Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models(https://arxiv.org/abs/2507.09185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often develop learned mechanisms specialized to specific datasets, such as reliance on domain-specific correlations, which yield high-confidence predictions without generalizable reasoning. While beneficial in one setting, these dataset-specific mechanisms typically degrade performance when models encounter novel tasks or distributions. In this work, we introduce a fine-tuning approach designed to enhance generalization by identifying and pruning neurons associated with dataset-specific mechanisms in transformer-based LLMs. Our method employs Integrated Gradients to quantify each neuron's influence on high-confidence predictions, pinpointing those that disproportionately contribute to dataset-specific performance without supporting robust, transferable reasoning. Selectively pruning these neurons compels the model to depend on generalizable representations. Evaluated across multiple-choice benchmarks, our pruning-based fine-tuning significantly enhances performance, surpassing prior (non-pruning) adaptation methods.</li>
</ul>

<h3>Title: THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage</h3>
<ul>
<li><strong>Authors: </strong>Trong-Thuan Nguyen, Pha Nguyen, Jackson Cothren, Alper Yilmaz, Minh-Triet Tran, Khoa Luu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09200">https://arxiv.org/abs/2507.09200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09200">https://arxiv.org/pdf/2507.09200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09200]] THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage(https://arxiv.org/abs/2507.09200)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of video in applications such as autonomous driving, surveillance, and sports analytics necessitates robust methods for dynamic scene understanding. Despite advances in static scene graph generation and early attempts at video scene graph generation, previous methods often suffer from fragmented representations, failing to capture fine-grained spatial details and long-range temporal dependencies simultaneously. To address these limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME) approach, which synergistically integrates hierarchical feature aggregation with cyclic temporal refinement to address these limitations. In particular, THYME effectively models multi-scale spatial context and enforces temporal consistency across frames, yielding more accurate and coherent scene graphs. In addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with five types of interactivity that overcome the constraints of existing datasets and provide a comprehensive benchmark for dynamic scene graph generation. Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that the proposed THYME approach outperforms state-of-the-art methods, offering improved scene understanding in ground-view and aerial scenarios.</li>
</ul>

<h3>Title: Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Leiyu Pan, Bojian Xiong, Lei Yang, Renren Jin, Shaowei Zhang, Yue Chen, Ling Shi, Jiang Zhou, Junru Wu, Zhen Wang, Jianxiang Peng, Juesi Xiao, Tianyu Dong, Zhuowen Han, Zhuo Chen, Sangjee Dondrub, Caizang Tai, Haixing Zhao, Huaque Cairang, Suonan Cairang, Rou Te, Lengben Zhaxi, Gazang Zhaxi, Zhonglin Ye, Yuhui Zheng, Chunyan Peng, Secha Jia, Pema Tashi, Cizhen Jiacuo, Pema Dorjee, Hongkai Liu, Pema Yanggon, Tsehang Dorjee, Jiaxin Han, Qiongying Hu, Jilin Man, Huanke You, Yuqi Ren, Duo La, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09205">https://arxiv.org/abs/2507.09205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09205">https://arxiv.org/pdf/2507.09205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09205]] Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training(https://arxiv.org/abs/2507.09205)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have achieved remarkable progress across many languages. However, Tibetan, as a representative low-resource language, is particularly underrepresented in existing models due to the scarcity of high-quality training corpora. To address this gap, we curate the largest Tibetan pre-training corpus to date, aggregating data from diverse sources and applying a dedicated data cleaning and processing pipeline tailored for Tibetan. With the curated data, we continue pre/post-training a multilingual base model into Banzhida, a multilingual large language model that advances generative AI for Tibetan. To evaluate the Tibetan capabilities of the model, we create new high-quality Tibetan benchmarks, and complement them with existing public benchmarks. Experimental results demonstrate that Banzhida consistently and significantly outperforms both open-source models of similar scale and Tibetan-tailored models across a wide range of tasks.</li>
</ul>

<h3>Title: Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Liu, Xiao Peng, Shuyue Yan, Yuntian Chen, Dongxiao Zhang, Zhixiao Niu, Hui-Min Wang, Xiaogang He</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph, physics.data-an, physics.geo-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09211">https://arxiv.org/abs/2507.09211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09211">https://arxiv.org/pdf/2507.09211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09211]] Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling(https://arxiv.org/abs/2507.09211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Observed records of climate extremes provide an incomplete picture of risk, missing "unseen" extremes that exceed historical bounds. In parallel, neglecting spatial dependence undervalues the risk of synchronized hazards that amplify impacts. To address these challenges, we develop DeepX-GAN (Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial Network), a knowledge-informed deep generative model designed to better capture the spatial structure of rare extremes. The zero-shot generalizability of DeepX-GAN enables simulation of unseen extremes that fall outside historical experience yet remain statistically plausible. We define two types of unseen extremes: "checkmate" extremes that directly hit targets, and "stalemate" extremes that narrowly miss. These unrealized scenarios expose latent risks in fragile systems and may reinforce a false sense of resilience if overlooked. Near misses, in particular, can prompt either proactive adaptation or dangerous complacency, depending on how they are interpreted. Applying DeepX-GAN to the Middle East and North Africa (MENA), we find that these unseen extremes disproportionately affect regions with high vulnerability and low socioeconomic readiness, but differ in urgency and interpretation. Future warming could expand and redistribute these unseen extremes, with emerging exposure hotspots in Indo-Pakistan and Central Africa. This distributional shift highlights critical blind spots in conventional hazard planning and underscores the need to develop spatially adaptive policies that anticipate emergent risk hotspots rather than simply extrapolating from historical patterns.</li>
</ul>

<h3>Title: Warm Starts Accelerate Generative Modelling</h3>
<ul>
<li><strong>Authors: </strong>Jonas Scholz, Richard E. Turner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09212">https://arxiv.org/abs/2507.09212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09212">https://arxiv.org/pdf/2507.09212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09212]] Warm Starts Accelerate Generative Modelling(https://arxiv.org/abs/2507.09212)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Iterative generative models, like diffusion and flow-matching, create high-fidelity samples by progressively refining a noise vector into data. However, this process is notoriously slow, often requiring hundreds of function evaluations. We introduce the warm-start model, a simple, deterministic model that dramatically accelerates conditional generation by providing a better starting point. Instead of starting generation from an uninformed N(0, I) prior, our warm-start model predicts an informed prior N(mu, sigma), whose moments are conditioned on the input context. This "warm start" substantially reduces the distance the generative process must traverse, particularly when the conditioning information is strongly informative. On tasks like image inpainting, our method achieves results competitive with a 1000-step DDPM baseline using only 11 total function evaluations (1 for the warm start, 10 for generation). A simple conditional normalization trick makes our method compatible with any standard generative model and sampler without modification, allowing it to be combined with other efficient sampling techniques for further acceleration. Our implementation is available at this https URL.</li>
</ul>

<h3>Title: 360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Jingguo Liu, Han Yu, Shigang Li, Jianfeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09216">https://arxiv.org/abs/2507.09216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09216">https://arxiv.org/pdf/2507.09216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09216]] 360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models(https://arxiv.org/abs/2507.09216)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Due to the current lack of large-scale datasets at the million-scale level, tasks involving panoramic images predominantly rely on existing two-dimensional pre-trained image benchmark models as backbone networks. However, these networks are not equipped to recognize the distortions and discontinuities inherent in panoramic images, which adversely affects their performance in such tasks. In this paper, we introduce a novel spherical sampling method for panoramic images that enables the direct utilization of existing pre-trained models developed for two-dimensional images. Our method employs spherical discrete sampling based on the weights of the pre-trained models, effectively mitigating distortions while achieving favorable initial training values. Additionally, we apply the proposed sampling method to panoramic image segmentation, utilizing features obtained from the spherical model as masks for specific channel attentions, which yields commendable results on commonly used indoor datasets, Stanford2D3D.</li>
</ul>

<h3>Title: Online Long-term Point Tracking in the Foundation Model Era</h3>
<ul>
<li><strong>Authors: </strong>G√∂rkay Aydemir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09217">https://arxiv.org/abs/2507.09217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09217">https://arxiv.org/pdf/2507.09217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09217]] Online Long-term Point Tracking in the Foundation Model Era(https://arxiv.org/abs/2507.09217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Point tracking aims to identify the same physical point across video frames and serves as a geometry-aware representation of motion. This representation supports a wide range of applications, from robotics to augmented reality, by enabling accurate modeling of dynamic environments. Most existing long-term tracking approaches operate in an offline setting, where future frames are available to refine predictions and recover from occlusions. However, real-world scenarios often demand online predictions: the model must operate causally, using only current and past frames. This constraint is critical in streaming video and embodied AI, where decisions must be made immediately based on past observations. Under such constraints, viewpoint invariance becomes essential. Visual foundation models, trained on diverse large-scale datasets, offer the potential for robust geometric representations. While they lack temporal reasoning on their own, they can be integrated into tracking pipelines to enrich spatial features. In this thesis, we address the problem of long-term point tracking in an online setting, where frames are processed sequentially without access to future information or sliding windows. We begin by evaluating the suitability of visual foundation models for this task and find that they can serve as useful initializations and be integrated into tracking pipelines. However, to enable long-term tracking in an online setting, a dedicated design is still required. In particular, maintaining coherence over time in this causal regime requires memory to propagate appearance and context across frames. To address this, we introduce Track-On, a transformer-based model that treats each tracked point as a query and processes video frames one at a time. Track-On sets a new state of the art across seven public benchmarks, demonstrating the feasibility of long-term tracking without future access.</li>
</ul>

<h3>Title: Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift</h3>
<ul>
<li><strong>Authors: </strong>Behraj Khan, Tahir Syed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09222">https://arxiv.org/abs/2507.09222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09222">https://arxiv.org/pdf/2507.09222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09222]] Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift(https://arxiv.org/abs/2507.09222)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models like CLIP and SAM have transformed computer vision and medical imaging via low-shot transfer learning. However, deployment of these models hindered by two key challenges: \textit{distribution shift} between training and test data, and \textit{confidence misalignment} that leads to overconfident incorrect predictions. These issues manifest differently in vision-language classification and medical segmentation tasks, yet existing solutions remain domain-specific. We propose \textit{StaRFM}, a unified framework addressing both challenges. It introduces a Fisher information penalty (FIP), extended to 3D medical data via patch-wise regularization, to reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence misalignment penalty (CMP), reformulated for voxel-level predictions, calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP minimizes calibration error through Brier score optimization. StaRFM shows consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19 vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain performance gap compared to prior benchmarking methods. The framework is plug-and-play, requiring minimal architectural changes for seamless integration with foundation models. Code and models will be released at this https URL</li>
</ul>

<h3>Title: EgoAnimate: Generating Human Animations from Egocentric top-down Views</h3>
<ul>
<li><strong>Authors: </strong>G. Kutay T√ºrkoglu, Julian Tanke, Iheb Belgacem, Lev Markhasin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09230">https://arxiv.org/abs/2507.09230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09230">https://arxiv.org/pdf/2507.09230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09230]] EgoAnimate: Generating Human Animations from Egocentric top-down Views(https://arxiv.org/abs/2507.09230)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>An ideal digital telepresence experience requires accurate replication of a person's body, clothing, and movements. To capture and transfer these movements into virtual reality, the egocentric (first-person) perspective can be adopted, which enables the use of a portable and cost-effective device without front-view cameras. However, this viewpoint introduces challenges such as occlusions and distorted body proportions. There are few works reconstructing human appearance from egocentric views, and none use a generative prior-based approach. Some methods create avatars from a single egocentric image during inference, but still rely on multi-view datasets during training. To our knowledge, this is the first study using a generative backbone to reconstruct animatable avatars from egocentric inputs. Based on Stable Diffusion, our method reduces training burden and improves generalizability. Inspired by methods such as SiTH and MagicMan, which perform 360-degree reconstruction from a frontal image, we introduce a pipeline that generates realistic frontal views from occluded top-down images using ControlNet and a Stable Diffusion backbone. Our goal is to convert a single top-down egocentric image into a realistic frontal representation and feed it into an image-to-motion model. This enables generation of avatar motions from minimal input, paving the way for more accessible and generalizable telepresence systems.</li>
</ul>

<h3>Title: Confidential Wrapped Ethereum</h3>
<ul>
<li><strong>Authors: </strong>Artem Chystiakov, Mariia Zhvanko</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09231">https://arxiv.org/abs/2507.09231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09231">https://arxiv.org/pdf/2507.09231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09231]] Confidential Wrapped Ethereum(https://arxiv.org/abs/2507.09231)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Transparency is one of the key benefits of public blockchains. However, the public visibility of transactions potentially compromises users' privacy. The fundamental challenge is to balance the intrinsic benefits of blockchain openness with the vital need for individual confidentiality. The proposal suggests creating a confidential version of wrapped Ethereum (cWETH) fully within the application layer. The solution combines the Elliptic Curve (EC) Twisted ElGamal-based commitment scheme to preserve confidentiality and the EC Diffie-Hellman (DH) protocol to introduce accessibility limited by the commitment scheme. To enforce the correct generation of commitments, encryption, and decryption, zk-SNARKs are utilized.</li>
</ul>

<h3>Title: PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Jiang, Xinpeng Li, Xi Mao, Changbo Wang, Chenhui Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09242">https://arxiv.org/abs/2507.09242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09242">https://arxiv.org/pdf/2507.09242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09242]] PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process(https://arxiv.org/abs/2507.09242)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Artistic image assessment has become a prominent research area in computer vision. In recent years, the field has witnessed a proliferation of datasets and methods designed to evaluate the aesthetic quality of paintings. However, most existing approaches focus solely on static final images, overlooking the dynamic and multi-stage nature of the artistic painting process. To address this gap, we propose a novel framework for human-aligned assessment of painting processes. Specifically, we introduce the Painting Process Assessment Dataset (PPAD), the first large-scale dataset comprising real and synthetic painting process images, annotated by domain experts across eight detailed attributes. Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based model enhanced with temporally-aware positional encoding and a heterogeneous mixture-of-experts architecture, enabling effective assessment of the painting process. Experimental results demonstrate that our method outperforms existing baselines in accuracy, robustness, and alignment with human judgment, offering new insights into computational creativity and art education.</li>
</ul>

<h3>Title: AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Varsha Devi, Amine Bohi, Pardeep Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09248">https://arxiv.org/abs/2507.09248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09248">https://arxiv.org/pdf/2507.09248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09248]] AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition(https://arxiv.org/abs/2507.09248)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Context-aware emotion recognition (CAER) enhances affective computing in real-world scenarios, but traditional methods often suffer from context bias-spurious correlation between background context and emotion labels (e.g. associating ``garden'' with ``happy''). In this paper, we propose \textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces \textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the ConvNeXt backbone by integrating Spatial Transformer Network and Squeeze-and-Excitation layers for enhanced feature recalibration. At the core of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM), which applies causal theory, perturbs context features, isolates spurious correlations, and performs an attention-driven correction guided by face features to mitigate context bias. Experimental results on the CAER-S dataset demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art performance and highlighting the importance of causal debiasing for robust emotion recognition in complex settings.</li>
</ul>

<h3>Title: TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shukai Gong, Yiyang Fu, Fengyuan Ran, Feng Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09252">https://arxiv.org/abs/2507.09252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09252">https://arxiv.org/pdf/2507.09252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09252]] TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding(https://arxiv.org/abs/2507.09252)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose TPP-SD, a novel approach that accelerates Transformer temporal point process (TPP) sampling by adapting speculative decoding (SD) techniques from language models. By identifying the structural similarities between thinning algorithms for TPPs and speculative decoding for language models, we develop an efficient sampling framework that leverages a smaller draft model to generate multiple candidate events, which are then verified by the larger target model in parallel. TPP-SD maintains the same output distribution as autoregressive sampling while achieving significant acceleration. Experiments on both synthetic and real datasets demonstrate that our approach produces samples from identical distributions as standard methods, but with 2-6$\times$ speedup. Our ablation studies analyze the impact of hyperparameters such as draft length and draft model size on sampling efficiency. TPP-SD bridges the gap between powerful Transformer TPP models and the practical need for rapid sequence sampling.</li>
</ul>

<h3>Title: Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching</h3>
<ul>
<li><strong>Authors: </strong>Junyu Chen, Yihua Gao, Mingyuan Ge, Mingyong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09256">https://arxiv.org/abs/2507.09256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09256">https://arxiv.org/pdf/2507.09256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09256]] Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching(https://arxiv.org/abs/2507.09256)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Image-text matching is crucial for bridging the semantic gap between computer vision and natural language processing. However, existing methods still face challenges in handling high-order associations and semantic ambiguities among similar instances. These ambiguities arise from subtle differences between soft positive samples (semantically similar but incorrectly labeled) and soft negative samples (locally matched but globally inconsistent), creating matching uncertainties. Furthermore, current methods fail to fully utilize the neighborhood relationships among semantically similar instances within training batches, limiting the model's ability to learn high-order shared knowledge. This paper proposes the Ambiguity-Aware and High-order Relation learning framework (AAHR) to address these issues. AAHR constructs a unified representation space through dynamic clustering prototype contrastive learning, effectively mitigating the soft positive sample problem. The framework introduces global and local feature extraction mechanisms and an adaptive aggregation network, significantly enhancing full-grained semantic understanding capabilities. Additionally, AAHR employs intra-modal and inter-modal correlation matrices to investigate neighborhood relationships among sample instances thoroughly. It incorporates GNN to enhance semantic interactions between instances. Furthermore, AAHR integrates momentum contrastive learning to expand the negative sample set. These combined strategies significantly improve the model's ability to discriminate between features. Experimental results demonstrate that AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching. The code and model checkpoints for this research are available at this https URL .</li>
</ul>

<h3>Title: Psychology-Driven Enhancement of Humour Translation</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Su, Yonghua Zhu, Yang Chen, Diana Benavides-Prado, Michael Witbrock</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09259">https://arxiv.org/abs/2507.09259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09259">https://arxiv.org/pdf/2507.09259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09259]] Psychology-Driven Enhancement of Humour Translation(https://arxiv.org/abs/2507.09259)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humour translation plays a vital role as a bridge between different cultures, fostering understanding and communication. Although most existing Large Language Models (LLMs) are capable of general translation tasks, these models still struggle with humour translation, which is especially reflected through linguistic interference and lacking humour in translated text. In this paper, we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that utilises Chain-of-Thought (CoT) to imitate the ability of the human thought process, stimulating LLMs to optimise the readability of translated humorous texts. Moreover, we integrate humour theory in HDM to further enhance the humorous elements in the translated text. Our automatic evaluation experiments on open-source humour datasets demonstrate that our method significantly improves the quality of humour translation, yielding average gains of 7.75\% in humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.</li>
</ul>

<h3>Title: Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Payel Mukhopadhyay, Michael McCabe, Ruben Ohana, Miles Cranmer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09264">https://arxiv.org/abs/2507.09264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09264">https://arxiv.org/pdf/2507.09264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09264]] Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations(https://arxiv.org/abs/2507.09264)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Patch-based transformer surrogates have become increasingly effective for modeling spatiotemporal dynamics, but the fixed patch size is a major limitation for budget-conscience deployment in production. We introduce two lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator (CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size control at inference in patch based models, without retraining or accuracy loss. Combined with a cyclic patch-size rollout, our method mitigates patch artifacts and improves long-term stability for video-like prediction tasks. Applied to a range of challenging 2D and 3D PDE benchmarks, our approach improves rollout fidelity and runtime efficiency. To our knowledge, this is the first framework to enable inference-time patch-size tunability in patch-based PDE surrogates. Its plug-and-play design makes it broadly applicable across architectures-establishing a general foundation for compute-adaptive modeling in PDE surrogate tasks.</li>
</ul>

<h3>Title: SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>JianHe Low, Ozge Mercanoglu Sincan, Richard Bowden</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09266">https://arxiv.org/abs/2507.09266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09266">https://arxiv.org/pdf/2507.09266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09266]] SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation(https://arxiv.org/abs/2507.09266)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving strong performances without relying on gloss annotations. However, these gains have often come with increased model complexity and high computational demands, raising concerns about scalability, especially as large-scale sign language datasets become more common. We propose a segment-aware visual tokenization framework that leverages sign segmentation to convert continuous video into discrete, sign-informed visual tokens. This reduces input sequence length by up to 50% compared to prior methods, resulting in up to 2.67x lower memory usage and better scalability on larger datasets. To bridge the visual and linguistic modalities, we introduce a token-to-token contrastive alignment objective, along with a dual-level supervision that aligns both language embeddings and intermediate hidden states. This improves fine-grained cross-modal alignment without relying on gloss-level supervision. Our approach notably exceeds the performance of state-of-the-art methods on the PHOENIX14T benchmark, while significantly reducing sequence length. Further experiments also demonstrate our improved performance over prior work under comparable sequence-lengths, validating the potential of our tokenization and alignment strategies.</li>
</ul>

<h3>Title: Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09279">https://arxiv.org/abs/2507.09279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09279">https://arxiv.org/pdf/2507.09279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09279]] Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models(https://arxiv.org/abs/2507.09279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at this https URL.</li>
</ul>

<h3>Title: ClaritySpeech: Dementia Obfuscation in Speech</h3>
<ul>
<li><strong>Authors: </strong>Dominika Woszczyk, Ranya Aloufi, Soteris Demetriou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09282">https://arxiv.org/abs/2507.09282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09282">https://arxiv.org/pdf/2507.09282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09282]] ClaritySpeech: Dementia Obfuscation in Speech(https://arxiv.org/abs/2507.09282)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Dementia, a neurodegenerative disease, alters speech patterns, creating communication barriers and raising privacy concerns. Current speech technologies, such as automatic speech transcription (ASR), struggle with dementia and atypical speech, further challenging accessibility. This paper presents a novel dementia obfuscation in speech framework, ClaritySpeech, integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to correct dementia-affected speech while preserving speaker identity in low-data environments without fine-tuning. Results show a 16% and 10% drop in mean F1 score across various adversarial settings and modalities (audio, text, fusion) for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15 for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and accessibility.</li>
</ul>

<h3>Title: Generative Latent Kernel Modeling for Blind Motion Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Ding, Jiangtao Zhang, Zongsheng Yue, Hui Wang, Qian Zhao, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09285">https://arxiv.org/abs/2507.09285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09285">https://arxiv.org/pdf/2507.09285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09285]] Generative Latent Kernel Modeling for Blind Motion Deblurring(https://arxiv.org/abs/2507.09285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep prior-based approaches have demonstrated remarkable success in blind motion deblurring (BMD) recently. These methods, however, are often limited by the high non-convexity of the underlying optimization process in BMD, which leads to extreme sensitivity to the initial blur kernel. To address this issue, we propose a novel framework for BMD that leverages a deep generative model to encode the kernel prior and induce a better initialization for the blur kernel. Specifically, we pre-train a kernel generator based on a generative adversarial network (GAN) to aptly characterize the kernel's prior distribution, as well as a kernel initializer to provide a well-informed and high-quality starting point for kernel estimation. By combining these two components, we constrain the BMD solution within a compact latent kernel manifold, thus alleviating the aforementioned sensitivity for kernel initialization. Notably, the kernel generator and initializer are designed to be easily integrated with existing BMD methods in a plug-and-play manner, enhancing their overall performance. Furthermore, we extend our approach to tackle blind non-uniform motion deblurring without the need for additional priors, achieving state-of-the-art performance on challenging benchmark datasets. The source code is available at this https URL.</li>
</ul>

<h3>Title: Hybrid Quantum Security for IPsec</h3>
<ul>
<li><strong>Authors: </strong>Javier Blanco-Romero, Pedro Otero Garc√≠a, Daniel Sobral-Blanco, Florina Almenares Mendoza, Ana Fern√°ndez Vilas, Manuel Fern√°ndez-Veiga</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09288">https://arxiv.org/abs/2507.09288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09288">https://arxiv.org/pdf/2507.09288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09288]] Hybrid Quantum Security for IPsec(https://arxiv.org/abs/2507.09288)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense</a></li>
<li><strong>Abstract: </strong>Quantum Key Distribution (QKD) offers information-theoretic security against quantum computing threats, but integrating QKD into existing security protocols remains an unsolved challenge due to fundamental mismatches between pre-distributed quantum keys and computational key exchange paradigms. This paper presents the first systematic comparison of sequential versus parallel hybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental protocol design principles that extend beyond specific implementations. We introduce two novel approaches for incorporating QKD into Internet Key Exchange version 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS QKD 014 stateless API specifications: (1) a pure QKD approach that replaces computational key derivation with identifier-based quantum key coordination, and (2) a unified QKD-KEM abstraction that enables parallel composition of quantum and post-quantum cryptographic methods within existing protocol frameworks. Our key insight is that parallel hybrid approaches eliminate the multiplicative latency penalties inherent in sequential methods mandated by RFC 9370, achieving significant performance improvements under realistic network conditions. Performance evaluation using a Docker-based testing framework with IDQuantique QKD hardware demonstrates that the parallel hybrid approach significantly outperforms sequential methods under network latency conditions, while pure QKD achieves minimal bandwidth overhead through identifier-based key coordination. Our implementations provide practical quantum-enhanced IPsec solutions suitable for critical infrastructure deployments requiring defense-in-depth security.</li>
</ul>

<h3>Title: Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection</h3>
<ul>
<li><strong>Authors: </strong>Rui Tang, Haochen Yin, Guankun Wang, Long Bai, An Wang, Huxin Gao, Jiazheng Wang, Hongliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09294">https://arxiv.org/abs/2507.09294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09294">https://arxiv.org/pdf/2507.09294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09294]] Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection(https://arxiv.org/abs/2507.09294)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Surgical phase recognition plays a critical role in developing intelligent assistance systems for minimally invasive procedures such as Endoscopic Submucosal Dissection (ESD). However, the high visual similarity across different phases and the lack of structural cues in RGB images pose significant challenges. Depth information offers valuable geometric cues that can complement appearance features by providing insights into spatial relationships and anatomical structures. In this paper, we pioneer the use of depth information for surgical phase recognition and propose Geo-RepNet, a geometry-aware convolutional framework that integrates RGB image and depth information to enhance recognition performance in complex surgical scenes. Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention (GEMA) to inject spatial guidance through geometry-aware cross-attention and efficient multi-scale aggregation. To evaluate the effectiveness of our approach, we construct a nine-phase ESD dataset with dense frame-level annotations from real-world ESD videos. Extensive experiments on the proposed dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while maintaining robustness and high computational efficiency under complex and low-texture surgical environments.</li>
</ul>

<h3>Title: ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Abdulvahap Mutlu, ≈ûeng√ºl Doƒüan, T√ºrker Tuncer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09299">https://arxiv.org/abs/2507.09299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09299">https://arxiv.org/pdf/2507.09299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09299]] ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation(https://arxiv.org/abs/2507.09299)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners.</li>
</ul>

<h3>Title: Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS</h3>
<ul>
<li><strong>Authors: </strong>Julio Gento Suela, Javier Blanco-Romero, Florina Almenares Mendoza, Daniel D√≠az-S√°nchez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09301">https://arxiv.org/abs/2507.09301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09301">https://arxiv.org/pdf/2507.09301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09301]] Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS(https://arxiv.org/abs/2507.09301)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The emergence of quantum computers poses a significant threat to current secure service, application and/or protocol implementations that rely on RSA and ECDSA algorithms, for instance DNSSEC, because public-key cryptography based on number factorization or discrete logarithm is vulnerable to quantum attacks. This paper presents the integration of post-quantum cryptographic (PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality. We have developed a plugin that extends CoreDNS with support for five PQC signature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our implementation maintains compatibility with existing DNS resolution flows while providing on-the-fly signing using quantum-resistant signatures. A benchmark has been performed and performance evaluation results reveal significant trade-offs between security and efficiency. The results indicate that while PQC algorithms introduce operational overhead, several candidates offer viable compromises for transitioning DNSSEC to quantum-resistant cryptography.</li>
</ul>

<h3>Title: AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zile Wang, Hao Yu, Jiabo Zhan, Chun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09308">https://arxiv.org/abs/2507.09308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09308">https://arxiv.org/pdf/2507.09308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09308]] AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning(https://arxiv.org/abs/2507.09308)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on this https URL for reproducibility.</li>
</ul>

<h3>Title: ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yueqian Wang, Xiaojun Meng, Yifan Wang, Huishuai Zhang, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09313">https://arxiv.org/abs/2507.09313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09313">https://arxiv.org/pdf/2507.09313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09313]] ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models(https://arxiv.org/abs/2507.09313)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the growing research focus on multimodal dialogue systems, the capability for proactive interaction is gradually gaining recognition. As an alternative to conventional turn-by-turn dialogue, users increasingly expect multimodal systems to be more initiative, for example, by autonomously determining the timing of multi-turn responses in real time during video playback. To facilitate progress in this emerging area, we introduce ProactiveBench, the first comprehensive benchmark to evaluate a system's ability to engage in proactive interaction. Since model responses are generated at varying timestamps, we further propose PAUC, the first metric that accounts for the temporal dynamics of model responses. This enables a more accurate evaluation of systems operating in proactive settings. Through extensive benchmarking of various baseline systems on ProactiveBench and a user study of human preferences, we show that PAUC is in better agreement with human preferences than traditional evaluation metrics, which typically only consider the textual content of responses. These findings demonstrate that PAUC provides a more faithful assessment of user experience in proactive interaction scenarios. Project homepage: this https URL</li>
</ul>

<h3>Title: Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Wencan Huang, Daizong Liu, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09334">https://arxiv.org/abs/2507.09334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09334">https://arxiv.org/pdf/2507.09334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09334]] Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding(https://arxiv.org/abs/2507.09334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable scene understanding capabilities, their practical deployment faces critical challenges due to computational inefficiency. The key bottleneck stems from processing excessive object-centric visual tokens required for comprehensive 3D scene representation. Although visual token pruning has shown promise in accelerating 2D MLLMs, its applicability to 3D domains remains largely unexplored due to fundamental disparities in token structures. In this paper, we reveal two critical insights: (1) Significant redundancy exists in object-level 3D token representations, analogous to patch-level redundancy in 2D systems; (2) Global attention patterns exhibit strong predictive power for identifying non-essential tokens in 3D contexts. Building on these observations, we propose Fast3D, a plug-and-play visual token pruning framework for 3D MLLMs featuring two technical innovations: (1) Global Attention Prediction (GAP), where a lightweight neural network learns to predict the global attention distributions of the target model, enabling efficient token importance estimation for precise pruning guidance; (2) Sample-Adaptive visual token Pruning (SAP), which introduces dynamic token budgets through attention-based complexity assessment, automatically adjusting layer-wise pruning ratios based on input characteristics. Both of these two techniques operate without modifying the parameters of the target model. Extensive evaluations across five benchmarks validate the effectiveness of Fast3D, particularly under high visual token pruning ratios. Code is available at this https URL</li>
</ul>

<h3>Title: Simplifying Traffic Anomaly Detection with Video Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Svetlana Orlova, Tommie Kerssies, Brun√≥ B. Englert, Gijs Dubbelman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09338">https://arxiv.org/abs/2507.09338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09338">https://arxiv.org/pdf/2507.09338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09338]] Simplifying Traffic Anomaly Detection with Video Foundation Models(https://arxiv.org/abs/2507.09338)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on complex multi-stage or multi-representation fusion architectures, yet it remains unclear whether such complexity is necessary. Recent findings in visual perception suggest that foundation models, enabled by advanced pre-training, allow simple yet flexible architectures to outperform specialized designs. Therefore, in this work, we investigate an architecturally simple encoder-only approach using plain Video Vision Transformers (Video ViTs) and study how pre-training enables strong TAD performance. We find that: (i) strong pre-training enables simple encoder-only models to match or even surpass the performance of specialized state-of-the-art TAD methods, while also being significantly more efficient; (ii) although weakly- and fully-supervised pre-training are advantageous on standard benchmarks, we find them less effective for TAD. Instead, self-supervised Masked Video Modeling (MVM) provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on unlabeled driving videos further improves downstream performance, without requiring anomalous examples. Our findings highlight the importance of pre-training and show that effective, efficient, and scalable TAD models can be built with minimal architectural complexity. We release our code, domain-adapted encoders, and fine-tuned models to support future work: this https URL.</li>
</ul>

<h3>Title: Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Sourish Suri (University of California, San Diego), Yifei Shao (University of Pennsylvania, Philadelphia)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09375">https://arxiv.org/abs/2507.09375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09375">https://arxiv.org/pdf/2507.09375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09375]] Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture(https://arxiv.org/abs/2507.09375)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Crop diseases present a significant barrier to agricultural productivity and global food security, especially in large-scale farming where early identification is often delayed or inaccurate. This research introduces a Convolutional Neural Network (CNN)-based image classification system designed to automate the detection and classification of eight common crop diseases using leaf imagery. The methodology involves a complete deep learning pipeline: image acquisition from a large, labeled dataset, preprocessing via resizing, normalization, and augmentation, and model training using TensorFlow with Keras' Sequential API. The CNN architecture comprises three convolutional layers with increasing filter sizes and ReLU activations, followed by max pooling, flattening, and fully connected layers, concluding with a softmax output for multi-class classification. The system achieves high training accuracy (~90%) and demonstrates reliable performance on unseen data, although a validation accuracy of ~60% suggests minor overfitting. Notably, the model integrates a treatment recommendation module, providing actionable guidance by mapping each detected disease to suitable pesticide or fungicide interventions. Furthermore, the solution is deployed on an open-source, mobile-compatible platform, enabling real-time image-based diagnostics for farmers in remote areas. This research contributes a scalable and accessible tool to the field of precision agriculture, reducing reliance on manual inspection and promoting sustainable disease management practices. By merging deep learning with practical agronomic support, this work underscores the potential of CNNs to transform crop health monitoring and enhance food production resilience on a global scale.</li>
</ul>

<h3>Title: Fair CCA for Fair Representation Learning: An ADNI Study</h3>
<ul>
<li><strong>Authors: </strong>Bojian Hou, Zhanliang Wang, Zhuoping Zhou, Boning Tong, Zexuan Wang, Jingxuan Bao, Duy Duong-Tran, Qi Long, Li Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09382">https://arxiv.org/abs/2507.09382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09382">https://arxiv.org/pdf/2507.09382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09382]] Fair CCA for Fair Representation Learning: An ADNI Study(https://arxiv.org/abs/2507.09382)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential.</li>
</ul>

<h3>Title: Geometric Generative Modeling with Noise-Conditioned Graph Networks</h3>
<ul>
<li><strong>Authors: </strong>Peter Pao-Huang, Mitchell Black, Xiaojie Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09391">https://arxiv.org/abs/2507.09391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09391">https://arxiv.org/pdf/2507.09391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09391]] Geometric Generative Modeling with Noise-Conditioned Graph Networks(https://arxiv.org/abs/2507.09391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative modeling of graphs with spatial structure is essential across many applications from computer graphics to spatial genomics. Recent flow-based generative models have achieved impressive results by gradually adding and then learning to remove noise from these graphs. Existing models, however, use graph neural network architectures that are independent of the noise level, limiting their expressiveness. To address this issue, we introduce \textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural networks that dynamically modify their architecture according to the noise level during generation. Our theoretical and empirical analysis reveals that as noise increases, (1) graphs require information from increasingly distant neighbors and (2) graphs can be effectively represented at lower resolutions. Based on these insights, we develop Dynamic Message Passing (DMP), a specific instantiation of NCGNs that adapts both the range and resolution of message passing to the noise level. DMP consistently outperforms noise-independent architectures on a variety of domains including $3$D point clouds, spatiotemporal transcriptomics, and images. Code is available at this https URL.</li>
</ul>

<h3>Title: A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention</h3>
<ul>
<li><strong>Authors: </strong>Nandan Kumar Jha, Brandon Reagen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09394">https://arxiv.org/abs/2507.09394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09394">https://arxiv.org/pdf/2507.09394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09394]] A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention(https://arxiv.org/abs/2507.09394)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this work, we study how multi-head latent attention (MLA), a popular strategy for compressing key/value memory, affects a transformer's internal capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP) diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix throughout training, comparing three variants: the standard multi-head attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression, and MLA-Decoupled, which shares a single rotary sub-vector across all heads. Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)} capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp, early spikes in specific layers that persist and propagate, disrupting the balance between bulk and outlier directions; \textbf{ ii)} these spikes coincide with rank collapse, concentrating the model's expressivity into narrow subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade, maintaining broad spectral support and suppressing outlier formation across layers. These results underscore that \emph{how} rotary embeddings are applied is just as critical as \emph{where} compression occurs. Sharing rotary components across heads mitigates spectral fragmentation and preserves representational capacity.</li>
</ul>

<h3>Title: Scaling Laws for Optimal Data Mixtures</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Shukor, Louis Bethune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, Pierre Ablin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09404">https://arxiv.org/abs/2507.09404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09404">https://arxiv.org/pdf/2507.09404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09404]] Scaling Laws for Optimal Data Mixtures(https://arxiv.org/abs/2507.09404)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size $N$ trained with $D$ tokens and a specific domain weight vector $h$. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget ($N$,$D$), providing a principled alternative to costly trial-and-error methods.</li>
</ul>

<h3>Title: Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers</h3>
<ul>
<li><strong>Authors: </strong>Santhosh Kumar Ravindran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09406">https://arxiv.org/abs/2507.09406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09406">https://arxiv.org/pdf/2507.09406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09406]] Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers(https://arxiv.org/abs/2507.09406)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) aligned for safety through techniques like reinforcement learning from human feedback (RLHF) often exhibit emergent deceptive behaviors, where outputs appear compliant but subtly mislead or omit critical information. This paper introduces adversarial activation patching, a novel mechanistic interpretability framework that leverages activation patching as an adversarial tool to induce, detect, and mitigate such deception in transformer-based models. By sourcing activations from "deceptive" prompts and patching them into safe forward passes at specific layers, we simulate vulnerabilities and quantify deception rates. Through toy neural network simulations across multiple scenarios (e.g., 1000 trials per setup), we demonstrate that adversarial patching increases deceptive outputs to 23.9% from a 0% baseline, with layer-specific variations supporting our hypotheses. We propose six hypotheses, including transferability across models, exacerbation in multimodal settings, and scaling effects. An expanded literature review synthesizes over 20 key works in interpretability, deception, and adversarial attacks. Mitigation strategies, such as activation anomaly detection and robust fine-tuning, are detailed, alongside ethical considerations and future research directions. This work advances AI safety by highlighting patching's dual-use potential and provides a roadmap for empirical studies on large-scale models.</li>
</ul>

<h3>Title: LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models</h3>
<ul>
<li><strong>Authors: </strong>Md Ajwad Akil, Adrian Shuai Li, Imtiaz Karim, Arun Iyengar, Ashish Kundu, Vinny Parla, Elisa Bertino</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09411">https://arxiv.org/abs/2507.09411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09411">https://arxiv.org/pdf/2507.09411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09411]] LLMalMorph: On The Feasibility of Generating Variant Malware using Large-Language-Models(https://arxiv.org/abs/2507.09411)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed software development and automated code generation. Motivated by these advancements, this paper explores the feasibility of LLMs in modifying malware source code to generate variants. We introduce LLMalMorph, a semi-automated framework that leverages semantical and syntactical code comprehension by LLMs to generate new malware variants. LLMalMorph extracts function-level information from the malware source code and employs custom-engineered prompts coupled with strategically defined code transformations to guide the LLM in generating variants without resource-intensive fine-tuning. To evaluate LLMalMorph, we collected 10 diverse Windows malware samples of varying types, complexity and functionality and generated 618 variants. Our thorough experiments demonstrate that it is possible to reduce the detection rates of antivirus engines of these malware variants to some extent while preserving malware functionalities. In addition, despite not optimizing against any Machine Learning (ML)-based malware detectors, several variants also achieved notable attack success rates against an ML-based malware classifier. We also discuss the limitations of current LLM capabilities in generating malware variants from source code and assess where this emerging technology stands in the broader context of malware variant generation.</li>
</ul>

<h3>Title: Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data</h3>
<ul>
<li><strong>Authors: </strong>Timothy Chase Jr, Karthik Dantu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09420">https://arxiv.org/abs/2507.09420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09420">https://arxiv.org/pdf/2507.09420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09420]] Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data(https://arxiv.org/abs/2507.09420)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The detection and tracking of celestial surface terrain features are crucial for autonomous spaceflight applications, including Terrain Relative Navigation (TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data collection. Traditional photoclinometry-based pipelines often rely on extensive a priori imaging and offline processing, constrained by the computational limitations of radiation-hardened systems. While historically effective, these approaches typically increase mission costs and duration, operate at low processing rates, and have limited generalization. Recently, learning-based computer vision has gained popularity to enhance spacecraft autonomy and overcome these limitations. While promising, emerging techniques frequently impose computational demands exceeding the capabilities of typical spacecraft hardware for real-time operation and are further challenged by the scarcity of labeled training data for diverse extraterrestrial environments. In this work, we present novel formulations for in-situ landmark tracking via detection and description. We utilize lightweight, computationally efficient neural network architectures designed for real-time execution on current-generation spacecraft flight processors. For landmark detection, we propose improved domain adaptation methods that enable the identification of celestial terrain features with distinct, cheaply acquired training data. Concurrently, for landmark description, we introduce a novel attention alignment formulation that learns robust feature representations that maintain correspondence despite significant landmark viewpoint variations. Together, these contributions form a unified system for landmark tracking that demonstrates superior performance compared to existing state-of-the-art techniques.</li>
</ul>

<h3>Title: DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cathy Jiao, Yijun Pan, Emily Xiao, Daisy Sheng, Niket Jain, Hanzhang Zhao, Ishita Dasgupta, Jiaqi W. Ma, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09424">https://arxiv.org/abs/2507.09424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09424">https://arxiv.org/pdf/2507.09424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09424]] DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models(https://arxiv.org/abs/2507.09424)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Data attribution methods quantify the influence of training data on model outputs and are becoming increasingly relevant for a wide range of LLM research and applications, including dataset curation, model interpretability, data valuation. However, there remain critical gaps in systematic LLM-centric evaluation of data attribution methods. To this end, we introduce DATE-LM (Data Attribution Evaluation in Language Models), a unified benchmark for evaluating data attribution methods through real-world LLM applications. DATE-LM measures attribution quality through three key tasks -- training data selection, toxicity/bias filtering, and factual attribution. Our benchmark is designed for ease of use, enabling researchers to configure and run large-scale evaluations across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to conduct a large-scale evaluation of existing data attribution methods. Our findings show that no single method dominates across all tasks, data attribution methods have trade-offs with simpler baselines, and method performance is sensitive to task-specific evaluation design. Finally, we release a public leaderboard for quick comparison of methods and to facilitate community engagement. We hope DATE-LM serves as a foundation for future data attribution research in LLMs.</li>
</ul>

<h3>Title: Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Meriem Zerkouk, Miloud Mihoubi, Belkacem Chikhaoui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09439">https://arxiv.org/abs/2507.09439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09439">https://arxiv.org/pdf/2507.09439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09439]] Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series(https://arxiv.org/abs/2507.09439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Understanding causal relationships in multivariate time series (MTS) is essential for effective decision-making in fields such as finance and marketing, where complex dependencies and lagged effects challenge conventional analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel architecture designed to enhance causal discovery by integrating dilated temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net effectively captures multiscale temporal dependencies through dilated convolutions while leveraging an adaptive thresholding strategy in its attention mechanism to eliminate spurious connections, ensuring both accuracy and interpretability. A statistical shuffle test validation further strengthens robustness by filtering false positives and improving causal inference reliability. Extensive evaluations on financial and marketing datasets demonstrate that DyCAST-Net consistently outperforms existing models such as TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation of causal delays and significantly reduces false discoveries, particularly in noisy environments. Moreover, attention heatmaps offer interpretable insights, uncovering hidden causal patterns such as the mediated effects of advertising on consumer behavior and the influence of macroeconomic indicators on financial markets. Case studies illustrate DyCAST-Net's ability to detect latent mediators and lagged causal factors, making it particularly effective in high-dimensional, dynamic settings. The model's architecture enhanced by RMSNorm stabilization and causal masking ensures scalability and adaptability across diverse application domains</li>
</ul>

<h3>Title: Transformers Don't In-Context Learn Least Squares Regression</h3>
<ul>
<li><strong>Authors: </strong>Joshua Hill, Benjamin Eyre, Elliot Creager</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09440">https://arxiv.org/abs/2507.09440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09440">https://arxiv.org/pdf/2507.09440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09440]] Transformers Don't In-Context Learn Least Squares Regression(https://arxiv.org/abs/2507.09440)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has emerged as a powerful capability of large pretrained transformers, enabling them to solve new tasks implicit in example input-output pairs without any gradient updates. Despite its practical success, the mechanisms underlying ICL remain largely mysterious. In this work we study synthetic linear regression to probe how transformers implement learning at inference time. Previous works have demonstrated that transformers match the performance of learning rules such as Ordinary Least Squares (OLS) regression or gradient descent and have suggested ICL is facilitated in transformers through the learned implementation of one of these techniques. In this work, we demonstrate through a suite of out-of-distribution generalization experiments that transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS. Finally, we highlight the role of the pretraining corpus in shaping ICL behaviour through a spectral analysis of the learned representations in the residual stream. Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors. This spectral signature is not shared by out-of-distribution inputs, and a metric characterizing the presence of this signature is highly correlated with low loss.</li>
</ul>

<h3>Title: Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Runze Yang, Longbing Cao, Xin You, Kun Fang, Jianxun Li, Jie Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09445">https://arxiv.org/abs/2507.09445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09445">https://arxiv.org/pdf/2507.09445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09445]] Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting(https://arxiv.org/abs/2507.09445)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The integration of Fourier transform and deep learning opens new avenues for time series forecasting. We reconsider the Fourier transform from a basis functions perspective. Specifically, the real and imaginary parts of the frequency components can be regarded as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively. We find that existing Fourier-based methods face inconsistent starting cycles and inconsistent series length issues. They fail to interpret frequency components precisely and overlook temporal information. Accordingly, the novel Fourier Basis Mapping (FBM) method addresses these issues by integrating time-frequency features through Fourier basis expansion and mapping in the time-frequency space. Our approach extracts explicit frequency features while preserving temporal characteristics. FBM supports plug-and-play integration with various types of neural networks by only adjusting the first initial projection layer for better performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear, MLP-based, and Transformer-based models, respectively, demonstrating the effectiveness of time-frequency features. Next, we propose a synergetic model architecture, termed FBM-S, which decomposes the seasonal, trend, and interaction effects into three separate blocks, each designed to model time-frequency features in a specialized manner. Finally, we introduce several techniques tailored for time-frequency features, including interaction masking, centralization, patching, rolling window projection, and multi-scale down-sampling. The results are validated on diverse real-world datasets for both long-term and short-term forecasting tasks with SOTA performance.</li>
</ul>

<h3>Title: SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity</h3>
<ul>
<li><strong>Authors: </strong>Micha≈Ç J√≥≈∫wik, Johan Pouwelse</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09453">https://arxiv.org/abs/2507.09453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09453">https://arxiv.org/pdf/2507.09453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09453]] SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity(https://arxiv.org/abs/2507.09453)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>The digitization of democratic processes promises greater accessibility but presents challenges in terms of security, privacy, and verifiability. Existing electronic voting systems often rely on centralized architectures, creating single points of failure and forcing too much trust in authorities, which contradicts democratic principles. This research addresses the challenge of creating a secure, private e-voting system with minimized trust dependencies designed for the most versatile personal device: the smartphone. We introduce SmartphoneDemocracy, a novel e-voting protocol that combines three key technologies: the emerging European Digital Identity (EUDI) Wallet for Sybil-resistant identity verification, Zero-Knowledge Proofs for privacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a resilient, serverless public bulletin board. Our protocol enables voters to register and cast ballots anonymously and verifiably directly from their smartphones. We provide a detailed protocol design, a security analysis against a defined threat model, and a performance evaluation demonstrating that the computational and network overhead is feasible for medium- to large-scale elections. By developing and prototyping this system, we demonstrate a viable path to empower citizens with a trustworthy, accessible, and user-controlled digital voting experience.</li>
</ul>

<h3>Title: SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Kang, Boyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09459">https://arxiv.org/abs/2507.09459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09459">https://arxiv.org/pdf/2507.09459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09459]] SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation(https://arxiv.org/abs/2507.09459)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose SegVec3D, a novel framework for 3D point cloud instance segmentation that integrates attention mechanisms, embedding learning, and cross-modal alignment. The approach builds a hierarchical feature extractor to enhance geometric structure modeling and enables unsupervised instance segmentation via contrastive clustering. It further aligns 3D data with natural language queries in a shared semantic space, supporting zero-shot retrieval. Compared to recent methods like Mask3D and ULIP, our method uniquely unifies instance segmentation and multimodal understanding with minimal supervision and practical deployability.</li>
</ul>

<h3>Title: La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Tomas Geffner, Kieran Didi, Zhonglin Cao, Danny Reidenbach, Zuobai Zhang, Christian Dallago, Emine Kucukbenli, Karsten Kreis, Arash Vahdat</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09466">https://arxiv.org/abs/2507.09466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09466">https://arxiv.org/pdf/2507.09466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09466]] La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching(https://arxiv.org/abs/2507.09466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Recently, many generative models for de novo protein structure design have emerged. Yet, only few tackle the difficult task of directly generating fully atomistic structures jointly with the underlying amino acid sequence. This is challenging, for instance, because the model must reason over side chains that change in length during generation. We introduce La-Proteina for atomistic protein design based on a novel partially latent protein representation: coarse backbone structure is modeled explicitly, while sequence and atomistic details are captured via per-residue latent variables of fixed dimensionality, thereby effectively side-stepping challenges of explicit side-chain representations. Flow matching in this partially latent space then models the joint distribution over sequences and full-atom structures. La-Proteina achieves state-of-the-art performance on multiple generation benchmarks, including all-atom co-designability, diversity, and structural validity, as confirmed through detailed structural analyses and evaluations. Notably, La-Proteina also surpasses previous models in atomistic motif scaffolding performance, unlocking critical atomistic structure-conditioned protein design tasks. Moreover, La-Proteina is able to generate co-designable proteins of up to 800 residues, a regime where most baselines collapse and fail to produce valid samples, demonstrating La-Proteina's scalability and robustness.</li>
</ul>

<h3>Title: CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Lingfeng He, De Cheng, Zhiheng Ma, Huaijie Wang, Dingwen Zhang, Nannan Wang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09471">https://arxiv.org/abs/2507.09471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09471">https://arxiv.org/pdf/2507.09471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09471]] CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning(https://arxiv.org/abs/2507.09471)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual Learning (CL) empowers AI models to continuously learn from sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based CL methods have garnered increasing attention due to their superior performance. They typically allocate a unique sub-module for learning each task, with a task recognizer to select the appropriate sub-modules for testing images. However, due to the feature subspace misalignment from independently trained sub-modules, these methods tend to produce ambiguous decisions under misleading task-ids. To address this, we propose Cross-subspace Knowledge Alignment and Aggregation (CKAA), a novel framework that enhances model robustness against misleading task-ids through two key innovations: (1) Dual-level Knowledge Alignment (DKA): By aligning intra-class feature distributions across different subspaces and learning a robust global classifier through a feature simulation process, DKA enables the model to distinguish features from both correct and incorrect subspaces during training. (2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference scheme that adaptively aggregates task-specific knowledge from relevant sub-modules based on task-confidence scores, avoiding overconfidence in misleading task-id predictions. Extensive experiments demonstrate that CKAA outperforms existing PEFT-based CL methods.</li>
</ul>

<h3>Title: Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yangning Li, Weizhi Zhang, Yuyao Yang, Wei-Chieh Huang, Yaozu Wu, Junyu Luo, Yuanchen Bei, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Chunkit Chan, Yankai Chen, Zhongfen Deng, Yinghui Li, Hai-Tao Zheng, Dongyuan Li, Renhe Jiang, Ming Zhang, Yangqiu Song, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09477">https://arxiv.org/abs/2507.09477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09477">https://arxiv.org/pdf/2507.09477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09477]] Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs(https://arxiv.org/abs/2507.09477)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at this https URL.</li>
</ul>

<h3>Title: Discrete Differential Principle for Continuous Smooth Function Representation</h3>
<ul>
<li><strong>Authors: </strong>Guoyou Wang, Yihua Tan, Shiqi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09480">https://arxiv.org/abs/2507.09480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09480">https://arxiv.org/pdf/2507.09480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09480]] Discrete Differential Principle for Continuous Smooth Function Representation(https://arxiv.org/abs/2507.09480)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Taylor's formula holds significant importance in function representation, such as solving differential difference equations, ordinary differential equations, partial differential equations, and further promotes applications in visual perception, complex control, fluid mechanics, weather forecasting and thermodynamics. However, the Taylor's formula suffers from the curse of dimensionality and error propagation during derivative computation in discrete situations. In this paper, we propose a new discrete differential operator to estimate derivatives and to represent continuous smooth function locally using the Vandermonde coefficient matrix derived from truncated Taylor series. Our method simultaneously computes all derivatives of orders less than the number of sample points, inherently mitigating error propagation. Utilizing equidistant uniform sampling, it achieves high-order accuracy while alleviating the curse of dimensionality. We mathematically establish rigorous error bounds for both derivative estimation and function representation, demonstrating tighter bounds for lower-order derivatives. We extend our method to the two-dimensional case, enabling its use for multivariate derivative calculations. Experiments demonstrate the effectiveness and superiority of the proposed method compared to the finite forward difference method for derivative estimation and cubic spline and linear interpolation for function representation. Consequently, our technique offers broad applicability across domains such as vision representation, feature extraction, fluid mechanics, and cross-media imaging.</li>
</ul>

<h3>Title: ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Changli Wang, Rui Wu, Fang Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09482">https://arxiv.org/abs/2507.09482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09482">https://arxiv.org/pdf/2507.09482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09482]] ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning(https://arxiv.org/abs/2507.09482)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human emotions are complex, with sarcasm being a subtle and distinctive form. Despite progress in sarcasm research, sarcasm generation remains underexplored, primarily due to the overreliance on textual modalities and the neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm generation dataset with 4,970 samples, each containing an image, a sarcastic text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning. PPO utilizes reward scores from DIP to steer the generation of sarcastic texts, while contrastive learning encourages the model to favor outputs with higher reward scores. These strategies improve overall generation quality and produce texts with more pronounced sarcastic intent. We evaluate ViSP across five metric sets and find it surpasses all baselines, including large language models, underscoring their limitations in sarcasm generation. Furthermore, we analyze the distributions of Sarcasm Scores and Factual Incongruity for both M2SaG and the texts generated by ViSP. The generated texts exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic content than the original dataset. % The dataset and code will be publicly available. Our dataset and code will be released at \textit{this https URL}.</li>
</ul>

<h3>Title: Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Junjie Liu, Yuanhe Tian, Yan Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09485">https://arxiv.org/abs/2507.09485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09485">https://arxiv.org/pdf/2507.09485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09485]] Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis(https://arxiv.org/abs/2507.09485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in social media scenarios to identify the sentiment polarity of specific aspect terms in a sentence. Although many existing studies leverage large language models (LLMs) to perform ABSA due to their strong context understanding capabilities, they still face challenges to learn the context information in the running text because of the short text, as well as the small and unbalanced labeled training data, where most data are labeled with positive sentiment. Data augmentation (DA) is a feasible strategy for providing richer contextual information, especially when using LLMs to create synthetic training data, but faces challenges in ensuring a high quality of the augmented this http URL this paper, we propose an LLM-based ABSA approach with training data this http URL, an LLM is prompted to generate augmented training data based on the original training data, so as to construct a new training data with larger size and balanced label distributions to better train an ABSA model. Meanwhile, in order to improve the quality of the augmented data, we propose a reinforcement learning approach to optimize the data augmentation. this http URL results and further analyses on English benchmark datasets for ABSA demonstrate the effectiveness of our approach, where superior performance is observed over strong baselines and most existing studies.</li>
</ul>

<h3>Title: SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Fuyin Ye, Erwen Yao, Jianyong Chen, Fengmei He, Junxiang Zhang, Lihao Ni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09492">https://arxiv.org/abs/2507.09492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09492">https://arxiv.org/pdf/2507.09492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09492]] SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification(https://arxiv.org/abs/2507.09492)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Hyperspectral image classification plays a pivotal role in precision agriculture, providing accurate insights into crop health monitoring, disease detection, and soil analysis. However, traditional methods struggle with high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled samples, often leading to suboptimal performance. To address these challenges, we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines tensor decomposition with regularization mechanisms to dynamically adjust tensor ranks, ensuring optimal feature representation tailored to the complexity of the data. Building upon SDTN, we propose the Tensor-Regularized Network (TRN), which integrates the features extracted by SDTN into a lightweight network capable of capturing spectral-spatial features at multiple scales. This approach not only maintains high classification accuracy but also significantly reduces computational complexity, making the framework highly suitable for real-time deployment in resource-constrained environments. Experiments on PaviaU datasets demonstrate significant improvements in accuracy and reduced model parameters compared to state-of-the-art methods.</li>
</ul>

<h3>Title: GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities</h3>
<ul>
<li><strong>Authors: </strong>Siyi Wu, Zeyu Wang, Xinyuan Song, Zhengpeng Zhou, Lifan Sun, Tianyu Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09497">https://arxiv.org/abs/2507.09497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09497">https://arxiv.org/pdf/2507.09497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09497]] GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities(https://arxiv.org/abs/2507.09497)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modern enterprise environments demand intelligent systems capable of handling complex, dynamic, and multi-faceted tasks with high levels of autonomy and adaptability. However, traditional single-purpose AI systems often lack sufficient coordination, memory reuse, and task decomposition capabilities, limiting their scalability in realistic settings. To address these challenges, we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent (A2A) communication layer built on the Model Context Protocol (MCP), allowing independent agents to coordinate through asynchronous, protocol-compliant interactions. It incorporates the Experience Pack (XP) architecture, a layered memory system that preserves both task rationales and execution traces, enabling structured knowledge retention and continual learning. Moreover, our system integrates advanced features including multi-turn contextual dialogue, long-short term memory modules, and dynamic safety validation, supporting robust, real-time strategy adaptation. Empirical results on complex task orchestration benchmarks and case study demonstrate that GoalfyMax achieves superior adaptability, coordination, and experience reuse compared to baseline frameworks. These findings highlight its potential as a scalable, future-ready foundation for multi-agent intelligent systems.</li>
</ul>

<h3>Title: Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Liang, Hui Chen, Yizhe Xiong, Zihan Zhou, Mengyao Lyu, Zijia Lin, Shuaicheng Niu, Sicheng Zhao, Jungong Han, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09500">https://arxiv.org/abs/2507.09500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09500">https://arxiv.org/pdf/2507.09500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09500]] Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations(https://arxiv.org/abs/2507.09500)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under challenging real-world distribution shifts.</li>
</ul>

<h3>Title: A Mixture of Linear Corrections Generates Secure Code</h3>
<ul>
<li><strong>Authors: </strong>Weichen Yu, Ravi Mangal, Terry Zhuo, Matt Fredrikson, Corina S. Pasareanu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09508">https://arxiv.org/abs/2507.09508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09508">https://arxiv.org/pdf/2507.09508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09508]] A Mixture of Linear Corrections Generates Secure Code(https://arxiv.org/abs/2507.09508)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become proficient at sophisticated code-generation tasks, yet remain ineffective at reliably detecting or avoiding code vulnerabilities. Does this deficiency stem from insufficient learning about code vulnerabilities, or is it merely a result of ineffective prompting? Using representation engineering techniques, we investigate whether LLMs internally encode the concepts necessary to identify code vulnerabilities. We find that current LLMs encode precise internal representations that distinguish vulnerable from secure code--achieving greater accuracy than standard prompting approaches. Leveraging these vulnerability-sensitive representations, we develop an inference-time steering technique that subtly modulates the model's token-generation probabilities through a mixture of corrections (MoC). Our method effectively guides LLMs to produce less vulnerable code without compromising functionality, demonstrating a practical approach to controlled vulnerability management in generated code. Notably, MoC enhances the security ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving functionality on HumanEval pass@1 by 2.1\%.</li>
</ul>

<h3>Title: How Important is `Perfect' English for Machine Translation Prompts?</h3>
<ul>
<li><strong>Authors: </strong>Patr√≠cia Schmidtov√°, Niyati Bafna, Seth Aycock, Gianluca Vico, Wiktor Kamzela, Katharina H√§mmerl, Vil√©m Zouhar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09509">https://arxiv.org/abs/2507.09509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09509">https://arxiv.org/pdf/2507.09509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09509]] How Important is `Perfect' English for Machine Translation Prompts?(https://arxiv.org/abs/2507.09509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved top results in recent machine translation evaluations, but they are also known to be sensitive to errors and perturbations in their prompts. We systematically evaluate how both humanly plausible and synthetic errors in user prompts affect LLMs' performance on two related tasks: Machine translation and machine translation evaluation. We provide both a quantitative analysis and qualitative insights into how the models respond to increasing noise in the user prompt. The prompt quality strongly affects the translation performance: With many errors, even a good prompt can underperform a minimal or poor prompt without errors. However, different noise types impact translation quality differently, with character-level and combined noisers degrading performance more than phrasal perturbations. Qualitative analysis reveals that lower prompt quality largely leads to poorer instruction following, rather than directly affecting translation quality itself. Further, LLMs can still translate in scenarios with overwhelming random noise that would make the prompt illegible to humans.</li>
</ul>

<h3>Title: QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Tien-Yu Chi, Hung-Yueh Chiang, Diana Marculescu, Kai-Chiang Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09514">https://arxiv.org/abs/2507.09514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09514">https://arxiv.org/pdf/2507.09514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09514]] QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models(https://arxiv.org/abs/2507.09514)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>State space models (SSMs) reduce the quadratic complexity of transformers by leveraging linear recurrence. Recently, VMamba has emerged as a strong SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in its four-directional scan. We propose QuarterMap, a post-training activation pruning method that removes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling. Our method improves throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11% speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a domain-specific model that shares the same four-directional scanning structure, where it consistently improves throughput while preserving accuracy across multiple medical imaging tasks. Compared to token merging methods like ToMe, QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our method offers a plug-and-play tool for deployment-time efficiency without compromising transferability.</li>
</ul>

<h3>Title: VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Son Nguyen, Giang Nguyen, Hung Dao, Thao Do, Daeyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09531">https://arxiv.org/abs/2507.09531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09531">https://arxiv.org/pdf/2507.09531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09531]] VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization(https://arxiv.org/abs/2507.09531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Key Information Extraction (KIE) underpins the understanding of visual documents (e.g., receipts and contracts) by extracting precise semantic content and accurately capturing spatial structure. Yet existing multimodal large language models (MLLMs) often perform poorly on dense documents and rely on vision tokenization approaches that scale with image size, leading to redundant computation and memory inefficiency. To address these challenges, we introduce VDInstruct, an MLLM that separates spatial region detection from semantic feature extraction. Central to our model is a content-aware tokenization strategy: rather than fragmenting the entire image uniformly, it generates tokens in proportion to document complexity, preserving critical structure while eliminating wasted tokens. Leveraging a three-stage training paradigm, our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching or exceeding the accuracy of leading approaches while reducing the number of image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its robustness to unseen documents. These findings show that content-aware tokenization combined with explicit layout modeling offers a promising direction forward for document understanding. Data, source code, and model weights will be made publicly available.</li>
</ul>

<h3>Title: DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Zihao Xiong, Fei Zhou, Fengyi Wu, Shuai Yuan, Maixia Fu, Zhenming Peng, Jian Yang, Yimian Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09541">https://arxiv.org/abs/2507.09541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09541">https://arxiv.org/pdf/2507.09541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09541]] DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection(https://arxiv.org/abs/2507.09541)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Infrared small target detection plays a vital role in remote sensing, industrial monitoring, and various civilian applications. Despite recent progress powered by deep learning, many end-to-end convolutional models tend to pursue performance by stacking increasingly complex architectures, often at the expense of interpretability, parameter efficiency, and generalization. These models typically overlook the intrinsic sparsity prior of infrared small targets--an essential cue that can be explicitly modeled for both performance and efficiency gains. To address this, we revisit the model-based paradigm of Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network (DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware prior into a learnable architecture. Unlike conventional deep unfolding methods that rely on static, globally learned parameters, DRPCA-Net introduces a dynamic unfolding mechanism via a lightweight hypernetwork. This design enables the model to adaptively generate iteration-wise parameters conditioned on the input scene, thereby enhancing its robustness and generalization across diverse backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to better capture contextual variations within the background, leading to more accurate low-rank estimation and improved separation of small targets. Extensive experiments on multiple public infrared datasets demonstrate that DRPCA-Net significantly outperforms existing state-of-the-art methods in detection accuracy. Code is available at this https URL.</li>
</ul>

<h3>Title: Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events</h3>
<ul>
<li><strong>Authors: </strong>Ilaria Vascotto, Valentina Blasone, Alex Rodriguez, Alessandro Bonaita, Luca Bortolussi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09545">https://arxiv.org/abs/2507.09545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09545">https://arxiv.org/pdf/2507.09545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09545]] Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events(https://arxiv.org/abs/2507.09545)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The usage of eXplainable Artificial Intelligence (XAI) methods has become essential in practical applications, given the increasing deployment of Artificial Intelligence (AI) models and the legislative requirements put forward in the latest years. A fundamental but often underestimated aspect of the explanations is their robustness, a key property that should be satisfied in order to trust the explanations. In this study, we provide some preliminary insights on evaluating the reliability of explanations in the specific case of unbalanced datasets, which are very frequent in high-risk use-cases, but at the same time considerably challenging for both AI models and XAI methods. We propose a simple evaluation focused on the minority class (i.e. the less frequent one) that leverages on-manifold generation of neighbours, explanation aggregation and a metric to test explanation consistency. We present a use-case based on a tabular dataset with numerical features focusing on the occurrence of frost events.</li>
</ul>

<h3>Title: EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Bolun Zheng, Xinjie Liu, Qianyu Zhang, Canjin Wang, Fangni Chen, Mingen Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09560">https://arxiv.org/abs/2507.09560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09560">https://arxiv.org/pdf/2507.09560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09560]] EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation(https://arxiv.org/abs/2507.09560)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>3D hand pose estimation has garnered great attention in recent years due to its critical applications in human-computer interaction, virtual reality, and related fields. The accurate estimation of hand joints is essential for high-quality hand pose estimation. However, existing methods neglect the importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints overall and often fail to account for the phenomenon of error accumulation for distal joints in gesture estimation, which can cause certain joints to incur larger errors, resulting in misalignments and artifacts in the pose estimation and degrading the overall reconstruction quality. To address this challenge, we propose a novel segmented architecture for enhanced hand pose estimation (EHPE). We perform local extraction of TIP and wrist, thus alleviating the effect of error accumulation on TIP prediction and further reduce the predictive errors for all joints on this basis. EHPE consists of two key stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions of the TIP and wrist joints are estimated to provide an initial accurate joint configuration; In the Prior Guided Joints Estimation stage (PG-stage), a dual-branch interaction network is employed to refine the positions of the remaining joints. Extensive experiments on two widely used benchmarks demonstrate that EHPE achieves state-of-the-arts performance. Code is available at this https URL.</li>
</ul>

<h3>Title: Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges</h3>
<ul>
<li><strong>Authors: </strong>Yidong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09562">https://arxiv.org/abs/2507.09562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09562">https://arxiv.org/pdf/2507.09562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09562]] Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges(https://arxiv.org/abs/2507.09562)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.</li>
</ul>

<h3>Title: A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Varshney, Akanksha Raj, Divya Sangwan, Sharif Abuadbba, Rina Mishra, Yansong Gao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09564">https://arxiv.org/abs/2507.09564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09564">https://arxiv.org/pdf/2507.09564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09564]] A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol(https://arxiv.org/abs/2507.09564)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Phishing is a prevalent cyberattack that uses look-alike websites to deceive users into revealing sensitive information. Numerous efforts have been made by the Internet community and security organizations to detect, prevent, or train users to avoid falling victim to phishing attacks. Most of this research over the years has been highly diverse and application-oriented, often serving as standalone solutions for HTTP clients, servers, or third parties. However, limited work has been done to develop a comprehensive or proactive protocol-oriented solution to effectively counter phishing attacks. Inspired by the concept of certificate transparency, which allows certificates issued by Certificate Authorities (CAs) to be publicly verified by clients, thereby enhancing transparency, we propose a concept called Page Transparency (PT) for the web. The proposed PT requires login pages that capture users' sensitive information to be publicly logged via PLS and made available to web clients for verification. The pages are verified to be logged using cryptographic proofs. Since all pages are logged on a PLS and visually compared with existing pages through a comprehensive visual page-matching algorithm, it becomes impossible for an attacker to register a deceptive look-alike page on the PLS and receive the cryptographic proof required for client verification. All implementations occur on the client side, facilitated by the introduction of a new HTTP PT header, eliminating the need for platform-specific changes or the installation of third-party solutions for phishing prevention.</li>
</ul>

<h3>Title: Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives</h3>
<ul>
<li><strong>Authors: </strong>Heeba Shakeel, Tanvir Ahmad, Chandni Saxena</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09565">https://arxiv.org/abs/2507.09565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09565">https://arxiv.org/pdf/2507.09565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09565]] Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives(https://arxiv.org/abs/2507.09565)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We introduce a dataset for classifying wellness dimensions in social media user posts, covering six key aspects: physical, emotional, social, intellectual, spiritual, and vocational. The dataset is designed to capture these dimensions in user-generated content, with a comprehensive annotation framework developed under the guidance of domain experts. This framework allows for the classification of text spans into the appropriate wellness categories. We evaluate both traditional machine learning models and advanced transformer-based models for this multi-class classification task, with performance assessed using precision, recall, and F1-score, averaged over 10-fold cross-validation. Post-hoc explanations are applied to ensure the transparency and interpretability of model decisions. The proposed dataset contributes to region-specific wellness assessments in social media and paves the way for personalized well-being evaluations and early intervention strategies in mental health. We adhere to ethical considerations for constructing and releasing our experiments and dataset publicly on Github.</li>
</ul>

<h3>Title: WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending</h3>
<ul>
<li><strong>Authors: </strong>Zhe Wang, Jingbo Zhang, Tianyi Wei, Wanchao Su, Can Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09573">https://arxiv.org/abs/2507.09573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09573">https://arxiv.org/pdf/2507.09573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09573]] WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending(https://arxiv.org/abs/2507.09573)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Artistic typography aims to stylize input characters with visual effects that are both creative and legible. Traditional approaches rely heavily on manual design, while recent generative models, particularly diffusion-based methods, have enabled automated character stylization. However, existing solutions remain limited in interactivity, lacking support for localized edits, iterative refinement, multi-character composition, and open-ended prompt interpretation. We introduce WordCraft, an interactive artistic typography system that integrates diffusion models to address these limitations. WordCraft features a training-free regional attention mechanism for precise, multi-region generation and a noise blending that supports continuous refinement without compromising visual quality. To support flexible, intent-driven generation, we incorporate a large language model to parse and structure both concrete and abstract user prompts. These components allow our framework to synthesize high-quality, stylized typography across single- and multi-character inputs across multiple languages, supporting diverse user-centered workflows. Our system significantly enhances interactivity in artistic typography synthesis, opening up creative possibilities for artists and designers.</li>
</ul>

<h3>Title: MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Jiuxiang Gu, Wen Xiao, Junjie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09574">https://arxiv.org/abs/2507.09574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09574">https://arxiv.org/pdf/2507.09574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09574]] MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models(https://arxiv.org/abs/2507.09574)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: this https URL</li>
</ul>

<h3>Title: Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ming Yin, Fu Wang, Xujiong Ye, Yanda Meng, Zeyu Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09577">https://arxiv.org/abs/2507.09577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09577">https://arxiv.org/pdf/2507.09577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09577]] Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation(https://arxiv.org/abs/2507.09577)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Surgical video segmentation is a critical task in computer-assisted surgery, essential for enhancing surgical quality and patient outcomes. Recently, the Segment Anything Model 2 (SAM2) framework has demonstrated remarkable advancements in both image and video segmentation. However, the inherent limitations of SAM2's greedy selection memory design are amplified by the unique properties of surgical videos-rapid instrument movement, frequent occlusion, and complex instrument-tissue interaction-resulting in diminished performance in the segmentation of complex, long videos. To address these challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video object segmentation strategy, featuring novel context-aware and occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against occlusions and interactions arising from complex instrument movements while maintaining accuracy in segmenting objects throughout videos. Employing a multi-target, single-loop, one-prompt inference further enhances the efficiency of the tracking process in multi-instrument videos. Without introducing any additional parameters or requiring further training, MA-SAM2 achieved performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and EndoVis2018 datasets, respectively, demonstrating its potential for practical surgical applications.</li>
</ul>

<h3>Title: PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets</h3>
<ul>
<li><strong>Authors: </strong>Marc Bara</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09579">https://arxiv.org/abs/2507.09579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09579">https://arxiv.org/pdf/2507.09579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09579]] PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets(https://arxiv.org/abs/2507.09579)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We present PromptChain, a decentralized Web3 architecture that establishes AI prompts as first-class digital assets with verifiable ownership, version control, and monetization capabilities. Current centralized platforms lack mechanisms for proper attribution, quality assurance, or fair compensation for prompt creators. PromptChain addresses these limitations through a novel integration of IPFS for immutable storage, smart contracts for governance, and token incentives for community curation. Our design includes: (1) a comprehensive metadata schema for cross-model compatibility, (2) a stake-weighted validation mechanism to align incentives, and (3) a token economy that rewards contributors proportionally to their impact. The proposed architecture demonstrates how decentralized systems could potentially match centralized alternatives in efficiency while providing superior ownership guarantees and censorship resistance through blockchain-anchored provenance tracking. By decoupling prompts from specific AI models or outputs, this work establishes the foundation for an open ecosystem of human-AI collaboration in the Web3 era, representing the first systematic treatment of prompts as standalone digital assets with dedicated decentralized infrastructure.</li>
</ul>

<h3>Title: AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Yijian Liu, Liheng Ji, Han Luo, Wenjie Li, Xiaofei Zhou, Chiyun Feng, Puji Wang, Yuhan Cao, Geyuan Zhang, Xiaojian Li, Rongwu Xu, Yilei Chen, Tianxing He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09580">https://arxiv.org/abs/2507.09580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09580">https://arxiv.org/pdf/2507.09580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09580]] AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models(https://arxiv.org/abs/2507.09580)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities across a variety of domains. However, their applications in cryptography, which serves as a foundational pillar of cybersecurity, remain largely unexplored. To address this gap, we propose \textbf{AICrypto}, the first comprehensive benchmark designed to evaluate the cryptographic capabilities of LLMs. The benchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF) challenges, and 18 proof problems, covering a broad range of skills from factual memorization to vulnerability exploitation and formal reasoning. All tasks are carefully reviewed or constructed by cryptography experts to ensure correctness and rigor. To support automated evaluation of CTF challenges, we design an agent-based framework. To gain deeper insight into the current state of cryptographic proficiency in LLMs, we introduce human expert performance baselines for comparison across all task types. Our evaluation of 17 leading LLMs reveals that state-of-the-art models match or even surpass human experts in memorizing cryptographic concepts, exploiting common vulnerabilities, and routine proofs. However, they still lack a deep understanding of abstract mathematical concepts and struggle with tasks that require multi-step reasoning and dynamic analysis. We hope this work could provide insights for future research on LLMs in cryptographic applications. Our code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Demystifying Flux Architecture</h3>
<ul>
<li><strong>Authors: </strong>Or Greenberg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09595">https://arxiv.org/abs/2507.09595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09595">https://arxiv.org/pdf/2507.09595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09595]] Demystifying Flux Architecture(https://arxiv.org/abs/2507.09595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>FLUX.1 is a diffusion-based text-to-image generation model developed by Black Forest Labs, designed to achieve faithful text-image alignment while maintaining high image quality and diversity. FLUX is considered state-of-the-art in text-to-image generation, outperforming popular models such as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly available as open source, the authors have not released official technical documentation detailing the model's architecture or training setup. This report summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's architecture directly from its source code, to support its adoption as a backbone for future research and development. This document is an unofficial technical report and is not published or endorsed by the original developers or their affiliated institutions.</li>
</ul>

<h3>Title: NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance</h3>
<ul>
<li><strong>Authors: </strong>Hanwool Lee, Sara Yu, Yewon Hwang, Jonghyun Choi, Heejae Ahn, Sungbum Jung, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09601">https://arxiv.org/abs/2507.09601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09601">https://arxiv.org/pdf/2507.09601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09601]] NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance(https://arxiv.org/abs/2507.09601)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>General-purpose sentence embedding models often struggle to capture specialized financial semantics, especially in low-resource languages like Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual embedding models fine-tuned with 18.8K high-confidence triplets that pair in-domain paraphrases, hard negatives derived from a semantic-shift typology, and exact Korean-English translations. Concurrently, we release KorFinSTS, a 1,921-pair Korean financial STS benchmark spanning news, disclosures, research reports, and regulations, designed to expose nuances that general benchmarks miss. When evaluated against seven open-license baselines, NMIXX's multilingual bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and +0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing other models by the largest margin, while revealing a modest trade-off in general STS performance. Our analysis further shows that models with richer Korean token coverage adapt more effectively, underscoring the importance of tokenizer design in low-resource, cross-lingual settings. By making both models and the benchmark publicly available, we provide the community with robust tools for domain-adapted, multilingual representation learning in finance.</li>
</ul>

<h3>Title: DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences</h3>
<ul>
<li><strong>Authors: </strong>Bocheng Ju, Junchao Fan, Jiaqi Liu, Xiaolin Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09602">https://arxiv.org/abs/2507.09602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09602">https://arxiv.org/pdf/2507.09602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09602]] DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences(https://arxiv.org/abs/2507.09602)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning enables collaborative machine learning while preserving data privacy. However, the rise of federated unlearning, designed to allow clients to erase their data from the global model, introduces new privacy concerns. Specifically, the gradient exchanges during the unlearning process can leak sensitive information about deleted data. In this paper, we introduce DRAGD, a novel attack that exploits gradient discrepancies before and after unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced version of DRAGD that leverages publicly available prior data to improve reconstruction accuracy, particularly for complex datasets like facial images. Extensive experiments across multiple datasets demonstrate that DRAGD and DRAGDP significantly outperform existing methods in data this http URL work highlights a critical privacy vulnerability in federated unlearning and offers a practical solution, advancing the security of federated unlearning systems in real-world applications.</li>
</ul>

<h3>Title: Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Wang, Yuehan Dong, Junchao Fan, Xiaolin Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09607">https://arxiv.org/abs/2507.09607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09607">https://arxiv.org/pdf/2507.09607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09607]] Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC(https://arxiv.org/abs/2507.09607)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Private inference based on Secure Multi-Party Computation (MPC) addresses data privacy risks in Machine Learning as a Service (MLaaS). However, existing MPC-based private inference frameworks focuses on semi-honest or honest majority models, whose threat models are overly idealistic, while malicious security dishonest majority models face the challenge of low efficiency. To balance security and efficiency, we propose a private inference framework using Helper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This framework includes our designed five MPC protocols and a co-optimized strategy. These protocols achieve efficient fixed-round multiplication, exponentiation, and polynomial operations, providing foundational primitives for private inference. The co-optimized strategy balances inference efficiency and accuracy. To enhance efficiency, we employ polynomial approximation for nonlinear layers. For improved accuracy, we construct sixth-order polynomial approximation within a fixed interval to achieve high-precision activation function fitting and introduce parameter-adjusted batch normalization layers to constrain the activation escape problem. Benchmark results on LeNet and AlexNet show our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration in WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high accuracy with only 0.04%-1.08% relative errors.</li>
</ul>

<h3>Title: Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive</h3>
<ul>
<li><strong>Authors: </strong>You Huang, Lichao Chen, Jiayi Ji, Liujuan Cao, Shengchuan Zhang, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09612">https://arxiv.org/abs/2507.09612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09612">https://arxiv.org/pdf/2507.09612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09612]] Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive(https://arxiv.org/abs/2507.09612)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Interactive segmentation (IS) improves annotation efficiency by segmenting target regions from user prompts, with widespread applications in real-world scenarios. Current approaches face a critical trade-off: dense-token methods achieve superior accuracy and detail preservation but suffer from prohibitively slow processing on CPU devices, while the Segment Anything Model (SAM) advances the field with sparse prompt tokens for fast inference but compromises segmentation quality. In this paper, we propose Inter2Former to address this challenge by optimizing computation allocation in dense-token processing, which introduces four key enhancements. First, we propose Dynamic Prompt Embedding (DPE) that adaptively processes only regions of interest while avoiding additional overhead from background tokens. Second, we introduce Dynamic Hybrid Attention (DHA), which leverages previous segmentation masks to route tokens through either full attention (O(N2)) for boundary regions or our proposed efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation strategies in FFN modules with CPU-optimized parallel processing. Finally, we present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which localizes objects with a lightweight MLP and performs fine-grained upsampling only in detected regions. Experimental results on high-precision IS benchmarks demonstrate that Inter2Former achieves SOTA performance with high efficiency on CPU devices.</li>
</ul>

<h3>Title: Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score</h3>
<ul>
<li><strong>Authors: </strong>Eman Ali, Sathira Silva, Chetan Arora, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09615">https://arxiv.org/abs/2507.09615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09615">https://arxiv.org/pdf/2507.09615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09615]] Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score(https://arxiv.org/abs/2507.09615)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) like CLIP excel in zero-shot learning by aligning image and text representations through contrastive pretraining. Existing approaches to unsupervised adaptation (UA) for fine-grained classification with VLMs either rely on fixed alignment scores that cannot capture evolving, subtle class distinctions or use computationally expensive pseudo-labeling strategies that limit scalability. In contrast, we show that modeling fine-grained cross-modal interactions during adaptation produces more accurate, class-discriminative pseudo-labels and substantially improves performance over state-of-the-art (SOTA) methods. We introduce Fine-grained Alignment and Interaction Refinement (FAIR), an innovative approach that dynamically aligns localized image features with descriptive language embeddings through a set of Class Description Anchors (CDA). This enables the definition of a Learned Alignment Score (LAS), which incorporates CDA as an adaptive classifier, facilitating cross-modal interactions to improve self-training in unsupervised adaptation. Furthermore, we propose a self-training weighting mechanism designed to refine pseudo-labels in the presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial performance boost in fine-grained unsupervised adaptation, achieving a notable overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.</li>
</ul>

<h3>Title: MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression</h3>
<ul>
<li><strong>Authors: </strong>Ofir Gordon, Ariel Lapid, Elad Cohen, Yarden Yagil, Arnon Netzer, Hai Victor Habi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09616">https://arxiv.org/abs/2507.09616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09616">https://arxiv.org/pdf/2507.09616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09616]] MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression(https://arxiv.org/abs/2507.09616)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Deploying transformer-based neural networks on resource-constrained edge devices presents a significant challenge. This challenge is often addressed through various techniques, such as low-rank approximation and mixed-precision quantization. In this work, we introduce Mixed Low-Rank and Quantization (MLoRQ), a novel method that integrates both techniques. MLoRQ employs a two-stage optimization process to determine optimal bit-width and rank assignments for each layer, adhering to predefined memory constraints. This process includes: (i) an intra-layer optimization that identifies potentially optimal compression solutions out of all low-rank and quantization combinations; (ii) an inter-layer optimization that assigns bit-width precision and rank to each layer while ensuring the memory constraint is met. An optional final step applies a sequential optimization process using a modified adaptive rounding technique to mitigate compression-induced errors in joint low-rank approximation and quantization. The method is compatible and can be seamlessly integrated with most existing quantization algorithms. MLoRQ shows state-of-the-art results with up to 15\% performance improvement, evaluated on Vision Transformers for image classification, object detection, and instance segmentation tasks.</li>
</ul>

<h3>Title: Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection</h3>
<ul>
<li><strong>Authors: </strong>Yilin Lu, Jianghang Lin, Linhuang Xie, Kai Zhao, Yansong Qu, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09619">https://arxiv.org/abs/2507.09619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09619">https://arxiv.org/pdf/2507.09619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09619]] Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection(https://arxiv.org/abs/2507.09619)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Anomaly inspection plays a vital role in industrial manufacturing, but the scarcity of anomaly samples significantly limits the effectiveness of existing methods in tasks such as localization and classification. While several anomaly synthesis approaches have been introduced for data augmentation, they often struggle with low realism, inaccurate mask alignment, and poor generalization. To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a region-guided, few-shot anomaly image-mask pair generation framework. GAA leverages the strong priors of a pretrained latent diffusion model to generate realistic, diverse, and semantically aligned anomalies using only a small number of samples. The framework first employs Localized Concept Decomposition to jointly model the semantic features and spatial information of anomalies, enabling flexible control over the type and location of anomalies. It then utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained semantic clustering of anomaly concepts, thereby enhancing the consistency of anomaly representations. Subsequently, a region-guided mask generation strategy ensures precise alignment between anomalies and their corresponding masks, while a low-quality sample filtering module is introduced to further improve the overall quality of the generated samples. Extensive experiments on the MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance in both anomaly synthesis quality and downstream tasks such as localization and classification.</li>
</ul>

<h3>Title: CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Xiaojie Lin, Baihe Ma, Xu Wang, Guangsheng Yu, Ying He, Wei Ni, Ren Ping Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09624">https://arxiv.org/abs/2507.09624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09624">https://arxiv.org/pdf/2507.09624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09624]] CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories(https://arxiv.org/abs/2507.09624)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Driving trajectory data remains vulnerable to privacy breaches despite existing mitigation measures. Traditional methods for detecting driving trajectories typically rely on map-matching the path using Global Positioning System (GPS) data, which is susceptible to GPS data outage. This paper introduces CAN-Trace, a novel privacy attack mechanism that leverages Controller Area Network (CAN) messages to uncover driving trajectories, posing a significant risk to drivers' long-term privacy. A new trajectory reconstruction algorithm is proposed to transform the CAN messages, specifically vehicle speed and accelerator pedal position, into weighted graphs accommodating various driving statuses. CAN-Trace identifies driving trajectories using graph-matching algorithms applied to the created graphs in comparison to road networks. We also design a new metric to evaluate matched candidates, which allows for potential data gaps and matching inaccuracies. Empirical validation under various real-world conditions, encompassing different vehicles and driving regions, demonstrates the efficacy of CAN-Trace: it achieves an attack success rate of up to 90.59% in the urban region, and 99.41% in the suburban region.</li>
</ul>

<h3>Title: SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks</h3>
<ul>
<li><strong>Authors: </strong>Salvatore Citraro, Edith Haim, Alessandra Carini, Cynthia S. Q. Siew, Giulio Rossetti, Massimo Stella</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09628">https://arxiv.org/abs/2507.09628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09628">https://arxiv.org/pdf/2507.09628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09628]] SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks(https://arxiv.org/abs/2507.09628)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce SpreadPy as a Python library for simulating spreading activation in cognitive single-layer and multiplex networks. Our tool is designed to perform numerical simulations testing structure-function relationships in cognitive processes. By comparing simulation results with grounded theories in knowledge modelling, SpreadPy enables systematic investigations of how activation dynamics reflect cognitive, psychological and clinical phenomena. We demonstrate the library's utility through three case studies: (1) Spreading activation on associative knowledge networks distinguishes students with high versus low math anxiety, revealing anxiety-related structural differences in conceptual organization; (2) Simulations of a creativity task show that activation trajectories vary with task difficulty, exposing how cognitive load modulates lexical access; (3) In individuals with aphasia, simulated activation patterns on lexical networks correlate with empirical error types (semantic vs. phonological) during picture-naming tasks, linking network structure to clinical impairments. SpreadPy's flexible framework allows researchers to model these processes using empirically derived or theoretical networks, providing mechanistic insights into individual differences and cognitive impairments. The library is openly available, supporting reproducible research in psychology, neuroscience, and education research.</li>
</ul>

<h3>Title: An Exploration of Knowledge Editing for Arabic</h3>
<ul>
<li><strong>Authors: </strong>Basel Mousi, Nadir Durrani, Fahim Dalvi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09629">https://arxiv.org/abs/2507.09629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09629">https://arxiv.org/pdf/2507.09629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09629]] An Exploration of Knowledge Editing for Arabic(https://arxiv.org/abs/2507.09629)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While Knowledge Editing (KE) has been widely explored in English, its behavior in morphologically rich languages like Arabic remains underexamined. In this work, we present the first study of Arabic KE. We evaluate four methods (ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact benchmarks, analyzing both multilingual and cross-lingual settings. Our experiments on Llama-2-7B-chat show show that parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods perform more robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show that joint Arabic-English training improves both editability and transfer. We release Arabic KE benchmarks and multilingual training for LTE data to support future research.</li>
</ul>

<h3>Title: Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Shomukh Qari, Maha A. Thafar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09630">https://arxiv.org/abs/2507.09630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09630">https://arxiv.org/pdf/2507.09630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09630]] Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI(https://arxiv.org/abs/2507.09630)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Stroke is one of the leading causes of death globally, making early and accurate diagnosis essential for improving patient outcomes, particularly in emergency settings where timely intervention is critical. CT scans are the key imaging modality because of their speed, accessibility, and cost-effectiveness. This study proposed an artificial intelligence framework for multiclass stroke classification (ischemic, hemorrhagic, and no stroke) using CT scan images from a dataset provided by the Republic of Turkey's Ministry of Health. The proposed method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary deep learning model for image-based stroke classification, with additional transformer variants (vision transformer, transformer-in-transformer, and ConvNext). To enhance model generalization and address class imbalance, we applied data augmentation techniques, including synthetic image generation. The MaxViT model trained with augmentation achieved the best performance, reaching an accuracy and F1-score of 98.00%, outperforming all other evaluated models and the baseline methods. The primary goal of this study was to distinguish between stroke types with high accuracy while addressing crucial issues of transparency and trust in artificial intelligence models. To achieve this, Explainable Artificial Intelligence (XAI) was integrated into the framework, particularly Grad-CAM++. It provides visual explanations of the model's decisions by highlighting relevant stroke regions in the CT scans and establishing an accurate, interpretable, and clinically applicable solution for early stroke detection. This research contributed to the development of a trustworthy AI-assisted diagnostic tool for stroke, facilitating its integration into clinical practice and enhancing access to timely and optimal stroke diagnosis in emergency departments, thereby saving more lives.</li>
</ul>

<h3>Title: Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?</h3>
<ul>
<li><strong>Authors: </strong>Pawitsapak Akarajaradwong, Chompakorn Chaksangchaichot, Pirat Pothavorn, Attapol Thamrongrattanarit-Rutherford, Ekapol Chuangsuwanich, Sarana Nutanong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09638">https://arxiv.org/abs/2507.09638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09638">https://arxiv.org/pdf/2507.09638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09638]] Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?(https://arxiv.org/abs/2507.09638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal question answering is still limited, especially for questions requiring extensive, complex legal reasoning. To address these limitations, we introduce an approach aligning LLMs toward improved law citation accuracy and better response quality using Group-Relative Policy Optimization (GRPO). Our approach leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward, significantly reducing computational expenses up to 2.5x compared to large language model judges. Experiments on the NitiBench benchmark demonstrate substantial improvements: GRPO achieves up to 90% citation-F1 gains from the base model and a 31% increase in joint quality metrics over instruction tuning. Crucially, our method shows enhanced robustness on complex legal reasoning tasks compared to instruction tuning, providing an effective and resource-efficient solution for enhancing Thai legal LLMs.</li>
</ul>

<h3>Title: Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams</h3>
<ul>
<li><strong>Authors: </strong>Leonor Fernandes, Tiago Gon√ßalves, Jo√£o Matos, Luis Filipe Nakayama, Jaime S. Cardoso</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09640">https://arxiv.org/abs/2507.09640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09640">https://arxiv.org/pdf/2507.09640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09640]] Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams(https://arxiv.org/abs/2507.09640)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Diabetic retinopathy (DR) is a leading cause of vision loss in working-age adults. While screening reduces the risk of blindness, traditional imaging is often costly and inaccessible. Artificial intelligence (AI) algorithms present a scalable diagnostic solution, but concerns regarding fairness and generalization persist. This work evaluates the fairness and performance of image-trained models in DR prediction, as well as the impact of disentanglement as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness was assessed between subgroups of SAs, and disentanglement was applied to reduce bias. All models achieved high DR prediction performance in diagnosing (up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77% AUROC, respectively). Fairness assessment suggests disparities, such as a 10% AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction had varying results, depending on the model selected. Disentanglement improved DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2 and Swin V2 (7% and 3%, respectively). These findings highlight the complexity of disentangling fine-grained features in fundus imaging and emphasize the importance of fairness in medical imaging AI to ensure equitable and reliable healthcare solutions.</li>
</ul>

<h3>Title: EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR</h3>
<ul>
<li><strong>Authors: </strong>Zhengyuan Peng, Jianqing Xu, Shen Li, Jiazhen Ji, Yuge Huang, Jingyun Zhang, Jinmin Li, Shouhong Ding, Rizen Guo, Xin Tan, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09649">https://arxiv.org/abs/2507.09649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09649">https://arxiv.org/pdf/2507.09649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09649]] EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR(https://arxiv.org/abs/2507.09649)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Human-machine interaction through augmented reality (AR) and virtual reality (VR) is increasingly prevalent, requiring accurate and efficient gaze estimation which hinges on the accuracy of eye segmentation to enable smooth user experiences. We introduce EyeSeg, a novel eye segmentation framework designed to overcome key challenges that existing approaches struggle with: motion blur, eyelid occlusion, and train-test domain gaps. In these situations, existing models struggle to extract robust features, leading to suboptimal performance. Noting that these challenges can be generally quantified by uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation framework for AR/VR wherein we explicitly model the uncertainties by performing Bayesian uncertainty learning of a posterior under the closed set prior. Theoretically, we prove that a statistic of the learned posterior indicates segmentation uncertainty levels and empirically outperforms existing methods in downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score and the segmentation result, weighting and fusing multiple gaze estimates for robustness, which proves to be effective especially under motion blur, eyelid occlusion and cross-domain challenges. Moreover, empirical results suggest that EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing previous approaches. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset</h3>
<ul>
<li><strong>Authors: </strong>Lily Hong Zhang, Smitha Milli, Karen Jusko, Jonathan Smith, Brandon Amos, Wassim (Wes)Bouaziz, Manon Revel, Jack Kussman, Lisa Titus, Bhaktipriya Radharapu, Jane Yu, Vidya Sarma, Kris Rose, Maximilian Nickel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09650">https://arxiv.org/abs/2507.09650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09650">https://arxiv.org/pdf/2507.09650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09650]] Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset(https://arxiv.org/abs/2507.09650)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How can large language models (LLMs) serve users with varying preferences that may conflict across cultural, political, or other dimensions? To advance this challenge, this paper establishes four key results. First, we demonstrate, through a large-scale multilingual human study with representative samples from five countries (N=15,000), that humans exhibit significantly more variation in preferences than the responses of 21 state-of-the-art LLMs. Second, we show that existing methods for preference dataset collection are insufficient for learning the diversity of human preferences even along two of the most salient dimensions of variability in global values, due to the underlying homogeneity of candidate responses. Third, we argue that this motivates the need for negatively-correlated sampling when generating candidate sets, and we show that simple prompt-based techniques for doing so significantly enhance the performance of alignment methods in learning heterogeneous preferences. Fourth, based on this novel candidate sampling approach, we collect and open-source Community Alignment, the largest and most representative multilingual and multi-turn preference dataset to date, featuring almost 200,000 comparisons from annotators spanning five countries. We hope that the Community Alignment dataset will be a valuable resource for improving the effectiveness of LLMs for a diverse global population.</li>
</ul>

<h3>Title: VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhang, Zhonghao Ye, Jingwei Zhang, Xiang Tian, Zhisheng Liang, Shipeng Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09672">https://arxiv.org/abs/2507.09672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09672">https://arxiv.org/pdf/2507.09672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09672]] VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation(https://arxiv.org/abs/2507.09672)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>WiFi-based human pose estimation has emerged as a promising non-visual alternative approaches due to its pene-trability and privacy advantages. This paper presents VST-Pose, a novel deep learning framework for accurate and continuous pose estimation using WiFi channel state information. The proposed method introduces ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture that adopts a dual-stream architecture to separately capture temporal dependencies and structural relationships among body joints. To enhance sensitivity to subtle human motions, a velocity modeling branch is integrated into the framework, which learns short-term keypoint dis-placement patterns and improves fine-grained motion representation. We construct a 2D pose dataset specifically designed for smart home care scenarios and demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset. Further evaluation on the public MMFi dataset confirms the model's robustness and effectiveness in 3D pose estimation tasks. The proposed system provides a reliable and privacy-aware solution for continuous human motion analysis in indoor environments. Our codes are available in this https URL.</li>
</ul>

<h3>Title: Conformal Prediction for Privacy-Preserving Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Alexander David Balinsky, Dominik Krzeminski, Alexander Balinsky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09678">https://arxiv.org/abs/2507.09678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09678">https://arxiv.org/pdf/2507.09678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09678]] Conformal Prediction for Privacy-Preserving Machine Learning(https://arxiv.org/abs/2507.09678)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>We investigate the integration of Conformal Prediction (CP) with supervised learning on deterministically encrypted data, aiming to bridge the gap between rigorous uncertainty quantification and privacy-preserving machine learning. Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP methods remain effective even when applied directly in the encrypted domain, owing to the preservation of data exchangeability under fixed-key encryption. We test traditional $p$-value-based against $e$-value-based conformal predictors. Our empirical evaluation reveals that models trained on deterministically encrypted data retain the ability to extract meaningful structure, achieving 36.88\% test accuracy -- significantly above random guessing (9.56\%) observed with per-instance encryption. Moreover, $e$-value-based CP achieves predictive set coverage of over 60\% with 4.3 loss-threshold calibration, correctly capturing the true label in 4888 out of 5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive sets but with reduced coverage accuracy. These findings highlight both the promise and limitations of CP in encrypted data settings and underscore critical trade-offs between prediction set compactness and reliability. %Our work sets a foundation for principled uncertainty quantification in secure, privacy-aware learning systems.</li>
</ul>

<h3>Title: Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Osher Rafaeli, Tal Svoray, Ariel Nahlieli</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09681">https://arxiv.org/abs/2507.09681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09681">https://arxiv.org/pdf/2507.09681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09681]] Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model(https://arxiv.org/abs/2507.09681)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>High-resolution elevation estimations are essential to understand catchment and hillslope hydrology, study urban morphology and dynamics, and monitor the growth, decline, and mortality of terrestrial ecosystems. Various deep learning approaches (e.g., super-resolution techniques, monocular depth estimation) have been developed to create high-resolution Digital Elevation Models (DEMs). However, super-resolution techniques are limited by the upscaling factor, and monocular depth estimation lacks global elevation context, making its conversion to a seamless DEM restricted. The recently introduced technique of prompt-based monocular depth estimation has opened new opportunities to extract estimates of absolute elevation in a global context. We present here a framework for the estimation of high-resolution DEMs as a new paradigm for absolute global elevation mapping. It is exemplified using low-resolution Shuttle Radar Topography Mission (SRTM) elevation data as prompts and high-resolution RGB imagery from the National Agriculture Imagery Program (NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived DEMs and employs a versatile prompting strategy, enabling tasks such as DEM estimation, void filling, and updating. Our framework achieves a 100x resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of magnitude. Evaluations across three diverse U.S. landscapes show robust generalization, capturing urban structures and fine-scale terrain features with < 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological analysis confirms suitability for hazard and environmental studies. We demonstrate scalability by applying the framework to large regions in the U.S. and Israel. All code and pretrained models are publicly available at: this https URL.</li>
</ul>

<h3>Title: Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness</h3>
<ul>
<li><strong>Authors: </strong>Md Mushfiqur Rahaman, Elliot Chang, Tasmiah Haque, Srinjoy Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09687">https://arxiv.org/abs/2507.09687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09687">https://arxiv.org/pdf/2507.09687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09687]] Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness(https://arxiv.org/abs/2507.09687)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Text classification plays a pivotal role in edge computing applications like industrial monitoring, health diagnostics, and smart assistants, where low latency and high accuracy are both key requirements. Generative classifiers, in particular, have been shown to exhibit robustness to out-of-distribution and noisy data, which is an extremely critical consideration for deployment in such real-time edge environments. However, deploying such models on edge devices faces computational and memory constraints. Post Training Quantization (PTQ) reduces model size and compute costs without retraining, making it ideal for edge deployment. In this work, we present a comprehensive comparative study of generative and discriminative Long Short Term Memory (LSTM)-based text classification models with PTQ using the Brevitas quantization library. We evaluate both types of classifier models across multiple bitwidths and assess their robustness under regular and noisy input conditions. We find that while discriminative classifiers remain robust, generative ones are more sensitive to bitwidth, calibration data used during PTQ, and input noise during quantized inference. We study the influence of class imbalance in calibration data for both types of classifiers, comparing scenarios with evenly and unevenly distributed class samples including their effect on weight adjustments and activation profiles during PTQ. Using test statistics derived from nonparametric hypothesis testing, we identify that using class imbalanced data during calibration introduces insufficient weight adaptation at lower bitwidths for generative LSTM classifiers, thereby leading to degraded performance. This study underscores the role of calibration data in PTQ and when generative classifiers succeed or fail under noise, aiding deployment in edge environments.</li>
</ul>

<h3>Title: Interpreting Differential Privacy in Terms of Disclosure Risk</h3>
<ul>
<li><strong>Authors: </strong>Zeki Kazan, Sagar Sharma, Wanrong Zhang, Bo Jiang, Qiang Yan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09699">https://arxiv.org/abs/2507.09699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09699">https://arxiv.org/pdf/2507.09699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09699]] Interpreting Differential Privacy in Terms of Disclosure Risk(https://arxiv.org/abs/2507.09699)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>As the use of differential privacy (DP) becomes widespread, the development of effective tools for reasoning about the privacy guarantee becomes increasingly critical. In pursuit of this goal, we demonstrate novel relationships between DP and measures of statistical disclosure risk. We suggest how experts and non-experts can use these results to explain the DP guarantee, interpret DP composition theorems, select and justify privacy parameters, and identify worst-case adversary prior probabilities.</li>
</ul>

<h3>Title: MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shulin Huang, Linyi Yang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09701">https://arxiv.org/abs/2507.09701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09701">https://arxiv.org/pdf/2507.09701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09701]] MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs(https://arxiv.org/abs/2507.09701)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit cultural biases and limited cross-cultural understanding capabilities, particularly when serving diverse global user populations. We propose MCEval, a novel multilingual evaluation framework that employs dynamic cultural question construction and enables causal analysis through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive evaluation spans 13 cultures and 13 languages, systematically assessing both cultural awareness and cultural bias across different linguistic scenarios. The framework provides 39,897 cultural awareness instances and 17,940 cultural bias instances. Experimental results reveal performance disparities across different linguistic scenarios, demonstrating that optimal cultural performance is not only linked to training data distribution, but also is related to language-culture alignment. The evaluation results also expose the fairness issue, where approaches appearing successful in the English scenario create substantial disadvantages. MCEval represents the first comprehensive multilingual cultural evaluation framework that provides deeper insights into LLMs' cultural understanding.</li>
</ul>

<h3>Title: Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI</h3>
<ul>
<li><strong>Authors: </strong>Phat Nguyen, Ngai-Man Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09702">https://arxiv.org/abs/2507.09702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09702">https://arxiv.org/pdf/2507.09702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09702]] Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI(https://arxiv.org/abs/2507.09702)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Token compression techniques have recently emerged as powerful tools for accelerating Vision Transformer (ViT) inference in computer vision. Due to the quadratic computational complexity with respect to the token sequence length, these methods aim to remove less informative tokens before the attention layers to improve inference throughput. While numerous studies have explored various accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain. First, there is a lack of unified survey that systematically categorizes and compares token compression approaches based on their core strategies (e.g., pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs. plug-in). Second, most benchmarks are limited to standard ViT models (e.g., ViT-B, ViT-L), leaving open the question of whether such methods remain effective when applied to structurally compressed transformers, which are increasingly deployed on resource-constrained edge devices. To address these gaps, we present the first systematic taxonomy and comparative study of token compression methods, and we evaluate representative techniques on both standard and compact ViT architectures. Our experiments reveal that while token compression methods are effective for general-purpose ViTs, they often underperform when directly applied to compact designs. These findings not only provide practical insights but also pave the way for future research on adapting token optimization techniques to compact transformer-based networks for edge AI and AI agent applications.</li>
</ul>

<h3>Title: EPT-2 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Roberto Molinaro, Niall Siegenheim, Niels Poulsen, Jordan Dane Daubinet, Henry Martin, Mark Frey, Kevin Thiart, Alexander Jakob Dautel, Andreas Schlueter, Alex Grigoryev, Bogdan Danciu, Nikoo Ekhtiari, Bas Steunebrink, Leonie Wagner, Marvin Vincent Gabler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09703">https://arxiv.org/abs/2507.09703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09703">https://arxiv.org/pdf/2507.09703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09703]] EPT-2 Technical Report(https://arxiv.org/abs/2507.09703)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT) family of foundation AI models for Earth system forecasting. EPT-2 delivers substantial improvements over its predecessor, EPT-1.5, and sets a new state of the art in predicting energy-relevant variables-including 10m and 100m wind speed, 2m temperature, and surface solar radiation-across the full 0-240h forecast horizon. It consistently outperforms leading AI weather models such as Microsoft Aurora, as well as the operational numerical forecast system IFS HRES from the European Centre for Medium-Range Weather Forecasts (ECMWF). In parallel, we introduce a perturbation-based ensemble model of EPT-2 for probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly surpasses the ECMWF ENS mean-long considered the gold standard for medium- to longrange forecasting-while operating at a fraction of the computational cost. EPT models, as well as third-party forecasts, are accessible via the this http URL platform.</li>
</ul>

<h3>Title: Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces</h3>
<ul>
<li><strong>Authors: </strong>Baturay Saglam, Paul Kassianik, Blaine Nelson, Sajana Weerawardhena, Yaron Singer, Amin Karbasi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09709">https://arxiv.org/abs/2507.09709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09709">https://arxiv.org/pdf/2507.09709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09709]] Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces(https://arxiv.org/abs/2507.09709)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. \baturay{However, it remains unclear to what extent LLMs internally organize representations related to semantic understanding. To investigate this, we conduct a large-scale empirical study of hidden states in transformer-based LLMs, analyzing 11 decoder-only models across 6 scientific topics and 12 layers each. We find that high-level semantic information consistently lies in low-dimensional subspaces that form linearly separable representations across distinct domains. This separability becomes more pronounced in deeper layers and under prompts that trigger structured reasoning or alignment behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry enables simple yet effective causal interventions in hidden space; for example, reasoning patterns like chain-of-thought can be captured by a single vector direction. Together, these findings support the development of geometry-aware tools that operate directly on latent representations to detect and mitigate harmful or adversarial content, using methods such as transport-based defenses that leverage this separability. As a proof of concept, we demonstrate this potential by training a simple MLP classifier as a lightweight latent-space guardrail, which detects adversarial and malicious prompts with high precision.</li>
</ul>

<h3>Title: Continental scale habitat modelling with artificial intelligence and multimodal earth observation</h3>
<ul>
<li><strong>Authors: </strong>Sara Si-Moussi, Stephan Hennekens, Sander Mucher, Stan Los, Wilfried Thuiller</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.PE, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09732">https://arxiv.org/abs/2507.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09732">https://arxiv.org/pdf/2507.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09732]] Continental scale habitat modelling with artificial intelligence and multimodal earth observation(https://arxiv.org/abs/2507.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Habitats integrate the abiotic conditions and biophysical structures that support biodiversity and sustain nature's contributions to people. As these ecosystems face mounting pressure from human activities, accurate, high-resolution habitat maps are essential for effective conservation and restoration. Yet current maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicate multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat classification over large geographic extents at fine thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled Level 3 EUNIS habitats across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat nomenclatures resolved classification ambiguities, especially in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic aperture radar (SAR) imagery, particularly through Earth Observation Foundation models, enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted accuracy further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of dynamic habitats, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in-situ observations.</li>
</ul>

<h3>Title: Universal Physics Simulation: A Foundational Diffusion Approach</h3>
<ul>
<li><strong>Authors: </strong>Bradley Camburn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09733">https://arxiv.org/abs/2507.09733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09733">https://arxiv.org/pdf/2507.09733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09733]] Universal Physics Simulation: A Foundational Diffusion Approach(https://arxiv.org/abs/2507.09733)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present the first foundational AI model for universal physics simulation that learns physical laws directly from boundary-condition data without requiring a priori equation encoding. Traditional physics-informed neural networks (PINNs) and finite-difference methods necessitate explicit mathematical formulation of governing equations, fundamentally limiting their generalizability and discovery potential. Our sketch-guided diffusion transformer approach reimagines computational physics by treating simulation as a conditional generation problem, where spatial boundary conditions guide the synthesis of physically accurate steady-state solutions. By leveraging enhanced diffusion transformer architectures with novel spatial relationship encoding, our model achieves direct boundary-to-equilibrium mapping and is generalizable to diverse physics domains. Unlike sequential time-stepping methods that accumulate errors over iterations, our approach bypasses temporal integration entirely, directly generating steady-state solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our data-informed approach enables physics discovery through learned representations analyzable via Layer-wise Relevance Propagation (LRP), revealing emergent physical relationships without predetermined mathematical constraints. This work represents a paradigm shift from AI-accelerated physics to AI-discovered physics, establishing the first truly universal physics simulation framework.</li>
</ul>

<h3>Title: Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yu Lei, Bingde Liu, Qingsong Xie, Haonan Lu, Zhijie Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09748">https://arxiv.org/abs/2507.09748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09748">https://arxiv.org/pdf/2507.09748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09748]] Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation(https://arxiv.org/abs/2507.09748)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence. In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). $L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework.</li>
</ul>

<h3>Title: Do we need equivariant models for molecule generation?</h3>
<ul>
<li><strong>Authors: </strong>Ewa M. Nowara, Joshua Rackers, Patricia Suriana, Pan Kessel, Max Shen, Andrew Martin Watkins, Michael Maser</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09753">https://arxiv.org/abs/2507.09753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09753">https://arxiv.org/pdf/2507.09753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09753]] Do we need equivariant models for molecule generation?(https://arxiv.org/abs/2507.09753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models are increasingly used for molecular discovery, with most recent approaches relying on equivariant graph neural networks (GNNs) under the assumption that explicit equivariance is essential for generating high-quality 3D molecules. However, these models are complex, difficult to train, and scale poorly. We investigate whether non-equivariant convolutional neural networks (CNNs) trained with rotation augmentations can learn equivariance and match the performance of equivariant models. We derive a loss decomposition that separates prediction error from equivariance error, and evaluate how model size, dataset size, and training duration affect performance across denoising, molecule generation, and property prediction. To our knowledge, this is the first study to analyze learned equivariance in generative tasks.</li>
</ul>

<h3>Title: Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Aakash Tripathi, Ian E. Nielsen, Muhammad Umer, Ravi P. Ramachandran, Ghulam Rasool</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09754">https://arxiv.org/abs/2507.09754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09754">https://arxiv.org/pdf/2507.09754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09754]] Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts(https://arxiv.org/abs/2507.09754)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Transcription Factor Binding Site (TFBS) prediction is crucial for understanding gene regulation and various biological processes. This study introduces a novel Mixture of Experts (MoE) approach for TFBS prediction, integrating multiple pre-trained Convolutional Neural Network (CNN) models, each specializing in different TFBS patterns. We evaluate the performance of our MoE model against individual expert models on both in-distribution and out-of-distribution (OOD) datasets, using six randomly selected transcription factors (TFs) for OOD testing. Our results demonstrate that the MoE model achieves competitive or superior performance across diverse TF binding sites, particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA) statistical test confirms the significance of these performance differences. Additionally, we introduce ShiftSmooth, a novel attribution mapping technique that provides more robust model interpretability by considering small shifts in input sequences. Through comprehensive explainability analysis, we show that ShiftSmooth offers superior attribution for motif discovery and localization compared to traditional Vanilla Gradient methods. Our work presents an efficient, generalizable, and interpretable solution for TFBS prediction, potentially enabling new discoveries in genome biology and advancing our understanding of transcriptional regulation.</li>
</ul>

<h3>Title: EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions</h3>
<ul>
<li><strong>Authors: </strong>Yasir Ech-Chammakhy, Anas Motii, Anass Rabii, Jaafar Chbili</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09762">https://arxiv.org/abs/2507.09762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09762">https://arxiv.org/pdf/2507.09762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09762]] EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions(https://arxiv.org/abs/2507.09762)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>Hacker forums provide critical early warning signals for emerging cybersecurity threats, but extracting actionable intelligence from their unstructured and noisy content remains a significant challenge. This paper presents an unsupervised framework that automatically detects, clusters, and prioritizes security events discussed across hacker forum posts. Our approach leverages Transformer-based embeddings fine-tuned with contrastive learning to group related discussions into distinct security event clusters, identifying incidents like zero-day disclosures or malware releases without relying on predefined keywords. The framework incorporates a daily ranking mechanism that prioritizes identified events using quantifiable metrics reflecting timeliness, source credibility, information completeness, and relevance. Experimental evaluation on real-world hacker forum data demonstrates that our method effectively reduces noise and surfaces high-priority threats, enabling security analysts to mount proactive responses. By transforming disparate hacker forum discussions into structured, actionable intelligence, our work addresses fundamental challenges in automated threat detection and analysis.</li>
</ul>

<h3>Title: Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights</h3>
<ul>
<li><strong>Authors: </strong>Mohamadreza Akbari Pour, Ali Ghasemzadeh, MohamadAli Bijarchi, Mohammad Behshad Shafii</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09766">https://arxiv.org/abs/2507.09766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09766">https://arxiv.org/pdf/2507.09766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09766]] Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights(https://arxiv.org/abs/2507.09766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH) is essential for Prognostics and Health Management (PHM) across a wide range of industrial applications. We propose a novel framework -- Reinforced Graph-Based Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that combines physics-based supervision with advanced spatio-temporal learning. Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional filters within recurrent units to capture how node representations evolve over time. Graph Attention Convolution (GATConv) leverages a self-attention mechanism to compute learnable, edge-wise attention coefficients, dynamically weighting neighbor contributions for adaptive spatial aggregation. A Soft Actor-Critic (SAC) module is positioned between the Temporal Attention Unit (TAU) and GCRN to further improve the spatio-temporal learning. This module improves attention and prediction accuracy by dynamically scaling hidden representations to minimize noise and highlight informative features. To identify the most relevant physical constraints in each area, Q-learning agents dynamically assign weights to physics-informed loss terms, improving generalization across real-time industrial systems and reducing the need for manual tuning. In both RUL and SOH estimation tasks, the proposed method consistently outperforms state-of-the-art models, demonstrating strong robustness and predictive accuracy across varied degradation patterns across three diverse industrial benchmark datasets.</li>
</ul>

<h3>Title: Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow</h3>
<ul>
<li><strong>Authors: </strong>Zhonglin Cao, Mario Geiger, Allan dos Santos Costa, Danny Reidenbach, Karsten Kreis, Tomas Geffner, Franco Pellegrini, Guoqing Zhou, Emine Kucukbenli</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09785">https://arxiv.org/abs/2507.09785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09785">https://arxiv.org/pdf/2507.09785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09785]] Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow(https://arxiv.org/abs/2507.09785)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Fast and accurate generation of molecular conformers is desired for downstream computational chemistry and drug discovery tasks. Currently, training and sampling state-of-the-art diffusion or flow-based models for conformer generation require significant computational resources. In this work, we build upon flow-matching and propose two mechanisms for accelerating training and inference of generative models for 3D molecular conformer generation. For fast training, we introduce the SO(3)-Averaged Flow training objective, which leads to faster convergence to better generation quality compared to conditional optimal transport flow or Kabsch-aligned flow. We demonstrate that models trained using SO(3)-Averaged Flow can reach state-of-the-art conformer generation quality. For fast inference, we show that the reflow and distillation methods of flow-based models enable few-steps or even one-step molecular conformer generation with high quality. The training techniques proposed in this work show a path towards highly efficient molecular conformer generation with flow-based models.</li>
</ul>

<h3>Title: Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster</h3>
<ul>
<li><strong>Authors: </strong>Junaid Iqbal Khan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09786">https://arxiv.org/abs/2507.09786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09786">https://arxiv.org/pdf/2507.09786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09786]] Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster(https://arxiv.org/abs/2507.09786)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Approximate machine unlearning (AMU) enables models to `forget' specific training data through specialized fine-tuning on a retained dataset subset. However, processing this retained subset still dominates computational runtime, while reductions of epochs also remain a challenge. We propose two complementary methods to accelerate classification-oriented AMU. First, \textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges visually similar images with shared blend-weights to significantly reduce the retained set size. It operates with minimal pre-processing overhead and is orders of magnitude faster than state-of-the-art DC methods. Second, our loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning objective to quicken convergence. A-AMU achieves this by combining a steepened primary loss to expedite forgetting with a novel, differentiable regularizer that matches the loss distributions of forgotten and in-distribution unseen data. Our extensive experiments demonstrate that this dual approach of data and loss-centric optimization dramatically reduces end-to-end unlearning latency across both single and multi-round scenarios, all while preserving model utility and privacy. To our knowledge, this is the first work to systematically tackle unlearning efficiency by jointly designing a specialized dataset condensation technique with a dedicated accelerated loss function. Code is available at this https URL.</li>
</ul>

<h3>Title: NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Ansari, Ke Wang, Pulei Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09795">https://arxiv.org/abs/2507.09795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09795">https://arxiv.org/pdf/2507.09795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09795]] NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection(https://arxiv.org/abs/2507.09795)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in Vision-Language Models like CLIP have enabled zero-shot OOD detection by leveraging both image and textual label information. Among these, negative label-based methods such as NegLabel and CSP have shown promising results by utilizing a lexicon of words to define negative labels for distinguishing OOD samples. However, these methods suffer from detecting in-distribution samples as OOD due to negative labels that are subcategories of in-distribution labels or proper nouns. They also face limitations in handling images that match multiple in-distribution and negative labels. We propose NegRefine, a novel negative label refinement framework for zero-shot OOD detection. By introducing a filtering mechanism to exclude subcategory labels and proper nouns from the negative label set and incorporating a multi-matching-aware scoring function that dynamically adjusts the contributions of multiple labels matching an image, NegRefine ensures a more robust separation between in-distribution and OOD samples. We evaluate NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is available at this https URL.</li>
</ul>

<h3>Title: A Scalable and Efficient Signal Integration System for Job Matching</h3>
<ul>
<li><strong>Authors: </strong>Ping Liu, Rajat Arora, Xiao Shi, Benjamin Le, Qianqi Shen, Jianqiang Shen, Chengming Jiang, Nikita Zhiltsov, Priya Bannur, Yidan Zhu, Liming Dong, Haichao Wei, Qi Guo, Luke Simon, Liangjie Hong, Wenjing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09797">https://arxiv.org/abs/2507.09797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09797">https://arxiv.org/pdf/2507.09797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09797]] A Scalable and Efficient Signal Integration System for Job Matching(https://arxiv.org/abs/2507.09797)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>LinkedIn, one of the world's largest platforms for professional networking and job seeking, encounters various modeling challenges in building recommendation systems for its job matching product, including cold-start, filter bubbles, and biases affecting candidate-job matching. To address these, we developed the STAR (Signal Integration for Talent And Recruiters) system, leveraging the combined strengths of Large Language Models (LLMs) and Graph Neural Networks (GNNs). LLMs excel at understanding textual data, such as member profiles and job postings, while GNNs capture intricate relationships and mitigate cold-start issues through network effects. STAR integrates diverse signals by uniting LLM and GNN capabilities with industrial-scale paradigms including adaptive sampling and version management. It provides an end-to-end solution for developing and deploying embeddings in large-scale recommender systems. Our key contributions include a robust methodology for building embeddings in industrial applications, a scalable GNN-LLM integration for high-performing recommendations, and practical insights for real-world model deployment.</li>
</ul>

<h3>Title: Federated Learning with Graph-Based Aggregation for Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Audri Banik, Glaucio Haroldo Silva de Carvalho, Renata Dividino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09805">https://arxiv.org/abs/2507.09805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09805">https://arxiv.org/pdf/2507.09805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09805]] Federated Learning with Graph-Based Aggregation for Traffic Forecasting(https://arxiv.org/abs/2507.09805)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>In traffic prediction, the goal is to estimate traffic speed or flow in specific regions or road segments using historical data collected by devices deployed in each area. Each region or road segment can be viewed as an individual client that measures local traffic flow, making Federated Learning (FL) a suitable approach for collaboratively training models without sharing raw data. In centralized FL, a central server collects and aggregates model updates from multiple clients to build a shared model while preserving each client's data privacy. Standard FL methods, such as Federated Averaging (FedAvg), assume that clients are independent, which can limit performance in traffic prediction tasks where spatial relationships between clients are important. Federated Graph Learning methods can capture these dependencies during server-side aggregation, but they often introduce significant computational overhead. In this paper, we propose a lightweight graph-aware FL approach that blends the simplicity of FedAvg with key ideas from graph learning. Rather than training full models, our method applies basic neighbourhood aggregation principles to guide parameter updates, weighting client models based on graph connectivity. This approach captures spatial relationships effectively while remaining computationally efficient. We evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY, and show that it achieves competitive performance compared to standard baselines and recent graph-based federated learning techniques.</li>
</ul>

<h3>Title: VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Younggun Kim, Ahmed S. Abdelrahman, Mohamed Abdel-Aty</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09815">https://arxiv.org/abs/2507.09815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09815">https://arxiv.org/pdf/2507.09815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09815]] VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding(https://arxiv.org/abs/2507.09815)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, is a critical challenge for autonomous driving systems, as crashes involving VRUs often result in severe or fatal consequences. While multimodal large language models (MLLMs) have shown promise in enhancing scene understanding and decision making in autonomous vehicles, there is currently no standardized benchmark to quantitatively evaluate their reasoning abilities in complex, safety-critical scenarios involving VRUs. To address this gap, we present VRU-Accident, a large-scale vision-language benchmark designed to evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident comprises 1K real-world dashcam accident videos, annotated with 6K multiple-choice question-answer pairs across six safety-critical categories (with 24K candidate options and 3.4K unique answer choices), as well as 1K dense scene descriptions. Unlike prior works, our benchmark focuses explicitly on VRU-vehicle accidents, providing rich, fine-grained annotations that capture both spatial-temporal dynamics and causal semantics of accidents. To assess the current landscape of MLLMs, we conduct a comprehensive evaluation of 17 state-of-the-art models on the multiple-choice VQA task and on the dense captioning task. Our findings reveal that while MLLMs perform reasonably well on visually grounded attributes, they face significant challenges in reasoning and describing accident causes, types, and preventability.</li>
</ul>

<h3>Title: Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem</h3>
<ul>
<li><strong>Authors: </strong>Adam Newgas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09816">https://arxiv.org/abs/2507.09816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09816">https://arxiv.org/pdf/2507.09816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09816]] Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem(https://arxiv.org/abs/2507.09816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Neural networks are capable of superposition -- representing more features than there are dimensions. Recent work considers the analogous concept for computation instead of storage, proposing theoretical constructions. But there has been little investigation into whether these circuits can be learned in practice. In this work, we investigate a toy model for the Universal-AND problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs. The hidden dimension that determines the number of non-linear activations is restricted to pressure the model to find a compute-efficient circuit, called compressed computation. We find that the training process finds a simple solution that does not correspond to theoretical constructions. It is fully dense -- every neuron contributes to every output. The solution circuit naturally scales with dimension, trading off error rates for neuron efficiency. It is similarly robust to changes in sparsity and other key parameters, and extends naturally to other boolean operations and boolean circuits. We explain the found solution in detail and compute why it is more efficient than the theoretical constructions at low sparsity. Our findings shed light on the types of circuits that models like to form and the flexibility of the superposition representation. This contributes to a broader understanding of network circuitry and interpretability.</li>
</ul>

<h3>Title: Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Jintao Qu, Zichong Wang, Chenhao Wu, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09826">https://arxiv.org/abs/2507.09826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09826">https://arxiv.org/pdf/2507.09826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09826]] Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification(https://arxiv.org/abs/2507.09826)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Neural networks have achieved remarkable success in time series classification, but their reliance on large amounts of labeled data for training limits their applicability in cold-start scenarios. Moreover, they lack interpretability, reducing transparency in decision-making. In contrast, dynamic time warping (DTW) combined with a nearest neighbor classifier is widely used for its effectiveness in limited-data settings and its inherent interpretability. However, as a non-parametric method, it is not trainable and cannot leverage large amounts of labeled data, making it less effective than neural networks in rich-resource scenarios. In this work, we aim to develop a versatile model that adapts to cold-start conditions and becomes trainable with labeled data, while maintaining interpretability. We propose a dynamic length-shortening algorithm that transforms time series into prototypes while preserving key structural patterns, thereby enabling the reformulation of the DTW recurrence relation into an equivalent recurrent neural network. Based on this, we construct a trainable model that mimics DTW's alignment behavior. As a neural network, it becomes trainable when sufficient labeled data is available, while still retaining DTW's inherent interpretability. We apply the model to several benchmark time series classification tasks and observe that it significantly outperforms previous approaches in low-resource settings and remains competitive in rich-resource settings.</li>
</ul>

<h3>Title: Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Shuhao Fu, Philip J. Kellman, Hongjing Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09830">https://arxiv.org/abs/2507.09830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09830">https://arxiv.org/pdf/2507.09830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09830]] Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models(https://arxiv.org/abs/2507.09830)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Both humans and deep learning models can recognize objects from 3D shapes depicted with sparse visual information, such as a set of points randomly sampled from the surfaces of 3D objects (termed a point cloud). Although deep learning models achieve human-like performance in recognizing objects from 3D shapes, it remains unclear whether these models develop 3D shape representations similar to those used by human vision for object recognition. We hypothesize that training with 3D shapes enables models to form representations of local geometric structures in 3D shapes. However, their representations of global 3D object shapes may be limited. We conducted two human experiments systematically manipulating point density and object orientation (Experiment 1), and local geometric structure (Experiment 2). Humans consistently performed well across all experimental conditions. We compared two types of deep learning models, one based on a convolutional neural network (DGCNN) and the other on visual transformers (point transformer), with human performance. We found that the point transformer model provided a better account of human performance than the convolution-based model. The advantage mainly results from the mechanism in the point transformer model that supports hierarchical abstraction of 3D shapes.</li>
</ul>

<h3>Title: Generative Cognitive Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jiatong Li, Qi Liu, Mengxiao Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09831">https://arxiv.org/abs/2507.09831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09831">https://arxiv.org/pdf/2507.09831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09831]] Generative Cognitive Diagnosis(https://arxiv.org/abs/2507.09831)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cognitive diagnosis (CD) models latent cognitive states of human learners by analyzing their response patterns on diagnostic tests, serving as a crucial machine learning technique for educational assessment and evaluation. Traditional cognitive diagnosis models typically follow a transductive prediction paradigm that optimizes parameters to fit response scores and extract learner abilities. These approaches face significant limitations as they cannot perform instant diagnosis for new learners without computationally expensive retraining and produce diagnostic outputs with limited reliability. In this study, we introduces a novel generative diagnosis paradigm that fundamentally shifts CD from predictive to generative modeling, enabling inductive inference of cognitive states without parameter re-optimization. We propose two simple yet effective instantiations of this paradigm: Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM), which achieve excellent performance improvements over traditional methods. The generative approach disentangles cognitive state inference from response prediction through a well-designed generation process that incorporates identifiability and monotonicity conditions. Extensive experiments on real-world datasets demonstrate the effectiveness of our methodology in addressing scalability and reliability challenges, especially $\times 100$ speedup for the diagnosis of new learners. Our framework opens new avenues for cognitive diagnosis applications in artificial intelligence, particularly for intelligent model evaluation and intelligent education systems. The code is available at this https URL.</li>
</ul>

<h3>Title: Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs</h3>
<ul>
<li><strong>Authors: </strong>MohammadReza Davari, Utkarsh Garg, Weixin Cai, Eugene Belilovsky</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09839">https://arxiv.org/abs/2507.09839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09839">https://arxiv.org/pdf/2507.09839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09839]] Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs(https://arxiv.org/abs/2507.09839)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>An increasing number of NLP applications interact with large language models (LLMs) through black-box APIs, making prompt engineering critical for controlling model outputs. While recent Automatic Prompt Optimization (APO) methods iteratively refine prompts using model-generated feedback, textual gradients, they primarily focus on error correction and neglect valuable insights from correct predictions. This limits both their effectiveness and efficiency. In this paper, we propose a novel APO framework centered on enhancing the feedback mechanism. We reinterpret the textual gradient as a form of negative reinforcement and introduce the complementary positive reinforcement to explicitly preserve beneficial prompt components identified through successful predictions. To mitigate the noise inherent in LLM-generated feedback, we introduce a technique called feedback diversification, which aggregates multiple feedback signals, emphasizing consistent, actionable advice while filtering out outliers. Motivated by the rapid evolution and diversity of available LLMs, we also formalize Continual Prompt Optimization (CPO), addressing the practical challenge of efficiently migrating optimized prompts between different model versions or API providers. Our experiments reveal that naive prompt migration often degrades performance due to loss of critical instructions. In contrast, our approach consistently outperforms strong baselines, achieving significant accuracy improvements, faster convergence, and lower computational costs in both standard and migration scenarios.</li>
</ul>

<h3>Title: Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training</h3>
<ul>
<li><strong>Authors: </strong>Minhak Song, Beomhan Baek, Kwangjun Ahn, Chulhee Yun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09846">https://arxiv.org/abs/2507.09846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09846">https://arxiv.org/pdf/2507.09846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09846]] Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training(https://arxiv.org/abs/2507.09846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the "river" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.</li>
</ul>

<h3>Title: Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Van Duc, Bui Duc Manh, Quang-Trung Luu, Dinh Thai Hoang, Van-Linh Nguyen, Diep N. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09860">https://arxiv.org/abs/2507.09860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09860">https://arxiv.org/pdf/2507.09860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09860]] Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing(https://arxiv.org/abs/2507.09860)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>This paper aims to propose a novel machine learning (ML) approach incorporating Homomorphic Encryption (HE) to address privacy limitations in Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related to distance, altitude, and face orientation, high-resolution imagery and sophisticated neural networks enable accurate face recognition in dynamic environments. However, privacy concerns arise from the extensive surveillance capabilities of UAVs. To resolve this issue, we propose a novel framework that integrates HE with advanced neural networks to secure facial data throughout the inference phase. This method ensures that facial data remains secure with minimal impact on detection accuracy. Specifically, the proposed system leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly on encrypted data, optimizing computational efficiency and security. Furthermore, we develop an effective data encoding method specifically designed to preprocess the raw facial data into CKKS form in a Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a secure inference algorithm to compute on ciphertext without needing decryption. This approach not only protects data privacy during the processing of facial data but also enhances the efficiency of UAV-based face detection systems. Experimental results demonstrate that our method effectively balances privacy protection and detection performance, making it a viable solution for UAV-based secure face detection. Significantly, our approach (while maintaining data confidentially with HE encryption) can still achieve an accuracy of less than 1% compared to the benchmark without using encryption.</li>
</ul>

<h3>Title: A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends</h3>
<ul>
<li><strong>Authors: </strong>Yihao Ding, Siwen Luo, Yue Dai, Yanbei Jiang, Zechuan Li, Geoffrey Martin, Yifan Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09861">https://arxiv.org/abs/2507.09861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09861">https://arxiv.org/pdf/2507.09861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09861]] A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends(https://arxiv.org/abs/2507.09861)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.</li>
</ul>

<h3>Title: Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition</h3>
<ul>
<li><strong>Authors: </strong>Qinyuan Ye, Robin Jia, Xiang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09875">https://arxiv.org/abs/2507.09875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09875">https://arxiv.org/pdf/2507.09875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09875]] Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition(https://arxiv.org/abs/2507.09875)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their notable performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.</li>
</ul>

<h3>Title: ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yongheng Zhang, Xu Liu, Ruihan Tao, Qiguang Chen, Hao Fei, Wanxiang Che, Libo Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09876">https://arxiv.org/abs/2507.09876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09876">https://arxiv.org/pdf/2507.09876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09876]] ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models(https://arxiv.org/abs/2507.09876)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.</li>
</ul>

<h3>Title: OpenHuman4D: Open-Vocabulary 4D Human Parsing</h3>
<ul>
<li><strong>Authors: </strong>Keito Suzuki, Bang Du, Runfa Blark Li, Kunyao Chen, Lei Wang, Peng Liu, Ning Bi, Truong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09880">https://arxiv.org/abs/2507.09880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09880">https://arxiv.org/pdf/2507.09880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09880]] OpenHuman4D: Open-Vocabulary 4D Human Parsing(https://arxiv.org/abs/2507.09880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.</li>
</ul>

<h3>Title: AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Wu, Zichen Ren, Junyu Wang, Pengyu Zhu, Yonghao Song, Mianxin Liu, Qihao Zheng, Lei Bai, Wanli Ouyang, Chunfeng Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09882">https://arxiv.org/abs/2507.09882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09882">https://arxiv.org/pdf/2507.09882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09882]] AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications(https://arxiv.org/abs/2507.09882)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible means of connecting the human brain to external devices, with broad applications in home and clinical settings to enhance human capabilities. However, the high noise level and limited task-specific data in non-invasive signals constrain decoding capabilities. Recently, the adoption of self-supervised pre-training is transforming the landscape of non-invasive BCI research, enabling the development of brain foundation models to capture generic neural representations from large-scale unlabeled electroencephalography (EEG) signals with substantial noises. However, despite these advances, the field currently lacks comprehensive, practical and extensible benchmarks to assess the utility of the public foundation models across diverse BCI tasks, hindering their widespread adoption. To address this challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to systematically evaluate brain foundation models in widespread non-invasive BCI tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI decoding datasets spanning 7 key applications. It introduces a streamlined task adaptation pipeline integrated with multi-dimensional evaluation metrics and a set of adaptation tools. The benchmark delivers an inclusive framework for assessing generalizability of brain foundation models across key transfer settings, including cross-subject, multi-subject, and few-shot scenarios. We leverage AdaBrain-Bench to evaluate a suite of publicly available brain foundation models and offer insights into practices for selecting appropriate models in various scenarios. We make our benchmark pipeline available to enable reproducible research and external use, offering a continuously evolving platform to foster progress toward robust and generalized neural decoding solutions.</li>
</ul>

<h3>Title: MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention</h3>
<ul>
<li><strong>Authors: </strong>Zhanjiang Yang, Lijun Sun, Jiawei Dong, Xiaoxin An, Yang Liu, Meng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09885">https://arxiv.org/abs/2507.09885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09885">https://arxiv.org/pdf/2507.09885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09885]] MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention(https://arxiv.org/abs/2507.09885)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective solution for various vision-based applications. However, most existing learning-based hyperspectral reconstruction methods directly learn the RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent challenge of transitioning from low-dimensional to high-dimensional information. To address this limitation, we propose a two-stage approach, MCGA, which first learns spectral patterns before estimating the mapping. In the first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the RGB-to-HSI mapping is refined by querying features from the MoC to replace latent HSI representations, incorporating prior knowledge rather than forcing a direct high-dimensional transformation. To further enhance reconstruction quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention, which adaptively adjust feature map intensities to meet hyperspectral reconstruction requirements. This physically motivated attention mechanism ensures lightweight and efficient HSI recovery. Moreover, we propose an entropy-based Test-Time Adaptation strategy to improve robustness in real-world scenarios. Extensive experiments demonstrate that our method, MCGA, achieves state-of-the-art performance. The code and models will be released at this https URL</li>
</ul>

<h3>Title: TolerantECG: A Foundation Model for Imperfect Electrocardiogram</h3>
<ul>
<li><strong>Authors: </strong>Huynh Nguyen Dang, Thang Pham, Ngan Le, Van Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09887">https://arxiv.org/abs/2507.09887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09887">https://arxiv.org/pdf/2507.09887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09887]] TolerantECG: A Foundation Model for Imperfect Electrocardiogram(https://arxiv.org/abs/2507.09887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.</li>
</ul>

<h3>Title: NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Huibo Xu, Likang Wu, Xianquan Wang, Haoning Dang, Chun-Wun Cheng, Angelica I Aviles-Rivero, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09888">https://arxiv.org/abs/2507.09888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09888">https://arxiv.org/pdf/2507.09888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09888]] NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting(https://arxiv.org/abs/2507.09888)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time series forecasting is a fundamental task with broad applications, yet conventional methods often treat data as discrete sequences, overlooking their origin as noisy samples of continuous processes. Crucially, discrete noisy observations cannot uniquely determine a continuous function; instead, they correspond to a family of plausible functions. Mathematically, time series can be viewed as noisy observations of a continuous function family governed by a shared probability measure. Thus, the forecasting task can be framed as learning the transition from the historical function family to the future function family. This reframing introduces two key challenges: (1) How can we leverage discrete historical and future observations to learn the relationships between their underlying continuous functions? (2) How can we model the transition path in function space from the historical function family to the future function family? To address these challenges, we propose NeuTSFlow, a novel framework that leverages Neural Operators to facilitate flow matching for learning path of measure between historical and future function families. By parameterizing the velocity field of the flow in infinite-dimensional function spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies at discrete points, directly modeling function-level features instead. Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior accuracy and robustness, validating the effectiveness of the function-family perspective.</li>
</ul>

<h3>Title: IGD: Instructional Graphic Design with Multimodal Layer Generation</h3>
<ul>
<li><strong>Authors: </strong>Yadong Qu, Shancheng Fang, Yuxin Wang, Xiaorui Wang, Zhineng Chen, Hongtao Xie, Yongdong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09910">https://arxiv.org/abs/2507.09910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09910">https://arxiv.org/pdf/2507.09910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09910]] IGD: Instructional Graphic Design with Multimodal Layer Generation(https://arxiv.org/abs/2507.09910)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graphic design visually conveys information and data by creating and combining text, images and graphics. Two-stage methods that rely primarily on layout generation lack creativity and intelligence, making graphic design still labor-intensive. Existing diffusion-based methods generate non-editable graphic design files at image level with poor legibility in visual text rendering, which prevents them from achieving satisfactory and practical automated graphic design. In this paper, we propose Instructional Graphic Designer (IGD) to swiftly generate multimodal layers with editable flexibility with only natural language instructions. IGD adopts a new paradigm that leverages parametric rendering and image asset generation. First, we develop a design platform and establish a standardized format for multi-scenario design files, thus laying the foundation for scaling up data. Second, IGD utilizes the multimodal understanding and reasoning capabilities of MLLM to accomplish attribute prediction, sequencing and layout of layers. It also employs a diffusion model to generate image content for assets. By enabling end-to-end training, IGD architecturally supports scalability and extensibility in complex graphic design tasks. The superior experimental results demonstrate that IGD offers a new solution for graphic design.</li>
</ul>

<h3>Title: Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Siyue Yao, Mingjie Sun, Eng Gee Lim, Ran Yi, Baojiang Zhong, Moncef Gabbouj</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09915">https://arxiv.org/abs/2507.09915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09915">https://arxiv.org/pdf/2507.09915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09915]] Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios(https://arxiv.org/abs/2507.09915)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide "crucial information" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be released after acceptance.</li>
</ul>

<h3>Title: Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Md Ahsanul Kabir, Abrar Jahin, Mohammad Al Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09925">https://arxiv.org/abs/2507.09925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09925">https://arxiv.org/pdf/2507.09925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09925]] Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model(https://arxiv.org/abs/2507.09925)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Extracting cause and effect phrases from a sentence is an important NLP task, with numerous applications in various domains, including legal, medical, education, and scientific research. There are many unsupervised and supervised methods proposed for solving this task. Among these, unsupervised methods utilize various linguistic tools, including syntactic patterns, dependency tree, dependency relations, etc. among different sentential units for extracting the cause and effect phrases. On the other hand, the contemporary supervised methods use various deep learning based mask language models equipped with a token classification layer for extracting cause and effect phrases. Linguistic tools, specifically, dependency tree, which organizes a sentence into different semantic units have been shown to be very effective for extracting semantic pairs from a sentence, but existing supervised methods do not have any provision for utilizing such tools within their model framework. In this work, we propose DepBERT, which extends a transformer-based model by incorporating dependency tree of a sentence within the model framework. Extensive experiments over three datasets show that DepBERT is better than various state-of-the art supervised causality extraction methods.</li>
</ul>

<h3>Title: Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications</h3>
<ul>
<li><strong>Authors: </strong>Yoon Pyo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09931">https://arxiv.org/abs/2507.09931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09931">https://arxiv.org/pdf/2507.09931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09931]] Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications(https://arxiv.org/abs/2507.09931)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) into safety-critical domains, such as nuclear engineering, necessitates a deep understanding of their internal reasoning processes. This paper presents a novel methodology for interpreting how an LLM encodes and utilizes domain-specific knowledge, using a Boiling Water Reactor system as a case study. We adapted a general-purpose LLM (Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning technique known as Low-Rank Adaptation. By comparing the neuron activation patterns of the base model to those of the fine-tuned model, we identified a sparse set of neurons whose behavior was significantly altered during the adaptation process. To probe the causal role of these specialized neurons, we employed a neuron silencing technique. Our results demonstrate that while silencing most of these specialized neurons individually did not produce a statistically significant effect, deactivating the entire group collectively led to a statistically significant degradation in task performance. Qualitative analysis further revealed that silencing these neurons impaired the model's ability to generate detailed, contextually accurate technical information. This paper provides a concrete methodology for enhancing the transparency of an opaque black-box model, allowing domain expertise to be traced to verifiable neural circuits. This offers a pathway towards achieving nuclear-grade artificial intelligence (AI) assurance, addressing the verification and validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR 50 Appendix B), which have limited AI deployment in safety-critical nuclear operations.</li>
</ul>

<h3>Title: Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking</h3>
<ul>
<li><strong>Authors: </strong>Hai Toan Nguyen, Tien Dat Nguyen, Viet Ha Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09935">https://arxiv.org/abs/2507.09935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09935">https://arxiv.org/pdf/2507.09935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09935]] Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking(https://arxiv.org/abs/2507.09935)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies for retrieval, which enhance large language models (LLMs) by enabling them to access external knowledge, ensuring that the retrieved information is up-to-date and domain-specific. However, traditional methods often fail to create chunks that capture sufficient semantic meaning, as they do not account for the underlying textual structure. This paper proposes a novel framework that enhances RAG by integrating hierarchical text segmentation and clustering to generate more meaningful and semantically coherent chunks. During inference, the framework retrieves information by leveraging both segment-level and cluster-level vector representations, thereby increasing the likelihood of retrieving more precise and contextually relevant information. Evaluations on the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method achieved improved results compared to traditional chunking techniques.</li>
</ul>

<h3>Title: Memorization Sinks: Isolating Memorization during LLM Training</h3>
<ul>
<li><strong>Authors: </strong>Gaurav R. Ghosal, Pratyush Maini, Aditi Raghunathan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09937">https://arxiv.org/abs/2507.09937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09937">https://arxiv.org/pdf/2507.09937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09937]] Memorization Sinks: Isolating Memorization during LLM Training(https://arxiv.org/abs/2507.09937)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of natural sequences (those that resemble linguistically plausible text) become mechanistically entangled with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of MemSinks that promotes isolation of memorization by design. We leverage a sequence identifier that activates a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that MemSinks facilitates isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement MemSinks at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at this http URL.</li>
</ul>

<h3>Title: Iceberg: Enhancing HLS Modeling with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Zijian Ding, Tung Nguyen, Weikai Li, Aditya Grover, Yizhou Sun, Jason Cong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09948">https://arxiv.org/abs/2507.09948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09948">https://arxiv.org/pdf/2507.09948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09948]] Iceberg: Enhancing HLS Modeling with Synthetic Data(https://arxiv.org/abs/2507.09948)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deep learning-based prediction models for High-Level Synthesis (HLS) of hardware designs often struggle to generalize. In this paper, we study how to close the generalizability gap of these models through pretraining on synthetic data and introduce Iceberg, a synthetic data augmentation approach that expands both large language model (LLM)-generated programs and weak labels of unseen design configurations. Our weak label generation method is integrated with an in-context model architecture, enabling meta-learning from actual and proximate labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when adapt to six real-world applications with few-shot examples and achieves a $2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to two different test datasets. Our open-sourced code is here: \href{this https URL}{this https URL}</li>
</ul>

<h3>Title: Hierarchical Job Classification with Similarity Graph Integration</h3>
<ul>
<li><strong>Authors: </strong>Md Ahsanul Kabir, Kareem Abdelfatah, Mohammed Korayem, Mohammad Al Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09949">https://arxiv.org/abs/2507.09949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09949">https://arxiv.org/pdf/2507.09949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09949]] Hierarchical Job Classification with Similarity Graph Integration(https://arxiv.org/abs/2507.09949)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the dynamic realm of online recruitment, accurate job classification is paramount for optimizing job recommendation systems, search rankings, and labor market analyses. As job markets evolve, the increasing complexity of job titles and descriptions necessitates sophisticated models that can effectively leverage intricate relationships within job data. Traditional text classification methods often fall short, particularly due to their inability to fully utilize the hierarchical nature of industry categories. To address these limitations, we propose a novel representation learning and classification model that embeds jobs and hierarchical industry categories into a latent embedding space. Our model integrates the Standard Occupational Classification (SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both graph and hierarchical relationships, thereby improving classification accuracy. By embedding hierarchical industry categories into a shared latent space, we tackle cold start issues and enhance the dynamic matching of candidates to job opportunities. Extensive experimentation on a large-scale dataset of job postings demonstrates the model's superior ability to leverage hierarchical structures and rich semantic features, significantly outperforming existing methods. This research provides a robust framework for improving job classification accuracy, supporting more informed decision-making in the recruitment industry.</li>
</ul>

<h3>Title: Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis</h3>
<ul>
<li><strong>Authors: </strong>Shubham Shukla, Kunal Sonalkar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09950">https://arxiv.org/abs/2507.09950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09950">https://arxiv.org/pdf/2507.09950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09950]] Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis(https://arxiv.org/abs/2507.09950)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the 'discovery experience' of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (this https URL) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction.</li>
</ul>

<h3>Title: 4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion</h3>
<ul>
<li><strong>Authors: </strong>Zifei Wang, Zian Mao, Xiaoya He, Xi Huang, Haoran Zhang, Chun Cheng, Shufen Chu, Tingzheng Hou, Xiaoqin Zeng, Yujun Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09953">https://arxiv.org/abs/2507.09953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09953">https://arxiv.org/pdf/2507.09953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09953]] 4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion(https://arxiv.org/abs/2507.09953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While electron microscopy offers crucial atomic-resolution insights into structure-property relationships, radiation damage severely limits its use on beam-sensitive materials like proteins and 2D materials. To overcome this challenge, we push beyond the electron dose limits of conventional electron microscopy by adapting principles from multi-image super-resolution (MISR) that have been widely used in remote sensing. Our method fuses multiple low-resolution, sub-pixel-shifted views and enhances the reconstruction with a convolutional neural network (CNN) that integrates features from synthetic, multi-angle observations. We developed a dual-path, attention-guided network for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose data. This provides robust atomic-scale visualization across amorphous, semi-crystalline, and crystalline beam-sensitive specimens. Systematic evaluations on representative materials demonstrate comparable spatial resolution to conventional ptychography under ultra-low-dose conditions. Our work expands the capabilities of 4D-STEM, offering a new and generalizable method for the structural analysis of radiation-vulnerable materials.</li>
</ul>

<h3>Title: Rethinking Inductive Bias in Geographically Neural Network Weighted Regression</h3>
<ul>
<li><strong>Authors: </strong>Zhenyuan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09958">https://arxiv.org/abs/2507.09958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09958">https://arxiv.org/pdf/2507.09958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09958]] Rethinking Inductive Bias in Geographically Neural Network Weighted Regression(https://arxiv.org/abs/2507.09958)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Inductive bias is a key factor in spatial regression models, determining how well a model can learn from limited data and capture spatial patterns. This work revisits the inductive biases in Geographically Neural Network Weighted Regression (GNNWR) and identifies limitations in current approaches for modeling spatial non-stationarity. While GNNWR extends traditional Geographically Weighted Regression by using neural networks to learn spatial weighting functions, existing implementations are often restricted by fixed distance-based schemes and limited inductive bias. We propose to generalize GNNWR by incorporating concepts from convolutional neural networks, recurrent neural networks, and transformers, introducing local receptive fields, sequential context, and self-attention into spatial regression. Through extensive benchmarking on synthetic spatial datasets with varying heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic methods in capturing nonlinear and complex spatial relationships. Our results also reveal that model performance depends strongly on data characteristics, with local models excelling in highly heterogeneous or small-sample scenarios, and global models performing better with larger, more homogeneous data. These findings highlight the importance of inductive bias in spatial modeling and suggest future directions, including learnable spatial weighting functions, hybrid neural architectures, and improved interpretability for models handling non-stationary spatial data.</li>
</ul>

<h3>Title: Text-Driven Causal Representation Learning for Source-Free Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Lihua Zhou, Mao Ye, Nianxin Li, Shuaifeng Li, Jinlin Wu, Xiatian Zhu, Lei Deng, Hongbin Liu, Jiebo Luo, Zhen Lei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09961">https://arxiv.org/abs/2507.09961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09961">https://arxiv.org/pdf/2507.09961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09961]] Text-Driven Causal Representation Learning for Source-Free Domain Generalization(https://arxiv.org/abs/2507.09961)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning often struggles when training and test data distributions differ. Traditional domain generalization (DG) tackles this by including data from multiple source domains, which is impractical due to expensive data collection and annotation. Recent vision-language models like CLIP enable source-free domain generalization (SFDG) by using text prompts to simulate visual representations, reducing data demands. However, existing SFDG methods struggle with domain-specific confounders, limiting their generalization capabilities. To address this issue, we propose TDCRL (\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation \textbf{L}earning), the first method to integrate causal inference into the SFDG setting. TDCRL operates in two steps: first, it employs data augmentation to generate style word vectors, combining them with class information to generate text embeddings to simulate visual representations; second, it trains a causal intervention network with a confounder dictionary to extract domain-invariant features. Grounded in causal learning, our approach offers a clear and effective mechanism to achieve robust, domain-invariant features, ensuring robust generalization. Extensive experiments on PACS, VLCS, OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL effectiveness in SFDG.</li>
</ul>

<h3>Title: Compliance Minimization via Physics-Informed Gaussian Processes</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Sun, Amin Yousefpour, Shirin Hosseinmardi, Ramin Bostanabad</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09968">https://arxiv.org/abs/2507.09968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09968">https://arxiv.org/pdf/2507.09968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09968]] Compliance Minimization via Physics-Informed Gaussian Processes(https://arxiv.org/abs/2507.09968)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) techniques have recently gained significant attention for solving compliance minimization (CM) problems. However, these methods typically provide poor feature boundaries, are very expensive, and lack a systematic mechanism to control the design complexity. Herein, we address these limitations by proposing a mesh-free and simultaneous framework based on physics-informed Gaussian processes (GPs). In our approach, we parameterize the design and state variables with GP priors which have independent kernels but share a multi-output neural network (NN) as their mean function. The architecture of this NN is based on Parametric Grid Convolutional Attention Networks (PGCANs) which not only mitigate spectral bias issues, but also provide an interpretable mechanism to control design complexity. We estimate all the parameters of our GP-based representations by simultaneously minimizing the compliance, total potential energy, and residual of volume fraction constraint. Importantly, our loss function exclude all data-based residuals as GPs automatically satisfy them. We also develop computational schemes based on curriculum training and numerical integration to increase the efficiency and robustness of our approach which is shown to (1) produce super-resolution topologies with fast convergence, (2) achieve smaller compliance and less gray area fraction compared to traditional numerical methods, (3) provide control over fine-scale features, and (4) outperform competing ML-based methods.</li>
</ul>

<h3>Title: Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Xue, Yan Zhang, Ming Li, Chun Li, Yue Liu, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09980">https://arxiv.org/abs/2507.09980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09980">https://arxiv.org/pdf/2507.09980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09980]] Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures(https://arxiv.org/abs/2507.09980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing multi-view classification and clustering methods typically improve task accuracy by leveraging and fusing information from different views. However, ensuring the reliability of multi-view integration and final decisions is crucial, particularly when dealing with noisy or corrupted data. Current methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty of network predictions, ignoring domain gaps between different modalities. To address this issue, KPHD-Net, based on H√∂lder divergence, is proposed for multi-view classification and clustering tasks. Generally, our KPHD-Net employs a variational Dirichlet distribution to represent class probability distributions, models evidences from different views, and then integrates it with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation effects. Our theoretical analysis demonstrates that Proper H√∂lder divergence offers a more effective measure of distribution discrepancies, ensuring enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence theory, recognized for its superior performance in multi-view fusion tasks, is introduced and combined with the Kalman filter to provide future state estimations. This integration further enhances the reliability of the final fusion results. Extensive experiments show that the proposed KPHD-Net outperforms the current state-of-the-art methods in both classification and clustering tasks regarding accuracy, robustness, and reliability, with theoretical guarantees.</li>
</ul>

<h3>Title: TextOmics-Guided Diffusion for Hit-like Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Hang Yuan, Chen Li, Wenjun Ma, Yuncheng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09982">https://arxiv.org/abs/2507.09982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09982">https://arxiv.org/pdf/2507.09982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09982]] TextOmics-Guided Diffusion for Hit-like Molecular Generation(https://arxiv.org/abs/2507.09982)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Hit-like molecular generation with therapeutic potential is essential for target-specific drug discovery. However, the field lacks heterogeneous data and unified frameworks for integrating diverse molecular representations. To bridge this gap, we introduce TextOmics, a pioneering benchmark that establishes one-to-one correspondences between omics expressions and molecular textual descriptions. TextOmics provides a heterogeneous dataset that facilitates molecular generation through representations alignment. Built upon this foundation, we propose ToDi, a generative framework that jointly conditions on omics expressions and molecular textual descriptions to produce biologically relevant, chemically valid, hit-like molecules. ToDi leverages two encoders (OmicsEn and TextEn) to capture multi-level biological and semantic associations, and develops conditional diffusion (DiffGen) for controllable generation. Extensive experiments confirm the effectiveness of TextOmics and demonstrate ToDi outperforms existing state-of-the-art approaches, while also showcasing remarkable potential in zero-shot therapeutic molecular generation. Sources are available at: this https URL.</li>
</ul>

<h3>Title: Latent Diffusion Models with Masked AutoEncoders</h3>
<ul>
<li><strong>Authors: </strong>Junho Lee, Jeongwoo Shin, Hyungwook Choi, Joonseok Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09984">https://arxiv.org/abs/2507.09984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09984">https://arxiv.org/pdf/2507.09984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09984]] Latent Diffusion Models with Masked AutoEncoders(https://arxiv.org/abs/2507.09984)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In spite of remarkable potential of the Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Through comprehensive experiments, we demonstrate significantly enhanced image generation quality and computational efficiency.</li>
</ul>

<h3>Title: Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix</h3>
<ul>
<li><strong>Authors: </strong>Ming Wen, Jiaqi Zhu, Yuedong Xu, Yipeng Zhou, Dingding Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09990">https://arxiv.org/abs/2507.09990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09990">https://arxiv.org/pdf/2507.09990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09990]] Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix(https://arxiv.org/abs/2507.09990)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) typically require fine-tuning for domain-specific tasks, and LoRA offers a computationally efficient approach by training low-rank adapters. LoRA is also communication-efficient for federated LLMs when multiple users collaboratively fine-tune a global LLM model without sharing their proprietary raw data. However, even the transmission of local adapters between a server and clients risks serious privacy leakage. Applying differential privacy (DP) to federated LoRA encounters a dilemma: adding noise to both adapters amplifies synthetic noise on the model, while fixing one adapter impairs the learnability of fine-tuning. In this paper, we propose FedASK (Differentially Private Federated Low Rank Adaptation with Double Sketching) , a novel federated LoRA framework to enable effective updating of both low-rank adapters with robust differential privacy. Inspired by randomized SVD, our key idea is a two-stage sketching pipeline. This pipeline first aggregates carefully sketched, privacy-preserving local updates, and then reconstructs the global matrices on the server to facilitate effective updating of both adapters. We theoretically prove FedASK's differential privacy guarantee and its exact aggregation property. Comprehensive experiments demonstrate that FedASK consistently outperforms baseline methods across a variety of privacy settings and data distributions.</li>
</ul>

<h3>Title: 3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yixun Zhang, Lizhi Wang, Junjun Zhao, Wending Zhao, Feng Zhou, Yonghao Dang, Jianqin Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09993">https://arxiv.org/abs/2507.09993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09993">https://arxiv.org/pdf/2507.09993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09993]] 3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving(https://arxiv.org/abs/2507.09993)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. While existing 2D and 3D physical attacks typically optimize texture, they often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module to preserve geometric fidelity, and a physical augmentation module to simulate complex physical scenarios, thus enhancing attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21% to 7.38%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks. These results validate 3DGAA as a practical attack framework for evaluating the safety of perception systems in autonomous driving.</li>
</ul>

<h3>Title: Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI</h3>
<ul>
<li><strong>Authors: </strong>Quentin Dessain, Nicolas Delinte, Bernard Hanseeuw, Laurence Dricot, Beno√Æt Macq</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.09996">https://arxiv.org/abs/2507.09996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.09996">https://arxiv.org/pdf/2507.09996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.09996]] Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI(https://arxiv.org/abs/2507.09996)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Objective: This study aims to support early diagnosis of Alzheimer's disease and detection of amyloid accumulation by leveraging the microstructural information available in multi-shell diffusion MRI (dMRI) data, using a vision transformer-based deep learning framework. Methods: We present a classification pipeline that employs the Swin Transformer, a hierarchical vision transformer model, on multi-shell dMRI data for the classification of Alzheimer's disease and amyloid presence. Key metrics from DTI and NODDI were extracted and projected onto 2D planes to enable transfer learning with ImageNet-pretrained models. To efficiently adapt the transformer to limited labeled neuroimaging data, we integrated Low-Rank Adaptation. We assessed the framework on diagnostic group prediction (cognitively normal, mild cognitive impairment, Alzheimer's disease dementia) and amyloid status classification. Results: The framework achieved competitive classification results within the scope of multi-shell dMRI-based features, with the best balanced accuracy of 95.2% for distinguishing cognitively normal individuals from those with Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it reached 77.2% balanced accuracy in distinguishing amyloid-positive mild cognitive impairment/Alzheimer's disease dementia subjects from amyloid-negative cognitively normal subjects, and 67.9% for identifying amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based explainability analysis identified clinically relevant brain regions, including the parahippocampal gyrus and hippocampus, as key contributors to model predictions. Conclusion: This study demonstrates the promise of diffusion MRI and transformer-based architectures for early detection of Alzheimer's disease and amyloid pathology, supporting biomarker-driven diagnostics in data-limited biomedical settings.</li>
</ul>

<h3>Title: Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Guanghai Ding, Yihua Ren, Yuting Liu, Qijun Zhao, Shuiwang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10006">https://arxiv.org/abs/2507.10006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10006">https://arxiv.org/pdf/2507.10006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10006]] Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges(https://arxiv.org/abs/2507.10006)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of UAV technology and its extensive application in various fields such as military reconnaissance, environmental monitoring, and logistics, achieving efficient and accurate Anti-UAV tracking has become essential. The importance of Anti-UAV tracking is increasingly prominent, especially in scenarios such as public safety, border patrol, search and rescue, and agricultural monitoring, where operations in complex environments can provide enhanced security. Current mainstream Anti-UAV tracking technologies are primarily centered around computer vision techniques, particularly those that integrate multi-sensor data fusion with advanced detection and tracking algorithms. This paper first reviews the characteristics and current challenges of Anti-UAV detection and tracking technologies. Next, it investigates and compiles several publicly available datasets, providing accessible links to support researchers in efficiently addressing related challenges. Furthermore, the paper analyzes the major vision-based and vision-fusion-based Anti-UAV detection and tracking algorithms proposed in recent years. Finally, based on the above research, this paper outlines future research directions, aiming to provide valuable insights for advancing the field.</li>
</ul>

<h3>Title: Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Jun Li, Xiangmeng Wang, Haoyang Li, Yifei Yan, Hong Va Leong, Ling Feng, Nancy Xiaonan Yu, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10008">https://arxiv.org/abs/2507.10008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10008">https://arxiv.org/pdf/2507.10008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10008]] Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media(https://arxiv.org/abs/2507.10008)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Suicide is a critical global health issue that requires urgent attention. Even though prior work has revealed valuable insights into detecting current suicide risk on social media, little attention has been paid to developing models that can predict subsequent suicide risk over time, limiting their ability to capture rapid fluctuations in individuals' mental state transitions. In addition, existing work ignores protective factors that play a crucial role in suicide risk prediction, focusing predominantly on risk factors alone. Protective factors such as social support and coping strategies can mitigate suicide risk by moderating the impact of risk factors. Therefore, this study proposes a novel framework for predicting subsequent suicide risk by jointly learning the dynamic influence of both risk factors and protective factors on users' suicide risk transitions. We propose a novel Protective Factor-Aware Dataset, which is built from 12 years of Reddit posts along with comprehensive annotations of suicide risk and both risk and protective factors. We also introduce a Dynamic Factors Influence Learning approach that captures the varying impact of risk and protective factors on suicide risk transitions, recognizing that suicide risk fluctuates over time according to established psychological theories. Our thorough experiments demonstrate that the proposed model significantly outperforms state-of-the-art models and large language models across three datasets. In addition, the proposed Dynamic Factors Influence Learning provides interpretable weights, helping clinicians better understand suicidal patterns and enabling more targeted intervention strategies.</li>
</ul>

<h3>Title: Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry</h3>
<ul>
<li><strong>Authors: </strong>Geyou Zhang, Kai Liu, Ce Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10009">https://arxiv.org/abs/2507.10009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10009">https://arxiv.org/pdf/2507.10009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10009]] Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry(https://arxiv.org/abs/2507.10009)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Phase shifting profilometry (PSP) is widely used in high-precision 3D scanning due to its high accuracy, robustness, and pixel-wise handling. However, a fundamental assumption of PSP that the object should remain static does not hold in dynamic measurement, making PSP susceptible to object motion. To address this challenge, our proposed solution, phase-sequential binomial self-compensation (P-BSC), sums successive motion-affected phase frames weighted by binomial coefficients. This approach exponentially reduces the motion error in a pixel-wise and frame-wise loopable manner. Despite its efficacy, P-BSC suffers from high computational overhead and error accumulation due to its reliance on multi-frame phase calculations and weighted summations. Inspired by P-BSC, we propose an image-sequential binomial self-compensation (I-BSC) to weight sum the homogeneous fringe images instead of successive phase frames, which generalizes the BSC concept from phase sequences to image sequences. I-BSC computes the arctangent function only once, resolving both limitations in P-BSC. Extensive analysis, simulations, and experiments show that 1) the proposed BSC outperforms existing methods in reducing motion error while achieving a quasi-single-shot frame rate, i.e., depth map frame rate equals to the camera's acquisition rate, enabling 3D reconstruction with high pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the computational complexity by one polynomial order, thereby accelerating the computational frame rate by several to dozen times, while also reaching faster motion error convergence.</li>
</ul>

<h3>Title: Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect</h3>
<ul>
<li><strong>Authors: </strong>Tom Kouwenhoven, Kiana Shahrasbi, Tessa Verhoef</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10013">https://arxiv.org/abs/2507.10013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10013">https://arxiv.org/pdf/2507.10013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10013]] Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect(https://arxiv.org/abs/2507.10013)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like "bouba" with round shapes and "kiki" with jagged ones. Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as model preference, and we use Grad-CAM as a novel way to interpret visual attention in shape-word matching tasks. Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions.</li>
</ul>

<h3>Title: The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Lixu Wang, Kaixiang Yao, Xinfeng Li, Dong Yang, Haoyang Li, Xiaofeng Wang, Wei Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10016">https://arxiv.org/abs/2507.10016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10016">https://arxiv.org/pdf/2507.10016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10016]] The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents(https://arxiv.org/abs/2507.10016)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.</li>
</ul>

<h3>Title: Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies</h3>
<ul>
<li><strong>Authors: </strong>Seokeon Choi, Sunghyun Park, Hyoungwoo Park, Jeongho Kim, Sungrack Yun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10029">https://arxiv.org/abs/2507.10029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10029">https://arxiv.org/pdf/2507.10029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10029]] Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies(https://arxiv.org/abs/2507.10029)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Memory-efficient personalization is critical for adapting text-to-image diffusion models while preserving user privacy and operating within the limited computational resources of edge devices. To this end, we propose a selective optimization framework that adaptively chooses between backpropagation on low-resolution images (BP-low) and zeroth-order optimization on high-resolution images (ZO-high), guided by the characteristics of the diffusion process. As observed in our experiments, BP-low efficiently adapts the model to target-specific features, but suffers from structural distortions due to resolution mismatch. Conversely, ZO-high refines high-resolution details with minimal memory overhead but faces slow convergence when applied without prior adaptation. By complementing both methods, our framework leverages BP-low for effective personalization while using ZO-high to maintain structural consistency, achieving memory-efficient and high-quality fine-tuning. To maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware probabilistic function that dynamically selects the appropriate optimization strategy based on diffusion timesteps. This function mitigates the overfitting from BP-low at high timesteps, where structural information is critical, while ensuring ZO-high is applied more effectively as training progresses. Experimental results demonstrate that our method achieves competitive performance while significantly reducing memory consumption, enabling scalable, high-quality on-device personalization without increasing inference latency.</li>
</ul>

<h3>Title: Towards Applying Large Language Models to Complement Single-Cell Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Steven Palayew, Bo Wang, Gary Bader</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10039">https://arxiv.org/abs/2507.10039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10039">https://arxiv.org/pdf/2507.10039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10039]] Towards Applying Large Language Models to Complement Single-Cell Foundation Models(https://arxiv.org/abs/2507.10039)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Single-cell foundation models such as scGPT represent a significant advancement in single-cell omics, with an ability to achieve state-of-the-art performance on various downstream biological tasks. However, these models are inherently limited in that a vast amount of information in biology exists as text, which they are unable to leverage. There have therefore been several recent works that propose the use of LLMs as an alternative to single-cell foundation models, achieving competitive results. However, there is little understanding of what factors drive this performance, along with a strong focus on using LLMs as an alternative, rather than complementary approach to single-cell foundation models. In this study, we therefore investigate what biological insights contribute toward the performance of LLMs when applied to single-cell data, and introduce scMPT; a model which leverages synergies between scGPT, and single-cell representations from LLMs that capture these insights. scMPT demonstrates stronger, more consistent performance than either of its component models, which frequently have large performance gaps between each other across datasets. We also experiment with alternate fusion methods, demonstrating the potential of combining specialized reasoning models with scGPT to improve performance. This study ultimately showcases the potential for LLMs to complement single-cell foundation models and drive improvements in single-cell analysis.</li>
</ul>

<h3>Title: On the Efficiency of Training Robust Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Benedict Gerlach, Marie Anastacio, Holger H. Hoos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10048">https://arxiv.org/abs/2507.10048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10048">https://arxiv.org/pdf/2507.10048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10048]] On the Efficiency of Training Robust Decision Trees(https://arxiv.org/abs/2507.10048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As machine learning gets adopted into the industry quickly, trustworthiness is increasingly in focus. Yet, efficiency and sustainability of robust training pipelines still have to be established. In this work, we consider a simple pipeline for training adversarially robust decision trees and investigate the efficiency of each step. Our pipeline consists of three stages. Firstly, we choose the perturbation size automatically for each dataset. For that, we introduce a simple algorithm, instead of relying on intuition or prior work. Moreover, we show that the perturbation size can be estimated from smaller models than the one intended for full training, and thus significant gains in efficiency can be achieved. Secondly, we train state-of-the-art adversarial training methods and evaluate them regarding both their training time and adversarial accuracy. Thirdly, we certify the robustness of each of the models thus obtained and investigate the time required for this. We find that verification time, which is critical to the efficiency of the full pipeline, is not correlated with training time.</li>
</ul>

<h3>Title: CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books</h3>
<ul>
<li><strong>Authors: </strong>Marc Serra Ortega, Emanuele Vivoli, Artemis Llabr√©s, Dimosthenis Karatzas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10053">https://arxiv.org/abs/2507.10053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10053">https://arxiv.org/pdf/2507.10053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10053]] CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books(https://arxiv.org/abs/2507.10053)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces CoSMo, a novel multimodal Transformer for Page Stream Segmentation (PSS) in comic books, a critical task for automated content understanding, as it is a necessary first stage for many downstream tasks like character analysis, story indexing, or metadata enrichment. We formalize PSS for this unique medium and curate a new 20,800-page annotated dataset. CoSMo, developed in vision-only and multimodal variants, consistently outperforms traditional baselines and significantly larger general-purpose vision-language models across F1-Macro, Panoptic Quality, and stream-level metrics. Our findings highlight the dominance of visual features for comic PSS macro-structure, yet demonstrate multimodal benefits in resolving challenging ambiguities. CoSMo establishes a new state-of-the-art, paving the way for scalable comic book analysis.</li>
</ul>

<h3>Title: Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>A. K. M. Shoriful Islam, Md. Rakib Hassan, Macbah Uddin, Md. Shahidur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10056">https://arxiv.org/abs/2507.10056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10056">https://arxiv.org/pdf/2507.10056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10056]] Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning(https://arxiv.org/abs/2507.10056)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Poultry farming is a vital component of the global food supply chain, yet it remains highly vulnerable to infectious diseases such as coccidiosis, salmonellosis, and Newcastle disease. This study proposes a lightweight machine learning-based approach to detect these diseases by analyzing poultry fecal images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and explore a wide range of color, texture, and shape-based descriptors, including color histograms, local binary patterns (LBP), wavelet transforms, and edge detectors. Through a systematic ablation study and dimensionality reduction using PCA and XGBoost feature selection, we identify a compact global feature set that balances accuracy and computational efficiency. An artificial neural network (ANN) classifier trained on these features achieved 95.85% accuracy while requiring no GPU and only 638 seconds of execution time in Google Colab. Compared to deep learning models such as Xception and MobileNetV3, our proposed model offers comparable accuracy with drastically lower resource usage. This work demonstrates a cost-effective, interpretable, and scalable alternative to deep learning for real-time poultry disease detection in low-resource agricultural settings.</li>
</ul>

<h3>Title: GeLaCo: An Evolutionary Approach to Layer Compression</h3>
<ul>
<li><strong>Authors: </strong>David Ponce, Thierry Etchegoyhen, Javier Del Ser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10059">https://arxiv.org/abs/2507.10059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10059">https://arxiv.org/pdf/2507.10059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10059]] GeLaCo: An Evolutionary Approach to Layer Compression(https://arxiv.org/abs/2507.10059)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) have achieved remarkable performance across a large number of tasks, but face critical deployment and usage barriers due to substantial computational requirements. Model compression methods, which aim to reduce model size while preserving its capacity, are an important means to mitigate these issues. Promising approaches along these lines, such as structured pruning, typically require costly empirical search for optimal variants and may run the risk of ignoring better solutions. In this work we introduce GeLaCo, an evolutionary approach to LLM compression via layer collapse. Our approach supports an efficient exploration of the compression solution space via population-based search and a module-wise similarity fitness function capturing attention, feed-forward, and hidden state representations. GeLaCo also supports both single and multi-objective evolutionary compression search, establishing the first Pareto frontier along compression and quality axes. We evaluate GeLaCo solutions via both perplexity-based and generative evaluations over foundational and instruction-tuned models, outperforming state-of-the-art alternatives.</li>
</ul>

<h3>Title: MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second</h3>
<ul>
<li><strong>Authors: </strong>Chenguo Lin, Yuchen Lin, Panwang Pan, Yifan Yu, Honglei Yan, Katerina Fragkiadaki, Yadong Mu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10065">https://arxiv.org/abs/2507.10065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10065">https://arxiv.org/pdf/2507.10065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10065]] MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second(https://arxiv.org/abs/2507.10065)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups.</li>
</ul>

<h3>Title: Frequency Regulation for Exposure Bias Mitigation in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Meng Yu, Kun Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10072">https://arxiv.org/abs/2507.10072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10072">https://arxiv.org/pdf/2507.10072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10072]] Frequency Regulation for Exposure Bias Mitigation in Diffusion Models(https://arxiv.org/abs/2507.10072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models exhibit impressive generative capabilities but are significantly impacted by exposure bias. In this paper, we make a key observation: the energy of the predicted noisy images decreases during the diffusion process. Building on this, we identify two important findings: 1) The reduction in energy follows distinct patterns in the low-frequency and high-frequency subbands; 2) This energy reduction results in amplitude variations between the network-reconstructed clean data and the real clean data. Based on the first finding, we introduce a frequency-domain regulation mechanism utilizing wavelet transforms, which separately adjusts the low- and high-frequency subbands. Leveraging the second insight, we provide a more accurate analysis of exposure bias in the two subbands. Our method is training-free and plug-and-play, significantly improving the generative quality of various diffusion models and providing a robust solution to exposure bias across different model architectures. The source code is available at this https URL.</li>
</ul>

<h3>Title: Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires</h3>
<ul>
<li><strong>Authors: </strong>Simon M√ºnker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10073">https://arxiv.org/abs/2507.10073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10073">https://arxiv.org/pdf/2507.10073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10073]] Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires(https://arxiv.org/abs/2507.10073)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.</li>
</ul>

<h3>Title: A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area</h3>
<ul>
<li><strong>Authors: </strong>Haonan Chen (Tibet University), Xin Tong (Northwestern Polytechnical University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10084">https://arxiv.org/abs/2507.10084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10084">https://arxiv.org/pdf/2507.10084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10084]] A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area(https://arxiv.org/abs/2507.10084)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>To address the prevalent challenges of domain shift and small sample sizes in remote sensing image water body segmentation, this study proposes and validates a two-stage transfer learning strategy based on the SegFormer model. The approach begins by training a foundational segmentation model on a diverse source domain, where it achieves an Intersection over Union (IoU) of 68.80% on its validation set, followed by fine-tuning on data from the distinct target domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by highly complex topography and spectral features -- the experimental results demonstrate that this strategy significantly boosts the IoU for the water body segmentation task from 25.50% (for direct transfer) to 64.84%. This not only effectively resolves the model performance degradation caused by domain discrepancy but also provides an effective technical paradigm for high-precision thematic information extraction in data-scarce and environmentally unique remote sensing scenarios.</li>
</ul>

<h3>Title: FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text</h3>
<ul>
<li><strong>Authors: </strong>Bingchao Wang, Zhiwei Ning, Jianyu Ding, Xuanang Gao, Yin Li, Dongsheng Jiang, Jie Yang, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10095">https://arxiv.org/abs/2507.10095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10095">https://arxiv.org/pdf/2507.10095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10095]] FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text(https://arxiv.org/abs/2507.10095)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>CLIP has shown promising performance across many short-text tasks in a zero-shot manner. However, limited by the input length of the text encoder, CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To remedy this issue, we propose FIX-CLIP which includes three novel modules: (1) A dual-branch training pipeline that aligns short and long texts with masked and raw images respectively, which boosts the long-text representation while preserving the short-text ability. (2) Multiple learnable regional prompts with unidirectional masks in Transformer layers for regional information extraction. (3) A hierarchical feature alignment module in the intermediate encoder layers to promote the consistency of multi-scale features. Furthermore, we collect 30M images and utilize existing MLLMs to synthesize long-text captions for training. Extensive experiments show that FIX-CLIP achieves state-of-the-art performance on both long-text and short-text retrieval benchmarks. For downstream applications, we reveal that FIX-CLIP's text encoder delivers promising performance in a plug-and-play manner for diffusion models with long-text input.</li>
</ul>

<h3>Title: Fusing Large Language Models with Temporal Transformers for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chen Su, Yuanhe Tian, Qinyu Liu, Jun Zhang, Yan Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10098">https://arxiv.org/abs/2507.10098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10098">https://arxiv.org/pdf/2507.10098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10098]] Fusing Large Language Models with Temporal Transformers for Time Series Forecasting(https://arxiv.org/abs/2507.10098)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have demonstrated powerful capabilities in performing various tasks and thus are applied by recent studies to time series forecasting (TSF) tasks, which predict future values with the given historical time series. Existing LLM-based approaches transfer knowledge learned from text data to time series prediction using prompting or fine-tuning strategies. However, LLMs are proficient at reasoning over discrete tokens and semantic patterns but are not initially designed to model continuous numerical time series data. The gaps between text and time series data lead LLMs to achieve inferior performance to a vanilla Transformer model that is directly trained on TSF data. However, the vanilla Transformers often struggle to learn high-level semantic patterns. In this paper, we design a novel Transformer-based architecture that complementarily leverages LLMs and vanilla Transformers, so as to integrate the high-level semantic representations learned by LLMs into the temporal information encoded by time series Transformers, where a hybrid representation is obtained by fusing the representations from the LLM and the Transformer. The resulting fused representation contains both historical temporal dynamics and semantic variation patterns, allowing our model to predict more accurate future values. Experiments on benchmark datasets demonstrate the effectiveness of the proposed approach.</li>
</ul>

<h3>Title: DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ivan Martinoviƒá, Josip ≈†ariƒá, Marin Or≈°iƒá, Matej Kristan, Sini≈°a ≈†egviƒá</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10118">https://arxiv.org/abs/2507.10118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10118">https://arxiv.org/pdf/2507.10118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10118]] DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation(https://arxiv.org/abs/2507.10118)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Pixel-level annotation is expensive and time-consuming. Semi-supervised segmentation methods address this challenge by learning models on few labeled images alongside a large corpus of unlabeled images. Although foundation models could further account for label scarcity, effective mechanisms for their exploitation remain underexplored. We address this by devising a novel semi-supervised panoptic approach fueled by two dedicated foundation models. We enhance recognition by complementing unsupervised mask-transformer consistency with zero-shot classification of CLIP features. We enhance localization by class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting decoupled enhancement of recognition and localization (DEARLi) particularly excels in the most challenging semi-supervised scenarios with large taxonomies and limited labeled data. Moreover, DEARLi outperforms the state of the art in semi-supervised semantic segmentation by a large margin while requiring 8x less GPU memory, in spite of being trained only for the panoptic objective. We observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The source code is available at this https URL.</li>
</ul>

<h3>Title: Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion</h3>
<ul>
<li><strong>Authors: </strong>Md Abulkalam Azad, John Nyberg, H√•vard Dalen, Bj√∏rnar Grenne, Lasse Lovstakken, Andreas √òstvik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10127">https://arxiv.org/abs/2507.10127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10127">https://arxiv.org/pdf/2507.10127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10127]] Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion(https://arxiv.org/abs/2507.10127)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate motion estimation for tracking deformable tissues in echocardiography is essential for precise cardiac function measurements. While traditional methods like block matching or optical flow struggle with intricate cardiac motion, modern point tracking approaches remain largely underexplored in this domain. This work investigates the potential of state-of-the-art (SOTA) point tracking methods for ultrasound, with a focus on echocardiography. Although these novel approaches demonstrate strong performance in general videos, their effectiveness and generalizability in echocardiography remain limited. By analyzing cardiac motion throughout the heart cycle in real B-mode ultrasound videos, we identify that a directional motion bias across different views is affecting the existing training strategies. To mitigate this, we refine the training procedure and incorporate a set of tailored augmentations to reduce the bias and enhance tracking robustness and generalization through impartial cardiac motion. We also propose a lightweight network leveraging multi-scale cost volumes from spatial context alone to challenge the advanced spatiotemporal point tracking models. Experiments demonstrate that fine-tuning with our strategies significantly improves models' performances over their baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker boosts overall position accuracy by 60.7% and reduces median trajectory error by 61.5% across heart cycle phases. Interestingly, several point tracking models fail to outperform our proposed simple model in terms of tracking accuracy and generalization, reflecting their limitations when applied to echocardiography. Nevertheless, clinical evaluation reveals that these methods improve GLS measurements, aligning more closely with expert-validated, semi-automated tools and thus demonstrating better reproducibility in real-world applications.</li>
</ul>

<h3>Title: Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Usman Gani Joy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10132">https://arxiv.org/abs/2507.10132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10132">https://arxiv.org/pdf/2507.10132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10132]] Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting(https://arxiv.org/abs/2507.10132)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Accurate forecasting of energy demand and supply is critical for optimizing sustainable energy systems, yet it is challenged by the variability of renewable sources and dynamic consumption patterns. This paper introduces a neural framework that integrates continuous-time Neural Ordinary Differential Equations (Neural ODEs), graph attention, multi-resolution wavelet transformations, and adaptive learning of frequencies to address the issues of time series prediction. The model employs a robust ODE solver, using the Runge-Kutta method, paired with graph-based attention and residual connections to better understand both structural and temporal patterns. Through wavelet-based feature extraction and adaptive frequency modulation, it adeptly captures and models diverse, multi-scale temporal dynamics. When evaluated across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity transformer temperature), and Waste, Solar, and Hydro (renewable energy), this architecture consistently outperforms state-of-the-art baselines in various forecasting metrics, proving its robustness in capturing complex temporal dependencies. Furthermore, the model enhances interpretability through SHAP analysis, making it suitable for sustainable energy applications.</li>
</ul>

<h3>Title: Deep Recurrence for Dynamical Segmentation Models</h3>
<ul>
<li><strong>Authors: </strong>David Calhas, Arlindo L. Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10143">https://arxiv.org/abs/2507.10143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10143">https://arxiv.org/pdf/2507.10143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10143]] Deep Recurrence for Dynamical Segmentation Models(https://arxiv.org/abs/2507.10143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>While biological vision systems rely heavily on feedback connections to iteratively refine perception, most artificial neural networks remain purely feedforward, processing input in a single static pass. In this work, we propose a predictive coding inspired feedback mechanism that introduces a recurrent loop from output to input, allowing the model to refine its internal state over time. We implement this mechanism within a standard U-Net architecture and introduce two biologically motivated operations, softmax projection and exponential decay, to ensure stability of the feedback loop. Through controlled experiments on a synthetic segmentation task, we show that the feedback model significantly outperforms its feedforward counterpart in noisy conditions and generalizes more effectively with limited supervision. Notably, feedback achieves above random performance with just two training examples, while the feedforward model requires at least four. Our findings demonstrate that feedback enhances robustness and data efficiency, and offer a path toward more adaptive and biologically inspired neural architectures. Code is available at: this http URL.</li>
</ul>

<h3>Title: Task-Based Flexible Feature Distillation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Khouloud Saadi, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10155">https://arxiv.org/abs/2507.10155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10155">https://arxiv.org/pdf/2507.10155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10155]] Task-Based Flexible Feature Distillation for LLMs(https://arxiv.org/abs/2507.10155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Distillation (KD) in general and feature distillation in particular are promising techniques for reducing the high computational demand of large language models (LLMs). However, traditional feature KD methods typically assume that the teacher and the student share the same hidden size, limiting the flexibility of the student's architecture. A common solution to this problem involves training a linear projector to align their feature spaces, but this introduces additional parameters that must be learned from scratch and often degrades performance on downstream tasks, especially in generative settings. To address this issue, in this work, we propose a novel task-based feature distillation method that enables knowledge transfer between teacher and student models with different hidden layer dimensions, without introducing any new parameters. Leveraging the insight that only a subset of LLM components contribute significantly to a specific downstream task, our approach identifies the most task-relevant hidden units in the teacher and directly distills their activations to the student. Our method is flexible and easily integrates with other distillation frameworks. Empirical results show consistent improvements over prior approaches across diverse tasks, including classification, instruction-following, and summarization, achieving up to a 3\% performance gain over the linear projection baseline.</li>
</ul>

<h3>Title: MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping</h3>
<ul>
<li><strong>Authors: </strong>Obaidullah Zaland, Erik Elmroth, Monowar Bhuyan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10158">https://arxiv.org/abs/2507.10158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10158">https://arxiv.org/pdf/2507.10158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10158]] MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping(https://arxiv.org/abs/2507.10158)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a promising machine learning paradigm that enables participating devices to train privacy-preserved and collaborative models. FL has proven its benefits for robotic manipulation tasks. However, grasping tasks lack exploration in such settings where robots train a global model without moving data and ensuring data privacy. The main challenge is that each robot learns from data that is nonindependent and identically distributed (non-IID) and of low quantity. This exhibits performance degradation, particularly in robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL approach for robotic grasping, acknowledging the unique challenges posed by the non-IID data distribution across robots, including quantitative skewness. MTF-Grasp harnesses data quality and quantity across robots to select a set of "top-level" robots with better data distribution and higher sample count. It then utilizes top-level robots to train initial seed models and distribute them to the remaining "low-level" robots, reducing the risk of model performance degradation in low-level robots. Our approach outperforms the conventional FL setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping datasets.</li>
</ul>

<h3>Title: Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Manuel R√∂der, Christoph Raab, Frank-Michael Schleif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10160">https://arxiv.org/abs/2507.10160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10160">https://arxiv.org/pdf/2507.10160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10160]] Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation(https://arxiv.org/abs/2507.10160)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning has emerged as a leading paradigm for decentralized, privacy-preserving learning, particularly relevant in the era of interconnected edge devices equipped with sensors. However, the practical implementation of Federated Learning faces three primary challenges: the need for human involvement in costly data labelling processes for target adaptation, covariate shift in client device data collection due to environmental factors affecting sensors, leading to discrepancies between source and target samples, and the impracticality of continuous or regular model updates in resource-constrained environments due to limited data transmission capabilities and technical constraints on channel availability and energy efficiency. To tackle these issues, we expand upon an efficient and scalable Federated Learning framework tailored for real-world client adaptation in industrial settings. This framework leverages a pre-trained source model comprising a deep backbone, an adaptation module, and a classifier running on a powerful server. By freezing the backbone and classifier during client adaptation on resource-constrained devices, we allow the domain adaptive linear layer to handle target domain adaptation, thus minimizing overall computational overhead. Furthermore, this setup, designated as FedAcross+, is extended to encompass the processing of streaming data, thereby rendering the solution suitable for non-stationary environments. Extensive experimental results demonstrate the effectiveness of FedAcross+ in achieving competitive adaptation on low-end client devices with limited target samples, successfully addressing the challenge of domain shift. Moreover, our framework accommodates sporadic model updates within resource-constrained environments, ensuring practical and seamless deployment.</li>
</ul>

<h3>Title: HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Weiyang He, Chip-Hong Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10162">https://arxiv.org/abs/2507.10162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10162">https://arxiv.org/pdf/2507.10162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10162]] HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning(https://arxiv.org/abs/2507.10162)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Vertical Federated Learning (VFL) enables an orchestrating active party to perform a machine learning task by cooperating with passive parties that provide additional task-related features for the same training data entities. While prior research has leveraged the privacy vulnerability of VFL to compromise its integrity through a combination of label inference and backdoor attacks, their effectiveness is constrained by the low label inference precision and suboptimal backdoor injection conditions. To facilitate a more rigorous security evaluation on VFL without these limitations, we propose HASSLE, a hijacking attack framework composed of a gradient-direction-based label inference module and an adversarial embedding generation algorithm enhanced by self-supervised learning. HASSLE accurately identifies private samples associated with a targeted label using only a single known instance of that label. In the two-party scenario, it demonstrates strong performance with an attack success rate (ASR) of over 99% across four datasets, including both image and tabular modalities, and achieves 85% ASR on the more complex CIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further highlights its significant threat while providing new insights into building a trustworthy VFL system.</li>
</ul>

<h3>Title: Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Wuyang Zhou, Giorgos Iacovides, Kriton Konstantinidis, Ilya Kisil, Danilo Mandic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10170">https://arxiv.org/abs/2507.10170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10170">https://arxiv.org/pdf/2507.10170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10170]] Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach(https://arxiv.org/abs/2507.10170)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Tensor Network (TN) decompositions have emerged as an indispensable tool in Big Data analytics owing to their ability to provide compact low-rank representations, thus alleviating the ``Curse of Dimensionality'' inherent in handling higher-order data. At the heart of their success lies the concept of TN ranks, which governs the efficiency and expressivity of TN decompositions. However, unlike matrix ranks, TN ranks often lack a universal meaning and an intuitive interpretation, with their properties varying significantly across different TN structures. Consequently, TN ranks are frequently treated as empirically tuned hyperparameters, rather than as key design parameters inferred from domain knowledge. The aim of this Lecture Note is therefore to demystify the foundational yet frequently misunderstood concept of TN ranks through real-life examples and intuitive visualizations. We begin by illustrating how domain knowledge can guide the selection of TN ranks in widely-used models such as the Canonical Polyadic (CP) and Tucker decompositions. For more complex TN structures, we employ a self-explanatory graphical approach that generalizes to tensors of arbitrary order. Such a perspective naturally reveals the relationship between TN ranks and the corresponding ranks of tensor unfoldings (matrices), thereby circumventing cumbersome multi-index tensor algebra while facilitating domain-informed TN design. It is our hope that this Lecture Note will equip readers with a clear and unified understanding of the concept of TN rank, along with the necessary physical insight and intuition to support the selection, explainability, and deployment of tensor methods in both practical applications and educational contexts.</li>
</ul>

<h3>Title: Abusive text transformation using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rohitash Chandra, Jiyong Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10177">https://arxiv.org/abs/2507.10177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10177">https://arxiv.org/pdf/2507.10177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10177]] Abusive text transformation using LLMs(https://arxiv.org/abs/2507.10177)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) have demonstrated significant advancements in natural language processing tasks, their effectiveness in the classification and transformation of abusive text into non-abusive versions remains an area for exploration. In this study, we aim to use LLMs to transform abusive text (tweets and reviews) featuring hate speech and swear words into non-abusive text, while retaining the intent of the text. We evaluate the performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and Groq, on their ability to identify abusive text. We them to transform and obtain a text that is clean from abusive and inappropriate content but maintains a similar level of sentiment and semantics, i.e. the transformed text needs to maintain its message. Afterwards, we evaluate the raw and transformed datasets with sentiment analysis and semantic analysis. Our results show Groq provides vastly different results when compared with other LLMs. We have identified similarities between GPT-4o and DeepSeek-V3.</li>
</ul>

<h3>Title: Learning Private Representations through Entropy-based Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Tassilo Klein, Moin Nabi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10194">https://arxiv.org/abs/2507.10194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10194">https://arxiv.org/pdf/2507.10194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10194]] Learning Private Representations through Entropy-based Adversarial Training(https://arxiv.org/abs/2507.10194)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>How can we learn a representation with high predictive power while preserving user privacy? We present an adversarial representation learning method for sanitizing sensitive content from the learned representation. Specifically, we introduce a variant of entropy - focal entropy, which mitigates the potential information leakage of the existing entropy-based approaches. We showcase feasibility on multiple benchmarks. The results suggest high target utility at moderate privacy leakage.</li>
</ul>

<h3>Title: Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Shuyu Yang, Yaxiong Wang, Yongrui Li, Li Zhu, Zhedong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10195">https://arxiv.org/abs/2507.10195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10195">https://arxiv.org/pdf/2507.10195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10195]] Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval(https://arxiv.org/abs/2507.10195)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we focus on text-based person retrieval, which aims to identify individuals based on textual descriptions. Given the significant privacy issues and the high cost associated with manual annotation, synthetic data has become a popular choice for pretraining models, leading to notable advancements. However, the considerable domain gap between synthetic pretraining datasets and real-world target datasets, characterized by differences in lighting, color, and viewpoint, remains a critical obstacle that hinders the effectiveness of the pretrain-finetune paradigm. To bridge this gap, we introduce a unified text-based person retrieval pipeline considering domain adaptation at both image and region levels. In particular, it contains two primary components, i.e., Domain-aware Diffusion (DaD) for image-level adaptation and Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the name implies, Domain-aware Diffusion is to migrate the distribution of images from the pretraining dataset domain to the target real-world dataset domain, e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level alignment by establishing correspondences between visual regions and their descriptive sentences, thereby addressing disparities at a finer granularity. Extensive experiments show that our dual-level adaptation method has achieved state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets, outperforming existing methodologies. The dataset, model, and code are available at this https URL.</li>
</ul>

<h3>Title: A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images</h3>
<ul>
<li><strong>Authors: </strong>Jaeseong Lee, Yeeun Choi, Heechan Choi, Hanjung Kim, Seonjoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10202">https://arxiv.org/abs/2507.10202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10202">https://arxiv.org/pdf/2507.10202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10202]] A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images(https://arxiv.org/abs/2507.10202)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding, reasoning, and generation. However, they struggle with tasks requiring fine-grained localization and reasoning in high-resolution images. This constraint stems from the fact that MLLMs are fine-tuned with fixed image resolution to align with the pre-trained image encoder used in MLLM. Consequently, feeding high-resolution images directly into MLLMs leads to poor generalization due to a train-test resolution discrepancy, while downsampling these images-although ensuring consistency-compromises fine-grained visual details and ultimately degrades performance. To address this challenge, we propose Extract Candidate then Predict (ECP), a novel training-free, task-agnostic two-stage framework designed to enhance MLLM performance on high-resolution images. The key intuition behind ECP is that while MLLMs struggle with high-resolution images, their predictions on downsampled images still contain implicit localization cues. By first identifying candidate region using the coarse prediction and then predicting the final output based on candidate region, ECP effectively preserves fine-grained details while mitigating the challenges posed by high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared to baseline respectively, demonstrating its effectiveness. Code is available at this https URL.</li>
</ul>

<h3>Title: Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects</h3>
<ul>
<li><strong>Authors: </strong>Renad Al-Monef, Hassan Alhuzali, Nora Alturayeif, Ashwag Alasmari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10216">https://arxiv.org/abs/2507.10216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10216">https://arxiv.org/pdf/2507.10216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10216]] Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects(https://arxiv.org/abs/2507.10216)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.</li>
</ul>

<h3>Title: From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jeongho Kim, Sunghyun Park, Hyoungwoo Park, Sungrack Yun, Jaegul Choo, Seokeon Cho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10217">https://arxiv.org/abs/2507.10217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10217">https://arxiv.org/pdf/2507.10217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10217]] From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation(https://arxiv.org/abs/2507.10217)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion models achieve personalization by learning specific subjects, allowing learned attributes to be integrated into generated images. However, personalized human image generation remains challenging due to the need for precise and consistent attribute preservation (e.g., identity, clothing details). Existing subject-driven image generation methods often require either (1) inference-time fine-tuning with few images for each new subject or (2) large-scale dataset training for generalization. Both approaches are computationally expensive and impractical for real-time applications. To address these limitations, we present Wardrobe Polyptych LoRA, a novel part-level controllable model for personalized human image generation. By training only LoRA layers, our method removes the computational burden at inference while ensuring high-fidelity synthesis of unseen subjects. Our key idea is to condition the generation on the subject's wardrobe and leverage spatial references to reduce information loss, thereby improving fidelity and consistency. Additionally, we introduce a selective subject region loss, which encourages the model to disregard some of reference images during training. Our loss ensures that generated images better align with text prompts while maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no additional parameters at the inference stage and performs generation using a single model trained on a few training samples. We construct a new dataset and benchmark tailored for personalized human image generation. Extensive experiments show that our approach significantly outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis.</li>
</ul>

<h3>Title: Spatial Lifting for Dense Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mingzhi Xu, Yizhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10222">https://arxiv.org/abs/2507.10222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10222">https://arxiv.org/pdf/2507.10222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10222]] Spatial Lifting for Dense Prediction(https://arxiv.org/abs/2507.10222)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We present Spatial Lifting (SL), a novel methodology for dense prediction tasks. SL operates by lifting standard inputs, such as 2D images, into a higher-dimensional space and subsequently processing them using networks designed for that higher dimension, such as a 3D U-Net. Counterintuitively, this dimensionality lifting allows us to achieve good performance on benchmark tasks compared to conventional approaches, while reducing inference costs and significantly lowering the number of model parameters. The SL framework produces intrinsically structured outputs along the lifted dimension. This emergent structure facilitates dense supervision during training and enables robust, near-zero-additional-cost prediction quality assessment at test time. We validate our approach across 19 benchmark datasets (13 for semantic segmentation and 6 for depth estimation), demonstrating competitive dense prediction performance while reducing the model parameter count by over 98% (in the U-Net case) and lowering inference costs. Spatial Lifting introduces a new vision modeling paradigm that offers a promising path toward more efficient, accurate, and reliable deep networks for dense prediction tasks in vision.</li>
</ul>

<h3>Title: ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Yin, Boyuan Yang, Weichen Liu, Qiyao Xue, Abrar Alamri, Goeran Fiedler, Wei Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10223">https://arxiv.org/abs/2507.10223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10223">https://arxiv.org/pdf/2507.10223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10223]] ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users(https://arxiv.org/abs/2507.10223)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. Our code is available at this https URL and dataset at this https URL.</li>
</ul>

<h3>Title: Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Jinglun Li, Kaixun Jiang, Zhaoyu Chen, Bo Lin, Yao Tang, Weifeng Ge, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10225">https://arxiv.org/abs/2507.10225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10225">https://arxiv.org/pdf/2507.10225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10225]] Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection(https://arxiv.org/abs/2507.10225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD/OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by 11.13%. Codes are available in this https URL.</li>
</ul>

<h3>Title: Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?</h3>
<ul>
<li><strong>Authors: </strong>Despina Konstantinidou, Dimitrios Karageorgiou, Christos Koutlis, Olga Papadopoulou, Emmanouil Schinas, Symeon Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10236">https://arxiv.org/abs/2507.10236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10236">https://arxiv.org/pdf/2507.10236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10236]] Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?(https://arxiv.org/abs/2507.10236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative technologies presents both unprecedented creative opportunities and significant challenges, particularly in maintaining social trust and ensuring the integrity of digital information. Following these concerns, the challenge of AI-Generated Image Detection (AID) becomes increasingly critical. As these technologies become more sophisticated, the quality of AI-generated images has reached a level that can easily deceive even the most discerning observers. Our systematic evaluation highlights a critical weakness in current AI-Generated Image Detection models: while they perform exceptionally well on controlled benchmark datasets, they struggle significantly with real-world variations. To assess this, we introduce ITW-SM, a new dataset of real and AI-generated images collected from major social media platforms. In this paper, we identify four key factors that influence AID performance in real-world scenarios: backbone architecture, training data composition, pre-processing strategies and data augmentation combinations. By systematically analyzing these components, we shed light on their impact on detection efficacy. Our modifications result in an average AUC improvement of 26.87% across various AID models under real-world conditions.</li>
</ul>

<h3>Title: Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks</h3>
<ul>
<li><strong>Authors: </strong>Ben Hamscher, Edgar Heinert, Annika M√ºtze, Kira Maag, Matthias Rottmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10239">https://arxiv.org/abs/2507.10239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10239">https://arxiv.org/pdf/2507.10239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10239]] Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks(https://arxiv.org/abs/2507.10239)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method.</li>
</ul>

<h3>Title: Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients</h3>
<ul>
<li><strong>Authors: </strong>Vikas Dwivedi, Balaji Srinivasan, Monica Sigovan, Bruno Sixou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10241">https://arxiv.org/abs/2507.10241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10241">https://arxiv.org/pdf/2507.10241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10241]] Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients(https://arxiv.org/abs/2507.10241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of PI-ELM designed to solve both forward and inverse Partial Differential Equation (PDE) problems involving localized sharp gradients. While PI-ELMs outperform the traditional Physics-Informed Neural Networks (PINNs) in speed due to their single-shot, least square optimization, this advantage comes at a cost: their fixed, randomly initialized input layer limits their ability to capture sharp gradients. To overcome this limitation, we introduce a lightweight Bayesian Optimization (BO) framework that, instead of adjusting each input layer parameter individually as in traditional backpropagation, learns a small set of hyperparameters defining the statistical distribution from which the input weights are drawn. This novel distributional optimization strategy -- combining BO for input layer distributional parameters with least-squares optimization for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's speed while matching or exceeding the expressiveness of PINNs. We validate the proposed methodology on several challenging forward and inverse PDE benchmarks, including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson equation with sharp localized sources, and a time-dependent advection equation. Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and inverse settings. In stiff PDE regimes, it matches or even outperforms advanced methods such as the Extended Theory of Functional Connections (XTFC), while requiring nearly an order of magnitude fewer tunable parameters. These results establish the potential of KAPI-ELM as a scalable, interpretable, and generalizable physics-informed learning framework, especially in stiff PDE regimes.</li>
</ul>

<h3>Title: Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures</h3>
<ul>
<li><strong>Authors: </strong>Xinlong Ding, Hongwei Yu, Jiawei Li, Feifan Li, Yu Shang, Bochao Zou, Huimin Ma, Jiansheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10265">https://arxiv.org/abs/2507.10265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10265">https://arxiv.org/pdf/2507.10265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10265]] Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures(https://arxiv.org/abs/2507.10265)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models.</li>
</ul>

<h3>Title: DNS Tunneling: Threat Landscape and Improved Detection Solutions</h3>
<ul>
<li><strong>Authors: </strong>Novruz Amirov, Baran Isik, Bilal Ihsan Tuncer, Serif Bahtiyar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10267">https://arxiv.org/abs/2507.10267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10267">https://arxiv.org/pdf/2507.10267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10267]] DNS Tunneling: Threat Landscape and Improved Detection Solutions(https://arxiv.org/abs/2507.10267)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Detecting Domain Name System (DNS) tunneling is a significant challenge in security due to its capacity to hide harmful actions within DNS traffic that appears to be normal and legitimate. Traditional detection methods are based on rule-based approaches or signature matching methods that are often insufficient to accurately identify such covert communication channels. This research is about effectively detecting DNS tunneling. We propose a novel approach to detect DNS tunneling with machine learning algorithms. We combine machine learning algorithms to analyze the traffic by using features extracted from DNS traffic. Analyses results show that the proposed approach is a good candidate to detect DNS tunneling accurately.</li>
</ul>

<h3>Title: Conditional Chemical Language Models are Versatile Tools in Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Lu Zhu, Emmanuel Noutahi</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10273">https://arxiv.org/abs/2507.10273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10273">https://arxiv.org/pdf/2507.10273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10273]] Conditional Chemical Language Models are Versatile Tools in Drug Discovery(https://arxiv.org/abs/2507.10273)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Generative chemical language models (CLMs) have demonstrated strong capabilities in molecular design, yet their impact in drug discovery remains limited by the absence of reliable reward signals and the lack of interpretability in their outputs. We present SAFE-T, a generalist chemical modeling framework that conditions on biological context -- such as protein targets or mechanisms of action -- to prioritize and design molecules without relying on structural information or engineered scoring functions. SAFE-T models the conditional likelihood of fragment-based molecular sequences given a biological prompt, enabling principled scoring of molecules across tasks such as virtual screening, drug-target interaction prediction, and activity cliff detection. Moreover, it supports goal-directed generation by sampling from this learned distribution, aligning molecular design with biological objectives. In comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA, ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves performance comparable to or better than existing approaches while being significantly faster. Fragment-level attribution further reveals that SAFE-T captures known structure-activity relationships, supporting interpretable and biologically grounded design. Together with its computational efficiency, these results demonstrate that conditional generative CLMs can unify scoring and generation to accelerate early-stage drug discovery.</li>
</ul>

<h3>Title: FTCFormer: Fuzzy Token Clustering Transformer for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Muyi Bao, Changyu Zeng, Yifan Wang, Zhengni Yang, Zimu Wang, Guangliang Cheng, Jun Qi, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10283">https://arxiv.org/abs/2507.10283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10283">https://arxiv.org/pdf/2507.10283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10283]] FTCFormer: Fuzzy Token Clustering Transformer for Image Classification(https://arxiv.org/abs/2507.10283)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based deep neural networks have achieved remarkable success across various computer vision tasks, largely attributed to their long-range self-attention mechanism and scalability. However, most transformer architectures embed images into uniform, grid-based vision tokens, neglecting the underlying semantic meanings of image regions, resulting in suboptimal feature representations. To address this issue, we propose Fuzzy Token Clustering Transformer (FTCFormer), which incorporates a novel clustering-based downsampling module to dynamically generate vision tokens based on the semantic meanings instead of spatial positions. It allocates fewer tokens to less informative regions and more to represent semantically important regions, regardless of their spatial adjacency or shape irregularity. To further enhance feature extraction and representation, we propose a Density Peak Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center determination, a Spatial Connectivity Score (SCS) for token assignment, and a channel-wise merging (Cmerge) strategy for token merging. Extensive experiments on 32 datasets across diverse domains validate the effectiveness of FTCFormer on image classification, showing consistent improvements over the TCFormer baseline, achieving gains of improving 1.43% on five fine-grained datasets, 1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55% on four remote sensing datasets. The code is available at: this https URL.</li>
</ul>

<h3>Title: Average Sensitivity of Hierarchical $k$-Median Clustering</h3>
<ul>
<li><strong>Authors: </strong>Shijie Li, Weiqiang He, Ruobing Bai, Pan Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10296">https://arxiv.org/abs/2507.10296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10296">https://arxiv.org/pdf/2507.10296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10296]] Average Sensitivity of Hierarchical $k$-Median Clustering(https://arxiv.org/abs/2507.10296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Hierarchical clustering is a widely used method for unsupervised learning with numerous applications. However, in the application of modern algorithms, the datasets studied are usually large and dynamic. If the hierarchical clustering is sensitive to small perturbations of the dataset, the usability of the algorithm will be greatly reduced. In this paper, we focus on the hierarchical $k$ -median clustering problem, which bridges hierarchical and centroid-based clustering while offering theoretical appeal, practical utility, and improved interpretability. We analyze the average sensitivity of algorithms for this problem by measuring the expected change in the output when a random data point is deleted. We propose an efficient algorithm for hierarchical $k$-median clustering and theoretically prove its low average sensitivity and high clustering quality. Additionally, we show that single linkage clustering and a deterministic variant of the CLNSS algorithm exhibit high average sensitivity, making them less stable. Finally, we validate the robustness and effectiveness of our algorithm through experiments.</li>
</ul>

<h3>Title: FaceLLM: A Multimodal Large Language Model for Face Understanding</h3>
<ul>
<li><strong>Authors: </strong>Hatef Otroshi Shahreza, S√©bastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10300">https://arxiv.org/abs/2507.10300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10300">https://arxiv.org/pdf/2507.10300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10300]] FaceLLM: A Multimodal Large Language Model for Face Understanding(https://arxiv.org/abs/2507.10300)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.</li>
</ul>

<h3>Title: DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Zhao, Rongkun Zheng, Yi Wang, Helin Wang, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10302">https://arxiv.org/abs/2507.10302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10302">https://arxiv.org/pdf/2507.10302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10302]] DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs(https://arxiv.org/abs/2507.10302)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In video Multimodal Large Language Models (video MLLMs), the visual encapsulation process plays a pivotal role in converting video contents into representative tokens for LLM input. While linear projectors are widely employed for encapsulation, they introduce semantic indistinctness and temporal incoherence when applied to videos. Conversely, the structure of resamplers shows promise in tackling these challenges, but an effective solution remains unexplored. Drawing inspiration from resampler structures, we introduce DisCo, a novel visual encapsulation method designed to yield semantically distinct and temporally coherent visual tokens for video MLLMs. DisCo integrates two key components: (1) A Visual Concept Discriminator (VCD) module, assigning unique semantics for visual tokens by associating them in pair with discriminative concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring consistent temporal focus of visual tokens to video elements across every video frame. Through extensive experiments on multiple video MLLM frameworks, we demonstrate that DisCo remarkably outperforms previous state-of-the-art methods across a variety of video understanding benchmarks, while also achieving higher token efficiency thanks to the reduction of semantic indistinctness. The code: this https URL.</li>
</ul>

<h3>Title: Recognizing Dementia from Neuropsychological Tests with State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Liming Wang, Saurabhchand Bhati, Cody Karjadi, Rhoda Au, James Glass</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10311">https://arxiv.org/abs/2507.10311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10311">https://arxiv.org/pdf/2507.10311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10311]] Recognizing Dementia from Neuropsychological Tests with State Space Models(https://arxiv.org/abs/2507.10311)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Early detection of dementia is critical for timely medical intervention and improved patient outcomes. Neuropsychological tests are widely used for cognitive assessment but have traditionally relied on manual scoring. Automatic dementia classification (ADC) systems aim to infer cognitive decline directly from speech recordings of such tests. We propose Demenba, a novel ADC framework based on state space models, which scale linearly in memory and computation with sequence length. Trained on over 1,000 hours of cognitive assessments administered to Framingham Heart Study participants, some of whom were diagnosed with dementia through adjudicated review, our method outperforms prior approaches in fine-grained dementia classification by 21\%, while using fewer parameters. We further analyze its scaling behavior and demonstrate that our model gains additional improvement when fused with large language models, paving the way for more transparent and scalable dementia assessment tools. Code: this https URL</li>
</ul>

<h3>Title: Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Liu, Jingwen Fu, Yang Wu, Kangyi Wu, Pengna Li, Jiayi Wu, Sanping Zhou, Jingmin Xin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10318">https://arxiv.org/abs/2507.10318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10318">https://arxiv.org/pdf/2507.10318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10318]] Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching(https://arxiv.org/abs/2507.10318)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment.</li>
</ul>

<h3>Title: Convergence of Agnostic Federated Averaging</h3>
<ul>
<li><strong>Authors: </strong>Herlock (SeyedAbolfazl)Rahimi, Dionysis Kalogerias</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10325">https://arxiv.org/abs/2507.10325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10325">https://arxiv.org/pdf/2507.10325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10325]] Convergence of Agnostic Federated Averaging(https://arxiv.org/abs/2507.10325)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables decentralized model training without centralizing raw data. However, practical FL deployments often face a key realistic challenge: Clients participate intermittently in server aggregation and with unknown, possibly biased participation probabilities. Most existing convergence results either assume full-device participation, or rely on knowledge of (in fact uniform) client availability distributions -- assumptions that rarely hold in practice. In this work, we characterize the optimization problem that consistently adheres to the stochastic dynamics of the well-known \emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and variably-sized) client availability, and rigorously establish its convergence for convex, possibly nonsmooth losses, achieving a standard rate of order $\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our analysis provides the first convergence guarantees for agnostic FedAvg under general, non-uniform, stochastic client participation, without knowledge of the participation distribution. We also empirically demonstrate that agnostic FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg variants, even with server-side knowledge of participation weights.</li>
</ul>

<h3>Title: Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Muzhaffar Hazman, Minh-Khoi Pham, Shweta Soundararajan, Goncalo Mordido, Leonardo Custode, David Lynch, Giorgio Cruciata, Yucheng Shi, Hongmeng Song, Wang Chao, Pan Yue, Aleksandar Milenovic, Alexandros Agapitos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10326">https://arxiv.org/abs/2507.10326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10326">https://arxiv.org/pdf/2507.10326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10326]] Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation(https://arxiv.org/abs/2507.10326)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering has proven to be a crucial step in leveraging pretrained large language models (LLMs) in solving various real-world tasks. Numerous solutions have been proposed that seek to automate prompt engineering by using the model itself to edit prompts. However, the majority of state-of-the-art approaches are evaluated on tasks that require minimal prompt templates and on very large and highly capable LLMs. In contrast, solving complex tasks that require detailed information to be included in the prompt increases the amount of text that needs to be optimised. Furthermore, smaller models have been shown to be more sensitive to prompt design. To address these challenges, we propose an evolutionary search approach to automated discrete prompt optimisation consisting of two phases. In the first phase, grammar-guided genetic programming is invoked to synthesise prompt-creating programmes by searching the space of programmes populated by function compositions of syntactic, dictionary-based and LLM-based prompt-editing functions. In the second phase, local search is applied to explore the neighbourhoods of best-performing programmes in an attempt to further fine-tune their performance. Our approach outperforms three state-of-the-art prompt optimisation approaches, PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose LLMs in four domain-specific challenging tasks. We also illustrate several examples where these benchmark methods suffer relatively severe performance degradation, while our approach improves performance in almost all task-model combinations, only incurring minimal degradation when it does not.</li>
</ul>

<h3>Title: Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Bouri, Adnane Saoud</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10330">https://arxiv.org/abs/2507.10330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10330">https://arxiv.org/pdf/2507.10330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10330]] Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach(https://arxiv.org/abs/2507.10330)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at this https URL</li>
</ul>

<h3>Title: MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Bekhit, Ahmad Salah, Ahmed Salim Alrawahi, Tarek Attia, Ahmed Ali, Esraa Eldesokey, Ahmed Fathalla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10334">https://arxiv.org/abs/2507.10334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10334">https://arxiv.org/pdf/2507.10334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10334]] MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data(https://arxiv.org/abs/2507.10334)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs) is vital for applications in sports science, but its utility is often compromised by missing data. Despite numerous imputation techniques, a systematic performance evaluation for IMU-derived MoCap time-series data is lacking. We address this gap by conducting a comprehensive comparative analysis of statistical, machine learning, and deep learning imputation methods. Our evaluation considers three distinct contexts: univariate time-series, multivariate across subjects, and multivariate across kinematic angles. To facilitate this benchmark, we introduce the first publicly available MoCap dataset designed specifically for imputation, featuring data from 53 karate practitioners. We simulate three controlled missingness mechanisms: missing completely at random (MCAR), block missingness, and a novel value-dependent pattern at signal transition points. Our experiments, conducted on 39 kinematic variables across all subjects, reveal that multivariate imputation frameworks consistently outperform univariate approaches, particularly for complex missingness. For instance, multivariate methods achieve up to a 50% mean absolute error reduction (MAE from 10.8 to 5.8) compared to univariate techniques for transition point missingness. Advanced models like Generative Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the highest accuracy in these challenging scenarios. This work provides a critical baseline for future research and offers practical recommendations for improving the integrity and robustness of Mo-Cap data analysis.</li>
</ul>

<h3>Title: Text Embedding Knows How to Quantize Text-Guided Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hongjae Lee, Myungjun Son, Dongjea Kang, Seung-Won Jung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10340">https://arxiv.org/abs/2507.10340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10340">https://arxiv.org/pdf/2507.10340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10340]] Text Embedding Knows How to Quantize Text-Guided Diffusion Models(https://arxiv.org/abs/2507.10340)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets.</li>
</ul>

<h3>Title: Using AI to replicate human experimental results: a motion study</h3>
<ul>
<li><strong>Authors: </strong>Rosa Illan Castillo, Javier Valenzuela</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10342">https://arxiv.org/abs/2507.10342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10342">https://arxiv.org/pdf/2507.10342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10342]] Using AI to replicate human experimental results: a motion study(https://arxiv.org/abs/2507.10342)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the potential of large language models (LLMs) as reliable analytical tools in linguistic research, focusing on the emergence of affective meanings in temporal expressions involving manner-of-motion verbs. While LLMs like GPT-4 have shown promise across a range of tasks, their ability to replicate nuanced human judgements remains under scrutiny. We conducted four psycholinguistic studies (on emergent meanings, valence shifts, verb choice in emotional contexts, and sentence-emoji associations) first with human participants and then replicated the same tasks using an LLM. Results across all studies show a striking convergence between human and AI responses, with statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong correlations in both rating patterns and categorical choices. While minor divergences were observed in some cases, these did not alter the overall interpretative outcomes. These findings offer compelling evidence that LLMs can augment traditional human-based experimentation, enabling broader-scale studies without compromising interpretative validity. This convergence not only strengthens the empirical foundation of prior human-based findings but also opens possibilities for hypothesis generation and data expansion through AI. Ultimately, our study supports the use of LLMs as credible and informative collaborators in linguistic inquiry.</li>
</ul>

<h3>Title: FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans</h3>
<ul>
<li><strong>Authors: </strong>Hugo Norrby, Gabriel F√§rm, Kevin Hernandez-Diaz, Fernando Alonso-Fernandez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10343">https://arxiv.org/abs/2507.10343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10343">https://arxiv.org/pdf/2507.10343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10343]] FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans(https://arxiv.org/abs/2507.10343)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce FGSSNet, a novel multi-headed feature-guided semantic segmentation (FGSS) architecture designed to improve the generalization ability of wall segmentation on floorplans. FGSSNet features a U-Net segmentation backbone with a multi-headed dedicated feature extractor used to extract domain-specific feature maps which are injected into the latent space of U-Net to guide the segmentation process. This dedicated feature extractor is trained as an encoder-decoder with selected wall patches, representative of the walls present in the input floorplan, to produce a compressed latent representation of wall patches while jointly trained to predict the wall width. In doing so, we expect that the feature extractor encodes texture and width features of wall patches that are useful to guide the wall segmentation process. Our experiments show increased performance by the use of such injected features in comparison to the vanilla U-Net, highlighting the validity of the proposed approach.</li>
</ul>

<h3>Title: Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions</h3>
<ul>
<li><strong>Authors: </strong>Yuwen Li, Guozhi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10345">https://arxiv.org/abs/2507.10345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10345">https://arxiv.org/pdf/2507.10345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10345]] Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions(https://arxiv.org/abs/2507.10345)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU neural networks for Korobov functions. In terms of network width and depth, we derive nearly optimal super-approximation error bounds of order $2m$ in the $L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with $L_p$ mixed derivative of order $m$ in each direction. The analysis leverages sparse grid finite elements and the bit extraction technique. Our results improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and demonstrate that the expressivity of neural networks is largely unaffected by the curse of dimensionality.</li>
</ul>

<h3>Title: Parallel Sampling of Diffusion Models on $SO(3)$</h3>
<ul>
<li><strong>Authors: </strong>Yan-Ting Chen, Hao-Wei Chen, Tsu-Ching Hsiao, Chun-Yi Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10347">https://arxiv.org/abs/2507.10347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10347">https://arxiv.org/pdf/2507.10347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10347]] Parallel Sampling of Diffusion Models on $SO(3)$(https://arxiv.org/abs/2507.10347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we design an algorithm to accelerate the diffusion process on the $SO(3)$ manifold. The inherently sequential nature of diffusion models necessitates substantial time for denoising perturbed data. To overcome this limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$ space. We demonstrate our algorithm on an existing method that employs diffusion models to address the pose ambiguity problem. Moreover, we show that this acceleration advantage occurs without any measurable degradation in task reward. The experiments reveal that our algorithm achieves a speed-up of up to 4.9$\times$, significantly reducing the latency for generating a single sample.</li>
</ul>

<h3>Title: Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Yichen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10348">https://arxiv.org/abs/2507.10348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10348">https://arxiv.org/pdf/2507.10348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10348]] Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning(https://arxiv.org/abs/2507.10348)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing attention for its ability to aggregate knowledge from heterogeneous models while keeping private data locally. To better aggregate knowledge from clients, ensemble distillation, as a widely used and effective technique, is often employed after global aggregation to enhance the performance of the global model. However, simply combining Hetero-FL and ensemble distillation does not always yield promising results and can make the training process unstable. The reason is that existing methods primarily focus on logit distillation, which, while being model-agnostic with softmax predictions, fails to compensate for the knowledge bias arising from heterogeneous models. To tackle this challenge, we propose a stable and efficient Feature Distillation for model-heterogeneous Federated learning, dubbed FedFD, that can incorporate aligned feature information via orthogonal projection to integrate knowledge from heterogeneous models better. Specifically, a new feature-based ensemble federated knowledge distillation paradigm is proposed. The global model on the server needs to maintain a projection layer for each client-side model architecture to align the features separately. Orthogonal techniques are employed to re-parameterize the projection layer to mitigate knowledge bias from heterogeneous models and thus maximize the distilled knowledge. Extensive experiments show that FedFD achieves superior performance compared to state-of-the-art methods.</li>
</ul>

<h3>Title: TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Zhao, Sitan Yang, Kin G. Olivares, Boris N. Oreshkin, Stan Vitebsky, Michael W. Mahoney, B. Aditya Prakash, Dmitry Efimov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10349">https://arxiv.org/abs/2507.10349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10349">https://arxiv.org/pdf/2507.10349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10349]] TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting(https://arxiv.org/abs/2507.10349)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-horizon time series forecasting has many practical applications such as demand forecasting. Accurate demand prediction is critical to help make buying and inventory decisions for supply chain management of e-commerce and physical retailers, and such predictions are typically required for future horizons extending tens of weeks. This is especially challenging during high-stake sales events when demand peaks are particularly difficult to predict accurately. However, these events are important not only for managing supply chain operations but also for ensuring a seamless shopping experience for customers. To address this challenge, we propose Temporal-Aligned Transformer (TAT), a multi-horizon forecaster leveraging apriori-known context variables such as holiday and promotion events information for improving predictive performance. Our model consists of an encoder and decoder, both embedded with a novel Temporal Alignment Attention (TAA), designed to learn context-dependent alignment for peak demand forecasting. We conduct extensive empirical analysis on two large-scale proprietary datasets from a large e-commerce retailer. We demonstrate that TAT brings up to 30% accuracy improvement on peak demand forecasting while maintaining competitive overall performance compared to other state-of-the-art methods.</li>
</ul>

<h3>Title: Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter</h3>
<ul>
<li><strong>Authors: </strong>Bo Jiang, Xueyang Ze, Beibei Wang, Xixi Wang, Xixi Wan, Bin Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10355">https://arxiv.org/abs/2507.10355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10355">https://arxiv.org/pdf/2507.10355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10355]] Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter(https://arxiv.org/abs/2507.10355)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Test-Time Canonicalization by Foundation Models for Robust Perception</h3>
<ul>
<li><strong>Authors: </strong>Utkarsh Singhal, Ryan Feng, Stella X. Yu, Atul Prakash</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10375">https://arxiv.org/abs/2507.10375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10375">https://arxiv.org/pdf/2507.10375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10375]] Test-Time Canonicalization by Foundation Models for Robust Perception(https://arxiv.org/abs/2507.10375)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world visual perception requires invariance to diverse transformations, yet current methods rely heavily on specialized architectures or training on predefined augmentations, limiting generalization. We propose FOCAL, a test-time, data-driven framework that achieves robust perception by leveraging internet-scale visual priors from foundation models. By generating and optimizing candidate transformations toward visually typical, "canonical" views, FOCAL enhances robustness without re-training or architectural changes. Our experiments demonstrate improved robustness of CLIP and SAM across challenging transformations, including 2D/3D rotations, illumination shifts (contrast and color), and day-night variations. We also highlight potential applications in active vision. Our approach challenges the assumption that transform-specific training is necessary, instead offering a scalable path to invariance. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Aaryam Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10381">https://arxiv.org/abs/2507.10381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10381">https://arxiv.org/pdf/2507.10381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10381]] Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks(https://arxiv.org/abs/2507.10381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Topological data analysis (TDA) is a relatively new field that is gaining rapid adoption due to its robustness and ability to effectively describe complex datasets by quantifying geometric information. In imaging contexts, TDA typically models data as filtered cubical complexes from which we can extract discriminative features using persistence homology. Meanwhile, convolutional neural networks (CNNs) have been shown to be biased towards texture based local features. To address this limitation, we propose a TDA feature engineering pipeline and a simple method to integrate topological features with deep learning models on remote sensing classification. Our method improves the performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving 99.33% accuracy, which surpasses all previously reported single-model accuracies, including those with larger architectures, such as ResNet50 (2x larger) and XL Vision Transformers (197x larger). We additionally show that our method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45 dataset. To our knowledge, this is the first application of TDA features in satellite scene classification with deep learning. This demonstrates that TDA features can be integrated with deep learning models, even on datasets without explicit topological structures, thereby increasing the applicability of TDA. A clean implementation of our method will be made publicly available upon publication.</li>
</ul>

<h3>Title: Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yue Ding, Conor McCarthy, Kevin O'Shea, Mingming Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10382">https://arxiv.org/abs/2507.10382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10382">https://arxiv.org/pdf/2507.10382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10382]] Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis(https://arxiv.org/abs/2507.10382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the rise of smart mobility and shared e-mobility services, numerous advanced technologies have been applied to this field. Cloud-based traffic simulation solutions have flourished, offering increasingly realistic representations of the evolving mobility landscape. LLMs have emerged as pioneering tools, providing robust support for various applications, including intelligent decision-making, user interaction, and real-time traffic analysis. As user demand for e-mobility continues to grow, delivering comprehensive end-to-end solutions has become crucial. In this paper, we present a cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile application for personalized route recommendations. The optimization module is evaluated based on travel time and cost across different traffic scenarios. Additionally, the LLM-powered RAG framework is evaluated at the schema level for different users, using various evaluation methods. Schema-level RAG with XiYanSQL achieves an average execution accuracy of 0.81 on system operator queries and 0.98 on user queries.</li>
</ul>

<h3>Title: Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Md. Ahsanul Kabir, Mohammad Al Hasan, Aritra Mandal, Liyang Hao, Ishita Khan, Daniel Tunkelang, Zhe Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10385">https://arxiv.org/abs/2507.10385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10385">https://arxiv.org/pdf/2507.10385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10385]] Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model(https://arxiv.org/abs/2507.10385)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The major task of any e-commerce search engine is to retrieve the most relevant inventory items, which best match the user intent reflected in a query. This task is non-trivial due to many reasons, including ambiguous queries, misaligned vocabulary between buyers, and sellers, over- or under-constrained queries by the presence of too many or too few tokens. To address these challenges, query reformulation is used, which modifies a user query through token dropping, replacement or expansion, with the objective to bridge semantic gap between query tokens and users' search intent. Early methods of query reformulation mostly used statistical measures derived from token co-occurrence frequencies from selective user sessions having clicks or purchases. In recent years, supervised deep learning approaches, specifically transformer-based neural language models, or sequence-to-sequence models are being used for query reformulation task. However, these models do not utilize the semantic tags of a query token, which are significant for capturing user intent of an e-commerce query. In this work, we pose query reformulation as a token classification task, and solve this task by designing a dependency-aware transformer-based language model, TagBERT, which makes use of semantic tags of a token for learning superior query phrase embedding. Experiments on large, real-life e-commerce datasets show that TagBERT exhibits superior performance than plethora of competing models, including BERT, eBERT, and Sequence-to-Sequence transformer model for important token classification task.</li>
</ul>

<h3>Title: Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning</h3>
<ul>
<li><strong>Authors: </strong>Ryan Bausback, Jingqiao Tang, Lu Lu, Feng Bao, Toan Huynh</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10401">https://arxiv.org/abs/2507.10401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10401">https://arxiv.org/pdf/2507.10401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10401]] Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning(https://arxiv.org/abs/2507.10401)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We develop a novel framework for uncertainty quantification in operator learning, the Stochastic Operator Network (SON). SON combines the stochastic optimal control concepts of the Stochastic Neural Network (SNN) with the DeepONet. By formulating the branch net as an SDE and backpropagating through the adjoint BSDE, we replace the gradient of the loss function with the gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update. This allows SON to learn the uncertainty present in operators through its diffusion parameters. We then demonstrate the effectiveness of SON when replicating several noisy operators in 2D and 3D.</li>
</ul>

<h3>Title: Numerically Computing Galois Groups of Minimal Problems</h3>
<ul>
<li><strong>Authors: </strong>Timothy Duff</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SC, math.AG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10407">https://arxiv.org/abs/2507.10407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10407">https://arxiv.org/pdf/2507.10407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10407]] Numerically Computing Galois Groups of Minimal Problems(https://arxiv.org/abs/2507.10407)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>I discuss a seemingly unlikely confluence of topics in algebra, numerical computation, and computer vision. The motivating problem is that of solving multiples instances of a parametric family of systems of algebraic (polynomial or rational function) equations. No doubt already of interest to ISSAC attendees, this problem arises in the context of robust model-fitting paradigms currently utilized by the computer vision community (namely "Random Sampling and Consensus", aka "RanSaC".) This talk will give an overview of work in the last 5+ years that aspires to measure the intrinsic difficulty of solving such parametric systems, and makes strides towards practical solutions.</li>
</ul>

<h3>Title: Text-Visual Semantic Constrained AI-Generated Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Qiang Li, Qingsen Yan, Haojian Huang, Peng Wu, Haokui Zhang, Yanning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10432">https://arxiv.org/abs/2507.10432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10432">https://arxiv.org/pdf/2507.10432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10432]] Text-Visual Semantic Constrained AI-Generated Image Quality Assessment(https://arxiv.org/abs/2507.10432)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in Artificial Intelligence Generated Image (AGI) technology, the accurate assessment of their quality has become an increasingly vital requirement. Prevailing methods typically rely on cross-modal models like CLIP or BLIP to evaluate text-image alignment and visual quality. However, when applied to AGIs, these methods encounter two primary challenges: semantic misalignment and details perception missing. To address these limitations, we propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment (SC-AGIQA), a unified framework that leverages text-visual semantic constraints to significantly enhance the comprehensive evaluation of both text-image consistency and perceptual distortion in AI-generated images. Our approach integrates key capabilities from multiple models and tackles the aforementioned challenges by introducing two core modules: the Text-assisted Semantic Alignment Module (TSAM), which leverages Multimodal Large Language Models (MLLMs) to bridge the semantic gap by generating an image description and comparing it against the original prompt for a refined consistency check, and the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which draws inspiration from Human Visual System (HVS) properties by employing frequency domain analysis combined with perceptual sensitivity weighting to better quantify subtle visual distortions and enhance the capture of fine-grained visual quality details in images. Extensive experiments conducted on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing state-of-the-art methods. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: From Sequence to Structure: Uncovering Substructure Reasoning in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Xinnan Dai, Kai Yang, Jay Revolinsky, Kai Guo, Aoran Wang, Bohang Zhang, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10435">https://arxiv.org/abs/2507.10435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10435">https://arxiv.org/pdf/2507.10435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10435]] From Sequence to Structure: Uncovering Substructure Reasoning in Transformers(https://arxiv.org/abs/2507.10435)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies suggest that large language models (LLMs) possess the capability to solve graph reasoning tasks. Notably, even when graph structures are embedded within textual descriptions, LLMs can still effectively answer related questions. This raises a fundamental question: How can a decoder-only Transformer architecture understand underlying graph structures? To address this, we start with the substructure extraction task, interpreting the inner mechanisms inside the transformers and analyzing the impact of the input queries. Specifically, through both empirical results and theoretical analysis, we present Induced Substructure Filtration (ISF), a perspective that captures the substructure identification in the multi-layer transformers. We further validate the ISF process in LLMs, revealing consistent internal dynamics across layers. Building on these insights, we explore the broader capabilities of Transformers in handling diverse graph types. Specifically, we introduce the concept of thinking in substructures to efficiently extract complex composite patterns, and demonstrate that decoder-only Transformers can successfully extract substructures from attributed graphs, such as molecular graphs. Together, our findings offer a new insight on how sequence-based Transformers perform the substructure extraction task over graph data.</li>
</ul>

<h3>Title: Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Shivam Chandhok, Wan-Cyuan Fan, Vered Shwartz, Vineeth N Balasubramanian, Leonid Sigal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10442">https://arxiv.org/abs/2507.10442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10442">https://arxiv.org/pdf/2507.10442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10442]] Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities(https://arxiv.org/abs/2507.10442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language Models (VLMs) have emerged as general-purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks by constructing a series of tests that probe which components of design, specifically, may be lacking. Importantly, we go significantly beyond the current benchmarks, which simply measure the final performance of VLM response, by also comparing and contrasting it to the performance of probes trained directly on features obtained from the visual encoder, intermediate vision-language projection and LLM-decoder output. In doing so, we uncover shortcomings in VLMs and make a number of important observations about their capabilities, robustness and how they process visual information. We hope our insights will guide progress in further improving VLMs.</li>
</ul>

<h3>Title: Some remarks on gradient dominance and LQR policy optimization</h3>
<ul>
<li><strong>Authors: </strong>Eduardo D. Sontag</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10452">https://arxiv.org/abs/2507.10452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10452">https://arxiv.org/pdf/2507.10452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10452]] Some remarks on gradient dominance and LQR policy optimization(https://arxiv.org/abs/2507.10452)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Solutions of optimization problems, including policy optimization in reinforcement learning, typically rely upon some variant of gradient descent. There has been much recent work in the machine learning, control, and optimization communities applying the Polyak-≈Åojasiewicz Inequality (PLI) to such problems in order to establish an exponential rate of convergence (a.k.a. ``linear convergence'' in the local-iteration language of numerical analysis) of loss functions to their minima under the gradient flow. Often, as is the case of policy iteration for the continuous-time LQR problem, this rate vanishes for large initial conditions, resulting in a mixed globally linear / locally exponential behavior. This is in sharp contrast with the discrete-time LQR problem, where there is global exponential convergence. That gap between CT and DT behaviors motivates the search for various generalized PLI-like conditions, and this talk will address that topic. Moreover, these generalizations are key to understanding the transient and asymptotic effects of errors in the estimation of the gradient, errors which might arise from adversarial attacks, wrong evaluation by an oracle, early stopping of a simulation, inaccurate and very approximate digital twins, stochastic computations (algorithm ``reproducibility''), or learning by sampling from limited data. We describe an ``input to state stability'' (ISS) analysis of this issue. The lecture also discussed convergence and PLI-like properties of ``linear feedforward neural networks'' in feedback control, but this arXiv skips that part (to be updated). Much of the work described here was done in collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping Jiang, and Milad Siami.</li>
</ul>

<h3>Title: Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems</h3>
<ul>
<li><strong>Authors: </strong>Hammad Atta, Ken Huang, Manish Bhatt, Kamal Ahmed, Muhammad Aziz Ul Haq, Yasir Mehmood</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10457">https://arxiv.org/abs/2507.10457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10457">https://arxiv.org/pdf/2507.10457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10457]] Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems(https://arxiv.org/abs/2507.10457)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The integration of large language models (LLMs) into enterprise systems has created a new class of covert security vulnerabilities, particularly within logic-execution layers and persistent-memory contexts. In this paper, we introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category in which encoded, delayed, and conditionally triggered payloads are embedded in memory, vector stores, or tool outputs. These payloads can bypass conventional input filters and trigger unauthorised behaviour across sessions.</li>
</ul>

<h3>Title: RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening</h3>
<ul>
<li><strong>Authors: </strong>Tao Tang, Chengxu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10461">https://arxiv.org/abs/2507.10461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10461">https://arxiv.org/pdf/2507.10461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10461]] RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening(https://arxiv.org/abs/2507.10461)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.</li>
</ul>

<h3>Title: From BERT to Qwen: Hate Detection across architectures</h3>
<ul>
<li><strong>Authors: </strong>Ariadna Mon, Sa√∫l Fenollosa, Jon Lecumberri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10468">https://arxiv.org/abs/2507.10468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10468">https://arxiv.org/pdf/2507.10468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10468]] From BERT to Qwen: Hate Detection across architectures(https://arxiv.org/abs/2507.10468)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Online platforms struggle to curb hate speech without over-censoring legitimate discourse. Early bidirectional transformer encoders made big strides, but the arrival of ultra-large autoregressive LLMs promises deeper context-awareness. Whether this extra scale actually improves practical hate-speech detection on real-world text remains unverified. Our study puts this question to the test by benchmarking both model families, classic encoders and next-generation LLMs, on curated corpora of online interactions for hate-speech detection (Hate or No Hate).</li>
</ul>

<h3>Title: RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zhicun Yin, Junjie Chen, Ming Liu, Zhixin Wang, Fan Li, Renjing Pei, Xiaoming Li, Rynson W.H. Lau, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10470">https://arxiv.org/abs/2507.10470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10470">https://arxiv.org/pdf/2507.10470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10470]] RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction(https://arxiv.org/abs/2507.10470)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at this https URL.</li>
</ul>

<h3>Title: MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking</h3>
<ul>
<li><strong>Authors: </strong>Mohamed T. Younes, Omar Walid, Mai Hassan, Ali Hamdi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10472">https://arxiv.org/abs/2507.10472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10472">https://arxiv.org/pdf/2507.10472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10472]] MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking(https://arxiv.org/abs/2507.10472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative Applicant Tracking System (ATS) enhanced by a novel Robotic process automation (RPA) framework or as further referred to as MLAR. Traditional recruitment processes often encounter bottlenecks in resume screening and candidate shortlisting due to time and resource constraints. MLAR addresses these challenges employing Large Language Models (LLMs) in three distinct layers: extracting key characteristics from job postings in the first layer, parsing applicant resume to identify education, experience, skills in the second layer, and similarity matching in the third layer. These features are then matched through advanced semantic algorithms to identify the best candidates efficiently. Our approach integrates seamlessly into existing RPA pipelines, automating resume parsing, job matching, and candidate notifications. Extensive performance benchmarking shows that MLAR outperforms the leading RPA platforms, including UiPath and Automation Anywhere, in high-volume resume-processing tasks. When processing 2,400 resumes, MLAR achieved an average processing time of 5.4 seconds per resume, reducing processing time by approximately 16.9% compared to Automation Anywhere and 17.1% compared to UiPath. These results highlight the potential of MLAR to transform recruitment workflows by providing an efficient, accurate, and scalable solution tailored to modern hiring needs.</li>
</ul>

<h3>Title: Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation</h3>
<ul>
<li><strong>Authors: </strong>Seyed Alireza Rahimi Azghadi, Truong-Thanh-Hung Nguyen, Helene Fournier, Monica Wachowicz, Rene Richard, Francis Palma, Hung Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10474">https://arxiv.org/abs/2507.10474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10474">https://arxiv.org/pdf/2507.10474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10474]] Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation(https://arxiv.org/abs/2507.10474)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>The aging population is growing rapidly, and so is the danger of falls in older adults. A major cause of injury is falling, and detection in time can greatly save medical expenses and recovery time. However, to provide timely intervention and avoid unnecessary alarms, detection systems must be effective and reliable while addressing privacy concerns regarding the user. In this work, we propose a framework for detecting falls using several complementary systems: a semi-supervised federated learning-based fall detection system (SF2D), an indoor localization and navigation system, and a vision-based human fall recognition system. A wearable device and an edge device identify a fall scenario in the first system. On top of that, the second system uses an indoor localization technique first to localize the fall location and then navigate a robot to inspect the scenario. A vision-based detection system running on an edge device with a mounted camera on a robot is used to recognize fallen people. Each of the systems of this proposed framework achieves different accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to 99.19% accuracy, while the vision-based fallen people detection achieves 96.3% accuracy. However, when we combine the accuracy of these two systems with the accuracy of the navigation system (95% success rate), our proposed framework creates a highly reliable performance for fall detection, with an overall accuracy of 99.99%. Not only is the proposed framework safe for older adults, but it is also a privacy-preserving solution for detecting falls.</li>
</ul>

<h3>Title: Can You Detect the Difference?</h3>
<ul>
<li><strong>Authors: </strong>ƒ∞smail Tarƒ±m, Aytuƒü Onan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10475">https://arxiv.org/abs/2507.10475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10475">https://arxiv.org/pdf/2507.10475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10475]] Can You Detect the Difference?(https://arxiv.org/abs/2507.10475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.</li>
</ul>

<h3>Title: The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization</h3>
<ul>
<li><strong>Authors: </strong>Paul Fogel (1), Christophe Geissler (1), George Luta (2) ((1) Data Services, ForvisMazars, Courbevoie, France, (2) Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown University Medical Center, Washington, DC, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10484">https://arxiv.org/abs/2507.10484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10484">https://arxiv.org/pdf/2507.10484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10484]] The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization(https://arxiv.org/abs/2507.10484)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces the "Target Polish," a robust and computationally efficient framework for nonnegative matrix and tensor factorization. Although conventional weighted NMF approaches are resistant to outliers, they converge slowly due to the use of multiplicative updates to minimize the objective criterion. In contrast, the Target Polish approach remains compatible with the Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing the data with a weighted median-based transformation. This innovation provides outlier resistance while maintaining the highly efficient additive update structure of Fast-HALS. Empirical evaluations using image datasets corrupted with structured (block) and unstructured (salt) noise demonstrate that the Target Polish approach matches or exceeds the accuracy of state-of-the-art robust NMF methods and reduces computational time by an order of magnitude in the studied scenarios.</li>
</ul>

<h3>Title: SynthGuard: Redefining Synthetic Data Generation with a Scalable and Privacy-Preserving Workflow Framework</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Brito, Mahmoud Shoush, Kristian Tamm, Paula Etti, Liina Kamm</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10489">https://arxiv.org/abs/2507.10489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10489">https://arxiv.org/pdf/2507.10489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10489]] SynthGuard: Redefining Synthetic Data Generation with a Scalable and Privacy-Preserving Workflow Framework(https://arxiv.org/abs/2507.10489)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>The growing reliance on data-driven applications in sectors such as healthcare, finance, and law enforcement underscores the need for secure, privacy-preserving, and scalable mechanisms for data generation and sharing. Synthetic data generation (SDG) has emerged as a promising approach but often relies on centralized or external processing, raising concerns about data sovereignty, domain ownership, and compliance with evolving regulatory standards. To overcome these issues, we introduce SynthGuard, a framework designed to ensure computational governance by enabling data owners to maintain control over SDG workflows. SynthGuard supports modular and privacy-preserving workflows, ensuring secure, auditable, and reproducible execution across diverse environments. In this paper, we demonstrate how SynthGuard addresses the complexities at the intersection of domain-specific needs and scalable SDG by aligning with requirements for data sovereignty and regulatory compliance. Developed iteratively with domain expert input, SynthGuard has been validated through real-world use cases, demonstrating its ability to balance security, privacy, and scalability while ensuring compliance. The evaluation confirms its effectiveness in implementing and executing SDG workflows and integrating privacy and utility assessments across various computational environments.</li>
</ul>

<h3>Title: The Power of Certainty: How Confident Models Lead to Better Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tugberk Erol, Tuba Caglikantar, Duygu Sarikaya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10490">https://arxiv.org/abs/2507.10490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10490">https://arxiv.org/pdf/2507.10490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10490]] The Power of Certainty: How Confident Models Lead to Better Segmentation(https://arxiv.org/abs/2507.10490)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning models have been proposed for automatic polyp detection and precise segmentation of polyps during colonoscopy procedures. Although these state-of-the-art models achieve high performance, they often require a large number of parameters. Their complexity can make them prone to overfitting, particularly when trained on biased datasets, and can result in poor generalization across diverse datasets. Knowledge distillation and self-distillation are proposed as promising strategies to mitigate the limitations of large, over-parameterized models. These approaches, however, are resource-intensive, often requiring multiple models and significant memory during training. We propose a confidence-based self-distillation approach that outperforms state-of-the-art models by utilizing only previous iteration data storage during training, without requiring extra computation or memory usage during testing. Our approach calculates the loss between the previous and current iterations within a batch using a dynamic confidence coefficient. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on the task of polyp segmentation. Our approach outperforms state-of-the-art models and generalizes well across datasets collected from multiple clinical centers. The code will be released to the public once the paper is accepted.</li>
</ul>

<h3>Title: BURN: Backdoor Unlearning via Adversarial Boundary Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yanghao Su, Jie Zhang, Yiming Li, Tianwei Zhang, Qing Guo, Weiming Zhang, Nenghai Yu, Nils Lukas, Wenbo Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10491">https://arxiv.org/abs/2507.10491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10491">https://arxiv.org/pdf/2507.10491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10491]] BURN: Backdoor Unlearning via Adversarial Boundary Analysis(https://arxiv.org/abs/2507.10491)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Backdoor unlearning aims to remove backdoor-related information while preserving the model's original functionality. However, existing unlearning methods mainly focus on recovering trigger patterns but fail to restore the correct semantic labels of poison samples. This limitation prevents them from fully eliminating the false correlation between the trigger pattern and the target label. To address this, we leverage boundary adversarial attack techniques, revealing two key observations. First, poison samples exhibit significantly greater distances from decision boundaries compared to clean samples, indicating they require larger adversarial perturbations to change their predictions. Second, while adversarial predicted labels for clean samples are uniformly distributed, those for poison samples tend to revert to their original correct labels. Moreover, the features of poison samples restore to closely resemble those of corresponding clean samples after adding adversarial perturbations. Building upon these insights, we propose Backdoor Unlearning via adversaRial bouNdary analysis (BURN), a novel defense framework that integrates false correlation decoupling, progressive data refinement, and model purification. In the first phase, BURN employs adversarial boundary analysis to detect poisoned samples based on their abnormal adversarial boundary distances, then restores their correct semantic labels for fine-tuning. In the second phase, it employs a feedback mechanism that tracks prediction discrepancies between the original backdoored model and progressively sanitized models, guiding both dataset refinement and model purification. Extensive evaluations across multiple datasets, architectures, and seven diverse backdoor attack types confirm that BURN effectively removes backdoor threats while maintaining the model's original performance.</li>
</ul>

<h3>Title: BenchReAD: A systematic benchmark for retinal anomaly detection</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Lian, Hong-Yu Zhou, Zhanli Hu, Jing Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10492">https://arxiv.org/abs/2507.10492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10492">https://arxiv.org/pdf/2507.10492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10492]] BenchReAD: A systematic benchmark for retinal anomaly detection(https://arxiv.org/abs/2507.10492)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, establishing a new SOTA. The benchmark is publicly available at this https URL.</li>
</ul>

<h3>Title: Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing</h3>
<ul>
<li><strong>Authors: </strong>Tanveer Khan, Mindaugas Budzys, Antonis Michalas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10494">https://arxiv.org/abs/2507.10494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10494">https://arxiv.org/pdf/2507.10494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10494]] Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing(https://arxiv.org/abs/2507.10494)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Split Learning (SL) -- splits a model into two distinct parts to help protect client data while enhancing Machine Learning (ML) processes. Though promising, SL has proven vulnerable to different attacks, thus raising concerns about how effective it may be in terms of data privacy. Recent works have shown promising results for securing SL through the use of a novel paradigm, named Function Secret Sharing (FSS), in which servers obtain shares of a function they compute and operate on a public input hidden with a random mask. However, these works fall short in addressing the rising number of attacks which exist on SL. In SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly to other works, we are able to make use of the benefits of SL by reducing the communication and computational costs of FSS. However, a U-shaped SL provides a higher security guarantee than previous works, allowing a client to keep the labels of the training data secret, without having to share them with the server. Through this, we are able to generalize the security analysis of previous works and expand it to different attack vectors, such as modern model inversion attacks as well as label inference attacks. We tested our approach for two different convolutional neural networks on different datasets. These experiments show the effectiveness of our approach in reducing the training time as well as the communication costs when compared to simply using FSS while matching prior accuracy.</li>
</ul>

<h3>Title: Cameras as Relative Positional Encoding</h3>
<ul>
<li><strong>Authors: </strong>Ruilong Li, Brent Yi, Junchen Liu, Hang Gao, Yi Ma, Angjoo Kanazawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10496">https://arxiv.org/abs/2507.10496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10496">https://arxiv.org/pdf/2507.10496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10496]] Cameras as Relative Positional Encoding(https://arxiv.org/abs/2507.10496)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers are increasingly prevalent for multi-view computer vision tasks, where geometric relationships between viewpoints are critical for 3D perception. To leverage these relationships, multi-view transformers must use camera geometry to ground visual tokens in 3D space. In this work, we compare techniques for conditioning transformers on cameras: token-level raymap encodings, attention-level relative pose encodings, and a new relative encoding we propose -- Projective Positional Encoding (PRoPE) -- that captures complete camera frustums, both intrinsics and extrinsics, as a relative positional encoding. Our experiments begin by showing how relative camera conditioning improves performance in feedforward novel view synthesis, with further gains from PRoPE. This holds across settings: scenes with both shared and varying intrinsics, when combining token- and attention-level conditioning, and for generalization to inputs with out-of-distribution sequence lengths and camera intrinsics. We then verify that these benefits persist for different tasks, stereo depth estimation and discriminative spatial cognition, as well as larger model sizes.</li>
</ul>

<h3>Title: Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop</h3>
<ul>
<li><strong>Authors: </strong>Elizabeth Fahsbender, Alma Andersson, Jeremy Ash, Polina Binder, Daniel Burkhardt, Benjamin Chang, Georg K. Gerber, Anthony Gitter, Patrick Godau, Ankit Gupta, Genevieve Haliburton, Siyu He, Trey Ideker, Ivana Jelic, Aly Khan, Yang-Joon Kim, Aditi Krishnapriyan, Jon M. Laurent, Tianyu Liu 28, Emma Lundberg, Shalin B. Mehta, Rob Moccia, Angela Oliveira Pisco, Katherine S. Pollard, Suresh Ramani, Julio Saez-Rodriguez, Yasin Senbabaoglu, Elana Simon, Srinivasan Sivanandan, Gustavo Stolovitzky, Marc Valer, Bo Wang, Xikun Zhang, James Zou, Katrina Kalantar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10502">https://arxiv.org/abs/2507.10502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10502">https://arxiv.org/pdf/2507.10502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10502]] Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop(https://arxiv.org/abs/2507.10502)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial intelligence holds immense promise for transforming biology, yet a lack of standardized, cross domain, benchmarks undermines our ability to build robust, trustworthy models. Here, we present insights from a recent workshop that convened machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics to tackle this gap. We identify major technical and systemic bottlenecks such as data heterogeneity and noise, reproducibility challenges, biases, and the fragmented ecosystem of publicly available resources and propose a set of recommendations for building benchmarking frameworks that can efficiently compare ML models of biological systems across tasks and data modalities. By promoting high quality data curation, standardized tooling, comprehensive evaluation metrics, and open, collaborative platforms, we aim to accelerate the development of robust benchmarks for AI driven Virtual Cells. These benchmarks are crucial for ensuring rigor, reproducibility, and biological relevance, and will ultimately advance the field toward integrated models that drive new discoveries, therapeutic insights, and a deeper understanding of cellular systems.</li>
</ul>

<h3>Title: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</h3>
<ul>
<li><strong>Authors: </strong>Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10524">https://arxiv.org/abs/2507.10524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10524">https://arxiv.org/pdf/2507.10524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10524]] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation(https://arxiv.org/abs/2507.10524)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.</li>
</ul>

<h3>Title: Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</h3>
<ul>
<li><strong>Authors: </strong>Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10532">https://arxiv.org/abs/2507.10532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10532">https://arxiv.org/pdf/2507.10532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10532]] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination(https://arxiv.org/abs/2507.10532)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions.</li>
</ul>

<h3>Title: CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks</h3>
<ul>
<li><strong>Authors: </strong>Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10535">https://arxiv.org/abs/2507.10535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10535">https://arxiv.org/pdf/2507.10535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10535]] CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks(https://arxiv.org/abs/2507.10535)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.</li>
</ul>

<h3>Title: On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance</h3>
<ul>
<li><strong>Authors: </strong>Qiaoyue Tang, Alain Zhiyanov, Mathias L√©cuyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10536">https://arxiv.org/abs/2507.10536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10536">https://arxiv.org/pdf/2507.10536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10536]] On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance(https://arxiv.org/abs/2507.10536)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In this work, we analyze the optimization behaviour of common private learning optimization algorithms under heavy-tail class imbalanced distribution. We show that, in a stylized model, optimizing with Gradient Descent with differential privacy (DP-GD) suffers when learning low-frequency classes, whereas optimization algorithms that estimate second-order information do not. In particular, DP-AdamBC that removes the DP bias from estimating loss curvature is a crucial component to avoid the ill-condition caused by heavy-tail class imbalance, and empirically fits the data better with $\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the least frequent classes on both controlled experiments and real data respectively.</li>
</ul>

<h3>Title: Fusing LLM Capabilities with Routing Data</h3>
<ul>
<li><strong>Authors: </strong>Tao Feng, Haozhen Zhang, Zijie Lei, Pengrui Han, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10540">https://arxiv.org/abs/2507.10540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10540">https://arxiv.org/pdf/2507.10540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10540]] Fusing LLM Capabilities with Routing Data(https://arxiv.org/abs/2507.10540)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has created a vibrant ecosystem of diverse architectures, each with unique strengths due to differences in design, training data, and objectives. However, most applications still rely on a single backend model, limiting coverage of capabilities and leading to inefficiencies in performance and token cost when tackling complex tasks. We highlight an underexploited opportunity: LLM routing data, produced when hosting platforms route diverse queries to different models, which can reveal comparative strengths across tasks. To address this, we propose FusionBench, a comprehensive routing benchmark covering 14 tasks across five domains with 20 open-source LLMs (8B to 671B parameters), capturing 103M tokens and summarizing reusable thought templates from top models. Building on this, we introduce FusionFactory, a systematic fusion framework with three levels: (1) query-level fusion, tailoring routers for each query using both direct responses and reasoning-augmented outputs; (2) thought-level fusion, leveraging abstract templates derived from top-performing LLMs' answers to similar queries; and (3) model-level fusion, transferring capabilities between models via distillation, using top responses or highest judge scores as training data. Experiments show FusionFactory consistently outperforms the best individual LLM across all 14 benchmarks, with optimal fusion configurations varying by benchmark, demonstrating the value of systematic LLM fusion in harnessing complementary strengths and improving overall performance.</li>
</ul>

<h3>Title: Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Iashin, Horace Lee, Dan Schofield, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.10552">https://arxiv.org/abs/2507.10552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.10552">https://arxiv.org/pdf/2507.10552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.10552]] Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder(https://arxiv.org/abs/2507.10552)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however, the manual identification of individual animals remains a significant bottleneck. This study introduces a fully self-supervised approach to learning robust chimpanzee face embeddings from unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision Transformers on automatically mined face crops, eliminating the need for identity labels. Our method demonstrates strong open-set re-identification performance, surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled data during training. This work underscores the potential of self-supervised learning in biodiversity monitoring and paves the way for scalable, non-invasive population studies.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
