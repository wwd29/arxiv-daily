<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2023-12-18</h1>
<h2>secure</h2>
<h3>Title: TIFace: Improving Facial Reconstruction through Tensorial Radiance Fields and Implicit Surfaces. (arXiv:2312.09527v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09527">http://arxiv.org/abs/2312.09527</a></li>
<li>Code URL: <a href="https://github.com/ruijiezhu94/ti-face">https://github.com/ruijiezhu94/ti-face</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09527]] TIFace: Improving Facial Reconstruction through Tensorial Radiance Fields and Implicit Surfaces(http://arxiv.org/abs/2312.09527)</code></li>
<li>Summary: <p>This report describes the solution that secured the first place in the "View
Synthesis Challenge for Human Heads (VSCHH)" at the ICCV 2023 workshop. Given
the sparse view images of human heads, the objective of this challenge is to
synthesize images from novel viewpoints. Due to the complexity of textures on
the face and the impact of lighting, the baseline method TensoRF yields results
with significant artifacts, seriously affecting facial reconstruction. To
address this issue, we propose TI-Face, which improves facial reconstruction
through tensorial radiance fields (T-Face) and implicit surfaces (I-Face),
respectively. Specifically, we employ an SAM-based approach to obtain the
foreground mask, thereby filtering out intense lighting in the background.
Additionally, we design mask-based constraints and sparsity constraints to
eliminate rendering artifacts effectively. The experimental results demonstrate
the effectiveness of the proposed improvements and superior performance of our
method on face reconstruction. The code will be available at
https://github.com/RuijieZhu94/TI-Face.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: SlowTrack: Increasing the Latency of Camera-based Perception in Autonomous Driving Using Adversarial Examples. (arXiv:2312.09520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09520">http://arxiv.org/abs/2312.09520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09520]] SlowTrack: Increasing the Latency of Camera-based Perception in Autonomous Driving Using Adversarial Examples(http://arxiv.org/abs/2312.09520)</code></li>
<li>Summary: <p>In Autonomous Driving (AD), real-time perception is a critical component
responsible for detecting surrounding objects to ensure safe driving. While
researchers have extensively explored the integrity of AD perception due to its
safety and security implications, the aspect of availability (real-time
performance) or latency has received limited attention. Existing works on
latency-based attack have focused mainly on object detection, i.e., a component
in camera-based AD perception, overlooking the entire camera-based AD
perception, which hinders them to achieve effective system-level effects, such
as vehicle crashes. In this paper, we propose SlowTrack, a novel framework for
generating adversarial attacks to increase the execution time of camera-based
AD perception. We propose a novel two-stage attack strategy along with the
three new loss function designs. Our evaluation is conducted on four popular
camera-based AD perception pipelines, and the results demonstrate that
SlowTrack significantly outperforms existing latency-based attacks while
maintaining comparable imperceptibility levels. Furthermore, we perform the
evaluation on Baidu Apollo, an industry-grade full-stack AD system, and LGSVL,
a production-grade AD simulator, with two scenarios to compare the system-level
effects of SlowTrack and existing attacks. Our evaluation results show that the
system-level effects can be significantly improved, i.e., the vehicle crash
rate of SlowTrack is around 95% on average while existing works only have
around 30%.
</p></li>
</ul>

<h3>Title: LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer. (arXiv:2312.09935v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09935">http://arxiv.org/abs/2312.09935</a></li>
<li>Code URL: <a href="https://github.com/ziyuzhao-zzy/logostylefool">https://github.com/ziyuzhao-zzy/logostylefool</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09935]] LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer(http://arxiv.org/abs/2312.09935)</code></li>
<li>Summary: <p>Video recognition systems are vulnerable to adversarial examples. Recent
studies show that style transfer-based and patch-based unrestricted
perturbations can effectively improve attack efficiency. These attacks,
however, face two main challenges: 1) Adding large stylized perturbations to
all pixels reduces the naturalness of the video and such perturbations can be
easily detected. 2) Patch-based video attacks are not extensible to targeted
attacks due to the limited search space of reinforcement learning that has been
widely used in video attacks recently. In this paper, we focus on the video
black-box setting and propose a novel attack framework named LogoStyleFool by
adding a stylized logo to the clean video. We separate the attack into three
stages: style reference selection, reinforcement-learning-based logo style
transfer, and perturbation optimization. We solve the first challenge by
scaling down the perturbation range to a regional logo, while the second
challenge is addressed by complementing an optimization stage after
reinforcement learning. Experimental results substantiate the overall
superiority of LogoStyleFool over three state-of-the-art patch-based attacks in
terms of attack performance and semantic preservation. Meanwhile, LogoStyleFool
still maintains its performance against two existing patch-based defense
methods. We believe that our research is beneficial in increasing the attention
of the security community to such subregional style transfer attacks.
</p></li>
</ul>

<h3>Title: DECLASSIFLOW: A Static Analysis for Modeling Non-Speculative Knowledge to Relax Speculative Execution Security Measures (Full Version). (arXiv:2312.09336v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09336">http://arxiv.org/abs/2312.09336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09336]] DECLASSIFLOW: A Static Analysis for Modeling Non-Speculative Knowledge to Relax Speculative Execution Security Measures (Full Version)(http://arxiv.org/abs/2312.09336)</code></li>
<li>Summary: <p>Speculative execution attacks undermine the security of constant-time
programming, the standard technique used to prevent microarchitectural side
channels in security-sensitive software such as cryptographic code.
Constant-time code must therefore also deploy a defense against speculative
execution attacks to prevent leakage of secret data stored in memory or the
processor registers. Unfortunately, contemporary defenses, such as speculative
load hardening (SLH), can only satisfy this strong security guarantee at a very
high performance cost.
</p>
<p>This paper proposes DECLASSIFLOW, a static program analysis and protection
framework to efficiently protect constant-time code from speculative leakage.
DECLASSIFLOW models "attacker knowledge" -- data which is inherently
transmitted (or, implicitly declassified) by the code's non-speculative
execution -- and statically removes protection on such data from points in the
program where it is already guaranteed to leak non-speculatively. Overall,
DECLASSIFLOW ensures that data which never leaks during the non-speculative
execution does not leak during speculative execution, but with lower overhead
than conservative protections like SLH.
</p></li>
</ul>

<h3>Title: Security layers and related services within the Horizon Europe NEUROPULS project. (arXiv:2312.09383v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09383">http://arxiv.org/abs/2312.09383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09383]] Security layers and related services within the Horizon Europe NEUROPULS project(http://arxiv.org/abs/2312.09383)</code></li>
<li>Summary: <p>In the contemporary security landscape, the incorporation of photonics has
emerged as a transformative force, unlocking a spectrum of possibilities to
enhance the resilience and effectiveness of security primitives. This
integration represents more than a mere technological augmentation; it
signifies a paradigm shift towards innovative approaches capable of delivering
security primitives with key properties for low-power systems. This not only
augments the robustness of security frameworks, but also paves the way for
novel strategies that adapt to the evolving challenges of the digital age. This
paper discusses the security layers and related services that will be
developed, modeled, and evaluated within the Horizon Europe NEUROPULS project.
These layers will exploit novel implementations for security primitives based
on physical unclonable functions (PUFs) using integrated photonics technology.
Their objective is to provide a series of services to support the secure
operation of a neuromorphic photonic accelerator for edge computing
applications.
</p></li>
</ul>

<h3>Title: VDOO: A Short, Fast, Post-Quantum Multivariate Digital Signature Scheme. (arXiv:2312.09535v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09535">http://arxiv.org/abs/2312.09535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09535]] VDOO: A Short, Fast, Post-Quantum Multivariate Digital Signature Scheme(http://arxiv.org/abs/2312.09535)</code></li>
<li>Summary: <p>Hard lattice problems are predominant in constructing post-quantum
cryptosystems. However, we need to continue developing post-quantum
cryptosystems based on other quantum hard problems to prevent a complete
collapse of post-quantum cryptography due to a sudden breakthrough in solving
hard lattice problems. Solving large multivariate quadratic systems is one such
quantum hard problem.
</p>
<p>Unbalanced Oil-Vinegar is a signature scheme based on the hardness of solving
multivariate equations. In this work, we present a post-quantum digital
signature algorithm VDOO (Vinegar-Diagonal-Oil-Oil) based on solving
multivariate equations. We introduce a new layer called the diagonal layer over
the oil-vinegar-based signature scheme Rainbow. This layer helps to improve the
security of our scheme without increasing the parameters considerably. Due to
this modification, the complexity of the main computational bottleneck of
multivariate quadratic systems i.e. the Gaussian elimination reduces
significantly. Thus making our scheme one of the fastest multivariate quadratic
signature schemes. Further, we show that our carefully chosen parameters can
resist all existing state-of-the-art attacks. The signature sizes of our scheme
for the National Institute of Standards and Technology's security level of I,
III, and V are 96, 226, and 316 bytes, respectively. This is the smallest
signature size among all known post-quantum signature schemes of similar
security.
</p></li>
</ul>

<h3>Title: Madtls: Fine-grained Middlebox-aware End-to-end Security for Industrial Communication. (arXiv:2312.09650v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09650">http://arxiv.org/abs/2312.09650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09650]] Madtls: Fine-grained Middlebox-aware End-to-end Security for Industrial Communication(http://arxiv.org/abs/2312.09650)</code></li>
<li>Summary: <p>Industrial control systems increasingly rely on middlebox functionality such
as intrusion detection or in-network processing. However, traditional
end-to-end security protocols interfere with the necessary access to in-flight
data. While recent work on middlebox-aware end-to-end security protocols for
the traditional Internet promises to address the dilemma between end-to-end
security guarantees and middleboxes, the current state-of-the-art lacks
critical features for industrial communication. Most importantly, industrial
settings require fine-grained access control for middleboxes to truly operate
in a least-privilege mode. Likewise, advanced applications even require that
middleboxes can inject specific messages (e.g., emergency shutdowns).
Meanwhile, industrial scenarios often expose tight latency and bandwidth
constraints not found in the traditional Internet. As the current
state-of-the-art misses critical features, we propose Middlebox-aware DTLS
(Madtls), a middlebox-aware end-to-end security protocol specifically tailored
to the needs of industrial networks. Madtls provides bit-level read and write
access control of middleboxes to communicated data with minimal bandwidth and
processing overhead, even on constrained hardware.
</p></li>
</ul>

<h3>Title: ESAT:Extended Security in the Air using TESLA. (arXiv:2312.09870v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09870">http://arxiv.org/abs/2312.09870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09870]] ESAT:Extended Security in the Air using TESLA(http://arxiv.org/abs/2312.09870)</code></li>
<li>Summary: <p>The Automatic Dependent Surveillance-Broadcast (ADS-B) is a surveillance
technology that becomes mandatory in many airspaces. It improves safety,
increases efficiency and reduces air traffic congestion by broadcasting
aircraft navigation data. Yet, ADS-B is vulnerable to spoofing attacks as it
lacks mechanisms to ensure the integrity and authenticity of the data being
supplied. None of the existing cryptographic solutions fully meet the backward
compatibility and bandwidth preservation requirements of the standard. Hence,
we propose Extended Security in the Air using TESLA (ESAT), an enhanced
approach that integrates TESLA, phase-overlay modulation techniques and
certificate-based PKI. As a result, entity authentication, data origin
authentication, and data integrity are the security services that ESAT offers.
To assess compliance with the standard, we designed an SDR-based implementation
of ESAT and performed backwards compatibility tests on commercial and general
aviation (GA) ADS-B in receivers. Besides, we calculated the 1090ES band's
activity factor and analyzed the channel occupancy rate according to ITU-R
SM.2256-1 recommendation. Also, we performed a bit error rate analysis of ESAT
messages. The results suggest that ESAT is backward compatible, does not incur
significant communication overhead, and has an error rate that is acceptable
for Eb/No values above 14 dB.
</p></li>
</ul>

<h2>privacy</h2>
<h2>protect</h2>
<h3>Title: When and How to Aggregate Message Authentication Codes on Lossy Channels?. (arXiv:2312.09660v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09660">http://arxiv.org/abs/2312.09660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09660]] When and How to Aggregate Message Authentication Codes on Lossy Channels?(http://arxiv.org/abs/2312.09660)</code></li>
<li>Summary: <p>Aggregation of message authentication codes (MACs) is a proven and efficient
method to preserve valuable bandwidth in resource-constrained environments:
Instead of appending a long authentication tag to each message, the integrity
protection of multiple messages is aggregated into a single tag. However, while
such aggregation saves bandwidth, a single lost message typically means that
authentication information for multiple messages cannot be verified anymore.
With the significant increase of bandwidth-constrained lossy communication, as
applications shift towards wireless channels, it thus becomes paramount to
study the impact of packet loss on the diverse MAC aggregation schemes proposed
over the past 15 years to assess when and how to aggregate message
authentication. Therefore, we empirically study all relevant MAC aggregation
schemes in the context of lossy channels, investigating achievable goodput
improvements, the resulting verification delays, processing overhead, and
resilience to denial-of-service attacks. Our analysis shows the importance of
carefully choosing and configuring MAC aggregation, as selecting and correctly
parameterizing the right scheme can, e.g., improve goodput by 39% to 444%,
depending on the scenario. However, since no aggregation scheme performs best
in all scenarios, we provide guidelines for network operators to select optimal
schemes and parameterizations suiting specific network settings.
</p></li>
</ul>

<h3>Title: Silent Guardian: Protecting Text from Malicious Exploitation by Large Language Models. (arXiv:2312.09669v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09669">http://arxiv.org/abs/2312.09669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09669]] Silent Guardian: Protecting Text from Malicious Exploitation by Large Language Models(http://arxiv.org/abs/2312.09669)</code></li>
<li>Summary: <p>The rapid development of large language models (LLMs) has yielded impressive
success in various downstream tasks. However, the vast potential and remarkable
capabilities of LLMs also raise new security and privacy concerns if they are
exploited for nefarious purposes due to their open-endedness. For example, LLMs
may be used to plagiarize or imitate writing, thereby infringing the copyright
of the original content, or to create indiscriminate fake information based on
a certain source text. In some cases, LLMs can even analyze text from the
Internet to infer personal privacy. Unfortunately, previous text protection
research could not foresee the emergence of powerful LLMs, rendering it no
longer effective in this new context. To bridge this gap, we introduce Silent
Guardian (SG), a text protection mechanism against LLMs, which allows LLMs to
refuse to generate response when receiving protected text, preventing the
malicious use of text from the source. Specifically, we first propose the
concept of Truncation Protection Examples (TPE). By carefully modifying the
text to be protected, TPE can induce LLMs to first sample the end token, thus
directly terminating the interaction. In addition, to efficiently construct TPE
in the discrete space of text data, we propose a novel optimization algorithm
called Super Taliored Protection (STP), which is not only highly efficient but
also maintains the semantic consistency of the text during the optimization
process. The comprehensive experimental evaluation demonstrates that SG can
effectively protect the target text under various configurations and achieve
almost 100% protection success rate in some cases. Notably, SG also exhibits
relatively good transferability and robustness, making its application in
practical scenarios possible.
</p></li>
</ul>

<h3>Title: Beyond Over-Protection: A Targeted Approach to Spectre Mitigation and Performance Optimization. (arXiv:2312.09770v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09770">http://arxiv.org/abs/2312.09770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09770]] Beyond Over-Protection: A Targeted Approach to Spectre Mitigation and Performance Optimization(http://arxiv.org/abs/2312.09770)</code></li>
<li>Summary: <p>Since the advent of Spectre attacks, researchers and practitioners have
developed a range of hardware and software measures to counter transient
execution attacks. A prime example of such mitigation is speculative load
hardening in LLVM, which protects against leaks by tracking the speculation
state and masking values during misspeculation. LLVM relies on static analysis
to harden programs using slh that often results in over-protection, which
incurs performance overhead. We extended an existing side-channel model
validation framework, Scam-V, to check the vulnerability of programs to
Spectre-PHT attacks and optimize the protection of programs using the slh
approach. We illustrate the efficacy of Scam-V by first demonstrating that it
can automatically identify Spectre vulnerabilities in real programs, e.g.,
fragments of crypto-libraries. We then develop an optimization mechanism that
validates the necessity of slh hardening w.r.t. the target platform. Our
experiments showed that hardening introduced by LLVM in most cases could be
significantly improved when the underlying microarchitecture properties are
considered.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Continual Adversarial Defense. (arXiv:2312.09481v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09481">http://arxiv.org/abs/2312.09481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09481]] Continual Adversarial Defense(http://arxiv.org/abs/2312.09481)</code></li>
<li>Summary: <p>In response to the rapidly evolving nature of adversarial attacks on a
monthly basis, numerous defenses have been proposed to generalize against as
many known attacks as possible. However, designing a defense method that can
generalize to all types of attacks, including unseen ones, is not realistic
because the environment in which defense systems operate is dynamic and
comprises various unique attacks used by many attackers. The defense system
needs to upgrade itself by utilizing few-shot defense feedback and efficient
memory. Therefore, we propose the first continual adversarial defense (CAD)
framework that adapts to any attacks in a dynamic scenario, where various
attacks emerge stage by stage. In practice, CAD is modeled under four
principles: (1) continual adaptation to new attacks without catastrophic
forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4)
high accuracy on both clean and adversarial images. We leverage cutting-edge
continual learning, few-shot learning, and ensemble learning techniques to
qualify the principles. Experiments conducted on CIFAR-10 and ImageNet-100
validate the effectiveness of our approach against multiple stages of 10 modern
adversarial attacks and significant improvements over 10 baseline methods. In
particular, CAD is capable of quickly adapting with minimal feedback and a low
cost of defense failure, while maintaining good performance against old
attacks. Our research sheds light on a brand-new paradigm for continual defense
adaptation against dynamic and evolving attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Embodied Adversarial Attack: A Dynamic Robust Physical Attack in Autonomous Driving. (arXiv:2312.09554v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09554">http://arxiv.org/abs/2312.09554</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09554]] Embodied Adversarial Attack: A Dynamic Robust Physical Attack in Autonomous Driving(http://arxiv.org/abs/2312.09554)</code></li>
<li>Summary: <p>As physical adversarial attacks become extensively applied in unearthing the
potential risk of security-critical scenarios, especially in autonomous
driving, their vulnerability to environmental changes has also been brought to
light. The non-robust nature of physical adversarial attack methods brings
less-than-stable performance consequently. To enhance the robustness of
physical adversarial attacks in the real world, instead of statically
optimizing a robust adversarial example via an off-line training manner like
the existing methods, this paper proposes a brand new robust adversarial attack
framework: Embodied Adversarial Attack (EAA) from the perspective of dynamic
adaptation, which aims to employ the paradigm of embodied intelligence:
Perception-Decision-Control to dynamically adjust the optimal attack strategy
according to the current situations in real time. For the perception module,
given the challenge of needing simulation for the victim's viewpoint, EAA
innovatively devises a Perspective Transformation Network to estimate the
target's transformation from the attacker's perspective. For the decision and
control module, EAA adopts the laser-a highly manipulable medium to implement
physical attacks, and further trains an attack agent with reinforcement
learning to make it capable of instantaneously determining the best attack
strategy based on the perceived information. Finally, we apply our framework to
the autonomous driving scenario. A variety of experiments verify the high
effectiveness of our method under complex scenes.
</p></li>
</ul>

<h3>Title: Towards Transferable Targeted 3D Adversarial Attack in the Physical World. (arXiv:2312.09558v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09558">http://arxiv.org/abs/2312.09558</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09558]] Towards Transferable Targeted 3D Adversarial Attack in the Physical World(http://arxiv.org/abs/2312.09558)</code></li>
<li>Summary: <p>Compared with transferable untargeted attacks, transferable targeted
adversarial attacks could specify the misclassification categories of
adversarial samples, posing a greater threat to security-critical tasks. In the
meanwhile, 3D adversarial samples, due to their potential of multi-view
robustness, can more comprehensively identify weaknesses in existing deep
learning systems, possessing great application value. However, the field of
transferable targeted 3D adversarial attacks remains vacant. The goal of this
work is to develop a more effective technique that could generate transferable
targeted 3D adversarial examples, filling the gap in this field. To achieve
this goal, we design a novel framework named TT3D that could rapidly
reconstruct from few multi-view images into Transferable Targeted 3D textured
meshes. While existing mesh-based texture optimization methods compute
gradients in the high-dimensional mesh space and easily fall into local optima,
leading to unsatisfactory transferability and distinct distortions, TT3D
innovatively performs dual optimization towards both feature grid and
Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which
significantly enhances black-box transferability while enjoying naturalness.
Experimental results show that TT3D not only exhibits superior cross-model
transferability but also maintains considerable adaptability across different
renders and vision tasks. More importantly, we produce 3D adversarial examples
with 3D printing techniques in the real world and verify their robust
performance under various scenarios.
</p></li>
</ul>

<h3>Title: OSTINATO: Cross-host Attack Correlation Through Attack Activity Similarity Detection. (arXiv:2312.09321v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09321">http://arxiv.org/abs/2312.09321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09321]] OSTINATO: Cross-host Attack Correlation Through Attack Activity Similarity Detection(http://arxiv.org/abs/2312.09321)</code></li>
<li>Summary: <p>Modern attacks against enterprises often have multiple targets inside the
enterprise network. Due to the large size of these networks and increasingly
stealthy attacks, attacker activities spanning multiple hosts are extremely
difficult to correlate during a threat-hunting effort. In this paper, we
present a method for an efficient cross-host attack correlation across multiple
hosts. Unlike previous works, our approach does not require lateral movement
detection techniques or host-level modifications. Instead, our approach relies
on an observation that attackers have a few strategic mission objectives on
every host that they infiltrate, and there exist only a handful of techniques
for achieving those objectives. The central idea behind our approach involves
comparing (OS agnostic) activities on different hosts and correlating the hosts
that display the use of similar tactics, techniques, and procedures. We
implement our approach in a tool called Ostinato and successfully evaluate it
in threat hunting scenarios involving DARPA-led red team engagements spanning
500 hosts and in another multi-host attack scenario. Ostinato successfully
detected 21 additional compromised hosts, which the underlying host-based
detection system overlooked in activities spanning multiple days of the attack
campaign. Additionally, Ostinato successfully reduced alarms generated from the
underlying detection system by more than 90%, thus helping to mitigate the
threat alert fatigue problem
</p></li>
</ul>

<h3>Title: A Malware Classification Survey on Adversarial Attacks and Defences. (arXiv:2312.09636v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09636">http://arxiv.org/abs/2312.09636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09636]] A Malware Classification Survey on Adversarial Attacks and Defences(http://arxiv.org/abs/2312.09636)</code></li>
<li>Summary: <p>As the number and complexity of malware attacks continue to increase, there
is an urgent need for effective malware detection systems. While deep learning
models are effective at detecting malware, they are vulnerable to adversarial
attacks. Attacks like this can create malicious files that are resistant to
detection, creating a significant cybersecurity risk. Recent research has seen
the development of several adversarial attack and response approaches aiming at
strengthening deep learning models' resilience to such attacks. This survey
study offers an in-depth look at current research in adversarial attack and
defensive strategies for malware classification in cybersecurity. The methods
are classified into four categories: generative models, feature-based
approaches, ensemble methods, and hybrid tactics. The article outlines
cutting-edge procedures within each area, assessing their benefits and
drawbacks. Each topic presents cutting-edge approaches and explores their
advantages and disadvantages. In addition, the study discusses the datasets and
assessment criteria that are often utilized on this subject. Finally, it
identifies open research difficulties and suggests future study options. This
document is a significant resource for malware categorization and cyber
security researchers and practitioners.
</p></li>
</ul>

<h3>Title: FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge. (arXiv:2312.09665v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09665">http://arxiv.org/abs/2312.09665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09665]] FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge(http://arxiv.org/abs/2312.09665)</code></li>
<li>Summary: <p>Speech recognition systems driven by DNNs have revolutionized human-computer
interaction through voice interfaces, which significantly facilitate our daily
lives. However, the growing popularity of these systems also raises special
concerns on their security, particularly regarding backdoor attacks. A backdoor
attack inserts one or more hidden backdoors into a DNN model during its
training process, such that it does not affect the model's performance on
benign inputs, but forces the model to produce an adversary-desired output if a
specific trigger is present in the model input. Despite the initial success of
current audio backdoor attacks, they suffer from the following limitations: (i)
Most of them require sufficient knowledge, which limits their widespread
adoption. (ii) They are not stealthy enough, thus easy to be detected by
humans. (iii) Most of them cannot attack live speech, reducing their
practicality. To address these problems, in this paper, we propose FlowMur, a
stealthy and practical audio backdoor attack that can be launched with limited
knowledge. FlowMur constructs an auxiliary dataset and a surrogate model to
augment adversary knowledge. To achieve dynamicity, it formulates trigger
generation as an optimization problem and optimizes the trigger over different
attachment positions. To enhance stealthiness, we propose an adaptive data
poisoning method according to Signal-to-Noise Ratio (SNR). Furthermore, ambient
noise is incorporated into the process of trigger generation and data poisoning
to make FlowMur robust to ambient noise and improve its practicality. Extensive
experiments conducted on two datasets demonstrate that FlowMur achieves high
attack performance in both digital and physical settings while remaining
resilient to state-of-the-art defenses. In particular, a human study confirms
that triggers generated by FlowMur are not easily detected by participants.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: TAB: Text-Align Anomaly Backbone Model for Industrial Inspection Tasks. (arXiv:2312.09480v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09480">http://arxiv.org/abs/2312.09480</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09480]] TAB: Text-Align Anomaly Backbone Model for Industrial Inspection Tasks(http://arxiv.org/abs/2312.09480)</code></li>
<li>Summary: <p>In recent years, the focus on anomaly detection and localization in
industrial inspection tasks has intensified. While existing studies have
demonstrated impressive outcomes, they often rely heavily on extensive training
datasets or robust features extracted from pre-trained models trained on
diverse datasets like ImageNet. In this work, we propose a novel framework
leveraging the visual-linguistic CLIP model to adeptly train a backbone model
tailored to the manufacturing domain. Our approach concurrently considers
visual and text-aligned embedding spaces for normal and abnormal conditions.
The resulting pre-trained backbone markedly enhances performance in industrial
downstream tasks, particularly in anomaly detection and localization. Notably,
this improvement is substantiated through experiments conducted on multiple
datasets such as MVTecAD, BTAD, and KSDD2. Furthermore, using our pre-trained
backbone weights allows previous works to achieve superior performance in
few-shot scenarios with less training data. The proposed anomaly backbone
provides a foundation model for more precise anomaly detection and
localization.
</p></li>
</ul>

<h3>Title: Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09486">http://arxiv.org/abs/2312.09486</a></li>
<li>Code URL: <a href="https://github.com/kiwi12138/realistictta">https://github.com/kiwi12138/realistictta</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09486]] Unraveling Batch Normalization for Realistic Test-Time Adaptation(http://arxiv.org/abs/2312.09486)</code></li>
<li>Summary: <p>While recent test-time adaptations exhibit efficacy by adjusting batch
normalization to narrow domain disparities, their effectiveness diminishes with
realistic mini-batches due to inaccurate target estimation. As previous
attempts merely introduce source statistics to mitigate this issue, the
fundamental problem of inaccurate target estimation still persists, leaving the
intrinsic test-time domain shifts unresolved. This paper delves into the
problem of mini-batch degradation. By unraveling batch normalization, we
discover that the inexact target statistics largely stem from the substantially
reduced class diversity in batch. Drawing upon this insight, we introduce a
straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge
the class diversity gap between training and testing batches. Importantly, our
TEMA adaptively extends the scope of typical methods beyond the current batch
to incorporate a diverse set of class information, which in turn boosts an
accurate target estimation. Built upon this foundation, we further design a
novel layer-wise rectification strategy to consistently promote test-time
performance. Our proposed method enjoys a unique advantage as it requires
neither training nor tuning parameters, offering a truly hassle-free solution.
It significantly enhances model robustness against shifted domains and
maintains resilience in diverse real-world scenarios with various batch sizes,
achieving state-of-the-art performance on several major benchmarks. Code is
available at \url{https://github.com/kiwi12138/RealisticTTA}.
</p></li>
</ul>

<h3>Title: Single PW takes a shortcut to compound PW in US imaging. (arXiv:2312.09514v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09514">http://arxiv.org/abs/2312.09514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09514]] Single PW takes a shortcut to compound PW in US imaging(http://arxiv.org/abs/2312.09514)</code></li>
<li>Summary: <p>Reconstruction of ultrasound (US) images from radio-frequency data can be
conceptualized as a linear inverse problem. Traditional deep learning
approaches, which aim to improve the quality of US images by directly learning
priors, often encounter challenges in generalization. Recently, diffusion-based
generative models have received significant attention within the research
community due to their robust performance in image reconstruction tasks.
However, a limitation of these models is their inherent low speed in generating
image samples from pure Gaussian noise progressively. In this study, we exploit
the inherent similarity between the US images reconstructed from a single plane
wave (PW) and PW compounding PWC). We hypothesize that a single PW can take a
shortcut to reach the diffusion trajectory of PWC, removing the need to begin
with Gaussian noise. By employing an advanced diffusion model, we demonstrate
its effectiveness in US image reconstruction, achieving a substantial reduction
in sampling steps. In-vivo experimental results indicate that our approach can
reduce sampling steps by 60%, while preserving comparable performance metrics
with the conventional diffusion model.
</p></li>
</ul>

<h3>Title: Hierarchical Graph Pattern Understanding for Zero-Shot VOS. (arXiv:2312.09525v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09525">http://arxiv.org/abs/2312.09525</a></li>
<li>Code URL: <a href="https://github.com/nust-machine-intelligence-laboratory/hgpu">https://github.com/nust-machine-intelligence-laboratory/hgpu</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09525]] Hierarchical Graph Pattern Understanding for Zero-Shot VOS(http://arxiv.org/abs/2312.09525)</code></li>
<li>Summary: <p>The optical flow guidance strategy is ideal for obtaining motion information
of objects in the video. It is widely utilized in video segmentation tasks.
However, existing optical flow-based methods have a significant dependency on
optical flow, which results in poor performance when the optical flow
estimation fails for a particular scene. The temporal consistency provided by
the optical flow could be effectively supplemented by modeling in a structural
form. This paper proposes a new hierarchical graph neural network (GNN)
architecture, dubbed hierarchical graph pattern understanding (HGPU), for
zero-shot video object segmentation (ZS-VOS). Inspired by the strong ability of
GNNs in capturing structural relations, HGPU innovatively leverages motion cues
(\ie, optical flow) to enhance the high-order representations from the
neighbors of target frames. Specifically, a hierarchical graph pattern encoder
with message aggregation is introduced to acquire different levels of motion
and appearance features in a sequential manner. Furthermore, a decoder is
designed for hierarchically parsing and understanding the transformed
multi-modal contexts to achieve more accurate and robust results. HGPU achieves
state-of-the-art performance on four publicly available benchmarks (DAVIS-16,
YouTube-Objects, Long-Videos and DAVIS-17). Code and pre-trained model can be
found at \url{https://github.com/NUST-Machine-Intelligence-Laboratory/HGPU}.
</p></li>
</ul>

<h3>Title: Adversarial Robustness on Image Classification with $k$-means. (arXiv:2312.09533v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09533">http://arxiv.org/abs/2312.09533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09533]] Adversarial Robustness on Image Classification with $k$-means(http://arxiv.org/abs/2312.09533)</code></li>
<li>Summary: <p>In this paper we explore the challenges and strategies for enhancing the
robustness of $k$-means clustering algorithms against adversarial
manipulations. We evaluate the vulnerability of clustering algorithms to
adversarial attacks, emphasising the associated security risks. Our study
investigates the impact of incremental attack strength on training, introduces
the concept of transferability between supervised and unsupervised models, and
highlights the sensitivity of unsupervised models to sample distributions. We
additionally introduce and evaluate an adversarial training method that
improves testing performance in adversarial scenarios, and we highlight the
importance of various parameters in the proposed training method, such as
continuous learning, centroid initialisation, and adversarial step-count.
</p></li>
</ul>

<h3>Title: Semantic-Aware Transformation-Invariant RoI Align. (arXiv:2312.09609v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09609">http://arxiv.org/abs/2312.09609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09609]] Semantic-Aware Transformation-Invariant RoI Align(http://arxiv.org/abs/2312.09609)</code></li>
<li>Summary: <p>Great progress has been made in learning-based object detection methods in
the last decade. Two-stage detectors often have higher detection accuracy than
one-stage detectors, due to the use of region of interest (RoI) feature
extractors which extract transformation-invariant RoI features for different
RoI proposals, making refinement of bounding boxes and prediction of object
categories more robust and accurate. However, previous RoI feature extractors
can only extract invariant features under limited transformations. In this
paper, we propose a novel RoI feature extractor, termed Semantic RoI Align
(SRA), which is capable of extracting invariant RoI features under a variety of
transformations for two-stage detectors. Specifically, we propose a semantic
attention module to adaptively determine different sampling areas by leveraging
the global and local semantic relationship within the RoI. We also propose a
Dynamic Feature Sampler which dynamically samples features based on the RoI
aspect ratio to enhance the efficiency of SRA, and a new position embedding,
\ie Area Embedding, to provide more accurate position information for SRA
through an improved sampling area representation. Experiments show that our
model significantly outperforms baseline models with slight computational
overhead. In addition, it shows excellent generalization ability and can be
used to improve performance with various state-of-the-art backbones and
detection methods.
</p></li>
</ul>

<h3>Title: TOP-ReID: Multi-spectral Object Re-Identification with Token Permutation. (arXiv:2312.09612v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09612">http://arxiv.org/abs/2312.09612</a></li>
<li>Code URL: <a href="https://github.com/924973292/top-reid">https://github.com/924973292/top-reid</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09612]] TOP-ReID: Multi-spectral Object Re-Identification with Token Permutation(http://arxiv.org/abs/2312.09612)</code></li>
<li>Summary: <p>Multi-spectral object Re-identification (ReID) aims to retrieve specific
objects by leveraging complementary information from different image spectra.
It delivers great advantages over traditional single-spectral ReID in complex
visual environment. However, the significant distribution gap among different
image spectra poses great challenges for effective multi-spectral feature
representations. In addition, most of current Transformer-based ReID methods
only utilize the global feature of class tokens to achieve the holistic
retrieval, ignoring the local discriminative ones. To address the above issues,
we step further to utilize all the tokens of Transformers and propose a cyclic
token permutation framework for multi-spectral object ReID, dubbled TOP-ReID.
More specifically, we first deploy a multi-stream deep network based on vision
Transformers to preserve distinct information from different image spectra.
Then, we propose a Token Permutation Module (TPM) for cyclic multi-spectral
feature aggregation. It not only facilitates the spatial feature alignment
across different image spectra, but also allows the class token of each
spectrum to perceive the local details of other spectra. Meanwhile, we propose
a Complementary Reconstruction Module (CRM), which introduces dense token-level
reconstruction constraints to reduce the distribution gap across different
image spectra. With the above modules, our proposed framework can generate more
discriminative multi-spectral features for robust object ReID. Extensive
experiments on three ReID benchmarks (i.e., RGBNT201, RGBNT100 and MSVR310)
verify the effectiveness of our methods. The code is available at
https://github.<a href="http://export.arxiv.org/abs/com/9249732">com/9249732</a>92/TOP-ReID.
</p></li>
</ul>

<h3>Title: TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification. (arXiv:2312.09627v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09627">http://arxiv.org/abs/2312.09627</a></li>
<li>Code URL: <a href="https://github.com/asuradayuci/tf-clip">https://github.com/asuradayuci/tf-clip</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09627]] TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification(http://arxiv.org/abs/2312.09627)</code></li>
<li>Summary: <p>Large-scale language-image pre-trained models (e.g., CLIP) have shown
superior performances on many cross-modal retrieval tasks. However, the problem
of transferring the knowledge learned from such models to video-based person
re-identification (ReID) has barely been explored. In addition, there is a lack
of decent text descriptions in current ReID benchmarks. To address these
issues, in this work, we propose a novel one-stage text-free CLIP-based
learning framework named TF-CLIP for video-based person ReID. More
specifically, we extract the identity-specific sequence feature as the
CLIP-Memory to replace the text feature. Meanwhile, we design a
Sequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To
capture temporal information, we further propose a Temporal Memory Diffusion
(TMD) module, which consists of two key components: Temporal Memory
Construction (TMC) and Memory Diffusion (MD). Technically, TMC allows the
frame-level memories in a sequence to communicate with each other, and to
extract temporal information based on the relations within the sequence. MD
further diffuses the temporal memories to each token in the original features
to obtain more robust sequence features. Extensive experiments demonstrate that
our proposed method shows much better results than other state-of-the-art
methods on MARS, LS-VID and iLIDS-VID. The code is available at
https://github.com/AsuradaYuci/TF-CLIP.
</p></li>
</ul>

<h3>Title: RANRAC: Robust Neural Scene Representations via Random Ray Consensus. (arXiv:2312.09780v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09780">http://arxiv.org/abs/2312.09780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09780]] RANRAC: Robust Neural Scene Representations via Random Ray Consensus(http://arxiv.org/abs/2312.09780)</code></li>
<li>Summary: <p>We introduce RANRAC, a robust reconstruction algorithm for 3D objects
handling occluded and distracted images, which is a particularly challenging
scenario that prior robust reconstruction methods cannot deal with. Our
solution supports single-shot reconstruction by involving light-field networks,
and is also applicable to photo-realistic, robust, multi-view reconstruction
from real-world images based on neural radiance fields. While the algorithm
imposes certain limitations on the scene representation and, thereby, the
supported scene types, it reliably detects and excludes inconsistent
perspectives, resulting in clean images without floating artifacts. Our
solution is based on a fuzzy adaption of the random sample consensus paradigm,
enabling its application to large scale models. We interpret the minimal number
of samples to determine the model parameters as a tunable hyperparameter. This
is applicable, as a cleaner set of samples improves reconstruction quality.
Further, this procedure also handles outliers. Especially for conditioned
models, it can result in the same local minimum in the latent space as would be
obtained with a completely clean set. We report significant improvements for
novel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the
baseline.
</p></li>
</ul>

<h3>Title: Fragility, Robustness and Antifragility in Deep Learning. (arXiv:2312.09821v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09821">http://arxiv.org/abs/2312.09821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09821]] Fragility, Robustness and Antifragility in Deep Learning(http://arxiv.org/abs/2312.09821)</code></li>
<li>Summary: <p>We propose a systematic analysis of deep neural networks (DNNs) based on a
signal processing technique for network parameter removal, in the form of
synaptic filters that identifies the fragility, robustness and antifragility
characteristics of DNN parameters. Our proposed analysis investigates if the
DNN performance is impacted negatively, invariantly, or positively on both
clean and adversarially perturbed test datasets when the DNN undergoes synaptic
filtering. We define three \textit{filtering scores} for quantifying the
fragility, robustness and antifragility characteristics of DNN parameters based
on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii)
the difference in performances of clean and adversarial datasets. We validate
the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and
ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet
datasets. The filtering scores, for a given network architecture, identify
network parameters that are invariant in characteristics across different
datasets over learning epochs. Vice-versa, for a given dataset, the filtering
scores identify the parameters that are invariant in characteristics across
different network architectures. We show that our synaptic filtering method
improves the test accuracy of ResNet and ShuffleNet models on adversarial
datasets when only the robust and antifragile parameters are selectively
retrained at any given epoch, thus demonstrating applications of the proposed
strategy in improving model robustness.
</p></li>
</ul>

<h3>Title: PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment. (arXiv:2312.09866v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09866">http://arxiv.org/abs/2312.09866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09866]] PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment(http://arxiv.org/abs/2312.09866)</code></li>
<li>Summary: <p>Neural implicit scene representations have recently shown encouraging results
in dense visual SLAM. However, existing methods produce low-quality scene
reconstruction and low-accuracy localization performance when scaling up to
large indoor scenes and long sequences. These limitations are mainly due to
their single, global radiance field with finite capacity, which does not adapt
to large scenarios. Their end-to-end pose networks are also not robust enough
with the growth of cumulative errors in large scenes. To this end, we present
PLGSLAM, a neural visual SLAM system which performs high-fidelity surface
reconstruction and robust camera tracking in real time. To handle large-scale
indoor scenes, PLGSLAM proposes a progressive scene representation method which
dynamically allocates new local scene representation trained with frames within
a local sliding window. This allows us to scale up to larger indoor scenes and
improves robustness (even under pose drifts). In local scene representation,
PLGSLAM utilizes tri-planes for local high-frequency features. We also
incorporate multi-layer perceptron (MLP) networks for the low-frequency
feature, smoothness, and scene completion in unobserved areas. Moreover, we
propose local-to-global bundle adjustment method with a global keyframe
database to address the increased pose drifts on long sequences. Experimental
results demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction
results and tracking performance across various datasets and scenarios (both in
small and large-scale indoor environments). The code will be open-sourced upon
paper acceptance.
</p></li>
</ul>

<h3>Title: TMP: Temporal Motion Propagation for Online Video Super-Resolution. (arXiv:2312.09909v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09909">http://arxiv.org/abs/2312.09909</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09909]] TMP: Temporal Motion Propagation for Online Video Super-Resolution(http://arxiv.org/abs/2312.09909)</code></li>
<li>Summary: <p>Online video super-resolution (online-VSR) highly relies on an effective
alignment module to aggregate temporal information, while the strict latency
requirement makes accurate and efficient alignment very challenging. Though
much progress has been achieved, most of the existing online-VSR methods
estimate the motion fields of each frame separately to perform alignment, which
is computationally redundant and ignores the fact that the motion fields of
adjacent frames are correlated. In this work, we propose an efficient Temporal
Motion Propagation (TMP) method, which leverages the continuity of motion field
to achieve fast pixel-level alignment among consecutive frames. Specifically,
we first propagate the offsets from previous frames to the current frame, and
then refine them in the neighborhood, which significantly reduces the matching
space and speeds up the offset estimation process. Furthermore, to enhance the
robustness of alignment, we perform spatial-wise weighting on the warped
features, where the positions with more precise offsets are assigned higher
importance. Experiments on benchmark datasets demonstrate that the proposed TMP
method achieves leading online-VSR accuracy as well as inference speed. The
source code of TMP can be found at
\href{https://github.com/xtudbxk/TMP}{https://github.com/xtudbxk/TMP}.
</p></li>
</ul>

<h3>Title: No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models. (arXiv:2312.09494v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09494">http://arxiv.org/abs/2312.09494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09494]] No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models(http://arxiv.org/abs/2312.09494)</code></li>
<li>Summary: <p>To reduce the computation cost and the energy consumption in large language
models (LLM), skimming-based acceleration dynamically drops unimportant tokens
of the input sequence progressively along layers of the LLM while preserving
the tokens of semantic importance. However, our work for the first time reveals
the acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this
paper, we propose No-Skim, a general framework to help the owners of
skimming-based LLM to understand and measure the robustness of their
acceleration scheme. Specifically, our framework searches minimal and
unnoticeable perturbations at character-level and token-level to generate
adversarial inputs that sufficiently increase the remaining token ratio, thus
increasing the computation cost and energy consumption. We systematically
evaluate the vulnerability of the skimming acceleration in various LLM
architectures including BERT and RoBERTa on the GLUE benchmark. In the worst
case, the perturbation found by No-Skim substantially increases the running
cost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation
framework to various scenarios, making the evaluation conductible with
different level of knowledge.
</p></li>
</ul>

<h3>Title: Phoneme-aware Encoding for Prefix-tree-based Contextual ASR. (arXiv:2312.09582v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09582">http://arxiv.org/abs/2312.09582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09582]] Phoneme-aware Encoding for Prefix-tree-based Contextual ASR(http://arxiv.org/abs/2312.09582)</code></li>
<li>Summary: <p>In speech recognition applications, it is important to recognize
context-specific rare words, such as proper nouns. Tree-constrained Pointer
Generator (TCPGen) has shown promise for this purpose, which efficiently biases
such words with a prefix tree. While the original TCPGen relies on
grapheme-based encoding, we propose extending it with phoneme-aware encoding to
better recognize words of unusual pronunciations. As TCPGen handles biasing
words as subword units, we propose obtaining subword-level phoneme-aware
encoding by using alignment between phonemes and subwords. Furthermore, we
propose injecting phoneme-level predictions from CTC into queries of TCPGen so
that the model better interprets the phoneme-aware encodings. We conducted ASR
experiments with TCPGen for RNN transducer. We observed that proposed
phoneme-aware encoding outperformed ordinary grapheme-based encoding on both
the English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness
of our approach across linguistically diverse languages.
</p></li>
</ul>

<h3>Title: Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach. (arXiv:2312.09718v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09718">http://arxiv.org/abs/2312.09718</a></li>
<li>Code URL: <a href="https://github.com/homoscribens/shortcut_reasoning">https://github.com/homoscribens/shortcut_reasoning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09718]] Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach(http://arxiv.org/abs/2312.09718)</code></li>
<li>Summary: <p>Shortcut reasoning is an irrational process of inference, which degrades the
robustness of an NLP model. While a number of previous work has tackled the
identification of shortcut reasoning, there are still two major limitations:
(i) a method for quantifying the severity of the discovered shortcut reasoning
is not provided; (ii) certain types of shortcut reasoning may be missed. To
address these issues, we propose a novel method for identifying shortcut
reasoning. The proposed method quantifies the severity of the shortcut
reasoning by leveraging out-of-distribution data and does not make any
assumptions about the type of tokens triggering the shortcut reasoning. Our
experiments on Natural Language Inference and Sentiment Analysis demonstrate
that our framework successfully discovers known and unknown shortcut reasoning
in the previous work.
</p></li>
</ul>

<h3>Title: RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding. (arXiv:2312.09932v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09932">http://arxiv.org/abs/2312.09932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09932]] RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding(http://arxiv.org/abs/2312.09932)</code></li>
<li>Summary: <p>Natural language understanding (NLU) using neural network pipelines often
requires additional context that is not solely present in the input data.
Through Prior research, it has been evident that NLU benchmarks are susceptible
to manipulation by neural models, wherein these models exploit statistical
artifacts within the encoded external knowledge to artificially inflate
performance metrics for downstream tasks. Our proposed approach, known as the
Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by
incorporating three distinct objectives within the neural network pipeline.
Firstly, the Recap objective involves paraphrasing the input text using a
paraphrasing model in order to summarize and encapsulate its essence. Secondly,
the Deliberation objective entails encoding external graph information related
to entities mentioned in the input text, utilizing a graph embedding model.
Finally, the Respond objective employs a classification head model that
utilizes representations from the Recap and Deliberation modules to generate
the final prediction. By cascading these three models and minimizing a combined
loss, we mitigate the potential for gaming the benchmark and establish a robust
method for capturing the underlying semantic patterns, thus enabling accurate
predictions. To evaluate the effectiveness of the RDR method, we conduct tests
on multiple GLUE benchmark tasks. Our results demonstrate improved performance
compared to competitive baselines, with an enhancement of up to 2\% on standard
metrics. Furthermore, we analyze the observed evidence for semantic
understanding exhibited by RDR models, emphasizing their ability to avoid
gaming the benchmark and instead accurately capture the true underlying
semantic patterns.
</p></li>
</ul>

<h3>Title: Entropy Causal Graphs for Multivariate Time Series Anomaly Detection. (arXiv:2312.09478v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09478">http://arxiv.org/abs/2312.09478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09478]] Entropy Causal Graphs for Multivariate Time Series Anomaly Detection(http://arxiv.org/abs/2312.09478)</code></li>
<li>Summary: <p>Many multivariate time series anomaly detection frameworks have been proposed
and widely applied. However, most of these frameworks do not consider intrinsic
relationships between variables in multivariate time series data, thus ignoring
the causal relationship among variables and degrading anomaly detection
performance. This work proposes a novel framework called CGAD, an entropy
Causal Graph for multivariate time series Anomaly Detection. CGAD utilizes
transfer entropy to construct graph structures that unveil the underlying
causal relationships among time series data. Weighted graph convolutional
networks combined with causal convolutions are employed to model both the
causal graph structures and the temporal patterns within multivariate time
series data. Furthermore, CGAD applies anomaly scoring, leveraging median
absolute deviation-based normalization to improve the robustness of the anomaly
identification process. Extensive experiments demonstrate that CGAD outperforms
state-of-the-art methods on real-world datasets with a 15% average improvement
based on three different multivariate time series anomaly detection metrics.
</p></li>
</ul>

<h3>Title: Adaptive Integration of Partial Label Learning and Negative Learning for Enhanced Noisy Label Learning. (arXiv:2312.09505v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09505">http://arxiv.org/abs/2312.09505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09505]] Adaptive Integration of Partial Label Learning and Negative Learning for Enhanced Noisy Label Learning(http://arxiv.org/abs/2312.09505)</code></li>
<li>Summary: <p>There has been significant attention devoted to the effectiveness of various
domains, such as semi-supervised learning, contrastive learning, and
meta-learning, in enhancing the performance of methods for noisy label learning
(NLL) tasks. However, most existing methods still depend on prior assumptions
regarding clean samples amidst different sources of noise (\eg, a pre-defined
drop rate or a small subset of clean samples). In this paper, we propose a
simple yet powerful idea called \textbf{NPN}, which revolutionizes
\textbf{N}oisy label learning by integrating \textbf{P}artial label learning
(PLL) and \textbf{N}egative learning (NL). Toward this goal, we initially
decompose the given label space adaptively into the candidate and complementary
labels, thereby establishing the conditions for PLL and NL. We propose two
adaptive data-driven paradigms of label disambiguation for PLL: hard
disambiguation and soft disambiguation. Furthermore, we generate reliable
complementary labels using all non-candidate labels for NL to enhance model
robustness through indirect supervision. To maintain label reliability during
the later stage of model training, we introduce a consistency regularization
term that encourages agreement between the outputs of multiple augmentations.
Experiments conducted on both synthetically corrupted and real-world noisy
datasets demonstrate the superiority of NPN compared to other state-of-the-art
(SOTA) methods. The source code has been made available at
{\color{purple}{\url{https://github.com/NUST-Machine-Intelligence-Laboratory/NPN}}}.
</p></li>
</ul>

<h3>Title: A Novel Hybrid Ordinal Learning Model with Health Care Application. (arXiv:2312.09540v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09540">http://arxiv.org/abs/2312.09540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09540]] A Novel Hybrid Ordinal Learning Model with Health Care Application(http://arxiv.org/abs/2312.09540)</code></li>
<li>Summary: <p>Ordinal learning (OL) is a type of machine learning models with broad utility
in health care applications such as diagnosis of different grades of a disease
(e.g., mild, modest, severe) and prediction of the speed of disease progression
(e.g., very fast, fast, moderate, slow). This paper aims to tackle a situation
when precisely labeled samples are limited in the training set due to cost or
availability constraints, whereas there could be an abundance of samples with
imprecise labels. We focus on imprecise labels that are intervals, i.e., one
can know that a sample belongs to an interval of labels but cannot know which
unique label it has. This situation is quite common in health care datasets due
to limitations of the diagnostic instrument, sparse clinical visits, or/and
patient dropout. Limited research has been done to develop OL models with
imprecise/interval labels. We propose a new Hybrid Ordinal Learner (HOL) to
integrate samples with both precise and interval labels to train a robust OL
model. We also develop a tractable and efficient optimization algorithm to
solve the HOL formulation. We compare HOL with several recently developed OL
methods on four benchmarking datasets, which demonstrate the superior
performance of HOL. Finally, we apply HOL to a real-world dataset for
predicting the speed of progressing to Alzheimer's Disease (AD) for individuals
with Mild Cognitive Impairment (MCI) based on a combination of multi-modality
neuroimaging and demographic/clinical datasets. HOL achieves high accuracy in
the prediction and outperforms existing methods. The capability of accurately
predicting the speed of progression to AD for each individual with MCI has the
potential for helping facilitate more individually-optimized interventional
strategies.
</p></li>
</ul>

<h3>Title: Rethinking Causal Relationships Learning in Graph Neural Networks. (arXiv:2312.09613v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09613">http://arxiv.org/abs/2312.09613</a></li>
<li>Code URL: <a href="https://github.com/yaoyao-yaoyao-cell/crcg">https://github.com/yaoyao-yaoyao-cell/crcg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09613]] Rethinking Causal Relationships Learning in Graph Neural Networks(http://arxiv.org/abs/2312.09613)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) demonstrate their significance by effectively
modeling complex interrelationships within graph-structured data. To enhance
the credibility and robustness of GNNs, it becomes exceptionally crucial to
bolster their ability to capture causal relationships. However, despite recent
advancements that have indeed strengthened GNNs from a causal learning
perspective, conducting an in-depth analysis specifically targeting the causal
modeling prowess of GNNs remains an unresolved issue. In order to
comprehensively analyze various GNN models from a causal learning perspective,
we constructed an artificially synthesized dataset with known and controllable
causal relationships between data and labels. The rationality of the generated
data is further ensured through theoretical foundations. Drawing insights from
analyses conducted using our dataset, we introduce a lightweight and highly
adaptable GNN module designed to strengthen GNNs' causal learning capabilities
across a diverse range of tasks. Through a series of experiments conducted on
both synthetic datasets and other real-world datasets, we empirically validate
the effectiveness of the proposed module.
</p></li>
</ul>

<h3>Title: Quilt: Robust Data Segment Selection against Concept Drifts. (arXiv:2312.09691v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09691">http://arxiv.org/abs/2312.09691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09691]] Quilt: Robust Data Segment Selection against Concept Drifts(http://arxiv.org/abs/2312.09691)</code></li>
<li>Summary: <p>Continuous machine learning pipelines are common in industrial settings where
models are periodically trained on data streams. Unfortunately, concept drifts
may occur in data streams where the joint distribution of the data X and label
y, P(X, y), changes over time and possibly degrade model accuracy. Existing
concept drift adaptation approaches mostly focus on updating the model to the
new data possibly using ensemble techniques of previous models and tend to
discard the drifted historical data. However, we contend that explicitly
utilizing the drifted data together leads to much better model accuracy and
propose Quilt, a data-centric framework for identifying and selecting data
segments that maximize model accuracy. To address the potential downside of
efficiency, Quilt extends existing data subset selection techniques, which can
be used to reduce the training data without compromising model accuracy. These
techniques cannot be used as is because they only assume virtual drifts where
the posterior probabilities P(y|X) are assumed not to change. In contrast, a
key challenge in our setup is to also discard undesirable data segments with
concept drifts. Quilt thus discards drifted data segments and selects data
segment subsets holistically for accurate and efficient model training. The two
operations use gradient-based scores, which have little computation overhead.
In our experiments, we show that Quilt outperforms state-of-the-art drift
adaptation and data selection baselines on synthetic and real datasets.
</p></li>
</ul>

<h3>Title: Verification-Friendly Deep Neural Networks. (arXiv:2312.09748v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09748">http://arxiv.org/abs/2312.09748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09748]] Verification-Friendly Deep Neural Networks(http://arxiv.org/abs/2312.09748)</code></li>
<li>Summary: <p>Machine learning techniques often lack formal correctness guarantees. This is
evidenced by the widespread adversarial examples that plague most deep-learning
applications. This resulted in several research efforts that aim at verifying
deep neural networks, with a particular focus on safety-critical applications.
However, formal verification techniques still face major scalability and
precision challenges when dealing with the complexity of such networks. The
over-approximation introduced during the formal verification process to tackle
the scalability challenge often results in inconclusive analysis. To address
this challenge, we propose a novel framework to generate Verification-friendly
Neural Networks (VNNs). We present a post-training optimization framework to
achieve a balance between preserving prediction performance and robustness in
the resulting networks. Our proposed framework proves to result in networks
that are comparable to the original ones in terms of prediction performance,
while amenable to verification. This essentially enables us to establish
robustness for more VNNs than their deep neural network counterparts, in a more
time-efficient manner.
</p></li>
</ul>

<h3>Title: Hypergraph-MLP: Learning on Hypergraphs without Message Passing. (arXiv:2312.09778v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09778">http://arxiv.org/abs/2312.09778</a></li>
<li>Code URL: <a href="https://github.com/tbh-98/hypergraph-mlp">https://github.com/tbh-98/hypergraph-mlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09778]] Hypergraph-MLP: Learning on Hypergraphs without Message Passing(http://arxiv.org/abs/2312.09778)</code></li>
<li>Summary: <p>Hypergraphs are vital in modelling data with higher-order relations
containing more than two entities, gaining prominence in machine learning and
signal processing. Many hypergraph neural networks leverage message passing
over hypergraph structures to enhance node representation learning, yielding
impressive performances in tasks like hypergraph node classification. However,
these message-passing-based models face several challenges, including
oversmoothing as well as high latency and sensitivity to structural
perturbations at inference time. To tackle those challenges, we propose an
alternative approach where we integrate the information about hypergraph
structures into training supervision without explicit message passing, thus
also removing the reliance on it at inference. Specifically, we introduce
Hypergraph-MLP, a novel learning framework for hypergraph-structured data,
where the learning model is a straightforward multilayer perceptron (MLP)
supervised by a loss function based on a notion of signal smoothness on
hypergraphs. Experiments on hypergraph node classification tasks demonstrate
that Hypergraph-MLP achieves competitive performance compared to existing
baselines, and is considerably faster and more robust against structural
perturbations at inference.
</p></li>
</ul>

<h3>Title: Physics-informed Neural Network Estimation of Material Properties in Soft Tissue Nonlinear Biomechanical Models. (arXiv:2312.09787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09787">http://arxiv.org/abs/2312.09787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09787]] Physics-informed Neural Network Estimation of Material Properties in Soft Tissue Nonlinear Biomechanical Models(http://arxiv.org/abs/2312.09787)</code></li>
<li>Summary: <p>The development of biophysical models for clinical applications is rapidly
advancing in the research community, thanks to their predictive nature and
their ability to assist the interpretation of clinical data. However,
high-resolution and accurate multi-physics computational models are
computationally expensive and their personalisation involves fine calibration
of a large number of parameters, which may be space-dependent, challenging
their clinical translation. In this work, we propose a new approach which
relies on the combination of physics-informed neural networks (PINNs) with
three-dimensional soft tissue nonlinear biomechanical models, capable of
reconstructing displacement fields and estimating heterogeneous
patient-specific biophysical properties. The proposed learning algorithm
encodes information from a limited amount of displacement and, in some cases,
strain data, that can be routinely acquired in the clinical setting, and
combines it with the physics of the problem, represented by a mathematical
model based on partial differential equations, to regularise the problem and
improve its convergence properties. Several benchmarks are presented to show
the accuracy and robustness of the proposed method and its great potential to
enable the robust and effective identification of patient-specific,
heterogeneous physical properties, s.a. tissue stiffness properties. In
particular, we demonstrate the capability of the PINN to detect the presence,
location and severity of scar tissue, which is beneficial to develop
personalised simulation models for disease diagnosis, especially for cardiac
applications.
</p></li>
</ul>

<h3>Title: Sketch and shift: a robust decoder for compressive clustering. (arXiv:2312.09940v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09940">http://arxiv.org/abs/2312.09940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09940]] Sketch and shift: a robust decoder for compressive clustering(http://arxiv.org/abs/2312.09940)</code></li>
<li>Summary: <p>Compressive learning is an emerging approach to drastically reduce the memory
footprint of large-scale learning, by first summarizing a large dataset into a
low-dimensional sketch vector, and then decoding from this sketch the latent
information needed for learning. In light of recent progress on information
preservation guarantees for sketches based on random features, a major
objective is to design easy-to-tune algorithms (called decoders) to robustly
and efficiently extract this information. To address the underlying non-convex
optimization problems, various heuristics have been proposed. In the case of
compressive clustering, the standard heuristic is CL-OMPR, a variant of sliding
Frank-Wolfe. Yet, CL-OMPR is hard to tune, and the examination of its
robustness was overlooked. In this work, we undertake a scrutinized examination
of CL-OMPR to circumvent its limitations. In particular, we show how this
algorithm can fail to recover the clusters even in advantageous scenarios. To
gain insight, we show how the deficiencies of this algorithm can be attributed
to optimization difficulties related to the structure of a correlation function
appearing at core steps of the algorithm. To address these limitations, we
propose an alternative decoder offering substantial improvements over CL-OMPR.
Its design is notably inspired from the mean shift algorithm, a classic
approach to detect the local maxima of kernel density estimators. The proposed
algorithm can extract clustering information from a sketch of the MNIST dataset
that is 10 times smaller than previously.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Text-Guided Face Recognition using Multi-Granularity Cross-Modal Contrastive Learning. (arXiv:2312.09367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09367">http://arxiv.org/abs/2312.09367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09367]] Text-Guided Face Recognition using Multi-Granularity Cross-Modal Contrastive Learning(http://arxiv.org/abs/2312.09367)</code></li>
<li>Summary: <p>State-of-the-art face recognition (FR) models often experience a significant
performance drop when dealing with facial images in surveillance scenarios
where images are in low quality and often corrupted with noise. Leveraging
facial characteristics, such as freckles, scars, gender, and ethnicity, becomes
highly beneficial in improving FR performance in such scenarios. In this paper,
we introduce text-guided face recognition (TGFR) to analyze the impact of
integrating facial attributes in the form of natural language descriptions. We
hypothesize that adding semantic information into the loop can significantly
improve the image understanding capability of an FR algorithm compared to other
soft biometrics. However, learning a discriminative joint embedding within the
multimodal space poses a considerable challenge due to the semantic gap in the
unaligned image-text representations, along with the complexities arising from
ambiguous and incoherent textual descriptions of the face. To address these
challenges, we introduce a face-caption alignment module (FCAM), which
incorporates cross-modal contrastive losses across multiple granularities to
maximize the mutual information between local and global features of the
face-caption pair. Within FCAM, we refine both facial and textual features for
learning aligned and discriminative features. We also design a face-caption
fusion module (FCFM) that applies fine-grained interactions and coarse-grained
associations among cross-modal features. Through extensive experiments
conducted on three face-caption datasets, proposed TGFR demonstrates remarkable
improvements, particularly on low-quality images, over existing FR models and
outperforms other related methods and benchmarks.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Exploring the Feasibility of Generating Realistic 3D Models of Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's Impact on Model Generation. (arXiv:2312.09682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09682">http://arxiv.org/abs/2312.09682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09682]] Exploring the Feasibility of Generating Realistic 3D Models of Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's Impact on Model Generation(http://arxiv.org/abs/2312.09682)</code></li>
<li>Summary: <p>Many species face the threat of extinction. It's important to study these
species and gather information about them as much as possible to preserve
biodiversity. Due to the rarity of endangered species, there is a limited
amount of data available, making it difficult to apply data requiring
generative AI methods to this domain. We aim to study the feasibility of
generating consistent and real-like 3D models of endangered animals using
limited data. Such a phenomenon leads us to utilize zero-shot stable diffusion
models that can generate a 3D model out of a single image of the target
species. This paper investigates the intricate relationship between elevation
angle and the output quality of 3D model generation, focusing on the innovative
approach presented in DreamGaussian. DreamGaussian, a novel framework utilizing
Generative Gaussian Splatting along with novel mesh extraction and refinement
algorithms, serves as the focal point of our study. We conduct a comprehensive
analysis, analyzing the effect of varying elevation angles on DreamGaussian's
ability to reconstruct 3D scenes accurately. Through an empirical evaluation,
we demonstrate how changes in elevation angle impact the generated images'
spatial coherence, structural integrity, and perceptual realism. We observed
that giving a correct elevation angle with the input image significantly
affects the result of the generated 3D model. We hope this study to be
influential for the usability of AI to preserve endangered animals; while the
penultimate aim is to obtain a model that can output biologically consistent 3D
models via small samples, the qualitative interpretation of an existing
state-of-the-art model such as DreamGaussian will be a step forward in our
goal.
</p></li>
</ul>

<h3>Title: Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning. (arXiv:2312.09783v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09783">http://arxiv.org/abs/2312.09783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09783]] Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning(http://arxiv.org/abs/2312.09783)</code></li>
<li>Summary: <p>Explaining predictions of black-box neural networks is crucial when applied
to decision-critical tasks. Thus, attribution maps are commonly used to
identify important image regions, despite prior work showing that humans prefer
explanations based on similar examples. To this end, ProtoPNet learns a set of
class-representative feature vectors (prototypes) for case-based reasoning.
During inference, similarities of latent features to prototypes are linearly
classified to form predictions and attribution maps are provided to explain the
similarity. In this work, we evaluate whether architectures for case-based
reasoning fulfill established axioms required for faithful explanations using
the example of ProtoPNet. We show that such architectures allow the extraction
of faithful explanations. However, we prove that the attribution maps used to
explain the similarities violate the axioms. We propose a new procedure to
extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,
these explanations are Shapley values, calculated on the similarity scores of
each prototype. They allow to faithfully answer which prototypes are present in
an unseen image and quantify each pixel's contribution to that presence,
thereby complying with all axioms. The theoretical violations of ProtoPNet
manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,
RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,
ResNeXt50). Our experiments show a qualitative difference between the
explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the
explanations with the Area Over the Perturbation Curve, on which ProtoPFaith
outperforms ProtoPNet on all experiments by a factor $&gt;10^3$.
</p></li>
</ul>

<h3>Title: Information Extraction from Unstructured data using Augmented-AI and Computer Vision. (arXiv:2312.09880v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09880">http://arxiv.org/abs/2312.09880</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09880]] Information Extraction from Unstructured data using Augmented-AI and Computer Vision(http://arxiv.org/abs/2312.09880)</code></li>
<li>Summary: <p>Process of information extraction (IE) is often used to extract meaningful
information from unstructured and unlabeled data. Conventional methods of data
extraction including application of OCR and passing extraction engine, are
inefficient on large data and have their limitation. In this paper, a peculiar
technique of information extraction is proposed using A2I and computer vision
technologies, which also includes NLP.
</p></li>
</ul>

<h3>Title: Open Domain Knowledge Extraction for Knowledge Graphs. (arXiv:2312.09424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09424">http://arxiv.org/abs/2312.09424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09424]] Open Domain Knowledge Extraction for Knowledge Graphs(http://arxiv.org/abs/2312.09424)</code></li>
<li>Summary: <p>The quality of a knowledge graph directly impacts the quality of downstream
applications (e.g. the number of answerable questions using the graph). One
ongoing challenge when building a knowledge graph is to ensure completeness and
freshness of the graph's entities and facts. In this paper, we introduce ODKE,
a scalable and extensible framework that sources high-quality entities and
facts from open web at scale. ODKE utilizes a wide range of extraction models
and supports both streaming and batch processing at different latency. We
reflect on the challenges and design decisions made and share lessons learned
when building and deploying ODKE to grow an industry-scale open domain
knowledge graph.
</p></li>
</ul>

<h3>Title: Grammatical information in BERT sentence embeddings as two-dimensional arrays. (arXiv:2312.09890v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09890">http://arxiv.org/abs/2312.09890</a></li>
<li>Code URL: <a href="https://github.com/clcl-geneva/blm-snfdisentangling">https://github.com/clcl-geneva/blm-snfdisentangling</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09890]] Grammatical information in BERT sentence embeddings as two-dimensional arrays(http://arxiv.org/abs/2312.09890)</code></li>
<li>Summary: <p>Sentence embeddings induced with various transformer architectures encode
much semantic and syntactic information in a distributed manner in a
one-dimensional array. We investigate whether specific grammatical information
can be accessed in these distributed representations. Using data from a task
developed to test rule-like generalizations, our experiments on detecting
subject-verb agreement yield several promising results. First, we show that
while the usual sentence representations encoded as one-dimensional arrays do
not easily support extraction of rule-like regularities, a two-dimensional
reshaping of these vectors allows various learning architectures to access such
information. Next, we show that various architectures can detect patterns in
these two-dimensional reshaped sentence embeddings and successfully learn a
model based on smaller amounts of simpler training data, which performs well on
more complex test data. This indicates that current sentence embeddings contain
information that is regularly distributed, and which can be captured when the
embeddings are reshaped into higher dimensional arrays. Our results cast light
on representations produced by language models and help move towards developing
few-shot learning approaches.
</p></li>
</ul>

<h3>Title: Symplectic Autoencoders for Model Reduction of Hamiltonian Systems. (arXiv:2312.10004v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10004">http://arxiv.org/abs/2312.10004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10004]] Symplectic Autoencoders for Model Reduction of Hamiltonian Systems(http://arxiv.org/abs/2312.10004)</code></li>
<li>Summary: <p>Many applications, such as optimization, uncertainty quantification and
inverse problems, require repeatedly performing simulations of
large-dimensional physical systems for different choices of parameters. This
can be prohibitively expensive.
</p>
<p>In order to save computational cost, one can construct surrogate models by
expressing the system in a low-dimensional basis, obtained from training data.
This is referred to as model reduction.
</p>
<p>Past investigations have shown that, when performing model reduction of
Hamiltonian systems, it is crucial to preserve the symplectic structure
associated with the system in order to ensure long-term numerical stability.
</p>
<p>Up to this point structure-preserving reductions have largely been limited to
linear transformations. We propose a new neural network architecture in the
spirit of autoencoders, which are established tools for dimension reduction and
feature extraction in data science, to obtain more general mappings.
</p>
<p>In order to train the network, a non-standard gradient descent approach is
applied that leverages the differential-geometric structure emerging from the
network design.
</p>
<p>The new architecture is shown to significantly outperform existing designs in
accuracy.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space. (arXiv:2312.09817v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09817">http://arxiv.org/abs/2312.09817</a></li>
<li>Code URL: <a href="https://github.com/hasanmohsin/betapredbayes_fl">https://github.com/hasanmohsin/betapredbayes_fl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09817]] Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space(http://arxiv.org/abs/2312.09817)</code></li>
<li>Summary: <p>Federated Learning (FL) involves training a model over a dataset distributed
among clients, with the constraint that each client's dataset is localized and
possibly heterogeneous. In FL, small and noisy datasets are common,
highlighting the need for well-calibrated models that represent the uncertainty
of predictions. The closest FL techniques to achieving such goals are the
Bayesian FL methods which collect parameter samples from local posteriors, and
aggregate them to approximate the global posterior. To improve scalability for
larger models, one common Bayesian approach is to approximate the global
predictive posterior by multiplying local predictive posteriors. In this work,
we demonstrate that this method gives systematically overconfident predictions,
and we remedy this by proposing $\beta$-Predictive Bayes, a Bayesian FL
algorithm that interpolates between a mixture and product of the predictive
posteriors, using a tunable parameter $\beta$. This parameter is tuned to
improve the global ensemble's calibration, before it is distilled to a single
model. Our method is evaluated on a variety of regression and classification
datasets to demonstrate its superiority in calibration to other baselines, even
as data heterogeneity increases. Code available at
https://github.com/hasanmohsin/betaPredBayes_FL
</p></li>
</ul>

<h3>Title: Dynamic Heterogeneous Federated Learning with Multi-Level Prototypes. (arXiv:2312.09881v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09881">http://arxiv.org/abs/2312.09881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09881]] Dynamic Heterogeneous Federated Learning with Multi-Level Prototypes(http://arxiv.org/abs/2312.09881)</code></li>
<li>Summary: <p>Federated learning shows promise as a privacy-preserving collaborative
learning technique. Existing heterogeneous federated learning mainly focuses on
skewing the label distribution across clients. However, most approaches suffer
from catastrophic forgetting and concept drift, mainly when the global
distribution of all classes is extremely unbalanced and the data distribution
of the client dynamically evolves over time. In this paper, we study the new
task, i.e., Dynamic Heterogeneous Federated Learning (DHFL), which addresses
the practical scenario where heterogeneous data distributions exist among
different clients and dynamic tasks within the client. Accordingly, we propose
a novel federated learning framework named Federated Multi-Level Prototypes
(FedMLP) and design federated multi-level regularizations. To mitigate concept
drift, we construct prototypes and semantic prototypes to provide fruitful
generalization knowledge and ensure the continuity of prototype spaces. To
maintain the model stability and consistency of convergence, three
regularizations are introduced as training losses, i.e., prototype-based
regularization, semantic prototype-based regularization, and federated
inter-task regularization. Extensive experiments show that the proposed method
achieves state-of-the-art performance in various settings.
</p></li>
</ul>

<h3>Title: GreenLightningAI: An Efficient AI System with Decoupled Structural and Quantitative Knowledge. (arXiv:2312.09971v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09971">http://arxiv.org/abs/2312.09971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09971]] GreenLightningAI: An Efficient AI System with Decoupled Structural and Quantitative Knowledge(http://arxiv.org/abs/2312.09971)</code></li>
<li>Summary: <p>The number and complexity of artificial intelligence (AI) applications is
growing relentlessly. As a result, even with the many algorithmic and
mathematical advances experienced over past decades as well as the impressive
energy efficiency and computational capacity of current hardware accelerators,
training the most powerful and popular deep neural networks comes at very high
economic and environmental costs. Recognising that additional optimisations of
conventional neural network training is very difficult, this work takes a
radically different approach by proposing GreenLightningAI, a new AI system
design consisting of a linear model that is capable of emulating the behaviour
of deep neural networks by subsetting the model for each particular sample. The
new AI system stores the information required to select the system subset for a
given sample (referred to as structural information) separately from the linear
model parameters (referred to as quantitative knowledge). In this paper we
present a proof of concept, showing that the structural information stabilises
far earlier than the quantitative knowledge. Additionally, we show
experimentally that the structural information can be kept unmodified when
re-training the AI system with new samples while still achieving a validation
accuracy similar to that obtained when re-training a neural network with
similar size. Since the proposed AI system is based on a linear model, multiple
copies of the model, trained with different datasets, can be easily combined.
This enables faster and greener (re)-training algorithms, including incremental
re-training and federated incremental re-training.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark. (arXiv:2312.09857v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09857">http://arxiv.org/abs/2312.09857</a></li>
<li>Code URL: <a href="https://github.com/ericssonresearch/uda-4-tsc">https://github.com/ericssonresearch/uda-4-tsc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09857]] Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark(http://arxiv.org/abs/2312.09857)</code></li>
<li>Summary: <p>Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to
train models for unlabeled target data. Despite extensive research in domains
like computer vision and natural language processing, UDA remains underexplored
for time series data, which has widespread real-world applications ranging from
medicine and manufacturing to earth observation and human activity recognition.
Our paper addresses this gap by introducing a comprehensive benchmark for
evaluating UDA techniques for time series classification, with a focus on deep
learning methods. We provide seven new benchmark datasets covering various
domain shifts and temporal dynamics, facilitating fair and standardized UDA
method assessments with state of the art neural network backbones (e.g.
Inception) for time series data. This benchmark offers insights into the
strengths and limitations of the evaluated approaches while preserving the
unsupervised nature of domain adaptation, making it directly applicable to
practical problems. Our paper serves as a vital resource for researchers and
practitioners, advancing domain adaptation solutions for time series data and
fostering innovation in this critical field. The implementation code of this
benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: LatentEditor: Text Driven Local Editing of 3D Scenes. (arXiv:2312.09313v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09313">http://arxiv.org/abs/2312.09313</a></li>
<li>Code URL: <a href="https://github.com/umarkhalidAI/LatentEditor">https://github.com/umarkhalidAI/LatentEditor</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09313]] LatentEditor: Text Driven Local Editing of 3D Scenes(http://arxiv.org/abs/2312.09313)</code></li>
<li>Summary: <p>While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.
</p></li>
</ul>

<h3>Title: CAGE: Controllable Articulation GEneration. (arXiv:2312.09570v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09570">http://arxiv.org/abs/2312.09570</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09570]] CAGE: Controllable Articulation GEneration(http://arxiv.org/abs/2312.09570)</code></li>
<li>Summary: <p>We address the challenge of generating 3D articulated objects in a
controllable fashion. Currently, modeling articulated 3D objects is either
achieved through laborious manual authoring, or using methods from prior work
that are hard to scale and control directly. We leverage the interplay between
part shape, connectivity, and motion using a denoising diffusion-based method
with attention modules designed to extract correlations between part
attributes. Our method takes an object category label and a part connectivity
graph as input and generates an object's geometry and motion parameters. The
generated objects conform to user-specified constraints on the object category,
part shape, and part articulation. Our experiments show that our method
outperforms the state-of-the-art in articulated object generation, producing
more realistic objects while conforming better to user constraints.
</p>
<p>Video Summary at: <a href="http://youtu.be/cH_rbKbyTpE">this http URL</a>
</p></li>
</ul>

<h3>Title: Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models. (arXiv:2312.09608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09608">http://arxiv.org/abs/2312.09608</a></li>
<li>Code URL: <a href="https://github.com/hutaihang/faster-diffusion">https://github.com/hutaihang/faster-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09608]] Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models(http://arxiv.org/abs/2312.09608)</code></li>
<li>Summary: <p>One of the key components within diffusion models is the UNet for noise
prediction. While several works have explored basic properties of the UNet
decoder, its encoder largely remains unexplored. In this work, we conduct the
first comprehensive study of the UNet encoder. We empirically analyze the
encoder features and provide insights to important questions regarding their
changes at the inference process. In particular, we find that encoder features
change gently, whereas the decoder features exhibit substantial variations
across different time-steps. This finding inspired us to omit the encoder at
certain adjacent time-steps and reuse cyclically the encoder features in the
previous time-steps for the decoder. Further based on this observation, we
introduce a simple yet effective encoder propagation scheme to accelerate the
diffusion sampling for a diverse set of tasks. By benefiting from our
propagation scheme, we are able to perform in parallel the decoder at certain
adjacent time-steps. Additionally, we introduce a prior noise injection method
to improve the texture details in the generated image. Besides the standard
text-to-image task, we also validate our approach on other tasks:
text-to-video, personalized generation and reference-guided generation. Without
utilizing any knowledge distillation technique, our approach accelerates both
the Stable Diffusion (SD) and the DeepFloyd-IF models sampling by 41$\%$ and
24$\%$ respectively, while maintaining high-quality generation performance. Our
code is available in
\href{https://github.com/hutaiHang/Faster-Diffusion}{FasterDiffusion}.
</p></li>
</ul>

<h3>Title: DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models. (arXiv:2312.09767v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09767">http://arxiv.org/abs/2312.09767</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09767]] DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models(http://arxiv.org/abs/2312.09767)</code></li>
<li>Summary: <p>Diffusion models have shown remarkable success in a variety of downstream
generative tasks, yet remain under-explored in the important and challenging
expressive talking head generation. In this work, we propose a DreamTalk
framework to fulfill this gap, which employs meticulous design to unlock the
potential of diffusion models in generating expressive talking heads.
Specifically, DreamTalk consists of three crucial components: a denoising
network, a style-aware lip expert, and a style predictor. The diffusion-based
denoising network is able to consistently synthesize high-quality audio-driven
face motions across diverse expressions. To enhance the expressiveness and
accuracy of lip motions, we introduce a style-aware lip expert that can guide
lip-sync while being mindful of the speaking styles. To eliminate the need for
expression reference video or text, an extra diffusion-based style predictor is
utilized to predict the target expression directly from the audio. By this
means, DreamTalk can harness powerful diffusion models to generate expressive
faces effectively and reduce the reliance on expensive style references.
Experimental results demonstrate that DreamTalk is capable of generating
photo-realistic talking faces with diverse speaking styles and achieving
accurate lip motions, surpassing existing state-of-the-art counterparts.
</p></li>
</ul>

<h3>Title: Latent Diffusion Models with Image-Derived Annotations for Enhanced AI-Assisted Cancer Diagnosis in Histopathology. (arXiv:2312.09792v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09792">http://arxiv.org/abs/2312.09792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09792]] Latent Diffusion Models with Image-Derived Annotations for Enhanced AI-Assisted Cancer Diagnosis in Histopathology(http://arxiv.org/abs/2312.09792)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) based image analysis has an immense potential to
support diagnostic histopathology, including cancer diagnostics. However,
developing supervised AI methods requires large-scale annotated datasets. A
potentially powerful solution is to augment training data with synthetic data.
Latent diffusion models, which can generate high-quality, diverse synthetic
images, are promising. However, the most common implementations rely on
detailed textual descriptions, which are not generally available in this
domain. This work proposes a method that constructs structured textual prompts
from automatically extracted image features. We experiment with the PCam
dataset, composed of tissue patches only loosely annotated as healthy or
cancerous. We show that including image-derived features in the prompt, as
opposed to only healthy and cancerous labels, improves the Fr\'echet Inception
Distance (FID) from 178.8 to 90.2. We also show that pathologists find it
challenging to detect synthetic images, with a median sensitivity/specificity
of 0.55/0.55. Finally, we show that synthetic data effectively trains AI
models.
</p></li>
</ul>

<h3>Title: Socio-Economic Deprivation Analysis: Diffusion Maps. (arXiv:2312.09830v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09830">http://arxiv.org/abs/2312.09830</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09830]] Socio-Economic Deprivation Analysis: Diffusion Maps(http://arxiv.org/abs/2312.09830)</code></li>
<li>Summary: <p>This report proposes a model to predict the location of the most deprived
areas in a city using data from the census. A census data is very high
dimensional and needs to be simplified. We use a novel algorithm to reduce
dimensionality and find patterns: The diffusion map. Features are defined by
eigenvectors of the Laplacian matrix that defines the diffusion map.
Eigenvectors corresponding to the smallest eigenvalues indicate specific
population features. Previous work has found qualitatively that the second most
important dimension for describing the census data in Bristol is linked to
deprivation. In this report, we analyse how good this dimension is as a model
for predicting deprivation by comparing with the recognised measures. The
Pearson correlation coefficient was found to be over 0.7. The top 10 per cent
of deprived areas in the UK which also locate in Bristol are extracted to test
the accuracy of the model. There are 52 most deprived areas, and 38 areas are
correctly identified by comparing to the model. The influence of scores of IMD
domains that do not correlate with the models, Eigenvector 2 entries of
non-deprived OAs and orthogonality of Eigenvectors cause the model to fail the
prediction of 14 deprived areas.
</p>
<p>However, overall, the model shows a high performance to predict the future
deprivation of overall areas where the project considers. This project is
expected to support the government to allocate resources and funding.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Weight subcloning: direct initialization of transformers using larger pretrained ones. (arXiv:2312.09299v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09299">http://arxiv.org/abs/2312.09299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09299]] Weight subcloning: direct initialization of transformers using larger pretrained ones(http://arxiv.org/abs/2312.09299)</code></li>
<li>Summary: <p>Training large transformer models from scratch for a target task requires
lots of data and is computationally demanding. The usual practice of transfer
learning overcomes this challenge by initializing the model with weights of a
pretrained model of the same size and specification to increase the convergence
and training speed. However, what if no pretrained model of the required size
is available? In this paper, we introduce a simple yet effective technique to
transfer the knowledge of a pretrained model to smaller variants. Our approach
called weight subcloning expedites the training of scaled-down transformers by
initializing their weights from larger pretrained models.
</p>
<p>Weight subcloning involves an operation on the pretrained model to obtain the
equivalent initialized scaled-down model. It consists of two key steps: first,
we introduce neuron importance ranking to decrease the embedding dimension per
layer in the pretrained model. Then, we remove blocks from the transformer
model to match the number of layers in the scaled-down network. The result is a
network ready to undergo training, which gains significant improvements in
training speed compared to random initialization. For instance, we achieve 4x
faster training for vision transformers in image classification and language
models designed for next token prediction.
</p></li>
</ul>

<h3>Title: Multiscale Vision Transformer With Deep Clustering-Guided Refinement for Weakly Supervised Object Localization. (arXiv:2312.09584v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09584">http://arxiv.org/abs/2312.09584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09584]] Multiscale Vision Transformer With Deep Clustering-Guided Refinement for Weakly Supervised Object Localization(http://arxiv.org/abs/2312.09584)</code></li>
<li>Summary: <p>This work addresses the task of weakly-supervised object localization. The
goal is to learn object localization using only image-level class labels, which
are much easier to obtain compared to bounding box annotations. This task is
important because it reduces the need for labor-intensive ground-truth
annotations. However, methods for object localization trained using weak
supervision often suffer from limited accuracy in localization. To address this
challenge and enhance localization accuracy, we propose a multiscale object
localization transformer (MOLT). It comprises multiple object localization
transformers that extract patch embeddings across various scales. Moreover, we
introduce a deep clustering-guided refinement method that further enhances
localization accuracy by utilizing separately extracted image segments. These
segments are obtained by clustering pixels using convolutional neural networks.
Finally, we demonstrate the effectiveness of our proposed method by conducting
experiments on the publicly available ILSVRC-2012 dataset.
</p></li>
</ul>

<h3>Title: Part Representation Learning with Teacher-Student Decoder for Occluded Person Re-identification. (arXiv:2312.09797v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09797">http://arxiv.org/abs/2312.09797</a></li>
<li>Code URL: <a href="https://github.com/hh23333/tsd">https://github.com/hh23333/tsd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09797]] Part Representation Learning with Teacher-Student Decoder for Occluded Person Re-identification(http://arxiv.org/abs/2312.09797)</code></li>
<li>Summary: <p>Occluded person re-identification (ReID) is a very challenging task due to
the occlusion disturbance and incomplete target information. Leveraging
external cues such as human pose or parsing to locate and align part features
has been proven to be very effective in occluded person ReID. Meanwhile, recent
Transformer structures have a strong ability of long-range modeling.
Considering the above facts, we propose a Teacher-Student Decoder (TSD)
framework for occluded person ReID, which utilizes the Transformer decoder with
the help of human parsing. More specifically, our proposed TSD consists of a
Parsing-aware Teacher Decoder (PTD) and a Standard Student Decoder (SSD). PTD
employs human parsing cues to restrict Transformer's attention and imparts this
information to SSD through feature distillation. Thereby, SSD can learn from
PTD to aggregate information of body parts automatically. Moreover, a mask
generator is designed to provide discriminative regions for better ReID. In
addition, existing occluded person ReID benchmarks utilize occluded samples as
queries, which will amplify the role of alleviating occlusion interference and
underestimate the impact of the feature absence issue. Contrastively, we
propose a new benchmark with non-occluded queries, serving as a complement to
the existing benchmark. Extensive experiments demonstrate that our proposed
method is superior and the new benchmark is essential. The source codes are
available at https://github.com/hh23333/TSD.
</p></li>
</ul>

<h3>Title: DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing. (arXiv:2312.09955v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09955">http://arxiv.org/abs/2312.09955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09955]] DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing(http://arxiv.org/abs/2312.09955)</code></li>
<li>Summary: <p>Images acquired in hazy conditions have degradations induced in them.
Dehazing such images is a vexed and ill-posed problem. Scores of prior-based
and learning-based approaches have been proposed to mitigate the effect of haze
and generate haze-free images. Many conventional methods are constrained by
their lack of awareness regarding scene depth and their incapacity to capture
long-range dependencies. In this paper, a method that uses residual learning
and vision transformers in an attention module is proposed. It essentially
comprises two networks: In the first one, the network takes the ratio of a hazy
image and the approximated transmission matrix to estimate a residual map. The
second network takes this residual image as input and passes it through
convolution layers before superposing it on the generated feature maps. It is
then passed through global context and depth-aware transformer encoders to
obtain channel attention. The attention module then infers the spatial
attention map before generating the final haze-free image. Experimental
results, including several quantitative metrics, demonstrate the efficiency and
scalability of the suggested methodology.
</p></li>
</ul>

<h3>Title: Point Transformer V3: Simpler, Faster, Stronger. (arXiv:2312.10035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10035">http://arxiv.org/abs/2312.10035</a></li>
<li>Code URL: <a href="https://github.com/pointcept/pointtransformerv3">https://github.com/pointcept/pointtransformerv3</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10035]] Point Transformer V3: Simpler, Faster, Stronger(http://arxiv.org/abs/2312.10035)</code></li>
<li>Summary: <p>This paper is not motivated to seek innovation within the attention
mechanism. Instead, it focuses on overcoming the existing trade-offs between
accuracy and efficiency within the context of point cloud processing,
leveraging the power of scale. Drawing inspiration from recent advances in 3D
large-scale representation learning, we recognize that model performance is
more influenced by scale than by intricate design. Therefore, we present Point
Transformer V3 (PTv3), which prioritizes simplicity and efficiency over the
accuracy of certain mechanisms that are minor to the overall performance after
scaling, such as replacing the precise neighbor search by KNN with an efficient
serialized neighbor mapping of point clouds organized with specific patterns.
This principle enables significant scaling, expanding the receptive field from
16 to 1024 points while remaining efficient (a 3x increase in processing speed
and a 10x improvement in memory efficiency compared with its predecessor,
PTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that
span both indoor and outdoor scenarios. Further enhanced with multi-dataset
joint training, PTv3 pushes these results to a higher level.
</p></li>
</ul>

<h3>Title: MANTIS at #SMM4H 2023: Leveraging Hybrid and Ensemble Models for Detection of Social Anxiety Disorder on Reddit. (arXiv:2312.09451v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09451">http://arxiv.org/abs/2312.09451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09451]] MANTIS at #SMM4H 2023: Leveraging Hybrid and Ensemble Models for Detection of Social Anxiety Disorder on Reddit(http://arxiv.org/abs/2312.09451)</code></li>
<li>Summary: <p>This paper presents our system employed for the Social Media Mining for
Health 2023 Shared Task 4: Binary classification of English Reddit posts
self-reporting a social anxiety disorder diagnosis. We systematically
investigate and contrast the efficacy of hybrid and ensemble models that
harness specialized medical domain-adapted transformers in conjunction with
BiLSTM neural networks. The evaluation results outline that our best performing
model obtained 89.31% F1 on the validation set and 83.76% F1 on the test set.
</p></li>
</ul>

<h3>Title: Picking the Underused Heads: A Network Pruning Perspective of Attention Head Selection for Fusing Dialogue Coreference Information. (arXiv:2312.09541v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09541">http://arxiv.org/abs/2312.09541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09541]] Picking the Underused Heads: A Network Pruning Perspective of Attention Head Selection for Fusing Dialogue Coreference Information(http://arxiv.org/abs/2312.09541)</code></li>
<li>Summary: <p>The Transformer-based models with the multi-head self-attention mechanism are
widely used in natural language processing, and provide state-of-the-art
results. While the pre-trained language backbones are shown to implicitly
capture certain linguistic knowledge, explicitly incorporating structure-aware
features can bring about further improvement on the downstream tasks. However,
such enhancement often requires additional neural components and increases
training parameter size. In this work, we investigate the attention head
selection and manipulation strategy for feature injection from a network
pruning perspective, and conduct a case study on dialogue summarization. We
first rank attention heads in a Transformer-based summarizer with layer-wise
importance. We then select the underused heads through extensive analysis, and
inject structure-aware features by manipulating the selected heads.
Experimental results show that the importance-based head selection is effective
for feature injection, and dialogue summarization can be improved by
incorporating coreference information via head manipulation.
</p></li>
</ul>

<h3>Title: Exploring Automatic Text Simplification of German Narrative Documents. (arXiv:2312.09907v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09907">http://arxiv.org/abs/2312.09907</a></li>
<li>Code URL: <a href="https://github.com/tschomacker/aligned-narrative-documents">https://github.com/tschomacker/aligned-narrative-documents</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09907]] Exploring Automatic Text Simplification of German Narrative Documents(http://arxiv.org/abs/2312.09907)</code></li>
<li>Summary: <p>In this paper, we apply transformer-based Natural Language Generation (NLG)
techniques to the problem of text simplification. Currently, there are only a
few German datasets available for text simplification, even fewer with larger
and aligned documents, and not a single one with narrative texts. In this
paper, we explore to which degree modern NLG techniques can be applied to
German narrative text simplifications. We use Longformer attention and a
pre-trained mBART model. Our findings indicate that the existing approaches for
German are not able to solve the task properly. We conclude on a few directions
for future research to address this problem.
</p></li>
</ul>

<h3>Title: LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language. (arXiv:2312.09993v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09993">http://arxiv.org/abs/2312.09993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09993]] LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language(http://arxiv.org/abs/2312.09993)</code></li>
<li>Summary: <p>Large Language Models represent state-of-the-art linguistic models designed
to equip computers with the ability to comprehend natural language. With its
exceptional capacity to capture complex contextual relationships, the LLaMA
(Large Language Model Meta AI) family represents a novel advancement in the
field of natural language processing by releasing foundational models designed
to improve the natural language understanding abilities of the transformer
architecture thanks to their large amount of trainable parameters (7, 13, and
70 billion parameters). In many natural language understanding tasks, these
models obtain the same performances as private company models such as OpenAI
Chat-GPT with the advantage to make publicly available weights and code for
research and commercial uses. In this work, we investigate the possibility of
Language Adaptation for LLaMA models, explicitly focusing on addressing the
challenge of Italian Language coverage. Adopting an open science approach, we
explore various tuning approaches to ensure a high-quality text generated in
Italian suitable for common tasks in this underrepresented language in the
original models' datasets. We aim to release effective text generation models
with strong linguistic properties for many tasks that seem challenging using
multilingual or general-purpose LLMs. By leveraging an open science philosophy,
this study contributes to Language Adaptation strategies for the Italian
language by introducing the novel LLaMAntino family of Italian LLMs.
</p></li>
</ul>

<h3>Title: Accelerating Neural Network Training: A Brief Review. (arXiv:2312.10024v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10024">http://arxiv.org/abs/2312.10024</a></li>
<li>Code URL: <a href="https://github.com/sahilnokhwal/annt">https://github.com/sahilnokhwal/annt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10024]] Accelerating Neural Network Training: A Brief Review(http://arxiv.org/abs/2312.10024)</code></li>
<li>Summary: <p>The process of training a deep neural network is characterized by significant
time requirements and associated costs. Although researchers have made
considerable progress in this area, further work is still required due to
resource constraints. This study examines innovative approaches to expedite the
training process of deep neural networks (DNN), with specific emphasis on three
state-of-the-art models such as ResNet50, Vision Transformer (ViT), and
EfficientNet. The research utilizes sophisticated methodologies, including
Gradient Accumulation (GA), Automatic Mixed Precision (AMP), and Pin Memory
(PM), in order to optimize performance and accelerate the training procedure.
</p>
<p>The study examines the effects of these methodologies on the DNN models
discussed earlier, assessing their efficacy with regard to training rate and
computational efficacy. The study showcases the efficacy of including GA as a
strategic approach, resulting in a noteworthy decrease in the duration required
for training. This enables the models to converge at a faster pace. The
utilization of AMP enhances the speed of computations by taking advantage of
the advantages offered by lower precision arithmetic while maintaining the
correctness of the model.
</p>
<p>Furthermore, this study investigates the application of Pin Memory as a
strategy to enhance the efficiency of data transmission between the central
processing unit and the graphics processing unit, thereby offering a promising
opportunity for enhancing overall performance. The experimental findings
demonstrate that the combination of these sophisticated methodologies
significantly accelerates the training of DNNs, offering vital insights for
experts seeking to improve the effectiveness of deep learning processes.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Image Deblurring using GAN. (arXiv:2312.09496v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09496">http://arxiv.org/abs/2312.09496</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09496]] Image Deblurring using GAN(http://arxiv.org/abs/2312.09496)</code></li>
<li>Summary: <p>In recent years, deep generative models, such as Generative Adversarial
Network (GAN), has grabbed significant attention in the field of computer
vision. This project focuses on the application of GAN in image deblurring with
the aim of generating clearer images from blurry inputs caused by factors such
as motion blur. However, traditional image restoration techniques have
limitations in handling complex blurring patterns. Hence, a GAN-based framework
is proposed as a solution to generate high-quality deblurred images. The
project defines a GAN model in Tensorflow and trains it with GoPRO dataset. The
Generator will intake blur images directly to create fake images to convince
the Discriminator which will receive clear images at the same time and
distinguish between the real image and the fake image. After obtaining the
trained parameters, the model was used to deblur motion-blur images taken in
daily life as well as testing set for validation. The result shows that the
pretrained network of GAN can obtain sharper pixels in image, achieving an
average of 29.3 Peak Signal-to-Noise Ratio (PSNR) and 0.72 Structural
Similarity Assessment (SSIM). This help to effectively address the challenges
posed by image blurring, leading to the generation of visually pleasing and
sharp images. By exploiting the adversarial learning framework, the proposed
approach enhances the potential for real-world applications in image
restoration.
</p></li>
</ul>

<h3>Title: Fast Sampling generative model for Ultrasound image reconstruction. (arXiv:2312.09510v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09510">http://arxiv.org/abs/2312.09510</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09510]] Fast Sampling generative model for Ultrasound image reconstruction(http://arxiv.org/abs/2312.09510)</code></li>
<li>Summary: <p>Image reconstruction from radio-frequency data is pivotal in ultrafast plane
wave ultrasound imaging. Unlike the conventional delay-and-sum (DAS) technique,
which relies on somewhat imprecise assumptions, deep learning-based methods
perform image reconstruction by training on paired data, leading to a notable
enhancement in image quality. Nevertheless, these strategies often exhibit
limited generalization capabilities. Recently, denoising diffusion models have
become the preferred paradigm for image reconstruction tasks. However, their
reliance on an iterative sampling procedure results in prolonged generation
time. In this paper, we propose a novel sampling framework that concurrently
enforces data consistency of ultrasound signals and data-driven priors. By
leveraging the advanced diffusion model, the generation of high-quality images
is substantially expedited. Experimental evaluations on an in-vivo dataset
indicate that our approach with a single plane wave surpasses DAS with spatial
coherent compounding of 75 plane waves.
</p></li>
</ul>

<h3>Title: Style Generation in Robot Calligraphy with Deep Generative Adversarial Networks. (arXiv:2312.09673v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09673">http://arxiv.org/abs/2312.09673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09673]] Style Generation in Robot Calligraphy with Deep Generative Adversarial Networks(http://arxiv.org/abs/2312.09673)</code></li>
<li>Summary: <p>Robot calligraphy is an emerging exploration of artificial intelligence in
the fields of art and education. Traditional calligraphy generation researches
mainly focus on methods such as tool-based image processing, generative models,
and style transfer. Unlike the English alphabet, the number of Chinese
characters is tens of thousands, which leads to difficulties in the generation
of a style consistent Chinese calligraphic font with over 6000 characters. Due
to the lack of high-quality data sets, formal definitions of calligraphy
knowledge, and scientific art evaluation methods, The results generated are
frequently of low quality and falls short of professional-level requirements.
To address the above problem, this paper proposes an automatic calligraphy
generation model based on deep generative adversarial networks (deepGAN) that
can generate style calligraphy fonts with professional standards. The key
highlights of the proposed method include: (1) The datasets use a
high-precision calligraphy synthesis method to ensure its high quality and
sufficient quantity; (2) Professional calligraphers are invited to conduct a
series of Turing tests to evaluate the gap between model generation results and
human artistic level; (3) Experimental results indicate that the proposed model
is the state-of-the-art among current calligraphy generation methods. The
Turing tests and similarity evaluations validate the effectiveness of the
proposed method.
</p></li>
</ul>

<h3>Title: GSQA: An End-to-End Model for Generative Spoken Question Answering. (arXiv:2312.09781v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09781">http://arxiv.org/abs/2312.09781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09781]] GSQA: An End-to-End Model for Generative Spoken Question Answering(http://arxiv.org/abs/2312.09781)</code></li>
<li>Summary: <p>In recent advancements in spoken question answering (QA), end-to-end models
have made significant strides. However, previous research has primarily focused
on extractive span selection. While this extractive-based approach is effective
when answers are present directly within the input, it falls short in
addressing abstractive questions, where answers are not directly extracted but
inferred from the given information. To bridge this gap, we introduce the first
end-to-end Generative Spoken Question Answering (GSQA) model that empowers the
system to engage in abstractive reasoning. The challenge in training our GSQA
model lies in the absence of a spoken abstractive QA dataset. We propose using
text models for initialization and leveraging the extractive QA dataset to
transfer knowledge from the text generative model to the spoken generative
model. Experimental results indicate that our model surpasses the previous
extractive model by 3% on extractive QA datasets. Furthermore, the GSQA model
has only been fine-tuned on the spoken extractive QA dataset. Despite not
having seen any spoken abstractive QA data, it can still closely match the
performance of the cascade model. In conclusion, our GSQA model shows the
potential to generalize to a broad spectrum of questions, thus further
expanding spoken question answering capabilities of abstractive QA. Our code is
available at
\href{https://voidful.github.io/GSQA}{https://voidful.github.io/GSQA}
</p></li>
</ul>

<h3>Title: Generative Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2312.09895v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09895">http://arxiv.org/abs/2312.09895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09895]] Generative Context-aware Fine-tuning of Self-supervised Speech Models(http://arxiv.org/abs/2312.09895)</code></li>
<li>Summary: <p>When performing tasks like automatic speech recognition or spoken language
understanding for a given utterance, access to preceding text or audio provides
contextual information can improve performance. Considering the recent advances
in generative large language models (LLM), we hypothesize that an LLM could
generate useful context information using the preceding text. With appropriate
prompts, LLM could generate a prediction of the next sentence or abstractive
text like titles or topics. In this paper, we study the use of LLM-generated
context information and propose an approach to distill the generated
information during fine-tuning of self-supervised speech models, which we refer
to as generative context-aware fine-tuning. This approach allows the fine-tuned
model to make improved predictions without access to the true surrounding
segments or to the LLM at inference time, while requiring only a very small
additional context module. We evaluate the proposed approach using the SLUE and
Libri-light benchmarks for several downstream tasks: automatic speech
recognition, named entity recognition, and sentiment analysis. The results show
that generative context-aware fine-tuning outperforms a context injection
fine-tuning approach that accesses the ground-truth previous text, and is
competitive with a generative context injection fine-tuning approach that
requires the LLM at inference time.
</p></li>
</ul>

<h3>Title: Unbiasing Enhanced Sampling on a High-dimensional Free Energy Surface with Deep Generative Model. (arXiv:2312.09404v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09404">http://arxiv.org/abs/2312.09404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09404]] Unbiasing Enhanced Sampling on a High-dimensional Free Energy Surface with Deep Generative Model(http://arxiv.org/abs/2312.09404)</code></li>
<li>Summary: <p>Biased enhanced sampling methods utilizing collective variables (CVs) are
powerful tools for sampling conformational ensembles. Due to high intrinsic
dimensions, efficiently generating conformational ensembles for complex systems
requires enhanced sampling on high-dimensional free energy surfaces. While
methods like temperature-accelerated molecular dynamics (TAMD) can adopt many
CVs in a simulation, unbiasing the simulation requires accurate modeling of a
high-dimensional CV probability distribution, which is challenging for
traditional density estimation techniques. Here we propose an unbiasing method
based on the score-based diffusion model, a deep generative learning method
that excels in density estimation across complex data landscapes. We test the
score-based diffusion unbiasing method on TAMD simulations. The results
demonstrate that this unbiasing approach significantly outperforms traditional
unbiasing methods, and can generate accurate unbiased conformational ensembles
for simulations with a number of CVs higher than usual ranges.
</p></li>
</ul>

<h3>Title: Automating reward function configuration for drug design. (arXiv:2312.09865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09865">http://arxiv.org/abs/2312.09865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09865]] Automating reward function configuration for drug design(http://arxiv.org/abs/2312.09865)</code></li>
<li>Summary: <p>Designing reward functions that guide generative molecular design (GMD)
algorithms to desirable areas of chemical space is of critical importance in
AI-driven drug discovery. Traditionally, this has been a manual and error-prone
task; the selection of appropriate computational methods to approximate
biological assays is challenging and the aggregation of computed values into a
single score even more so, leading to potential reliance on trial-and-error
approaches. We propose a novel approach for automated reward configuration that
relies solely on experimental data, mitigating the challenges of manual reward
adjustment on drug discovery projects. Our method achieves this by constructing
a ranking over experimental data based on Pareto dominance over the
multi-objective space, then training a neural network to approximate the reward
function such that rankings determined by the predicted reward correlate with
those determined by the Pareto dominance relation. We validate our method using
two case studies. In the first study we simulate Design-Make-Test-Analyse
(DMTA) cycles by alternating reward function updates and generative runs guided
by that function. We show that the learned function adapts over time to yield
compounds that score highly with respect to evaluation functions taken from the
literature. In the second study we apply our algorithm to historical data from
four real drug discovery projects. We show that our algorithm yields reward
functions that outperform the predictive accuracy of human-defined functions,
achieving an improvement of up to 0.4 in Spearman's correlation against a
ground truth evaluation function that encodes the target drug profile for that
project. Our method provides an efficient data-driven way to configure reward
functions for GMD, and serves as a strong baseline for future research into
transformative approaches for the automation of drug discovery.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Osprey: Pixel Understanding with Visual Instruction Tuning. (arXiv:2312.10032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10032">http://arxiv.org/abs/2312.10032</a></li>
<li>Code URL: <a href="https://github.com/circleradon/osprey">https://github.com/circleradon/osprey</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10032]] Osprey: Pixel Understanding with Visual Instruction Tuning(http://arxiv.org/abs/2312.10032)</code></li>
<li>Summary: <p>Multimodal large language models (MLLMs) have recently achieved impressive
general-purpose vision-language capabilities through visual instruction tuning.
However, current MLLMs primarily focus on image-level or box-level
understanding, falling short of achieving fine-grained vision-language
alignment at the pixel level. Besides, the lack of mask-based instruction data
limits their advancements. In this paper, we propose Osprey, a mask-text
instruction tuning approach, to extend MLLMs by incorporating fine-grained mask
regions into language instruction, aiming at achieving pixel-wise visual
understanding. To achieve this goal, we first meticulously curate a mask-based
region-text dataset with 724K samples, and then design a vision-language model
by injecting pixel-level representation into LLM. Especially, Osprey adopts a
convolutional CLIP backbone as the vision encoder and employs a mask-aware
visual extractor to extract precise visual mask features from high resolution
input. Experimental results demonstrate Osprey's superiority in various region
understanding tasks, showcasing its new capability for pixel-level instruction
tuning. In particular, Osprey can be integrated with Segment Anything Model
(SAM) seamlessly to obtain multi-granularity semantics. The source code,
dataset and demo can be found at https://github.com/CircleRadon/Osprey.
</p></li>
</ul>

<h3>Title: Self-Evaluation Improves Selective Generation in Large Language Models. (arXiv:2312.09300v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09300">http://arxiv.org/abs/2312.09300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09300]] Self-Evaluation Improves Selective Generation in Large Language Models(http://arxiv.org/abs/2312.09300)</code></li>
<li>Summary: <p>Safe deployment of large language models (LLMs) may benefit from a reliable
method for assessing their generated content to determine when to abstain or to
selectively generate. While likelihood-based metrics such as perplexity are
widely employed, recent research has demonstrated the limitations of using
sequence-level probability estimates given by LLMs as reliable indicators of
generation quality. Conversely, LLMs have demonstrated strong calibration at
the token level, particularly when it comes to choosing correct answers in
multiple-choice questions or evaluating true/false statements. In this work, we
reformulate open-ended generation tasks into token-level prediction tasks, and
leverage LLMs' superior calibration at the token level. We instruct an LLM to
self-evaluate its answers, employing either a multi-way comparison or a
point-wise evaluation approach, with the option to include a ``None of the
above'' option to express the model's uncertainty explicitly. We benchmark a
range of scoring methods based on self-evaluation and evaluate their
performance in selective generation using TruthfulQA and TL;DR. Through
experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based
scores not only improve accuracy, but also correlate better with the overall
quality of generated content.
</p></li>
</ul>

<h3>Title: Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM. (arXiv:2312.09366v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09366">http://arxiv.org/abs/2312.09366</a></li>
<li>Code URL: <a href="https://github.com/mbzuai-oryx/climategpt">https://github.com/mbzuai-oryx/climategpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09366]] Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM(http://arxiv.org/abs/2312.09366)</code></li>
<li>Summary: <p>Climate change is one of the most significant challenges we face together as
a society. Creating awareness and educating policy makers the wide-ranging
impact of climate change is an essential step towards a sustainable future.
Recently, Large Language Models (LLMs) like ChatGPT and Bard have shown
impressive conversational abilities and excel in a wide variety of NLP tasks.
While these models are close-source, recently alternative open-source LLMs such
as Stanford Alpaca and Vicuna have shown promising results. However, these
open-source models are not specifically tailored for climate related domain
specific information and also struggle to generate meaningful responses in
other languages such as, Arabic. To this end, we propose a light-weight Arabic
Mini-ClimateGPT that is built on an open-source LLM and is specifically
fine-tuned on a conversational-style instruction tuning curated Arabic dataset
Clima500-Instruct with over 500k instructions about climate change and
sustainability. Further, our model also utilizes a vector embedding based
retrieval mechanism during inference. We validate our proposed model through
quantitative and qualitative evaluations on climate-related queries. Our model
surpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation.
Furthermore, our human expert evaluation reveals an 81.6% preference for our
model's responses over multiple popular open-source models. Our open-source
demos, code-base and models are available here
https://github.com/mbzuai-oryx/ClimateGPT.
</p></li>
</ul>

<h3>Title: Marathon: A Race Through the Realm of Long Context with Large Language Models. (arXiv:2312.09542v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09542">http://arxiv.org/abs/2312.09542</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09542]] Marathon: A Race Through the Realm of Long Context with Large Language Models(http://arxiv.org/abs/2312.09542)</code></li>
<li>Summary: <p>Although there are currently many benchmarks available for evaluating the
long context understanding and reasoning capability of large language models,
with the expansion of the context window in these models, the existing long
context benchmarks are no longer sufficient for evaluating the long context
understanding and reasoning capability of large language models. In this paper,
we have developed a fresh long context evaluation benchmark, which we name it
Marathon in the form of multiple choice questions, inspired by benchmarks such
as MMLU, for assessing the long context comprehension capability of large
language models quickly, accurately, and objectively. We have evaluated several
of the latest and most popular large language models, as well as three recent
and effective long context optimization methods, on our benchmark. This
showcases the long context reasoning and comprehension capabilities of these
large language models and validates the effectiveness of these optimization
methods. Marathon is available at
https://huggingface.co/datasets/Lemoncoke/Marathon.
</p></li>
</ul>

<h3>Title: GPT-4 Surpassing Human Performance in Linguistic Pragmatics. (arXiv:2312.09545v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09545">http://arxiv.org/abs/2312.09545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09545]] GPT-4 Surpassing Human Performance in Linguistic Pragmatics(http://arxiv.org/abs/2312.09545)</code></li>
<li>Summary: <p>As Large Language Models (LLMs) become increasingly integrated into everyday
life, their capabilities to understand and emulate human cognition are under
steady examination. This study investigates the ability of LLMs to comprehend
and interpret linguistic pragmatics, an aspect of communication that considers
context and implied meanings. Using Grice's communication principles, LLMs and
human subjects (N=76) were evaluated based on their responses to various
dialogue-based tasks. The findings revealed the superior performance and speed
of LLMs, particularly GPT4, over human subjects in interpreting pragmatics.
GPT4 also demonstrated accuracy in the pre-testing of human-written samples,
indicating its potential in text analysis. In a comparative analysis of LLMs
using human individual and average scores, the models exhibited significant
chronological improvement. The models were ranked from lowest to highest score,
with GPT2 positioned at 78th place, GPT3 ranking at 23rd, Bard at 10th, GPT3.5
placing 5th, Best Human scoring 2nd, and GPT4 achieving the top spot. The
findings highlight the remarkable progress made in the development and
performance of these LLMs. Future studies should consider diverse subjects,
multiple languages, and other cognitive aspects to fully comprehend the
capabilities of LLMs. This research holds significant implications for the
development and application of AI-based models in communication-centered
sectors.
</p></li>
</ul>

<h3>Title: Extending Context Window of Large Language Models via Semantic Compression. (arXiv:2312.09571v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09571">http://arxiv.org/abs/2312.09571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09571]] Extending Context Window of Large Language Models via Semantic Compression(http://arxiv.org/abs/2312.09571)</code></li>
<li>Summary: <p>Transformer-based Large Language Models (LLMs) often impose limitations on
the length of the text input to ensure the generation of fluent and relevant
responses. This constraint restricts their applicability in scenarios involving
long texts. We propose a novel semantic compression method that enables
generalization to texts that are 6-8 times longer, without incurring
significant computational costs or requiring fine-tuning. Our proposed
framework draws inspiration from source coding in information theory and
employs a pre-trained model to reduce the semantic redundancy of long inputs
before passing them to the LLMs for downstream tasks. Experimental results
demonstrate that our method effectively extends the context window of LLMs
across a range of tasks including question answering, summarization, few-shot
learning, and information retrieval. Furthermore, the proposed semantic
compression method exhibits consistent fluency in text generation while
reducing the associated computational overhead.
</p></li>
</ul>

<h3>Title: Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models. (arXiv:2312.09601v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09601">http://arxiv.org/abs/2312.09601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09601]] Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models(http://arxiv.org/abs/2312.09601)</code></li>
<li>Summary: <p>Binary code summarization, while invaluable for understanding code semantics,
is challenging due to its labor-intensive nature. This study delves into the
potential of large language models (LLMs) for binary code comprehension. To
this end, we present BinSum, a comprehensive benchmark and dataset of over 557K
binary functions and introduce a novel method for prompt synthesis and
optimization. To more accurately gauge LLM performance, we also propose a new
semantic similarity metric that surpasses traditional exact-match approaches.
Our extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2,
and Code Llama, reveals 10 pivotal insights. This evaluation generates 4
billion inference tokens, incurred a total expense of 11,418 US dollars and 873
NVIDIA A100 GPU hours. Our findings highlight both the transformative potential
of LLMs in this field and the challenges yet to be overcome.
</p></li>
</ul>

<h3>Title: RJUA-QA: A Comprehensive QA Dataset for Urology. (arXiv:2312.09785v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09785">http://arxiv.org/abs/2312.09785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09785]] RJUA-QA: A Comprehensive QA Dataset for Urology(http://arxiv.org/abs/2312.09785)</code></li>
<li>Summary: <p>We introduce RJUA-QA, a novel medical dataset for question answering (QA) and
reasoning with clinical evidence, contributing to bridge the gap between
general large language models (LLMs) and medical-specific LLM applications.
RJUA-QA is derived from realistic clinical scenarios and aims to facilitate
LLMs in generating reliable diagnostic and advice. The dataset contains 2,132
curated Question-Context-Answer pairs, corresponding about 25,000 diagnostic
records and clinical cases. The dataset covers 67 common urological disease
categories, where the disease coverage exceeds 97.6\% of the population seeking
medical services in urology. Each data instance in RJUA-QA comprises: (1) a
question mirroring real patient to inquiry about clinical symptoms and medical
conditions, (2) a context including comprehensive expert knowledge, serving as
a reference for medical examination and diagnosis, (3) a doctor response
offering the diagnostic conclusion and suggested examination guidance, (4) a
diagnosed clinical disease as the recommended diagnostic outcome, and (5)
clinical advice providing recommendations for medical examination. RJUA-QA is
the first medical QA dataset for clinical reasoning over the patient inquiries,
where expert-level knowledge and experience are required for yielding
diagnostic conclusions and medical examination advice. A comprehensive
evaluation is conducted to evaluate the performance of both medical-specific
and general LLMs on the RJUA-QA dataset.
</p></li>
</ul>

<h3>Title: ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs). (arXiv:2312.09801v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09801">http://arxiv.org/abs/2312.09801</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09801]] ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)(http://arxiv.org/abs/2312.09801)</code></li>
<li>Summary: <p>We introduce a novel writing method called Probing Chain of Thought (ProCoT),
which prevents students from cheating using a Large Language Model (LLM), such
as ChatGPT, while enhancing their active learning through such models. LLMs
have disrupted education and many other feilds. For fear of students cheating,
many educationists have resorted to banning their use, as their outputs can be
human-like and hard to detect in some cases. These LLMs are also known for
hallucinations (i.e. fake facts). We conduct studies with ProCoT in two
different courses with a combined total of about 66 students. The students in
each course were asked to prompt an LLM of their choice with one question from
a set of four and required to affirm or refute statements in the LLM output by
using peer reviewed references. The results show two things: (1) ProCoT
stimulates creative/critical thinking and writing of students through
engagement with LLMs when we compare the LLM solely output to ProCoT output and
(2) ProCoT can prevent cheating because of clear limitations in existing LLMs
when we compare students ProCoT output to LLM ProCoT output. We also discover
that most students prefer to give answers in fewer words than LLMs, which are
typically verbose. The average word counts for students, ChatGPT (v3.5) and
Phind (v8) are 208, 391 and 383, respectively.
</p></li>
</ul>

<h3>Title: SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models. (arXiv:2312.09818v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09818">http://arxiv.org/abs/2312.09818</a></li>
<li>Code URL: <a href="https://github.com/smile-data/smile">https://github.com/smile-data/smile</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09818]] SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models(http://arxiv.org/abs/2312.09818)</code></li>
<li>Summary: <p>Despite the recent advances of the artificial intelligence, building social
intelligence remains a challenge. Among social signals, laughter is one of the
distinctive expressions that occurs during social interactions between humans.
In this work, we tackle a new challenge for machines to understand the
rationale behind laughter in video, Video Laugh Reasoning. We introduce this
new task to explain why people laugh in a particular video and a dataset for
this task. Our proposed dataset, SMILE, comprises video clips and language
descriptions of why people laugh. We propose a baseline by leveraging the
reasoning capacity of large language models (LLMs) with textual video
representation. Experiments show that our baseline can generate plausible
explanations for laughter. We further investigate the scalability of our
baseline by probing other video understanding tasks and in-the-wild videos. We
release our dataset, code, and model checkpoints on
https://github.com/SMILE-data/SMILE.
</p></li>
</ul>

<h3>Title: Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China. (arXiv:2312.09917v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09917">http://arxiv.org/abs/2312.09917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09917]] Red AI? Inconsistent Responses from GPT3(http://arxiv.org/abs/2312.09917)</code></li>
<li>Summary: <p>The rising popularity of ChatGPT and other AI-powered large language models
(LLMs) has led to increasing studies highlighting their susceptibility to
mistakes and biases. However, most of these studies focus on models trained on
English texts. Taking an innovative approach, this study investigates political
biases in GPT's multilingual models. We posed the same question about
high-profile political issues in the United States and China to GPT in both
English and simplified Chinese, and our analysis of the bilingual responses
revealed that GPT's bilingual models' political "knowledge" (content) and the
political "attitude" (sentiment) are significantly more inconsistent on
political issues in China. The simplified Chinese GPT models not only tended to
provide pro-China information but also presented the least negative sentiment
towards China's problems, whereas the English GPT was significantly more
negative towards China. This disparity may stem from Chinese state censorship
and US-China geopolitical tensions, which influence the training corpora of GPT
bilingual models. Moreover, both Chinese and English models tended to be less
critical towards the issues of "their own" represented by the language used,
than the issues of "the other." This suggests that GPT multilingual models
could potentially develop a "political identity" and an associated sentiment
bias based on their training language. We discussed the implications of our
findings for information transmission and communication in an increasingly
divided world.
</p></li>
</ul>

<h3>Title: The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment. (arXiv:2312.09979v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09979">http://arxiv.org/abs/2312.09979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09979]] The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment(http://arxiv.org/abs/2312.09979)</code></li>
<li>Summary: <p>Supervised fine-tuning (SFT) is a crucial step for large language models
(LLMs), enabling them to align with human instructions and enhance their
capabilities in downstream tasks. When the models are required to align with a
broader range of downstream tasks, or there is a desire to notably improve the
performance on a specific task, a substantial increase in fine-tuning data
often emerges as the solution. However, we find that large-scale increases in
instruction data can disrupt the world knowledge previously stored in the LLMs,
i.e., world knowledge forgetting. In this paper, we introduce LoRAMoE to
address above challenge. The LoRAMoE is a plugin version of Mixture of Experts
(MoE). The plugin-form ensures the integrity of world knowledge by freezing the
backbone model during the training phase. And we propose the use of localized
balancing constraints to coordinate parts of experts for task utilization,
meanwhile enables other experts to to fully leverage the world knowledge stored
in the models. Experimental results demonstrate that LoRAMoE can reasonly
coordinate experts based on data type during inference, and even dramatically
increasing instruction data does not result in knowledge forgetting. Moreover,
LoRAMoE provides additional benefits for the performance of downstream tasks,
indicating the potential of our approach for multi-task learning.
</p></li>
</ul>

<h3>Title: ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent. (arXiv:2312.10003v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10003">http://arxiv.org/abs/2312.10003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10003]] ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent(http://arxiv.org/abs/2312.10003)</code></li>
<li>Summary: <p>Answering complex natural language questions often necessitates multi-step
reasoning and integrating external information. Several systems have combined
knowledge retrieval with a large language model (LLM) to answer such questions.
These systems, however, suffer from various failure cases, and we cannot
directly train them end-to-end to fix such failures, as interaction with
external knowledge is non-differentiable. To address these deficiencies, we
define a ReAct-style LLM agent with the ability to reason and act upon external
knowledge. We further refine the agent through a ReST-like method that
iteratively trains on previous trajectories, employing growing-batch
reinforcement learning with AI feedback for continuous self-improvement and
self-distillation. Starting from a prompted large model and after just two
iterations of the algorithm, we can produce a fine-tuned small model that
achieves comparable performance on challenging compositional question-answering
benchmarks with two orders of magnitude fewer parameters.
</p></li>
</ul>

<h3>Title: Faithful Persona-based Conversational Dataset Generation with Large Language Models. (arXiv:2312.10007v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10007">http://arxiv.org/abs/2312.10007</a></li>
<li>Code URL: <a href="https://github.com/google-research-datasets/Synthetic-Persona-Chat">https://github.com/google-research-datasets/Synthetic-Persona-Chat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10007]] Faithful Persona-based Conversational Dataset Generation with Large Language Models(http://arxiv.org/abs/2312.10007)</code></li>
<li>Summary: <p>High-quality conversational datasets are essential for developing AI models
that can communicate with users. One way to foster deeper interactions between
a chatbot and its user is through personas, aspects of the user's character
that provide insights into their personality, motivations, and behaviors.
Training Natural Language Processing (NLP) models on a diverse and
comprehensive persona-based dataset can lead to conversational models that
create a deeper connection with the user, and maintain their engagement. In
this paper, we leverage the power of Large Language Models (LLMs) to create a
large, high-quality conversational dataset from a seed dataset. We propose a
Generator-Critic architecture framework to expand the initial dataset, while
improving the quality of its conversations. The Generator is an LLM prompted to
output conversations. The Critic consists of a mixture of expert LLMs that
control the quality of the generated conversations. These experts select the
best generated conversations, which we then use to improve the Generator. We
release Synthetic-Persona-Chat, consisting of 20k conversations seeded from
Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our
generation framework on different dimensions through extensive experiments, and
observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat
during Turing test decreases from 17.2% to 8.8% over three iterations.
</p></li>
</ul>

<h3>Title: Challenges with unsupervised LLM knowledge discovery. (arXiv:2312.10029v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.10029">http://arxiv.org/abs/2312.10029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.10029]] Challenges with unsupervised LLM knowledge discovery(http://arxiv.org/abs/2312.10029)</code></li>
<li>Summary: <p>We show that existing unsupervised methods on large language model (LLM)
activations do not discover knowledge -- instead they seem to discover whatever
feature of the activations is most prominent. The idea behind unsupervised
knowledge elicitation is that knowledge satisfies a consistency structure,
which can be used to discover knowledge. We first prove theoretically that
arbitrary features (not just knowledge) satisfy the consistency structure of a
particular leading unsupervised knowledge-elicitation method,
contrast-consistent search (Burns et al. - <a href="http://export.arxiv.org/abs/2212.03827">arXiv:2212.03827</a>). We then present a
series of experiments showing settings in which unsupervised methods result in
classifiers that do not predict knowledge, but instead predict a different
prominent feature. We conclude that existing unsupervised methods for
discovering latent knowledge are insufficient, and we contribute sanity checks
to apply to evaluating future knowledge elicitation methods. Conceptually, we
hypothesise that the identification issues explored here, e.g. distinguishing a
model's knowledge from that of a simulated character's, will persist for future
unsupervised methods.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: SAR image segmentation algorithms based on I-divergence-TV model. (arXiv:2312.09365v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09365">http://arxiv.org/abs/2312.09365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09365]] SAR image segmentation algorithms based on I-divergence-TV model(http://arxiv.org/abs/2312.09365)</code></li>
<li>Summary: <p>In this paper, we propose a novel variational active contour model based on
I-divergence-TV model to segment Synthetic aperture radar (SAR) images with
multiplicative gamma noise, which hybrides edge-based model with region-based
model. The proposed model can efficiently stop the contours at weak or blurred
edges, and can automatically detect the exterior and interior boundaries of
images. We incorporate the global convex segmentation method and split Bregman
technique into the proposed model, and propose a fast fixed point algorithm to
solve the global convex segmentation question[25]. Experimental results for
synthetic images and real SAR images show that the proposed fast fixed point
algorithm is robust and efficient compared with the state-of-the-art approach.
</p></li>
</ul>

<h3>Title: High-Resolution Maps of Left Atrial Displacements and Strains Estimated with 3D CINE MRI and Unsupervised Neural Networks. (arXiv:2312.09387v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09387">http://arxiv.org/abs/2312.09387</a></li>
<li>Code URL: <a href="https://github.com/cgalaz01/aladdin_cmr_la">https://github.com/cgalaz01/aladdin_cmr_la</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09387]] High-Resolution Maps of Left Atrial Displacements and Strains Estimated with 3D CINE MRI and Unsupervised Neural Networks(http://arxiv.org/abs/2312.09387)</code></li>
<li>Summary: <p>The functional analysis of the left atrium (LA) is important for evaluating
cardiac health and understanding diseases like atrial fibrillation. Cine MRI is
ideally placed for the detailed 3D characterisation of LA motion and
deformation, but it is lacking appropriate acquisition and analysis tools. In
this paper, we present Analysis for Left Atrial Displacements and Deformations
using unsupervIsed neural Networks, \textit{Aladdin}, to automatically and
reliably characterise regional LA deformations from high-resolution 3D Cine
MRI. The tool includes: an online few-shot segmentation network (Aladdin-S), an
online unsupervised image registration network (Aladdin-R), and a strain
calculations pipeline tailored to the LA. We create maps of LA Displacement
Vector Field (DVF) magnitude and LA principal strain values from images of 10
healthy volunteers and 8 patients with cardiovascular disease (CVD). We
additionally create an atlas of these biomarkers using the data from the
healthy volunteers. Aladdin is able to accurately track the LA wall across the
cardiac cycle and characterize its motion and deformation. The overall DVF
magnitude and principal strain values are significantly higher in the healthy
group vs CVD patients: $2.85 \pm 1.59~mm$ and $0.09 \pm 0.05$ vs $1.96 \pm
0.74~mm$ and $0.03 \pm 0.04$, respectively. The time course of these metrics is
also different in the two groups, with a more marked active contraction phase
observed in the healthy cohort. Finally, utilizing the LA atlas allows us to
identify regional deviations from the population distribution that may indicate
focal tissue abnormalities. The proposed tool for the quantification of novel
regional LA deformation biomarkers should have important clinical applications.
The source code, anonymized images, generated maps and atlas are publicly
available: https://github.com/cgalaz01/aladdin_cmr_la.
</p></li>
</ul>

<h3>Title: WeatherProof: A Paired-Dataset Approach to Semantic Segmentation in Adverse Weather. (arXiv:2312.09534v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09534">http://arxiv.org/abs/2312.09534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09534]] WeatherProof: A Paired-Dataset Approach to Semantic Segmentation in Adverse Weather(http://arxiv.org/abs/2312.09534)</code></li>
<li>Summary: <p>The introduction of large, foundational models to computer vision has led to
drastically improved performance on the task of semantic segmentation. However,
these existing methods exhibit a large performance drop when testing on images
degraded by weather conditions such as rain, fog, or snow. We introduce a
general paired-training method that can be applied to all current foundational
model architectures that leads to improved performance on images in adverse
weather conditions. To this end, we create the WeatherProof Dataset, the first
semantic segmentation dataset with accurate clear and adverse weather image
pairs, which not only enables our new training paradigm, but also improves the
evaluation of the performance gap between clear and degraded segmentation. We
find that training on these paired clear and adverse weather frames which share
an underlying scene results in improved performance on adverse weather data.
With this knowledge, we propose a training pipeline which accentuates the
advantages of paired-data training using consistency losses and language
guidance, which leads to performance improvements by up to 18.4% as compared to
standard training procedures.
</p></li>
</ul>

<h3>Title: AEGIS-Net: Attention-guided Multi-Level Feature Aggregation for Indoor Place Recognition. (arXiv:2312.09538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09538">http://arxiv.org/abs/2312.09538</a></li>
<li>Code URL: <a href="https://github.com/yuhangming/aegis-net">https://github.com/yuhangming/aegis-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09538]] AEGIS-Net: Attention-guided Multi-Level Feature Aggregation for Indoor Place Recognition(http://arxiv.org/abs/2312.09538)</code></li>
<li>Summary: <p>We present AEGIS-Net, a novel indoor place recognition model that takes in
RGB point clouds and generates global place descriptors by aggregating
lower-level color, geometry features and higher-level implicit semantic
features. However, rather than simple feature concatenation, self-attention
modules are employed to select the most important local features that best
describe an indoor place. Our AEGIS-Net is made of a semantic encoder, a
semantic decoder and an attention-guided feature embedding. The model is
trained in a 2-stage process with the first stage focusing on an auxiliary
semantic segmentation task and the second one on the place recognition task. We
evaluate our AEGIS-Net on the ScanNetPR dataset and compare its performance
with a pre-deep-learning feature-based method and five state-of-the-art
deep-learning-based methods. Our AEGIS-Net achieves exceptional performance and
outperforms all six methods.
</p></li>
</ul>

<h3>Title: MobileSAMv2: Faster Segment Anything to Everything. (arXiv:2312.09579v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09579">http://arxiv.org/abs/2312.09579</a></li>
<li>Code URL: <a href="https://github.com/chaoningzhang/mobilesam">https://github.com/chaoningzhang/mobilesam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09579]] MobileSAMv2: Faster Segment Anything to Everything(http://arxiv.org/abs/2312.09579)</code></li>
<li>Summary: <p>Segment anything model (SAM) addresses two practical yet challenging
segmentation tasks: \textbf{segment anything (SegAny)}, which utilizes a
certain point to predict the mask for a single object of interest, and
\textbf{segment everything (SegEvery)}, which predicts the masks for all
objects on the image. What makes SegAny slow for SAM is its heavyweight image
encoder, which has been addressed by MobileSAM via decoupled knowledge
distillation. The efficiency bottleneck of SegEvery with SAM, however, lies in
its mask decoder because it needs to first generate numerous masks with
redundant grid-search prompts and then perform filtering to obtain the final
valid masks. We propose to improve its efficiency by directly generating the
final masks with only valid prompts, which can be obtained through object
discovery. Our proposed approach not only helps reduce the total time on the
mask decoder by at least 16 times but also achieves superior performance.
Specifically, our approach yields an average performance boost of 3.6\% (42.5\%
\textit{v.s.} 38.9\%) for zero-shot object proposal on the LVIS dataset with
the mask AR@$K$ metric. Qualitative results show that our approach generates
fine-grained masks while avoiding over-segmenting things. This project
targeting faster SegEvery than the original SAM is termed MobileSAMv2 to
differentiate from MobileSAM which targets faster SegAny. Moreover, we
demonstrate that our new prompt sampling is also compatible with the distilled
image encoders in MobileSAM, contributing to a unified framework for efficient
SegAny and SegEvery. The code is available at the same link as MobileSAM
Project
\href{https://github.com/ChaoningZhang/MobileSAM}{\textcolor{red}{https://github.com/ChaoningZhang/MobileSAM}}.
\end{abstract}
</p></li>
</ul>

<h3>Title: Density Matters: Improved Core-set for Active Domain Adaptive Segmentation. (arXiv:2312.09595v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09595">http://arxiv.org/abs/2312.09595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09595]] Density Matters: Improved Core-set for Active Domain Adaptive Segmentation(http://arxiv.org/abs/2312.09595)</code></li>
<li>Summary: <p>Active domain adaptation has emerged as a solution to balance the expensive
annotation cost and the performance of trained models in semantic segmentation.
However, existing works usually ignore the correlation between selected samples
and its local context in feature space, which leads to inferior usage of
annotation budgets. In this work, we revisit the theoretical bound of the
classical Core-set method and identify that the performance is closely related
to the local sample distribution around selected samples. To estimate the
density of local samples efficiently, we introduce a local proxy estimator with
Dynamic Masked Convolution and develop a Density-aware Greedy algorithm to
optimize the bound. Extensive experiments demonstrate the superiority of our
approach. Moreover, with very few labels, our scheme achieves comparable
performance to the fully supervised counterpart.
</p></li>
</ul>

<h3>Title: Collaborating Foundation models for Domain Generalized Semantic Segmentation. (arXiv:2312.09788v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09788">http://arxiv.org/abs/2312.09788</a></li>
<li>Code URL: <a href="https://github.com/yasserben/clouds">https://github.com/yasserben/clouds</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09788]] Collaborating Foundation models for Domain Generalized Semantic Segmentation(http://arxiv.org/abs/2312.09788)</code></li>
<li>Summary: <p>Domain Generalized Semantic Segmentation (DGSS) deals with training a model
on a labeled source domain with the aim of generalizing to unseen domains
during inference. Existing DGSS methods typically effectuate robust features by
means of Domain Randomization (DR). Such an approach is often limited as it can
only account for style diversification and not content. In this work, we take
an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative
FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In
detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP
backbone for its robust feature representation, (ii) generative models to
diversify the content, thereby covering various modes of the possible target
distribution, and (iii) Segment Anything Model (SAM) for iteratively refining
the predictions of the segmentation model. Extensive experiments show that our
CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under
varying weather conditions, notably outperforming prior methods by 5.6% and
6.7% on averaged miou, respectively. The code is available at :
https://github.com/yasserben/CLOUDS
</p></li>
</ul>

<h3>Title: Multi-stage Learning for Radar Pulse Activity Segmentation. (arXiv:2312.09489v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.09489">http://arxiv.org/abs/2312.09489</a></li>
<li>Code URL: <a href="https://github.com/abcxyzi/radseg">https://github.com/abcxyzi/radseg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.09489]] Multi-stage Learning for Radar Pulse Activity Segmentation(http://arxiv.org/abs/2312.09489)</code></li>
<li>Summary: <p>Radio signal recognition is a crucial function in electronic warfare. Precise
identification and localisation of radar pulse activities are required by
electronic warfare systems to produce effective countermeasures. Despite the
importance of these tasks, deep learning-based radar pulse activity recognition
methods have remained largely underexplored. While deep learning for radar
modulation recognition has been explored previously, classification tasks are
generally limited to short and non-interleaved IQ signals, limiting their
applicability to military applications. To address this gap, we introduce an
end-to-end multi-stage learning approach to detect and localise pulse
activities of interleaved radar signals across an extended time horizon. We
propose a simple, yet highly effective multi-stage architecture for
incrementally predicting fine-grained segmentation masks that localise radar
pulse activities across multiple channels. We demonstrate the performance of
our approach against several reference models on a novel radar dataset, while
also providing a first-of-its-kind benchmark for radar pulse activity
segmentation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
