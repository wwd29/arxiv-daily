<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Committee Moderation on Encrypted Messaging Platforms. (arXiv:2306.01241v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01241">http://arxiv.org/abs/2306.01241</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01241] Committee Moderation on Encrypted Messaging Platforms](http://arxiv.org/abs/2306.01241) #secure</code></li>
<li>Summary: <p>Encrypted messaging services like WhatsApp, Facebook Messenger, and Signal
provide secure and deniable communication for billions across the world, but
these exact properties prevent holding users accountable for sending messages
that are abusive, misinformative, or otherwise harmful to society. Previous
works have addressed this concern by allowing a moderator to verify the
identity of a message's sender if a message is reported; if not reported,
messages maintain all security guarantees. Using primitives from threshold
cryptography, this work extends the message-reporting protocol Hecate from
Issa, Alhaddad, and Varia to a setting in which consensus among a group of
moderators is required to reveal and verify the identity of a message's sender.
</p></li>
</ul>

<h3>Title: Blockchain Model for Environment/Infrastructure Monitoring in Cloud-Enabled High-Altitude Platform Systems. (arXiv:2306.01616v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01616">http://arxiv.org/abs/2306.01616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01616] Blockchain Model for Environment/Infrastructure Monitoring in Cloud-Enabled High-Altitude Platform Systems](http://arxiv.org/abs/2306.01616) #secure</code></li>
<li>Summary: <p>The recently accentuated features of augmenting conventional wireless
networks with high altitude platform systems (HAPS) have fueled a plethora of
applications, which promise to offer new services to ground users, as well to
enhance the efficiency and pervasion of existing applications. Cloud-enabled
HAPS, which aims to create HAPS-based datacenters that offer cloud services to
users, has particularly emerged as a promising key enabler to provide
large-scale equitable services from the sky. Although offering cloud services
from the HAPS proves to be efficient, its practical deployment at the
stratosphere level still faces many challenges such as high energy
requirements, physical maintenance, and is particularly prone to security
considerations. Safeguarding the cloud-enabled HAPS against various
cyberattacks is a necessity to guarantee its safe operation. This paper
proposes a blockchain model to secure cloud-enabled HAPS networks that contain
a large number of HAPS stations from recurring cyberattacks within the context
of the environment and infrastructure monitoring (EIM) application. To this
end, the paper first presents a detailed blockchain framework, and describes
the ways of integrating the developed framework into the various system
components. We then discuss the details of the system implementation, including
the storing and consuming of cloud transactions, the generation of new blocks,
and the blockchain consensus protocol that is tailored to the EIM requirements.
Finally, we present numerical simulations that illustrate the performance of
the system in terms of throughput, latency, and resilience to attacks.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Network Agnostic MPC with Statistical Security. (arXiv:2306.01401v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01401">http://arxiv.org/abs/2306.01401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01401] Network Agnostic MPC with Statistical Security](http://arxiv.org/abs/2306.01401) #security</code></li>
<li>Summary: <p>We initiate the study of the network agnostic MPC protocols with statistical
security. Network agnostic protocols give the best possible security guarantees
irrespective of the underlying network type. We consider the general-adversary
model, where the adversary is characterized by an adversary structure which
enumerates all possible candidate subsets of corrupt parties. The
$\mathcal{Q}^{(k)}$ condition enforces that the union of no $k$ subsets from
the adversary structure covers the party set. Given an unconditionally-secure
PKI setup, known statistically-secure synchronous MPC protocols are secure
against adversary structures satisfying the $\mathcal{Q}^{(2)}$ condition.
Known statistically-secure asynchronous MPC protocols can tolerate
$\mathcal{Q}^{(3)}$ adversary structures. Fix a set of $n$ parties $\mathcal{P}
= {P_1, ... ,P_n}$ and adversary structures $\mathcal{Z}_s$ and
$\mathcal{Z}_a$, satisfying the $\mathcal{Q}^{(2)}$ and $\mathcal{Q}^{(3)}$
conditions respectively, where $\mathcal{Z}_a \subset \mathcal{Z}_s$. Then,
given an unconditionally-secure PKI, we ask whether it is possible to design a
statistically-secure MPC protocol resilient against $\mathcal{Z}_s$ and
$\mathcal{Z}_a$ in a synchronous and an asynchronous network respectively if
the parties in $\mathcal{P}$ are unaware of the network type. We show that it
is possible iff $\mathcal{Z}_s$ and $\mathcal{Z}_a$ satisfy the
$\mathcal{Q}^{(2,1)}$ condition, meaning that the union of any two subsets from
$\mathcal{Z}_s$ and any one subset from $\mathcal{Z}_a$ is a proper subset of
$\mathcal{P}$. We design several important network agnostic building blocks
with the $\mathcal{Q}^{(2,1)}$ condition, such as Byzantine broadcast,
Byzantine agreement, information checking protocol, verifiable secret-sharing
and secure multiplication protocol, whose complexity is polynomial in $n$ and
$|\mathcal{Z}_s|$.
</p></li>
</ul>

<h3>Title: Spatio-Temporal Deep Learning-Assisted Reduced Security-Constrained Unit Commitment. (arXiv:2306.01570v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01570">http://arxiv.org/abs/2306.01570</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01570] Spatio-Temporal Deep Learning-Assisted Reduced Security-Constrained Unit Commitment](http://arxiv.org/abs/2306.01570) #security</code></li>
<li>Summary: <p>Security-constrained unit commitment (SCUC) is a computationally complex
process utilized in power system day-ahead scheduling and market clearing. SCUC
is run daily and requires state-of-the-art algorithms to speed up the process.
The constraints and data associated with SCUC are both geographically and
temporally correlated to ensure the reliability of the solution, which further
increases the complexity. In this paper, an advanced machine learning (ML)
model is used to study the patterns in power system historical data, which
inherently considers both spatial and temporal (ST) correlations in
constraints. The ST-correlated ML model is trained to understand spatial
correlation by considering graph neural networks (GNN) whereas temporal
sequences are studied using long short-term memory (LSTM) networks. The
proposed approach is validated on several test systems namely, IEEE 24-Bus
system, IEEE-73 Bus system, IEEE 118-Bus system, and synthetic South-Carolina
(SC) 500-Bus system. Moreover, B-{\theta} and power transfer distribution
factor (PTDF) based SCUC formulations were considered in this research.
Simulation results demonstrate that the ST approach can effectively predict
generator commitment schedule and classify critical and non-critical lines in
the system which are utilized for model reduction of SCUC to obtain
computational enhancement without loss in solution quality
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Privacy-Preserving Remote Heart Rate Estimation from Facial Videos. (arXiv:2306.01141v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01141">http://arxiv.org/abs/2306.01141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01141] Privacy-Preserving Remote Heart Rate Estimation from Facial Videos](http://arxiv.org/abs/2306.01141) #privacy</code></li>
<li>Summary: <p>Remote Photoplethysmography (rPPG) is the process of estimating PPG from
facial videos. While this approach benefits from contactless interaction, it is
reliant on videos of faces, which often constitutes an important privacy
concern. Recent research has revealed that deep learning techniques are
vulnerable to attacks, which can result in significant data breaches making
deep rPPG estimation even more sensitive. To address this issue, we propose a
data perturbation method that involves extraction of certain areas of the face
with less identity-related information, followed by pixel shuffling and
blurring. Our experiments on two rPPG datasets (PURE and UBFC) show that our
approach reduces the accuracy of facial recognition algorithms by over 60%,
with minimal impact on rPPG extraction. We also test our method on three facial
recognition datasets (LFW, CALFW, and AgeDB), where our approach reduced
performance by nearly 50%. Our findings demonstrate the potential of our
approach as an effective privacy-preserving solution for rPPG estimation.
</p></li>
</ul>

<h3>Title: Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging. (arXiv:2306.01176v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01176">http://arxiv.org/abs/2306.01176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01176] Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging](http://arxiv.org/abs/2306.01176) #privacy</code></li>
<li>Summary: <p>Snapshot compressive imaging emerges as a promising technology for acquiring
real-world hyperspectral signals. It uses an optical encoder and compressively
produces the 2D measurement, followed by which the 3D hyperspectral data can be
retrieved via training a deep reconstruction network. Existing reconstruction
models are trained with a single hardware instance, whose performance is
vulnerable to hardware perturbation or replacement, demonstrating an
overfitting issue to the physical configuration. This defect limits the
deployment of pre-trained models since they would suffer from large performance
degradation when are assembled to unseen hardware. To better facilitate the
reconstruction model with new hardware, previous efforts resort to centralized
training by collecting multi-hardware and data, which is impractical when
dealing with proprietary assets among institutions. In light of this, federated
learning (FL) has become a feasible solution to enable cross-hardware
cooperation without breaking privacy. However, the naive FedAvg is subject to
client drift upon data heterogeneity owning to the hardware inconsistency. In
this work, we tackle this challenge by marrying prompt tuning with FL to
snapshot compressive imaging for the first time and propose an federated
hardware-prompt learning (FedHP) method. Rather than mitigating the client
drift by rectifying the gradients, which only takes effect on the learning
manifold but fails to touch the heterogeneity rooted in the input data space,
the proposed FedHP globally learns a hardware-conditioned prompter to align the
data distribution, which serves as an indicator of the data inconsistency
stemming from different pre-defined coded apertures. Extensive experiments
demonstrate that the proposed method well coordinates the pre-trained model to
indeterminate hardware configurations.
</p></li>
</ul>

<h3>Title: Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models. (arXiv:2306.01322v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01322">http://arxiv.org/abs/2306.01322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01322] Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models](http://arxiv.org/abs/2306.01322) #privacy</code></li>
<li>Summary: <p>Knowledge distillation in neural networks refers to compressing a large model
or dataset into a smaller version of itself. We introduce Privacy Distillation,
a framework that allows a text-to-image generative model to teach another model
without exposing it to identifiable data. Here, we are interested in the
privacy issue faced by a data provider who wishes to share their data via a
multimodal generative model. A question that immediately arises is ``How can a
data provider ensure that the generative model is not leaking identifiable
information about a patient?''. Our solution consists of (1) training a first
diffusion model on real data (2) generating a synthetic dataset using this
model and filtering it to exclude images with a re-identifiability risk (3)
training a second diffusion model on the filtered synthetic data only. We
showcase that datasets sampled from models trained with privacy distillation
can effectively reduce re-identification risk whilst maintaining downstream
performance.
</p></li>
</ul>

<h3>Title: SASMU: boost the performance of generalized recognition model using synthetic face dataset. (arXiv:2306.01449v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01449">http://arxiv.org/abs/2306.01449</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01449] SASMU: boost the performance of generalized recognition model using synthetic face dataset](http://arxiv.org/abs/2306.01449) #privacy</code></li>
<li>Summary: <p>Nowadays, deploying a robust face recognition product becomes easy with the
development of face recognition techniques for decades. Not only profile image
verification but also the state-of-the-art method can handle the in-the-wild
image almost perfectly. However, the concern of privacy issues raise rapidly
since mainstream research results are powered by tons of web-crawled data,
which faces the privacy invasion issue. The community tries to escape this
predicament completely by training the face recognition model with synthetic
data but faces severe domain gap issues, which still need to access real images
and identity labels to fine-tune the model. In this paper, we propose SASMU, a
simple, novel, and effective method for face recognition using a synthetic
dataset. Our proposed method consists of spatial data augmentation (SA) and
spectrum mixup (SMU). We first analyze the existing synthetic datasets for
developing a face recognition system. Then, we reveal that heavy data
augmentation is helpful for boosting performance when using synthetic data. By
analyzing the previous frequency mixup studies, we proposed a novel method for
domain generalization. Extensive experimental results have demonstrated the
effectiveness of SASMU, achieving state-of-the-art performance on several
common benchmarks, such as LFW, AgeDB-30, CA-LFW, CFP-FP, and CP-LFW.
</p></li>
</ul>

<h3>Title: Driving Context into Text-to-Text Privatization. (arXiv:2306.01457v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01457">http://arxiv.org/abs/2306.01457</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01457] Driving Context into Text-to-Text Privatization](http://arxiv.org/abs/2306.01457) #privacy</code></li>
<li>Summary: <p>\textit{Metric Differential Privacy} enables text-to-text privatization by
adding calibrated noise to the vector of a word derived from an embedding space
and projecting this noisy vector back to a discrete vocabulary using a nearest
neighbor search. Since words are substituted without context, this mechanism is
expected to fall short at finding substitutes for words with ambiguous
meanings, such as \textit{'bank'}. To account for these ambiguous words, we
leverage a sense embedding and incorporate a sense disambiguation step prior to
noise injection. We encompass our modification to the privatization mechanism
with an estimation of privacy and utility. For word sense disambiguation on the
\textit{Words in Context} dataset, we demonstrate a substantial increase in
classification accuracy by $6.05\%$.
</p></li>
</ul>

<h3>Title: Guiding Text-to-Text Privatization by Syntax. (arXiv:2306.01471v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01471">http://arxiv.org/abs/2306.01471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01471] Guiding Text-to-Text Privatization by Syntax](http://arxiv.org/abs/2306.01471) #privacy</code></li>
<li>Summary: <p>Metric Differential Privacy is a generalization of differential privacy
tailored to address the unique challenges of text-to-text privatization. By
adding noise to the representation of words in the geometric space of
embeddings, words are replaced with words located in the proximity of the noisy
representation. Since embeddings are trained based on word co-occurrences, this
mechanism ensures that substitutions stem from a common semantic context.
Without considering the grammatical category of words, however, this mechanism
cannot guarantee that substitutions play similar syntactic roles. We analyze
the capability of text-to-text privatization to preserve the grammatical
category of words after substitution and find that surrogate texts consist
almost exclusively of nouns. Lacking the capability to produce surrogate texts
that correlate with the structure of the sensitive texts, we encompass our
analysis by transforming the privatization step into a candidate selection
problem in which substitutions are directed to words with matching grammatical
properties. We demonstrate a substantial improvement in the performance of
downstream tasks by up to $4.66\%$ while retaining comparative privacy
guarantees.
</p></li>
</ul>

<h3>Title: TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01181">http://arxiv.org/abs/2306.01181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01181] TMI! Finetuned Models Leak Private Information from their Pretraining Data](http://arxiv.org/abs/2306.01181) #privacy</code></li>
<li>Summary: <p>Transfer learning has become an increasingly popular technique in machine
learning as a way to leverage a pretrained model trained for one task to assist
with building a finetuned model for a related task. This paradigm has been
especially popular for privacy in machine learning, where the pretrained model
is considered public, and only the data for finetuning is considered sensitive.
However, there are reasons to believe that the data used for pretraining is
still sensitive, making it essential to understand how much information the
finetuned model leaks about the pretraining data. In this work we propose a new
membership-inference threat model where the adversary only has access to the
finetuned model and would like to infer the membership of the pretraining data.
To realize this threat model, we implement a novel metaclassifier-based attack,
TMI, that leverages the influence of memorized pretraining samples on
predictions in the downstream task. We evaluate TMI on both vision and natural
language tasks across multiple transfer learning settings, including finetuning
with differential privacy. Through our evaluation, we find that TMI can
successfully infer membership of pretraining examples using query access to the
finetuned model.
</p></li>
</ul>

<h3>Title: Harnessing large-language models to generate private synthetic text. (arXiv:2306.01684v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01684">http://arxiv.org/abs/2306.01684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01684] Harnessing large-language models to generate private synthetic text](http://arxiv.org/abs/2306.01684) #privacy</code></li>
<li>Summary: <p>Differentially private (DP) training methods like DP-SGD can protect
sensitive training data by ensuring that ML models will not reveal private
information. An alternative approach, which this paper studies, is to use a
sensitive dataset to generate a new synthetic dataset which is differentially
private with respect to the original data. Doing so has several advantages:
synthetic data can be reused for other tasks (including for hyper parameter
tuning), retained indefinitely, or shared with third parties without
sacrificing privacy.
</p></li>
</ul>

<p>However, obtaining DP data is much harder than introducing DP during
training. To make it feasible for text, recent work has utilized public data by
starting with a pre-trained generative language model and privately finetuning
it on sensitive data. This model can be used to sample a DP synthetic dataset.
While this strategy seems straightforward, executing it has proven problematic.
Previous approaches either show significant performance loss, or have, as we
show, critical design flaws.
</p>
<p>In this paper we demonstrate that a proper training objective along with
tuning fewer parameters results in excellent DP synthetic data quality. Our
approach is competitive with direct DP-training of downstream classifiers in
terms of performance on downstream tasks. We also demonstrate that our DP
synthetic data is not only useful for downstream classifier training, but also
to tune those same models.
</p>

<h3>Title: Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01121">http://arxiv.org/abs/2306.01121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01121] Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards](http://arxiv.org/abs/2306.01121) #privacy</code></li>
<li>Summary: <p>In this paper, we study the problem of (finite horizon tabular) Markov
decision processes (MDPs) with heavy-tailed rewards under the constraint of
differential privacy (DP). Compared with the previous studies for private
reinforcement learning that typically assume rewards are sampled from some
bounded or sub-Gaussian distributions to ensure DP, we consider the setting
where reward distributions have only finite $(1+v)$-th moments with some $v \in
(0,1]$. By resorting to robust mean estimators for rewards, we first propose
two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and
another is for policy optimization. Under each framework, we consider both
joint differential privacy (JDP) and local differential privacy (LDP) models.
Based on our frameworks, we provide regret upper bounds for both JDP and LDP
cases and show that the moment of distribution and privacy budget both have
significant impacts on regrets. Finally, we establish a lower bound of regret
minimization for heavy-tailed MDPs in JDP model by reducing it to the
instance-independent lower bound of heavy-tailed multi-armed bandits in DP
model. We also show the lower bound for the problem in LDP by adopting some
private minimax methods. Our results reveal that there are fundamental
differences between the problem of private RL with sub-Gaussian and that with
heavy-tailed rewards.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: FedCIP: Federated Client Intellectual Property Protection with Traitor Tracking. (arXiv:2306.01356v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01356">http://arxiv.org/abs/2306.01356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01356] FedCIP: Federated Client Intellectual Property Protection with Traitor Tracking](http://arxiv.org/abs/2306.01356) #protect</code></li>
<li>Summary: <p>Federated learning is an emerging privacy-preserving distributed machine
learning that enables multiple parties to collaboratively learn a shared model
while keeping each party's data private. However, federated learning faces two
main problems: semi-honest server privacy inference attacks and malicious
client-side model theft. To address privacy inference attacks, parameter-based
encrypted federated learning secure aggregation can be used. To address model
theft, a watermark-based intellectual property protection scheme can verify
model ownership. Although watermark-based intellectual property protection
schemes can help verify model ownership, they are not sufficient to address the
issue of continuous model theft by uncaught malicious clients in federated
learning. Existing IP protection schemes that have the ability to track
traitors are also not compatible with federated learning security aggregation.
Thus, in this paper, we propose a Federated Client-side Intellectual Property
Protection (FedCIP), which is compatible with federated learning security
aggregation and has the ability to track traitors. To the best of our
knowledge, this is the first IP protection scheme in federated learning that is
compatible with secure aggregation and tracking capabilities.
</p></li>
</ul>

<h3>Title: Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy. (arXiv:2306.01699v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01699">http://arxiv.org/abs/2306.01699</a></li>
<li>Code URL: <a href="https://github.com/siamakghodsi/masc">https://github.com/siamakghodsi/masc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01699] Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy](http://arxiv.org/abs/2306.01699) #protect</code></li>
<li>Summary: <p>Group imbalance, resulting from inadequate or unrepresentative data
collection methods, is a primary cause of representation bias in datasets.
Representation bias can exist with respect to different groups of one or more
protected attributes and might lead to prejudicial and discriminatory outcomes
toward certain groups of individuals; in cases where a learning model is
trained on such biased data. This paper presents MASC, a data augmentation
approach that leverages affinity clustering to balance the representation of
non-protected and protected groups of a target dataset by utilizing instances
of the same protected attributes from similar datasets that are categorized in
the same cluster as the target dataset by sharing instances of the protected
attribute. The proposed method involves constructing an affinity matrix by
quantifying distribution discrepancies between dataset pairs and transforming
them into a symmetric pairwise similarity matrix. A non-parametric spectral
clustering is then applied to this affinity matrix, automatically categorizing
the datasets into an optimal number of clusters. We perform a step-by-step
experiment as a demo of our method to show the procedure of the proposed data
augmentation method and evaluate and discuss its performance. A comparison with
other data augmentation methods, both pre- and post-augmentation, is conducted,
along with a model evaluation analysis of each method. Our method can handle
non-binary protected attributes so, in our experiments, bias is measured in a
non-binary protected attribute setup w.r.t. racial groups distribution for two
separate minority groups in comparison with the majority group before and after
debiasing. Empirical results imply that our method of augmenting dataset biases
using real (genuine) data from similar contexts can effectively debias the
target datasets comparably to existing data augmentation strategies.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations. (arXiv:2306.01125v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01125">http://arxiv.org/abs/2306.01125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01125] Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations](http://arxiv.org/abs/2306.01125) #defense</code></li>
<li>Summary: <p>Learned Image Compression (LIC) has recently become the trending technique
for image transmission due to its notable performance. Despite its popularity,
the robustness of LIC with respect to the quality of image reconstruction
remains under-explored. In this paper, we introduce an imperceptible attack
approach designed to effectively degrade the reconstruction quality of LIC,
resulting in the reconstructed image being severely disrupted by noise where
any object in the reconstructed images is virtually impossible. More
specifically, we generate adversarial examples by introducing a Frobenius
norm-based loss function to maximize the discrepancy between original images
and reconstructed adversarial examples. Further, leveraging the insensitivity
of high-frequency components to human vision, we introduce Imperceptibility
Constraint (IC) to ensure that the perturbations remain inconspicuous.
Experiments conducted on the Kodak dataset using various LIC models demonstrate
effectiveness. In addition, we provide several findings and suggestions for
designing future defenses.
</p></li>
</ul>

<h3>Title: Adaptive Attractors: A Defense Strategy against ML Adversarial Collusion Attacks. (arXiv:2306.01400v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01400">http://arxiv.org/abs/2306.01400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01400] Adaptive Attractors: A Defense Strategy against ML Adversarial Collusion Attacks](http://arxiv.org/abs/2306.01400) #defense</code></li>
<li>Summary: <p>In the seller-buyer setting on machine learning models, the seller generates
different copies based on the original model and distributes them to different
buyers, such that adversarial samples generated on one buyer's copy would
likely not work on other copies. A known approach achieves this using
attractor-based rewriter which injects different attractors to different
copies. This induces different adversarial regions in different copies, making
adversarial samples generated on one copy not replicable on others. In this
paper, we focus on a scenario where multiple malicious buyers collude to
attack. We first give two formulations and conduct empirical studies to analyze
effectiveness of collusion attack under different assumptions on the attacker's
capabilities and properties of the attractors. We observe that existing
attractor-based methods do not effectively mislead the colluders in the sense
that adversarial samples found are influenced more by the original model
instead of the attractors as number of colluders increases. Based on this
observation, we propose using adaptive attractors whose weight is guided by a
U-shape curve to cover the shortfalls. Experimentation results show that when
using our approach, the attack success rate of a collusion attack converges to
around 15% even when lots of copies are applied for collusion. In contrast,
when using the existing attractor-based rewriter with fixed weight, the attack
success rate increases linearly with the number of copies used for collusion.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations. (arXiv:2306.01273v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01273">http://arxiv.org/abs/2306.01273</a></li>
<li>Code URL: <a href="https://github.com/quocnsh/votetrans">https://github.com/quocnsh/votetrans</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01273] VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations](http://arxiv.org/abs/2306.01273) #attack</code></li>
<li>Summary: <p>Adversarial attacks reveal serious flaws in deep learning models. More
dangerously, these attacks preserve the original meaning and escape human
recognition. Existing methods for detecting these attacks need to be trained
using original/adversarial data. In this paper, we propose detection without
training by voting on hard labels from predictions of transformations, namely,
VoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the
hard labels of input text and its transformation. The evaluation demonstrates
that VoteTRANS effectively detects adversarial text across various
state-of-the-art attacks, models, and datasets.
</p></li>
</ul>

<h3>Title: Compatibility and Timing Attacks for JPEG Steganalysis. (arXiv:2306.01317v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01317">http://arxiv.org/abs/2306.01317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01317] Compatibility and Timing Attacks for JPEG Steganalysis](http://arxiv.org/abs/2306.01317) #attack</code></li>
<li>Summary: <p>This paper introduces a novel compatibility attack to detect a steganographic
message embedded in the DCT domain of a JPEG image at high-quality factors
(close to 100). Because the JPEG compression is not a surjective function, i.e.
not every DCT blocks can be mapped from a pixel block, embedding a message in
the DCT domain can create incompatible blocks. We propose a method to find such
a block, which directly proves that a block has been modified during the
embedding. This theoretical method provides many advantages such as being
completely independent to Cover Source Mismatch, having good detection power,
and perfect reliability since false alarms are impossible as soon as
incompatible blocks are found. We show that finding an incompatible block is
equivalent to proving the infeasibility of an Integer Linear Programming
problem. However, solving such a problem requires considerable computational
power and has not been reached for 8x8 blocks. Instead, a timing attack
approach is presented to perform steganalysis without potentially any false
alarms for large computing power.
</p></li>
</ul>

<h3>Title: Covert Communication Based on the Poisoning Attack in Federated Learning. (arXiv:2306.01342v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01342">http://arxiv.org/abs/2306.01342</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01342] Covert Communication Based on the Poisoning Attack in Federated Learning](http://arxiv.org/abs/2306.01342) #attack</code></li>
<li>Summary: <p>Covert communication has become an important area of research in computer
security. It involves hiding specific information on a carrier for message
transmission and is often used to transmit private data, military secrets, and
even malware. In deep learning, many methods have been developed for hiding
information in models to achieve covert communication. However, these methods
are not applicable to federated learning, where model aggregation invalidates
the exact information embedded in the model by the client. To address this
problem, we propose a novel method for covert communication in federated
learning based on the poisoning attack. Our approach achieves 100% accuracy in
covert message transmission between two clients and is shown to be both
stealthy and robust through extensive experiments. However, existing defense
methods are limited in their effectiveness against our attack scheme,
highlighting the urgent need for new protection methods to be developed. Our
study emphasizes the necessity of research in covert communication and serves
as a foundation for future research in federated learning attacks and defenses.
</p></li>
</ul>

<h3>Title: Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization. (arXiv:2306.01613v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01613">http://arxiv.org/abs/2306.01613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01613] Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization](http://arxiv.org/abs/2306.01613) #attack</code></li>
<li>Summary: <p>Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a
fraction of the training data is manipulated to deliberately degrade the
algorithms' performance. Optimal attacks can be formulated as bilevel
optimization problems and help to assess their robustness in worst-case
scenarios. We show that current approaches, which typically assume that
hyperparameters remain constant, lead to an overly pessimistic view of the
algorithms' robustness and of the impact of regularization. We propose a novel
optimal attack formulation that considers the effect of the attack on the
hyperparameters and models the attack as a multiobjective bilevel optimization
problem. This allows to formulate optimal attacks, learn hyperparameters and
evaluate robustness under worst-case conditions. We apply this attack
formulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our
evaluation on multiple datasets confirms the limitations of previous strategies
and evidences the benefits of using $L_2$ and $L_1$ regularization to dampen
the effect of poisoning attacks.
</p></li>
</ul>

<h3>Title: Poisoning Network Flow Classifiers. (arXiv:2306.01655v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01655">http://arxiv.org/abs/2306.01655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01655] Poisoning Network Flow Classifiers](http://arxiv.org/abs/2306.01655) #attack</code></li>
<li>Summary: <p>As machine learning (ML) classifiers increasingly oversee the automated
monitoring of network traffic, studying their resilience against adversarial
attacks becomes critical. This paper focuses on poisoning attacks, specifically
backdoor attacks, against network traffic flow classifiers. We investigate the
challenging scenario of clean-label poisoning where the adversary's
capabilities are constrained to tampering only with the training data - without
the ability to arbitrarily modify the training labels or any other component of
the training process. We describe a trigger crafting strategy that leverages
model interpretability techniques to generate trigger patterns that are
effective even at very low poisoning rates. Finally, we design novel strategies
to generate stealthy triggers, including an approach based on generative
Bayesian network models, with the goal of minimizing the conspicuousness of the
trigger, and thus making detection of an ongoing poisoning campaign more
challenging. Our findings provide significant insights into the feasibility of
poisoning attacks on network traffic classifiers used in multiple scenarios,
including detecting malicious communication and application classification.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks. (arXiv:2306.01148v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01148">http://arxiv.org/abs/2306.01148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01148] Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks](http://arxiv.org/abs/2306.01148) #robust</code></li>
<li>Summary: <p>For the task of image classification, neural networks primarily rely on
visual patterns. In robust networks, we would expect for visually similar
classes to be represented similarly. We consider the problem of when
semantically similar classes are visually dissimilar, and when visual
similarity is present among non-similar classes. We propose a data augmentation
technique with the goal of better aligning semantically similar classes with
arbitrary (non-visual) semantic relationships. We leverage recent work in
diffusion-based semantic mixing to generate semantic hybrids of two classes,
and these hybrids are added to the training set as augmented data. We evaluate
whether the method increases semantic alignment by evaluating model performance
on adversarially perturbed data, with the idea that it should be easier for an
adversary to switch one class to a similarly represented class. Results
demonstrate that there is an increase in alignment of semantically similar
classes when using our proposed data augmentation method.
</p></li>
</ul>

<h3>Title: SelFLoc: Selective Feature Fusion for Large-scale Point Cloud-based Place Recognition. (arXiv:2306.01205v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01205">http://arxiv.org/abs/2306.01205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01205] SelFLoc: Selective Feature Fusion for Large-scale Point Cloud-based Place Recognition](http://arxiv.org/abs/2306.01205) #robust</code></li>
<li>Summary: <p>Point cloud-based place recognition is crucial for mobile robots and
autonomous vehicles, especially when the global positioning sensor is not
accessible. LiDAR points are scattered on the surface of objects and buildings,
which have strong shape priors along different axes. To enhance message passing
along particular axes, Stacked Asymmetric Convolution Block (SACB) is designed,
which is one of the main contributions in this paper. Comprehensive experiments
demonstrate that asymmetric convolution and its corresponding strategies
employed by SACB can contribute to the more effective representation of point
cloud feature. On this basis, Selective Feature Fusion Block (SFFB), which is
formed by stacking point- and channel-wise gating layers in a predefined
sequence, is proposed to selectively boost salient local features in certain
key regions, as well as to align the features before fusion phase. SACBs and
SFFBs are combined to construct a robust and accurate architecture for point
cloud-based place recognition, which is termed SelFLoc. Comparative
experimental results show that SelFLoc achieves the state-of-the-art (SOTA)
performance on the Oxford and other three in-house benchmarks with an
improvement of 1.6 absolute percentages on mean average recall@1.
</p></li>
</ul>

<h3>Title: Counting Crowds in Bad Weather. (arXiv:2306.01209v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01209">http://arxiv.org/abs/2306.01209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01209] Counting Crowds in Bad Weather](http://arxiv.org/abs/2306.01209) #robust</code></li>
<li>Summary: <p>Crowd counting has recently attracted significant attention in the field of
computer vision due to its wide applications to image understanding. Numerous
methods have been proposed and achieved state-of-the-art performance for
real-world tasks. However, existing approaches do not perform well under
adverse weather such as haze, rain, and snow since the visual appearances of
crowds in such scenes are drastically different from those images in clear
weather of typical datasets. In this paper, we propose a method for robust
crowd counting in adverse weather scenarios. Instead of using a two-stage
approach that involves image restoration and crowd counting modules, our model
learns effective features and adaptive queries to account for large appearance
variations. With these weather queries, the proposed model can learn the
weather information according to the degradation of the input image and
optimize with the crowd counting module simultaneously. Experimental results
show that the proposed algorithm is effective in counting crowds under
different weather types on benchmark datasets. The source code and trained
models will be made available to the public.
</p></li>
</ul>

<h3>Title: Towards Robust GAN-generated Image Detection: a Multi-view Completion Representation. (arXiv:2306.01364v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01364">http://arxiv.org/abs/2306.01364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01364] Towards Robust GAN-generated Image Detection: a Multi-view Completion Representation](http://arxiv.org/abs/2306.01364) #robust</code></li>
<li>Summary: <p>GAN-generated image detection now becomes the first line of defense against
the malicious uses of machine-synthesized image manipulations such as
deepfakes. Although some existing detectors work well in detecting clean, known
GAN samples, their success is largely attributable to overfitting unstable
features such as frequency artifacts, which will cause failures when facing
unknown GANs or perturbation attacks. To overcome the issue, we propose a
robust detection framework based on a novel multi-view image completion
representation. The framework first learns various view-to-image tasks to model
the diverse distributions of genuine images. Frequency-irrelevant features can
be represented from the distributional discrepancies characterized by the
completion models, which are stable, generalized, and robust for detecting
unknown fake patterns. Then, a multi-view classification is devised with
elaborated intra- and inter-view learning strategies to enhance view-specific
feature representation and cross-view feature aggregation, respectively. We
evaluated the generalization ability of our framework across six popular GANs
at different resolutions and its robustness against a broad range of
perturbation attacks. The results confirm our method's improved effectiveness,
generalization, and robustness over various baselines.
</p></li>
</ul>

<h3>Title: Masked Autoencoder for Unsupervised Video Summarization. (arXiv:2306.01395v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01395">http://arxiv.org/abs/2306.01395</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01395] Masked Autoencoder for Unsupervised Video Summarization](http://arxiv.org/abs/2306.01395) #robust</code></li>
<li>Summary: <p>Summarizing a video requires a diverse understanding of the video, ranging
from recognizing scenes to evaluating how much each frame is essential enough
to be selected as a summary. Self-supervised learning (SSL) is acknowledged for
its robustness and flexibility to multiple downstream tasks, but the video SSL
has not shown its value for dense understanding tasks like video summarization.
We claim an unsupervised autoencoder with sufficient self-supervised learning
does not need any extra downstream architecture design or fine-tuning weights
to be utilized as a video summarization model. The proposed method to evaluate
the importance score of each frame takes advantage of the reconstruction score
of the autoencoder's decoder. We evaluate the method in major unsupervised
video summarization benchmarks to show its effectiveness under various
experimental settings.
</p></li>
</ul>

<h3>Title: Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal. (arXiv:2306.01398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01398">http://arxiv.org/abs/2306.01398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01398] Evaluating The Robustness of Self-Supervised Representations to Background/Foreground Removal](http://arxiv.org/abs/2306.01398) #robust</code></li>
<li>Summary: <p>Despite impressive empirical advances of SSL in solving various tasks, the
problem of understanding and characterizing SSL representations learned from
input data remains relatively under-explored. We provide a comparative analysis
of how the representations produced by SSL models differ when masking parts of
the input. Specifically, we considered state-of-the-art SSL pretrained models,
such as DINOv2, MAE, and SwaV, and analyzed changes at the representation
levels across 4 Image Classification datasets. First, we generate variations of
the datasets by applying foreground and background segmentation. Then, we
conduct statistical analysis using Canonical Correlation Analysis (CCA) and
Centered Kernel Alignment (CKA) to evaluate the robustness of the
representations learned in SSL models. Empirically, we show that not all models
lead to representations that separate foreground, background, and complete
images. Furthermore, we test different masking strategies by occluding the
center regions of the images to address cases where foreground and background
are difficult. For example, the DTD dataset that focuses on texture rather
specific objects.
</p></li>
</ul>

<h3>Title: Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01423">http://arxiv.org/abs/2306.01423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01423] Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation](http://arxiv.org/abs/2306.01423) #robust</code></li>
<li>Summary: <p>Network optimization is a crucial step in the field of deep learning, as it
directly affects the performance of models in various domains such as computer
vision. Despite the numerous optimizers that have been developed over the
years, the current methods are still limited in their ability to accurately and
quickly identify gradient trends, which can lead to sub-optimal network
performance. In this paper, we propose a novel deep optimizer called
Fast-Adaptive Moment Estimation (FAME), which for the first time estimates
gradient moments using a Triple Exponential Moving Average (TEMA).
Incorporating TEMA into the optimization process provides richer and more
accurate information on data changes and trends, as compared to the standard
Exponential Moving Average used in essentially all current leading adaptive
optimization methods. Our proposed FAME optimizer has been extensively
validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100,
PASCAL-VOC, MS-COCO, and Cityscapes, using 14 different learning architectures,
six optimizers, and various vision tasks, including detection, classification
and semantic understanding. The results demonstrate that our FAME optimizer
outperforms other leading optimizers in terms of both robustness and accuracy.
</p></li>
</ul>

<h3>Title: HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01623">http://arxiv.org/abs/2306.01623</a></li>
<li>Code URL: <a href="https://github.com/anirudhs123/home">https://github.com/anirudhs123/home</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01623] HomE: Homography-Equivariant Video Representation Learning](http://arxiv.org/abs/2306.01623) #robust</code></li>
<li>Summary: <p>Recent advances in self-supervised representation learning have enabled more
efficient and robust model performance without relying on extensive labeled
data. However, most works are still focused on images, with few working on
videos and even fewer on multi-view videos, where more powerful inductive
biases can be leveraged for self-supervision. In this work, we propose a novel
method for representation learning of multi-view videos, where we explicitly
model the representation space to maintain Homography Equivariance (HomE). Our
method learns an implicit mapping between different views, culminating in a
representation space that maintains the homography relationship between
neighboring views. We evaluate our HomE representation via action recognition
and pedestrian intent prediction as downstream tasks. On action classification,
our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than
most state-of-the-art self-supervised learning methods. Similarly, on the STIP
dataset, we outperform the state-of-the-art by 6% for pedestrian intent
prediction one second into the future while also obtaining an accuracy of 91.2%
for pedestrian action (cross vs. not-cross) classification. Code is available
at https://github.com/anirudhs123/HomE.
</p></li>
</ul>

<h3>Title: Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts. (arXiv:2306.01031v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01031">http://arxiv.org/abs/2306.01031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01031] Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts](http://arxiv.org/abs/2306.01031) #robust</code></li>
<li>Summary: <p>This paper presents a novel algorithm for building an automatic speech
recognition (ASR) model with imperfect training data. Imperfectly transcribed
speech is a prevalent issue in human-annotated speech corpora, which degrades
the performance of ASR models. To address this problem, we propose Bypass
Temporal Classification (BTC) as an expansion of the Connectionist Temporal
Classification (CTC) criterion. BTC explicitly encodes the uncertainties
associated with transcripts during training. This is accomplished by enhancing
the flexibility of the training graph, which is implemented as a weighted
finite-state transducer (WFST) composition. The proposed algorithm improves the
robustness and accuracy of ASR systems, particularly when working with
imprecisely transcribed speech corpora. Our implementation will be
open-sourced.
</p></li>
</ul>

<h3>Title: Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents. (arXiv:2306.01058v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01058">http://arxiv.org/abs/2306.01058</a></li>
<li>Code URL: <a href="https://github.com/cchen23/layout_distribution_shift">https://github.com/cchen23/layout_distribution_shift</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01058] Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents](http://arxiv.org/abs/2306.01058) #robust</code></li>
<li>Summary: <p>Recent work has shown that infusing layout features into language models
(LMs) improves processing of visually-rich documents such as scientific papers.
Layout-infused LMs are often evaluated on documents with familiar layout
features (e.g., papers from the same publisher), but in practice models
encounter documents with unfamiliar distributions of layout features, such as
new combinations of text sizes and styles, or new spatial configurations of
textual elements. In this work we test whether layout-infused LMs are robust to
layout distribution shifts. As a case study we use the task of scientific
document structure recovery, segmenting a scientific paper into its structural
categories (e.g., "title", "caption", "reference"). To emulate distribution
shifts that occur in practice we re-partition the GROTOAP2 dataset. We find
that under layout distribution shifts model performance degrades by up to 20
F1. Simple training strategies, such as increasing training diversity, can
reduce this degradation by over 35% relative F1; however, models fail to reach
in-distribution performance in any tested out-of-distribution conditions. This
work highlights the need to consider layout distribution shifts during model
evaluation, and presents a methodology for conducting such evaluations.
</p></li>
</ul>

<h3>Title: Improving the Robustness of Summarization Systems with Dual Augmentation. (arXiv:2306.01090v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01090">http://arxiv.org/abs/2306.01090</a></li>
<li>Code URL: <a href="https://github.com/iriscxy/robustness">https://github.com/iriscxy/robustness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01090] Improving the Robustness of Summarization Systems with Dual Augmentation](http://arxiv.org/abs/2306.01090) #robust</code></li>
<li>Summary: <p>A robust summarization system should be able to capture the gist of the
document, regardless of the specific word choices or noise in the input. In
this work, we first explore the summarization models' robustness against
perturbations including word-level synonym substitution and noise. To create
semantic-consistent substitutes, we propose a SummAttacker, which is an
efficient approach to generating adversarial samples based on language models.
Experimental results show that state-of-the-art summarization models have a
significant decrease in performance on adversarial and noisy test sets. Next,
we analyze the vulnerability of the summarization systems and explore improving
the robustness by data augmentation. Specifically, the first brittleness factor
we found is the poor understanding of infrequent words in the input.
Correspondingly, we feed the encoder with more diverse cases created by
SummAttacker in the input space. The other factor is in the latent space, where
the attacked inputs bring more variations to the hidden states. Hence, we
construct adversarial decoder input and devise manifold softmixing operation in
hidden space to introduce more diversity. Experimental results on Gigaword and
CNN/DM datasets demonstrate that our approach achieves significant improvements
over strong baselines and exhibits higher robustness on noisy, attacked, and
clean datasets.
</p></li>
</ul>

<h3>Title: Examining the Causal Effect of First Names on Language Models: The Case of Social Commonsense Reasoning. (arXiv:2306.01117v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01117">http://arxiv.org/abs/2306.01117</a></li>
<li>Code URL: <a href="https://github.com/sullamij/causal-first-names">https://github.com/sullamij/causal-first-names</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01117] Examining the Causal Effect of First Names on Language Models: The Case of Social Commonsense Reasoning](http://arxiv.org/abs/2306.01117) #robust</code></li>
<li>Summary: <p>As language models continue to be integrated into applications of personal
and societal relevance, ensuring these models' trustworthiness is crucial,
particularly with respect to producing consistent outputs regardless of
sensitive attributes. Given that first names may serve as proxies for
(intersectional) socio-demographic representations, it is imperative to examine
the impact of first names on commonsense reasoning capabilities. In this paper,
we study whether a model's reasoning given a specific input differs based on
the first names provided. Our underlying assumption is that the reasoning about
Alice should not differ from the reasoning about James. We propose and
implement a controlled experimental framework to measure the causal effect of
first names on commonsense reasoning, enabling us to distinguish between model
predictions due to chance and caused by actual factors of interest. Our results
indicate that the frequency of first names has a direct effect on model
prediction, with less frequent names yielding divergent predictions compared to
more frequent names. To gain insights into the internal mechanisms of models
that are contributing to these behaviors, we also conduct an in-depth
explainable analysis. Overall, our findings suggest that to ensure model
robustness, it is essential to augment datasets with more diverse first names
during the configuration stage.
</p></li>
</ul>

<h3>Title: Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01505">http://arxiv.org/abs/2306.01505</a></li>
<li>Code URL: <a href="https://github.com/zerohd4869/sacl">https://github.com/zerohd4869/sacl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01505] Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations](http://arxiv.org/abs/2306.01505) #robust</code></li>
<li>Summary: <p>Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations. The framework applies contrast-aware
adversarial training to generate worst-case samples and uses a joint
class-spread contrastive learning objective on both original and adversarial
samples. It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training strategy to learn more diverse features from context and enhance the
model's context robustness. We develop a sequence-based method SACL-LSTM under
this framework, to learn label-consistent and context-robust emotional features
for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of the SACL framework.
</p></li>
</ul>

<h3>Title: Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01110">http://arxiv.org/abs/2306.01110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01110] Comparative Study on the Effects of Noise in ML-Based Anxiety Detection](http://arxiv.org/abs/2306.01110) #robust</code></li>
<li>Summary: <p>Wearable health devices are ushering in a new age of continuous and
noninvasive remote monitoring. One application of this technology is in anxiety
detection. Many advancements in anxiety detection have happened in controlled
lab settings, but noise prevents these advancements from generalizing to
real-world conditions. We seek to progress the field by studying how noise
impacts model performance and developing models that are robust to noisy,
real-world conditions and, hence, attuned to the commotion of everyday life. In
this study we look to investigate why and how previous methods have failed.
Using the wearable stress and affect detection (WESAD) dataset, we compare the
effect of various intensities of noise on machine learning models classifying
levels of physiological arousal in the three-class classification problem:
baseline vs. stress vs. amusement. Before introducing noise, our baseline model
performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss
potential sources of this discrepancy in results through a careful evaluation
of feature extraction and model architecture choices. Finally, after the
introduction of noise, we provide a thorough analysis of the effect of noise on
each model architecture.
</p></li>
</ul>

<h3>Title: What if We Enrich day-ahead Solar Irradiance Time Series Forecasting with Spatio-Temporal Context?. (arXiv:2306.01112v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01112">http://arxiv.org/abs/2306.01112</a></li>
<li>Code URL: <a href="https://github.com/gitbooo/CrossViVit">https://github.com/gitbooo/CrossViVit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01112] What if We Enrich day-ahead Solar Irradiance Time Series Forecasting with Spatio-Temporal Context?](http://arxiv.org/abs/2306.01112) #robust</code></li>
<li>Summary: <p>Solar power harbors immense potential in mitigating climate change by
substantially reducing CO$_{2}$ emissions. Nonetheless, the inherent
variability of solar irradiance poses a significant challenge for seamlessly
integrating solar power into the electrical grid. While the majority of prior
research has centered on employing purely time series-based methodologies for
solar forecasting, only a limited number of studies have taken into account
factors such as cloud cover or the surrounding physical context. In this paper,
we put forth a deep learning architecture designed to harness spatio-temporal
context using satellite data, to attain highly accurate \textit{day-ahead}
time-series forecasting for any given station, with a particular emphasis on
forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology
to extract a distribution for each time step prediction, which can serve as a
very valuable measure of uncertainty attached to the forecast. When evaluating
models, we propose a testing scheme in which we separate particularly difficult
examples from easy ones, in order to capture the model performances in crucial
situations, which in the case of this study are the days suffering from varying
cloudy conditions. Furthermore, we present a new multi-modal dataset gathering
satellite imagery over a large zone and time series for solar irradiance and
other related physical variables from multiple geographically diverse solar
stations. Our approach exhibits robust performance in solar irradiance
forecasting, including zero-shot generalization tests at unobserved solar
stations, and holds great promise in promoting the effective integration of
solar power into the grid.
</p></li>
</ul>

<h3>Title: Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms. (arXiv:2306.01213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01213">http://arxiv.org/abs/2306.01213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01213] Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms](http://arxiv.org/abs/2306.01213) #robust</code></li>
<li>Summary: <p>Learning disentangled causal representations is a challenging problem that
has gained significant attention recently due to its implications for
extracting meaningful information for downstream tasks. In this work, we define
a new notion of causal disentanglement from the perspective of independent
causal mechanisms. We propose ICM-VAE, a framework for learning causally
disentangled representations supervised by causally related observed labels. We
model causal mechanisms using learnable flow-based diffeomorphic functions to
map noise variables to latent causal variables. Further, to promote the
disentanglement of causal factors, we propose a causal disentanglement prior
that utilizes the known causal structure to encourage learning a causally
factorized distribution in the latent space. Under relatively mild conditions,
we provide theoretical results showing the identifiability of causal factors
and mechanisms up to permutation and elementwise reparameterization. We
empirically demonstrate that our framework induces highly disentangled causal
factors, improves interventional robustness, and is compatible with
counterfactual generation.
</p></li>
</ul>

<h3>Title: Calibrating Multimodal Learning. (arXiv:2306.01265v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01265">http://arxiv.org/abs/2306.01265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01265] Calibrating Multimodal Learning](http://arxiv.org/abs/2306.01265) #robust</code></li>
<li>Summary: <p>Multimodal machine learning has achieved remarkable progress in a wide range
of scenarios. However, the reliability of multimodal learning remains largely
unexplored. In this paper, through extensive empirical studies, we identify
current multimodal classification methods suffer from unreliable predictive
confidence that tend to rely on partial modalities when estimating confidence.
Specifically, we find that the confidence estimated by current models could
even increase when some modalities are corrupted. To address the issue, we
introduce an intuitive principle for multimodal learning, i.e., the confidence
should not increase when one modality is removed. Accordingly, we propose a
novel regularization technique, i.e., Calibrating Multimodal Learning (CML)
regularization, to calibrate the predictive confidence of previous methods.
This technique could be flexibly equipped by existing models and improve the
performance in terms of confidence calibration, classification accuracy, and
model robustness.
</p></li>
</ul>

<h3>Title: Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01271">http://arxiv.org/abs/2306.01271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01271] Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training](http://arxiv.org/abs/2306.01271) #robust</code></li>
<li>Summary: <p>Adversarial training is a standard method to train deep neural networks to be
robust to adversarial perturbation. Similar to surprising $\textit{clean
generalization}$ ability in the standard deep learning setting, neural networks
trained by adversarial training also generalize well for $\textit{unseen clean
data}$. However, in constrast with clean generalization, while adversarial
training method is able to achieve low $\textit{robust training error}$, there
still exists a significant $\textit{robust generalization gap}$, which promotes
us exploring what mechanism leads to both $\textit{clean generalization and
robust overfitting (CGRO)}$ during learning process. In this paper, we provide
a theoretical understanding of this CGRO phenomenon in adversarial training.
First, we propose a theoretical framework of adversarial training, where we
analyze $\textit{feature learning process}$ to explain how adversarial training
leads network learner to CGRO regime. Specifically, we prove that, under our
patch-structured dataset, the CNN model provably partially learns the true
feature but exactly memorizes the spurious features from training-adversarial
examples, which thus results in clean generalization and robust overfitting.
For more general data assumption, we then show the efficiency of CGRO
classifier from the perspective of $\textit{representation complexity}$. On the
empirical side, to verify our theoretical analysis in real-world vision
dataset, we investigate the $\textit{dynamics of loss landscape}$ during
training. Moreover, inspired by our experiments, we prove a robust
generalization bound based on $\textit{global flatness}$ of loss landscape,
which may be an independent interest.
</p></li>
</ul>

<h3>Title: A Closer Look at the Adversarial Robustness of Deep Equilibrium Models. (arXiv:2306.01429v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01429">http://arxiv.org/abs/2306.01429</a></li>
<li>Code URL: <a href="https://github.com/minicheshire/deq-white-box-robustness">https://github.com/minicheshire/deq-white-box-robustness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01429] A Closer Look at the Adversarial Robustness of Deep Equilibrium Models](http://arxiv.org/abs/2306.01429) #robust</code></li>
<li>Summary: <p>Deep equilibrium models (DEQs) refrain from the traditional layer-stacking
paradigm and turn to find the fixed point of a single layer. DEQs have achieved
promising performance on different applications with featured memory
efficiency. At the same time, the adversarial vulnerability of DEQs raises
concerns. Several works propose to certify robustness for monotone DEQs.
However, limited efforts are devoted to studying empirical robustness for
general DEQs. To this end, we observe that an adversarially trained DEQ
requires more forward steps to arrive at the equilibrium state, or even
violates its fixed-point structure. Besides, the forward and backward tracks of
DEQs are misaligned due to the black-box solvers. These facts cause gradient
obfuscation when applying the ready-made attacks to evaluate or adversarially
train DEQs. Given this, we develop approaches to estimate the intermediate
gradients of DEQs and integrate them into the attacking pipelines. Our
approaches facilitate fully white-box evaluations and lead to effective
adversarial defense for DEQs. Extensive experiments on CIFAR-10 validate the
adversarial robustness of DEQs competitive with deep networks of similar sizes.
</p></li>
</ul>

<h3>Title: Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics. (arXiv:2306.01435v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01435">http://arxiv.org/abs/2306.01435</a></li>
<li>Code URL: <a href="https://github.com/minicheshire/deq-regulating-neural-dynamics">https://github.com/minicheshire/deq-regulating-neural-dynamics</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01435] Improving Adversarial Robustness of DEQs with Explicit Regulations Along the Neural Dynamics](http://arxiv.org/abs/2306.01435) #robust</code></li>
<li>Summary: <p>Deep equilibrium (DEQ) models replace the multiple-layer stacking of
conventional deep networks with a fixed-point iteration of a single-layer
transformation. Having been demonstrated to be competitive in a variety of
real-world scenarios, the adversarial robustness of general DEQs becomes
increasingly crucial for their reliable deployment. Existing works improve the
robustness of general DEQ models with the widely-used adversarial training (AT)
framework, but they fail to exploit the structural uniquenesses of DEQ models.
To this end, we interpret DEQs through the lens of neural dynamics and find
that AT under-regulates intermediate states. Besides, the intermediate states
typically provide predictions with a high prediction entropy. Informed by the
correlation between the entropy of dynamical systems and their stability
properties, we propose reducing prediction entropy by progressively updating
inputs along the neural dynamics. During AT, we also utilize random
intermediate states to compute the loss function. Our methods regulate the
neural dynamics of DEQ models in this manner. Extensive experiments demonstrate
that our methods substantially increase the robustness of DEQ models and even
outperform the strong deep network baselines.
</p></li>
</ul>

<h3>Title: Multi-Objective Population Based Training. (arXiv:2306.01436v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01436">http://arxiv.org/abs/2306.01436</a></li>
<li>Code URL: <a href="https://github.com/arkadiyd/mo-pbt">https://github.com/arkadiyd/mo-pbt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01436] Multi-Objective Population Based Training](http://arxiv.org/abs/2306.01436) #robust</code></li>
<li>Summary: <p>Population Based Training (PBT) is an efficient hyperparameter optimization
algorithm. PBT is a single-objective algorithm, but many real-world
hyperparameter optimization problems involve two or more conflicting
objectives. In this work, we therefore introduce a multi-objective version of
PBT, MO-PBT. Our experiments on diverse multi-objective hyperparameter
optimization problems (Precision/Recall, Accuracy/Fairness,
Accuracy/Adversarial Robustness) show that MO-PBT outperforms random search,
single-objective PBT, and the state-of-the-art multi-objective hyperparameter
optimization algorithm MO-ASHA.
</p></li>
</ul>

<h3>Title: Robust low-rank training via approximate orthonormal constraints. (arXiv:2306.01485v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01485">http://arxiv.org/abs/2306.01485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01485] Robust low-rank training via approximate orthonormal constraints](http://arxiv.org/abs/2306.01485) #robust</code></li>
<li>Summary: <p>With the growth of model and data sizes, a broad effort has been made to
design pruning techniques that reduce the resource demand of deep learning
pipelines, while retaining model performance. In order to reduce both inference
and training costs, a prominent line of work uses low-rank matrix
factorizations to represent the network weights. Although able to retain
accuracy, we observe that low-rank methods tend to compromise model robustness
against adversarial perturbations. By modeling robustness in terms of the
condition number of the neural network, we argue that this loss of robustness
is due to the exploding singular values of the low-rank weight matrices. Thus,
we introduce a robust low-rank training algorithm that maintains the network's
weights on the low-rank matrix manifold while simultaneously enforcing
approximate orthonormal constraints. The resulting model reduces both training
and inference costs while ensuring well-conditioning and thus better
adversarial robustness, without compromising model accuracy. This is shown by
extensive numerical evidence and by our main approximation theorem that shows
the computed robust low-rank network well-approximates the ideal full model,
provided a highly performing low-rank sub-network exists.
</p></li>
</ul>

<h3>Title: Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01631">http://arxiv.org/abs/2306.01631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01631] Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network](http://arxiv.org/abs/2306.01631) #robust</code></li>
<li>Summary: <p>The precise prediction of molecular properties holds paramount importance in
facilitating the development of innovative treatments and comprehending the
intricate interplay between chemicals and biological systems. In this study, we
propose a novel approach that integrates graph representations of individual
molecular structures with multi-domain information from biomedical knowledge
graphs (KGs). Integrating information from both levels, we can pre-train a more
extensive and robust representation for both molecule-level and KG-level
prediction tasks with our novel self-supervision strategy. For performance
evaluation, we fine-tune our pre-trained model on 11 challenging chemical
property prediction tasks. Results from our framework demonstrate our
fine-tuned models outperform existing state-of-the-art models.
</p></li>
</ul>

<h3>Title: MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01697">http://arxiv.org/abs/2306.01697</a></li>
<li>Code URL: <a href="https://github.com/luludak/mutatenn">https://github.com/luludak/mutatenn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01697] MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators](http://arxiv.org/abs/2306.01697) #robust</code></li>
<li>Summary: <p>With the research advancement of Artificial Intelligence in the last years,
there are new opportunities to mitigate real-world problems and advance
technologically. Image recognition models in particular, are assigned with
perception tasks to mitigate complex real-world challenges and lead to new
solutions. Furthermore, the computational complexity and demand for resources
of such models has also increased. To mitigate this, model optimization and
hardware acceleration has come into play, but effectively integrating such
concepts is a challenging and error-prone process.
</p></li>
</ul>

<p>In order to allow developers and researchers to explore the robustness of
deep learning image recognition models deployed on different hardware
acceleration devices, we propose MutateNN, a tool that provides mutation
testing and analysis capabilities for that purpose. To showcase its
capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep
neural network models. We deployed our mutants on 4 different devices of
varying computational capabilities and observed discrepancies in mutants
related to conditional operations, as well as some unstable behaviour with
those related to arithmetic types.
</p>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01016">http://arxiv.org/abs/2306.01016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01016] PV2TEA: Patching Visual Modality to Textual-Established Information Extraction](http://arxiv.org/abs/2306.01016) #extraction</code></li>
<li>Summary: <p>Information extraction, e.g., attribute value extraction, has been
extensively studied and formulated based only on text. However, many attributes
can benefit from image-based extraction, like color, shape, pattern, among
others. The visual modality has long been underutilized, mainly due to
multimodal annotation difficulty. In this paper, we aim to patch the visual
modality to the textual-established attribute information extractor. The
cross-modality integration faces several unique challenges: (C1) images and
textual descriptions are loosely paired intra-sample and inter-samples; (C2)
images usually contain rich backgrounds that can mislead the prediction; (C3)
weakly supervised labels from textual-established extractors are biased for
multimodal training. We present PV2TEA, an encoder-decoder architecture
equipped with three bias reduction schemes: (S1) Augmented label-smoothed
contrast to improve the cross-modality alignment for loosely-paired image and
text; (S2) Attention-pruning that adaptively distinguishes the visual
foreground; (S3) Two-level neighborhood regularization that mitigates the label
textual bias via reliability estimation. Empirical results on real-world
e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1
increase over unimodal baselines.
</p></li>
</ul>

<h3>Title: Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung Disease Classification. (arXiv:2306.01111v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01111">http://arxiv.org/abs/2306.01111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01111] Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung Disease Classification](http://arxiv.org/abs/2306.01111) #extraction</code></li>
<li>Summary: <p>Interstitial lung diseases (ILD) present diagnostic challenges due to their
varied manifestations and overlapping imaging features. To address this, we
propose a machine learning approach that utilizes CLIP, a multimodal (image and
text) self-supervised model, for ILD classification. We extensively integrate
zero-shot CLIP throughout our workflow, starting from the initial extraction of
image patches from volumetric CT scans and proceeding to ILD classification
using "patch montages". Furthermore, we investigate how domain adaptive
pretraining (DAPT) CLIP with task-specific images (CT "patch montages"
extracted with ILD-specific prompts for CLIP) and/or text (lung-specific
sections of radiology reports) affects downstream ILD classification
performance. By leveraging CLIP-extracted "patch montages" and DAPT, we achieve
strong zero-shot ILD classification results, including an AUROC of 0.893,
without the need for any labeled training data. This work highlights the
versatility and potential of multimodal models like CLIP for medical image
classification tasks where labeled data is scarce.
</p></li>
</ul>

<h3>Title: Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study. (arXiv:2306.01169v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01169">http://arxiv.org/abs/2306.01169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01169] Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study](http://arxiv.org/abs/2306.01169) #extraction</code></li>
<li>Summary: <p>Text summarization is a downstream natural language processing (NLP) task
that challenges the understanding and generation capabilities of language
models. Considerable progress has been made in automatically summarizing short
texts, such as news articles, often leading to satisfactory results. However,
summarizing long documents remains a major challenge. This is due to the
complex contextual information in the text and the lack of open-source
benchmarking datasets and evaluation frameworks that can be used to develop and
test model performance. In this work, we use ChatGPT, the latest breakthrough
in the field of large language models (LLMs), together with the extractive
summarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a
hybrid extraction and summarization pipeline for long documents such as
business articles and books. We work with the world-renowned company
getAbstract AG and leverage their expertise and experience in professional book
summarization. A practical study has shown that machine-generated summaries can
perform at least as well as human-written summaries when evaluated using
current automated evaluation metrics. However, a closer examination of the
texts generated by ChatGPT through human evaluations has shown that there are
still critical issues in terms of text coherence, faithfulness, and style.
Overall, our results show that the use of ChatGPT is a very promising but not
yet mature approach for summarizing long documents and can at best serve as an
inspiration for human editors. We anticipate that our work will inform NLP
researchers about the extent to which ChatGPT's capabilities for summarizing
long documents overlap with practitioners' needs. Further work is needed to
test the proposed hybrid summarization pipeline, in particular involving GPT-4,
and to propose a new evaluation framework tailored to the task of summarizing
long documents.
</p></li>
</ul>

<h3>Title: Chemical Property-Guided Neural Networks for Naphtha Composition Prediction. (arXiv:2306.01391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01391">http://arxiv.org/abs/2306.01391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01391] Chemical Property-Guided Neural Networks for Naphtha Composition Prediction](http://arxiv.org/abs/2306.01391) #extraction</code></li>
<li>Summary: <p>The naphtha cracking process heavily relies on the composition of naphtha,
which is a complex blend of different hydrocarbons. Predicting the naphtha
composition accurately is crucial for efficiently controlling the cracking
process and achieving maximum performance. Traditional methods, such as gas
chromatography and true boiling curve, are not feasible due to the need for
pilot-plant-scale experiments or cost constraints. In this paper, we propose a
neural network framework that utilizes chemical property information to improve
the performance of naphtha composition prediction. Our proposed framework
comprises two parts: a Watson K factor estimation network and a naphtha
composition prediction network. Both networks share a feature extraction
network based on Convolutional Neural Network (CNN) architecture, while the
output layers use Multi-Layer Perceptron (MLP) based networks to generate two
different outputs - Watson K factor and naphtha composition. The naphtha
composition is expressed in percentages, and its sum should be 100%. To enhance
the naphtha composition prediction, we utilize a distillation simulator to
obtain the distillation curve from the naphtha composition, which is dependent
on its chemical properties. By designing a loss function between the estimated
and simulated Watson K factors, we improve the performance of both Watson K
estimation and naphtha composition prediction. The experimental results show
that our proposed framework can predict the naphtha composition accurately
while reflecting real naphtha chemical properties.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Graph Learning for Low Probability of Detection in Wireless Ad-Hoc Networks. (arXiv:2306.01143v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01143">http://arxiv.org/abs/2306.01143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01143] Federated Graph Learning for Low Probability of Detection in Wireless Ad-Hoc Networks](http://arxiv.org/abs/2306.01143) #federate</code></li>
<li>Summary: <p>Low probability of detection (LPD) has recently emerged as a means to enhance
the privacy and security of wireless networks. Unlike existing wireless
security techniques, LPD measures aim to conceal the entire existence of
wireless communication instead of safeguarding the information transmitted from
users. Motivated by LPD communication, in this paper, we study a
privacy-preserving and distributed framework based on graph neural networks to
minimise the detectability of a wireless ad-hoc network as a whole and predict
an optimal communication region for each node in the wireless network, allowing
them to communicate while remaining undetected from external actors. We also
demonstrate the effectiveness of the proposed method in terms of two
performance measures, i.e., mean absolute error and median absolute error.
</p></li>
</ul>

<h3>Title: Federated Learning of Models Pre-Trained on Different Features with Consensus Graphs. (arXiv:2306.01240v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01240">http://arxiv.org/abs/2306.01240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01240] Federated Learning of Models Pre-Trained on Different Features with Consensus Graphs](http://arxiv.org/abs/2306.01240) #federate</code></li>
<li>Summary: <p>Learning an effective global model on private and decentralized datasets has
become an increasingly important challenge of machine learning when applied in
practice. Existing distributed learning paradigms, such as Federated Learning,
enable this via model aggregation which enforces a strong form of modeling
homogeneity and synchronicity across clients. This is however not suitable to
many practical scenarios. For example, in distributed sensing, heterogeneous
sensors reading data from different views of the same phenomenon would need to
use different models for different data modalities. Local learning therefore
happens in isolation but inference requires merging the local models to achieve
consensus. To enable consensus among local models, we propose a feature fusion
approach that extracts local representations from local models and incorporates
them into a global representation that improves the prediction performance.
Achieving this requires addressing two non-trivial problems. First, we need to
learn an alignment between similar feature components which are arbitrarily
arranged across clients to enable representation aggregation. Second, we need
to learn a consensus graph that captures the high-order interactions between
local feature spaces and how to combine them to achieve a better prediction.
This paper presents solutions to these problems and demonstrates them in
real-world applications on time series data such as power grids and traffic
networks.
</p></li>
</ul>

<h3>Title: Federated Learning Games for Reconfigurable Intelligent Surfaces via Causal Representations. (arXiv:2306.01306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01306">http://arxiv.org/abs/2306.01306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01306] Federated Learning Games for Reconfigurable Intelligent Surfaces via Causal Representations](http://arxiv.org/abs/2306.01306) #federate</code></li>
<li>Summary: <p>In this paper, we investigate the problem of robust Reconfigurable
Intelligent Surface (RIS) phase-shifts configuration over heterogeneous
communication environments. The problem is formulated as a distributed learning
problem over different environments in a Federated Learning (FL) setting.
Equivalently, this corresponds to a game played between multiple RISs, as
learning agents, in heterogeneous environments. Using Invariant Risk
Minimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS
configuration problem by learning invariant causal representations across
multiple environments and then predicting the phases. The solution corresponds
to playing according to Best Response Dynamics (BRD) which yields the Nash
Equilibrium of the FL game. The representation learner and the phase predictor
are modeled by two neural networks, and their performance is validated via
simulations against other benchmarks from the literature. Our results show that
causality-based learning yields a predictor that is 15% more accurate in unseen
Out-of-Distribution (OoD) environments.
</p></li>
</ul>

<h3>Title: Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01334">http://arxiv.org/abs/2306.01334</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01334] Federated Domain Generalization: A Survey](http://arxiv.org/abs/2306.01334) #federate</code></li>
<li>Summary: <p>Machine learning typically relies on the assumption that training and testing
distributions are identical and that data is centrally stored for training and
testing. However, in real-world scenarios, distributions may differ
significantly and data is often distributed across different devices,
organizations, or edge nodes. Consequently, it is imperative to develop models
that can effectively generalize to unseen distributions where data is
distributed across different domains. In response to this challenge, there has
been a surge of interest in federated domain generalization (FDG) in recent
years. FDG combines the strengths of federated learning (FL) and domain
generalization (DG) techniques to enable multiple source domains to
collaboratively learn a model capable of directly generalizing to unseen
domains while preserving data privacy. However, generalizing the federated
model under domain shifts is a technically challenging problem that has
received scant attention in the research area so far. This paper presents the
first survey of recent advances in this area. Initially, we discuss the
development process from traditional machine learning to domain adaptation and
domain generalization, leading to FDG as well as provide the corresponding
formal definition. Then, we categorize recent methodologies into four classes:
federated domain alignment, data manipulation, learning strategies, and
aggregation optimization, and present suitable algorithms in detail for each
category. Next, we introduce commonly used datasets, applications, evaluations,
and benchmarks. Finally, we conclude this survey by providing some potential
research topics for the future.
</p></li>
</ul>

<h3>Title: Resource-Efficient Federated Hyperdimensional Computing. (arXiv:2306.01339v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01339">http://arxiv.org/abs/2306.01339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01339] Resource-Efficient Federated Hyperdimensional Computing](http://arxiv.org/abs/2306.01339) #federate</code></li>
<li>Summary: <p>In conventional federated hyperdimensional computing (HDC), training larger
models usually results in higher predictive performance but also requires more
computational, communication, and energy resources. If the system resources are
limited, one may have to sacrifice the predictive performance by reducing the
size of the HDC model. The proposed resource-efficient federated
hyperdimensional computing (RE-FHDC) framework alleviates such constraints by
training multiple smaller independent HDC sub-models and refining the
concatenated HDC model using the proposed dropout-inspired procedure. Our
numerical comparison demonstrates that the proposed framework achieves a
comparable or higher predictive performance while consuming less computational
and wireless resources than the baseline federated HDC implementation.
</p></li>
</ul>

<h3>Title: On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01431">http://arxiv.org/abs/2306.01431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01431] On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions](http://arxiv.org/abs/2306.01431) #federate</code></li>
<li>Summary: <p>As Federated Learning (FL) has gained increasing attention, it has become
widely acknowledged that straightforwardly applying stochastic gradient descent
(SGD) on the overall framework when learning over a sequence of tasks results
in the phenomenon known as <code>catastrophic forgetting''. Consequently, much FL
research has centered on devising federated increasing learning methods to
alleviate forgetting while augmenting knowledge. On the other hand, forgetting
is not always detrimental. The selective amnesia, also known as federated
unlearning, which entails the elimination of specific knowledge, can address
privacy concerns and create additional</code>space'' for acquiring new knowledge.
However, there is a scarcity of extensive surveys that encompass recent
advancements and provide a thorough examination of this issue. In this
manuscript, we present an extensive survey on the topic of knowledge editing
(augmentation/removal) in Federated Learning, with the goal of summarizing the
state-of-the-art research and expanding the perspective for various domains.
Initially, we introduce an integrated paradigm, referred to as Federated
Editable Learning (FEL), by reevaluating the entire lifecycle of FL. Secondly,
we provide a comprehensive overview of existing methods, evaluate their
position within the proposed paradigm, and emphasize the current challenges
they face. Lastly, we explore potential avenues for future research and
identify unresolved issues.
</p></li>
</ul>

<h3>Title: Decentralized Federated Learning: A Survey and Perspective. (arXiv:2306.01603v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01603">http://arxiv.org/abs/2306.01603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01603] Decentralized Federated Learning: A Survey and Perspective](http://arxiv.org/abs/2306.01603) #federate</code></li>
<li>Summary: <p>Federated learning (FL) has been gaining attention for its ability to share
knowledge while maintaining user data, protecting privacy, increasing learning
efficiency, and reducing communication overhead. Decentralized FL (DFL) is a
decentralized network architecture that eliminates the need for a central
server in contrast to centralized FL (CFL). DFL enables direct communication
between clients, resulting in significant savings in communication resources.
In this paper, a comprehensive survey and profound perspective is provided for
DFL. First, a review of the methodology, challenges, and variants of CFL is
conducted, laying the background of DFL. Then, a systematic and detailed
perspective on DFL is introduced, including iteration order, communication
protocols, network topologies, paradigm proposals, and temporal variability.
Next, based on the definition of DFL, several extended variants and
categorizations are proposed with state-of-the-art technologies. Lastly, in
addition to summarizing the current challenges in the DFL, some possible
solutions and future research directions are also discussed.
</p></li>
</ul>

<h3>Title: Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation. (arXiv:2306.01648v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01648">http://arxiv.org/abs/2306.01648</a></li>
<li>Code URL: <a href="https://github.com/ucr-optml/fedmsa">https://github.com/ucr-optml/fedmsa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01648] Federated Multi-Sequence Stochastic Approximation with Local Hypergradient Estimation](http://arxiv.org/abs/2306.01648) #federate</code></li>
<li>Summary: <p>Stochastic approximation with multiple coupled sequences (MSA) has found
broad applications in machine learning as it encompasses a rich class of
problems including bilevel optimization (BLO), multi-level compositional
optimization (MCO), and reinforcement learning (specifically, actor-critic
methods). However, designing provably-efficient federated algorithms for MSA
has been an elusive question even for the special case of double sequence
approximation (DSA). Towards this goal, we develop FedMSA which is the first
federated algorithm for MSA, and establish its near-optimal communication
complexity. As core novelties, (i) FedMSA enables the provable estimation of
hypergradients in BLO and MCO via local client updates, which has been a
notable bottleneck in prior theory, and (ii) our convergence guarantees are
sensitive to the heterogeneity-level of the problem. We also incorporate
momentum and variance reduction techniques to achieve further acceleration
leading to near-optimal rates. Finally, we provide experiments that support our
theory and demonstrate the empirical benefits of FedMSA. As an example, FedMSA
enables order-of-magnitude savings in communication rounds compared to prior
federated BLO schemes.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Proxy Re-encryption based Fair Trade Protocol for Digital Goods Transactions via Smart Contracts. (arXiv:2306.01299v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01299">http://arxiv.org/abs/2306.01299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01299] Proxy Re-encryption based Fair Trade Protocol for Digital Goods Transactions via Smart Contracts](http://arxiv.org/abs/2306.01299) #fair</code></li>
<li>Summary: <p>With the massive amount of digital data generated everyday, transactions of
digital goods become a trend. One of the essential requirements for such
transactions is fairness, which is defined as that both of the seller and the
buyer get what they want, or neither. Current fair trade protocols generally
involve a trusted third-party (TTP), which achieves fairness by heavily relying
on the TTP's behaviors and the two parties' trust in the TTP. With the
emergence of Blockchain, its decentralization and transparency make it a very
good candidate to replace the TTP. In this work, we attempt to design a secure
and fair protocol for digital goods transactions through smart contracts on
Blockchain. To ensure security of the digital goods, we propose an advanced
passive proxy re-encryption (PRE) scheme, which enables smart contracts to
transfer the decryption right to a buyer after receiving his/her payment.
Furthermore, based on smart contracts and the proposed passive PRE scheme, a
fair trade protocol for digital goods transactions is proposed, whose fairness
is guaranteed by the arbitration protocol. The proposed protocol supports
Ciphertext publicity and repeatable sale, while involving less number of
interactions. Comprehensive experiment results validate the feasibility and
effectiveness of the proposed protocol.
</p></li>
</ul>

<h3>Title: Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01007">http://arxiv.org/abs/2306.01007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01007] Towards Fair Disentangled Online Learning for Changing Environments](http://arxiv.org/abs/2306.01007) #fair</code></li>
<li>Summary: <p>In the problem of online learning for changing environments, data are
sequentially received one after another over time, and their distribution
assumptions may vary frequently. Although existing methods demonstrate the
effectiveness of their learning algorithms by providing a tight bound on either
dynamic regret or adaptive regret, most of them completely ignore learning with
model fairness, defined as the statistical parity across different
sub-population (e.g., race and gender). Another drawback is that when adapting
to a new environment, an online learner needs to update model parameters with a
global change, which is costly and inefficient. Inspired by the sparse
mechanism shift hypothesis, we claim that changing environments in online
learning can be attributed to partial changes in learned parameters that are
specific to environments and the rest remain invariant to changing
environments. To this end, in this paper, we propose a novel algorithm under
the assumption that data collected at each time can be disentangled with two
representations, an environment-invariant semantic factor and an
environment-specific variation factor. The semantic factor is further used for
fair prediction under a group fairness constraint. To evaluate the sequence of
model parameters generated by the learner, a novel regret is proposed in which
it takes a mixed form of dynamic and static regret metrics followed by a
fairness-aware long-term constraint. The detailed analysis provides theoretical
guarantees for loss regret and violation of cumulative fairness constraints.
Empirical evaluations on real-world datasets demonstrate our proposed method
sequentially outperforms baseline methods in model accuracy and fairness.
</p></li>
</ul>

<h3>Title: Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01147">http://arxiv.org/abs/2306.01147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01147] Smooth Monotonic Networks](http://arxiv.org/abs/2306.01147) #fair</code></li>
<li>Summary: <p>Monotonicity constraints are powerful regularizers in statistical modelling.
They can support fairness in computer supported decision making and increase
plausibility in data-driven scientific models. The seminal min-max (MM) neural
network architecture ensures monotonicity, but often gets stuck in undesired
local optima during training because of vanishing gradients. We propose a
simple modification of the MM network using strictly-increasing smooth
non-linearities that alleviates this problem. The resulting smooth min-max
(SMM) network module inherits the asymptotic approximation properties from the
MM architecture. It can be used within larger deep learning systems trained
end-to-end. The SMM module is considerably simpler and less computationally
demanding than state-of-the-art neural networks for monotonic modelling. Still,
in our experiments, it compared favorably to alternative neural and non-neural
approaches in terms of generalization performance.
</p></li>
</ul>

<h3>Title: Hyperparameters in Reinforcement Learning and How To Tune Them. (arXiv:2306.01324v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01324">http://arxiv.org/abs/2306.01324</a></li>
<li>Code URL: <a href="https://github.com/facebookresearch/how-to-autorl">https://github.com/facebookresearch/how-to-autorl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01324] Hyperparameters in Reinforcement Learning and How To Tune Them](http://arxiv.org/abs/2306.01324) #fair</code></li>
<li>Summary: <p>In order to improve reproducibility, deep reinforcement learning (RL) has
been adopting better scientific practices such as standardized evaluation
metrics and reporting. However, the process of hyperparameter optimization
still varies widely across papers, which makes it challenging to compare RL
algorithms fairly. In this paper, we show that hyperparameter choices in RL can
significantly affect the agent's final performance and sample efficiency, and
that the hyperparameter landscape can strongly depend on the tuning seed which
may lead to overfitting. We therefore propose adopting established best
practices from AutoML, such as the separation of tuning and testing seeds, as
well as principled hyperparameter optimization (HPO) across a broad search
space. We support this by comparing multiple state-of-the-art HPO tools on a
range of RL algorithms and environments to their hand-tuned counterparts,
demonstrating that HPO approaches often have higher performance and lower
compute overhead. As a result of our findings, we recommend a set of best
practices for the RL community, which should result in stronger empirical
results with fewer computational costs, better reproducibility, and thus faster
progress. In order to encourage the adoption of these practices, we provide
plug-and-play implementations of the tuning algorithms used in this paper at
https://github.com/facebookresearch/how-to-autorl.
</p></li>
</ul>

<h3>Title: Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations. (arXiv:2306.01333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01333">http://arxiv.org/abs/2306.01333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01333] Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations](http://arxiv.org/abs/2306.01333) #fair</code></li>
<li>Summary: <p>Artificial Intelligence (AI) has significantly revolutionized radiology,
promising improved patient outcomes and streamlined processes. However, it's
critical to ensure the fairness of AI models to prevent stealthy bias and
disparities from leading to unequal outcomes. This review discusses the concept
of fairness in AI, focusing on bias auditing using the Aequitas toolkit, and
its real-world implications in radiology, particularly in disease screening
scenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models'
decisions, identifying hidden biases that may result in disparities across
different demographic groups and imaging equipment brands. This toolkit
operates on statistical theories, analyzing a large dataset to reveal a model's
fairness. It excels in its versatility to handle various variables
simultaneously, especially in a field as diverse as radiology. The review
explicates essential fairness metrics: Equal and Proportional Parity, False
Positive Rate Parity, False Discovery Rate Parity, False Negative Rate Parity,
and False Omission Rate Parity. Each metric serves unique purposes and offers
different insights. We present hypothetical scenarios to demonstrate their
relevance in disease screening settings, and how disparities can lead to
significant real-world impacts.
</p></li>
</ul>

<h3>Title: Fair multilingual vandalism detection system for Wikipedia. (arXiv:2306.01650v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01650">http://arxiv.org/abs/2306.01650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01650] Fair multilingual vandalism detection system for Wikipedia](http://arxiv.org/abs/2306.01650) #fair</code></li>
<li>Summary: <p>This paper presents a novel design of the system aimed at supporting the
Wikipedia community in addressing vandalism on the platform. To achieve this,
we collected a massive dataset of 47 languages, and applied advanced filtering
and feature engineering techniques, including multilingual masked language
modeling to build the training dataset from human-generated data. The
performance of the system was evaluated through comparison with the one used in
production in Wikipedia, known as ORES. Our research results in a significant
increase in the number of languages covered, making Wikipedia patrolling more
efficient to a wider range of communities. Furthermore, our model outperforms
ORES, ensuring that the results provided are not only more accurate but also
less biased against certain groups of contributors.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01439">http://arxiv.org/abs/2306.01439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01439] Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction](http://arxiv.org/abs/2306.01439) #interpretability</code></li>
<li>Summary: <p>The limited priors required by neural networks make them the dominating
choice to encode and learn policies using reinforcement learning (RL). However,
they are also black-boxes, making it hard to understand the agent's behaviour,
especially when working on the image level. Therefore, neuro-symbolic RL aims
at creating policies that are interpretable in the first place. Unfortunately,
interpretability is not explainability. To achieve both, we introduce Neurally
gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural
network-based agents to guide the search of candidate-weighted logic rules,
then uses differentiable logic to train the logic agents. Our experimental
evaluation demonstrates that NUDGE agents can induce interpretable and
explainable policies while outperforming purely neural ones and showing good
flexibility to environments of different initial states and problem sizes.
</p></li>
</ul>

<h3>Title: Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. (arXiv:2306.01499v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01499">http://arxiv.org/abs/2306.01499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01499] Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today](http://arxiv.org/abs/2306.01499) #interpretability</code></li>
<li>Summary: <p>Recent investigations show that large language models (LLMs), specifically
GPT-4, not only have remarkable capabilities in common Natural Language
Processing (NLP) tasks but also exhibit human-level performance on various
professional and academic benchmarks. However, whether GPT-4 can be directly
used in practical applications and replace traditional artificial intelligence
(AI) tools in specialized domains requires further experimental validation. In
this paper, we explore the potential of LLMs such as GPT-4 to outperform
traditional AI tools in dementia diagnosis. Comprehensive comparisons between
GPT-4 and traditional AI tools are conducted to examine their diagnostic
accuracy in a clinical setting. Experimental results on two real clinical
datasets show that, although LLMs like GPT-4 demonstrate potential for future
advancements in dementia diagnosis, they currently do not surpass the
performance of traditional AI tools. The interpretability and faithfulness of
GPT-4 are also evaluated by comparison with real doctors. We discuss the
limitations of GPT-4 in its current state and propose future research
directions to enhance GPT-4 in dementia diagnosis.
</p></li>
</ul>

<h3>Title: SPINEX: Similarity-based Predictions and Explainable Neighbors Exploration for Regression and Classification Tasks in Machine Learning. (arXiv:2306.01029v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01029">http://arxiv.org/abs/2306.01029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01029] SPINEX: Similarity-based Predictions and Explainable Neighbors Exploration for Regression and Classification Tasks in Machine Learning](http://arxiv.org/abs/2306.01029) #interpretability</code></li>
<li>Summary: <p>The field of machine learning (ML) has witnessed significant advancements in
recent years. However, many existing algorithms lack interpretability and
struggle with high-dimensional and imbalanced data. This paper proposes SPINEX,
a novel similarity-based interpretable neighbor exploration algorithm designed
to address these limitations. This algorithm combines ensemble learning and
feature interaction analysis to achieve accurate predictions and meaningful
insights by quantifying each feature's contribution to predictions and
identifying interactions between features, thereby enhancing the
interpretability of the algorithm. To evaluate the performance of SPINEX,
extensive experiments on 59 synthetic and real datasets were conducted for both
regression and classification tasks. The results demonstrate that SPINEX
achieves comparative performance and, in some scenarios, may outperform
commonly adopted ML algorithms. The same findings demonstrate the effectiveness
and competitiveness of SPINEX, making it a promising approach for various
real-world applications.
</p></li>
</ul>

<h3>Title: Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables. (arXiv:2306.01464v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01464">http://arxiv.org/abs/2306.01464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01464] Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables](http://arxiv.org/abs/2306.01464) #interpretability</code></li>
<li>Summary: <p>In recent years, the community of 'explainable artificial intelligence' (XAI)
has created a vast body of methods to bridge a perceived gap between model
'complexity' and 'interpretability'. However, a concrete problem to be solved
by XAI methods has not yet been formally stated. As a result, XAI methods are
lacking theoretical and empirical evidence for the 'correctness' of their
explanations, limiting their potential use for quality-control and transparency
purposes. At the same time, Haufe et al. (2014) showed, using simple toy
examples, that even standard interpretations of linear models can be highly
misleading. Specifically, high importance may be attributed to so-called
suppressor variables lacking any statistical relation to the prediction target.
This behavior has been confirmed empirically for a large array of XAI methods
in Wilming et al. (2022). Here, we go one step further by deriving analytical
expressions for the behavior of a variety of popular XAI methods on a simple
two-dimensional binary classification problem involving Gaussian
class-conditional distributions. We show that the majority of the studied
approaches will attribute non-zero importance to a non-class-related suppressor
feature in the presence of correlated noise. This poses important limitations
on the interpretations and conclusions that the outputs of these XAI methods
can afford.
</p></li>
</ul>

<h3>Title: Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01589">http://arxiv.org/abs/2306.01589</a></li>
<li>Code URL: <a href="https://github.com/isakfalk/atomistic_transfer_mekrr">https://github.com/isakfalk/atomistic_transfer_mekrr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01589] Transfer learning for atomistic simulations using GNNs and kernel mean embeddings](http://arxiv.org/abs/2306.01589) #interpretability</code></li>
<li>Summary: <p>Interatomic potentials learned using machine learning methods have been
successfully applied to atomistic simulations. However, deep learning pipelines
are notoriously data-hungry, while generating reference calculations is
computationally demanding. To overcome this difficulty, we propose a transfer
learning algorithm that leverages the ability of graph neural networks (GNNs)
in describing chemical environments, together with kernel mean embeddings. We
extract a feature map from GNNs pre-trained on the OC20 dataset and use it to
learn the potential energy surface from system-specific datasets of catalytic
processes. Our method is further enhanced by a flexible kernel function that
incorporates chemical species information, resulting in improved performance
and interpretability. We test our approach on a series of realistic datasets of
increasing complexity, showing excellent generalization and transferability
performance, and improving on methods that rely on GNNs or ridge regression
alone, as well as similar fine-tuning approaches. We make the code available to
the community at https://github.com/IsakFalk/atomistic_transfer_mekrr.
</p></li>
</ul>

<h3>Title: XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models. (arXiv:2306.01668v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01668">http://arxiv.org/abs/2306.01668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01668] XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models](http://arxiv.org/abs/2306.01668) #interpretability</code></li>
<li>Summary: <p>As machine learning models become increasingly prevalent in medical
diagnostics, the need for interpretability and transparency becomes paramount.
The XAI Renaissance signifies a significant shift in the field, aiming to
redefine the interpretability of medical diagnostic models. This paper explores
the innovative approaches and methodologies within the realm of Explainable AI
(XAI) that are revolutionizing the interpretability of medical diagnostic
models. By shedding light on the underlying decision-making process, XAI
techniques empower healthcare professionals to understand, trust, and
effectively utilize these models for accurate and reliable medical diagnoses.
This review highlights the key advancements in XAI for medical diagnostics and
their potential to transform the healthcare landscape, ultimately improving
patient outcomes and fostering trust in AI-driven diagnostic systems.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models. (arXiv:2306.01461v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01461">http://arxiv.org/abs/2306.01461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01461] PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models](http://arxiv.org/abs/2306.01461) #diffusion</code></li>
<li>Summary: <p>This paper presents PolyDiffuse, a novel structured reconstruction algorithm
that transforms visual sensor data into polygonal shapes with Diffusion Models
(DM), an emerging machinery amid exploding generative AI, while formulating
reconstruction as a generation process conditioned on sensor data. The task of
structured reconstruction poses two fundamental challenges to DM: 1) A
structured geometry is a <code>set'' (e.g., a set of polygons for a floorplan
geometry), where a sample of $N$ elements has $N!$ different but equivalent
representations, making the denoising highly ambiguous; and 2) A</code>reconstruction'' task has a single solution, where an initial noise needs to
be chosen carefully, while any initial noise works for a generation task. Our
technical contribution is the introduction of a Guided Set Diffusion Model
where 1) the forward diffusion process learns guidance networks to control
noise injection so that one representation of a sample remains distinct from
its other permutation variants, thus resolving denoising ambiguity; and 2) the
reverse denoising process reconstructs polygonal shapes, initialized and
directed by the guidance networks, as a conditional generation process subject
to the sensor data. We have evaluated our approach for reconstructing two types
of polygonal shapes: floorplan as a set of polygons and HD map for autonomous
cars as a set of polylines. Through extensive experiments on standard
benchmarks, we demonstrate that PolyDiffuse significantly advances the current
state of the art and enables broader practical applications.
</p></li>
</ul>

<h3>Title: Denoising Diffusion Semantic Segmentation with Mask Prior Modeling. (arXiv:2306.01721v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01721">http://arxiv.org/abs/2306.01721</a></li>
<li>Code URL: <a href="https://github.com/opengvlab/ddps">https://github.com/opengvlab/ddps</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01721] Denoising Diffusion Semantic Segmentation with Mask Prior Modeling](http://arxiv.org/abs/2306.01721) #diffusion</code></li>
<li>Summary: <p>The evolution of semantic segmentation has long been dominated by learning
more discriminative image representations for classifying each pixel. Despite
the prominent advancements, the priors of segmentation masks themselves, e.g.,
geometric and semantic constraints, are still under-explored. In this paper, we
propose to ameliorate the semantic segmentation quality of existing
discriminative approaches with a mask prior modeled by a recently-developed
denoising diffusion generative model. Beginning with a unified architecture
that adapts diffusion models for mask prior modeling, we focus this work on a
specific instantiation with discrete diffusion and identify a variety of key
design choices for its successful application. Our exploratory analysis
revealed several important findings, including: (1) a simple integration of
diffusion models into semantic segmentation is not sufficient, and a
poorly-designed diffusion process might lead to degradation in segmentation
performance; (2) during the training, the object to which noise is added is
more important than the type of noise; (3) during the inference, the strict
diffusion denoising scheme may not be essential and can be relaxed to a simpler
scheme that even works better. We evaluate the proposed prior modeling with
several off-the-shelf segmentors, and our experimental results on ADE20K and
Cityscapes demonstrate that our approach could achieve competitively
quantitative performance and more appealing visual quality.
</p></li>
</ul>

<h3>Title: DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01657">http://arxiv.org/abs/2306.01657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01657] DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation](http://arxiv.org/abs/2306.01657) #diffusion</code></li>
<li>Summary: <p>Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.
</p></li>
</ul>

<h3>Title: DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01001">http://arxiv.org/abs/2306.01001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01001] DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model](http://arxiv.org/abs/2306.01001) #diffusion</code></li>
<li>Summary: <p>Electrical load forecasting is of great significance for the decision makings
in power systems, such as unit commitment and energy management. In recent
years, various self-supervised neural network-based methods have been applied
to electrical load forecasting to improve forecasting accuracy and capture
uncertainties. However, most current methods are based on Gaussian likelihood
methods, which aim to accurately estimate the distribution expectation under a
given covariate. This kind of approach is difficult to adapt to situations
where temporal data has a distribution shift and outliers. In this paper, we
propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty
and use the robust additive Cauchy distribution to estimate aleatoric
uncertainty. Rather than accurately forecasting conditional expectations, we
demonstrate our method's ability in separating two types of uncertainties and
dealing with the mutant scenarios.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Collect-and-Distribute Transformer for 3D Point Cloud Analysis. (arXiv:2306.01257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01257">http://arxiv.org/abs/2306.01257</a></li>
<li>Code URL: <a href="https://github.com/haibo-qiu/cdformer">https://github.com/haibo-qiu/cdformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01257] Collect-and-Distribute Transformer for 3D Point Cloud Analysis](http://arxiv.org/abs/2306.01257) #transformer</code></li>
<li>Summary: <p>Although remarkable advancements have been made recently in point cloud
analysis through the exploration of transformer architecture, it remains
challenging to effectively learn local and global structures within point
clouds. In this paper, we propose a new transformer architecture equipped with
a collect-and-distribute mechanism to communicate short- and long-range
contexts of point clouds, which we refer to as CDFormer. Specifically, we first
utilize self-attention to capture short-range interactions within each local
patch, and the updated local features are then collected into a set of proxy
reference points from which we can extract long-range contexts. Afterward, we
distribute the learned long-range contexts back to local points via
cross-attention. To address the position clues for short- and long-range
contexts, we also introduce context-aware position encoding to facilitate
position-aware communications between points. We perform experiments on four
popular point cloud datasets, namely ModelNet40, ScanObjectNN, S3DIS, and
ShapeNetPart, for classification and segmentation. Results show the
effectiveness of the proposed CDFormer, delivering several new state-of-the-art
performances on point cloud classification and segmentation tasks. The code is
available at \url{https://github.com/haibo-qiu/CDFormer}.
</p></li>
</ul>

<h3>Title: Transformer-based Annotation Bias-aware Medical Image Segmentation. (arXiv:2306.01340v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01340">http://arxiv.org/abs/2306.01340</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01340] Transformer-based Annotation Bias-aware Medical Image Segmentation](http://arxiv.org/abs/2306.01340) #transformer</code></li>
<li>Summary: <p>Manual medical image segmentation is subjective and suffers from
annotator-related bias, which can be mimicked or amplified by deep learning
methods. Recently, researchers have suggested that such bias is the combination
of the annotator preference and stochastic error, which are modeled by
convolution blocks located after decoder and pixel-wise independent Gaussian
distribution, respectively. It is unlikely that convolution blocks can
effectively model the varying degrees of preference at the full resolution
level. Additionally, the independent pixel-wise Gaussian distribution
disregards pixel correlations, leading to a discontinuous boundary. This paper
proposes a Transformer-based Annotation Bias-aware (TAB) medical image
segmentation model, which tackles the annotator-related bias via modeling
annotator preference and stochastic errors. TAB employs the Transformer with
learnable queries to extract the different preference-focused features. This
enables TAB to produce segmentation with various preferences simultaneously
using a single segmentation head. Moreover, TAB takes the multivariant normal
distribution assumption that models pixel correlations, and learns the
annotation distribution to disentangle the stochastic error. We evaluated our
TAB on an OD/OC segmentation benchmark annotated by six annotators. Our results
suggest that TAB outperforms existing medical image segmentation models which
take into account the annotator-related bias.
</p></li>
</ul>

<h3>Title: Adjustable Visual Appearance for Generalizable Novel View Synthesis. (arXiv:2306.01344v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01344">http://arxiv.org/abs/2306.01344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01344] Adjustable Visual Appearance for Generalizable Novel View Synthesis](http://arxiv.org/abs/2306.01344) #transformer</code></li>
<li>Summary: <p>We present a generalizable novel view synthesis method where it is possible
to modify the visual appearance of rendered views to match a target weather or
lighting condition. Our method is based on a generalizable transformer
architecture, trained on synthetically generated scenes under different
appearance conditions. This allows for rendering novel views in a consistent
manner of 3D scenes that were not included in the training set, along with the
ability to (i) modify their appearance to match the target condition and (ii)
smoothly interpolate between different conditions. Experiments on both real and
synthetic scenes are provided including both qualitative and quantitative
evaluations. Please refer to our project page for video results:
https://ava-nvs.github.io/
</p></li>
</ul>

<h3>Title: Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification. (arXiv:2306.01523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01523">http://arxiv.org/abs/2306.01523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01523] Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification](http://arxiv.org/abs/2306.01523) #transformer</code></li>
<li>Summary: <p>In this paper, we introduce a novel Synchronized Class Token Fusion (SCT
Fusion) architecture in the framework of multi-modal multi-label classification
(MLC) of remote sensing (RS) images. The proposed architecture leverages
modality-specific attention-based transformer encoders to process varying input
modalities, while exchanging information across modalities by synchronizing the
special class tokens after each transformer encoder block. The synchronization
involves fusing the class tokens with a trainable fusion transformation,
resulting in a synchronized class token that contains information from all
modalities. As the fusion transformation is trainable, it allows to reach an
accurate representation of the shared features among different modalities.
Experimental results show the effectiveness of the proposed architecture over
single-modality architectures and an early fusion multi-modal architecture when
evaluated on a multi-modal MLC dataset.
</p></li>
</ul>

<p>The code of the proposed architecture is publicly available at
https://git.tu-berlin.de/rsim/sct-fusion.
</p>

<h3>Title: A Novel Vision Transformer with Residual in Self-attention for Biomedical Image Classification. (arXiv:2306.01594v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01594">http://arxiv.org/abs/2306.01594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01594] A Novel Vision Transformer with Residual in Self-attention for Biomedical Image Classification](http://arxiv.org/abs/2306.01594) #transformer</code></li>
<li>Summary: <p>Biomedical image classification requires capturing of bio-informatics based
on specific feature distribution. In most of such applications, there are
mainly challenges due to limited availability of samples for diseased cases and
imbalanced nature of dataset. This article presents the novel framework of
multi-head self-attention for vision transformer (ViT) which makes capable of
capturing the specific image features for classification and analysis. The
proposed method uses the concept of residual connection for accumulating the
best attention output in each block of multi-head attention. The proposed
framework has been evaluated on two small datasets: (i) blood cell
classification dataset and (ii) brain tumor detection using brain MRI images.
The results show the significant improvement over traditional ViT and other
convolution based state-of-the-art classification models.
</p></li>
</ul>

<h3>Title: Backchannel Detection and Agreement Estimation from Video with Transformer Networks. (arXiv:2306.01656v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01656">http://arxiv.org/abs/2306.01656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01656] Backchannel Detection and Agreement Estimation from Video with Transformer Networks](http://arxiv.org/abs/2306.01656) #transformer</code></li>
<li>Summary: <p>Listeners use short interjections, so-called backchannels, to signify
attention or express agreement. The automatic analysis of this behavior is of
key importance for human conversation analysis and interactive conversational
agents. Current state-of-the-art approaches for backchannel analysis from
visual behavior make use of two types of features: features based on body pose
and features based on facial behavior. At the same time, transformer neural
networks have been established as an effective means to fuse input from
different data sources, but they have not yet been applied to backchannel
analysis. In this work, we conduct a comprehensive evaluation of multi-modal
transformer architectures for automatic backchannel analysis based on pose and
facial information. We address both the detection of backchannels as well as
the task of estimating the agreement expressed in a backchannel. In evaluations
on the MultiMediate'22 backchannel detection challenge, we reach 66.4% accuracy
with a one-layer transformer architecture, outperforming the previous state of
the art. With a two-layer transformer architecture, we furthermore set a new
state of the art (0.0604 MSE) on the task of estimating the amount of agreement
expressed in a backchannel.
</p></li>
</ul>

<h3>Title: MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates. (arXiv:2306.01685v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01685">http://arxiv.org/abs/2306.01685</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01685] MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates](http://arxiv.org/abs/2306.01685) #transformer</code></li>
<li>Summary: <p>This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using
Rank-1 updates, called MKOR, that improves the training time and convergence
properties of deep neural networks (DNNs). Second-order techniques, while
enjoying higher convergence rates vs first-order counterparts, have cubic
complexity with respect to either the model size and/or the training batch
size. Hence they exhibit poor scalability and performance in transformer
models, e.g. large language models (LLMs), because the batch sizes in these
models scale by the attention mechanism sequence length, leading to large model
size and batch sizes. MKOR's complexity is quadratic with respect to the model
size, alleviating the computation bottlenecks in second-order methods. Because
of their high computation complexity, state-of-the-art implementations of
second-order methods can only afford to update the second order information
infrequently, and thus do not fully exploit the promise of better convergence
from these updates. By reducing the communication complexity of the
second-order updates as well as achieving a linear communication complexity,
MKOR increases the frequency of second order updates. We also propose a hybrid
version of MKOR (called MKOR-H) that mid-training falls backs to a first order
optimizer if the second order updates no longer accelerate convergence. Our
experiments show that MKOR outperforms state -of-the-art first order methods,
e.g. the LAMB optimizer, and best implementations of second-order methods, i.e.
KAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64
GPUs.
</p></li>
</ul>

<h3>Title: Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01076">http://arxiv.org/abs/2306.01076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01076] Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding](http://arxiv.org/abs/2306.01076) #transformer</code></li>
<li>Summary: <p>Fine-tuned transformer models have shown superior performances in many
natural language tasks. However, the large model size prohibits deploying
high-performance transformer models on resource-constrained devices. This paper
proposes a quantization-aware tensor-compressed training approach to reduce the
model size, arithmetic operations, and ultimately runtime latency of
transformer-based models. We compress the embedding and linear layers of
transformers into small low-rank tensor cores, which significantly reduces
model parameters. A quantization-aware training with learnable scale factors is
used to further obtain low-precision representations of the tensor-compressed
models. The developed approach can be used for both end-to-end training and
distillation-based training. To improve the convergence, a layer-by-layer
distillation is applied to distill a quantized and tensor-compressed student
model from a pre-trained transformer. The performance is demonstrated in two
natural language understanding tasks, showing up to $63\times$ compression
ratio, little accuracy loss and remarkable inference and training speedup.
</p></li>
</ul>

<h3>Title: Learning Transformer Programs. (arXiv:2306.01128v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01128">http://arxiv.org/abs/2306.01128</a></li>
<li>Code URL: <a href="https://github.com/princeton-nlp/transformerprograms">https://github.com/princeton-nlp/transformerprograms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01128] Learning Transformer Programs](http://arxiv.org/abs/2306.01128) #transformer</code></li>
<li>Summary: <p>Recent research in mechanistic interpretability has attempted to
reverse-engineer Transformer models by carefully inspecting network weights and
activations. However, these approaches require considerable manual effort and
still fall short of providing complete, faithful descriptions of the underlying
algorithms. In this work, we introduce a procedure for training Transformers
that are mechanistically interpretable by design. We build on RASP [Weiss et
al., 2021], a programming language that can be compiled into Transformer
weights. Instead of compiling human-written programs into Transformers, we
design a modified Transformer that can be trained using gradient-based
optimization and then be automatically converted into a discrete,
human-readable program. We refer to these models as Transformer Programs. To
validate our approach, we learn Transformer Programs for a variety of problems,
including an in-context learning task, a suite of algorithmic problems (e.g.
sorting, recognizing Dyck-languages), and NLP tasks including named entity
recognition and text classification. The Transformer Programs can automatically
find reasonable solutions, performing on par with standard Transformers of
comparable size; and, more importantly, they are easy to interpret. To
demonstrate these advantages, we convert Transformers into Python programs and
use off-the-shelf code analysis tools to debug model errors and identify the
``circuits'' used to solve different sub-problems. We hope that Transformer
Programs open a new path toward the goal of intrinsically interpretable machine
learning.
</p></li>
</ul>

<h3>Title: Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01160">http://arxiv.org/abs/2306.01160</a></li>
<li>Code URL: <a href="https://github.com/epfml/dynamic-sparse-flash-attention">https://github.com/epfml/dynamic-sparse-flash-attention</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01160] Faster Causal Attention Over Large Sequences Through Sparse Flash Attention](http://arxiv.org/abs/2306.01160) #transformer</code></li>
<li>Summary: <p>Transformer-based language models have found many diverse applications
requiring them to process sequences of increasing length. For these
applications, the causal self-attention -- which is the only component scaling
quadratically w.r.t. the sequence length -- becomes a central concern. While
many works have proposed schemes to sparsify the attention patterns and reduce
the computational overhead of self-attention, those are often limited by
implementations concerns and end up imposing a simple and static structure over
the attention matrix. Conversely, implementing more dynamic sparse attentions
often results in runtimes significantly slower than computing the full
attention using the Flash implementation from Dao et al. (2022). We extend
FlashAttention to accommodate a large class of attention sparsity patterns
that, in particular, encompass key/query dropping and hashing-based attention.
This leads to implementations with no computational complexity overhead and a
multi-fold runtime speedup on top of FlashAttention. Even with relatively low
degrees of sparsity, our method improves visibly upon FlashAttention as the
sequence length increases. Without sacrificing perplexity, we increase the
training speed of a transformer language model by $2.0\times$ and $3.3\times$
for sequences of respectively $8k$ and $16k$ tokens.
</p></li>
</ul>

<h3>Title: Distilling Efficient Language-Specific Models for Cross-Lingual Transfer. (arXiv:2306.01709v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01709">http://arxiv.org/abs/2306.01709</a></li>
<li>Code URL: <a href="https://github.com/alanansell/bistil">https://github.com/alanansell/bistil</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01709] Distilling Efficient Language-Specific Models for Cross-Lingual Transfer](http://arxiv.org/abs/2306.01709) #transformer</code></li>
<li>Summary: <p>Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are
widely used for cross-lingual transfer learning. While these are pretrained to
represent hundreds of languages, end users of NLP systems are often interested
only in individual languages. For such purposes, the MMTs' language coverage
makes them unnecessarily expensive to deploy in terms of model size, inference
time, energy, and hardware cost. We thus propose to extract compressed,
language-specific models from MMTs which retain the capacity of the original
MMTs for cross-lingual transfer. This is achieved by distilling the MMT
bilingually, i.e., using data from only the source and target language of
interest. Specifically, we use a two-phase distillation approach, termed
BiStil: (i) the first phase distils a general bilingual model from the MMT,
while (ii) the second, task-specific phase sparsely fine-tunes the bilingual
"student" model using a task-tuned variant of the original MMT as its
"teacher". We evaluate this distillation technique in zero-shot cross-lingual
transfer across a number of standard cross-lingual benchmarks. The key results
indicate that the distilled models exhibit minimal degradation in target
language performance relative to the base MMT despite being significantly
smaller and faster. Furthermore, we find that they outperform multilingually
distilled models such as DistilmBERT and MiniLMv2 while having a very modest
training budget in comparison, even on a per-language basis. We also show that
bilingual models distilled from MMTs greatly outperform bilingual models
trained from scratch. Our code and models are available at
https://github.com/AlanAnsell/bistil.
</p></li>
</ul>

<h3>Title: Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans. (arXiv:2306.01729v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01729">http://arxiv.org/abs/2306.01729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01729] Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans](http://arxiv.org/abs/2306.01729) #transformer</code></li>
<li>Summary: <p>Task-oriented dialogue is difficult in part because it involves understanding
user intent, collecting information from the user, executing API calls, and
generating helpful and fluent responses. However, for complex tasks one must
also correctly do all of these things over multiple steps, and in a specific
order. While large pre-trained language models can be fine-tuned end-to-end to
create multi-step task-oriented dialogue agents that generate fluent text, our
experiments confirm that this approach alone cannot reliably perform new
multi-step tasks that are unseen during training. To address these limitations,
we augment the dialogue contexts given to \textmd{text2text} transformers with
known \textit{valid workflow names} and \textit{action plans}. Action plans
consist of sequences of actions required to accomplish a task, and are encoded
as simple sequences of keywords (e.g. verify-identity, pull-up-account,
reset-password, etc.). We perform extensive experiments on the Action-Based
Conversations Dataset (ABCD) with T5-small, base and large models, and show
that such models: a) are able to more readily generalize to unseen workflows by
following the provided plan, and b) are able to generalize to executing unseen
actions if they are provided in the plan. In contrast, models are unable to
fully accomplish new multi-step tasks when they are not provided action plan
information, even when given new valid workflow names.
</p></li>
</ul>

<h3>Title: White-Box Transformers via Sparse Rate Reduction. (arXiv:2306.01129v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01129">http://arxiv.org/abs/2306.01129</a></li>
<li>Code URL: <a href="https://github.com/ma-lab-berkeley/crate">https://github.com/ma-lab-berkeley/crate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01129] White-Box Transformers via Sparse Rate Reduction](http://arxiv.org/abs/2306.01129) #transformer</code></li>
<li>Summary: <p>In this paper, we contend that the objective of representation learning is to
compress and transform the distribution of the data, say sets of tokens,
towards a mixture of low-dimensional Gaussian distributions supported on
incoherent subspaces. The quality of the final representation can be measured
by a unified objective function called sparse rate reduction. From this
perspective, popular deep networks such as transformers can be naturally viewed
as realizing iterative schemes to optimize this objective incrementally.
Particularly, we show that the standard transformer block can be derived from
alternating optimization on complementary parts of this objective: the
multi-head self-attention operator can be viewed as a gradient descent step to
compress the token sets by minimizing their lossy coding rate, and the
subsequent multi-layer perceptron can be viewed as attempting to sparsify the
representation of the tokens. This leads to a family of white-box
transformer-like deep network architectures which are mathematically fully
interpretable. Despite their simplicity, experiments show that these networks
indeed learn to optimize the designed objective: they compress and sparsify
representations of large-scale real-world vision datasets such as ImageNet, and
achieve performance very close to thoroughly engineered transformers such as
ViT. Code is at \url{https://github.com/Ma-Lab-Berkeley/CRATE}.
</p></li>
</ul>

<h3>Title: Transforming ECG Diagnosis:An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection. (arXiv:2306.01249v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01249">http://arxiv.org/abs/2306.01249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01249] Transforming ECG Diagnosis:An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection](http://arxiv.org/abs/2306.01249) #transformer</code></li>
<li>Summary: <p>The emergence of deep learning has significantly enhanced the analysis of
electrocardiograms (ECGs), a non-invasive method that is essential for
assessing heart health. Despite the complexity of ECG interpretation, advanced
deep learning models outperform traditional methods. However, the increasing
complexity of ECG data and the need for real-time and accurate diagnosis
necessitate exploring more robust architectures, such as transformers. Here, we
present an in-depth review of transformer architectures that are applied to ECG
classification. Originally developed for natural language processing, these
models capture complex temporal relationships in ECG signals that other models
might overlook. We conducted an extensive search of the latest
transformer-based models and summarize them to discuss the advances and
challenges in their application and suggest potential future improvements. This
review serves as a valuable resource for researchers and practitioners and aims
to shed light on this innovative application in ECG interpretation.
</p></li>
</ul>

<h3>Title: Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01474">http://arxiv.org/abs/2306.01474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01474] Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning](http://arxiv.org/abs/2306.01474) #transformer</code></li>
<li>Summary: <p>Many processes in biology and drug discovery involve various 3D interactions
between different molecules, such as protein and protein, protein and small
molecule, etc. Designing a generalist model to learn universal molecular
interactions is valuable yet challenging, given that different molecules are
usually represented in different granularity. In this paper, we first propose
to universally represent a 3D molecule as a geometric graph of sets, in
contrast to conventional single-level representations. Upon the proposed
unified representation, we then propose a Generalist Equivariant Transformer
(GET) to effectively capture both sparse block-level and dense atom-level
interactions. To be specific, GET consists of a bilevel attention module, a
feed-forward module and a layer normalization module, where, notably, each
module is E(3) equivariant to meet the symmetry of 3D world. Extensive
experiments on the prediction of protein-protein affinity, ligand binding
affinity, and ligand efficacy prediction verify the effectiveness of our
proposed method against existing methods, and reveal its potential to learn
transferable knowledge across different domains and different tasks.
</p></li>
</ul>

<h3>Title: Centered Self-Attention Layers. (arXiv:2306.01610v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01610">http://arxiv.org/abs/2306.01610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01610] Centered Self-Attention Layers](http://arxiv.org/abs/2306.01610) #transformer</code></li>
<li>Summary: <p>The self-attention mechanism in transformers and the message-passing
mechanism in graph neural networks are repeatedly applied within deep learning
architectures. We show that this application inevitably leads to oversmoothing,
i.e., to similar representations at the deeper layers for different tokens in
transformers and different nodes in graph neural networks. Based on our
analysis, we present a correction term to the aggregating operator of these
mechanisms. Empirically, this simple term eliminates much of the oversmoothing
problem in visual transformers, obtaining performance in weakly supervised
segmentation that surpasses elaborate baseline methods that introduce multiple
auxiliary networks and training phrases. In graph neural networks, the
correction term enables the training of very deep architectures more
effectively than many recent solutions to the same problem.
</p></li>
</ul>

<h3>Title: Analyzing Credit Risk Model Problems through NLP-Based Clustering and Machine Learning: Insights from Validation Reports. (arXiv:2306.01618v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01618">http://arxiv.org/abs/2306.01618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01618] Analyzing Credit Risk Model Problems through NLP-Based Clustering and Machine Learning: Insights from Validation Reports](http://arxiv.org/abs/2306.01618) #transformer</code></li>
<li>Summary: <p>This paper explores the use of clustering methods and machine learning
algorithms, including Natural Language Processing (NLP), to identify and
classify problems identified in credit risk models through textual information
contained in validation reports. Using a unique dataset of 657 findings raised
by validation teams in a large international banking group between January 2019
and December 2022. The findings are classified into nine validation dimensions
and assigned a severity level by validators using their expert knowledge. The
authors use embedding generation for the findings' titles and observations
using four different pre-trained models, including "module_url" from
TensorFlow Hub and three models from the SentenceTransformer library, namely
"all-mpnet-base-v2", "all-MiniLM-L6-v2", and "paraphrase-mpnet-base-v2". The
paper uses and compares various clustering methods in grouping findings with
similar characteristics, enabling the identification of common problems within
each validation dimension and severity. The results of the study show that
clustering is an effective approach for identifying and classifying credit risk
model problems with accuracy higher than 60\%. The authors also employ machine
learning algorithms, including logistic regression and XGBoost, to predict the
validation dimension and its severity, achieving an accuracy of 80\% for
XGBoost algorithm. Furthermore, the study identifies the top 10 words that
predict a validation dimension and severity. Overall, this paper makes a
contribution by demonstrating the usefulness of clustering and machine learning
for analyzing textual information in validation reports, and providing insights
into the types of problems encountered in the development and validation of
credit risk models.
</p></li>
</ul>

<h3>Title: GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01690">http://arxiv.org/abs/2306.01690</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01690] GateON: an unsupervised method for large scale continual learning](http://arxiv.org/abs/2306.01690) #transformer</code></li>
<li>Summary: <p>The objective of continual learning (CL) is to learn tasks sequentially
without retraining on earlier tasks. However, when subjected to CL, traditional
neural networks exhibit catastrophic forgetting and limited generalization. To
overcome these problems, we introduce a novel method called 'Gate and Obstruct
Network' (GateON). GateON combines learnable gating of activity and online
estimation of parameter relevance to safeguard crucial knowledge from being
overwritten. Our method generates partially overlapping pathways between tasks
which permits forward and backward transfer during sequential learning. GateON
addresses the issue of network saturation after parameter fixation by a
re-activation mechanism of fixed neurons, enabling large-scale continual
learning. GateON is implemented on a wide range of networks (fully-connected,
CNN, Transformers), has low computational complexity, effectively learns up to
100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
CL-based NLP tasks.
</p></li>
</ul>

<h3>Title: The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01705">http://arxiv.org/abs/2306.01705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01705] The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles](http://arxiv.org/abs/2306.01705) #transformer</code></li>
<li>Summary: <p>Transformers use the dense self-attention mechanism which gives a lot of
flexibility for long-range connectivity. Over multiple layers of a deep
transformer, the number of possible connectivity patterns increases
exponentially. However, very few of these contribute to the performance of the
network, and even fewer are essential. We hypothesize that there are sparsely
connected sub-networks within a transformer, called information pathways which
can be trained independently. However, the dynamic (i.e., input-dependent)
nature of these pathways makes it difficult to prune dense self-attention
during training. But the overall distribution of these pathways is often
predictable. We take advantage of this fact to propose Stochastically
Subsampled self-Attention (SSA) - a general-purpose training strategy for
transformers that can reduce both the memory and computational cost of
self-attention by 4 to 8 times during training while also serving as a
regularization method - improving generalization over dense training. We show
that an ensemble of sub-models can be formed from the subsampled pathways
within a network, which can achieve better performance than its densely
attended counterpart. We perform experiments on a variety of NLP, computer
vision and graph learning tasks in both generative and discriminative settings
to provide empirical evidence for our claims and show the effectiveness of the
proposed method.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: 4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks. (arXiv:2306.01081v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01081">http://arxiv.org/abs/2306.01081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01081] 4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks](http://arxiv.org/abs/2306.01081) #generative</code></li>
<li>Summary: <p>Time varying sequences of 3D point clouds, or 4D point clouds, are now being
acquired at an increasing pace in several applications (e.g., LiDAR in
autonomous or assisted driving). In many cases, such volume of data is
transmitted, thus requiring that proper compression tools are applied to either
reduce the resolution or the bandwidth. In this paper, we propose a new
solution for upscaling and restoration of time-varying 3D video point clouds
after they have been heavily compressed. In consideration of recent growing
relevance of 3D applications, %We focused on a model allowing user-side
upscaling and artifact removal for 3D video point clouds, a real-time stream of
which would require . Our model consists of a specifically designed Graph
Convolutional Network (GCN) that combines Dynamic Edge Convolution and Graph
Attention Networks for feature aggregation in a Generative Adversarial setting.
By taking inspiration PointNet++, We present a different way to sample dense
point clouds with the intent to make these modules work in synergy to provide
each node enough features about its neighbourhood in order to later on generate
new vertices. Compared to other solutions in the literature that address the
same task, our proposed model is capable of obtaining comparable results in
terms of quality of the reconstruction, while using a substantially lower
number of parameters (about 300KB), making our solution deployable in edge
computing devices such as LiDAR.
</p></li>
</ul>

<h3>Title: DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection. (arXiv:2306.01272v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01272">http://arxiv.org/abs/2306.01272</a></li>
<li>Code URL: <a href="https://github.com/h-aboutalebi/deepfakeart">https://github.com/h-aboutalebi/deepfakeart</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01272] DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection](http://arxiv.org/abs/2306.01272) #generative</code></li>
<li>Summary: <p>The tremendous recent advances in generative artificial intelligence
techniques have led to significant successes and promise in a wide range of
different applications ranging from conversational agents and textual content
generation to voice and visual synthesis. Amid the rise in generative AI and
its increasing widespread adoption, there has been significant growing concern
over the use of generative AI for malicious purposes. In the realm of visual
content synthesis using generative AI, key areas of significant concern has
been image forgery (e.g., generation of images containing or derived from
copyright content), and data poisoning (i.e., generation of adversarially
contaminated images). Motivated to address these key concerns to encourage
responsible generative AI, we introduce the DeepfakeArt Challenge, a
large-scale challenge benchmark dataset designed specifically to aid in the
building of machine learning algorithms for generative AI art forgery and data
poisoning detection. Comprising of over 32,000 records across a variety of
generative forgery and data poisoning techniques, each entry consists of a pair
of images that are either forgeries / adversarially contaminated or not. Each
of the generated images in the DeepfakeArt Challenge benchmark dataset has been
quality checked in a comprehensive manner. The DeepfakeArt Challenge is a core
part of GenAI4Good, a global open source initiative for accelerating machine
learning for promoting responsible creation and deployment of generative AI for
good.
</p></li>
</ul>

<h3>Title: Quantifying Sample Anonymity in Score-Based Generative Models with Adversarial Fingerprinting. (arXiv:2306.01363v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01363">http://arxiv.org/abs/2306.01363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01363] Quantifying Sample Anonymity in Score-Based Generative Models with Adversarial Fingerprinting](http://arxiv.org/abs/2306.01363) #generative</code></li>
<li>Summary: <p>Recent advances in score-based generative models have led to a huge spike in
the development of downstream applications using generative models ranging from
data augmentation over image and video generation to anomaly detection. Despite
publicly available trained models, their potential to be used for privacy
preserving data sharing has not been fully explored yet. Training diffusion
models on private data and disseminating the models and weights rather than the
raw dataset paves the way for innovative large-scale data-sharing strategies,
particularly in healthcare, where safeguarding patients' personal health
information is paramount. However, publishing such models without individual
consent of, e.g., the patients from whom the data was acquired, necessitates
guarantees that identifiable training samples will never be reproduced, thus
protecting personal health data and satisfying the requirements of policymakers
and regulatory bodies. This paper introduces a method for estimating the upper
bound of the probability of reproducing identifiable training images during the
sampling process. This is achieved by designing an adversarial approach that
searches for anatomic fingerprints, such as medical devices or dermal art,
which could potentially be employed to re-identify training images. Our method
harnesses the learned score-based model to estimate the probability of the
entire subspace of the score function that may be utilized for one-to-one
reproduction of training samples. To validate our estimates, we generate
anomalies containing a fingerprint and investigate whether generated samples
from trained generative models can be uniquely mapped to the original training
samples. Overall our results show that privacy-breaching images are reproduced
at sampling time if the models were trained without care.
</p></li>
</ul>

<h3>Title: GANs Settle Scores!. (arXiv:2306.01654v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01654">http://arxiv.org/abs/2306.01654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01654] GANs Settle Scores!](http://arxiv.org/abs/2306.01654) #generative</code></li>
<li>Summary: <p>Generative adversarial networks (GANs) comprise a generator, trained to learn
the underlying distribution of the desired data, and a discriminator, trained
to distinguish real samples from those output by the generator. A majority of
GAN literature focuses on understanding the optimality of the discriminator
through integral probability metric (IPM) or divergence based analysis. In this
paper, we propose a unified approach to analyzing the generator optimization
through variational approach. In $f$-divergence-minimizing GANs, we show that
the optimal generator is the one that matches the score of its output
distribution with that of the data distribution, while in IPM GANs, we show
that this optimal generator matches score-like functions, involving the
flow-field of the kernel associated with a chosen IPM constraint space.
Further, the IPM-GAN optimization can be seen as one of smoothed
score-matching, where the scores of the data and the generator distributions
are convolved with the kernel associated with the constraint. The proposed
approach serves to unify score-based training and existing GAN flavors,
leveraging results from normalizing flows, while also providing explanations
for empirical phenomena such as the stability of non-saturating GAN losses.
Based on these results, we propose novel alternatives to $f$-GAN and IPM-GAN
training based on score and flow matching, and discriminator-guided Langevin
sampling.
</p></li>
</ul>

<h3>Title: Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?. (arXiv:2306.01706v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01706">http://arxiv.org/abs/2306.01706</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01706] Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?](http://arxiv.org/abs/2306.01706) #generative</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) aims to bridge the gap between source
and target domains in the absence of target domain labels using two main
techniques: input-level alignment (such as generative modeling and stylization)
and feature-level alignment (which matches the distribution of the feature
maps, e.g. gradient reversal layers). Motivated from the success of generative
modeling for image classification, stylization-based methods were recently
proposed for regression tasks, such as pose estimation. However, use of
input-level alignment via generative modeling and stylization incur additional
overhead and computational complexity which limit their use in real-world DA
tasks. To investigate the role of input-level alignment for DA, we ask the
following question: Is generative modeling-based stylization necessary for
visual domain adaptation in regression? Surprisingly, we find that
input-alignment has little effect on regression tasks as compared to
classification. Based on these insights, we develop a non-parametric
feature-level domain alignment method -- Implicit Stylization (ImSty) -- which
results in consistent improvements over SOTA regression task, without the need
for computationally intensive stylization and generative modeling. Our work
conducts a critical evaluation of the role of generative modeling and
stylization, at a time when these are also gaining popularity for domain
generalization.
</p></li>
</ul>

<h3>Title: Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01009">http://arxiv.org/abs/2306.01009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01009] Examining the Emergence of Deductive Reasoning in Generative Language Models](http://arxiv.org/abs/2306.01009) #generative</code></li>
<li>Summary: <p>We conduct a preliminary inquiry into the ability of generative transformer
models to deductively reason from premises provided. We observe notable
differences in the performance of models coming from different training setups
and find that the deductive reasoning ability increases with scale. Further, we
discover that the performance generally does not decrease with the length of
the deductive chain needed to reach the conclusion, with the exception of
OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of
transformer-decoder models, ranging from 117 million to 175 billion parameters
in size.
</p></li>
</ul>

<h3>Title: AbODE: Ab Initio Antibody Design using Conjoined ODEs. (arXiv:2306.01005v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01005">http://arxiv.org/abs/2306.01005</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01005] AbODE: Ab Initio Antibody Design using Conjoined ODEs](http://arxiv.org/abs/2306.01005) #generative</code></li>
<li>Summary: <p>Antibodies are Y-shaped proteins that neutralize pathogens and constitute the
core of our adaptive immune system. De novo generation of new antibodies that
target specific antigens holds the key to accelerating vaccine discovery.
However, this co-design of the amino acid sequence and the 3D structure
subsumes and accentuates some central challenges from multiple tasks, including
protein folding (sequence to structure), inverse folding (structure to
sequence), and docking (binding). We strive to surmount these challenges with a
new generative model AbODE that extends graph PDEs to accommodate both
contextual information and external interactions. Unlike existing approaches,
AbODE uses a single round of full-shot decoding and elicits continuous
differential attention that encapsulates and evolves with latent interactions
within the antibody as well as those involving the antigen. We unravel
fundamental connections between AbODE and temporal networks as well as
graph-matching networks. The proposed model significantly outperforms existing
methods on standard metrics across benchmarks.
</p></li>
</ul>

<h3>Title: On Feature Diversity in Energy-based Models. (arXiv:2306.01489v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01489">http://arxiv.org/abs/2306.01489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01489] On Feature Diversity in Energy-based Models](http://arxiv.org/abs/2306.01489) #generative</code></li>
<li>Summary: <p>Energy-based learning is a powerful learning paradigm that encapsulates
various discriminative and generative approaches. An energy-based model (EBM)
is typically formed of inner-model(s) that learn a combination of the different
features to generate an energy mapping for each input configuration. In this
paper, we focus on the diversity of the produced feature set. We extend the
probably approximately correct (PAC) theory of EBMs and analyze the effect of
redundancy reduction on the performance of EBMs. We derive generalization
bounds for various learning contexts, i.e., regression, classification, and
implicit regression, with different energy functions and we show that indeed
reducing redundancy of the feature set can consistently decrease the gap
between the true and empirical expectation of the energy and boosts the
performance of the model.
</p></li>
</ul>

<h3>Title: Balancing Exploration and Exploitation: Disentangled $\beta$-CVAE in De Novo Drug Design. (arXiv:2306.01683v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01683">http://arxiv.org/abs/2306.01683</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01683] Balancing Exploration and Exploitation: Disentangled $\beta$-CVAE in De Novo Drug Design](http://arxiv.org/abs/2306.01683) #generative</code></li>
<li>Summary: <p>Deep generative models have recently emerged as a promising de novo drug
design method. In this respect, deep generative conditional variational
autoencoder (CVAE) models are a powerful approach for generating novel
molecules with desired drug-like properties. However, molecular graph-based
models with disentanglement and multivariate explicit latent conditioning have
not been fully elucidated. To address this, we proposed a molecular-graph
$\beta$-CVAE model for de novo drug design. Here, we empirically tuned the
value of disentanglement and assessed its ability to generate molecules with
optimised univariate- or-multivariate properties. In particular, we optimised
the octanol-water partition coefficient (ClogP), molar refractivity (CMR),
quantitative estimate of drug-likeness (QED), and synthetic accessibility score
(SAS). Results suggest that a lower $\beta$ value increases the uniqueness of
generated molecules (exploration). Univariate optimisation results showed our
model generated molecular property averages of ClogP = 41.07% $\pm$ 0.01% and
CMR 66.76% $\pm$ 0.01% by the Ghose filter. Multivariate property optimisation
results showed that our model generated an average of 30.07% $\pm$ 0.01%
molecules for both desired properties. Furthermore, our model improved the QED
and SAS (exploitation) of molecules generated. Together, these results suggest
that the $\beta$-CVAE could balance exploration and exploitation through
disentanglement and is a promising model for de novo drug design, thus
providing a basis for future studies.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01006">http://arxiv.org/abs/2306.01006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01006] Scaling Evidence-based Instructional Design Expertise through Large Language Models](http://arxiv.org/abs/2306.01006) #large language model</code></li>
<li>Summary: <p>This paper presents a comprehensive exploration of leveraging Large Language
Models (LLMs), specifically GPT-4, in the field of instructional design. With a
focus on scaling evidence-based instructional design expertise, our research
aims to bridge the gap between theoretical educational studies and practical
implementation. We discuss the benefits and limitations of AI-driven content
generation, emphasizing the necessity of human oversight in ensuring the
quality of educational materials. This work is elucidated through two detailed
case studies where we applied GPT-4 in creating complex higher-order
assessments and active learning components for different courses. From our
experiences, we provide best practices for effectively using LLMs in
instructional design tasks, such as utilizing templates, fine-tuning, handling
unexpected output, implementing LLM chains, citing references, evaluating
output, creating rubrics, grading, and generating distractors. We also share
our vision of a future recommendation system, where a customized GPT-4 extracts
instructional design principles from educational studies and creates
personalized, evidence-supported strategies for users' unique educational
contexts. Our research contributes to understanding and optimally harnessing
the potential of AI-driven language models in enhancing educational outcomes.
</p></li>
</ul>

<h3>Title: Reimagining Retrieval Augmented Language Models for Answering Queries. (arXiv:2306.01061v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01061">http://arxiv.org/abs/2306.01061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01061] Reimagining Retrieval Augmented Language Models for Answering Queries](http://arxiv.org/abs/2306.01061) #large language model</code></li>
<li>Summary: <p>We present a reality check on large language models and inspect the promise
of retrieval augmented language models in comparison. Such language models are
semi-parametric, where models integrate model parameters and knowledge from
external data sources to make their predictions, as opposed to the parametric
nature of vanilla large language models. We give initial experimental findings
that semi-parametric architectures can be enhanced with views, a query
analyzer/planner, and provenance to make a significantly more powerful system
for question answering in terms of accuracy and efficiency, and potentially for
other NLP tasks
</p></li>
</ul>

<h3>Title: The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. (arXiv:2306.01116v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01116">http://arxiv.org/abs/2306.01116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01116] The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](http://arxiv.org/abs/2306.01116) #large language model</code></li>
<li>Summary: <p>Large language models are commonly trained on a mixture of filtered web data
and curated high-quality corpora, such as social media conversations, books, or
technical papers. This curation process is believed to be necessary to produce
performant models with broad zero-shot generalization abilities. However, as
larger models requiring pretraining on trillions of tokens are considered, it
is unclear how scalable is curation and whether we will run out of unique
high-quality data soon. At variance with previous beliefs, we show that
properly filtered and deduplicated web data alone can lead to powerful models;
even significantly outperforming models from the state-of-the-art trained on
The Pile. Despite extensive filtering, the high-quality data we extract from
the web is still plentiful, and we are able to obtain five trillion tokens from
CommonCrawl. We publicly release an extract of 600 billion tokens from our
RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.
</p></li>
</ul>

<h3>Title: Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning. (arXiv:2306.01150v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01150">http://arxiv.org/abs/2306.01150</a></li>
<li>Code URL: <a href="https://github.com/fanyin3639/rethinking-instruction-effectiveness">https://github.com/fanyin3639/rethinking-instruction-effectiveness</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01150] Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning](http://arxiv.org/abs/2306.01150) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have shown impressive performance in following
natural language instructions to solve unseen tasks. However, it remains
unclear whether models truly understand task definitions and whether the
human-written definitions are optimal. In this paper, we systematically study
the role of task definitions in instruction learning. We first conduct an
ablation analysis informed by human annotations to understand which parts of a
task definition are most important, and find that model performance only drops
substantially when removing contents describing the task output, in particular
label information. Next, we propose an automatic algorithm to compress task
definitions to a minimal supporting set of tokens, and find that 60\% of tokens
can be removed while maintaining or even improving model performance. Based on
these results, we propose two strategies to help models better leverage task
instructions: (1) providing only key information for tasks in a common
structured format, and (2) adding a meta-tuning stage to help the model better
understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L
improvement over 119 unseen test tasks.
</p></li>
</ul>

<h3>Title: Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation. (arXiv:2306.01183v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01183">http://arxiv.org/abs/2306.01183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01183] Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation](http://arxiv.org/abs/2306.01183) #large language model</code></li>
<li>Summary: <p>Very large language models (LLMs) perform extremely well on a spectrum of NLP
tasks in a zero-shot setting. However, little is known about their performance
on human-level NLP problems which rely on understanding psychological concepts,
such as assessing personality traits. In this work, we investigate the
zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'
social media posts. Through a set of systematic experiments, we find that
zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA
for broad classification upon injecting knowledge about the trait in the
prompts. However, when prompted to provide fine-grained classification, its
performance drops to close to a simple most frequent class (MFC) baseline. We
further analyze where GPT-3 performs better, as well as worse, than a
pretrained lexical model, illustrating systematic errors that suggest ways to
improve LLMs on human-level NLP tasks.
</p></li>
</ul>

<h3>Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. (arXiv:2306.01200v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01200">http://arxiv.org/abs/2306.01200</a></li>
<li>Code URL: <a href="https://github.com/jainsameer06/ice">https://github.com/jainsameer06/ice</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01200] Multi-Dimensional Evaluation of Text Summarization with In-Context Learning](http://arxiv.org/abs/2306.01200) #large language model</code></li>
<li>Summary: <p>Evaluation of natural language generation (NLG) is complex and
multi-dimensional. Generated text can be evaluated for fluency, coherence,
factuality, or any other dimensions of interest. Most frameworks that perform
such multi-dimensional evaluation require training on large manually or
synthetically generated datasets. In this paper, we study the efficacy of large
language models as multi-dimensional evaluators using in-context learning,
obviating the need for large training datasets. Our experiments show that
in-context learning-based evaluators are competitive with learned evaluation
frameworks for the task of text summarization, establishing state-of-the-art on
dimensions such as relevance and factual consistency. We then analyze the
effects of factors such as the selection and number of in-context examples on
performance. Finally, we study the efficacy of in-context learning based
evaluators in evaluating zero-shot summaries written by large language models
such as GPT-3.
</p></li>
</ul>

<h3>Title: How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01248">http://arxiv.org/abs/2306.01248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01248] How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?](http://arxiv.org/abs/2306.01248) #large language model</code></li>
<li>Summary: <p>Automatic summarization of legal case judgements has traditionally been
attempted by using extractive summarization methods. However, in recent years,
abstractive summarization models are gaining popularity since they can generate
more natural and coherent summaries. Legal domain-specific pre-trained
abstractive summarization models are now available. Moreover, general-domain
pre-trained Large Language Models (LLMs), such as ChatGPT, are known to
generate high-quality text and have the capacity for text summarization. Hence
it is natural to ask if these models are ready for off-the-shelf application to
automatically generate abstractive summaries for case judgements. To explore
this question, we apply several state-of-the-art domain-specific abstractive
summarization models and general-domain LLMs on Indian court case judgements,
and check the quality of the generated summaries. In addition to standard
metrics for summary quality, we check for inconsistencies and hallucinations in
the summaries. We see that abstractive summarization models generally achieve
slightly higher scores than extractive models in terms of standard summary
evaluation metrics such as ROUGE and BLEU. However, we often find inconsistent
or hallucinated information in the generated abstractive summaries. Overall,
our investigation indicates that the pre-trained abstractive summarization
models and LLMs are not yet ready for fully automatic deployment for case
judgement summarization; rather a human-in-the-loop approach including manual
checks for inconsistencies is more suitable at present.
</p></li>
</ul>

<h3>Title: An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01337">http://arxiv.org/abs/2306.01337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01337] An Empirical Study on Challenging Math Problem Solving with GPT-4](http://arxiv.org/abs/2306.01337) #large language model</code></li>
<li>Summary: <p>Employing Large Language Models (LLMs) to address mathematical problems is an
intriguing research endeavor, considering the abundance of math problems
expressed in natural language across numerous science and engineering fields.
While several prior works have investigated solving elementary mathematics
using LLMs, this work explores the frontier of using GPT-4 for solving more
complex and challenging math problems. We evaluate various ways of using GPT-4.
Some of them are adapted from existing work, and one is \MathChat, a
conversational problem-solving framework newly proposed in this work. We
perform the evaluation on difficult high school competition problems from the
MATH dataset, which shows the advantage of the proposed conversational
approach.
</p></li>
</ul>

<h3>Title: PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01545">http://arxiv.org/abs/2306.01545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01545] PassGPT: Password Modeling and (Guided) Generation with Large Language Models](http://arxiv.org/abs/2306.01545) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) successfully model natural language from vast
amounts of text without the need for explicit supervision. In this paper, we
investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a
LLM trained on password leaks for password generation. PassGPT outperforms
existing methods based on generative adversarial networks (GAN) by guessing
twice as many previously unseen passwords. Furthermore, we introduce the
concept of guided password generation, where we leverage PassGPT sampling
procedure to generate passwords matching arbitrary constraints, a feat lacking
in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the
entropy and probability distribution that PassGPT defines over passwords and
discuss their use in enhancing existing password strength estimators.
</p></li>
</ul>

<h3>Title: EmoUS: Simulating User Emotions in Task-Oriented Dialogues. (arXiv:2306.01579v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01579">http://arxiv.org/abs/2306.01579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01579] EmoUS: Simulating User Emotions in Task-Oriented Dialogues](http://arxiv.org/abs/2306.01579) #large language model</code></li>
<li>Summary: <p>Existing user simulators (USs) for task-oriented dialogue systems only model
user behaviour on semantic and natural language levels without considering the
user persona and emotions. Optimising dialogue systems with generic user
policies, which cannot model diverse user behaviour driven by different
emotional states, may result in a high drop-off rate when deployed in the real
world. Thus, we present EmoUS, a user simulator that learns to simulate user
emotions alongside user behaviour. EmoUS generates user emotions, semantic
actions, and natural language responses based on the user goal, the dialogue
history, and the user persona. By analysing what kind of system behaviour
elicits what kind of user emotions, we show that EmoUS can be used as a probe
to evaluate a variety of dialogue systems and in particular their effect on the
user's emotional state. Developing such methods is important in the age of
large language model chat-bots and rising ethical concerns.
</p></li>
</ul>

<h3>Title: Hierarchical Attention Encoder Decoder. (arXiv:2306.01070v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01070">http://arxiv.org/abs/2306.01070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01070] Hierarchical Attention Encoder Decoder](http://arxiv.org/abs/2306.01070) #large language model</code></li>
<li>Summary: <p>Recent advances in large language models have shown that autoregressive
modeling can generate complex and novel sequences that have many real-world
applications. However, these models must generate outputs autoregressively,
which becomes time-consuming when dealing with long sequences. Hierarchical
autoregressive approaches that compress data have been proposed as a solution,
but these methods still generate outputs at the original data frequency,
resulting in slow and memory-intensive models. In this paper, we propose a
model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture.
This model independently encodes input sub-sequences without global context,
processes these sequences using a lower-frequency model, and decodes outputs at
the original data frequency. By interpreting the encoder as an implicitly
defined embedding matrix and using sampled softmax estimation, we develop a
training algorithm that can train the entire model without a high-frequency
decoder, which is the most memory and compute-intensive part of hierarchical
approaches. In a final, brief phase, we train the decoder to generate data at
the original granularity. Our algorithm significantly reduces memory
requirements for training autoregressive models and it also improves the total
training wall-clock time.
</p></li>
</ul>

<h3>Title: Evaluating Language Models for Mathematics through Interactions. (arXiv:2306.01694v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01694">http://arxiv.org/abs/2306.01694</a></li>
<li>Code URL: <a href="https://github.com/collinskatie/checkmate">https://github.com/collinskatie/checkmate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01694] Evaluating Language Models for Mathematics through Interactions](http://arxiv.org/abs/2306.01694) #large language model</code></li>
<li>Summary: <p>The standard methodology of evaluating large language models (LLMs) based on
static pairs of inputs and outputs is insufficient for developing assistants:
this kind of assessments fails to take into account the essential interactive
element in their deployment, and therefore limits how we understand language
model capabilities. We introduce CheckMate, an adaptable prototype platform for
humans to interact with and evaluate LLMs. We conduct a study with CheckMate to
evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants
in proving undergraduate-level mathematics, with a mixed cohort of participants
from undergraduate students to professors of mathematics. We release the
resulting interaction and rating dataset, MathConverse. By analysing
MathConverse, we derive a preliminary taxonomy of human behaviours and uncover
that despite a generally positive correlation, there are notable instances of
divergence between correctness and perceived helpfulness in LLM generations,
amongst other findings. Further, we identify useful scenarios and existing
issues of GPT-4 in mathematical reasoning through a series of case studies
contributed by expert mathematicians. We conclude with actionable takeaways for
ML practitioners and mathematicians: models which communicate uncertainty,
respond well to user corrections, are more interpretable and concise may
constitute better assistants; interactive evaluation is a promising way to
continually navigate the capability of these models; humans should be aware of
language models' algebraic fallibility, and for that reason discern where they
should be used.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Segment Anything in High Quality. (arXiv:2306.01567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01567">http://arxiv.org/abs/2306.01567</a></li>
<li>Code URL: <a href="https://github.com/syscv/sam-hq">https://github.com/syscv/sam-hq</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01567] Segment Anything in High Quality](http://arxiv.org/abs/2306.01567) #segmentation</code></li>
<li>Summary: <p>The recent Segment Anything Model (SAM) represents a big leap in scaling up
segmentation models, allowing for powerful zero-shot capabilities and flexible
prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction
quality falls short in many cases, particularly when dealing with objects that
have intricate structures. We propose HQ-SAM, equipping SAM with the ability to
accurately segment any object, while maintaining SAM's original promptable
design, efficiency, and zero-shot generalizability. Our careful design reuses
and preserves the pre-trained model weights of SAM, while only introducing
minimal additional parameters and computation. We design a learnable
High-Quality Output Token, which is injected into SAM's mask decoder and is
responsible for predicting the high-quality mask. Instead of only applying it
on mask-decoder features, we first fuse them with early and final ViT features
for improved mask details. To train our introduced learnable parameters, we
compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is
only trained on the introduced detaset of 44k masks, which takes only 4 hours
on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 9 diverse segmentation
datasets across different downstream tasks, where 7 out of them are evaluated
in a zero-shot transfer protocol. Our code and models will be released at
https://github.com/SysCV/SAM-HQ.
</p></li>
</ul>

<h3>Title: Towards Source-free Domain Adaptive Semantic Segmentation via Importance-aware and Prototype-contrast Learning. (arXiv:2306.01598v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01598">http://arxiv.org/abs/2306.01598</a></li>
<li>Code URL: <a href="https://github.com/yihong-97/source-free_iapc">https://github.com/yihong-97/source-free_iapc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01598] Towards Source-free Domain Adaptive Semantic Segmentation via Importance-aware and Prototype-contrast Learning](http://arxiv.org/abs/2306.01598) #segmentation</code></li>
<li>Summary: <p>Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. Code will be made publicly available at
https://github.com/yihong-97/Source-free_IAPC.
</p></li>
</ul>

<h3>Title: Towards In-context Scene Understanding. (arXiv:2306.01667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01667">http://arxiv.org/abs/2306.01667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01667] Towards In-context Scene Understanding](http://arxiv.org/abs/2306.01667) #segmentation</code></li>
<li>Summary: <p>In-context learning$\unicode{x2013}$the ability to configure a model's
behavior with different prompts$\unicode{x2013}$has revolutionized the field of
natural language processing, alleviating the need for task-specific models and
paving the way for generalist models capable of assisting with any query.
Computer vision, in contrast, has largely stayed in the former regime:
specialized decoders and finetuning protocols are generally required to perform
dense tasks such as semantic segmentation and depth estimation. In this work we
explore a simple mechanism for in-context learning of such scene understanding
tasks: nearest neighbor retrieval from a prompt of annotated features. We
propose a new pretraining protocol$\unicode{x2013}$leveraging attention within
and across images$\unicode{x2013}$which yields representations particularly
useful in this regime. The resulting Hummingbird model, suitably prompted,
performs various scene understanding tasks without modification while
approaching the performance of specialists that have been finetuned for each
task. Moreover, Hummingbird can be configured to perform new tasks much more
efficiently than finetuned models, raising the possibility of scene
understanding in the interactive assistant regime.
</p></li>
</ul>

<h3>Title: Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23. (arXiv:2306.01327v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.01327">http://arxiv.org/abs/2306.01327</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.01327] Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23](http://arxiv.org/abs/2306.01327) #segmentation</code></li>
<li>Summary: <p>This paper describes the submission of the UPC Machine Translation group to
the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems
utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We
incorporate a Siamese pretraining step of the speech and text encoders with CTC
and Optimal Transport, to adapt the speech representations to the space of the
text model, thus maximizing transfer learning from MT. After this pretraining,
we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge
Distillation. Apart from the available ST corpora, we create synthetic data
with SegAugment to better adapt our models to the custom segmentations of the
IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C
tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released
IWSLT.ACLdev2023.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
