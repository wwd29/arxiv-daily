<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01292">http://arxiv.org/abs/2307.01292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01292] Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems](http://arxiv.org/abs/2307.01292) #secure</code></li>
<li>Summary: <p>With the emergence of large foundational models, model-serving systems are
becoming popular. In such a system, users send the queries to the server and
specify the desired performance metrics (e.g., accuracy, latency, etc.). The
server maintains a set of models (model zoo) in the back-end and serves the
queries based on the specified metrics. This paper examines the security,
specifically robustness against model extraction attacks, of such systems.
Existing black-box attacks cannot be directly applied to extract a victim
model, as models hide among the model zoo behind the inference serving
interface, and attackers cannot identify which model is being used. An
intermediate step is required to ensure that every input query gets the output
from the victim model. To this end, we propose a query-efficient fingerprinting
algorithm to enable the attacker to trigger any desired model consistently. We
show that by using our fingerprinting algorithm, model extraction can have
fidelity and accuracy scores within $1\%$ of the scores obtained if attacking
in a single-model setting and up to $14.6\%$ gain in accuracy and up to $7.7\%$
gain in fidelity compared to the naive attack. Finally, we counter the proposed
attack with a noise-based defense mechanism that thwarts fingerprinting by
adding noise to the specified performance metrics. Our defense strategy reduces
the attack's accuracy and fidelity by up to $9.8\%$ and $4.8\%$, respectively
(on medium-sized model extraction). We show that the proposed defense induces a
fundamental trade-off between the level of protection and system goodput,
achieving configurable and significant victim model extraction protection while
maintaining acceptable goodput ($>80\%$). We provide anonymous access to our
code.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana. (arXiv:2307.01767v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01767">http://arxiv.org/abs/2307.01767</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01767] Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana](http://arxiv.org/abs/2307.01767) #security</code></li>
<li>Summary: <p>The Ghana Cashew Disease Identification with Artificial Intelligence (CADI
AI) project demonstrates the importance of sound data work as a precondition
for the delivery of useful, localized datacentric solutions for public good
tasks such as agricultural productivity and food security. Drone collected data
and machine learning are utilized to determine crop stressors. Data, model and
the final app are developed jointly and made available to local farmers via a
desktop application.
</p></li>
</ul>

<h3>Title: Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives. (arXiv:2307.01390v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01390">http://arxiv.org/abs/2307.01390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01390] Adversarial Learning in Real-World Fraud Detection: Challenges and Perspectives](http://arxiv.org/abs/2307.01390) #security</code></li>
<li>Summary: <p>Data economy relies on data-driven systems and complex machine learning
applications are fueled by them. Unfortunately, however, machine learning
models are exposed to fraudulent activities and adversarial attacks, which
threaten their security and trustworthiness. In the last decade or so, the
research interest on adversarial machine learning has grown significantly,
revealing how learning applications could be severely impacted by effective
attacks. Although early results of adversarial machine learning indicate the
huge potential of the approach to specific domains such as image processing,
still there is a gap in both the research literature and practice regarding how
to generalize adversarial techniques in other domains and applications. Fraud
detection is a critical defense mechanism for data economy, as it is for other
applications as well, which poses several challenges for machine learning. In
this work, we describe how attacks against fraud detection systems differ from
other applications of adversarial machine learning, and propose a number of
interesting directions to bridge this gap.
</p></li>
</ul>

<h3>Title: The Path to Fault- and Intrusion-Resilient Manycore Systems on a Chip. (arXiv:2307.01783v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01783">http://arxiv.org/abs/2307.01783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01783] The Path to Fault- and Intrusion-Resilient Manycore Systems on a Chip](http://arxiv.org/abs/2307.01783) #security</code></li>
<li>Summary: <p>The hardware computing landscape is changing. What used to be distributed
systems can now be found on a chip with highly configurable, diverse,
specialized and general purpose units. Such Systems-on-a-Chip (SoC) are used to
control today's cyber-physical systems, being the building blocks of critical
infrastructures. They are deployed in harsh environments and are connected to
the cyberspace, which makes them exposed to both accidental faults and targeted
cyberattacks. This is in addition to the changing fault landscape that
continued technology scaling, emerging devices and novel application scenarios
will bring. In this paper, we discuss how the very features, distributed,
parallelized, reconfigurable, heterogeneous, that cause many of the imminent
and emerging security and resilience challenges, also open avenues for their
cure though SoC replication, diversity, rejuvenation, adaptation, and
hybridization. We show how to leverage these techniques at different levels
across the entire SoC hardware/software stack, calling for more research on the
topic.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: \textsc{SeePrivacy}: Automated Contextual Privacy Policy Generation for Mobile Applications. (arXiv:2307.01691v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01691">http://arxiv.org/abs/2307.01691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01691] \textsc{SeePrivacy}: Automated Contextual Privacy Policy Generation for Mobile Applications](http://arxiv.org/abs/2307.01691) #privacy</code></li>
<li>Summary: <p>Privacy policies have become the most critical approach to safeguarding
individuals' privacy and digital security. To enhance their presentation and
readability, researchers propose the concept of contextual privacy policies
(CPPs), aiming to fragment policies into shorter snippets and display them only
in corresponding contexts. In this paper, we propose a novel multi-modal
framework, namely SeePrivacy, designed to automatically generate contextual
privacy policies for mobile apps. Our method synergistically combines mobile
GUI understanding and privacy policy document analysis, yielding an impressive
overall 83.6% coverage rate for privacy-related context detection and an
accuracy of 0.92 in extracting corresponding policy segments. Remarkably, 96%
of the retrieved policy segments can be correctly matched with their contexts.
The user study shows SeePrivacy demonstrates excellent functionality and
usability (4.5/5). Specifically, participants exhibit a greater willingness to
read CPPs (4.1/5) compared to original privacy policies (2/5). Our solution
effectively assists users in comprehending privacy notices, and this research
establishes a solid foundation for further advancements and exploration.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Digital Sovereignty Strategies for Every Nation. (arXiv:2307.01791v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01791">http://arxiv.org/abs/2307.01791</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01791] Digital Sovereignty Strategies for Every Nation](http://arxiv.org/abs/2307.01791) #protect</code></li>
<li>Summary: <p>Digital Sovereignty must be on the agenda of every modern nation. Digital
technology is becoming part of our life details, from the vital essentials,
like food and water management, to transcendence in the Metaverse and Space.
Protecting these digital assets will, therefore, be inevitable for a modern
country to live, excel and lead. Digital Sovereignty is a strategic necessity
to protect these digital assets from the monopoly of friendly rational states,
and the threats of unfriendly Malicious states and behaviors. In this work, we
revisit the definition and scope of digital sovereignty through extending it to
cover the entire value chain of using, owning, and producing digital assets. We
emphasize the importance of protecting the operational resources, both raw
materials and human expertise, in addition to research and innovation necessary
to achieve sustainable sovereignty. We also show that digital sovereignty by
autonomy is often impossible, and by mutual cooperation is not always
sustainable. To this end, we propose implementing digital sovereignty using
Nash Equilibrium, often studied in Game Theory, to govern the relation with
Rational states. Finally, we propose a digital sovereignty agenda for different
country's digital profiles, based on their status quo, priorities, and
capabilities. We survey state-of-the-art digital technology that is useful to
make the current digital assets sovereign. Additionally, we propose a roadmap
that aims to develop a sovereign digital nation, as close as possible to
autonomy. Finally, we draw attention to the need of more research to better
understand and implement digital sovereignty from different perspectives:
technological, economic, and geopolitical.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack. (arXiv:2307.01520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01520">http://arxiv.org/abs/2307.01520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01520] LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack](http://arxiv.org/abs/2307.01520) #attack</code></li>
<li>Summary: <p>Deepfakes, malicious visual contents created by generative models, pose an
increasingly harmful threat to society. To proactively mitigate deepfake
damages, recent studies have employed adversarial perturbation to disrupt
deepfake model outputs. However, previous approaches primarily focus on
generating distorted outputs based on only predetermined target attributes,
leading to a lack of robustness in real-world scenarios where target attributes
are unknown. Additionally, the transferability of perturbations between two
prominent generative models, Generative Adversarial Networks (GANs) and
Diffusion Models, remains unexplored. In this paper, we emphasize the
importance of target attribute-transferability and model-transferability for
achieving robust deepfake disruption. To address this challenge, we propose a
simple yet effective disruption method called Latent Ensemble ATtack (LEAT),
which attacks the independent latent encoding process. By disrupting the latent
encoding process, it generates distorted output images in subsequent generation
processes, regardless of the given target attributes. This target
attribute-agnostic attack ensures robust disruption even when the target
attributes are unknown. Additionally, we introduce a Normalized Gradient
Ensemble strategy that effectively aggregates gradients for iterative gradient
attacks, enabling simultaneous attacks on various types of deepfake models,
involving both GAN-based and Diffusion-based models. Moreover, we demonstrate
the insufficiency of evaluating disruption quality solely based on pixel-level
differences. As a result, we propose an alternative protocol for
comprehensively evaluating the success of defense. Extensive experiments
confirm the efficacy of our method in disrupting deepfakes in real-world
scenarios, reporting a higher defense success rate compared to previous
methods.
</p></li>
</ul>

<h3>Title: Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01778">http://arxiv.org/abs/2307.01778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01778] Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling](http://arxiv.org/abs/2307.01778) #attack</code></li>
<li>Summary: <p>Recent works have proposed to craft adversarial clothes for evading person
detectors, while they are either only effective at limited viewing angles or
very conspicuous to humans. We aim to craft adversarial texture for clothes
based on 3D modeling, an idea that has been used to craft rigid adversarial
objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes
are non-rigid, leading to difficulties in physical realization. In order to
craft natural-looking adversarial clothes that can evade person detectors at
multiple viewing angles, we propose adversarial camouflage textures (AdvCaT)
that resemble one kind of the typical textures of daily clothes, camouflage
textures. We leverage the Voronoi diagram and Gumbel-softmax trick to
parameterize the camouflage textures and optimize the parameters via 3D
modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes
combining topologically plausible projection (TopoProj) and Thin Plate Spline
(TPS) to narrow the gap between digital and real-world objects. We printed the
developed 3D texture pieces on fabric materials and tailored them into T-shirts
and trousers. Experiments show high attack success rates of these clothes
against multiple detectors.
</p></li>
</ul>

<h3>Title: Deep Features for Contactless Fingerprint Presentation Attack Detection: Can They Be Generalized?. (arXiv:2307.01845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01845">http://arxiv.org/abs/2307.01845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01845] Deep Features for Contactless Fingerprint Presentation Attack Detection: Can They Be Generalized?](http://arxiv.org/abs/2307.01845) #attack</code></li>
<li>Summary: <p>The rapid evolution of high-end smartphones with advanced high-resolution
cameras has resulted in contactless capture of fingerprint biometrics that are
more reliable and suitable for verification. Similar to other biometric
systems, contactless fingerprint-verification systems are vulnerable to
presentation attacks. In this paper, we present a comparative study on the
generalizability of seven different pre-trained Convolutional Neural Networks
(CNN) and a Vision Transformer (ViT) to reliably detect presentation attacks.
Extensive experiments were carried out on publicly available smartphone-based
presentation attack datasets using four different Presentation Attack
Instruments (PAI). The detection performance of the eighth deep feature
technique was evaluated using the leave-one-out protocol to benchmark the
generalization performance for unseen PAI. The obtained results indicated the
best generalization performance with the ResNet50 CNN.
</p></li>
</ul>

<h3>Title: With Trail to Follow: Measurements of Real-world Non-fungible Token Phishing Attacks on Ethereum. (arXiv:2307.01579v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01579">http://arxiv.org/abs/2307.01579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01579] With Trail to Follow: Measurements of Real-world Non-fungible Token Phishing Attacks on Ethereum](http://arxiv.org/abs/2307.01579) #attack</code></li>
<li>Summary: <p>With the popularity of Non-Fungible Tokens (NFTs), NFTs have become a new
target of phishing attacks, posing a significant threat to the NFT trading
ecosystem. There has been growing anecdotal evidence that new means of NFT
phishing attacks have emerged in Ethereum ecosystem. Most of the existing
research focus on detecting phishing scam accounts for native cryptocurrency on
the blockchain, but there is a lack of research in the area of phishing attacks
of emerging NFTs. Although a few studies have recently started to focus on the
analysis and detection of NFT phishing attacks, NFT phishing attack means are
diverse and little has been done to understand these various types of NFT
phishing attacks. To the best of our knowledge, we are the first to conduct
case retrospective analysis and measurement study of real-world historical NFT
phishing attacks on Ethereum. By manually analyzing the existing scams reported
by Chainabuse, we classify NFT phishing attacks into four patterns. For each
pattern, we further investigate the tricks and working principles of them.
Based on 469 NFT phishing accounts collected up until October 2022 from
multiple channels, we perform a measurement study of on-chain transaction data
crawled from Etherscan to characterizing NFT phishing scams by analyzing the
modus operandi and preferences of NFT phishing scammers, as well as economic
impacts and whereabouts of stolen NFTs. We classify NFT phishing transactions
into one of the four patterns by log parsing and transaction record parsing. We
find these phishing accounts stole 19,514 NFTs for a total profit of 8,858.431
ETH (around 18.57 million dollars). We also observe that scammers remain highly
active in the last two years and favor certain categories and series of NFTs,
accompanied with signs of gang theft.
</p></li>
</ul>

<h3>Title: Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01610">http://arxiv.org/abs/2307.01610</a></li>
<li>Code URL: <a href="https://github.com/dependablesystemslab/mia_defense_hamp">https://github.com/dependablesystemslab/mia_defense_hamp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01610] Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction](http://arxiv.org/abs/2307.01610) #attack</code></li>
<li>Summary: <p>Machine learning (ML) models are vulnerable to membership inference attacks
(MIAs), which determine whether a given input is used for training the target
model. While there have been many efforts to mitigate MIAs, they often suffer
from limited privacy protection, large accuracy drop, and/or requiring
additional data that may be difficult to acquire. This work proposes a defense
technique, HAMP that can achieve both strong membership privacy and high
accuracy, without requiring extra data. To mitigate MIAs in different forms, we
observe that they can be unified as they all exploit the ML model's
overconfidence in predicting training samples through different proxies. This
motivates our design to enforce less confident prediction by the model, hence
forcing the model to behave similarly on the training and testing samples. HAMP
consists of a novel training framework with high-entropy soft labels and an
entropy-based regularizer to constrain the model's prediction while still
achieving high accuracy. To further reduce privacy risk, HAMP uniformly
modifies all the prediction outputs to become low-confidence outputs while
preserving the accuracy, which effectively obscures the differences between the
prediction on members and non-members. We conduct extensive evaluation on five
benchmark datasets, and show that HAMP provides consistently high accuracy and
strong membership privacy. Our comparison with seven state-of-the-art defenses
shows that HAMP achieves a superior privacy-utility trade off than those
techniques.
</p></li>
</ul>

<h3>Title: Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data. (arXiv:2307.01701v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01701">http://arxiv.org/abs/2307.01701</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01701] Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data](http://arxiv.org/abs/2307.01701) #attack</code></li>
<li>Summary: <p>Synthetic data is emerging as the most promising solution to share
individual-level data while safeguarding privacy. Membership inference attacks
(MIAs), based on shadow modeling, have become the standard to evaluate the
privacy of synthetic data. These attacks, however, currently assume the
attacker to have access to an auxiliary dataset sampled from a similar
distribution as the training dataset. This often is a very strong assumption
that would make an attack unlikely to happen in practice. We here show how this
assumption can be removed and how MIAs can be performed using only the
synthetic data. More specifically, in three different attack scenarios using
only synthetic data, our results demonstrate that MIAs are still successful,
across two real-world datasets and two synthetic data generators. These results
show how the strong hypothesis made when auditing synthetic data releases -
access to an auxiliary dataset - can be relaxed to perform an actual attack.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Direct Superpoints Matching for Fast and Robust Point Cloud Registration. (arXiv:2307.01362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01362">http://arxiv.org/abs/2307.01362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01362] Direct Superpoints Matching for Fast and Robust Point Cloud Registration](http://arxiv.org/abs/2307.01362) #robust</code></li>
<li>Summary: <p>Although deep neural networks endow the downsampled superpoints with
discriminative feature representations, directly matching them is usually not
used alone in state-of-the-art methods, mainly for two reasons. First, the
correspondences are inevitably noisy, so RANSAC-like refinement is usually
adopted. Such ad hoc postprocessing, however, is slow and not differentiable,
which can not be jointly optimized with feature learning. Second, superpoints
are sparse and thus more RANSAC iterations are needed. Existing approaches use
the coarse-to-fine strategy to propagate the superpoints correspondences to the
point level, which are not discriminative enough and further necessitates the
postprocessing refinement. In this paper, we present a simple yet effective
approach to extract correspondences by directly matching superpoints using a
global softmax layer in an end-to-end manner, which are used to determine the
rigid transformation between the source and target point cloud. Compared with
methods that directly predict corresponding points, by leveraging the rich
information from the superpoints matchings, we can obtain more accurate
estimation of the transformation and effectively filter out outliers without
any postprocessing refinement. As a result, our approach is not only fast, but
also achieves state-of-the-art results on the challenging ModelNet and 3DMatch
benchmarks. Our code and model weights will be publicly released.
</p></li>
</ul>

<h3>Title: Unsupervised Feature Learning with Emergent Data-Driven Prototypicality. (arXiv:2307.01421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01421">http://arxiv.org/abs/2307.01421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01421] Unsupervised Feature Learning with Emergent Data-Driven Prototypicality](http://arxiv.org/abs/2307.01421) #robust</code></li>
<li>Summary: <p>Given an image set without any labels, our goal is to train a model that maps
each image to a point in a feature space such that, not only proximity
indicates visual similarity, but where it is located directly encodes how
prototypical the image is according to the dataset.
</p></li>
</ul>

<p>Our key insight is to perform unsupervised feature learning in hyperbolic
instead of Euclidean space, where the distance between points still reflect
image similarity, and yet we gain additional capacity for representing
prototypicality with the location of the point: The closer it is to the origin,
the more prototypical it is. The latter property is simply emergent from
optimizing the usual metric learning objective: The image similar to many
training instances is best placed at the center of corresponding points in
Euclidean space, but closer to the origin in hyperbolic space.
</p>
<p>We propose an unsupervised feature learning algorithm in Hyperbolic space
with sphere pACKing. HACK first generates uniformly packed particles in the
Poincar\'e ball of hyperbolic space and then assigns each image uniquely to
each particle. Images after congealing are regarded more typical of the dataset
it belongs to. With our feature mapper simply trained to spread out training
instances in hyperbolic space, we observe that images move closer to the origin
with congealing, validating our idea of unsupervised prototypicality discovery.
We demonstrate that our data-driven prototypicality provides an easy and
superior unsupervised instance selection to reduce sample complexity, increase
model generalization with atypical instances and robustness with typical ones.
</p>

<h3>Title: Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01473">http://arxiv.org/abs/2307.01473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01473] Mitigating Bias: Enhancing Image Classification by Improving Model Explanations](http://arxiv.org/abs/2307.01473) #robust</code></li>
<li>Summary: <p>Deep learning models have demonstrated remarkable capabilities in learning
complex patterns and concepts from training data. However, recent findings
indicate that these models tend to rely heavily on simple and easily
discernible features present in the background of images rather than the main
concepts or objects they are intended to classify. This phenomenon poses a
challenge to image classifiers as the crucial elements of interest in images
may be overshadowed. In this paper, we propose a novel approach to address this
issue and improve the learning of main concepts by image classifiers. Our
central idea revolves around concurrently guiding the model's attention toward
the foreground during the classification task. By emphasizing the foreground,
which encapsulates the primary objects of interest, we aim to shift the focus
of the model away from the dominant influence of the background. To accomplish
this, we introduce a mechanism that encourages the model to allocate sufficient
attention to the foreground. We investigate various strategies, including
modifying the loss function or incorporating additional architectural
components, to enable the classifier to effectively capture the primary concept
within an image. Additionally, we explore the impact of different foreground
attention mechanisms on model performance and provide insights into their
effectiveness. Through extensive experimentation on benchmark datasets, we
demonstrate the efficacy of our proposed approach in improving the
classification accuracy of image classifiers. Our findings highlight the
importance of foreground attention in enhancing model understanding and
representation of the main concepts within images. The results of this study
contribute to advancing the field of image classification and provide valuable
insights for developing more robust and accurate deep-learning models.
</p></li>
</ul>

<h3>Title: LPN: Language-guided Prototypical Network for few-shot classification. (arXiv:2307.01515v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01515">http://arxiv.org/abs/2307.01515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01515] LPN: Language-guided Prototypical Network for few-shot classification](http://arxiv.org/abs/2307.01515) #robust</code></li>
<li>Summary: <p>Few-shot classification aims to adapt to new tasks with limited labeled
examples. To fully use the accessible data, recent methods explore suitable
measures for the similarity between the query and support images and better
high-dimensional features with meta-training and pre-training strategies.
However, the potential of multi-modality information has barely been explored,
which may bring promising improvement for few-shot classification. In this
paper, we propose a Language-guided Prototypical Network (LPN) for few-shot
classification, which leverages the complementarity of vision and language
modalities via two parallel branches. Concretely, to introduce language
modality with limited samples in the visual task, we leverage a pre-trained
text encoder to extract class-level text features directly from class names
while processing images with a conventional image encoder. Then, a
language-guided decoder is introduced to obtain text features corresponding to
each image by aligning class-level features with visual features. In addition,
to take advantage of class-level features and prototypes, we build a refined
prototypical head that generates robust prototypes in the text branch for
follow-up measurement. Finally, we aggregate the visual and text logits to
calibrate the deviation of a single modality. Extensive experiments demonstrate
the competitiveness of LPN against state-of-the-art methods on benchmark
datasets.
</p></li>
</ul>

<h3>Title: DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification. (arXiv:2307.01806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01806">http://arxiv.org/abs/2307.01806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01806] DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification](http://arxiv.org/abs/2307.01806) #robust</code></li>
<li>Summary: <p>In this paper, we propose a novel learning paradigm called "DeepFlorist" for
flower classification using ensemble learning as a meta-classifier. DeepFlorist
combines the power of deep learning with the robustness of ensemble methods to
achieve accurate and reliable flower classification results. The proposed
network architecture leverages a combination of dense convolutional and
convolutional neural networks (DCNNs and CNNs) to extract high-level features
from flower images, followed by a fully connected layer for classification. To
enhance the performance and generalization of DeepFlorist, an ensemble learning
approach is employed, incorporating multiple diverse models to improve the
classification accuracy. Experimental results on benchmark flower datasets
demonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art
methods in terms of accuracy and robustness. The proposed framework holds
significant potential for automated flower recognition systems in real-world
applications, enabling advancements in plant taxonomy, conservation efforts,
and ecological studies.
</p></li>
</ul>

<h3>Title: Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01214">http://arxiv.org/abs/2307.01214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01214] Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search](http://arxiv.org/abs/2307.01214) #robust</code></li>
<li>Summary: <p>Despite large-scale pre-trained language models have achieved striking
results for text classificaion, recent work has raised concerns about the
challenge of shortcut learning. In general, a keyword is regarded as a shortcut
if it creates a superficial association with the label, resulting in a false
prediction. Conversely, shortcut learning can be mitigated if the model relies
on robust causal features that help produce sound predictions. To this end,
many studies have explored post-hoc interpretable methods to mine shortcuts and
causal features for robustness and generalization. However, most existing
methods focus only on single word in a sentence and lack consideration of
word-group, leading to wrong causal features. To solve this problem, we propose
a new Word-Group mining approach, which captures the causal effect of any
keyword combination and orders the combinations that most affect the
prediction. Our approach bases on effective post-hoc analysis and beam search,
which ensures the mining effect and reduces the complexity. Then, we build a
counterfactual augmentation method based on the multiple word-groups, and use
an adaptive voting mechanism to learn the influence of different augmentated
samples on the prediction results, so as to force the model to pay attention to
effective causal features. We demonstrate the effectiveness of the proposed
method by several tasks on 8 affective review datasets and 4 toxic language
datasets, including cross-domain text classificaion, text attack and gender
fairness test.
</p></li>
</ul>

<h3>Title: CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01458">http://arxiv.org/abs/2307.01458</a></li>
<li>Code URL: <a href="https://github.com/meetyou-ai-lab/care-mi">https://github.com/meetyou-ai-lab/care-mi</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01458] CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care](http://arxiv.org/abs/2307.01458) #robust</code></li>
<li>Summary: <p>The recent advances in NLP, have led to a new trend of applying LLMs to
real-world scenarios. While the latest LLMs are astonishingly fluent when
interacting with humans, they suffer from the misinformation problem by
unintentionally generating factually false statements. This can lead to harmful
consequences, especially when produced within sensitive contexts, such as
healthcare. Yet few previous works have focused on evaluating misinformation in
the long-form generation of LLMs, especially for knowledge-intensive topics.
Moreover, although LLMs have been shown to perform well in different languages,
misinformation evaluation has been mostly conducted in English. To this end, we
present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a
sensitive topic, specifically the maternity and infant care domain; and 2) a
language other than English, namely Chinese. Most importantly, we provide an
innovative paradigm for building long-form generation evaluation benchmarks
that can be transferred to other knowledge-intensive domains and low-resourced
languages. Our proposed benchmark fills the gap between the extensive usage of
LLMs and the lack of datasets for assessing the misinformation generated by
these models. It contains 1,612 expert-checked questions, accompanied with
human-selected references. Using our benchmark, we conduct extensive
experiments and found that current Chinese LLMs are far from perfect in the
topic of maternity and infant care. In an effort to minimize the reliance on
human resources for performance evaluation, we offer a judgment model for
automatically assessing the long-form output of LLMs using the benchmark
questions. Moreover, we compare potential solutions for long-form generation
evaluation and provide insights for building more robust and efficient
automated metric.
</p></li>
</ul>

<h3>Title: SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification. (arXiv:2307.01488v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01488">http://arxiv.org/abs/2307.01488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01488] SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification](http://arxiv.org/abs/2307.01488) #robust</code></li>
<li>Summary: <p>Despite their promising performance across various natural language
processing (NLP) tasks, current NLP systems are vulnerable to textual
adversarial attacks. To defend against these attacks, most existing methods
apply adversarial training by incorporating adversarial examples. However,
these methods have to rely on ground-truth labels to generate adversarial
examples, rendering it impractical for large-scale model pre-training which is
commonly used nowadays for NLP and many other tasks. In this paper, we propose
a novel learning framework called SCAT (Self-supervised Contrastive Learning
via Adversarial Training), which can learn robust representations without
requiring labeled data. Specifically, SCAT modifies random augmentations of the
data in a fully labelfree manner to generate adversarial examples. Adversarial
training is achieved by minimizing the contrastive loss between the
augmentations and their adversarial counterparts. We evaluate SCAT on two text
classification datasets using two state-of-the-art attack schemes proposed
recently. Our results show that SCAT can not only train robust language models
from scratch, but it can also significantly improve the robustness of existing
pre-trained language models. Moreover, to demonstrate its flexibility, we show
that SCAT can also be combined with supervised adversarial training to further
enhance model robustness.
</p></li>
</ul>

<h3>Title: Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation. (arXiv:2307.01680v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01680">http://arxiv.org/abs/2307.01680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01680] Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation](http://arxiv.org/abs/2307.01680) #robust</code></li>
<li>Summary: <p>The automatic detection of hate speech online is an active research area in
NLP. Most of the studies to date are based on social media datasets that
contribute to the creation of hate speech detection models trained on them.
However, data creation processes contain their own biases, and models
inherently learn from these dataset-specific biases. In this paper, we perform
a large-scale cross-dataset comparison where we fine-tune language models on
different hate speech detection datasets. This analysis shows how some datasets
are more generalisable than others when used as training data. Crucially, our
experiments show how combining hate speech detection datasets can contribute to
the development of robust hate speech detection models. This robustness holds
even when controlling by data size and compared with the best individual
datasets.
</p></li>
</ul>

<h3>Title: Review of Deep Learning-based Malware Detection for Android and Windows System. (arXiv:2307.01494v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01494">http://arxiv.org/abs/2307.01494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01494] Review of Deep Learning-based Malware Detection for Android and Windows System](http://arxiv.org/abs/2307.01494) #robust</code></li>
<li>Summary: <p>Differentiating malware is important to determine their behaviors and level
of threat; as well as to devise defensive strategy against them. In response,
various anti-malware systems have been developed to distinguish between
different malwares. However, most of the recent malware families are Artificial
Intelligence (AI) enable and can deceive traditional anti-malware systems using
different obfuscation techniques. Therefore, only AI-enabled anti-malware
system is robust against these techniques and can detect different features in
the malware files that aid in malicious activities. In this study we review two
AI-enabled techniques for detecting malware in Windows and Android operating
system, respectively. Both the techniques achieved perfect accuracy in
detecting various malware families.
</p></li>
</ul>

<h3>Title: Robust Uncertainty Estimation for Classification of Maritime Objects. (arXiv:2307.01325v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01325">http://arxiv.org/abs/2307.01325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01325] Robust Uncertainty Estimation for Classification of Maritime Objects](http://arxiv.org/abs/2307.01325) #robust</code></li>
<li>Summary: <p>We explore the use of uncertainty estimation in the maritime domain, showing
the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset,
SHIPS. We present a method joining the intra-class uncertainty achieved using
Monte Carlo Dropout, with recent discoveries in the field of outlier detection,
to gain more holistic uncertainty measures. We explore the relationship between
the introduced uncertainty measures and examine how well they work on CIFAR10
and in a real-life setting. Our work improves the FPR95 by 8% compared to the
current highest-performing work when the models are trained without
out-of-distribution data. We increase the performance by 77% compared to a
vanilla implementation of the Wide ResNet. We release the SHIPS dataset and
show the effectiveness of our method by improving the FPR95 by 44.2% with
respect to the baseline. Our approach is model agnostic, easy to implement, and
often does not require model retraining.
</p></li>
</ul>

<h3>Title: Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer's Disease Progression via Counterfactual Inference. (arXiv:2307.01389v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01389">http://arxiv.org/abs/2307.01389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01389] Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer's Disease Progression via Counterfactual Inference](http://arxiv.org/abs/2307.01389) #robust</code></li>
<li>Summary: <p>Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning
with amyloidosis, followed by neuronal loss and deterioration in structure,
function, and cognition. The accumulation of amyloid-beta in the brain,
measured through 18F-florbetapir (AV45) positron emission tomography (PET)
imaging, has been widely used for early diagnosis of AD. However, the
relationship between amyloid-beta accumulation and AD pathophysiology remains
unclear, and causal inference approaches are needed to uncover how amyloid-beta
levels can impact AD development. In this paper, we propose a graph varying
coefficient neural network (GVCNet) for estimating the individual treatment
effect with continuous treatment levels using a graph convolutional neural
network. We highlight the potential of causal inference approaches, including
GVCNet, for measuring the regional causal connections between amyloid-beta
accumulation and AD pathophysiology, which may serve as a robust tool for early
diagnosis and tailored care.
</p></li>
</ul>

<h3>Title: Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network. (arXiv:2307.01622v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01622">http://arxiv.org/abs/2307.01622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01622] Renewable energy management in smart home environment via forecast embedded scheduling based on Recurrent Trend Predictive Neural Network](http://arxiv.org/abs/2307.01622) #robust</code></li>
<li>Summary: <p>Smart home energy management systems help the distribution grid operate more
efficiently and reliably, and enable effective penetration of distributed
renewable energy sources. These systems rely on robust forecasting,
optimization, and control/scheduling algorithms that can handle the uncertain
nature of demand and renewable generation. This paper proposes an advanced ML
algorithm, called Recurrent Trend Predictive Neural Network based Forecast
Embedded Scheduling (rTPNN-FES), to provide efficient residential demand
control. rTPNN-FES is a novel neural network architecture that simultaneously
forecasts renewable energy generation and schedules household appliances. By
its embedded structure, rTPNN-FES eliminates the utilization of separate
algorithms for forecasting and scheduling and generates a schedule that is
robust against forecasting errors. This paper also evaluates the performance of
the proposed algorithm for an IoT-enabled smart home. The evaluation results
reveal that rTPNN-FES provides near-optimal scheduling $37.5$ times faster than
the optimization while outperforming state-of-the-art forecasting techniques.
</p></li>
</ul>

<h3>Title: RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network. (arXiv:2307.01725v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01725">http://arxiv.org/abs/2307.01725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01725] RRCNN: A novel signal decomposition approach based on recurrent residue convolutional neural network](http://arxiv.org/abs/2307.01725) #robust</code></li>
<li>Summary: <p>The decomposition of non-stationary signals is an important and challenging
task in the field of signal time-frequency analysis. In the recent two decades,
many signal decomposition methods led by the empirical mode decomposition,
which was pioneered by Huang et al. in 1998, have been proposed by different
research groups. However, they still have some limitations. For example, they
are generally prone to boundary and mode mixing effects and are not very robust
to noise. Inspired by the successful applications of deep learning in fields
like image processing and natural language processing, and given the lack in
the literature of works in which deep learning techniques are used directly to
decompose non-stationary signals into simple oscillatory components, we use the
convolutional neural network, residual structure and nonlinear activation
function to compute in an innovative way the local average of the signal, and
study a new non-stationary signal decomposition method under the framework of
deep learning. We discuss the training process of the proposed model and study
the convergence analysis of the learning algorithm. In the experiments, we
evaluate the performance of the proposed model from two points of view: the
calculation of the local average and the signal decomposition. Furthermore, we
study the mode mixing, noise interference, and orthogonality properties of the
decomposed components produced by the proposed method. All results show that
the proposed model allows for better handling boundary effect, mode mixing
effect, robustness, and the orthogonality of the decomposed components than
existing methods.
</p></li>
</ul>

<h3>Title: FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices. (arXiv:2307.01780v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01780">http://arxiv.org/abs/2307.01780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01780] FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices](http://arxiv.org/abs/2307.01780) #robust</code></li>
<li>Summary: <p>Indoor localization plays a vital role in applications such as emergency
response, warehouse management, and augmented reality experiences. By deploying
machine learning (ML) based indoor localization frameworks on their mobile
devices, users can localize themselves in a variety of indoor and subterranean
environments. However, achieving accurate indoor localization can be
challenging due to heterogeneity in the hardware and software stacks of mobile
devices, which can result in inconsistent and inaccurate location estimates.
Traditional ML models also heavily rely on initial training data, making them
vulnerable to degradation in performance with dynamic changes across indoor
environments. To address the challenges due to device heterogeneity and lack of
adaptivity, we propose a novel embedded ML framework called FedHIL. Our
framework combines indoor localization and federated learning (FL) to improve
indoor localization accuracy in device-heterogeneous environments while also
preserving user data privacy. FedHIL integrates a domain-specific selective
weight adjustment approach to preserve the ML model's performance for indoor
localization during FL, even in the presence of extremely noisy data.
Experimental evaluations in diverse real-world indoor environments and with
heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL
and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62x
better localization accuracy on average than the best performing FL-based
indoor localization framework from prior work.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms. (arXiv:2307.01383v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01383">http://arxiv.org/abs/2307.01383</a></li>
<li>Code URL: <a href="https://github.com/yebigithub/BW_dairy">https://github.com/yebigithub/BW_dairy</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01383] Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms](http://arxiv.org/abs/2307.01383) #biometric</code></li>
<li>Summary: <p>Monitoring cow body weight is crucial to support farm management decisions
due to its direct relationship with the growth, nutritional status, and health
of dairy cows. Cow body weight is a repeated trait, however, the majority of
previous body weight prediction research only used data collected at a single
point in time. Furthermore, the utility of deep learning-based segmentation for
body weight prediction using videos remains unanswered. Therefore, the
objectives of this study were to predict cow body weight from repeatedly
measured video data, to compare the performance of the thresholding and Mask
R-CNN deep learning approaches, to evaluate the predictive ability of body
weight regression models, and to promote open science in the animal science
community by releasing the source code for video-based body weight prediction.
A total of 40,405 depth images and depth map files were obtained from 10
lactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were
investigated to segment the cow's body from the background, including single
thresholding, adaptive thresholding, and Mask R-CNN. Four image-derived
biometric features, such as dorsal length, abdominal width, height, and volume,
were estimated from the segmented images. On average, the Mask-RCNN approach
combined with a linear mixed model resulted in the best prediction coefficient
of determination and mean absolute percentage error of 0.98 and 2.03%,
respectively, in the forecasting cross-validation. The Mask-RCNN approach was
also the best in the leave-three-cows-out cross-validation. The prediction
coefficients of determination and mean absolute percentage error of the
Mask-RCNN coupled with the linear mixed model were 0.90 and 4.70%,
respectively. Our results suggest that deep learning-based segmentation
improves the prediction performance of cow body weight from longitudinal depth
video data.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Advancing Wound Filling Extraction on 3D Faces: A Auto-Segmentation and Wound Face Regeneration Approach. (arXiv:2307.01844v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01844">http://arxiv.org/abs/2307.01844</a></li>
<li>Code URL: <a href="https://github.com/simogroup/woundfilling3d">https://github.com/simogroup/woundfilling3d</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01844] Advancing Wound Filling Extraction on 3D Faces: A Auto-Segmentation and Wound Face Regeneration Approach](http://arxiv.org/abs/2307.01844) #extraction</code></li>
<li>Summary: <p>Facial wound segmentation plays a crucial role in preoperative planning and
optimizing patient outcomes in various medical applications. In this paper, we
propose an efficient approach for automating 3D facial wound segmentation using
a two-stream graph convolutional network. Our method leverages the Cir3D-FaIR
dataset and addresses the challenge of data imbalance through extensive
experimentation with different loss functions. To achieve accurate
segmentation, we conducted thorough experiments and selected a high-performing
model from the trained models. The selected model demonstrates exceptional
segmentation performance for complex 3D facial wounds. Furthermore, based on
the segmentation model, we propose an improved approach for extracting 3D
facial wound fillers and compare it to the results of the previous study. Our
method achieved a remarkable accuracy of 0.9999986\% on the test suite,
surpassing the performance of the previous method. From this result, we use 3D
printing technology to illustrate the shape of the wound filling. The outcomes
of this study have significant implications for physicians involved in
preoperative planning and intervention design. By automating facial wound
segmentation and improving the accuracy of wound-filling extraction, our
approach can assist in carefully assessing and optimizing interventions,
leading to enhanced patient outcomes. Additionally, it contributes to advancing
facial reconstruction techniques by utilizing machine learning and 3D
bioprinting for printing skin tissue implants. Our source code is available at
\url{https://github.com/SIMOGroup/WoundFilling3D}.
</p></li>
</ul>

<h3>Title: Discovering Patterns of Definitions and Methods from Scientific Documents. (arXiv:2307.01216v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01216">http://arxiv.org/abs/2307.01216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01216] Discovering Patterns of Definitions and Methods from Scientific Documents](http://arxiv.org/abs/2307.01216) #extraction</code></li>
<li>Summary: <p>The difficulties of automatic extraction of definitions and methods from
scientific documents lie in two aspects: (1) the complexity and diversity of
natural language texts, which requests an analysis method to support the
discovery of pattern; and, (2) a complete definition or method represented by a
scientific paper is usually distributed within text, therefore an effective
approach should not only extract single sentence definitions and methods but
also integrate the sentences to obtain a complete definition or method. This
paper proposes an analysis method for discovering patterns of definition and
method and uses the method to discover patterns of definition and method.
Completeness of the patterns at the semantic level is guaranteed by a complete
set of semantic relations that identify definitions and methods respectively.
The completeness of the patterns at the syntactic and lexical levels is
guaranteed by syntactic and lexical constraints. Experiments on the self-built
dataset and two public definition datasets show that the discovered patterns
are effective. The patterns can be used to extract definitions and methods from
scientific documents and can be tailored or extended to suit other
applications.
</p></li>
</ul>

<h3>Title: Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01323">http://arxiv.org/abs/2307.01323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01323] Semantic enrichment towards efficient speech representations](http://arxiv.org/abs/2307.01323) #extraction</code></li>
<li>Summary: <p>Over the past few years, self-supervised learned speech representations have
emerged as fruitful replacements for conventional surface representations when
solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual
models trained on massive textual data were introduced to encode language
agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make
profit from such textual models to enrich multilingual speech representations
with language agnostic semantics. By aiming for better semantic extraction on a
challenging Spoken Language Understanding task and in consideration with
computation costs, this study investigates a specific in-domain semantic
enrichment of the SAMU-XLSR model by specializing it on a small amount of
transcribed data from the downstream task. In addition, we show the benefits of
the use of same-domain French and Italian benchmarks for low-resource language
portability and explore cross-domain capacities of the enriched SAMU-XLSR.
</p></li>
</ul>

<h3>Title: Multi-Task Learning Improves Performance In Deep Argument Mining Models. (arXiv:2307.01401v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01401">http://arxiv.org/abs/2307.01401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01401] Multi-Task Learning Improves Performance In Deep Argument Mining Models](http://arxiv.org/abs/2307.01401) #extraction</code></li>
<li>Summary: <p>The successful analysis of argumentative techniques from user-generated text
is central to many downstream tasks such as political and market analysis.
Recent argument mining tools use state-of-the-art deep learning methods to
extract and annotate argumentative techniques from various online text corpora,
however each task is treated as separate and different bespoke models are
fine-tuned for each dataset. We show that different argument mining tasks share
common semantic and logical structure by implementing a multi-task approach to
argument mining that achieves better performance than state-of-the-art methods
for the same problems. Our model builds a shared representation of the input
text that is common to all tasks and exploits similarities between tasks in
order to further boost performance via parameter-sharing. Our results are
important for argument mining as they show that different tasks share
substantial similarities and suggest a holistic approach to the extraction of
argumentative techniques from text.
</p></li>
</ul>

<h3>Title: ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision. (arXiv:2307.01448v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01448">http://arxiv.org/abs/2307.01448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01448] ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision](http://arxiv.org/abs/2307.01448) #extraction</code></li>
<li>Summary: <p>Structured chemical reaction information plays a vital role for chemists
engaged in laboratory work and advanced endeavors such as computer-aided drug
design. Despite the importance of extracting structured reactions from
scientific literature, data annotation for this purpose is cost-prohibitive due
to the significant labor required from domain experts. Consequently, the
scarcity of sufficient training data poses an obstacle to the progress of
related models in this domain. In this paper, we propose ReactIE, which
combines two weakly supervised approaches for pre-training. Our method utilizes
frequent patterns within the text as linguistic cues to identify specific
characteristics of chemical reactions. Additionally, we adopt synthetic data
from patent records as distant supervision to incorporate domain knowledge into
the model. Experiments demonstrate that ReactIE achieves substantial
improvements and outperforms all existing baselines.
</p></li>
</ul>

<h3>Title: Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction. (arXiv:2307.01570v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01570">http://arxiv.org/abs/2307.01570</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01570] Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction](http://arxiv.org/abs/2307.01570) #extraction</code></li>
<li>Summary: <p>Internet of things (IoT) has been playing an important role in many sectors,
such as smart cities, smart agriculture, smart healthcare, and smart
manufacturing. However, IoT devices are highly vulnerable to cyber-attacks,
which may result in security breaches and data leakages. To effectively prevent
these attacks, a variety of machine learning-based network intrusion detection
methods for IoT networks have been developed, which often rely on either
feature extraction or feature selection techniques for reducing the dimension
of input data before being fed into machine learning models. This aims to make
the detection complexity low enough for real-time operations, which is
particularly vital in any intrusion detection systems. This paper provides a
comprehensive comparison between these two feature reduction methods of
intrusion detection in terms of various performance metrics, namely, precision
rate, recall rate, detection accuracy, as well as runtime complexity, in the
presence of the modern UNSW-NB15 dataset as well as both binary and multiclass
classification. For example, in general, the feature selection method not only
provides better detection performance but also lower training and inference
time compared to its feature extraction counterpart, especially when the number
of reduced features K increases. However, the feature extraction method is much
more reliable than its selection counterpart, particularly when K is very
small, such as K = 4. Additionally, feature extraction is less sensitive to
changing the number of reduced features K than feature selection, and this
holds true for both binary and multiclass classifications. Based on this
comparison, we provide a useful guideline for selecting a suitable intrusion
detection type for each specific scenario, as detailed in Tab. 14 at the end of
Section IV.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01514">http://arxiv.org/abs/2307.01514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01514] SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT](http://arxiv.org/abs/2307.01514) #federate</code></li>
<li>Summary: <p>Self-supervised learning in federated learning paradigm has been gaining a
lot of interest both in industry and research due to the collaborative learning
capability on unlabeled yet isolated data. However, self-supervised based
federated learning strategies suffer from performance degradation due to label
scarcity and diverse data distributions, i.e., data heterogeneity. In this
paper, we propose the SelfFed framework for Internet of Medical Things (IoMT).
Our proposed SelfFed framework works in two phases. The first phase is the
pre-training paradigm that performs augmentive modeling using Swin Transformer
based encoder in a decentralized manner. The first phase of SelfFed framework
helps to overcome the data heterogeneity issue. The second phase is the
fine-tuning paradigm that introduces contrastive network and a novel
aggregation strategy that is trained on limited labeled data for a target task
in a decentralized manner. This fine-tuning stage overcomes the label scarcity
problem. We perform our experimental analysis on publicly available medical
imaging datasets and show that our proposed SelfFed framework performs better
when compared to existing baselines concerning non-independent and identically
distributed (IID) data and label scarcity. Our method achieves a maximum
improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID
dataset. Further, our proposed method outperforms existing baselines even when
trained on a few (10%) labeled instances.
</p></li>
</ul>

<h3>Title: FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01217">http://arxiv.org/abs/2307.01217</a></li>
<li>Code URL: <a href="https://github.com/tsingz0/fedcp">https://github.com/tsingz0/fedcp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01217] FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy](http://arxiv.org/abs/2307.01217) #federate</code></li>
<li>Summary: <p>Recently, personalized federated learning (pFL) has attracted increasing
attention in privacy protection, collaborative learning, and tackling
statistical heterogeneity among clients, e.g., hospitals, mobile smartphones,
etc. Most existing pFL methods focus on exploiting the global information and
personalized information in the client-level model parameters while neglecting
that data is the source of these two kinds of information. To address this, we
propose the Federated Conditional Policy (FedCP) method, which generates a
conditional policy for each sample to separate the global information and
personalized information in its features and then processes them by a global
head and a personalized head, respectively. FedCP is more fine-grained to
consider personalization in a sample-specific manner than existing pFL methods.
Extensive experiments in computer vision and natural language processing
domains show that FedCP outperforms eleven state-of-the-art methods by up to
6.69%. Furthermore, FedCP maintains its superiority when some clients
accidentally drop out, which frequently happens in mobile settings. Our code is
public at https://github.com/TsingZ0/FedCP.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection. (arXiv:2307.01426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01426">http://arxiv.org/abs/2307.01426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01426] DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection](http://arxiv.org/abs/2307.01426) #fair</code></li>
<li>Summary: <p>A critical yet frequently overlooked challenge in the field of deepfake
detection is the lack of a standardized, unified, comprehensive benchmark. This
issue leads to unfair performance comparisons and potentially misleading
results. Specifically, there is a lack of uniformity in data processing
pipelines, resulting in inconsistent data inputs for detection models.
Additionally, there are noticeable differences in experimental settings, and
evaluation strategies and metrics lack standardization. To fill this gap, we
present the first comprehensive benchmark for deepfake detection, called
DeepfakeBench, which offers three key contributions: 1) a unified data
management system to ensure consistent input across all detectors, 2) an
integrated framework for state-of-the-art methods implementation, and 3)
standardized evaluation metrics and protocols to promote transparency and
reproducibility. Featuring an extensible, modular-based codebase, DeepfakeBench
contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series
of deepfake detection evaluation protocols and analysis tools, as well as
comprehensive evaluations. Moreover, we provide new insights based on extensive
analysis of these evaluations from various perspectives (e.g., data
augmentations, backbones). We hope that our efforts could facilitate future
research and foster innovation in this increasingly critical domain. All codes,
evaluations, and analyses of our benchmark are publicly available at
https://github.com/SCLBD/DeepfakeBench.
</p></li>
</ul>

<h3>Title: Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01288">http://arxiv.org/abs/2307.01288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01288] Fighting the disagreement in Explainable Machine Learning with consensus](http://arxiv.org/abs/2307.01288) #fair</code></li>
<li>Summary: <p>Machine learning (ML) models are often valued by the accuracy of their
predictions. However, in some areas of science, the inner workings of models
are as relevant as their accuracy. To understand how ML models work internally,
the use of interpretability algorithms is the preferred option. Unfortunately,
despite the diversity of algorithms available, they often disagree in
explaining a model, leading to contradictory explanations. To cope with this
issue, consensus functions can be applied once the models have been explained.
Nevertheless, the problem is not completely solved because the final result
will depend on the selected consensus function and other factors. In this
paper, six consensus functions have been evaluated for the explanation of five
ML models. The models were previously trained on four synthetic datasets whose
internal rules were known in advance. The models were then explained with
model-agnostic local and global interpretability algorithms. Finally, consensus
was calculated with six different functions, including one developed by the
authors. The results demonstrated that the proposed function is fairer than the
others and provides more consistent and accurate explanations.
</p></li>
</ul>

<h3>Title: Shapley Sets: Feature Attribution via Recursive Function Decomposition. (arXiv:2307.01777v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01777">http://arxiv.org/abs/2307.01777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01777] Shapley Sets: Feature Attribution via Recursive Function Decomposition](http://arxiv.org/abs/2307.01777) #fair</code></li>
<li>Summary: <p>Despite their ubiquitous use, Shapley value feature attributions can be
misleading due to feature interaction in both model and data. We propose an
alternative attribution approach, Shapley Sets, which awards value to sets of
features. Shapley Sets decomposes the underlying model into non-separable
variable groups using a recursive function decomposition algorithm with log
linear complexity in the number of variables. Shapley Sets attributes to each
non-separable variable group their combined value for a particular prediction.
We show that Shapley Sets is equivalent to the Shapley value over the
transformed feature set and thus benefits from the same axioms of fairness.
Shapley Sets is value function agnostic and we show theoretically and
experimentally how Shapley Sets avoids pitfalls associated with Shapley value
based alternatives and are particularly advantageous for data types with
complex dependency structure.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01578">http://arxiv.org/abs/2307.01578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01578] Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation](http://arxiv.org/abs/2307.01578) #interpretability</code></li>
<li>Summary: <p>Even though data annotation is extremely important for interpretability,
research and development of artificial intelligence solutions, most research
efforts such as active learning or few-shot learning focus on the sample
efficiency problem. This paper studies the neglected complementary problem of
getting annotated data given a predictor. For the simple binary classification
setting, we present the spectrum ranging from optimal general solutions to
practical efficient methods. The problem is framed as the full annotation of a
binary classification dataset with the minimal number of yes/no questions when
a predictor is available. For the case of general binary questions the solution
is found in coding theory, where the optimal questioning strategy is given by
the Huffman encoding of the possible labelings. However, this approach is
computationally intractable even for small dataset sizes. We propose an
alternative practical solution based on several heuristics and lookahead
minimization of proxy cost functions. The proposed solution is analysed,
compared with optimal solutions and evaluated on several synthetic and
real-world datasets. On these datasets, the method allows a significant
improvement ($23-86\%$) in annotation efficiency.
</p></li>
</ul>

<h3>Title: Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01225">http://arxiv.org/abs/2307.01225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01225] Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)](http://arxiv.org/abs/2307.01225) #interpretability</code></li>
<li>Summary: <p>Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have
shown impressive performance in NLP. However, their vulnerability to
adversarial examples poses a security risk. Existing defense methods lack
interpretability, making it hard to understand adversarial classifications and
identify model vulnerabilities. To address this, we propose the
Interpretability and Transparency-Driven Detection and Transformation (IT-DT)
framework. It focuses on interpretability and transparency in detecting and
transforming textual adversarial examples. IT-DT utilizes techniques like
attention maps, integrated gradients, and model feedback for interpretability
during detection. This helps identify salient features and perturbed words
contributing to adversarial classifications. In the transformation phase, IT-DT
uses pre-trained embeddings and model feedback to generate optimal replacements
for perturbed words. By finding suitable substitutions, we aim to convert
adversarial examples into non-adversarial counterparts that align with the
model's intended behavior while preserving the text's meaning. Transparency is
emphasized through human expert involvement. Experts review and provide
feedback on detection and transformation results, enhancing decision-making,
especially in complex scenarios. The framework generates insights and threat
intelligence empowering analysts to identify vulnerabilities and improve model
robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in
detecting and transforming adversarial examples. The approach enhances
interpretability, provides transparency, and enables accurate identification
and successful transformation of adversarial inputs. By combining technical
analysis and human expertise, IT-DT significantly improves the resilience and
trustworthiness of transformer-based text classifiers against adversarial
attacks.
</p></li>
</ul>

<h3>Title: Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction. (arXiv:2307.01238v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01238">http://arxiv.org/abs/2307.01238</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01238] Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction](http://arxiv.org/abs/2307.01238) #interpretability</code></li>
<li>Summary: <p>People with diabetes must carefully monitor their blood glucose levels,
especially after eating. Blood glucose regulation requires a proper combination
of food intake and insulin boluses. Glucose prediction is vital to avoid
dangerous post-meal complications in treating individuals with diabetes.
Although traditional methods, such as artificial neural networks, have shown
high accuracy rates, sometimes they are not suitable for developing
personalised treatments by physicians due to their lack of interpretability. In
this study, we propose a novel glucose prediction method emphasising
interpretability: Interpretable Sparse Identification by Grammatical Evolution.
Combined with a previous clustering stage, our approach provides finite
difference equations to predict postprandial glucose levels up to two hours
after meals. We divide the dataset into four-hour segments and perform
clustering based on blood glucose values for the twohour window before the
meal. Prediction models are trained for each cluster for the two-hour windows
after meals, allowing predictions in 15-minute steps, yielding up to eight
predictions at different time horizons. Prediction safety was evaluated based
on Parkes Error Grid regions. Our technique produces safe predictions through
explainable expressions, avoiding zones D (0.2% average) and E (0%) and
reducing predictions on zone C (6.2%). In addition, our proposal has slightly
better accuracy than other techniques, including sparse identification of
non-linear dynamics and artificial neural networks. The results demonstrate
that our proposal provides interpretable solutions without sacrificing
prediction accuracy, offering a promising approach to glucose prediction in
diabetes management that balances accuracy, interpretability, and computational
efficiency.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Human Trajectory Forecasting with Explainable Behavioral Uncertainty. (arXiv:2307.01817v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01817">http://arxiv.org/abs/2307.01817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01817] Human Trajectory Forecasting with Explainable Behavioral Uncertainty](http://arxiv.org/abs/2307.01817) #explainability</code></li>
<li>Summary: <p>Human trajectory forecasting helps to understand and predict human behaviors,
enabling applications from social robots to self-driving cars, and therefore
has been heavily investigated. Most existing methods can be divided into
model-free and model-based methods. Model-free methods offer superior
prediction accuracy but lack explainability, while model-based methods provide
explainability but cannot predict well. Combining both methodologies, we
propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM,
where a behavior SDE model is combined with Bayesian neural networks (BNNs).
While the NNs provide superior predictive power, the SDE offers strong
explainability with quantifiable uncertainty in behavior and observation. We
show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy,
compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to
drastically different scenes with different environments and crowd densities (~
20 times higher than the testing data). Finally, BNSP-SFM can provide
predictions with confidence to better explain potential causes of behaviors.
The code will be released upon acceptance.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols. (arXiv:2307.01346v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01346">http://arxiv.org/abs/2307.01346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01346] Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols](http://arxiv.org/abs/2307.01346) #diffusion</code></li>
<li>Summary: <p>We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from
only six-direction diffusion weighted images (DWI). Deep learning-based methods
have been recently proposed for dMRI parameter estimation, using either
voxel-wise fully-connected neural networks (FCN) or image-wise convolutional
neural networks (CNN). In the acute clinical context -- where pressure of time
limits the number of imaged directions to a minimum -- existing approaches
either require an infeasible number of training images volumes (image-wise
CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for
tractogram estimation. To overcome these limitations, we propose Patch-CNN, a
neural network with a minimal (non-voxel-wise) convolutional kernel
(3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of
allowing the network to leverage local anatomical information. Compared with
image-wise CNNs, the minimal kernel vastly reduces training data demand.
Evaluated against both conventional model fitting and a voxel-wise FCN,
Patch-CNN, trained with a single subject is shown to improve the estimation of
both scalar dMRI parameters and fibre orientation from six-direction DWIs. The
improved fibre orientation estimation is shown to produce improved tractogram.
</p></li>
</ul>

<h3>Title: Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations. (arXiv:2307.01533v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01533">http://arxiv.org/abs/2307.01533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01533] Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations](http://arxiv.org/abs/2307.01533) #diffusion</code></li>
<li>Summary: <p>This paper aims to address the unsupervised video anomaly detection (VAD)
problem, which involves classifying each frame in a video as normal or
abnormal, without any access to labels. To accomplish this, the proposed method
employs conditional diffusion models, where the input data is the
spatiotemporal features extracted from a pre-trained network, and the condition
is the features extracted from compact motion representations that summarize a
given video segment in terms of its motion and appearance. Our method utilizes
a data-driven threshold and considers a high reconstruction error as an
indicator of anomalous events. This study is the first to utilize compact
motion representations for VAD and the experiments conducted on two large-scale
VAD benchmarks demonstrate that they supply relevant information to the
diffusion model, and consequently improve VAD performances w.r.t the prior art.
Importantly, our method exhibits better generalization performance across
different datasets, notably outperforming both the state-of-the-art and
baseline methods. The code of our method is available at
https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion
</p></li>
</ul>

<h3>Title: Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01668">http://arxiv.org/abs/2307.01668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01668] Training Energy-Based Models with Diffusion Contrastive Divergences](http://arxiv.org/abs/2307.01668) #diffusion</code></li>
<li>Summary: <p>Energy-Based Models (EBMs) have been widely used for generative modeling.
Contrastive Divergence (CD), a prevailing training objective for EBMs, requires
sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which
leads to an irreconcilable trade-off between the computational burden and the
validity of the CD. Running MCMCs till convergence is computationally
intensive. On the other hand, short-run MCMC brings in an extra non-negligible
parameter gradient term that is difficult to handle. In this paper, we provide
a general interpretation of CD, viewing it as a special instance of our
proposed Diffusion Contrastive Divergence (DCD) family. By replacing the
Langevin dynamic used in CD with other EBM-parameter-free diffusion processes,
we propose a more efficient divergence. We show that the proposed DCDs are both
more computationally efficient than the CD and are not limited to a
non-negligible gradient term. We conduct intensive experiments, including both
synthesis data modeling and high-dimensional image denoising and generation, to
show the advantages of the proposed DCDs. On the synthetic data learning and
image denoising experiments, our proposed DCD outperforms CD by a large margin.
In image generation experiments, the proposed DCD is capable of training an
energy-based model for generating the Celab-A $32\times 32$ dataset, which is
comparable to existing EBMs.
</p></li>
</ul>

<h3>Title: Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT. (arXiv:2307.01740v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01740">http://arxiv.org/abs/2307.01740</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01740] Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT](http://arxiv.org/abs/2307.01740) #diffusion</code></li>
<li>Summary: <p>Stroke lesion volume is a key radiologic measurement for assessing the
prognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be
automatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion
probabilistic models have shown potentials of being used for image
segmentation. In this paper, a novel Synchronous image-label Diffusion
Probability Model (SDPM) is proposed for stroke lesion segmentation on NCCT
using Markov diffusion process. The proposed SDPM is fully based on a Latent
Variable Model (LVM), offering a complete probabilistic elaboration. An
additional net-stream, parallel with a noise prediction stream, is introduced
to obtain initial noisy label estimates for efficiently inferring the final
labels. By optimizing the specified variational boundaries, the trained model
can infer multiple label estimates for reference given the input images with
noises. The proposed model was assessed on three stroke lesion datasets
including one public and two private datasets. Compared to several U-net and
transformer-based segmentation methods, our proposed SDPM model is able to
achieve state-of-the-art performance. The code is publicly available.
</p></li>
</ul>

<h3>Title: DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01831">http://arxiv.org/abs/2307.01831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01831] DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation](http://arxiv.org/abs/2307.01831) #diffusion</code></li>
<li>Summary: <p>Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful
effectiveness in generating high-quality 2D images. However, it is still being
determined whether the Transformer architecture performs equally well in 3D
shape generation, as previous 3D diffusion methods mostly adopted the U-Net
architecture. To bridge this gap, we propose a novel Diffusion Transformer for
3D shape generation, namely DiT-3D, which can directly operate the denoising
process on voxelized point clouds using plain Transformers. Compared to
existing U-Net approaches, our DiT-3D is more scalable in model size and
produces much higher quality generations. Specifically, the DiT-3D adopts the
design philosophy of DiT but modifies it by incorporating 3D positional and
patch embeddings to adaptively aggregate input from voxelized point clouds. To
reduce the computational cost of self-attention in 3D shape generation, we
incorporate 3D window attention into Transformer blocks, as the increased 3D
token length resulting from the additional dimension of voxels can lead to high
computation. Finally, linear and devoxelization layers are used to predict the
denoised point clouds. In addition, our transformer architecture supports
efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on
ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on
the ShapeNet dataset demonstrate that the proposed DiT-3D achieves
state-of-the-art performance in high-fidelity and diverse 3D point cloud
generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy
of the state-of-the-art method by 4.59 and increases the Coverage metric by
3.51 when evaluated on Chamfer Distance.
</p></li>
</ul>

<h3>Title: SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01646">http://arxiv.org/abs/2307.01646</a></li>
<li>Code URL: <a href="https://github.com/qiyan98/swingnn">https://github.com/qiyan98/swingnn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01646] SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation](http://arxiv.org/abs/2307.01646) #diffusion</code></li>
<li>Summary: <p>Diffusion models based on permutation-equivariant networks can learn
permutation-invariant distributions for graph data. However, in comparison to
their non-invariant counterparts, we have found that these invariant models
encounter greater learning challenges since 1) their effective target
distributions exhibit more modes; 2) their optimal one-step denoising scores
are the score functions of Gaussian mixtures with more components. Motivated by
this analysis, we propose a non-invariant diffusion model, called
$\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message
passing network and utilizes shifted window based self-attention inspired by
SwinTransformers. Further, through systematic ablations, we identify several
critical training and sampling techniques that significantly improve the sample
quality of graph generation. At last, we introduce a simple post-processing
trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably
converts any graph generative model to a permutation-invariant one. Extensive
experiments on synthetic and real-world protein and molecule datasets show that
our SwinGNN achieves state-of-the-art performances. Our code is released at
https://github.com/qiyan98/SwinGNN .
</p></li>
</ul>

<h3>Title: On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01717">http://arxiv.org/abs/2307.01717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01717] On the Constrained Time-Series Generation Problem](http://arxiv.org/abs/2307.01717) #diffusion</code></li>
<li>Summary: <p>Synthetic time series are often used in practical applications to augment the
historical time series dataset for better performance of machine learning
algorithms, amplify the occurrence of rare events, and also create
counterfactual scenarios described by the time series.
Distributional-similarity (which we refer to as realism) as well as the
satisfaction of certain numerical constraints are common requirements in
counterfactual time series scenario generation requests. For instance, the US
Federal Reserve publishes synthetic market stress scenarios given by the
constrained time series for financial institutions to assess their performance
in hypothetical recessions. Existing approaches for generating constrained time
series usually penalize training loss to enforce constraints, and reject
non-conforming samples. However, these approaches would require re-training if
we change constraints, and rejection sampling can be computationally expensive,
or impractical for complex constraints. In this paper, we propose a novel set
of methods to tackle the constrained time series generation problem and provide
efficient sampling while ensuring the realism of generated time series. In
particular, we frame the problem using a constrained optimization framework and
then we propose a set of generative methods including <code>GuidedDiffTime'', a
guided diffusion model to generate realistic time series. Empirically, we
evaluate our work on several datasets for financial and energy data, where
incorporating constraints is critical. We show that our approaches outperform
existing work both qualitatively and quantitatively. Most importantly, we show
that our</code>GuidedDiffTime'' model is the only solution where re-training is not
necessary for new constraints, resulting in a significant carbon footprint
reduction.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions. (arXiv:2307.01530v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01530">http://arxiv.org/abs/2307.01530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01530] Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions](http://arxiv.org/abs/2307.01530) #transformer</code></li>
<li>Summary: <p>Harvesting fully ripe tomatoes with mobile robots presents significant
challenges in real-world scenarios. These challenges arise from factors such as
occlusion caused by leaves and branches, as well as the color similarity
between tomatoes and the surrounding foliage during the fruit development
stage. The natural environment further compounds these issues with varying
light conditions, viewing angles, occlusion factors, and different maturity
levels. To overcome these obstacles, this research introduces a novel framework
that leverages a convolutional transformer architecture to autonomously
recognize and grade tomatoes, irrespective of their occlusion level, lighting
conditions, and ripeness. The proposed model is trained and tested using
carefully annotated images curated specifically for this purpose. The dataset
is prepared under various lighting conditions, viewing perspectives, and
employs different mobile camera sensors, distinguishing it from existing
datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The
effectiveness of the proposed framework in handling cluttered and occluded
tomato instances was evaluated using two additional public datasets, Laboro
Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results
across these three datasets demonstrate the exceptional performance of our
proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and
66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato,
and Rob2Pheno Annotated Tomato, respectively. The results underscore the
superiority of the proposed model in accurately detecting and delineating
tomatoes compared to baseline methods and previous approaches. Specifically,
the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a
mean IoU of 66.41% on the KUTomaData image dataset.
</p></li>
</ul>

<h3>Title: In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification. (arXiv:2307.01645v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01645">http://arxiv.org/abs/2307.01645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01645] In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification](http://arxiv.org/abs/2307.01645) #transformer</code></li>
<li>Summary: <p>Self-supervised learning (SSL) has emerged as a promising approach for remote
sensing image classification due to its ability to leverage large amounts of
unlabeled data. In contrast to traditional supervised learning, SSL aims to
learn representations of data without the need for explicit labels. This is
achieved by formulating auxiliary tasks that can be used to create
pseudo-labels for the unlabeled data and learn pre-trained models. The
pre-trained models can then be fine-tuned on downstream tasks such as remote
sensing image scene classification. The paper analyzes the effectiveness of SSL
pre-training using Million AID - a large unlabeled remote sensing dataset on
various remote sensing image scene classification datasets as downstream tasks.
More specifically, we evaluate the effectiveness of SSL pre-training using the
iBOT framework coupled with Vision transformers (ViT) in contrast to supervised
pre-training of ViT using the ImageNet dataset. The comprehensive experimental
work across 14 datasets with diverse properties reveals that in-domain SSL
leads to improved predictive performance of models compared to the supervised
counterparts.
</p></li>
</ul>

<h3>Title: Exploring Transformers for On-Line Handwritten Signature Verification. (arXiv:2307.01663v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01663">http://arxiv.org/abs/2307.01663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01663] Exploring Transformers for On-Line Handwritten Signature Verification](http://arxiv.org/abs/2307.01663) #transformer</code></li>
<li>Summary: <p>The application of mobile biometrics as a user-friendly authentication method
has increased in the last years. Recent studies have proposed novel behavioral
biometric recognition systems based on Transformers, which currently outperform
the state of the art in several application scenarios. On-line handwritten
signature verification aims to verify the identity of subjects, based on their
biometric signatures acquired using electronic devices such as tablets or
smartphones. This paper investigates the suitability of architectures based on
recent Transformers for on-line signature verification. In particular, four
different configurations are studied, two of them rely on the Vanilla
Transformer encoder, and the two others have been successfully applied to the
tasks of gait and activity recognition. We evaluate the four proposed
configurations according to the experimental protocol proposed in the
SVC-onGoing competition. The results obtained in our experiments are promising,
and promote the use of Transformers for on-line signature verification.
</p></li>
</ul>

<h3>Title: Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification. (arXiv:2307.01759v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01759">http://arxiv.org/abs/2307.01759</a></li>
<li>Code URL: <a href="https://github.com/lugges991/metaformer">https://github.com/lugges991/metaformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01759] Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification](http://arxiv.org/abs/2307.01759) #transformer</code></li>
<li>Summary: <p>Autism spectrum disorder (ASD) is a prevalent psychiatric condition
characterized by atypical cognitive, emotional, and social patterns. Timely and
accurate diagnosis is crucial for effective interventions and improved outcomes
in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced
Transformer framework, METAFormer, ASD classification. Our framework utilizes
resting-state functional magnetic resonance imaging data from the ABIDE I
dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer
employs a multi-atlas approach, where flattened connectivity matrices from the
AAL, CC200, and DOS160 atlases serve as input to the transformer encoder.
Notably, we demonstrate that self-supervised pretraining, involving the
reconstruction of masked values from the input, significantly enhances
classification performance without the need for additional or separate training
data. Through stratified cross-validation, we evaluate the proposed framework
and show that it surpasses state-of-the-art performance on the ABIDE I dataset,
with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our
framework is available at https://github.com/Lugges991/METAFormer
</p></li>
</ul>

<h3>Title: EdgeFace: Efficient Face Recognition Model for Edge Devices. (arXiv:2307.01838v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01838">http://arxiv.org/abs/2307.01838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01838] EdgeFace: Efficient Face Recognition Model for Edge Devices](http://arxiv.org/abs/2307.01838) #transformer</code></li>
<li>Summary: <p>In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.
</p></li>
</ul>

<h3>Title: Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01201">http://arxiv.org/abs/2307.01201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01201] Schema-learning and rebinding as mechanisms of in-context learning and emergence](http://arxiv.org/abs/2307.01201) #transformer</code></li>
<li>Summary: <p>In-context learning (ICL) is one of the most powerful and most unexpected
capabilities to emerge in recent transformer-based large language models
(LLMs). Yet the mechanisms that underlie it are poorly understood. In this
paper, we demonstrate that comparable ICL capabilities can be acquired by an
alternative sequence prediction learning method using clone-structured causal
graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike
transformer-based LLMs, they are {\em interpretable}, which considerably
simplifies the task of explaining how ICL works. Specifically, we show that it
uses a combination of (a) learning template (schema) circuits for pattern
completion, (b) retrieving relevant templates in a context-sensitive manner,
and (c) rebinding of novel tokens to appropriate slots in the templates. We go
on to marshall evidence for the hypothesis that similar mechanisms underlie ICL
in LLMs. For example, we find that, with CSCGs as with LLMs, different
capabilities emerge at different levels of overparameterization, suggesting
that overparameterization helps in learning more complex template (schema)
circuits. By showing how ICL can be achieved with small models and datasets, we
open up a path to novel architectures, and take a vital step towards a more
general understanding of the mechanics behind this important capability.
</p></li>
</ul>

<h3>Title: Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01377">http://arxiv.org/abs/2307.01377</a></li>
<li>Code URL: <a href="https://github.com/osu-starlab/shiftablecontext">https://github.com/osu-starlab/shiftablecontext</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01377] Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation](http://arxiv.org/abs/2307.01377) #transformer</code></li>
<li>Summary: <p>Transformer models using segment-based processing have been an effective
architecture for simultaneous speech translation. However, such models create a
context mismatch between training and inference environments, hindering
potential translation accuracy. We solve this issue by proposing Shiftable
Context, a simple yet effective scheme to ensure that consistent segment and
context sizes are maintained throughout training and inference, even with the
presence of partially filled segments due to the streaming nature of
simultaneous translation. Shiftable Context is also broadly applicable to
segment-based transformers for streaming tasks. Our experiments on the
English-German, English-French, and English-Spanish language pairs from the
MUST-C dataset demonstrate that when applied to the Augmented Memory
Transformer, a state-of-the-art model for simultaneous speech translation, the
proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU
scores across each wait-k value for the three language pairs, respectively,
with a minimal impact on computation-aware Average Lagging.
</p></li>
</ul>

<h3>Title: Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01381">http://arxiv.org/abs/2307.01381</a></li>
<li>Code URL: <a href="https://github.com/osu-starlab/implicitmemory">https://github.com/osu-starlab/implicitmemory</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01381] Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation](http://arxiv.org/abs/2307.01381) #transformer</code></li>
<li>Summary: <p>Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment's attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.
</p></li>
</ul>

<h3>Title: ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01387">http://arxiv.org/abs/2307.01387</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01387] ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis](http://arxiv.org/abs/2307.01387) #transformer</code></li>
<li>Summary: <p>The computational analysis of poetry is limited by the scarcity of tools to
automatically analyze and scan poems. In a multilingual settings, the problem
is exacerbated as scansion and rhyme systems only exist for individual
languages, making comparative studies very challenging and time consuming. In
this work, we present \textsc{Alberti}, the first multilingual pre-trained
large language model for poetry. Through domain-specific pre-training (DSP), we
further trained multilingual BERT on a corpus of over 12 million verses from 12
languages. We evaluated its performance on two structural poetry tasks: Spanish
stanza type classification, and metrical pattern prediction for Spanish,
English and German. In both cases, \textsc{Alberti} outperforms multilingual
BERT and other transformers-based models of similar sizes, and even achieves
state-of-the-art results for German when compared to rule-based systems,
demonstrating the feasibility and effectiveness of DSP in the poetry domain.
</p></li>
</ul>

<h3>Title: The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01784">http://arxiv.org/abs/2307.01784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01784] The Inner Sentiments of a Thought](http://arxiv.org/abs/2307.01784) #transformer</code></li>
<li>Summary: <p>Transformer-based large-scale language models (LLMs) are able to generate
highly realistic text. They are duly able to express, and at least implicitly
represent, a wide range of sentiments and color, from the obvious, such as
valence and arousal to the subtle, such as determination and admiration. We
provide a first exploration of these representations and how they can be used
for understanding the inner sentimental workings of single sentences. We train
predictors of the quantiles of the distributions of final sentiments of
sentences from the hidden representations of an LLM applied to prefixes of
increasing lengths. After showing that predictors of distributions of valence,
determination, admiration, anxiety and annoyance are well calibrated, we
provide examples of using these predictors for analyzing sentences,
illustrating, for instance, how even ordinary conjunctions (e.g., "but") can
dramatically alter the emotional trajectory of an utterance. We then show how
to exploit the distributional predictions to generate sentences with sentiments
in the tails of distributions. We discuss the implications of our results for
the inner workings of thoughts, for instance for psychiatric dysfunction.
</p></li>
</ul>

<h3>Title: Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch. (arXiv:2307.01236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01236">http://arxiv.org/abs/2307.01236</a></li>
<li>Code URL: <a href="https://github.com/topal-team/rockmate">https://github.com/topal-team/rockmate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01236] Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch](http://arxiv.org/abs/2307.01236) #transformer</code></li>
<li>Summary: <p>We propose Rockmate to control the memory requirements when training PyTorch
DNN models. Rockmate is an automatic tool that starts from the model code and
generates an equivalent model, using a predefined amount of memory for
activations, at the cost of a few re-computations. Rockmate automatically
detects the structure of computational and data dependencies and rewrites the
initial model as a sequence of complex blocks. We show that such a structure is
widespread and can be found in many models in the literature (Transformer based
models, ResNet, RegNets,...). This structure allows us to solve the problem in
a fast and efficient way, using an adaptation of Checkmate (too slow on the
whole model but general) at the level of individual blocks and an adaptation of
Rotor (fast but limited to sequential models) at the level of the sequence
itself. We show through experiments on many models that Rockmate is as fast as
Rotor and as efficient as Checkmate, and that it allows in many cases to obtain
a significantly lower memory consumption for activations (by a factor of 2 to
5) for a rather negligible overhead (of the order of 10% to 20%). Rockmate is
open source and available at https://github.com/topal-team/rockmate.
</p></li>
</ul>

<h3>Title: Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series. (arXiv:2307.01482v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01482">http://arxiv.org/abs/2307.01482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01482] Nexus sine qua non: Essentially connected neural networks for spatial-temporal forecasting of multivariate time series](http://arxiv.org/abs/2307.01482) #transformer</code></li>
<li>Summary: <p>Modeling and forecasting multivariate time series not only facilitates the
decision making of practitioners, but also deepens our scientific understanding
of the underlying dynamical systems. Spatial-temporal graph neural networks
(STGNNs) are emerged as powerful predictors and have become the de facto models
for learning spatiotemporal representations in recent years. However, existing
architectures of STGNNs tend to be complicated by stacking a series of fancy
layers. The designed models could be either redundant or enigmatic, which pose
great challenges on their complexity and scalability. Such concerns prompt us
to re-examine the designs of modern STGNNs and identify core principles that
contribute to a powerful and efficient neural predictor. Here we present a
compact predictive model that is fully defined by a dense encoder-decoder and a
message-passing layer, powered by node identifications, without any complex
sequential modules, e.g., TCNs, RNNs, and Transformers. Empirical results
demonstrate how a simple and elegant model with proper inductive basis can
compare favorably w.r.t. the state of the art with elaborate designs, while
being much more interpretable and computationally efficient for
spatial-temporal forecasting problem. We hope our findings would open new
horizons for future studies to revisit the design of more concise neural
forecasting architectures.
</p></li>
</ul>

<h3>Title: Deep Attention Q-Network for Personalized Treatment Recommendation. (arXiv:2307.01519v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01519">http://arxiv.org/abs/2307.01519</a></li>
<li>Code URL: <a href="https://github.com/stevenmsm/rl-icu-daqn">https://github.com/stevenmsm/rl-icu-daqn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01519] Deep Attention Q-Network for Personalized Treatment Recommendation](http://arxiv.org/abs/2307.01519) #transformer</code></li>
<li>Summary: <p>Tailoring treatment for individual patients is crucial yet challenging in
order to achieve optimal healthcare outcomes. Recent advances in reinforcement
learning offer promising personalized treatment recommendations; however, they
rely solely on current patient observations (vital signs, demographics) as the
patient's state, which may not accurately represent the true health status of
the patient. This limitation hampers policy learning and evaluation, ultimately
limiting treatment effectiveness. In this study, we propose the Deep Attention
Q-Network for personalized treatment recommendations, utilizing the Transformer
architecture within a deep reinforcement learning framework to efficiently
incorporate all past patient observations. We evaluated the model on real-world
sepsis and acute hypotension cohorts, demonstrating its superiority to
state-of-the-art models. The source code for our model is available at
https://github.com/stevenmsm/RL-ICU-DAQN.
</p></li>
</ul>

<h3>Title: Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01597">http://arxiv.org/abs/2307.01597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01597] Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework](http://arxiv.org/abs/2307.01597) #transformer</code></li>
<li>Summary: <p>Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in
various domains. While state-of-the-art deep learning models excel in regular
Time Series Forecasting (TSF), they struggle to achieve comparable results in
PHSF. This can be attributed to the challenges posed by the high degree of
non-stationarity in peak-hour series, which makes direct forecasting more
difficult than standard TSF. Additionally, manually extracting the maximum
value from regular forecasting results leads to suboptimal performance due to
models minimizing the mean deficit. To address these issues, this paper
presents Seq2Peak, a novel framework designed specifically for PHSF tasks,
bridging the performance gap observed in TSF models. Seq2Peak offers two key
components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and
a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid
loss function that utilizes both the original series and peak-hour series as
supervised signals. Extensive experimentation on publicly available time series
datasets demonstrates the effectiveness of the proposed framework, yielding a
remarkable average relative improvement of 37.7\% across four real-world
datasets for both transformer- and non-transformer-based TSF models.
</p></li>
</ul>

<h3>Title: SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01616">http://arxiv.org/abs/2307.01616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01616] SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting](http://arxiv.org/abs/2307.01616) #transformer</code></li>
<li>Summary: <p>Multivariate time series forecasting plays a critical role in diverse
domains. While recent advancements in deep learning methods, especially
Transformers, have shown promise, there remains a gap in addressing the
significance of inter-series dependencies. This paper introduces SageFormer, a
Series-aware Graph-enhanced Transformer model designed to effectively capture
and model dependencies between series using graph structures. SageFormer
tackles two key challenges: effectively representing diverse temporal patterns
across series and mitigating redundant information among series. Importantly,
the proposed series-aware framework seamlessly integrates with existing
Transformer-based models, augmenting their ability to model inter-series
dependencies. Through extensive experiments on real-world and synthetic
datasets, we showcase the superior performance of SageFormer compared to
previous state-of-the-art approaches.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Consistent Multimodal Generation via A Unified GAN Framework. (arXiv:2307.01425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01425">http://arxiv.org/abs/2307.01425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01425] Consistent Multimodal Generation via A Unified GAN Framework](http://arxiv.org/abs/2307.01425) #generative</code></li>
<li>Summary: <p>We investigate how to generate multimodal image outputs, such as RGB, depth,
and surface normals, with a single generative model. The challenge is to
produce outputs that are realistic, and also consistent with each other. Our
solution builds on the StyleGAN3 architecture, with a shared backbone and
modality-specific branches in the last layers of the synthesis network, and we
propose per-modality fidelity discriminators and a cross-modality consistency
discriminator. In experiments on the Stanford2D3D dataset, we demonstrate
realistic and consistent generation of RGB, depth, and normal images. We also
show a training recipe to easily extend our pretrained model on a new domain,
even with a few pairwise data. We further evaluate the use of synthetically
generated RGB and depth pairs for training or fine-tuning depth estimators.
Code will be available at https://github.com/jessemelpolio/MultimodalGAN.
</p></li>
</ul>

<h3>Title: Learning Lie Group Symmetry Transformations with Neural Networks. (arXiv:2307.01583v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01583">http://arxiv.org/abs/2307.01583</a></li>
<li>Code URL: <a href="https://github.com/victoria-klein/learning-lie-group-symmetries">https://github.com/victoria-klein/learning-lie-group-symmetries</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01583] Learning Lie Group Symmetry Transformations with Neural Networks](http://arxiv.org/abs/2307.01583) #generative</code></li>
<li>Summary: <p>The problem of detecting and quantifying the presence of symmetries in
datasets is useful for model selection, generative modeling, and data analysis,
amongst others. While existing methods for hard-coding transformations in
neural networks require prior knowledge of the symmetries of the task at hand,
this work focuses on discovering and characterizing unknown symmetries present
in the dataset, namely, Lie group symmetry transformations beyond the
traditional ones usually considered in the field (rotation, scaling, and
translation). Specifically, we consider a scenario in which a dataset has been
transformed by a one-parameter subgroup of transformations with different
parameter values for each data point. Our goal is to characterize the
transformation group and the distribution of the parameter values. The results
showcase the effectiveness of the approach in both these settings.
</p></li>
</ul>

<h3>Title: Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01230">http://arxiv.org/abs/2307.01230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01230] Large Language and Text-to-3D Models for Engineering Design Optimization](http://arxiv.org/abs/2307.01230) #generative</code></li>
<li>Summary: <p>The current advances in generative AI for learning large neural network
models with the capability to produce essays, images, music and even 3D assets
from text prompts create opportunities for a manifold of disciplines. In the
present paper, we study the potential of deep text-to-3D models in the
engineering domain, with focus on the chances and challenges when integrating
and interacting with 3D assets in computational simulation-based design
optimization. In contrast to traditional design optimization of 3D geometries
that often searches for the optimum designs using numerical representations,
such as B-Spline surface or deformation parameters in vehicle aerodynamic
optimization, natural language challenges the optimization framework by
requiring a different interpretation of variation operators while at the same
time may ease and motivate the human user interaction. Here, we propose and
realize a fully automated evolutionary design optimization framework using
Shap-E, a recently published text-to-3D asset network by OpenAI, in the context
of aerodynamic vehicle optimization. For representing text prompts in the
evolutionary optimization, we evaluate (a) a bag-of-words approach based on
prompt templates and Wordnet samples, and (b) a tokenisation approach based on
prompt templates and the byte pair encoding method from GPT4. Our main findings
from the optimizations indicate that, first, it is important to ensure that the
designs generated from prompts are within the object class of application, i.e.
diverse and novel designs need to be realistic, and, second, that more research
is required to develop methods where the strength of text prompt variations and
the resulting variations of the 3D designs share causal relations to some
degree to improve the optimization.
</p></li>
</ul>

<h3>Title: Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01370">http://arxiv.org/abs/2307.01370</a></li>
<li>Code URL: <a href="https://github.com/shreyahavaldar/multicultural_emotion">https://github.com/shreyahavaldar/multicultural_emotion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01370] Multilingual Language Models are not Multicultural: A Case Study in Emotion](http://arxiv.org/abs/2307.01370) #generative</code></li>
<li>Summary: <p>Emotions are experienced and expressed differently across the world. In order
to use Large Language Models (LMs) for multilingual tasks that require
emotional sensitivity, LMs must reflect this cultural variation in emotion. In
this study, we investigate whether the widely-used multilingual LMs in 2023
reflect differences in emotional expressions across cultures and languages. We
find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,
and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding
to prompts in other languages. Our results show that multilingual LMs do not
successfully learn the culturally appropriate nuances of emotion and we
highlight possible research directions towards correcting this.
</p></li>
</ul>

<h3>Title: Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data. (arXiv:2307.01764v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01764">http://arxiv.org/abs/2307.01764</a></li>
<li>Code URL: <a href="https://github.com/the-anonymous-bs/espnet">https://github.com/the-anonymous-bs/espnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01764] Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data](http://arxiv.org/abs/2307.01764) #generative</code></li>
<li>Summary: <p>Manually annotating fine-grained slot-value labels for task-oriented dialogue
(ToD) systems is an expensive and time-consuming endeavour. This motivates
research into slot-filling methods that operate with limited amounts of
labelled data. Moreover, the majority of current work on ToD is based solely on
text as the input modality, neglecting the additional challenges of imperfect
automatic speech recognition (ASR) when working with spoken language. In this
work, we propose a Knowledge-Aware Audio-Grounded generative slot-filling
framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for
ToD with speech input. KA2G achieves robust and data-efficient slot filling for
speech-based ToD by 1) framing it as a text generation task, 2) grounding text
generation additionally in the audio modality, and 3) conditioning on available
external knowledge (e.g. a predefined list of possible slot values). We show
that combining both modalities within the KA2G framework improves the
robustness against ASR errors. Further, the knowledge-aware slot-value
generator in KA2G, implemented via a pointer generator mechanism, particularly
benefits few-shot and zero-shot learning. Experiments, conducted on the
standard speech-based single-turn SLURP dataset and a multi-turn dataset
extracted from a commercial ToD system, display strong and consistent gains
over prior work, especially in few-shot and zero-shot setups.
</p></li>
</ul>

<h3>Title: Generative Flow Networks: a Markov Chain Perspective. (arXiv:2307.01422v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01422">http://arxiv.org/abs/2307.01422</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01422] Generative Flow Networks: a Markov Chain Perspective](http://arxiv.org/abs/2307.01422) #generative</code></li>
<li>Summary: <p>While Markov chain Monte Carlo methods (MCMC) provide a general framework to
sample from a probability distribution defined up to normalization, they often
suffer from slow convergence to the target distribution when the latter is
highly multi-modal. Recently, Generative Flow Networks (GFlowNets) have been
proposed as an alternative framework to mitigate this issue when samples have a
clear compositional structure, by treating sampling as a sequential decision
making problem. Although they were initially introduced from the perspective of
flow networks, the recent advances of GFlowNets draw more and more inspiration
from the Markov chain literature, bypassing completely the need for flows. In
this paper, we formalize this connection and offer a new perspective for
GFlowNets using Markov chains, showing a unifying view for GFlowNets regardless
of the nature of the state space as recurrent Markov chains. Positioning
GFlowNets under the same theoretical framework as MCMC methods also allows us
to identify the similarities between both frameworks, and most importantly to
highlight their
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01848">http://arxiv.org/abs/2307.01848</a></li>
<li>Code URL: <a href="https://github.com/Gary3410/TaPA">https://github.com/Gary3410/TaPA</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01848] Embodied Task Planning with Large Language Models](http://arxiv.org/abs/2307.01848) #large language model</code></li>
<li>Summary: <p>Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.
</p></li>
</ul>

<h3>Title: Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01379">http://arxiv.org/abs/2307.01379</a></li>
<li>Code URL: <a href="https://github.com/jinhaoduan/shifting-attention-to-relevance">https://github.com/jinhaoduan/shifting-attention-to-relevance</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01379] Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models](http://arxiv.org/abs/2307.01379) #large language model</code></li>
<li>Summary: <p>Although Large Language Models (LLMs) have shown great potential in Natural
Language Generation, it is still challenging to characterize the uncertainty of
model generations, i.e., when users could trust model outputs. Our research is
derived from the heuristic facts that tokens are created unequally in
reflecting the meaning of generations by auto-regressive LLMs, i.e., some
tokens are more relevant (or representative) than others, yet all the tokens
are equally valued when estimating uncertainty. It is because of the linguistic
redundancy where mostly a few keywords are sufficient to convey the meaning of
a long sentence. We name these inequalities as generative inequalities and
investigate how they affect uncertainty estimation. Our results reveal that
considerable tokens and sentences containing limited semantics are weighted
equally or even heavily when estimating uncertainty. To tackle these biases
posed by generative inequalities, we propose to jointly Shifting Attention to
more Relevant (SAR) components from both the token level and the sentence level
while estimating uncertainty. We conduct experiments over popular
"off-the-shelf" LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful
commercial LLMs (e.g., Davinci from OpenAI), across various free-form
question-answering tasks. Experimental results and detailed demographic
analysis indicate the superior performance of SAR. Code is available at
https://github.com/jinhaoduan/shifting-attention-to-relevance.
</p></li>
</ul>

<h3>Title: Chain of Thought Prompting Elicits Knowledge Augmentation. (arXiv:2307.01640v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01640">http://arxiv.org/abs/2307.01640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01640] Chain of Thought Prompting Elicits Knowledge Augmentation](http://arxiv.org/abs/2307.01640) #large language model</code></li>
<li>Summary: <p>The knowledge-augmented deep learning paradigm refers to a paradigm in which
domain knowledge is identified and integrated into deep models. Conventional
methods typically employ task-specific approaches to gather external knowledge
from various sources. In contrast, large language models are extensively
pre-trained and can serve as a comprehensive source of external knowledge. In
this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments
knowledge for deep learning. CoT-KA avoids the need for additional knowledge
retrieval or knowledge reasoning models, as required in conventional
augmentation methods. Our results demonstrate that CoT-KA outperforms both pure
CoT-based methods and the non-augmented method across the majority of eleven
publicly available benchmarks for various reasoning tasks.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Semantic Segmentation on 3D Point Clouds with High Density Variations. (arXiv:2307.01489v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01489">http://arxiv.org/abs/2307.01489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01489] Semantic Segmentation on 3D Point Clouds with High Density Variations](http://arxiv.org/abs/2307.01489) #segmentation</code></li>
<li>Summary: <p>LiDAR scanning for surveying applications acquire measurements over wide
areas and long distances, which produces large-scale 3D point clouds with
significant local density variations. While existing 3D semantic segmentation
models conduct downsampling and upsampling to build robustness against varying
point densities, they are less effective under the large local density
variations characteristic of point clouds from surveying applications. To
alleviate this weakness, we propose a novel architecture called HDVNet that
contains a nested set of encoder-decoder pathways, each handling a specific
point density range. Limiting the interconnections between the feature maps
enables HDVNet to gauge the reliability of each feature based on the density of
a point, e.g., downweighting high density features not existing in low density
objects. By effectively handling input density variations, HDVNet outperforms
state-of-the-art models in segmentation accuracy on real point clouds with
inconsistent density, using just over half the weights.
</p></li>
</ul>

<h3>Title: Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01524">http://arxiv.org/abs/2307.01524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01524] Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation](http://arxiv.org/abs/2307.01524) #segmentation</code></li>
<li>Summary: <p>Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the
potential to radically change the way we travel. Many such vehicles currently
rely on segmentation and object detection algorithms to detect and track
objects around its surrounding. The data collected from the vehicles are often
sent to cloud servers to facilitate continual/life-long learning of these
algorithms. Considering the bandwidth constraints, the data is compressed
before sending it to servers, where it is typically decompressed for training
and analysis. In this work, we propose the use of a learning-based compression
Codec to reduce the overhead in latency incurred for the decompression
operation in the standard pipeline. We demonstrate that the learned compressed
representation can also be used to perform tasks like semantic segmentation in
addition to decompression to obtain the images. We experimentally validate the
proposed pipeline on the Cityscapes dataset, where we achieve a compression
factor up to $66 \times$ while preserving the information required to perform
segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved
using decompressed images while reducing the overall compute by $11\%$.
</p></li>
</ul>

<h3>Title: EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity. (arXiv:2307.01545v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01545">http://arxiv.org/abs/2307.01545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01545] EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity](http://arxiv.org/abs/2307.01545) #segmentation</code></li>
<li>Summary: <p>Many two-stage instance segmentation heads predict a coarse 28x28 mask per
instance, which is insufficient to capture the fine-grained details of many
objects. To address this issue, PointRend and RefineMask predict a 112x112
segmentation mask resulting in higher quality segmentations. Both methods
however have limitations by either not having access to neighboring features
(PointRend) or by performing computation at all spatial locations instead of
sparsely (RefineMask). In this work, we propose EffSeg performing fine-grained
instance segmentation in an efficient way by using our Structure-Preserving
Sparsity (SPS) method based on separately storing the active features, the
passive features and a dense 2D index map containing the feature indices. The
goal of the index map is to preserve the 2D spatial configuration or structure
between the features such that any 2D operation can still be performed. EffSeg
achieves similar performance on COCO compared to RefineMask, while reducing the
number of FLOPs by 71% and increasing the FPS by 29%. Code will be released.
</p></li>
</ul>

<h3>Title: Augment Features Beyond Color for Domain Generalized Segmentation. (arXiv:2307.01703v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01703">http://arxiv.org/abs/2307.01703</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01703] Augment Features Beyond Color for Domain Generalized Segmentation](http://arxiv.org/abs/2307.01703) #segmentation</code></li>
<li>Summary: <p>Domain generalized semantic segmentation (DGSS) is an essential but highly
challenging task, in which the model is trained only on source data and any
target data is not available. Previous DGSS methods can be partitioned into
augmentation-based and normalization-based ones. The former either introduces
extra biased data or only conducts channel-wise adjustments for data
augmentation, and the latter may discard beneficial visual information, both of
which lead to limited performance in DGSS. Contrarily, our method performs
inter-channel transformation and meanwhile evades domain-specific biases, thus
diversifying data and enhancing model generalization performance. Specifically,
our method consists of two modules: random image color augmentation (RICA) and
random feature distribution augmentation (RFDA). RICA converts images from RGB
to the CIELAB color model and randomizes color maps in a perception-based way
for image enhancement purposes. We further this augmentation by extending it
beyond color to feature space using a CycleGAN-based generative network, which
complements RICA and further boosts generalization capability. We conduct
extensive experiments, and the generalization results from the synthetic GTAV
and SYNTHIA to the real Cityscapes, BDDS, and Mapillary datasets show that our
method achieves state-of-the-art performance in DGSS.
</p></li>
</ul>

<h3>Title: Ben-ge: Extending BigEarthNet with Geographical and Environmental Data. (arXiv:2307.01741v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.01741">http://arxiv.org/abs/2307.01741</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.01741] Ben-ge: Extending BigEarthNet with Geographical and Environmental Data](http://arxiv.org/abs/2307.01741) #segmentation</code></li>
<li>Summary: <p>Deep learning methods have proven to be a powerful tool in the analysis of
large amounts of complex Earth observation data. However, while Earth
observation data are multi-modal in most cases, only single or few modalities
are typically considered. In this work, we present the ben-ge dataset, which
supplements the BigEarthNet-MM dataset by compiling freely and globally
available geographical and environmental data. Based on this dataset, we
showcase the value of combining different data modalities for the downstream
tasks of patch-based land-use/land-cover classification and land-use/land-cover
segmentation. ben-ge is freely available and expected to serve as a test bed
for fully supervised and self-supervised Earth observation applications.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
