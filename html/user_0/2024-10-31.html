<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-31</h1>
<h3>Title: Efficient Machine Translation with a BiLSTM-Attention Approach</h3>
<ul>
<li><strong>Authors: </strong>Yuxu Wu, Yiren Xing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22335">https://arxiv.org/abs/2410.22335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22335">https://arxiv.org/pdf/2410.22335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22335]] Efficient Machine Translation with a BiLSTM-Attention Approach(https://arxiv.org/abs/2410.22335)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>With the rapid development of Natural Language Processing (NLP) technology, the accuracy and efficiency of machine translation have become hot topics of research. This paper proposes a novel Seq2Seq model aimed at improving translation quality while reducing the storage space required by the model. The model employs a Bidirectional Long Short-Term Memory network (Bi-LSTM) as the encoder to capture the context information of the input sequence; the decoder incorporates an attention mechanism, enhancing the model's ability to focus on key information during the translation process. Compared to the current mainstream Transformer model, our model achieves superior performance on the WMT14 machine translation dataset while maintaining a smaller size. The study first introduces the design principles and innovative points of the model architecture, followed by a series of experiments to verify the effectiveness of the model. The experimental includes an assessment of the model's performance on different language pairs, as well as comparative analysis with traditional Seq2Seq models. The results show that while maintaining translation accuracy, our model significantly reduces the storage requirements, which is of great significance for translation applications in resource-constrained scenarios. our code are available at this https URL . Thanks for the support provided by MindSpore Community.</li>
</ul>

<h3>Title: Accelerating Augmentation Invariance Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Jinhong Lin, Cheng-En Wu, Yibing Wei, Pedro Morgado</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22364">https://arxiv.org/abs/2410.22364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22364">https://arxiv.org/pdf/2410.22364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22364]] Accelerating Augmentation Invariance Pretraining(https://arxiv.org/abs/2410.22364)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Our work tackles the computational challenges of contrastive learning methods, particularly for the pretraining of Vision Transformers (ViTs). Despite the effectiveness of contrastive learning, the substantial computational resources required for training often hinder their practical application. To mitigate this issue, we propose an acceleration framework, leveraging ViT's unique ability to generalize across inputs of varying sequence lengths. Our method employs a mix of sequence compression strategies, including randomized token dropout and flexible patch scaling, to reduce the cost of gradient estimation and accelerate convergence. We further provide an in-depth analysis of the gradient estimation error of various acceleration strategies as well as their impact on downstream tasks, offering valuable insights into the trade-offs between acceleration and performance. We also propose a novel procedure to identify an optimal acceleration schedule to adjust the sequence compression ratios to the training progress, ensuring efficient training without sacrificing downstream performance. Our approach significantly reduces computational overhead across various self-supervised learning algorithms on large-scale datasets. In ImageNet, our method achieves speedups of 4$\times$ in MoCo, 3.3$\times$ in SimCLR, and 2.5$\times$ in DINO, demonstrating substantial efficiency gains.</li>
</ul>

<h3>Title: Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Viacheslav Surkov, Chris Wendler, Mikhail Terekhov, Justin Deschenaux, Robert West, Caglar Gulcehre</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22366">https://arxiv.org/abs/2410.22366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22366">https://arxiv.org/pdf/2410.22366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22366]] Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders(https://arxiv.org/abs/2410.22366)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain. Code is available at this https URL</li>
</ul>

<h3>Title: A Hierarchical Language Model For Interpretable Graph Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sambhav Khurana, Xiner Li, Shurui Gui, Shuiwang Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22372">https://arxiv.org/abs/2410.22372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22372">https://arxiv.org/pdf/2410.22372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22372]] A Hierarchical Language Model For Interpretable Graph Reasoning(https://arxiv.org/abs/2410.22372)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are being increasingly explored for graph tasks. Despite their remarkable success in text-based tasks, LLMs' capabilities in understanding explicit graph structures remain limited, particularly with large graphs. In this work, we introduce Hierarchical Language Model for Graph (HLM-G), which employs a two-block architecture to capture node-centric local information and interaction-centric global structure, effectively enhancing graph structure understanding abilities. The proposed scheme allows LLMs to address various graph queries with high efficacy, efficiency, and robustness, while reducing computational costs on large-scale graph tasks. Furthermore, we demonstrate the interpretability of our model using intrinsic attention weights and established explainers. Comprehensive evaluations across diverse graph reasoning and real-world tasks of node, link, and graph-levels highlight the superiority of our method, marking a significant advancement in the application of LLMs to graph understanding.</li>
</ul>

<h3>Title: Machine Unlearning using Forgetting Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Amartya Hatua, Trung T. Nguyen, Filip Cano, Andrew H. Sung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22374">https://arxiv.org/abs/2410.22374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22374">https://arxiv.org/pdf/2410.22374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22374]] Machine Unlearning using Forgetting Neural Networks(https://arxiv.org/abs/2410.22374)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Modern computer systems store vast amounts of personal data, enabling advances in AI and ML but risking user privacy and trust. For privacy reasons, it is desired sometimes for an ML model to forget part of the data it was trained on. This paper presents a new approach to machine unlearning using forgetting neural networks (FNN). FNNs are neural networks with specific forgetting layers, that take inspiration from the processes involved when a human brain forgets. While FNNs had been proposed as a theoretical construct, they have not been previously used as a machine unlearning method. We describe four different types of forgetting layers and study their properties. In our experimental evaluation, we report our results on the MNIST handwritten digit recognition and fashion datasets. The effectiveness of the unlearned models was tested using Membership Inference Attacks (MIA). Successful experimental results demonstrate the great potential of our proposed method for dealing with the machine unlearning problem.</li>
</ul>

<h3>Title: Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance</h3>
<ul>
<li><strong>Authors: </strong>Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, Jaewoong Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22376">https://arxiv.org/abs/2410.22376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22376">https://arxiv.org/pdf/2410.22376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22376]] Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance(https://arxiv.org/abs/2410.22376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at this https URL.</li>
</ul>

<h3>Title: A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification</h3>
<ul>
<li><strong>Authors: </strong>Flavio Corradini, Marco Gori, Carlo Lucheroni, Marco Piangerelli, Martina Zannotti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22377">https://arxiv.org/abs/2410.22377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22377">https://arxiv.org/pdf/2410.22377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22377]] A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification(https://arxiv.org/abs/2410.22377)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable interest in the field of time series analysis, due to their ability to capture dependencies among variables and across time points. The objective of the presented systematic literature review is hence to provide a comprehensive overview of the various modeling approaches and application domains of GNNs for time series classification and forecasting. A database search was conducted, and over 150 journal papers were selected for a detailed examination of the current state-of-the-art in the field. This examination is intended to offer to the reader a comprehensive collection of proposed models, links to related source code, available datasets, benchmark models, and fitting results. All this information is hoped to assist researchers in future studies. To the best of our knowledge, this is the first systematic literature review presenting a detailed comparison of the results of current spatio-temporal GNN models in different domains. In addition, in its final part this review discusses current limitations and challenges in the application of spatio-temporal GNNs, such as comparability, reproducibility, explainability, poor information capacity, and scalability.</li>
</ul>

<h3>Title: Discrete Modeling via Boundary Conditional Diffusion Processes</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Gu, Xiaocheng Feng, Lei Huang, Yingsheng Wu, Zekun Zhou, Weihong Zhong, Kun Zhu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22380">https://arxiv.org/abs/2410.22380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22380">https://arxiv.org/pdf/2410.22380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22380]] Discrete Modeling via Boundary Conditional Diffusion Processes(https://arxiv.org/abs/2410.22380)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We present an novel framework for efficiently and effectively extending the powerful continuous diffusion processes to discrete modeling. Previous approaches have suffered from the discrepancy between discrete data and continuous modeling. Our study reveals that the absence of guidance from discrete boundaries in learning probability contours is one of the main reasons. To address this issue, we propose a two-step forward process that first estimates the boundary as a prior distribution and then rescales the forward trajectory to construct a boundary conditional diffusion model. The reverse process is proportionally adjusted to guarantee that the learned contours yield more precise discrete data. Experimental results indicate that our approach achieves strong performance in both language modeling and discrete image generation tasks. In language modeling, our approach surpasses previous state-of-the-art continuous diffusion language models in three translation tasks and a summarization task, while also demonstrating competitive performance compared to auto-regressive transformers. Moreover, our method achieves comparable results to continuous diffusion models when using discrete ordinal pixels and establishes a new state-of-the-art for categorical image generation on the Cifar-10 dataset.</li>
</ul>

<h3>Title: Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss</h3>
<ul>
<li><strong>Authors: </strong>José Manuel de Frutos, Manuel A. Vázquez, Pablo Olmos, Joaquín Míguez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22381">https://arxiv.org/abs/2410.22381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22381">https://arxiv.org/pdf/2410.22381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22381]] Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss(https://arxiv.org/abs/2410.22381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Traditional implicit generative models are capable of learning highly complex data distributions. However, their training involves distinguishing real data from synthetically generated data using adversarial discriminators, which can lead to unstable training dynamics and mode dropping issues. In this work, we build on the \textit{invariant statistical loss} (ISL) method introduced in \cite{de2024training}, and extend it to handle heavy-tailed and multivariate data distributions. The data generated by many real-world phenomena can only be properly characterised using heavy-tailed probability distributions, and traditional implicit methods struggle to effectively capture their asymptotic behavior. To address this problem, we introduce a generator trained with ISL, that uses input noise from a generalised Pareto distribution (GPD). We refer to this generative scheme as Pareto-ISL for conciseness. Our experiments demonstrate that Pareto-ISL accurately models the tails of the distributions while still effectively capturing their central characteristics. The original ISL function was conceived for 1D data sets. When the actual data is $n$-dimensional, a straightforward extension of the method was obtained by targeting the $n$ marginal distributions of the data. This approach is computationally infeasible and ineffective in high-dimensional spaces. To overcome this, we extend the 1D approach using random projections and define a new loss function suited for multivariate data, keeping problems tractable by adjusting the number of projections. We assess its performance in multidimensional generative modeling and explore its potential as a pretraining technique for generative adversarial networks (GANs) to prevent mode collapse, reporting promising results and highlighting its robustness across various hyperparameter settings.</li>
</ul>

<h3>Title: Exploiting Semantic Scene Reconstruction for Estimating Building Envelope Characteristics</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Xu, Malcolm Mielle, Antoine Laborde, Ali Waseem, Florent Forest, Olga Fink</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22383">https://arxiv.org/abs/2410.22383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22383">https://arxiv.org/pdf/2410.22383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22383]] Exploiting Semantic Scene Reconstruction for Estimating Building Envelope Characteristics(https://arxiv.org/abs/2410.22383)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Achieving the EU's climate neutrality goal requires retrofitting existing buildings to reduce energy use and emissions. A critical step in this process is the precise assessment of geometric building envelope characteristics to inform retrofitting decisions. Previous methods for estimating building characteristics, such as window-to-wall ratio, building footprint area, and the location of architectural elements, have primarily relied on applying deep-learning-based detection or segmentation techniques on 2D images. However, these approaches tend to focus on planar facade properties, limiting their accuracy and comprehensiveness when analyzing complete building envelopes in 3D. While neural scene representations have shown exceptional performance in indoor scene reconstruction, they remain under-explored for external building envelope analysis. This work addresses this gap by leveraging cutting-edge neural surface reconstruction techniques based on signed distance function (SDF) representations for 3D building analysis. We propose BuildNet3D, a novel framework to estimate geometric building characteristics from 2D image inputs. By integrating SDF-based representation with semantic modality, BuildNet3D recovers fine-grained 3D geometry and semantics of building envelopes, which are then used to automatically extract building characteristics. Our framework is evaluated on a range of complex building structures, demonstrating high accuracy and generalizability in estimating window-to-wall ratio and building footprint. The results underscore the effectiveness of BuildNet3D for practical applications in building analysis and retrofitting.</li>
</ul>

<h3>Title: FNDEX: Fake News and Doxxing Detection with Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Dorsaf Sallami, Esma Aïmeur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22390">https://arxiv.org/abs/2410.22390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22390">https://arxiv.org/pdf/2410.22390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22390]] FNDEX: Fake News and Doxxing Detection with Explainable AI(https://arxiv.org/abs/2410.22390)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>The widespread and diverse online media platforms and other internet-driven communication technologies have presented significant challenges in defining the boundaries of freedom of expression. Consequently, the internet has been transformed into a potential cyber weapon. Within this evolving landscape, two particularly hazardous phenomena have emerged: fake news and doxxing. Although these threats have been subjects of extensive scholarly analysis, the crossroads where they intersect remain unexplored. This research addresses this convergence by introducing a novel system. The Fake News and Doxxing Detection with Explainable Artificial Intelligence (FNDEX) system leverages the capabilities of three distinct transformer models to achieve high-performance detection for both fake news and doxxing. To enhance data security, a rigorous three-step anonymization process is employed, rooted in a pattern-based approach for anonymizing personally identifiable information. Finally, this research emphasizes the importance of generating coherent explanations for the outcomes produced by both detection models. Our experiments on realistic datasets demonstrate that our system significantly outperforms the existing baselines</li>
</ul>

<h3>Title: A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks</h3>
<ul>
<li><strong>Authors: </strong>Thomas Schmied, Thomas Adler, Vihang Patil, Maximilian Beck, Korbinian Pöppel, Johannes Brandstetter, Günter Klambauer, Razvan Pascanu, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22391">https://arxiv.org/abs/2410.22391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22391">https://arxiv.org/pdf/2410.22391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22391]] A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks(https://arxiv.org/abs/2410.22391)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.</li>
</ul>

<h3>Title: AAAR-1.0: Assessing AI's Potential to Assist Research</h3>
<ul>
<li><strong>Authors: </strong>Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22394">https://arxiv.org/abs/2410.22394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22394">https://arxiv.org/pdf/2410.22394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22394]] AAAR-1.0: Assessing AI's Potential to Assist Research(https://arxiv.org/abs/2410.22394)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions.</li>
</ul>

<h3>Title: Power side-channel leakage localization through adversarial training of deep neural networks</h3>
<ul>
<li><strong>Authors: </strong>Jimmy Gammell, Anand Raghunathan, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22425">https://arxiv.org/abs/2410.22425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22425">https://arxiv.org/pdf/2410.22425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22425]] Power side-channel leakage localization through adversarial training of deep neural networks(https://arxiv.org/abs/2410.22425)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Supervised deep learning has emerged as an effective tool for carrying out power side-channel attacks on cryptographic implementations. While increasingly-powerful deep learning-based attacks are regularly published, comparatively-little work has gone into using deep learning to defend against these attacks. In this work we propose a technique for identifying which timesteps in a power trace are responsible for leaking a cryptographic key, through an adversarial game between a deep learning-based side-channel attacker which seeks to classify a sensitive variable from the power traces recorded during encryption, and a trainable noise generator which seeks to thwart this attack by introducing a minimal amount of noise into the power traces. We demonstrate on synthetic datasets that our method can outperform existing techniques in the presence of common countermeasures such as Boolean masking and trace desynchronization. Results on real datasets are weak because the technique is highly sensitive to hyperparameters and early-stop point, and we lack a holdout dataset with ground truth knowledge of leaking points for model selection. Nonetheless, we believe our work represents an important first step towards deep side-channel leakage localization without relying on strong assumptions about the implementation or the nature of its leakage. An open-source PyTorch implementation of our experiments is provided.</li>
</ul>

<h3>Title: Embedding Watermarks in Diffusion Process for Model Intellectual Property Protection</h3>
<ul>
<li><strong>Authors: </strong>Jijia Yang, Sen Peng, Xiaohua Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22445">https://arxiv.org/abs/2410.22445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22445">https://arxiv.org/pdf/2410.22445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22445]] Embedding Watermarks in Diffusion Process for Model Intellectual Property Protection(https://arxiv.org/abs/2410.22445)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>In practical application, the widespread deployment of diffusion models often necessitates substantial investment in training. As diffusion models find increasingly diverse applications, concerns about potential misuse highlight the imperative for robust intellectual property protection. Current protection strategies either employ backdoor-based methods, integrating a watermark task as a simpler training objective with the main model task, or embedding watermarks directly into the final output samples. However, the former approach is fragile compared to existing backdoor defense techniques, while the latter fundamentally alters the expected output. In this work, we introduce a novel watermarking framework by embedding the watermark into the whole diffusion process, and theoretically ensure that our final output samples contain no additional information. Furthermore, we utilize statistical algorithms to verify the watermark from internally generated model samples without necessitating triggers as conditions. Detailed theoretical analysis and experimental validation demonstrate the effectiveness of our proposed method.</li>
</ul>

<h3>Title: Do Large Language Models Align with Core Mental Health Counseling Competencies?</h3>
<ul>
<li><strong>Authors: </strong>Viet Cuong Nguyen, Mohammad Taher, Dongwan Hong, Vinicius Konkolics Possobom, Vibha Thirunellayi Gopalakrishnan, Ekta Raj, Zihang Li, Heather J. Soled, Michael L. Birnbaum, Srijan Kumar, Munmun De Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22446">https://arxiv.org/abs/2410.22446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22446">https://arxiv.org/pdf/2410.22446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22446]] Do Large Language Models Align with Core Mental Health Counseling Competencies?(https://arxiv.org/abs/2410.22446)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Language Models (LLMs) offers promising potential to alleviate the global scarcity of mental health professionals. However, LLMs' alignment with essential mental health counseling competencies remains understudied. We introduce CounselingBench, a novel NCMHCE-based benchmark evaluating LLMs across five key mental health counseling competencies. Testing 22 general-purpose and medical-finetuned LLMs, we find frontier models exceed minimum thresholds but fall short of expert-level performance, with significant variations: they excel in Intake, Assessment & Diagnosis yet struggle with Core Counseling Attributes and Professional Practice & Ethics. Medical LLMs surprisingly underperform generalist models accuracy-wise, while at the same time producing slightly higher-quality justifications but making more context-related errors. Our findings highlight the complexities of developing AI systems for mental health counseling, particularly for competencies requiring empathy and contextual understanding. We found that frontier LLMs perform at a level exceeding the minimal required level of aptitude for all key mental health counseling competencies, but fall short of expert-level performance, and that current medical LLMs do not significantly improve upon generalist models in mental health counseling competencies. This underscores the critical need for specialized, mental health counseling-specific fine-tuned LLMs that rigorously aligns with core competencies combined with appropriate human supervision before any responsible real-world deployment can be considered.</li>
</ul>

<h3>Title: Addressing Issues with Working Memory in Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Clayton Bromley, Alexander Moore, Amar Saini, Douglas Poland, Carmen Carrano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22451">https://arxiv.org/abs/2410.22451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22451">https://arxiv.org/pdf/2410.22451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22451]] Addressing Issues with Working Memory in Video Object Segmentation(https://arxiv.org/abs/2410.22451)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Contemporary state-of-the-art video object segmentation (VOS) models compare incoming unannotated images to a history of image-mask relations via affinity or cross-attention to predict object masks. We refer to the internal memory state of the initial image-mask pair and past image-masks as a working memory buffer. While the current state of the art models perform very well on clean video data, their reliance on a working memory of previous frames leaves room for error. Affinity-based algorithms include the inductive bias that there is temporal continuity between consecutive frames. To account for inconsistent camera views of the desired object, working memory models need an algorithmic modification that regulates the memory updates and avoid writing irrelevant frames into working memory. A simple algorithmic change is proposed that can be applied to any existing working memory-based VOS model to improve performance on inconsistent views, such as sudden camera cuts, frame interjections, and extreme context changes. The resulting model performances show significant improvement on video data with these frame interjections over the same model without the algorithmic addition. Our contribution is a simple decision function that determines whether working memory should be updated based on the detection of sudden, extreme changes and the assumption that the object is no longer in frame. By implementing algorithmic changes, such as this, we can increase the real-world applicability of current VOS models.</li>
</ul>

<h3>Title: Brain age identification from diffusion MRI synergistically predicts neurodegenerative disease</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Gao, Michael E. Kim, Karthik Ramadass, Praitayini Kanakaraj, Aravind R. Krishnan, Adam M. Saunders, Nancy R. Newlin, Ho Hin Lee, Qi Yang, Warren D. Taylor, Brian D. Boyd, Lori L. Beason-Held, Susan M. Resnick, Lisa L. Barnes, David A. Bennett, Katherine D. Van Schaik, Derek B. Archer, Timothy J. Hohman, Angela L. Jefferson, Ivana Išgum, Daniel Moyer, Yuankai Huo, Kurt G. Schilling, Lianrui Zuo, Shunxing Bao, Nazirah Mohd Khairi, Zhiyuan Li, Christos Davatzikos, Bennett A. Landman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22454">https://arxiv.org/abs/2410.22454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22454">https://arxiv.org/pdf/2410.22454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22454]] Brain age identification from diffusion MRI synergistically predicts neurodegenerative disease(https://arxiv.org/abs/2410.22454)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Estimated brain age from magnetic resonance image (MRI) and its deviation from chronological age can provide early insights into potential neurodegenerative diseases, supporting early detection and implementation of prevention strategies. Diffusion MRI (dMRI), a widely used modality for brain age estimation, presents an opportunity to build an earlier biomarker for neurodegenerative disease prediction because it captures subtle microstructural changes that precede more perceptible macrostructural changes. However, the coexistence of macro- and micro-structural information in dMRI raises the question of whether current dMRI-based brain age estimation models are leveraging the intended microstructural information or if they inadvertently rely on the macrostructural information. To develop a microstructure-specific brain age, we propose a method for brain age identification from dMRI that minimizes the model's use of macrostructural information by non-rigidly registering all images to a standard template. Imaging data from 13,398 participants across 12 datasets were used for the training and evaluation. We compare our brain age models, trained with and without macrostructural information minimized, with an architecturally similar T1-weighted (T1w) MRI-based brain age model and two state-of-the-art T1w MRI-based brain age models that primarily use macrostructural information. We observe difference between our dMRI-based brain age and T1w MRI-based brain age across stages of neurodegeneration, with dMRI-based brain age being older than T1w MRI-based brain age in participants transitioning from cognitively normal (CN) to mild cognitive impairment (MCI), but younger in participants already diagnosed with Alzheimer's disease (AD). Approximately 4 years before MCI diagnosis, dMRI-based brain age yields better performance than T1w MRI-based brain ages in predicting transition from CN to MCI.</li>
</ul>

<h3>Title: Image2Struct: Benchmarking Structure Extraction for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Josselin Somerville Roberts, Tony Lee, Chi Heem Wong, Michihiro Yasunaga, Yifan Mai, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22456">https://arxiv.org/abs/2410.22456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22456">https://arxiv.org/pdf/2410.22456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22456]] Image2Struct: Benchmarking Structure Extraction for Vision-Language Models(https://arxiv.org/abs/2410.22456)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images. Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data. In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot). The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score. This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures. We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention. We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs. Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty. For transparency, we release the full results at this https URL.</li>
</ul>

<h3>Title: Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Gyusam Chang, Jiwon Lee, Donghyun Kim, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sujin Jang, Sangpil Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22461">https://arxiv.org/abs/2410.22461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22461">https://arxiv.org/pdf/2410.22461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22461]] Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection(https://arxiv.org/abs/2410.22461)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks. However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (\ie, direct transfer) due to the inevitable geometric misalignment between the source and target domains. In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors. In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks. We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes. Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (\ie, 1$\%$ and 5$\%)$, while preserving well-defined source knowledge for training efficiency. Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations. We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.</li>
</ul>

<h3>Title: Learning Identifiable Factorized Causal Representations of Cellular Responses</h3>
<ul>
<li><strong>Authors: </strong>Haiyi Mao, Romain Lopez, Kai Liu, Jan-Christian Huetter, David Richmond, Panayiotis Benos, Lin Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22472">https://arxiv.org/abs/2410.22472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22472">https://arxiv.org/pdf/2410.22472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22472]] Learning Identifiable Factorized Causal Representations of Cellular Responses(https://arxiv.org/abs/2410.22472)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The study of cells and their responses to genetic or chemical perturbations promises to accelerate the discovery of therapeutic targets. However, designing adequate and insightful models for such data is difficult because the response of a cell to perturbations essentially depends on its biological context (e.g., genetic background or cell type). For example, while discovering therapeutic targets, one may want to enrich for drugs that specifically target a certain cell type. This challenge emphasizes the need for methods that explicitly take into account potential interactions between drugs and contexts. Towards this goal, we propose a novel Factorized Causal Representation (FCR) learning method that reveals causal structure in single-cell perturbation data from several cell lines. Based on the framework of identifiable deep generative models, FCR learns multiple cellular representations that are disentangled, comprised of covariate-specific ($\mathbf{z}_x$), treatment-specific ($\mathbf{z}_{t}$), and interaction-specific ($\mathbf{z}_{tx}$) blocks. Based on recent advances in non-linear ICA theory, we prove the component-wise identifiability of $\mathbf{z}_{tx}$ and block-wise identifiability of $\mathbf{z}_t$ and $\mathbf{z}_x$. Then, we present our implementation of FCR, and empirically demonstrate that it outperforms state-of-the-art baselines in various tasks across four single-cell datasets.</li>
</ul>

<h3>Title: A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents</h3>
<ul>
<li><strong>Authors: </strong>Ankan Mullick, Sombit Bose, Abhilash Nandy, Gajula Sai Chaitanya, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22476">https://arxiv.org/abs/2410.22476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22476">https://arxiv.org/pdf/2410.22476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22476]] A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents(https://arxiv.org/abs/2410.22476)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with a single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is a notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing a multi-lingual multi-label intent dataset. We introduce a novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose a pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network-based system over baseline approaches in terms of accuracy and F1-score across various datasets.</li>
</ul>

<h3>Title: Scaling LLM Inference with Optimized Sample Compute Allocation</h3>
<ul>
<li><strong>Authors: </strong>Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22480">https://arxiv.org/abs/2410.22480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22480">https://arxiv.org/pdf/2410.22480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22480]] Scaling LLM Inference with Optimized Sample Compute Allocation(https://arxiv.org/abs/2410.22480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sampling is a basic operation in many inference-time algorithms of large language models (LLMs). To scale up inference efficiently with a limited compute, it is crucial to find an optimal allocation for sample compute budgets: Which sampling configurations (model, temperature, language, etc.) do we use? How many samples do we generate in each configuration? We formulate these choices as a learning problem and propose OSCA, an algorithm that Optimizes Sample Compute Allocation by finding an optimal mix of different inference configurations. Our experiments show that with our learned mixed allocation, we can achieve accuracy better than the best single configuration with 128x less compute on code generation and 25x less compute on 4 reasoning tasks. OSCA is also shown to be effective in agentic workflows beyond single-turn tasks, achieving a better accuracy on SWE-Bench with 3x less compute than the default configuration. Our code and generations are released at this https URL.</li>
</ul>

<h3>Title: Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22489">https://arxiv.org/abs/2410.22489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22489">https://arxiv.org/pdf/2410.22489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22489]] Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation(https://arxiv.org/abs/2410.22489)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code is available at this https URL .</li>
</ul>

<h3>Title: The PV-ALE Dataset: Enhancing Apple Leaf Disease Classification Through Transfer Learning with Convolutional Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Joseph Damilola Akinyemi, Kolawole John Adebayo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22490">https://arxiv.org/abs/2410.22490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22490">https://arxiv.org/pdf/2410.22490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22490]] The PV-ALE Dataset: Enhancing Apple Leaf Disease Classification Through Transfer Learning with Convolutional Neural Networks(https://arxiv.org/abs/2410.22490)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>As the global food security landscape continues to evolve, the need for accurate and reliable crop disease diagnosis has never been more pressing. To address global food security concerns, we extend the widely used PlantVillage dataset with additional apple leaf disease classes, enhancing diversity and complexity. Experimental evaluations on both original and extended datasets reveal that existing models struggle with the new additions, highlighting the need for more robust and generalizable computer vision models. Test F1 scores of 99.63% and 97.87% were obtained on the original and extended datasets, respectively. Our study provides a more challenging and diverse benchmark, paving the way for the development of accurate and reliable models for identifying apple leaf diseases under varying imaging conditions. The expanded dataset is available at this https URL enabling future research to build upon our findings.</li>
</ul>

<h3>Title: Unlocking Point Processes through Point Set Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Lüdke, Enric Rabasseda Raventós, Marcel Kollovieh, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22493">https://arxiv.org/abs/2410.22493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22493">https://arxiv.org/pdf/2410.22493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22493]] Unlocking Point Processes through Point Set Diffusion(https://arxiv.org/abs/2410.22493)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Point processes model the distribution of random point sets in mathematical spaces, such as spatial and temporal domains, with applications in fields like seismology, neuroscience, and economics. Existing statistical and machine learning models for point processes are predominantly constrained by their reliance on the characteristic intensity function, introducing an inherent trade-off between efficiency and flexibility. In this paper, we introduce Point Set Diffusion, a diffusion-based latent variable model that can represent arbitrary point processes on general metric spaces without relying on the intensity function. By directly learning to stochastically interpolate between noise and data point sets, our approach enables efficient, parallel sampling and flexible generation for complex conditional tasks defined on the metric space. Experiments on synthetic and real-world datasets demonstrate that Point Set Diffusion achieves state-of-the-art performance in unconditional and conditional generation of spatial and spatiotemporal point processes while providing up to orders of magnitude faster sampling than autoregressive baselines.</li>
</ul>

<h3>Title: Anticipating Future with Large Language Model for Simultaneous Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Siqi Ouyang, Oleksii Hrinchuk, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Lei Li, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22499">https://arxiv.org/abs/2410.22499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22499">https://arxiv.org/pdf/2410.22499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22499]] Anticipating Future with Large Language Model for Simultaneous Machine Translation(https://arxiv.org/abs/2410.22499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Simultaneous machine translation (SMT) takes streaming input utterances and incrementally produces target text. Existing SMT methods only use the partial utterance that has already arrived at the input and the generated hypothesis. Motivated by human interpreters' technique to forecast future words before hearing them, we propose $\textbf{T}$ranslation by $\textbf{A}$nticipating $\textbf{F}$uture (TAF), a method to improve translation quality while retraining low latency. Its core idea is to use a large language model (LLM) to predict future source words and opportunistically translate without introducing too much risk. We evaluate our TAF and multiple baselines of SMT on four language directions. Experiments show that TAF achieves the best translation quality-latency trade-off and outperforms the baselines by up to 5 BLEU points at the same latency (three words).</li>
</ul>

<h3>Title: Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rishabh Adiga, Besmira Nushi, Varun Chandrasekaran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22517">https://arxiv.org/abs/2410.22517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22517">https://arxiv.org/pdf/2410.22517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22517]] Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models(https://arxiv.org/abs/2410.22517)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explore the internal mechanisms of how bias emerges in large language models (LLMs) when provided with ambiguous comparative prompts: inputs that compare or enforce choosing between two or more entities without providing clear context for preference. Most approaches for bias mitigation focus on either post-hoc analysis or data augmentation. However, these are transient solutions, without addressing the root cause: the model itself. Numerous prior works show the influence of the attention module towards steering generations. We believe that analyzing attention is also crucial for understanding bias, as it provides insight into how the LLM distributes its focus across different entities and how this contributes to biased decisions. To this end, we first introduce a metric to quantify the LLM's preference for one entity over another. We then propose $\texttt{ATLAS}$ (Attention-based Targeted Layer Analysis and Scaling), a technique to localize bias to specific layers of the LLM by analyzing attention scores and then reduce bias by scaling attention in these biased layers. To evaluate our method, we conduct experiments across 3 datasets (BBQ, Crows-Pairs, and WinoGender) using $\texttt{GPT-2 XL}$ (1.5B), $\texttt{GPT-J}$ (6B), $\texttt{LLaMA-2}$ (7B) and $\texttt{LLaMA-3}$ (8B). Our experiments demonstrate that bias is concentrated in the later layers, typically around the last third. We also show how $\texttt{ATLAS}$ effectively mitigates bias through targeted interventions without compromising downstream performance and an average increase of only 0.82% in perplexity when the intervention is applied. We see an average improvement of 0.28 points in the bias score across all the datasets.</li>
</ul>

<h3>Title: FairSkin: Fair Diffusion for Skin Disease Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruichen Zhang, Yuguang Yao, Zhen Tan, Zhiming Li, Pan Wang, Jingtong Hu, Sijia Liu, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22551">https://arxiv.org/abs/2410.22551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22551">https://arxiv.org/pdf/2410.22551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22551]] FairSkin: Fair Diffusion for Skin Disease Image Generation(https://arxiv.org/abs/2410.22551)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion</a></li>
<li><strong>Abstract: </strong>Image generation is a prevailing technique for clinical data augmentation for advancing diagnostic accuracy and reducing healthcare disparities. Diffusion Model (DM) has become a leading method in generating synthetic medical images, but it suffers from a critical twofold bias: (1) The quality of images generated for Caucasian individuals is significantly higher, as measured by the Frechet Inception Distance (FID). (2) The ability of the downstream-task learner to learn critical features from disease images varies across different skin tones. These biases pose significant risks, particularly in skin disease detection, where underrepresentation of certain skin tones can lead to misdiagnosis or neglect of specific conditions. To address these challenges, we propose FairSkin, a novel DM framework that mitigates these biases through a three-level resampling mechanism, ensuring fairer representation across racial and disease categories. Our approach significantly improves the diversity and quality of generated images, contributing to more equitable skin disease detection in clinical settings.</li>
</ul>

<h3>Title: Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Jaekyeom Kim, Dong-Ki Kim, Lajanugen Logeswaran, Sungryull Sohn, Honglak Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22552">https://arxiv.org/abs/2410.22552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22552">https://arxiv.org/pdf/2410.22552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22552]] Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents(https://arxiv.org/abs/2410.22552)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Auto-Intent, a method to adapt a pre-trained large language model (LLM) as an agent for a target domain without direct fine-tuning, where we empirically focus on web navigation tasks. Our approach first discovers the underlying intents from target domain demonstrations unsupervisedly, in a highly compact form (up to three words). With the extracted intents, we train our intent predictor to predict the next intent given the agent's past observations and actions. In particular, we propose a self-exploration approach where top-k probable intent predictions are provided as a hint to the pre-trained LLM agent, which leads to enhanced decision-making capabilities. Auto-Intent substantially improves the performance of GPT-{3.5, 4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation benchmarks from Mind2Web and online navigation tasks from WebArena with its cross-benchmark generalization from Mind2Web.</li>
</ul>

<h3>Title: Remote Sensing for Weed Detection and Control</h3>
<ul>
<li><strong>Authors: </strong>Ishita Bansal, Peder Olsen, Roberto Estevão</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22554">https://arxiv.org/abs/2410.22554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22554">https://arxiv.org/pdf/2410.22554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22554]] Remote Sensing for Weed Detection and Control(https://arxiv.org/abs/2410.22554)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Italian ryegrass is a grass weed commonly found in winter wheat fields that are competitive with winter wheat for moisture and nutrients. Ryegrass can cause substantial reductions in yield and grain quality if not properly controlled with the use of herbicides. To control the cost and environmental impact we detect weeds in drone and satellite imagery. Satellite imagery is too coarse to be used for precision spraying, but can aid in planning drone flights and treatments. Drone images on the other hand have sufficiently good resolution for precision spraying. However, ryegrass is hard to distinguish from the crop and annotation requires expert knowledge. We used the Python segmentation models library to test more than 600 different neural network architectures for weed segmentation in drone images and we map accuracy versus the cost of the model prediction for these. Our best system applies herbicides to over 99% of the weeds while only spraying an area 30% larger than the annotated weed area. These models yield large savings if the weed covers a small part of the field.</li>
</ul>

<h3>Title: Lost and Found in Speculation: Hybrid Speculative Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Mohamadreza Rostami, Shaza Zeitouni, Rahul Kande, Chen Chen, Pouya Mahmoody, Jeyavijayan (JV)Rajendran, Ahmad-Reza Sadeghi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22555">https://arxiv.org/abs/2410.22555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22555">https://arxiv.org/pdf/2410.22555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22555]] Lost and Found in Speculation: Hybrid Speculative Vulnerability Detection(https://arxiv.org/abs/2410.22555)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Microarchitectural attacks represent a challenging and persistent threat to modern processors, exploiting inherent design vulnerabilities in processors to leak sensitive information or compromise systems. Of particular concern is the susceptibility of Speculative Execution, a fundamental part of performance enhancement, to such attacks. We introduce Specure, a novel pre-silicon verification method composing hardware fuzzing with Information Flow Tracking (IFT) to address speculative execution leakages. Integrating IFT enables two significant and non-trivial enhancements over the existing fuzzing approaches: i) automatic detection of microarchitectural information leakages vulnerabilities without golden model and ii) a novel Leakage Path coverage metric for efficient vulnerability detection. Specure identifies previously overlooked speculative execution vulnerabilities on the RISC-V BOOM processor and explores the vulnerability search space 6.45x faster than existing fuzzing techniques. Moreover, Specure detected known vulnerabilities 20x faster.</li>
</ul>

<h3>Title: Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components</h3>
<ul>
<li><strong>Authors: </strong>Carl Allen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22559">https://arxiv.org/abs/2410.22559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22559">https://arxiv.org/pdf/2410.22559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22559]] Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components(https://arxiv.org/abs/2410.22559)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Disentanglement, or identifying salient statistically independent factors of the data, is of interest in many areas of machine learning and statistics, with relevance to synthetic data generation with controlled properties, robust classification of features, parsimonious encoding, and a greater understanding of the generative process underlying the data. Disentanglement arises in several generative paradigms, including Variational Autoencoders (VAEs), Generative Adversarial Networks and diffusion models. Particular progress has recently been made in understanding disentanglement in VAEs, where the choice of diagonal posterior covariance matrices is shown to promote mutual orthogonality between columns of the decoder's Jacobian. We continue this thread to show how this linear independence translates to statistical independence, completing the chain in understanding how the VAE's objective identifies independent components of, or disentangles, the data.</li>
</ul>

<h3>Title: Vertical Federated Learning with Missing Features During Training and Inference</h3>
<ul>
<li><strong>Authors: </strong>Pedro Valdeira, Shiqiang Wang, Yuejie Chi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.DS, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22564">https://arxiv.org/abs/2410.22564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22564">https://arxiv.org/pdf/2410.22564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22564]] Vertical Federated Learning with Missing Features During Training and Inference(https://arxiv.org/abs/2410.22564)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Vertical federated learning trains models from feature-partitioned datasets across multiple clients, who collaborate without sharing their local data. Standard approaches assume that all feature partitions are available during both training and inference. Yet, in practice, this assumption rarely holds, as for many samples only a subset of the clients observe their partition. However, not utilizing incomplete samples during training harms generalization, and not supporting them during inference limits the utility of the model. Moreover, if any client leaves the federation after training, its partition becomes unavailable, rendering the learned model unusable. Missing feature blocks are therefore a key challenge limiting the applicability of vertical federated learning in real-world scenarios. To address this, we propose LASER-VFL, a vertical federated learning method for efficient training and inference of split neural network-based models that is capable of handling arbitrary sets of partitions. Our approach is simple yet effective, relying on the strategic sharing of model parameters and on task-sampling to train a family of predictors. We show that LASER-VFL achieves a $\mathcal{O}({1}/{\sqrt{T}})$ convergence rate for nonconvex objectives in general, $\mathcal{O}({1}/{T})$ for sufficiently large batch sizes, and linear convergence under the Polyak-Łojasiewicz inequality. Numerical experiments show improved performance of LASER-VFL over the baselines. Remarkably, this is the case even in the absence of missing features. For example, for CIFAR-100, we see an improvement in accuracy of $21.4\%$ when each of four feature blocks is observed with a probability of 0.5 and of $12.2\%$ when all features are observed.</li>
</ul>

<h3>Title: Flow Matching for Posterior Inference with Simulator Feedback</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Holzschuh, Nils Thuerey</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22573">https://arxiv.org/abs/2410.22573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22573">https://arxiv.org/pdf/2410.22573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22573]] Flow Matching for Posterior Inference with Simulator Feedback(https://arxiv.org/abs/2410.22573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative modeling is a powerful tool for solving inverse problems in physical sciences that can be used for sampling and likelihood evaluation with much lower inference times than traditional methods. We propose to refine flows with additional control signals based on a simulator. Control signals can include gradients and a problem-specific cost function if the simulator is differentiable, or they can be fully learned from the simulator output. In our proposed method, we pretrain the flow network and include feedback from the simulator exclusively for finetuning, therefore requiring only a small amount of additional parameters and compute. We motivate our design choices on several benchmark problems for simulation-based inference and evaluate flow matching with simulator feedback against classical MCMC methods for modeling strong gravitational lens systems, a challenging inverse problem in astronomy. We demonstrate that including feedback from the simulator improves the accuracy by $53\%$, making it competitive with traditional techniques while being up to $67$x faster for inference.</li>
</ul>

<h3>Title: BENCHAGENTS: Automated Benchmark Creation with Agent Interaction</h3>
<ul>
<li><strong>Authors: </strong>Natasha Butt, Varun Chandrasekaran, Neel Joshi, Besmira Nushi, Vidhisha Balachandran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22584">https://arxiv.org/abs/2410.22584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22584">https://arxiv.org/pdf/2410.22584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22584]] BENCHAGENTS: Automated Benchmark Creation with Agent Interaction(https://arxiv.org/abs/2410.22584)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Evaluations are limited by benchmark availability. As models evolve, there is a need to create benchmarks that can measure progress on new generative capabilities. However, creating new benchmarks through human annotations is slow and expensive, restricting comprehensive evaluations for any capability. We introduce BENCHAGENTS, a framework that methodically leverages large language models (LLMs) to automate benchmark creation for complex capabilities while inherently ensuring data and metric quality. BENCHAGENTS decomposes the benchmark creation process into planning, generation, data verification, and evaluation, each of which is executed by an LLM agent. These agents interact with each other and utilize human-in-the-loop feedback from benchmark developers to explicitly improve and flexibly control data diversity and quality. We use BENCHAGENTS to create benchmarks to evaluate capabilities related to planning and constraint satisfaction during text generation. We then use these benchmarks to study seven state-of-the-art models and extract new insights on common failure modes and model differences.</li>
</ul>

<h3>Title: Toxicity of the Commons: Curating Open-Source Pre-Training Data</h3>
<ul>
<li><strong>Authors: </strong>Catherine Arnett, Eliot Jones, Ivan P. Yamshchikov, Pierre-Carl Langlais</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22587">https://arxiv.org/abs/2410.22587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22587">https://arxiv.org/pdf/2410.22587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22587]] Toxicity of the Commons: Curating Open-Source Pre-Training Data(https://arxiv.org/abs/2410.22587)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, there researchers are working to make language models safer. We propose a data curation pipeline to reduce harmful outputs by models trained on public domain data. There are unique challenges to working with public domain data, as these sources differ from web text in both form and content. Many sources are historical documents and are the result of Optical Character Recognition (OCR). Consequently, current state-of-the-art approaches to toxicity filtering are often infeasible or inappropriate for open data models. In this paper, we introduce a new fully open-source pipeline for open-data toxicity filtering. Our contributions are threefold. We create a custom training dataset, ToxicCommons, which is composed of texts which have been classified across five different dimensions (racial/origin-based, gender/sex-based, religious, ability-based discrimination, and violence). We use this dataset to train a custom classifier, Celadon, that can be used to detect toxic content in open data more efficiently at a larger scale. Finally, we describe the balanced approach to content filtration that optimizes safety filtering with respect to the filtered data available for training.</li>
</ul>

<h3>Title: FGCE: Feasible Group Counterfactual Explanations for Auditing Fairness</h3>
<ul>
<li><strong>Authors: </strong>Christos Fragkathoulas, Vasiliki Papanikou, Evaggelia Pitoura, Evimaria Terzi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22591">https://arxiv.org/abs/2410.22591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22591">https://arxiv.org/pdf/2410.22591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22591]] FGCE: Feasible Group Counterfactual Explanations for Auditing Fairness(https://arxiv.org/abs/2410.22591)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This paper introduces the first graph-based framework for generating group counterfactual explanations to audit model fairness, a crucial aspect of trustworthy machine learning. Counterfactual explanations are instrumental in understanding and mitigating unfairness by revealing how inputs should change to achieve a desired outcome. Our framework, named Feasible Group Counterfactual Explanations (FGCEs), captures real-world feasibility constraints and constructs subgroups with similar counterfactuals, setting it apart from existing methods. It also addresses key trade-offs in counterfactual generation, including the balance between the number of counterfactuals, their associated costs, and the breadth of coverage achieved. To evaluate these trade-offs and assess fairness, we propose measures tailored to group counterfactual generation. Our experimental results on benchmark datasets demonstrate the effectiveness of our approach in managing feasibility constraints and trade-offs, as well as the potential of our proposed metrics in identifying and quantifying fairness issues.</li>
</ul>

<h3>Title: GRADE: Quantifying Sample Diversity in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22592">https://arxiv.org/abs/2410.22592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22592">https://arxiv.org/pdf/2410.22592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22592]] GRADE: Quantifying Sample Diversity in Text-to-Image Models(https://arxiv.org/abs/2410.22592)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models are remarkable at generating realistic images based on textual descriptions. However, textual prompts are inherently underspecified: they do not specify all possible attributes of the required image. This raises two key questions: Do T2I models generate diverse outputs on underspecified prompts? How can we automatically measure diversity? We propose GRADE: Granular Attribute Diversity Evaluation, an automatic method for quantifying sample diversity. GRADE leverages the world knowledge embedded in large language models and visual question-answering systems to identify relevant concept-specific axes of diversity (e.g., ``shape'' and ``color'' for the concept ``cookie''). It then estimates frequency distributions of concepts and their attributes and quantifies diversity using (normalized) entropy. GRADE achieves over 90% human agreement while exhibiting weak correlation to commonly used diversity metrics. We use GRADE to measure the overall diversity of 12 T2I models using 400 concept-attribute pairs, revealing that all models display limited variation. Further, we find that these models often exhibit default behaviors, a phenomenon where the model consistently generates concepts with the same attributes (e.g., 98% of the cookies are round). Finally, we demonstrate that a key reason for low diversity is due to underspecified captions in training data. Our work proposes a modern, semantically-driven approach to measure sample diversity and highlights the stunning homogeneity in outputs by T2I models.</li>
</ul>

<h3>Title: Are Large-Language Models Graph Algorithmic Reasoners?</h3>
<ul>
<li><strong>Authors: </strong>Alexander K Taylor, Anthony Cuturrufo, Vishal Yathish, Mingyu Derek Ma, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22597">https://arxiv.org/abs/2410.22597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22597">https://arxiv.org/pdf/2410.22597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22597]] Are Large-Language Models Graph Algorithmic Reasoners?(https://arxiv.org/abs/2410.22597)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We seek to address a core challenge facing current Large Language Models (LLMs). LLMs have demonstrated superior performance in many tasks, yet continue to struggle with reasoning problems on explicit graphs that require multiple steps. To address this gap, we introduce a novel benchmark designed to evaluate LLM performance on classical algorithmic reasoning tasks on explicit graphs. Our benchmark encompasses five fundamental algorithms: Breadth-First Search (BFS) and Depth-First Search (DFS) for connectivity, Dijkstra's algorithm and Floyd-Warshall algorithm for all nodes shortest path, and Prim's Minimum Spanning Tree (MST-Prim's) algorithm. Through extensive experimentation, we assess the capabilities of state-of-the-art LLMs in executing these algorithms step-by-step and systematically evaluate their performance at each stage. Our findings highlight the persistent challenges LLMs face in this domain and underscore the necessity for advanced prompting techniques and algorithmic instruction to enhance their graph reasoning abilities. This work presents MAGMA, the first comprehensive benchmark focused on LLMs completing classical graph algorithms, and provides a critical step toward understanding and improving their structured problem-solving skills.</li>
</ul>

<h3>Title: A Cascade Approach for APT Campaign Attribution in System Event Logs: Technique Hunting and Subgraph Matching</h3>
<ul>
<li><strong>Authors: </strong>Yi-Ting Huang, Ying-Ren Guo, Guo-Wei Wong, Meng Chang Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22602">https://arxiv.org/abs/2410.22602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22602">https://arxiv.org/pdf/2410.22602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22602]] A Cascade Approach for APT Campaign Attribution in System Event Logs: Technique Hunting and Subgraph Matching(https://arxiv.org/abs/2410.22602)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>As Advanced Persistent Threats (APTs) grow increasingly sophisticated, the demand for effective detection methods has intensified. This study addresses the challenge of identifying APT campaign attacks through system event logs. A cascading approach, name SFM, combines Technique hunting and APT campaign attribution. Our approach assumes that real-world system event logs contain a vast majority of normal events interspersed with few suspiciously malicious ones and that these logs are annotated with Techniques of MITRE ATT&CK framework for attack pattern recognition. Then, we attribute APT campaign attacks by aligning detected Techniques with known attack sequences to determine the most likely APT campaign. Evaluations on five real-world APT campaigns indicate that the proposed approach demonstrates reliable performance.</li>
</ul>

<h3>Title: FISC: Federated Domain Generalization via Interpolative Style Transfer and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Dung Thuy Nguyen, Taylor T. Johnson, Kevin Leach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22622">https://arxiv.org/abs/2410.22622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22622">https://arxiv.org/pdf/2410.22622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22622]] FISC: Federated Domain Generalization via Interpolative Style Transfer and Contrastive Learning(https://arxiv.org/abs/2410.22622)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) shows promise in preserving privacy and enabling collaborative learning. However, most current solutions focus on private data collected from a single domain. A significant challenge arises when client data comes from diverse domains (i.e., domain shift), leading to poor performance on unseen domains. Existing Federated Domain Generalization approaches address this problem but assume each client holds data for an entire domain, limiting their practicality in real-world scenarios with domain-based heterogeneity and client sampling. To overcome this, we introduce FISC, a novel FL domain generalization paradigm that handles more complex domain distributions across clients. FISC enables learning across domains by extracting an interpolative style from local styles and employing contrastive learning. This strategy gives clients multi-domain representations and unbiased convergent targets. Empirical results on multiple datasets, including PACS, Office-Home, and IWildCam, show FISC outperforms state-of-the-art (SOTA) methods. Our method achieves accuracy improvements ranging from 3.64% to 57.22% on unseen domains. Our code is available at this https URL.</li>
</ul>

<h3>Title: PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Ryozo Masukawa, Sanggeon Yun, Yoshiki Yamaguchi, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22623">https://arxiv.org/abs/2410.22623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22623">https://arxiv.org/pdf/2410.22623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22623]] PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation(https://arxiv.org/abs/2410.22623)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Video crime detection is a significant application of computer vision and artificial intelligence. However, existing datasets primarily focus on detecting severe crimes by analyzing entire video clips, often neglecting the precursor activities (i.e., privacy violations) that could potentially prevent these crimes. To address this limitation, we present PV-VTT (Privacy Violation Video To Text), a unique multimodal dataset aimed at identifying privacy violations. PV-VTT provides detailed annotations for both video and text in scenarios. To ensure the privacy of individuals in the videos, we only provide video feature vectors, avoiding the release of any raw video data. This privacy-focused approach allows researchers to use the dataset while protecting participant confidentiality. Recognizing that privacy violations are often ambiguous and context-dependent, we propose a Graph Neural Network (GNN)-based video description model. Our model generates a GNN-based prompt with image for Large Language Model (LLM), which deliver cost-effective and high-quality video descriptions. By leveraging a single video frame along with relevant text, our method reduces the number of input tokens required, maintaining descriptive quality while optimizing LLM API-usage. Extensive experiments validate the effectiveness and interpretability of our approach in video description tasks and flexibility of our PV-VTT dataset.</li>
</ul>

<h3>Title: CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Gong, Zhixiang Wei, Di Wang, Xianzheng Ma, Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Naoto Yokoya, Jing Zhang, Bo Du, Liangpei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22629">https://arxiv.org/abs/2410.22629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22629">https://arxiv.org/pdf/2410.22629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22629]] CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2410.22629)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The field of Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. Despite the substantial domain gaps in RS images that are characterized by variabilities such as location, wavelength, and sensor type, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies targeting the RSDG issue, especially for semantic segmentation tasks, where existing models are developed for specific unknown domains, struggling with issues of underfitting on other unknown scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 28 cross-domain settings across various regions, spectral bands, platforms, and climates, providing a comprehensive framework for testing the generalizability of future RSDG models. Extensive experiments on this benchmark demonstrate the superiority of CrossEarth over existing state-of-the-art methods.</li>
</ul>

<h3>Title: Consistency Diffusion Bridge Models</h3>
<ul>
<li><strong>Authors: </strong>Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22637">https://arxiv.org/abs/2410.22637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22637">https://arxiv.org/pdf/2410.22637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22637]] Consistency Diffusion Bridge Models(https://arxiv.org/abs/2410.22637)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample $4\times$ to $50\times$ faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from $64 \times 64$ to $256 \times 256$, as well as supporting downstream tasks such as semantic interpolation in the data space.</li>
</ul>

<h3>Title: Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruiyu Xiao, Lei Wu, Yuhang Gou, Weinan Zhang, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22642">https://arxiv.org/abs/2410.22642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22642">https://arxiv.org/pdf/2410.22642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22642]] Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation(https://arxiv.org/abs/2410.22642)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Argumentative essay generation (AEG) aims to generate complete texts on specific controversial topics or debates. Although current AEG methods can generate individual opinions, they often overlook the high-level connections between these opinions. This often leads to the generated results being mired in logical confusion, unable to proof their own arguments effectively. The generated essay may present evidence that contradicts the claims or they may fail to assemble the claims into logical flow. In this paper, we present a unified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for AEG with a focus on logical enhancement. Specifically, we first construct pseudo-labels for logical information,claims and grounds, using a large language model. We then propose a tree planning approach that introduces proof principles and ensures logical consistency. Extensive experimental results show that, benefiting from proof principle guidance, PESA generates argumentative essays with better logical validity and persuasiveness than strong baseline models.</li>
</ul>

<h3>Title: WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Aobo Liang, Yan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22649">https://arxiv.org/abs/2410.22649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22649">https://arxiv.org/pdf/2410.22649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22649]] WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series Forecasting(https://arxiv.org/abs/2410.22649)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent years, Transformer-based models (Transformers) have achieved significant success in multivariate time series forecasting (MTSF). However, previous works focus on extracting features either from the time domain or the frequency domain, which inadequately captures the trends and periodic characteristics. To address this issue, we propose a wavelet learning framework to model complex temporal dependencies of the time series data. The wavelet domain integrates both time and frequency information, allowing for the analysis of local characteristics of signals at different scales. Additionally, the Softmax self-attention mechanism used by Transformers has quadratic complexity, which leads to excessive computational costs when capturing long-term dependencies. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary position embeddings to inject relative positional information to sequence tokens and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix, offering linear complexity. We further propose WaveRoRA, which leverages RoRA to capture inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs.</li>
</ul>

<h3>Title: FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation</h3>
<ul>
<li><strong>Authors: </strong>Yuechun Gu, Jiajie He, Keke Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22651">https://arxiv.org/abs/2410.22651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22651">https://arxiv.org/pdf/2410.22651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22651]] FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation(https://arxiv.org/abs/2410.22651)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Training data privacy has been a top concern in AI modeling. While methods like differentiated private learning allow data contributors to quantify acceptable privacy loss, model utility is often significantly damaged. In practice, controlled data access remains a mainstream method for protecting data privacy in many industrial and research environments. In controlled data access, authorized model builders work in a restricted environment to access sensitive data, which can fully preserve data utility with reduced risk of data leak. However, unlike differential privacy, there is no quantitative measure for individual data contributors to tell their privacy risk before participating in a machine learning task. We developed the demo prototype FT-PrivacyScore to show that it's possible to efficiently and quantitatively estimate the privacy risk of participating in a model fine-tuning task. The demo source code will be available at \url{this https URL}.</li>
</ul>

<h3>Title: FlowDCN: Exploring DCN-like Architectures for Fast Image Generation with Arbitrary Resolution</h3>
<ul>
<li><strong>Authors: </strong>Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22655">https://arxiv.org/abs/2410.22655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22655">https://arxiv.org/pdf/2410.22655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22655]] FlowDCN: Exploring DCN-like Architectures for Fast Image Generation with Arbitrary Resolution(https://arxiv.org/abs/2410.22655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Arbitrary-resolution image generation still remains a challenging task in AIGC, as it requires handling varying resolutions and aspect ratios while maintaining high visual quality. Existing transformer-based diffusion methods suffer from quadratic computation cost and limited resolution extrapolation capabilities, making them less effective for this task. In this paper, we propose FlowDCN, a purely convolution-based generative model with linear time and memory complexity, that can efficiently generate high-quality images at arbitrary resolutions. Equipped with a new design of learnable group-wise deformable convolution block, our FlowDCN yields higher flexibility and capability to handle different resolutions with a single model. FlowDCN achieves the state-of-the-art 4.30 sFID on $256\times256$ ImageNet Benchmark and comparable resolution extrapolation results, surpassing transformer-based counterparts in terms of convergence speed (only $\frac{1}{5}$ images), visual quality, parameters ($8\%$ reduction) and FLOPs ($20\%$ reduction). We believe FlowDCN offers a promising solution to scalable and flexible image synthesis.</li>
</ul>

<h3>Title: Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Daehee Lee, Minjong Yoo, Woo Kyung Kim, Wonje Choi, Honguk Woo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22658">https://arxiv.org/abs/2410.22658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22658">https://arxiv.org/pdf/2410.22658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22658]] Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation(https://arxiv.org/abs/2410.22658)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in Franka-Kitchen and Meta-World demonstrate robust performance of IsCiL in both task adaptation and sample-efficiency. We also show a simple extension of IsCiL for task unlearning scenarios.</li>
</ul>

<h3>Title: Linguistics Theory Meets LLM: Code-Switched Text Generation via Equivalence Constrained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Garry Kuwanto, Chaitanya Agarwal, Genta Indra Winata, Derry Tanti Wijaya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22660">https://arxiv.org/abs/2410.22660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22660">https://arxiv.org/pdf/2410.22660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22660]] Linguistics Theory Meets LLM: Code-Switched Text Generation via Equivalence Constrained Large Language Models(https://arxiv.org/abs/2410.22660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Code-switching, the phenomenon of alternating between two or more languages in a single conversation, presents unique challenges for Natural Language Processing (NLP). Most existing research focuses on either syntactic constraints or neural generation, with few efforts to integrate linguistic theory with large language models (LLMs) for generating natural code-switched text. In this paper, we introduce EZSwitch, a novel framework that combines Equivalence Constraint Theory (ECT) with LLMs to produce linguistically valid and fluent code-switched text. We evaluate our method using both human judgments and automatic metrics, demonstrating a significant improvement in the quality of generated code-switching sentences compared to baseline LLMs. To address the lack of suitable evaluation metrics, we conduct a comprehensive correlation study of various automatic metrics against human scores, revealing that current metrics often fail to capture the nuanced fluency of code-switched text. Additionally, we create CSPref, a human preference dataset based on human ratings and analyze model performance across ``hard`` and ``easy`` examples. Our findings indicate that incorporating linguistic constraints into LLMs leads to more robust and human-aligned generation, paving the way for scalable code-switching text generation across diverse language pairs.</li>
</ul>

<h3>Title: Calibrating Practical Privacy Risks for Differentially Private Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuechun Gu, Keke Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22673">https://arxiv.org/abs/2410.22673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22673">https://arxiv.org/pdf/2410.22673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22673]] Calibrating Practical Privacy Risks for Differentially Private Machine Learning(https://arxiv.org/abs/2410.22673)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Differential privacy quantifies privacy through the privacy budget $\epsilon$, yet its practical interpretation is complicated by variations across models and datasets. Recent research on differentially private machine learning and membership inference has highlighted that with the same theoretical $\epsilon$ setting, the likelihood-ratio-based membership inference (LiRA) attacking success rate (ASR) may vary according to specific datasets and models, which might be a better indicator for evaluating real-world privacy risks. Inspired by this practical privacy measure, we study the approaches that can lower the attacking success rate to allow for more flexible privacy budget settings in model training. We find that by selectively suppressing privacy-sensitive features, we can achieve lower ASR values without compromising application-specific data utility. We use the SHAP and LIME model explainer to evaluate feature sensitivities and develop feature-masking strategies. Our findings demonstrate that the LiRA $ASR^M$ on model $M$ can properly indicate the inherent privacy risk of a dataset for modeling, and it's possible to modify datasets to enable the use of larger theoretical $\epsilon$ settings to achieve equivalent practical privacy protection. We have conducted extensive experiments to show the inherent link between ASR and the dataset's privacy risk. By carefully selecting features to mask, we can preserve more data utility with equivalent practical privacy protection and relaxed $\epsilon$ settings. The implementation details are shared online at the provided GitHub URL \url{this https URL}.</li>
</ul>

<h3>Title: Is Function Similarity Over-Engineered? Building a Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Rebecca Saul, Chang Liu, Noah Fleischmann, Richard Zak, Kristopher Micinski, Edward Raff, James Holt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22677">https://arxiv.org/abs/2410.22677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22677">https://arxiv.org/pdf/2410.22677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22677]] Is Function Similarity Over-Engineered? Building a Benchmark(https://arxiv.org/abs/2410.22677)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSE-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple basline, one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing, is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.</li>
</ul>

<h3>Title: Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion</h3>
<ul>
<li><strong>Authors: </strong>Ji Guo, Hongwei Li, Wenbo Jiang, Guoming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22678">https://arxiv.org/abs/2410.22678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22678">https://arxiv.org/pdf/2410.22678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22678]] Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion(https://arxiv.org/abs/2410.22678)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Networks (CNN) across various computer vision tasks. However, akin to CNN, ViTs are vulnerable to backdoor attacks, where the adversary embeds the backdoor into the victim model, causing it to make wrong predictions about testing samples containing a specific trigger. Existing backdoor attacks against ViTs have the limitation of failing to strike an optimal balance between attack stealthiness and attack effectiveness. In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB) targeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively erodes pixels in areas of maximal attention gradient, embedding a covert backdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves an optimal balance between attack stealthiness and attack effectiveness, ensuring the trigger remains invisible to human detection while preserving the model's accuracy on clean samples. Extensive experimental evaluations across various ViT architectures and datasets confirm the effectiveness of AGEB, achieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data Accuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated, demonstrating minimal visual discrepancies between the clean and the triggered images.</li>
</ul>

<h3>Title: Byzantine-Robust Federated Learning: An Overview With Focus on Developing Sybil-based Attacks to Backdoor Augmented Secure Aggregation Protocols</h3>
<ul>
<li><strong>Authors: </strong>Atharv Deshmukh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22680">https://arxiv.org/abs/2410.22680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22680">https://arxiv.org/pdf/2410.22680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22680]] Byzantine-Robust Federated Learning: An Overview With Focus on Developing Sybil-based Attacks to Backdoor Augmented Secure Aggregation Protocols(https://arxiv.org/abs/2410.22680)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) paradigms enable large numbers of clients to collaboratively train Machine Learning models on private data. However, due to their multi-party nature, traditional FL schemes are left vulnerable to Byzantine attacks that attempt to hurt model performance by injecting malicious backdoors. A wide variety of prevention methods have been proposed to protect frameworks from such attacks. This paper provides a exhaustive and updated taxonomy of existing methods and frameworks, before zooming in and conducting an in-depth analysis of the strengths and weaknesses of the Robustness of Federated Learning (RoFL) protocol. From there, we propose two novel Sybil-based attacks that take advantage of vulnerabilities in RoFL. Finally, we conclude with comprehensive proposals for future testing, describe and detail implementation of the proposed attacks, and offer direction for improvements in the RoFL protocol as well as Byzantine-robust frameworks as a whole.</li>
</ul>

<h3>Title: Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Yashvir S. Grewal, Edwin V. Bonilla, Thang D. Bui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22685">https://arxiv.org/abs/2410.22685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22685">https://arxiv.org/pdf/2410.22685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22685]] Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings(https://arxiv.org/abs/2410.22685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the-art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional entailment criteria between multiple generated responses and also depend on sequence likelihoods. While effective, these approaches often overestimate uncertainty due to their sensitivity to minor wording differences, additional correct information, and non-important words in the sequence. We propose a novel approach that leverages semantic embeddings to achieve smoother and more robust estimation of semantic uncertainty in LLMs. By capturing semantic similarities without depending on sequence likelihoods, our method inherently reduces any biases introduced by irrelevant words in the answers. Furthermore, we introduce an amortised version of our approach by explicitly modelling semantics as latent variables in a joint probabilistic model. This allows for uncertainty estimation in the embedding space with a single forward pass, significantly reducing computational overhead compared to existing multi-pass methods. Experiments across multiple question-answering datasets and frontier LLMs demonstrate that our embedding-based methods provide more accurate and nuanced uncertainty quantification than traditional approaches.</li>
</ul>

<h3>Title: Choice between Partial Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Henrik Marklund, Benjamin Van Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22690">https://arxiv.org/abs/2410.22690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22690">https://arxiv.org/pdf/2410.22690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22690]] Choice between Partial Trajectories(https://arxiv.org/abs/2410.22690)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As AI agents generate increasingly sophisticated behaviors, manually encoding human preferences to guide these agents becomes more challenging. To address this, it has been suggested that agents instead learn preferences from human choice data. This approach requires a model of choice behavior that the agent can use to interpret the data. For choices between partial trajectories of states and actions, previous models assume choice probabilities to be determined by the partial return or the cumulative advantage. We consider an alternative model based instead on the bootstrapped return, which adds to the partial return an estimate of the future return. Benefits of the bootstrapped return model stem from its treatment of human beliefs. Unlike partial return, choices based on bootstrapped return reflect human beliefs about the environment. Further, while recovering the reward function from choices based on cumulative advantage requires that those beliefs are correct, doing so from choices based on bootstrapped return does not. To motivate the bootstrapped return model, we formulate axioms and prove an Alignment Theorem. This result formalizes how, for a general class of human preferences, such models are able to disentangle goals from beliefs. This ensures recovery of an aligned reward function when learning from choices based on bootstrapped return. The bootstrapped return model also affords greater robustness to choice behavior. Even when choices are based on partial return, learning via a bootstrapped return model recovers an aligned reward function. The same holds with choices based on the cumulative advantage if the human and the agent both adhere to correct and consistent beliefs about the environment. On the other hand, if choices are based on bootstrapped return, learning via partial return or cumulative advantage models does not generally produce an aligned reward function.</li>
</ul>

<h3>Title: Exactly Minimax-Optimal Locally Differentially Private Sampling</h3>
<ul>
<li><strong>Authors: </strong>Hyun-Young Park, Shahab Asoodeh, Si-Hyeon Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22699">https://arxiv.org/abs/2410.22699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22699">https://arxiv.org/pdf/2410.22699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22699]] Exactly Minimax-Optimal Locally Differentially Private Sampling(https://arxiv.org/abs/2410.22699)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>The sampling problem under local differential privacy has recently been studied with potential applications to generative models, but a fundamental analysis of its privacy-utility trade-off (PUT) remains incomplete. In this work, we define the fundamental PUT of private sampling in the minimax sense, using the f-divergence between original and sampling distributions as the utility measure. We characterize the exact PUT for both finite and continuous data spaces under some mild conditions on the data distributions, and propose sampling mechanisms that are universally optimal for all f-divergences. Our numerical experiments demonstrate the superiority of our mechanisms over baselines, in terms of theoretical utilities for finite data space and of empirical utilities for continuous data space.</li>
</ul>

<h3>Title: Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images</h3>
<ul>
<li><strong>Authors: </strong>Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22705">https://arxiv.org/abs/2410.22705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22705">https://arxiv.org/pdf/2410.22705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22705]] Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images(https://arxiv.org/abs/2410.22705)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, watermark</a></li>
<li><strong>Abstract: </strong>Single-view 3D reconstruction methods like Triplane Gaussian Splatting (TGS) have enabled high-quality 3D model generation from just a single image input within seconds. However, this capability raises concerns about potential misuse, where malicious users could exploit TGS to create unauthorized 3D models from copyrighted images. To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed "geometry cloaks", into images before supplying them to TGS. These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional adversarial attacks that simply degrade output quality, our method forces TGS to fail the 3D reconstruction in a specific way - by generating an identifiable customized pattern that acts as a watermark. This watermark allows copyright holders to assert ownership over any attempted 3D reconstructions made from their protected images. Extensive experiments have verified the effectiveness of our geometry cloak. Our project is available at this https URL.</li>
</ul>

<h3>Title: FilterViT and DropoutViT: Lightweight Vision Transformer Models for Efficient Attention Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Bohang Sun (School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22709">https://arxiv.org/abs/2410.22709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22709">https://arxiv.org/pdf/2410.22709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22709]] FilterViT and DropoutViT: Lightweight Vision Transformer Models for Efficient Attention Mechanisms(https://arxiv.org/abs/2410.22709)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>In this study, we introduce FilterViT, an enhanced version of MobileViT, which leverages an attention-based mechanism for early-stage downsampling. Traditional QKV operations on high-resolution feature maps are computationally intensive due to the abundance of tokens. To address this, we propose a filter attention mechanism using a convolutional neural network (CNN) to generate an importance mask, focusing attention on key image regions. The method significantly reduces computational complexity while maintaining interpretability, as it highlights essential image areas. Experimental results show that FilterViT achieves substantial gains in both efficiency and accuracy compared to other models. We also introduce DropoutViT, a variant that uses a stochastic approach for pixel selection, further enhancing robustness.</li>
</ul>

<h3>Title: LoFLAT: Local Feature Matching using Focused Linear Attention Transformer</h3>
<ul>
<li><strong>Authors: </strong>Naijian Cao, Renjie He, Yuchao Dai, Mingyi He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22710">https://arxiv.org/abs/2410.22710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22710">https://arxiv.org/pdf/2410.22710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22710]] LoFLAT: Local Feature Matching using Focused Linear Attention Transformer(https://arxiv.org/abs/2410.22710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Local feature matching is an essential technique in image matching and plays a critical role in a wide range of vision-based applications. However, existing Transformer-based detector-free local feature matching methods encounter challenges due to the quadratic computational complexity of attention mechanisms, especially at high resolutions. However, while existing Transformer-based detector-free local feature matching methods have reduced computational costs using linear attention mechanisms, they still struggle to capture detailed local interactions, which affects the accuracy and robustness of precise local correspondences. In order to enhance representations of attention mechanisms while preserving low computational complexity, we propose the LoFLAT, a novel Local Feature matching using Focused Linear Attention Transformer in this paper. Our LoFLAT consists of three main modules: the Feature Extraction Module, the Feature Transformer Module, and the Matching Module. Specifically, the Feature Extraction Module firstly uses ResNet and a Feature Pyramid Network to extract hierarchical features. The Feature Transformer Module further employs the Focused Linear Attention to refine attention distribution with a focused mapping function and to enhance feature diversity with a depth-wise convolution. Finally, the Matching Module predicts accurate and robust matches through a coarse-to-fine strategy. Extensive experimental evaluations demonstrate that the proposed LoFLAT outperforms the LoFTR method in terms of both efficiency and accuracy.</li>
</ul>

<h3>Title: Enhancing binary classification: A new stacking method via leveraging computational geometry</h3>
<ul>
<li><strong>Authors: </strong>Wei Wu, Liang Tang, Zhongjie Zhao, Chung-Piaw Teo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22722">https://arxiv.org/abs/2410.22722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22722">https://arxiv.org/pdf/2410.22722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22722]] Enhancing binary classification: A new stacking method via leveraging computational geometry(https://arxiv.org/abs/2410.22722)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Stacking, a potent ensemble learning method, leverages a meta-model to harness the strengths of multiple base models, thereby enhancing prediction accuracy. Traditional stacking techniques typically utilize established learning models, such as logistic regression, as the meta-model. This paper introduces a novel approach that integrates computational geometry techniques, specifically solving the maximum weighted rectangle problem, to develop a new meta-model for binary classification. Our method is evaluated on multiple open datasets, with statistical analysis showing its stability and demonstrating improvements in accuracy compared to current state-of-the-art stacking methods with out-of-fold predictions. This new stacking method also boasts two significant advantages: enhanced interpretability and the elimination of hyperparameter tuning for the meta-model, thus increasing its practicality. These merits make our method highly applicable not only in stacking ensemble learning but also in various real-world applications, such as hospital health evaluation scoring and bank credit scoring systems, offering a fresh evaluation perspective.</li>
</ul>

<h3>Title: One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ji Guo, Wenbo Jiang, Rui Zhang, Guoming Lu, Hongwei Li, Weiren Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22725">https://arxiv.org/abs/2410.22725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22725">https://arxiv.org/pdf/2410.22725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22725]] One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks(https://arxiv.org/abs/2410.22725)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Recently, the success of Text-to-Image (T2I) models has led to the rise of numerous third-party platforms, which claim to provide cheaper API services and more flexibility in model options. However, this also raises a new security concern: Are these third-party services truly offering the models they claim? To address this problem, we propose the first T2I model verification method named Text-to-Image Model Verification via Non-Transferable Adversarial Attacks (TVN). The non-transferability of adversarial examples means that these examples are only effective on a target model and ineffective on other models, thereby allowing for the verification of the target model. TVN utilizes the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine similarity of a prompt's text encoding, generating non-transferable adversarial prompts. By calculating the CLIP-text scores between the non-transferable adversarial prompts without perturbations and the images, we can verify if the model matches the claimed target model, based on a 3-sigma threshold. The experiments showed that TVN performed well in both closed-set and open-set scenarios, achieving a verification accuracy of over 90\%. Moreover, the adversarial prompts generated by TVN significantly reduced the CLIP-text scores of the target model, while having little effect on other models.</li>
</ul>

<h3>Title: Offline Behavior Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shiye Lei, Sen Zhang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22728">https://arxiv.org/abs/2410.22728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22728">https://arxiv.org/pdf/2410.22728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22728]] Offline Behavior Distillation(https://arxiv.org/abs/2410.22728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Massive reinforcement learning (RL) data are typically collected to train policies offline without the need for interactions, but the large data volume can cause training inefficiencies. To tackle this issue, we formulate offline behavior distillation (OBD), which synthesizes limited expert behavioral data from sub-optimal RL data, enabling rapid policy learning. We propose two naive OBD objectives, DBC and PBC, which measure distillation performance via the decision difference between policies trained on distilled data and either offline data or a near-expert policy. Due to intractable bi-level optimization, the OBD objective is difficult to minimize to small values, which deteriorates PBC by its distillation performance guarantee with quadratic discount complexity $\mathcal{O}(1/(1-\gamma)^2)$. We theoretically establish the equivalence between the policy performance and action-value weighted decision difference, and introduce action-value weighted PBC (Av-PBC) as a more effective OBD objective. By optimizing the weighted decision difference, Av-PBC achieves a superior distillation guarantee with linear discount complexity $\mathcal{O}(1/(1-\gamma))$. Extensive experiments on multiple D4RL datasets reveal that Av-PBC offers significant improvements in OBD performance, fast distillation convergence speed, and robust cross-architecture/optimizer generalization.</li>
</ul>

<h3>Title: ETO:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses</h3>
<ul>
<li><strong>Authors: </strong>Junjie Ni, Guofeng Zhang, Guanglin Li, Yijin Li, Xinyang Liu, Zhaoyang Huang, Hujun Bao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22733">https://arxiv.org/abs/2410.22733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22733">https://arxiv.org/pdf/2410.22733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22733]] ETO:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses(https://arxiv.org/abs/2410.22733)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We tackle the efficiency problem of learning local feature this http URL advancements have given rise to purely CNN-based and transformer-based approaches, each augmented with deep learning techniques. While CNN-based methods often excel in matching speed, transformer-based methods tend to provide more accurate matches. We propose an efficient transformer-based network architecture for local feature this http URL technique is built on constructing multiple homography hypotheses to approximate the continuous correspondence in the real world and uni-directional cross-attention to accelerate the refinement. On the YFCC100M dataset, our matching accuracy is competitive with LoFTR, a state-of-the-art transformer-based architecture, while the inference speed is boosted to 4 times, even outperforming the CNN-based this http URL evaluations on other open datasets such as Megadepth, ScanNet, and HPatches demonstrate our method's efficacy, highlighting its potential to significantly enhance a wide array of downstream applications.</li>
</ul>

<h3>Title: MIXAD: Memory-Induced Explainable Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Minha Kim, Kishor Kumar Bhaumik, Amin Ahsan Ali, Simon S. Woo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22735">https://arxiv.org/abs/2410.22735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22735">https://arxiv.org/pdf/2410.22735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22735]] MIXAD: Memory-Induced Explainable Time Series Anomaly Detection(https://arxiv.org/abs/2410.22735)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>For modern industrial applications, accurately detecting and diagnosing anomalies in multivariate time series data is essential. Despite such need, most state-of-the-art methods often prioritize detection performance over model interpretability. Addressing this gap, we introduce MIXAD (Memory-Induced Explainable Time Series Anomaly Detection), a model designed for interpretable anomaly detection. MIXAD leverages a memory network alongside spatiotemporal processing units to understand the intricate dynamics and topological structures inherent in sensor relationships. We also introduce a novel anomaly scoring method that detects significant shifts in memory activation patterns during anomalies. Our approach not only ensures decent detection performance but also outperforms state-of-the-art baselines by 34.30% and 34.51% in interpretability metrics.</li>
</ul>

<h3>Title: A Game-Theoretic Approach for Security Control Selection</h3>
<ul>
<li><strong>Authors: </strong>Dylan Léveillé (Department of Systems and Computer Engineering, Carleton University), Jason Jaskolka (Department of Systems and Computer Engineering, Carleton University)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22762">https://arxiv.org/abs/2410.22762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22762">https://arxiv.org/pdf/2410.22762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22762]] A Game-Theoretic Approach for Security Control Selection(https://arxiv.org/abs/2410.22762)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>Selecting the combination of security controls that will most effectively protect a system's assets is a difficult task. If the wrong controls are selected, the system may be left vulnerable to cyber-attacks that can impact the confidentiality, integrity and availability of critical data and services. In practical settings, it is not possible to select and implement every control possible. Instead considerations, such as budget, effectiveness, and dependencies among various controls, must be considered to choose a combination of security controls that best achieve a set of system security objectives. In this paper, we propose a game-theoretic approach for selecting effective combinations of security controls based on expected attacker profiles and a set budget. The control selection problem is set up as a two-person zero-sum one-shot game. Valid control combinations for selection are generated using an algebraic formalism to account for dependencies among selected controls. We demonstrate the proposed approach on an illustrative financial system used in government departments under four different scenarios. The results illustrate how a security analyst can use the proposed approach to guide and support decision-making in the control selection activity when developing secure systems.</li>
</ul>

<h3>Title: Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot</h3>
<ul>
<li><strong>Authors: </strong>Sejin Lee, Dongha Kim, Min Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22767">https://arxiv.org/abs/2410.22767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22767">https://arxiv.org/pdf/2410.22767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22767]] Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot(https://arxiv.org/abs/2410.22767)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots.</li>
</ul>

<h3>Title: InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, Xiaogeng Liu, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22770">https://arxiv.org/abs/2410.22770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22770">https://arxiv.org/pdf/2410.22770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22770]] InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models(https://arxiv.org/abs/2410.22770)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense -- falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. InjecGuard demonstrates state-of-the-art performance on diverse benchmarks including NotInject, surpassing the existing best model by 30.8%, offering a robust and open-source solution for detecting prompt injection attacks. The code and datasets are released at this https URL.</li>
</ul>

<h3>Title: FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple Reference Images</h3>
<ul>
<li><strong>Authors: </strong>Zheng Yu, Yaohua Wang, Siying Cui, Aixi Zhang, Wei-Long Zheng, Senzhang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22771">https://arxiv.org/abs/2410.22771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22771">https://arxiv.org/pdf/2410.22771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22771]] FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple Reference Images(https://arxiv.org/abs/2410.22771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Facial parts swapping aims to selectively transfer regions of interest from the source image onto the target image while maintaining the rest of the target image unchanged. Most studies on face swapping designed specifically for full-face swapping, are either unable or significantly limited when it comes to swapping individual facial parts, which hinders fine-grained and customized character designs. However, designing such an approach specifically for facial parts swapping is challenged by a reasonable multiple reference feature fusion, which needs to be both efficient and effective. To overcome this challenge, FuseAnyPart is proposed to facilitate the seamless "fuse-any-part" customization of the face. In FuseAnyPart, facial parts from different people are assembled into a complete face in latent space within the Mask-based Fusion Module. Subsequently, the consolidated feature is dispatched to the Addition-based Injection Module for fusion within the UNet of the diffusion model to create novel characters. Extensive experiments qualitatively and quantitatively validate the superiority and robustness of FuseAnyPart. Source codes are available at this https URL.</li>
</ul>

<h3>Title: Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Arash Marioriyad, Parham Rezaei, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22775">https://arxiv.org/abs/2410.22775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22775">https://arxiv.org/pdf/2410.22775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22775]] Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models(https://arxiv.org/abs/2410.22775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E, have shown remarkable proficiency in producing high-quality, realistic, and natural images from textual descriptions. However, these models sometimes fail to accurately capture all the details specified in the input prompts, particularly concerning entities, attributes, and spatial relationships. This issue becomes more pronounced when the prompt contains novel or complex compositions, leading to what are known as compositional generation failure modes. Recently, a new open-source diffusion-based T2I model, FLUX, has been introduced, demonstrating strong performance in high-quality image generation. Additionally, autoregressive T2I models like LlamaGen have claimed competitive visual quality performance compared to diffusion-based models. In this study, we evaluate the compositional generation capabilities of these newly introduced models against established models using the T2I-CompBench benchmark. Our findings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on par with state-of-the-art diffusion models for compositional generation tasks under the same criteria, such as model size and inference time. On the other hand, the open-source diffusion-based model FLUX exhibits compositional generation capabilities comparable to the state-of-the-art closed-source model DALL-E3.</li>
</ul>

<h3>Title: Contrastive Learning and Adversarial Disentanglement for Privacy-Preserving Task-Oriented Semantic Communications</h3>
<ul>
<li><strong>Authors: </strong>Omar Erak, Omar Alhussein, Wen Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.IT, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22784">https://arxiv.org/abs/2410.22784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22784">https://arxiv.org/pdf/2410.22784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22784]] Contrastive Learning and Adversarial Disentanglement for Privacy-Preserving Task-Oriented Semantic Communications(https://arxiv.org/abs/2410.22784)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Task-oriented semantic communication systems have emerged as a promising approach to achieving efficient and intelligent data transmission, where only information relevant to a specific task is communicated. However, existing methods struggle to fully disentangle task-relevant and task-irrelevant information, leading to privacy concerns and subpar performance. To address this, we propose an information-bottleneck method, named CLAD (contrastive learning and adversarial disentanglement). CLAD leverages contrastive learning to effectively capture task-relevant features while employing adversarial disentanglement to discard task-irrelevant information. Additionally, due to the lack of reliable and reproducible methods to gain insight into the informativeness and minimality of the encoded feature vectors, we introduce a new technique to compute the information retention index (IRI), a comparative metric used as a proxy for the mutual information between the encoded features and the input, reflecting the minimality of the encoded features. The IRI quantifies the minimality and informativeness of the encoded feature vectors across different task-oriented communication techniques. Our extensive experiments demonstrate that CLAD outperforms state-of-the-art baselines in terms of task performance, privacy preservation, and IRI. CLAD achieves a predictive performance improvement of around 2.5-3%, along with a 77-90% reduction in IRI and a 57-76% decrease in adversarial accuracy.</li>
</ul>

<h3>Title: Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning</h3>
<ul>
<li><strong>Authors: </strong>Yiqin Lv, Qi Wang, Dong Liang, Zheng Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22788">https://arxiv.org/abs/2410.22788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22788">https://arxiv.org/pdf/2410.22788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22788]] Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning(https://arxiv.org/abs/2410.22788)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Meta learning is a promising paradigm in the era of large models and task distributional robustness has become an indispensable consideration in real-world scenarios. Recent advances have examined the effectiveness of tail task risk minimization in fast adaptation robustness improvement \citep{wang2023simple}. This work contributes to more theoretical investigations and practical enhancements in the field. Specifically, we reduce the distributionally robust strategy to a max-min optimization problem, constitute the Stackelberg equilibrium as the solution concept, and estimate the convergence rate. In the presence of tail risk, we further derive the generalization bound, establish connections with estimated quantiles, and practically improve the studied strategy. Accordingly, extensive evaluations demonstrate the significance of our proposal and its scalability to multimodal large models in boosting robustness.</li>
</ul>

<h3>Title: Adaptive Multi Scale Document Binarisation Using Vision Mamba</h3>
<ul>
<li><strong>Authors: </strong>Mohd. Azfar, Siddhant Bharadwaj, Ashwin Sasikumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22811">https://arxiv.org/abs/2410.22811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22811">https://arxiv.org/pdf/2410.22811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22811]] Adaptive Multi Scale Document Binarisation Using Vision Mamba(https://arxiv.org/abs/2410.22811)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Enhancing and preserving the readability of document images, particularly historical ones, is crucial for effective document image analysis. Numerous models have been proposed for this task, including convolutional-based, transformer-based, and hybrid convolutional-transformer architectures. While hybrid models address the limitations of purely convolutional or transformer-based methods, they often suffer from issues like quadratic time complexity. In this work, we propose a Mamba-based architecture for document binarisation, which efficiently handles long sequences by scaling linearly and optimizing memory usage. Additionally, we introduce novel modifications to the skip connections by incorporating Difference of Gaussians (DoG) features, inspired by conventional signal processing techniques. These multiscale high-frequency features enable the model to produce high-quality, detailed outputs.</li>
</ul>

<h3>Title: Universality of the $\pi^2/6$ Pathway in Avoiding Model Collapse</h3>
<ul>
<li><strong>Authors: </strong>Apratim Dey, David Donoho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22812">https://arxiv.org/abs/2410.22812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22812">https://arxiv.org/pdf/2410.22812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22812]] Universality of the $\pi^2/6$ Pathway in Avoiding Model Collapse(https://arxiv.org/abs/2410.22812)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Researchers in empirical machine learning recently spotlighted their fears of so-called Model Collapse. They imagined a discard workflow, where an initial generative model is trained with real data, after which the real data are discarded, and subsequently, the model generates synthetic data on which a new model is trained. They came to the conclusion that models degenerate as model-fitting generations proceed. However, other researchers considered an augment workflow, where the original real data continue to be used in each generation of training, augmented by synthetic data from models fit in all earlier generations. Empirical results on canonical datasets and learning procedures confirmed the occurrence of model collapse under the discard workflow and avoidance of model collapse under the augment workflow. Under the augment workflow, theoretical evidence also confirmed avoidance in particular instances; specifically, Gerstgrasser et al. (2024) found that for classical Linear Regression, test risk at any later generation is bounded by a moderate multiple, viz. pi-squared-over-6 of the test risk of training with the original real data alone. Some commentators questioned the generality of theoretical conclusions based on the generative model assumed in Gerstgrasser et al. (2024): could similar conclusions be reached for other task/model pairings? In this work, we demonstrate the universality of the pi-squared-over-6 augment risk bound across a large family of canonical statistical models, offering key insights into exactly why collapse happens under the discard workflow and is avoided under the augment workflow. In the process, we provide a framework that is able to accommodate a large variety of workflows (beyond discard and augment), thereby enabling an experimenter to judge the comparative merits of multiple different workflows by simulating a simple Gaussian process.</li>
</ul>

<h3>Title: Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients</h3>
<ul>
<li><strong>Authors: </strong>Jabin Koo, Minwoo Jang, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22815">https://arxiv.org/abs/2410.22815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22815">https://arxiv.org/pdf/2410.22815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22815]] Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients(https://arxiv.org/abs/2410.22815)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>Federated fine-tuning for Large Language Models (LLMs) has recently gained attention due to the heavy communication overhead of transmitting large model updates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A2 maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a 99.8% reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism boosts robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.</li>
</ul>

<h3>Title: Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Min, Yawei Luo, Jianwen Sun, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22817">https://arxiv.org/abs/2410.22817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22817">https://arxiv.org/pdf/2410.22817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22817]] Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis(https://arxiv.org/abs/2410.22817)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from sparse-view observations in a feed-forward inference manner, eliminating the need for scene-specific retraining required in conventional 3DGS. However, existing methods rely heavily on epipolar priors, which can be unreliable in complex realworld scenes, particularly in non-overlapping and occluded regions. In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based model for generalizable novel view synthesis that operates independently of epipolar line constraints. To enhance multiview feature extraction with 3D perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view completion pre-training on large-scale datasets. Additionally, we introduce an Iterative Cross-view Gaussians Alignment method to ensure consistent depth scales across different views. Our eFreeSplat represents an innovative approach for generalizable novel view synthesis. Different from the existing pure geometry-free methods, eFreeSplat focuses more on achieving epipolar-free feature matching and encoding by providing 3D priors through cross-view pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks using the RealEstate10K and ACID datasets. Extensive experiments demonstrate that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar priors, achieving superior geometry reconstruction and novel view synthesis quality. Project page: this https URL.</li>
</ul>

<h3>Title: EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Jia Li, Ge Li, Xuanming Zhang, Yunfei Zhao, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22821">https://arxiv.org/abs/2410.22821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22821">https://arxiv.org/pdf/2410.22821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22821]] EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations(https://arxiv.org/abs/2410.22821)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>How to evaluate Large Language Models (LLMs) in code generation remains an open question. Existing benchmarks have two limitations - data leakage and lack of domain-specific evaluation. The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains. To address these two limitations, we propose a new benchmark - EvoCodeBench, which has the following advances: (1) Evolving data. EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories. (2) A domain taxonomy and domain labels. Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. (3) Domain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g., gpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%. Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. EvoCodeBench has been released.</li>
</ul>

<h3>Title: How Well Do Large Language Models Disambiguate Swedish Words?</h3>
<ul>
<li><strong>Authors: </strong>Richard Johansson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22827">https://arxiv.org/abs/2410.22827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22827">https://arxiv.org/pdf/2410.22827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22827]] How Well Do Large Language Models Disambiguate Swedish Words?(https://arxiv.org/abs/2410.22827)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We evaluate a battery of recent large language models on two benchmarks for word sense disambiguation in Swedish. At present, all current models are less accurate than the best supervised disambiguators in cases where a training set is available, but most models outperform graph-based unsupervised systems. Different prompting approaches are compared, with a focus on how to express the set of possible senses in a given context. The best accuracies are achieved when human-written definitions of the senses are included in the prompts.</li>
</ul>

<h3>Title: HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Zhang, Qinfeng Li, Tianyu Du, Xuhong Zhang, Xinkui Zhao, Zhengwen Feng, Jianwei Yin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22832">https://arxiv.org/abs/2410.22832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22832">https://arxiv.org/pdf/2410.22832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22832]] HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models(https://arxiv.org/abs/2410.22832)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge, making them adaptable and cost-effective for various applications. However, the growing reliance on these systems also introduces potential security risks. In this work, we reveal a novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which enables attackers to manipulate the retrieval mechanisms of RAG systems by injecting malicious texts into the knowledge database. When the RAG system encounters target questions, it generates the attacker's pre-determined answers instead of the correct ones, undermining the integrity and trustworthiness of the system. We formalize HijackRAG as an optimization problem and propose both black-box and white-box attack strategies tailored to different levels of the attacker's knowledge. Extensive experiments on multiple benchmark datasets show that HijackRAG consistently achieves high attack success rates, outperforming existing baseline attacks. Furthermore, we demonstrate that the attack is transferable across different retriever models, underscoring the widespread risk it poses to RAG systems. Lastly, our exploration of various defense mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing the urgent need for more robust security measures to protect RAG systems in real-world deployments.</li>
</ul>

<h3>Title: Danoliteracy of Generative, Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Søren Vejlgaard Holm, Lars Kai Hansen, Martin Carsten Nielsen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22839">https://arxiv.org/abs/2410.22839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22839">https://arxiv.org/pdf/2410.22839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22839]] Danoliteracy of Generative, Large Language Models(https://arxiv.org/abs/2410.22839)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The language technology moonshot moment of Generative, Large Language Models (GLLMs) was not limited to English: These models brought a surge of technological applications, investments and hype to low-resource languages as well. However, the capabilities of these models in languages such as Danish were until recently difficult to verify beyond qualitative demonstrations due to a lack of applicable evaluation corpora. We present a GLLM benchmark to evaluate Danoliteracy, a measure of Danish language and cultural competency, across eight diverse scenarios such Danish citizenship tests and abstractive social media question answering. This limited-size benchmark is found to produce a robust ranking that correlates to human feedback at $\rho \sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings. Analyzing these model results across scenarios, we find one strong underlying factor explaining $95\%$ of scenario performance variance for GLLMs in Danish, suggesting a $g$ factor of model consistency in language adaption.</li>
</ul>

<h3>Title: Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions</h3>
<ul>
<li><strong>Authors: </strong>J. Quetzalcoatl Toledo-Marin, Sebastian Gonzalez, Hao Jia, Ian Lu, Deniz Sogutlu, Abhishek Abhishek, Colin Gay, Eric Paquet, Roger Melko, Geoffrey C. Fox, Maximilian Swiatlowski, Wojciech Fedorko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, hep-ph, physics.comp-ph, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22870">https://arxiv.org/abs/2410.22870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22870">https://arxiv.org/pdf/2410.22870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22870]] Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions(https://arxiv.org/abs/2410.22870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Particle collisions at accelerators such as the Large Hadron Collider, recorded and analyzed by experiments such as ATLAS and CMS, enable exquisite measurements of the Standard Model and searches for new phenomena. Simulations of collision events at these detectors have played a pivotal role in shaping the design of future experiments and analyzing ongoing ones. However, the quest for accuracy in Large Hadron Collider (LHC) collisions comes at an imposing computational cost, with projections estimating the need for millions of CPU-years annually during the High Luminosity LHC (HL-LHC) run \cite{collaboration2022atlas}. Simulating a single LHC event with \textsc{Geant4} currently devours around 1000 CPU seconds, with simulations of the calorimeter subdetectors in particular imposing substantial computational demands \cite{rousseau2023experimental}. To address this challenge, we propose a conditioned quantum-assisted deep generative model. Our model integrates a conditioned variational autoencoder (VAE) on the exterior with a conditioned Restricted Boltzmann Machine (RBM) in the latent space, providing enhanced expressiveness compared to conventional VAEs. The RBM nodes and connections are meticulously engineered to enable the use of qubits and couplers on D-Wave's Pegasus-structured \textit{Advantage} quantum annealer (QA) for sampling. We introduce a novel method for conditioning the quantum-assisted RBM using \textit{flux biases}. We further propose a novel adaptive mapping to estimate the effective inverse temperature in quantum annealers. The effectiveness of our framework is illustrated using Dataset 2 of the CaloChallenge \cite{calochallenge}.</li>
</ul>

<h3>Title: Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Ranaldi, Marco Valentino, Andrè Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22874">https://arxiv.org/abs/2410.22874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22874">https://arxiv.org/pdf/2410.22874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22874]] Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations(https://arxiv.org/abs/2410.22874)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) has emerged as a critical mechanism in contemporary NLP to support Large Language Models(LLMs) in systematically accessing richer factual context. However, the integration of RAG mechanisms brings its inherent challenges, as LLMs need to deal with potentially noisy contexts. Recent studies have shown that LLMs still struggle to critically analyse RAG-based in-context information, a limitation that may lead to incorrect inferences and hallucinations. In this paper, we investigate how to elicit critical reasoning in RAG via contrastive explanations. In particular, we propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant documents given a query, (ii) selects and exemplifies relevant passages, and (iii) generates explanations that explicitly contrast the relevance of the passages to (iv) support the final answer. We show the impact of C-RAG building contrastive reasoning demonstrations from LLMs to instruct smaller models for retrieval-augmented tasks. Extensive experiments demonstrate that C-RAG improves state-of-the-art RAG models while (a) requiring significantly fewer prompts and demonstrations and (b) being robust to perturbations in the retrieved documents.</li>
</ul>

<h3>Title: SFA-UNet: More Attention to Multi-Scale Contrast and Contextual Information in Infrared Small Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Imad Ali Shah, Fahad Mumtaz Malik, Muhammad Waqas Ashraf</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22881">https://arxiv.org/abs/2410.22881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22881">https://arxiv.org/pdf/2410.22881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22881]] SFA-UNet: More Attention to Multi-Scale Contrast and Contextual Information in Infrared Small Object Segmentation(https://arxiv.org/abs/2410.22881)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Computer vision researchers have extensively worked on fundamental infrared visual recognition for the past few decades. Among various approaches, deep learning has emerged as the most promising candidate. However, Infrared Small Object Segmentation (ISOS) remains a major focus due to several challenges including: 1) the lack of effective utilization of local contrast and global contextual information; 2) the potential loss of small objects in deep models; and 3) the struggling to capture fine-grained details and ignore noise. To address these challenges, we propose a modified U-Net architecture, named SFA-UNet, by combining Scharr Convolution (SC) and Fast Fourier Convolution (FFC) in addition to vertical and horizontal Attention gates (AG) into UNet. SFA-UNet utilizes double convolution layers with the addition of SC and FFC in its encoder and decoder layers. SC helps to learn the foreground-to-background contrast information whereas FFC provide multi-scale contextual information while mitigating the small objects vanishing problem. Additionally, the introduction of vertical AGs in encoder layers enhances the model's focus on the targeted object by ignoring irrelevant regions. We evaluated the proposed approach on publicly available, SIRST and IRSTD datasets, and achieved superior performance by an average 0.75% with variance of 0.025 of all combined metrics in multiple runs as compared to the existing state-of-the-art methods</li>
</ul>

<h3>Title: Adaptive Paradigm Synergy: Can a Cross-Paradigm Objective Enhance Long-Tailed Learning?</h3>
<ul>
<li><strong>Authors: </strong>Haowen Xiao, Guanghui Liu, Xinyi Gao, Yang Li, Fengmao Lv, Jielei Chu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22883">https://arxiv.org/abs/2410.22883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22883">https://arxiv.org/pdf/2410.22883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22883]] Adaptive Paradigm Synergy: Can a Cross-Paradigm Objective Enhance Long-Tailed Learning?(https://arxiv.org/abs/2410.22883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has achieved impressive results across several computer vision tasks, even rivaling supervised methods. However, its performance degrades on real-world datasets with long-tailed distributions due to difficulties in capturing inherent class imbalances. Although supervised long-tailed learning offers significant insights, the absence of labels in SSL prevents direct transfer of these this http URL bridge this gap, we introduce Adaptive Paradigm Synergy (APS), a cross-paradigm objective that seeks to unify the strengths of both paradigms. Our approach reexamines contrastive learning from a spatial structure perspective, dynamically adjusting the uniformity of latent space structure through adaptive temperature tuning. Furthermore, we draw on a re-weighting strategy from supervised learning to compensate for the shortcomings of temperature adjustment in explicit quantity this http URL experiments on commonly used long-tailed datasets demonstrate that APS improves performance effectively and efficiently. Our findings reveal the potential for deeper integration between supervised and self-supervised learning, paving the way for robust models that handle real-world class imbalance.</li>
</ul>

<h3>Title: Stealing User Prompts from Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Itay Yona, Ilia Shumailov, Jamie Hayes, Nicholas Carlini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22884">https://arxiv.org/abs/2410.22884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22884">https://arxiv.org/pdf/2410.22884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22884]] Stealing User Prompts from Mixture of Experts(https://arxiv.org/abs/2410.22884)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries can exploit Expert-Choice-Routing to fully disclose a victim's prompt. We successfully demonstrate the effectiveness of this attack on a two-layer Mixtral model, exploiting the tie-handling behavior of the this http URL CUDA implementation. Our results show that we can extract the entire prompt using $O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100 queries on average per token in the setting we consider. This is the first attack to exploit architectural flaws for the purpose of extracting user prompts, introducing a new class of LLM vulnerabilities.</li>
</ul>

<h3>Title: Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector</h3>
<ul>
<li><strong>Authors: </strong>Youcheng Huang, Fengbin Zhu, Jingkun Tang, Pan Zhou, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22888">https://arxiv.org/abs/2410.22888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22888">https://arxiv.org/pdf/2410.22888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22888]] Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector(https://arxiv.org/abs/2410.22888)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale Adervsarial images dataset with Diverse hArmful Responses (RADAR), given that existing datasets are either small-scale or only contain limited types of harmful responses. With the new RADAR dataset, we further develop a novel and effective iN-time Embedding-based AdveRSarial Image DEtection (NEARSIDE) method, which exploits a single vector that distilled from the hidden states of VLMs, which we call the attacking direction, to achieve the detection of adversarial images against benign ones in the input. Extensive experiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the effectiveness, efficiency, and cross-model transferrability of our proposed method. Our code is available at this https URL</li>
</ul>

<h3>Title: YOLOv11 for Vehicle Detection: Advancements, Performance, and Applications in Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Mujadded Al Rabbani Alif</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22898">https://arxiv.org/abs/2410.22898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22898">https://arxiv.org/pdf/2410.22898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22898]] YOLOv11 for Vehicle Detection: Advancements, Performance, and Applications in Intelligent Transportation Systems(https://arxiv.org/abs/2410.22898)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate vehicle detection is essential for the development of intelligent transportation systems, autonomous driving, and traffic monitoring. This paper presents a detailed analysis of YOLO11, the latest advancement in the YOLO series of deep learning models, focusing exclusively on vehicle detection tasks. Building upon the success of its predecessors, YOLO11 introduces architectural improvements designed to enhance detection speed, accuracy, and robustness in complex environments. Using a comprehensive dataset comprising multiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we evaluate YOLO11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLO11 surpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more occluded vehicles while maintaining a competitive inference time, making it well-suited for real-time applications. Comparative analysis shows significant improvements in the detection of complex vehicle geometries, further contributing to the development of efficient and scalable vehicle detection systems. This research highlights YOLO11's potential to enhance autonomous vehicle performance and traffic monitoring systems, offering insights for future developments in the field.</li>
</ul>

<h3>Title: HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shengkai Zhang, Nianhong Jiao, Tian Li, Chaojie Yang, Chenhui Xue, Boya Niu, Jun Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22901">https://arxiv.org/abs/2410.22901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22901">https://arxiv.org/pdf/2410.22901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22901]] HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models(https://arxiv.org/abs/2410.22901)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (\url{this https URL}).</li>
</ul>

<h3>Title: Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents</h3>
<ul>
<li><strong>Authors: </strong>Safwan Labbi, Daniil Tiapkin, Lorenzo Mancini, Paul Mangold, Eric Moulines</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22908">https://arxiv.org/abs/2410.22908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22908">https://arxiv.org/pdf/2410.22908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22908]] Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents(https://arxiv.org/abs/2410.22908)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In this paper, we present the Federated Upper Confidence Bound Value Iteration algorithm ($\texttt{Fed-UCBVI}$), a novel extension of the $\texttt{UCBVI}$ algorithm (Azar et al., 2017) tailored for the federated learning framework. We prove that the regret of $\texttt{Fed-UCBVI}$ scales as $\tilde{\mathcal{O}}(\sqrt{H^3 |\mathcal{S}| |\mathcal{A}| T / M})$, with a small additional term due to heterogeneity, where $|\mathcal{S}|$ is the number of states, $|\mathcal{A}|$ is the number of actions, $H$ is the episode length, $M$ is the number of agents, and $T$ is the number of episodes. Notably, in the single-agent setting, this upper bound matches the minimax lower bound up to polylogarithmic factors, while in the multi-agent scenario, $\texttt{Fed-UCBVI}$ has linear speed-up. To conduct our analysis, we introduce a new measure of heterogeneity, which may hold independent theoretical interest. Furthermore, we show that, unlike existing federated reinforcement learning approaches, $\texttt{Fed-UCBVI}$'s communication complexity only marginally increases with the number of agents.</li>
</ul>

<h3>Title: CopRA: A Progressive LoRA Training Strategy</h3>
<ul>
<li><strong>Authors: </strong>Zhan Zhuang, Xiequn Wang, Yulong Zhang, Wei Li, Yu Zhang, Ying Wei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22911">https://arxiv.org/abs/2410.22911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22911">https://arxiv.org/pdf/2410.22911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22911]] CopRA: A Progressive LoRA Training Strategy(https://arxiv.org/abs/2410.22911)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a parameter-efficient technique for rapidly fine-tuning foundation models. In standard LoRA training dynamics, models tend to quickly converge to a local optimum near the initialization. However, this local optimum may not be ideal for out-of-distribution data or tasks such as merging and pruning. In this work, we propose a novel progressive training strategy for LoRA with random layer dropping. This strategy also optimizes the Shapley value of LoRA parameters in each layer, treating each layer as a player in a cooperative game. We refer to this method as Cooperative LoRA (CopRA). Our experimental results demonstrate that parameters trained with CopRA exhibit linear mode connectivity, which enables efficient model merging. This also paves the way for federated learning and multi-task learning via LoRA merging. Additionally, by optimizing the Shapley value, CopRA shows superior performance in pruning tasks.</li>
</ul>

<h3>Title: Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration</h3>
<ul>
<li><strong>Authors: </strong>Yanchu Guan, Dong Wang, Yan Wang, Haiqing Wang, Renen Sun, Chenyi Zhuang, Jinjie Gu, Zhixuan Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22916">https://arxiv.org/abs/2410.22916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22916">https://arxiv.org/pdf/2410.22916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22916]] Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration(https://arxiv.org/abs/2410.22916)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autonomous mobile app interaction has become increasingly important with growing complexity of mobile applications. Developing intelligent agents that can effectively navigate and interact with mobile apps remains a significant challenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent (EBC-LLMAgent), a novel approach that combines large language models (LLMs) with behavior cloning by learning demonstrations to create intelligent and explainable agents for autonomous mobile app interaction. EBC-LLMAgent consists of three core modules: Demonstration Encoding, Code Generation, and UI Mapping, which work synergistically to capture user demonstrations, generate executable codes, and establish accurate correspondence between code and UI elements. We introduce the Behavior Cloning Chain Fusion technique to enhance the generalization capabilities of the agent. Extensive experiments on five popular mobile applications from diverse domains demonstrate the superior performance of EBC-LLMAgent, achieving high success rates in task completion, efficient generalization to unseen scenarios, and the generation of meaningful explanations.</li>
</ul>

<h3>Title: Simulation-Free Training of Neural ODEs on Paired Data</h3>
<ul>
<li><strong>Authors: </strong>Semin Kim, Jaehoon Yoo, Jinwoo Kim, Yeonwoo Cha, Saehoon Kim, Seunghoon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22918">https://arxiv.org/abs/2410.22918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22918">https://arxiv.org/pdf/2410.22918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22918]] Simulation-Free Training of Neural ODEs on Paired Data(https://arxiv.org/abs/2410.22918)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we investigate a method for simulation-free training of Neural Ordinary Differential Equations (NODEs) for learning deterministic mappings between paired data. Despite the analogy of NODEs as continuous-depth residual networks, their application in typical supervised learning tasks has not been popular, mainly due to the large number of function evaluations required by ODE solvers and numerical instability in gradient estimation. To alleviate this problem, we employ the flow matching framework for simulation-free training of NODEs, which directly regresses the parameterized dynamics function to a predefined target velocity field. Contrary to generative tasks, however, we show that applying flow matching directly between paired data can often lead to an ill-defined flow that breaks the coupling of the data pairs (e.g., due to crossing trajectories). We propose a simple extension that applies flow matching in the embedding space of data pairs, where the embeddings are learned jointly with the dynamic function to ensure the validity of the flow which is also easier to learn. We demonstrate the effectiveness of our method on both regression and classification tasks, where our method outperforms existing NODEs with a significantly lower number of function evaluations. The code is available at this https URL.</li>
</ul>

<h3>Title: High-Fidelity Document Stain Removal via A Large-Scale Real-World Dataset and A Memory-Augmented Transformer</h3>
<ul>
<li><strong>Authors: </strong>Mingxian Li, Hao Sun, Yingtie Lei, Xiaofeng Zhang, Yihang Dong, Yilin Zhou, Zimeng Li, Xuhang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22922">https://arxiv.org/abs/2410.22922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22922">https://arxiv.org/pdf/2410.22922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22922]] High-Fidelity Document Stain Removal via A Large-Scale Real-World Dataset and A Memory-Augmented Transformer(https://arxiv.org/abs/2410.22922)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Document images are often degraded by various stains, significantly impacting their readability and hindering downstream applications such as document digitization and analysis. The absence of a comprehensive stained document dataset has limited the effectiveness of existing document enhancement methods in removing stains while preserving fine-grained details. To address this challenge, we construct StainDoc, the first large-scale, high-resolution ($2145\times2245$) dataset specifically designed for document stain removal. StainDoc comprises over 5,000 pairs of stained and clean document images across multiple scenes. This dataset encompasses a diverse range of stain types, severities, and document backgrounds, facilitating robust training and evaluation of document stain removal algorithms. Furthermore, we propose StainRestorer, a Transformer-based document stain removal approach. StainRestorer employs a memory-augmented Transformer architecture that captures hierarchical stain representations at part, instance, and semantic levels via the DocMemory module. The Stain Removal Transformer (SRTransformer) leverages these feature representations through a dual attention mechanism: an enhanced spatial attention with an expanded receptive field, and a channel attention captures channel-wise feature importance. This combination enables precise stain removal while preserving document content integrity. Extensive experiments demonstrate StainRestorer's superior performance over state-of-the-art methods on the StainDoc dataset and its variants StainDoc\_Mark and StainDoc\_Seal, establishing a new benchmark for document stain removal. Our work highlights the potential of memory-augmented Transformers for this task and contributes a valuable dataset to advance future research.</li>
</ul>

<h3>Title: Multi-Agent Large Language Models for Conversational Task-Solving</h3>
<ul>
<li><strong>Authors: </strong>Jonas Becker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22932">https://arxiv.org/abs/2410.22932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22932">https://arxiv.org/pdf/2410.22932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22932]] Multi-Agent Large Language Models for Conversational Task-Solving(https://arxiv.org/abs/2410.22932)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>In an era where single large language models have dominated the landscape of artificial intelligence for years, multi-agent systems arise as new protagonists in conversational task-solving. While previous studies have showcased their potential in reasoning tasks and creative endeavors, an analysis of their limitations concerning the conversational paradigms and the impact of individual agents is missing. It remains unascertained how multi-agent discussions perform across tasks of varying complexity and how the structure of these conversations influences the process. To fill that gap, this work systematically evaluates multi-agent systems across various discussion paradigms, assessing their strengths and weaknesses in both generative tasks and question-answering tasks. Alongside the experiments, I propose a taxonomy of 20 multi-agent research studies from 2022 to 2024, followed by the introduction of a framework for deploying multi-agent LLMs in conversational task-solving. I demonstrate that while multi-agent systems excel in complex reasoning tasks, outperforming a single model by leveraging expert personas, they fail on basic tasks. Concretely, I identify three challenges that arise: 1) While longer discussions enhance reasoning, agents fail to maintain conformity to strict task requirements, which leads to problem drift, making shorter conversations more effective for basic tasks. 2) Prolonged discussions risk alignment collapse, raising new safety concerns for these systems. 3) I showcase discussion monopolization through long generations, posing the problem of fairness in decision-making for tasks like summarization. This work uncovers both the potential and challenges that arise with multi-agent interaction and varying conversational paradigms, providing insights into how future research could improve the efficiency, performance, and safety of multi-agent LLMs.</li>
</ul>

<h3>Title: Focus On This, Not That! Steering LLMs With Adaptive Feature Specification</h3>
<ul>
<li><strong>Authors: </strong>Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H.S. Torr, Francesco Pinto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22944">https://arxiv.org/abs/2410.22944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22944">https://arxiv.org/pdf/2410.22944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22944]] Focus On This, Not That! Steering LLMs With Adaptive Feature Specification(https://arxiv.org/abs/2410.22944)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Despite the success of Instruction Tuning (IT) in training large language models (LLMs) to perform arbitrary user-specified tasks, these models often still leverage spurious or biased features learned from their training data, leading to undesired behaviours when deploying them in new contexts. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across several experimental settings, we show that focus-tuned models can be adaptively steered by focusing on different features at inference-time: for instance, robustness can be improved by focusing on task-causal features and ignoring spurious features, and social bias can be mitigated by ignoring demographic categories. Furthermore, FIT can steer behaviour in new contexts, generalising under distribution shift and to new unseen features at inference time, and thereby facilitating more robust, fair, and controllable LLM applications in real-world environments.</li>
</ul>

<h3>Title: Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation</h3>
<ul>
<li><strong>Authors: </strong>Wei Dong, Yuan Sun, Yiting Yang, Xing Zhang, Zhijun Lin, Qingsen Yan, Haokui Zhang, Peng Wang, Yang Yang, Hengtao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22952">https://arxiv.org/abs/2410.22952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22952">https://arxiv.org/pdf/2410.22952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22952]] Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation(https://arxiv.org/abs/2410.22952)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViTs) involves adapting the model to downstream tasks by learning a low-rank adaptation matrix. This matrix is decomposed into a product of down-projection and up-projection matrices, with the bottleneck dimensionality being crucial for reducing the number of learnable parameters, as exemplified by prevalent methods like LoRA and Adapter. However, these low-rank strategies typically employ a fixed bottleneck dimensionality, which limits their flexibility in handling layer-wise variations. To address this limitation, we propose a novel PEFT approach inspired by Singular Value Decomposition (SVD) for representing the adaptation matrix. SVD decomposes a matrix into the product of a left unitary matrix, a diagonal matrix of scaling values, and a right unitary matrix. We utilize Householder transformations to construct orthogonal matrices that efficiently mimic the unitary matrices, requiring only a vector. The diagonal values are learned in a layer-wise manner, allowing them to flexibly capture the unique properties of each layer. This approach enables the generation of adaptation matrices with varying ranks across different layers, providing greater flexibility in adapting pre-trained models. Experiments on standard downstream vision tasks demonstrate that our method achieves promising fine-tuning performance.</li>
</ul>

<h3>Title: Retrieval-Augmented Generation with Estimation of Source Reliability</h3>
<ul>
<li><strong>Authors: </strong>Jeongyeon Hwang, Junyoung Park, Hyejin Park, Sangdon Park, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22954">https://arxiv.org/abs/2410.22954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22954">https://arxiv.org/pdf/2410.22954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22954]] Retrieval-Augmented Generation with Estimation of Source Reliability(https://arxiv.org/abs/2410.22954)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) addresses key limitations of large language models (LLMs), such as hallucinations and outdated knowledge, by incorporating external databases. These databases typically consult multiple sources to encompass up-to-date and various information. However, standard RAG methods often overlook the heterogeneous source reliability in the multi-source database and retrieve documents solely based on relevance, making them prone to propagating misinformation. To address this, we propose Reliability-Aware RAG (RA-RAG) which estimates the reliability of multiple sources and incorporates this information into both retrieval and aggregation processes. Specifically, it iteratively estimates source reliability and true answers for a set of queries with no labelling. Then, it selectively retrieves relevant documents from a few of reliable sources and aggregates them using weighted majority voting, where the selective retrieval ensures scalability while not compromising the performance. We also introduce a benchmark designed to reflect real-world scenarios with heterogeneous source reliability and demonstrate the effectiveness of RA-RAG compared to a set of baselines.</li>
</ul>

<h3>Title: A Study of Secure Algorithms for Vertical Federated Learning: Take Secure Logistic Regression as an Example</h3>
<ul>
<li><strong>Authors: </strong>Huan-Chih Wang, Ja-Ling Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22960">https://arxiv.org/abs/2410.22960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22960">https://arxiv.org/pdf/2410.22960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22960]] A Study of Secure Algorithms for Vertical Federated Learning: Take Secure Logistic Regression as an Example(https://arxiv.org/abs/2410.22960)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>After entering the era of big data, more and more companies build services with machine learning techniques. However, it is costly for companies to collect data and extract helpful handcraft features on their own. Although it is a way to combine with other companies' data for boosting the model's performance, this approach may be prohibited by laws. In other words, finding the balance between sharing data with others and keeping data from privacy leakage is a crucial topic worthy of close attention. This paper focuses on distributed data and conducts secure model training tasks on a vertical federated learning scheme. Here, secure implies that the whole process is executed in the encrypted domain. Therefore, the privacy concern is released.</li>
</ul>

<h3>Title: Dynamic Threshold-based Two-layer Online Unsupervised Anomaly Detector</h3>
<ul>
<li><strong>Authors: </strong>Yachao Yuan, Yu Huang, Yali Yuan, Jin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22967">https://arxiv.org/abs/2410.22967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22967">https://arxiv.org/pdf/2410.22967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22967]] Dynamic Threshold-based Two-layer Online Unsupervised Anomaly Detector(https://arxiv.org/abs/2410.22967)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, interpretability</a></li>
<li><strong>Abstract: </strong>The proliferation of the Internet of Things (IoT) has heightened the vulnerability to cyber threats, making it imperative to develop Anomaly Detection Systems (ADSs) capable of adapting to emerging or novel attacks. Prior research has predominantly concentrated on offline unsupervised learning techniques to protect ADSs, which are impractical for real-world applications. Furthermore, these studies often rely heavily on the assumption of known legitimate behaviors and fall short of meeting the interpretability requirements in security contexts, thereby hindering their practical adoption. In response, this paper introduces Adaptive NAD, a comprehensive framework aimed at enhancing and interpreting online unsupervised anomaly detection within security domains. We propose an interpretable two-layer anomaly detection approach that generates dependable, high-confidence pseudo-labels. Subsequently, we incorporate an online learning mechanism that updates Adaptive NAD using an innovative threshold adjustment method to accommodate new threats. Experimental findings reveal that Adaptive NAD surpasses state-of-the-art solutions by achieving improvements of over 5.4% and 23.0% in SPAUC on the CIC-Darknet2020 and CIC-DoHBrw-2020 datasets, respectively. The code for Adaptive NAD is publicly available at this https URL.</li>
</ul>

<h3>Title: Private Synthetic Text Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Ochs, Ivan Habernal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22971">https://arxiv.org/abs/2410.22971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22971">https://arxiv.org/pdf/2410.22971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22971]] Private Synthetic Text Generation with Diffusion Models(https://arxiv.org/abs/2410.22971)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>How capable are diffusion models of generating synthetics texts? Recent research shows their strengths, with performance reaching that of auto-regressive LLMs. But are they also good in generating synthetic data if the training was under differential privacy? Here the evidence is missing, yet the promises from private image generation look strong. In this paper we address this open question by extensive experiments. At the same time, we critically assess (and reimplement) previous works on synthetic private text generation with LLMs and reveal some unmet assumptions that might have led to violating the differential privacy guarantees. Our results partly contradict previous non-private findings and show that fully open-source LLMs outperform diffusion models in the privacy regime. Our complete source codes, datasets, and experimental setup is publicly available to foster future research.</li>
</ul>

<h3>Title: LumiSculpt: A Consistency Lighting Control Network for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Zhang, Dandan Zheng, Biao Gong, Jingdong Chen, Ming Yang, Weiming Dong, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22979">https://arxiv.org/abs/2410.22979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22979">https://arxiv.org/pdf/2410.22979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22979]] LumiSculpt: A Consistency Lighting Control Network for Video Generation(https://arxiv.org/abs/2410.22979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Lighting plays a pivotal role in ensuring the naturalness of video generation, significantly influencing the aesthetic quality of the generated content. However, due to the deep coupling between lighting and the temporal features of videos, it remains challenging to disentangle and model independent and coherent lighting attributes, limiting the ability to control lighting in video generation. In this paper, inspired by the established controllable T2I models, we propose LumiSculpt, which, for the first time, enables precise and consistent lighting control in T2V generation this http URL equips the video generation with strong interactive capabilities, allowing the input of custom lighting reference image sequences. Furthermore, the core learnable plug-and-play module of LumiSculpt facilitates remarkable control over lighting intensity, position, and trajectory in latent video diffusion models based on the advanced DiT this http URL, to effectively train LumiSculpt and address the issue of insufficient lighting data, we construct LumiHuman, a new lightweight and flexible dataset for portrait lighting of images and videos. Experimental results demonstrate that LumiSculpt achieves precise and high-quality lighting control in video generation.</li>
</ul>

<h3>Title: DisenTS: Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhiding Liu, Jiqian Yang, Qingyang Mao, Yuze Zhao, Mingyue Cheng, Zhi Li, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22981">https://arxiv.org/abs/2410.22981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22981">https://arxiv.org/pdf/2410.22981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22981]] DisenTS: Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting(https://arxiv.org/abs/2410.22981)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multivariate time series forecasting plays a crucial role in various real-world applications. Significant efforts have been made to integrate advanced network architectures and training strategies that enhance the capture of temporal dependencies, thereby improving forecasting accuracy. On the other hand, mainstream approaches typically utilize a single unified model with simplistic channel-mixing embedding or cross-channel attention operations to account for the critical intricate inter-channel dependencies. Moreover, some methods even trade capacity for robust prediction based on the channel-independent assumption. Nonetheless, as time series data may display distinct evolving patterns due to the unique characteristics of each channel (including multiple strong seasonalities and trend changes), the unified modeling methods could yield suboptimal results. To this end, we propose DisenTS, a tailored framework for modeling disentangled channel evolving patterns in general multivariate time series forecasting. The central idea of DisenTS is to model the potential diverse patterns within the multivariate time series data in a decoupled manner. Technically, the framework employs multiple distinct forecasting models, each tasked with uncovering a unique evolving pattern. To guide the learning process without supervision of pattern partition, we introduce a novel Forecaster Aware Gate (FAG) module that generates the routing signals adaptively according to both the forecasters' states and input series' characteristics. The forecasters' states are derived from the Linear Weight Approximation (LWA) strategy, which quantizes the complex deep neural networks into compact matrices. Additionally, the Similarity Constraint (SC) is further proposed to guide each model to specialize in an underlying pattern by minimizing the mutual information between the representations.</li>
</ul>

<h3>Title: Dual-Optimized Adaptive Graph Reconstruction for Multi-View Graph Clustering</h3>
<ul>
<li><strong>Authors: </strong>Zichen Wen, Tianyi Wu, Yazhou Ren, Yawen Ling, Chenhang Cui, Xiaorong Pu, Lifang He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22983">https://arxiv.org/abs/2410.22983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22983">https://arxiv.org/pdf/2410.22983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22983]] Dual-Optimized Adaptive Graph Reconstruction for Multi-View Graph Clustering(https://arxiv.org/abs/2410.22983)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Multi-view clustering is an important machine learning task for multi-media data, encompassing various domains such as images, videos, and texts. Moreover, with the growing abundance of graph data, the significance of multi-view graph clustering (MVGC) has become evident. Most existing methods focus on graph neural networks (GNNs) to extract information from both graph structure and feature data to learn distinguishable node representations. However, traditional GNNs are designed with the assumption of homophilous graphs, making them unsuitable for widely prevalent heterophilous graphs. Several techniques have been introduced to enhance GNNs for heterophilous graphs. While these methods partially mitigate the heterophilous graph issue, they often neglect the advantages of traditional GNNs, such as their simplicity, interpretability, and efficiency. In this paper, we propose a novel multi-view graph clustering method based on dual-optimized adaptive graph reconstruction, named DOAGC. It mainly aims to reconstruct the graph structure adapted to traditional GNNs to deal with heterophilous graph issues while maintaining the advantages of traditional GNNs. Specifically, we first develop an adaptive graph reconstruction mechanism that accounts for node correlation and original structural information. To further optimize the reconstruction graph, we design a dual optimization strategy and demonstrate the feasibility of our optimization strategy through mutual information theory. Numerous experiments demonstrate that DOAGC effectively mitigates the heterophilous graph problem.</li>
</ul>

<h3>Title: Higher-order Cross-structural Embedding Model for Time Series Analysis</h3>
<ul>
<li><strong>Authors: </strong>Guancen Lin, Cong Shen, Aijing Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22984">https://arxiv.org/abs/2410.22984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22984">https://arxiv.org/pdf/2410.22984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22984]] Higher-order Cross-structural Embedding Model for Time Series Analysis(https://arxiv.org/abs/2410.22984)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Time series analysis has gained significant attention due to its critical applications in diverse fields such as healthcare, finance, and sensor networks. The complexity and non-stationarity of time series make it challenging to capture the interaction patterns across different timestamps. Current approaches struggle to model higher-order interactions within time series, and focus on learning temporal or spatial dependencies separately, which limits performance in downstream tasks. To address these gaps, we propose Higher-order Cross-structural Embedding Model for Time Series (High-TS), a novel framework that jointly models both temporal and spatial perspectives by combining multiscale Transformer with Topological Deep Learning (TDL). Meanwhile, High-TS utilizes contrastive learning to integrate these two structures for generating robust and discriminative representations. Extensive experiments show that High-TS outperforms state-of-the-art methods in various time series tasks and demonstrate the importance of higher-order cross-structural information in improving model performance.</li>
</ul>

<h3>Title: VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jingkun Ma, Runzhe Zhan, Derek F. Wong, Yang Li, Di Sun, Hou Pong Chan, Lidia S. Chao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.22995">https://arxiv.org/abs/2410.22995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.22995">https://arxiv.org/pdf/2410.22995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.22995]] VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning(https://arxiv.org/abs/2410.22995)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although previous research on large language models (LLMs) and large multi-modal models (LMMs) has systematically explored mathematical problem-solving (MPS) within visual contexts, the analysis of how these models process visual information during problem-solving remains insufficient. To address this gap, we present VisAidMath, a benchmark for evaluating the MPS process related to visual information. We follow a rigorous data curation pipeline involving both automated processes and manual annotations to ensure data quality and reliability. Consequently, this benchmark includes 1,200 challenging problems from various mathematical branches, vision-aid formulations, and difficulty levels, collected from diverse sources such as textbooks, examination papers, and Olympiad problems. Based on the proposed benchmark, we conduct comprehensive evaluations on ten mainstream LLMs and LMMs, highlighting deficiencies in the visual-aided reasoning process. For example, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning task, even with a drop of 2 points when provided with golden visual aids. In-depth analysis reveals that the main cause of deficiencies lies in hallucination regarding the implicit visual reasoning process, shedding light on future research directions in the visual-aided MPS process.</li>
</ul>

<h3>Title: \textsc{Long$^2$RAG}: Evaluating Long-Context \& Long-Form Retrieval-Augmented Generation with Key Point Recall</h3>
<ul>
<li><strong>Authors: </strong>Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23000">https://arxiv.org/abs/2410.23000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23000">https://arxiv.org/pdf/2410.23000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23000]] \textsc{Long$^2$RAG}: Evaluating Long-Context \& Long-Form Retrieval-Augmented Generation with Key Point Recall(https://arxiv.org/abs/2410.23000)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is a promising approach to address the limitations of fixed knowledge in large language models (LLMs). However, current benchmarks for evaluating RAG systems suffer from two key deficiencies: (1) they fail to adequately measure LLMs' capability in handling \emph{long-context retrieval} due to a lack of datasets that reflect the characteristics of retrieved documents, and (2) they lack a comprehensive evaluation method for assessing LLMs' ability to generate \emph{long-form responses} that effectively exploits retrieved information. To address these shortcomings, we introduce the \textsc{Long$^2$RAG} benchmark and the Key Point Recall (\textit{KPR}) metric. \textsc{Long$^2$RAG} comprises 280 questions spanning 10 domains and across 8 question categories, each associated with 5 retrieved documents with an average length of 2,444 words. \textit{KPR} evaluates the extent to which LLMs incorporate key points extracted from the retrieved documents into their generated responses, providing a more nuanced assessment of their ability to exploit retrieved information. Our dataset and scripts are available at this https URL.</li>
</ul>

<h3>Title: Scoring Rules and Calibration for Imprecise Probabilities</h3>
<ul>
<li><strong>Authors: </strong>Christian Fröhlich, Robert C. Williamson</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23001">https://arxiv.org/abs/2410.23001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23001">https://arxiv.org/pdf/2410.23001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23001]] Scoring Rules and Calibration for Imprecise Probabilities(https://arxiv.org/abs/2410.23001)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>What does it mean to say that, for example, the probability for rain tomorrow is between 20% and 30%? The theory for the evaluation of precise probabilistic forecasts is well-developed and is grounded in the key concepts of proper scoring rules and calibration. For the case of imprecise probabilistic forecasts (sets of probabilities), such theory is still lacking. In this work, we therefore generalize proper scoring rules and calibration to the imprecise case. We develop these concepts as relative to data models and decision problems. As a consequence, the imprecision is embedded in a clear context. We establish a close link to the paradigm of (group) distributional robustness and in doing so provide new insights for it. We argue that proper scoring rules and calibration serve two distinct goals, which are aligned in the precise case, but intriguingly are not necessarily aligned in the imprecise case. The concept of decision-theoretic entropy plays a key role for both goals. Finally, we demonstrate the theoretical insights in machine learning practice, in particular we illustrate subtle pitfalls relating to the choice of loss function in distributional robustness.</li>
</ul>

<h3>Title: Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback</h3>
<ul>
<li><strong>Authors: </strong>Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, Brandon Amos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23022">https://arxiv.org/abs/2410.23022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23022">https://arxiv.org/pdf/2410.23022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23022]] Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback(https://arxiv.org/abs/2410.23022)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples; or are limited to reward functions expressible by compact code, which may require source code and have difficulty capturing nuanced semantics; or require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets nor source code. We make our code available at \url{URL} (coming soon).</li>
</ul>

<h3>Title: Offline Reinforcement Learning and Sequence Modeling for Downlink Link Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Samuele Peri, Alessio Russo, Gabor Fodor, Pablo Soldati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23031">https://arxiv.org/abs/2410.23031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23031">https://arxiv.org/pdf/2410.23031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23031]] Offline Reinforcement Learning and Sequence Modeling for Downlink Link Adaptation(https://arxiv.org/abs/2410.23031)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Contemporary radio access networks employ link adaption (LA) algorithms to optimize the modulation and coding schemes to adapt to the prevailing propagation conditions and are near-optimal in terms of the achieved spectral efficiency. LA is a challenging task in the presence of mobility, fast fading, and imperfect channel quality information and limited knowledge of the receiver characteristics at the transmitter, which render model-based LA algorithms complex and suboptimal. Model-based LA is especially difficult as connected user equipment devices become increasingly heterogeneous in terms of receiver capabilities, antenna configurations and hardware characteristics. Recognizing these difficulties, previous works have proposed reinforcement learning (RL) for LA, which faces deployment difficulties due to their potential negative impacts on live performance. To address this challenge, this paper considers offline RL to learn LA policies from data acquired in live networks with minimal or no intrusive effects on the network operation. We propose three LA designs based on batch-constrained deep Q-learning, conservative Q-learning, and decision transformers, showing that offline RL algorithms can achieve performance of state-of-the-art online RL methods when data is collected with a proper behavioral policy.</li>
</ul>

<h3>Title: Toward Understanding In-context vs. In-weight Learning</h3>
<ul>
<li><strong>Authors: </strong>Bryan Chan, Xinyi Chen, András György, Dale Schuurmans</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23042">https://arxiv.org/abs/2410.23042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23042">https://arxiv.org/pdf/2410.23042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23042]] Toward Understanding In-context vs. In-weight Learning(https://arxiv.org/abs/2410.23042)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>It has recently been demonstrated empirically that in-context learning emerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training. We provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning. We do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor. Through a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge. These theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results. We then extend the study to a full large language model, showing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour.</li>
</ul>

<h3>Title: Controlling Language and Diffusion Models by Transporting Activations</h3>
<ul>
<li><strong>Authors: </strong>Pau Rodriguez, Arno Blaas, Michal Klein, Luca Zappella, Nicholas Apostoloff, Marco Cuturi, Xavier Suau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23054">https://arxiv.org/abs/2410.23054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23054">https://arxiv.org/pdf/2410.23054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23054]] Controlling Language and Diffusion Models by Transporting Activations(https://arxiv.org/abs/2410.23054)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.</li>
</ul>

<h3>Title: CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised Models</h3>
<ul>
<li><strong>Authors: </strong>Aymene Mohammed Bouayed, Samuel Deslauriers-Gauthier, Adrian Iaccovelli, David Naccache</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23072">https://arxiv.org/abs/2410.23072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23072">https://arxiv.org/pdf/2410.23072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23072]] CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised Models(https://arxiv.org/abs/2410.23072)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Interpreting the decisions of Convolutional Neural Networks (CNNs) is essential for understanding their behavior, yet explainability remains a significant challenge, particularly for self-supervised models. Most existing methods for generating saliency maps rely on ground truth labels, restricting their use to supervised tasks. EigenCAM is the only notable label-independent alternative, leveraging Singular Value Decomposition to generate saliency maps applicable across CNN models, but it does not fully exploit the tensorial structure of feature maps. In this work, we introduce the Tucker Saliency Map (TSM) method, which applies Tucker tensor decomposition to better capture the inherent structure of feature maps, producing more accurate singular vectors and values. These are used to generate high-fidelity saliency maps, effectively highlighting objects of interest in the input. We further extend EigenCAM and TSM into multivector variants -Multivec-EigenCAM and Multivector Tucker Saliency Maps (MTSM)- which utilize all singular vectors and values, further improving saliency map quality. Quantitative evaluations on supervised classification models demonstrate that TSM, Multivec-EigenCAM, and MTSM achieve competitive performance with label-dependent methods. Moreover, TSM enhances explainability by approximately 50% over EigenCAM for both supervised and self-supervised models. Multivec-EigenCAM and MTSM further advance state-of-the-art explainability performance on self-supervised models, with MTSM achieving the best results.</li>
</ul>

<h3>Title: BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Junqi Zhao, Zhijin Fang, Shu Li, Shaohui Yang, Shichao He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23079">https://arxiv.org/abs/2410.23079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23079">https://arxiv.org/pdf/2410.23079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23079]] BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference(https://arxiv.org/abs/2410.23079)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are essential in natural language processing but often struggle with inference speed and computational efficiency, limiting real-time deployment. The key-value (KV) cache mechanism reduces computational overhead in transformer models, but challenges in maintaining contextual understanding remain. In this paper, we propose BUZZ, a novel KV caching algorithm that leverages structured contextual information to minimize cache memory usage while enhancing inference speed. BUZZ employs a beehive-structured sparse cache, incorporating a sliding window to capture recent information and dynamically segmenting historical tokens into chunks to prioritize important tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets: CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ (1) reduces cache memory usage by $\textbf{2.5}\times$ in LLM inference while maintaining over 99% accuracy in long-text summarization, and (2) surpasses state-of-the-art performance in multi-document question answering by $\textbf{7.69%}$ under the same memory limit, where full cache methods encounter out-of-memory issues. Additionally, BUZZ achieves significant inference speedup with a $\log{n}$ time complexity. The code is available at this https URL.</li>
</ul>

<h3>Title: Developing a Self-Explanatory Transformer</h3>
<ul>
<li><strong>Authors: </strong>Rasha Karakchi, Ryan Karbowniczak</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23083">https://arxiv.org/abs/2410.23083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23083">https://arxiv.org/pdf/2410.23083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23083]] Developing a Self-Explanatory Transformer(https://arxiv.org/abs/2410.23083)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, transformer</a></li>
<li><strong>Abstract: </strong>While IoT devices provide significant benefits, their rapid growth results in larger data volumes, increased complexity, and higher security risks. To manage these issues, techniques like encryption, compression, and mapping are used to process data efficiently and securely. General-purpose and AI platforms handle these tasks well, but mapping in natural language processing is often slowed by training times. This work explores a self-explanatory, training-free mapping transformer based on non-deterministic finite automata, designed for Field-Programmable Gate Arrays (FPGAs). Besides highlighting the advantages of this proposed approach in providing real-time, cost-effective processing and dataset-loading, we also address the challenges and considerations for enhancing the design in future iterations.</li>
</ul>

<h3>Title: S3PT: Scene Semantics and Structure Guided Clustering to Boost Self-Supervised Pre-Training for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Maciej K. Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, Ravi Kiran, Senthil Yogamani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23085">https://arxiv.org/abs/2410.23085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23085">https://arxiv.org/pdf/2410.23085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23085]] S3PT: Scene Semantics and Structure Guided Clustering to Boost Self-Supervised Pre-Training for Autonomous Driving(https://arxiv.org/abs/2410.23085)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent self-supervised clustering-based pre-training techniques like DINO and Cribo have shown impressive results for downstream detection and segmentation tasks. However, real-world applications such as autonomous driving face challenges with imbalanced object class and size distributions and complex scene geometries. In this paper, we propose S3PT a novel scene semantics and structure guided clustering to provide more scene-consistent objectives for self-supervised training. Specifically, our contributions are threefold: First, we incorporate semantic distribution consistent clustering to encourage better representation of rare classes such as motorcycles or animals. Second, we introduce object diversity consistent spatial clustering, to handle imbalanced and diverse object sizes, ranging from large background areas to small objects such as pedestrians and traffic signs. Third, we propose a depth-guided spatial clustering to regularize learning based on geometric information of the scene, thus further refining region separation on the feature level. Our learned representations significantly improve performance in downstream semantic segmentation and 3D object detection tasks on the nuScenes, nuImages, and Cityscapes datasets and show promising domain translation properties.</li>
</ul>

<h3>Title: PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via Existing MLLM Structures</h3>
<ul>
<li><strong>Authors: </strong>Tianxiang Wu, Minxin Nie, Ziqiang Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23089">https://arxiv.org/abs/2410.23089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23089">https://arxiv.org/pdf/2410.23089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23089]] PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via Existing MLLM Structures(https://arxiv.org/abs/2410.23089)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Multimodal Large Language Models (MLLMs) have activated the capabilitiesof Large Language Models (LLMs) in solving visual-language tasks by integratingvisual information. The prevailing approach in existing MLLMs involvesemploying an image encoder to extract visual features, converting thesefeatures into visual tokens via an adapter, and then integrating them with theprompt into the LLM. However, because the process of image encoding isprompt-agnostic, the extracted visual features only provide a coarsedescription of the image, impossible to focus on the requirements of theprompt. On one hand, it is easy for image features to lack information aboutthe prompt-specified objects, resulting in unsatisfactory responses. On theother hand, the visual features contain a large amount of irrelevantinformation, which not only increases the burden on memory but also worsens thegeneration effectiveness. To address the aforementioned issues, we propose\textbf{PIP-MM}, a framework that \textbf{P}re-\textbf{I}ntegrates\textbf{P}rompt information into the visual encoding process using existingmodules of MLLMs. Specifically, We utilize the frozen LLM in the MLLM tovectorize the input prompt, which summarizes the requirements of the this http URL, we input the prompt vector into our trained Multi-Layer Perceptron (MLP)to align with the visual input requirements, and subsequently replace the classembedding in the image encoder. Since our model only requires adding atrainable MLP, it can be applied to any MLLM. To validate the effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks. Automated evaluationmetrics and manual assessments demonstrate the strong performance of this http URL noteworthy is that our model maintains excellent generationresults even when half of the visual tokens are reduced.</li>
</ul>

<h3>Title: CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense</h3>
<ul>
<li><strong>Authors: </strong>Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23091">https://arxiv.org/abs/2410.23091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23091">https://arxiv.org/pdf/2410.23091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23091]] CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense(https://arxiv.org/abs/2410.23091)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition Benchmark).</li>
</ul>

<h3>Title: Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Dong Shu, Mengnan Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23099">https://arxiv.org/abs/2410.23099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23099">https://arxiv.org/pdf/2410.23099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23099]] Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning(https://arxiv.org/abs/2410.23099)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning can help Large Language Models (LLMs) to adapt new tasks without additional training. However, this performance heavily depends on the quality of the demonstrations, driving research into effective demonstration selection algorithms to optimize this process. These algorithms assist users in selecting the best $k$ input-label pairs (demonstration examples) based on a given test input, enabling LLMs to in-context learn the relationship between the provided examples and the test inputs. Despite all the proposed demonstration selection algorithms, their efficiency and effectiveness remain unclear. This lack of clarity make it difficult to apply these algorithms in real-world scenarios and poses challenges for future research aimed at developing improved methods. This paper revisits six proposed algorithms, evaluating them on five datasets from both efficiency and effectiveness perspectives. Our experiments reveal significant variations in algorithm performance across different tasks, with some methods struggling to outperform random selection in certain scenarios. We also find that increasing the number of demonstrations does not always lead to better performance, and that there are often trade-offs between accuracy and computational efficiency. Our code is available at this https URL.</li>
</ul>

<h3>Title: Automated Image-Based Identification and Consistent Classification of Fire Patterns with Quantitative Shape Analysis and Spatial Location Identification</h3>
<ul>
<li><strong>Authors: </strong>Pengkun Liu, Shuna Ni, Stanislav I. Stoliarov, Pingbo Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23105">https://arxiv.org/abs/2410.23105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23105">https://arxiv.org/pdf/2410.23105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23105]] Automated Image-Based Identification and Consistent Classification of Fire Patterns with Quantitative Shape Analysis and Spatial Location Identification(https://arxiv.org/abs/2410.23105)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Fire patterns, consisting of fire effects that offer insights into fire behavior and origin, are traditionally classified based on investigators' visual observations, leading to subjective interpretations. This study proposes a framework for quantitative fire pattern classification to support fire investigators, aiming for consistency and accuracy. The framework integrates four components. First, it leverages human-computer interaction to extract fire patterns from surfaces, combining investigator expertise with computational analysis. Second, it employs an aspect ratio-based random forest model to classify fire pattern shapes. Third, fire scene point cloud segmentation enables precise identification of fire-affected areas and the mapping of 2D fire patterns to 3D scenes. Lastly, spatial relationships between fire patterns and indoor elements support an interpretation of the fire scene. These components provide a method for fire pattern analysis that synthesizes qualitative and quantitative data. The framework's classification results achieve 93% precision on synthetic data and 83% on real fire patterns.</li>
</ul>

<h3>Title: Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Bazzaz, Seth Cooper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23108">https://arxiv.org/abs/2410.23108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23108">https://arxiv.org/pdf/2410.23108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23108]] Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models(https://arxiv.org/abs/2410.23108)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) are unsupervised models designed to learn and replicate a target distribution. The vanilla versions of these models can be extended to more controllable models. Conditional Generative Adversarial Networks (CGANs) extend vanilla GANs by conditioning both the generator and discriminator on some additional information (labels). Controllable models based on complementary learning, such as Rumi-GAN, have been introduced. Rumi-GANs leverage negative examples to enhance the generator's ability to learn positive examples. We evaluate the performance of two controllable GAN variants, CGAN and Rumi-GAN, in generating game levels targeting specific constraints of interest: playability and controllability. This evaluation is conducted under two scenarios: with and without the inclusion of negative examples. The goal is to determine whether incorporating negative examples helps the GAN models avoid generating undesirable outputs. Our findings highlight the strengths and weaknesses of each method in enforcing the generation of specific conditions when generating outputs based on given positive and negative examples.</li>
</ul>

<h3>Title: Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in Federated Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Navyansh Mahla, Ganesh Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23111">https://arxiv.org/abs/2410.23111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23111">https://arxiv.org/pdf/2410.23111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23111]] Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in Federated Fine-Tuning of Large Language Models(https://arxiv.org/abs/2410.23111)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in task generalization for both text and vision data. While fine-tuning these models can significantly enhance their performance on specific downstream tasks, it often requires high-quality data that cannot be shared due to privacy concerns. Federated Learning (FL) offers a promising solution for collaborative training without direct data sharing. However, many parameter-efficient fine-tuning strategies for LLMs in FL, particularly those based on Low-Rank Adaptation (LoRA), face limitations. In this paper, we critically analyze the convergence and performance guarantees of popular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to constrained subspace learning of low-rank matrices. This limitation hinders effective fine-tuning of LLMs in federated settings. Through rigorous analytical and empirical evaluations, we demonstrate that direct weight averaging outperforms LoRA-based strategies, leading to superior performance for fine-tuned models. Our comprehensive comparison exposes inefficiencies in LoRA approaches and underscores the advantages of full-rank weight aggregation. We extend our analysis to low-rank gradient-based optimizers, such as GaLore, used during local training steps. Our findings show that GaLore is a more effective alternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities. While privacy remains paramount in FL discourse, our focus is on assessing performance outcomes of federated fine-tuned models and evaluating various FL frameworks from both theoretical and empirical perspectives. Our findings advocate reassessing the reliance on LoRA within FL contexts, paving the way for more efficient training methodologies.</li>
</ul>

<h3>Title: On Memorization of Large Language Models in Logical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, Ravi Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23123">https://arxiv.org/abs/2410.23123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23123">https://arxiv.org/pdf/2410.23123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23123]] On Memorization of Large Language Models in Logical Reasoning(https://arxiv.org/abs/2410.23123)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, our analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Michael Crawshaw, Mingrui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23131">https://arxiv.org/abs/2410.23131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23131">https://arxiv.org/pdf/2410.23131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23131]] Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis(https://arxiv.org/abs/2410.23131)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy $\mathcal{O}(\epsilon^{-2})$ communication rounds to find an $\epsilon$-stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires $\mathcal{O}(\kappa^2 \epsilon^{-4})$ communication rounds, where $\kappa$ denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\epsilon$ and $\kappa$. Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients $(N = 250)$, demonstrating the effectiveness of our algorithm under periodic client participation.</li>
</ul>

<h3>Title: Revisiting MAE pre-training for 3D medical image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Leander Maerkisch, Paul F. Jäger, Klaus Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23132">https://arxiv.org/abs/2410.23132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23132">https://arxiv.org/pdf/2410.23132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23132]] Revisiting MAE pre-training for 3D medical image segmentation(https://arxiv.org/abs/2410.23132)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the potential of vast, untapped clinical datasets, for various downstream applications that suffer from the scarcity of labeled data. While SSL has revolutionized fields like natural language processing and computer vision, their adoption in 3D medical image computing has been limited by three key pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D medical image analysis, and insufficient evaluation practices. We address these issues by i) leveraging a large-scale dataset of 44k 3D brain MRI volumes and ii) using a Residual Encoder U-Net architecture within the state-of-the-art nnU-Net framework. iii) A robust development framework, incorporating 5 development and 8 testing brain MRI segmentation datasets, allowed performance-driven design decisions to optimize the simple concept of Masked Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses previous SSL methods but also outperforms the strong nnU-Net baseline by an average of approximately 3 Dice points. Furthermore, our model demonstrates exceptional stability, achieving the highest average rank of 2 out of 7 methods, compared to the second-best method's mean rank of 3.</li>
</ul>

<h3>Title: FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Tejaswini Medi, Steffen Jung, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23142">https://arxiv.org/abs/2410.23142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23142">https://arxiv.org/pdf/2410.23142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23142]] FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training(https://arxiv.org/abs/2410.23142)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair</a></li>
<li><strong>Abstract: </strong>Deep neural networks are susceptible to adversarial attacks and common corruptions, which undermine their robustness. In order to enhance model resilience against such challenges, Adversarial Training (AT) has emerged as a prominent solution. Nevertheless, adversarial robustness is often attained at the expense of model fairness during AT, i.e., disparity in class-wise robustness of the model. While distinctive classes become more robust towards such adversaries, hard to detect classes suffer. Recently, research has focused on improving model fairness specifically for perturbed images, overlooking the accuracy of the most likely non-perturbed data. Additionally, despite their robustness against the adversaries encountered during model training, state-of-the-art adversarial trained models have difficulty maintaining robustness and fairness when confronted with diverse adversarial threats or common corruptions. In this work, we address the above concerns by introducing a novel approach called Fair Targeted Adversarial Training (FAIR-TAT). We show that using targeted adversarial attacks for adversarial training (instead of untargeted attacks) can allow for more favorable trade-offs with respect to adversarial fairness. Empirical results validate the efficacy of our approach.</li>
</ul>

<h3>Title: FoLDTree: A ULDA-Based Decision Tree Framework for Efficient Oblique Splits and Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Siyu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23147">https://arxiv.org/abs/2410.23147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23147">https://arxiv.org/pdf/2410.23147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23147]] FoLDTree: A ULDA-Based Decision Tree Framework for Efficient Oblique Splits and Feature Selection(https://arxiv.org/abs/2410.23147)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional decision trees are limited by axis-orthogonal splits, which can perform poorly when true decision boundaries are oblique. While oblique decision tree methods address this limitation, they often face high computational costs, difficulties with multi-class classification, and a lack of effective feature selection. In this paper, we introduce LDATree and FoLDTree, two novel frameworks that integrate Uncorrelated Linear Discriminant Analysis (ULDA) and Forward ULDA into a decision tree structure. These methods enable efficient oblique splits, handle missing values, support feature selection, and provide both class labels and probabilities as model outputs. Through evaluations on simulated and real-world datasets, LDATree and FoLDTree consistently outperform axis-orthogonal and other oblique decision tree methods, achieving accuracy levels comparable to the random forest. The results highlight the potential of these frameworks as robust alternatives to traditional single-tree methods.</li>
</ul>

<h3>Title: SciPIP: An LLM-based Scientific Paper Idea Proposer</h3>
<ul>
<li><strong>Authors: </strong>Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23166">https://arxiv.org/abs/2410.23166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23166">https://arxiv.org/pdf/2410.23166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23166]] SciPIP: An LLM-based Scientific Paper Idea Proposer(https://arxiv.org/abs/2410.23166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The exponential growth of knowledge and the increasing complexity of interdisciplinary research pose significant challenges for researchers, including information overload and difficulties in exploring novel ideas. The advancements in large language models (LLMs), such as GPT-4, have shown great potential in enhancing idea proposals, but how to effectively utilize large models for reasonable idea proposal has not been thoroughly explored. This paper proposes a scientific paper idea proposer (SciPIP). Based on a user-provided research background, SciPIP retrieves helpful papers from a literature database while leveraging the capabilities of LLMs to generate more novel and feasible ideas. To this end, 1) we construct a literature retrieval database, extracting lots of papers' multi-dimension information for fast access. Then, a literature retrieval method based on semantics, entity, and citation co-occurrences is proposed to search relevant literature from multiple aspects based on the user-provided background. 2) After literature retrieval, we introduce dual-path idea proposal strategies, where one path infers solutions from the retrieved literature and the other path generates original ideas through model brainstorming. We then combine the two to achieve a good balance between feasibility and originality. Through extensive experiments on the natural language processing (NLP) field, we demonstrate that SciPIP can retrieve citations similar to those of existing top conference papers and generate many ideas consistent with them. Additionally, we evaluate the originality of other ideas generated by SciPIP using large language models, further validating the effectiveness of our proposed method. The code and the database are released at this https URL.</li>
</ul>

<h3>Title: TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Wang, Yue Fan, Muhammad Ferjad Naeem, Yongqin Xian, Jan Eric Lenssen, Liwei Wang, Federico Tombari, Bernt Schiele</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23168">https://arxiv.org/abs/2410.23168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23168">https://arxiv.org/pdf/2410.23168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23168]] TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters(https://arxiv.org/abs/2410.23168)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at \url{this https URL}.</li>
</ul>

<h3>Title: Does equivariance matter at scale?</h3>
<ul>
<li><strong>Authors: </strong>Johann Brehmer, Sönke Behrends, Pim de Haan, Taco Cohen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23179">https://arxiv.org/abs/2410.23179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23179">https://arxiv.org/pdf/2410.23179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23179]] Does equivariance matter at scale?(https://arxiv.org/abs/2410.23179)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Given large data sets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models.</li>
</ul>

<h3>Title: ProTransformer: Robustify Transformers via Plug-and-Play Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Hou, Weizhi Gao, Yuchen Shen, Feiyi Wang, Xiaorui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23182">https://arxiv.org/abs/2410.23182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23182">https://arxiv.org/pdf/2410.23182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23182]] ProTransformer: Robustify Transformers via Plug-and-Play Paradigm(https://arxiv.org/abs/2410.23182)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing Vicuna by an average of 10.4% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains.</li>
</ul>

<h3>Title: Continuous Spatio-Temporal Memory Networks for 4D Cardiac Cine MRI Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Meng Ye, Bingyu Xin, Leon Axel, Dimitris Metaxas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23191">https://arxiv.org/abs/2410.23191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23191">https://arxiv.org/pdf/2410.23191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23191]] Continuous Spatio-Temporal Memory Networks for 4D Cardiac Cine MRI Segmentation(https://arxiv.org/abs/2410.23191)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Current cardiac cine magnetic resonance image (cMR) studies focus on the end diastole (ED) and end systole (ES) phases, while ignoring the abundant temporal information in the whole image sequence. This is because whole sequence segmentation is currently a tedious process and inaccurate. Conventional whole sequence segmentation approaches first estimate the motion field between frames, which is then used to propagate the mask along the temporal axis. However, the mask propagation results could be prone to error, especially for the basal and apex slices, where through-plane motion leads to significant morphology and structural change during the cardiac cycle. Inspired by recent advances in video object segmentation (VOS), based on spatio-temporal memory (STM) networks, we propose a continuous STM (CSTM) network for semi-supervised whole heart and whole sequence cMR segmentation. Our CSTM network takes full advantage of the spatial, scale, temporal and through-plane continuity prior of the underlying heart anatomy structures, to achieve accurate and fast 4D segmentation. Results of extensive experiments across multiple cMR datasets show that our method can improve the 4D cMR segmentation performance, especially for the hard-to-segment regions.</li>
</ul>

<h3>Title: Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Sheryl Hsu, Omar Khattab, Chelsea Finn, Archit Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23214">https://arxiv.org/abs/2410.23214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23214">https://arxiv.org/pdf/2410.23214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23214]] Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval(https://arxiv.org/abs/2410.23214)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The hallucinations of large language models (LLMs) are increasingly mitigated by allowing LLMs to search for information and to ground their answers in real sources. Unfortunately, LLMs often struggle with posing the right search queries, especially when dealing with complex or otherwise indirect topics. Observing that LLMs can learn to search for relevant facts by $\textit{trying}$ different queries and learning to up-weight queries that successfully produce relevant results, we introduce $\underline{Le}$arning to $\underline{Re}$trieve by $\underline{T}$rying (LeReT), a reinforcement learning framework that explores search queries and uses preference-based optimization to improve their quality. \methodclass can improve the absolute retrieval accuracy by up to 29\% and the downstream generator evaluations by 17\%. The simplicity and flexibility of LeReT allows it to be applied to arbitrary off-the-shelf retrievers and makes it a promising technique for improving general LLM pipelines. Project website: this http URL.</li>
</ul>

<h3>Title: OS-ATLAS: A Foundation Action Model for Generalist GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23218">https://arxiv.org/abs/2410.23218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23218">https://arxiv.org/pdf/2410.23218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23218]] OS-ATLAS: A Foundation Action Model for Generalist GUI Agents(https://arxiv.org/abs/2410.23218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.</li>
</ul>

<h3>Title: DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET</h3>
<ul>
<li><strong>Authors: </strong>Yitong Li, Morteza Ghahremani, Youssef Wally, Christian Wachinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23219">https://arxiv.org/abs/2410.23219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23219">https://arxiv.org/pdf/2410.23219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23219]] DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET(https://arxiv.org/abs/2410.23219)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Diagnosing dementia, particularly for Alzheimer's Disease (AD) and frontotemporal dementia (FTD), is complex due to overlapping symptoms. While magnetic resonance imaging (MRI) and positron emission tomography (PET) data are critical for the diagnosis, integrating these modalities in deep learning faces challenges, often resulting in suboptimal performance compared to using single modalities. Moreover, the potential of multi-modal approaches in differential diagnosis, which holds significant clinical importance, remains largely unexplored. We propose a novel framework, DiaMond, to address these issues with vision Transformers to effectively integrate MRI and PET. DiaMond is equipped with self-attention and a novel bi-attention mechanism that synergistically combine MRI and PET, alongside a multi-modal normalization to reduce redundant dependency, thereby boosting the performance. DiaMond significantly outperforms existing multi-modal methods across various datasets, achieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN classification, and 76.5% in differential diagnosis of AD and FTD. We also validated the robustness of DiaMond in a comprehensive ablation study. The code is available at this https URL.</li>
</ul>

<h3>Title: COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences</h3>
<ul>
<li><strong>Authors: </strong>Yixin Liu, Argyris Oikonomou, Weiqiang Zheng, Yang Cai, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23223">https://arxiv.org/abs/2410.23223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23223">https://arxiv.org/pdf/2410.23223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23223]] COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences(https://arxiv.org/abs/2410.23223)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many alignment methods, including reinforcement learning from human feedback (RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to capture the full range of general human preferences. To achieve robust alignment with general preferences, we model the alignment problem as a two-player zero-sum game, where the Nash equilibrium policy guarantees a 50% win rate against any competing policy. However, previous algorithms for finding the Nash policy either diverge or converge to a Nash policy in a modified game, even in a simple synthetic setting, thereby failing to maintain the 50% win rate guarantee against all other policies. We propose a meta-algorithm, Convergent Meta Alignment Algorithm (COMAL), for language model alignment with general preferences, inspired by convergent algorithms in game theory. Theoretically, we prove that our meta-algorithm converges to an exact Nash policy in the last iterate. Additionally, our meta-algorithm is simple and can be integrated with many existing methods designed for RLHF and preference optimization with minimal changes. Experimental results demonstrate the effectiveness of the proposed framework when combined with existing preference policy optimization methods.</li>
</ul>

<h3>Title: (FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Seungjoo Lee, Thanh-Long V. Le, Jaemin Shin, Sung-Ju Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23227">https://arxiv.org/abs/2410.23227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23227">https://arxiv.org/pdf/2410.23227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23227]] (FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning(https://arxiv.org/abs/2410.23227)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed machine learning framework that trains accurate global models while preserving clients' privacy-sensitive data. However, most FL approaches assume that clients possess labeled data, which is often not the case in practice. Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not. However, a significant performance gap exists between Centralized Semi-Supervised Learning (SSL) and FSSL. This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data. We propose $(FL)^2$, a robust training method for unlabeled clients using sharpness-aware consistency regularization. We show that regularizing the original pseudo-labeling loss is suboptimal, and hence we carefully select unlabeled samples for regularization. We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client. Our experiments on three benchmark datasets demonstrate that our approach significantly improves performance and bridges the gap with SSL, particularly in scenarios with scarce labeled data.</li>
</ul>

<h3>Title: Emergence of meta-stable clustering in mean-field transformer models</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Bruno, Federico Pasqualotto, Andrea Agazzi</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23228">https://arxiv.org/abs/2410.23228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23228">https://arxiv.org/pdf/2410.23228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23228]] Emergence of meta-stable clustering in mean-field transformer models(https://arxiv.org/abs/2410.23228)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in (Geshkovski et al., 2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as a function of the inverse temperature parameter of the model, by the index maximizing a certain rescaling of Gegenbauer polynomials.</li>
</ul>

<h3>Title: PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D Matching</h3>
<ul>
<li><strong>Authors: </strong>Chen Ziwen, Zexiang Xu, Li Fuxin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23245">https://arxiv.org/abs/2410.23245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23245">https://arxiv.org/pdf/2410.23245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23245]] PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D Matching(https://arxiv.org/abs/2410.23245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a novel online, point-based 3D reconstruction method from posed monocular RGB videos. Our model maintains a global point cloud representation of the scene, continuously updating the features and 3D locations of points as new images are observed. It expands the point cloud with newly detected points while carefully removing redundancies. The point cloud updates and depth predictions for new points are achieved through a novel ray-based 2D-3D feature matching technique, which is robust against errors in previous point position predictions. In contrast to offline methods, our approach processes infinite-length sequences and provides real-time updates. Additionally, the point cloud imposes no pre-defined resolution or scene size constraints, and its unified global representation ensures view consistency across perspectives. Experiments on the ScanNet dataset show that our method achieves state-of-the-art quality among online MVS approaches. Project page: this https URL</li>
</ul>

<h3>Title: Evaluating Cultural and Social Awareness of LLM Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Haoyi Qiu, Alexander R. Fabbri, Divyansh Agarwal, Kung-Hsiang Huang, Sarah Tan, Nanyun Peng, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23252">https://arxiv.org/abs/2410.23252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23252">https://arxiv.org/pdf/2410.23252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23252]] Evaluating Cultural and Social Awareness of LLM Web Agents(https://arxiv.org/abs/2410.23252)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) expand into performing as agents for real-world applications beyond traditional NLP tasks, evaluating their robustness becomes increasingly important. However, existing benchmarks often overlook critical dimensions like cultural and social awareness. To address these, we introduce CASA, a benchmark designed to assess LLM agents' sensitivity to cultural and social norms across two web-based tasks: online shopping and social discussion forums. Our approach evaluates LLM agents' ability to detect and appropriately respond to norm-violating user queries and observations. Furthermore, we propose a comprehensive evaluation framework that measures awareness coverage, helpfulness in managing user queries, and the violation rate when facing misleading web content. Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates. To improve performance, we explore two methods: prompting and fine-tuning, and find that combining both methods can offer complementary advantages -- fine-tuning on culture-specific datasets significantly enhances the agents' ability to generalize across different regions, while prompting boosts the agents' ability to navigate complex tasks. These findings highlight the importance of constantly benchmarking LLM agents' cultural and social awareness during the development cycle.</li>
</ul>

<h3>Title: EMMA: End-to-End Multimodal Model for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, James Guo, Dragomir Anguelov, Mingxing Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23262">https://arxiv.org/abs/2410.23262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23262">https://arxiv.org/pdf/2410.23262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23262]] EMMA: End-to-End Multimodal Model for Autonomous Driving(https://arxiv.org/abs/2410.23262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built on a multi-modal large language model foundation, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. However, EMMA also exhibits certain limitations: it can process only a small amount of image frames, does not incorporate accurate 3D sensing modalities like LiDAR or radar and is computationally expensive. We hope that our results will inspire further research to mitigate these issues and to further evolve the state of the art in autonomous driving model architectures.</li>
</ul>

<h3>Title: Proportional Fairness in Non-Centroid Clustering</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Caragiannis, Evi Micha, Nisarg Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23273">https://arxiv.org/abs/2410.23273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23273">https://arxiv.org/pdf/2410.23273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23273]] Proportional Fairness in Non-Centroid Clustering(https://arxiv.org/abs/2410.23273)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points (agents) that are large and cohesive. Prior work applies this framework to centroid clustering, where the loss of an agent is its distance to the centroid assigned to its cluster. We expand the framework to non-centroid clustering, where the loss of an agent is a function of the other agents in its cluster, by adapting two proportional fairness criteria -- the core and its relaxation, fully justified representation (FJR) -- to this setting. We show that the core can be approximated only under structured loss functions, and even then, the best approximation we are able to establish, using an adaptation of the GreedyCapture algorithm developed for centroid clustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a natural loss function. In contrast, we design a new (inefficient) algorithm, GreedyCohesiveClustering, which achieves the relaxation FJR exactly under arbitrary loss functions, and show that the efficient GreedyCapture algorithm achieves a constant approximation of FJR. We also design an efficient auditing algorithm, which estimates the FJR approximation of any given clustering solution up to a constant factor. Our experiments on real data suggest that traditional clustering algorithms are highly unfair, whereas GreedyCapture is considerably fairer and incurs only a modest loss in common clustering objectives.</li>
</ul>

<h3>Title: Multi-student Diffusion Distillation for Better One-step Generators</h3>
<ul>
<li><strong>Authors: </strong>Yanke Song, Jonathan Lorraine, Weili Nie, Karsten Kreis, James Lucas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23274">https://arxiv.org/abs/2410.23274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23274">https://arxiv.org/pdf/2410.23274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23274]] Multi-student Diffusion Distillation for Better One-step Generators(https://arxiv.org/abs/2410.23274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve high-quality sample generation at the cost of a lengthy multistep inference procedure. To overcome this, diffusion distillation techniques produce student generators capable of matching or surpassing the teacher in a single step. However, the student model's inference speed is limited by the size of the teacher architecture, preventing real-time generation for computationally heavy applications. In this work, we introduce Multi-Student Distillation (MSD), a framework to distill a conditional teacher diffusion model into multiple single-step generators. Each student generator is responsible for a subset of the conditioning data, thereby obtaining higher generation quality for the same capacity. MSD trains multiple distilled students, allowing smaller sizes and, therefore, faster inference. Also, MSD offers a lightweight quality boost over single-student distillation with the same architecture. We demonstrate MSD is effective by training multiple same-sized or smaller students on single-step distillation using distribution matching and adversarial distillation techniques. With smaller students, MSD gets competitive results with faster inference for single-step generation. Using 4 same-sized students, MSD sets a new state-of-the-art for one-step image generation: FID 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014.</li>
</ul>

<h3>Title: SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yining Hong, Beide Liu, Maxine Wu, Yuanhao Zhai, Kai-Wei Chang, Lingjie Li, Kevin Lin, Chung-Ching Lin, Jianfeng Wang, Zhengyuan Yang, Yingnian Wu, Lijuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23277">https://arxiv.org/abs/2410.23277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23277">https://arxiv.org/pdf/2410.23277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23277]] SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation(https://arxiv.org/abs/2410.23277)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: this https URL</li>
</ul>

<h3>Title: RelationBooth: Towards Relation-Aware Customized Object Generation</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, Xiangtai Li, Ming-Husang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23280">https://arxiv.org/abs/2410.23280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23280">https://arxiv.org/pdf/2410.23280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23280]] RelationBooth: Towards Relation-Aware Customized Object Generation(https://arxiv.org/abs/2410.23280)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Customized image generation is crucial for delivering personalized content based on user-provided image prompts, aligning large-scale text-to-image diffusion models with individual needs. However, existing models often overlook the relationships between customized objects in generated images. Instead, this work addresses that gap by focusing on relation-aware customized image generation, which aims to preserve the identities from image prompts while maintaining the predicate relations described in text prompts. Specifically, we introduce RelationBooth, a framework that disentangles identity and relation learning through a well-curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges: generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on three benchmarks demonstrate the superiority of RelationBooth in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public.</li>
</ul>

<h3>Title: Provable acceleration for diffusion models under minimal assumptions</h3>
<ul>
<li><strong>Authors: </strong>Gen Li, Changxiao Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23285">https://arxiv.org/abs/2410.23285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23285">https://arxiv.org/pdf/2410.23285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23285]] Provable acceleration for diffusion models under minimal assumptions(https://arxiv.org/abs/2410.23285)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While score-based diffusion models have achieved exceptional sampling quality, their sampling speeds are often limited by the high computational burden of score function evaluations. Despite the recent remarkable empirical advances in speeding up the score-based samplers, theoretical understanding of acceleration techniques remains largely limited. To bridge this gap, we propose a novel training-free acceleration scheme for stochastic samplers. Under minimal assumptions -- namely, $L^2$-accurate score estimates and a finite second-moment condition on the target distribution -- our accelerated sampler provably achieves $\varepsilon$-accuracy in total variation within $\widetilde{O}(d^{5/4}/\sqrt{\varepsilon})$ iterations, thereby significantly improving upon the $\widetilde{O}(d/\varepsilon)$ iteration complexity of standard score-based samplers. Notably, our convergence theory does not rely on restrictive assumptions on the target distribution or higher-order score estimation guarantees.</li>
</ul>

<h3>Title: ReferEverything: Towards Segmenting Everything We Can Speak of in Videos</h3>
<ul>
<li><strong>Authors: </strong>Anurag Bagchi, Zhipeng Bao, Yu-Xiong Wang, Pavel Tokmakov, Martial Hebert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.23287">https://arxiv.org/abs/2410.23287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.23287">https://arxiv.org/pdf/2410.23287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.23287]] ReferEverything: Towards Segmenting Everything We Can Speak of in Videos(https://arxiv.org/abs/2410.23287)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
