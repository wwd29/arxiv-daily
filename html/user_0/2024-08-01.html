<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-01</h1>
<h3>Title: Success Probability in Multi-View Imaging</h3>
<ul>
<li><strong>Authors: </strong>Vadim Holodovsky, Masada Tzabari, Yoav Schechner, Alex Frid, Klaus Schilling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21027">https://arxiv.org/abs/2407.21027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21027">https://arxiv.org/pdf/2407.21027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21027]] Success Probability in Multi-View Imaging(https://arxiv.org/abs/2407.21027)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Platforms such as robots, security cameras, drones and satellites are used in multi-view imaging for three-dimensional (3D) recovery by stereoscopy or tomography. Each camera in the setup has a field of view (FOV). Multi-view analysis requires overlap of the FOVs of all cameras, or a significant subset of them. However, the success of such methods is not guaranteed, because the FOVs may not sufficiently overlap. The reason is that pointing of a camera from a mount or platform has some randomness (noise), due to imprecise platform control, typical to mechanical systems, and particularly moving systems such as satellites. So, success is probabilistic. This paper creates a framework to analyze this aspect. This is critical for setting limitations on the capabilities of imaging systems, such as resolution (pixel footprint), FOV, the size of domains that can be captured, and efficiency. The framework uses the fact that imprecise pointing can be mitigated by self-calibration - provided that there is sufficient overlap between pairs of views and sufficient visual similarity of views. We show an example considering the design of a formation of nanosatellites that seek 3D reconstruction of clouds.</li>
</ul>

<h3>Title: Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion</h3>
<ul>
<li><strong>Authors: </strong>Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, Juho Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21032">https://arxiv.org/abs/2407.21032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21032">https://arxiv.org/pdf/2407.21032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21032]] Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion(https://arxiv.org/abs/2407.21032)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper addresses the societal concerns arising from large-scale text-to-image diffusion models for generating potentially harmful or copyrighted content. Existing models rely heavily on internet-crawled data, wherein problematic concepts persist due to incomplete filtration processes. While previous approaches somewhat alleviate the issue, they often rely on text-specified concepts, introducing challenges in accurately capturing nuanced concepts and aligning model knowledge with human understandings. In response, we propose a framework named Human Feedback Inversion (HFI), where human feedback on model-generated images is condensed into textual tokens guiding the mitigation or removal of problematic images. The proposed framework can be built upon existing techniques for the same purpose, enhancing their alignment with human judgment. By doing so, we simplify the training objective with a self-distillation-based technique, providing a strong baseline for concept removal. Our experimental results demonstrate our framework significantly reduces objectionable content generation while preserving image quality, contributing to the ethical deployment of AI in the public sphere.</li>
</ul>

<h3>Title: Direct Unlearning Optimization for Robust and Safe Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, Gayoung Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21035">https://arxiv.org/abs/2407.21035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21035">https://arxiv.org/pdf/2407.21035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21035]] Direct Unlearning Optimization for Robust and Safe Text-to-Image Models(https://arxiv.org/abs/2407.21035)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) models have greatly benefited from large-scale datasets, but they also pose significant risks due to the potential generation of unsafe content. To mitigate this issue, researchers have developed unlearning techniques to remove the model's ability to generate potentially harmful content. However, these methods are easily bypassed by adversarial attacks, making them unreliable for ensuring the safety of generated images. In this paper, we propose Direct Unlearning Optimization (DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I models while preserving their performance on unrelated topics. DUO employs a preference optimization approach using curated paired image data, ensuring that the model learns to remove unsafe visual concepts while retaining unrelated features. Furthermore, we introduce an output-preserving regularization term to maintain the model's generative capabilities on safe content. Extensive experiments demonstrate that DUO can robustly defend against various state-of-the-art red teaming methods without significant performance degradation on unrelated topics, as measured by FID and CLIP scores. Our work contributes to the development of safer and more reliable T2I models, paving the way for their responsible deployment in both closed-source and open-source scenarios.</li>
</ul>

<h3>Title: An Application of Large Language Models to Coding Negotiation Transcripts</h3>
<ul>
<li><strong>Authors: </strong>Ray Friedman, Jaewoo Cho, Jeanne Brett, Xuhui Zhan, Ningyu Han, Sriram Kannan, Yingxiang Ma, Jesse Spencer-Smith, Elisabeth Jäckel, Alfred Zerres, Madison Hooper, Katie Babbit, Manish Acharya, Wendi Adair, Soroush Aslani, Tayfun Aykaç, Chris Bauman, Rebecca Bennett, Garrett Brady, Peggy Briggs, Cheryl Dowie, Chase Eck, Igmar Geiger, Frank Jacob, Molly Kern, Sujin Lee, Leigh Anne Liu, Wu Liu, Jeffrey Loewenstein, Anne Lytle, Li Ma, Michel Mann, Alexandra Mislin, Tyree Mitchell, Hannah Martensen née Nagler, Amit Nandkeolyar, Mara Olekalns, Elena Paliakova, Jennifer Parlamis, Jason Pierce, Nancy Pierce, Robin Pinkley, Nathalie Prime, Jimena Ramirez-Marin, Kevin Rockmann, William Ross, Zhaleh Semnani-Azad, Juliana Schroeder, Philip Smith, Elena Stimmer, Roderick Swaab, Leigh Thompson, Cathy Tinsley, Ece Tuncel, Laurie Weingart, Robert Wilken, JingJing Yao, Zhi-Xue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21037">https://arxiv.org/abs/2407.21037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21037">https://arxiv.org/pdf/2407.21037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21037]] An Application of Large Language Models to Coding Negotiation Transcripts(https://arxiv.org/abs/2407.21037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLM) have demonstrated impressive capabilities in the field of natural language processing (NLP). This paper explores the application of LLMs in negotiation transcript analysis by the Vanderbilt AI Negotiation Lab. Starting in September 2022, we applied multiple strategies using LLMs from zero shot learning to fine tuning models to in-context learning). The final strategy we developed is explained, along with how to access and use the model. This study provides a sense of both the opportunities and roadblocks for the implementation of LLMs in real life applications and offers a model for how LLMs can be applied to coding in other fields.</li>
</ul>

<h3>Title: Advancing Chart Question Answering with Robust Chart Component Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Zheng, Sijia Wang, Chris Thomas, Lifu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21038">https://arxiv.org/abs/2407.21038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21038">https://arxiv.org/pdf/2407.21038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21038]] Advancing Chart Question Answering with Robust Chart Component Recognition(https://arxiv.org/abs/2407.21038)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Chart comprehension presents significant challenges for machine learning models due to the diverse and intricate shapes of charts. Existing multimodal methods often overlook these visual features or fail to integrate them effectively for chart question answering (ChartQA). To address this, we introduce Chartformer, a unified framework that enhances chart component recognition by accurately identifying and classifying components such as bars, lines, pies, titles, legends, and axes. Additionally, we propose a novel Question-guided Deformable Co-Attention (QDCAt) mechanism, which fuses chart features encoded by Chartformer with the given question, leveraging the question's guidance to ground the correct answer. Extensive experiments demonstrate that the proposed approaches significantly outperform baseline models in chart component recognition and ChartQA tasks, achieving improvements of 3.2% in mAP and 15.4% in accuracy, respectively. These results underscore the robustness of our solution for detailed visual data interpretation across various applications.</li>
</ul>

<h3>Title: They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saeid Mahdavinejad, Peyman Adibi, Amirhassan Monadjemi, Pascal Hitzler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21041">https://arxiv.org/abs/2407.21041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21041">https://arxiv.org/pdf/2407.21041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21041]] They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models(https://arxiv.org/abs/2407.21041)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Depression is a common mental health issue that requires prompt diagnosis and treatment. Despite the promise of social media data for depression detection, the opacity of employed deep learning models hinders interpretability and raises bias concerns. We address this challenge by introducing ProtoDep, a novel, explainable framework for Twitter-based depression detection. ProtoDep leverages prototype learning and the generative power of Large Language Models to provide transparent explanations at three levels: (i) symptom-level explanations for each tweet and user, (ii) case-based explanations comparing the user to similar individuals, and (iii) transparent decision-making through classification weights. Evaluated on five benchmark datasets, ProtoDep achieves near state-of-the-art performance while learning meaningful prototypes. This multi-faceted approach offers significant potential to enhance the reliability and transparency of depression detection on social media, ultimately aiding mental health professionals in delivering more informed care.</li>
</ul>

<h3>Title: CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Yu Feng, Zhen Tian, Yifan Zhu, Zongfu Han, Haoran Luo, Guangwei Zhang, Meina Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21043">https://arxiv.org/abs/2407.21043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21043">https://arxiv.org/pdf/2407.21043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21043]] CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental Continual Learning(https://arxiv.org/abs/2407.21043)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The key challenge of cross-modal domain-incremental learning (DIL) is to enable the learning model to continuously learn from novel data with different feature distributions under the same task without forgetting old ones. However, existing top-performing methods still cause high forgetting rates, by lacking intra-domain knowledge extraction and inter-domain common prompting strategy. In this paper, we propose a simple yet effective framework, CP-Prompt, by training limited parameters to instruct a pre-trained model to learn new domains and avoid forgetting existing feature distributions. CP-Prompt captures intra-domain knowledge by compositionally inserting personalized prompts on multi-head self-attention layers and then learns the inter-domain knowledge with a common prompting strategy. CP-Prompt shows superiority compared with state-of-the-art baselines among three widely evaluated DIL tasks. The source code is available at this https URL.</li>
</ul>

<h3>Title: Unlocking the Potential: Benchmarking Large Language Models in Water Engineering and Research</h3>
<ul>
<li><strong>Authors: </strong>Boyan Xu, Liang Wen, Zihao Li, Yuxing Yang, Guanlan Wu, Xiongpeng Tang, Yu Li, Zihao Wu, Qingxian Su, Xueqing Shi, Yue Yang, Rui Tong, How Yong Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21045">https://arxiv.org/abs/2407.21045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21045">https://arxiv.org/pdf/2407.21045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21045]] Unlocking the Potential: Benchmarking Large Language Models in Water Engineering and Research(https://arxiv.org/abs/2407.21045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have sparked interest in their potential applications across various fields. This paper embarked on a pivotal inquiry: Can existing LLMs effectively serve as "water expert models" for water engineering and research tasks? This study was the first to evaluate LLMs' contributions across various water engineering and research tasks by establishing a domain-specific benchmark suite, namely, WaterER. Herein, we prepared 983 tasks related to water engineering and research, categorized into "wastewater treatment", "environmental restoration", "drinking water treatment and distribution", "sanitation", "anaerobic digestion" and "contaminants assessment". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5, Gemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the strengths of GPT-4 in handling diverse and complex tasks of water engineering and water research, the specialized capabilities of Gemini in academic contexts, Llama3's strongest capacity to answer Chinese water engineering questions and the competitive performance of Chinese-oriented models like GLM-4, ERNIE and QWEN in some water engineering tasks. More specifically, current LLMs excelled particularly in generating precise research gaps for papers on "contaminants and related water quality monitoring and assessment". Additionally, they were more adept at creating appropriate titles for research papers on "treatment processes for wastewaters", "environmental restoration", and "drinking water treatment". Overall, this study pioneered evaluating LLMs in water engineering and research by introducing the WaterER benchmark to assess the trustworthiness of their predictions. This standardized evaluation framework would also drive future advancements in LLM technology by using targeting datasets, propelling these models towards becoming true "water expert".</li>
</ul>

<h3>Title: Promises and Pitfalls of Generative Masked Language Modeling: Theoretical Framework and Practical Guidelines</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Li, Alexandre Kirchmeyer, Aashay Mehta, Yilong Qin, Boris Dadachev, Kishore Papineni, Sanjiv Kumar, Andrej Risteski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21046">https://arxiv.org/abs/2407.21046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21046">https://arxiv.org/pdf/2407.21046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21046]] Promises and Pitfalls of Generative Masked Language Modeling: Theoretical Framework and Practical Guidelines(https://arxiv.org/abs/2407.21046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autoregressive language models are the currently dominant paradigm for text generation, but they have some fundamental limitations that cannot be remedied by scale-for example inherently sequential and unidirectional generation. While alternate classes of models have been explored, we have limited mathematical understanding of their fundamental power and limitations. In this paper we focus on Generative Masked Language Models (GMLMs), a non-autoregressive paradigm in which we train a model to fit conditional probabilities of the data distribution via masking, which are subsequently used as inputs to a Markov Chain to draw samples from the model, These models empirically strike a promising speed-quality trade-off as each step can be typically parallelized by decoding the entire sequence in parallel. We develop a mathematical framework for analyzing and improving such models which sheds light on questions of sample complexity and inference speed and quality. Empirically, we adapt the T5 model for iteratively-refined parallel decoding, achieving 2-3x speedup in machine translation with minimal sacrifice in quality compared with autoregressive models. We run careful ablation experiments to give recommendations on key design choices, and make fine-grained observations on the common error modes in connection with our theory. Our mathematical analyses and empirical observations characterize both potentials and limitations of this approach, and can be applied to future works on improving understanding and performance of GMLMs. Our codes are released at this https URL</li>
</ul>

<h3>Title: Artificial Intelligence in Extracting Diagnostic Data from Dental Records</h3>
<ul>
<li><strong>Authors: </strong>Yao-Shun Chuang, Chun-Teh Lee, Oluwabunmi Tokede, Guo-Hao Lin, Ryan Brandon, Trung Duong Tran, Xiaoqian Jiang, Muhammad F. Walji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21050">https://arxiv.org/abs/2407.21050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21050">https://arxiv.org/pdf/2407.21050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21050]] Artificial Intelligence in Extracting Diagnostic Data from Dental Records(https://arxiv.org/abs/2407.21050)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This research addresses the issue of missing structured data in dental records by extracting diagnostic information from unstructured text. The updated periodontology classification system's complexity has increased incomplete or missing structured diagnoses. To tackle this, we use advanced AI and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a RoBERTa model. This significantly enhances the model's ability to understand medical and dental language. We evaluated the model using 120 randomly selected clinical notes from two datasets, demonstrating its improved diagnostic extraction accuracy. The results showed high accuracy in diagnosing periodontal status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In the subtype category, Site 2 achieved perfect scores, outperforming Site 1. This method enhances extraction accuracy and broadens its use across dental contexts. The study underscores AI and NLP's transformative impact on healthcare delivery and management. Integrating AI and NLP technologies enhances documentation and simplifies administrative tasks by precisely extracting complex clinical information. This approach effectively addresses challenges in dental diagnostics. Using synthetic training data from LLMs optimizes the training process, improving accuracy and efficiency in identifying periodontal diagnoses from clinical notes. This innovative method holds promise for broader healthcare applications, potentially improving patient care quality.</li>
</ul>

<h3>Title: An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice</h3>
<ul>
<li><strong>Authors: </strong>Roma Shusterman, Allison C. Waters, Shannon O`Neill, Phan Luu, Don M. Tucker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21051">https://arxiv.org/abs/2407.21051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21051">https://arxiv.org/pdf/2407.21051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21051]] An Active Inference Strategy for Prompting Reliable Responses from Large Language Models in Medical Practice(https://arxiv.org/abs/2407.21051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continuing advances in Large Language Models (LLMs) in artificial intelligence offer important capacities in intuitively accessing and using medical knowledge in many contexts, including education and training as well as assessment and treatment. Most of the initial literature on LLMs in medicine has emphasized that LLMs are unsuitable for medical use because they are non-deterministic, may provide incorrect or harmful responses, and cannot be regulated to assure quality control. If these issues could be corrected, optimizing LLM technology could benefit patients and physicians by providing affordable, point-of-care medical knowledge. Our proposed framework refines LLM responses by restricting their primary knowledge base to domain-specific datasets containing validated medical information. Additionally, we introduce an actor-critic LLM prompting protocol based on active inference principles of human cognition, where a Therapist agent initially responds to patient queries, and a Supervisor agent evaluates and adjusts responses to ensure accuracy and reliability. We conducted a validation study where expert cognitive behaviour therapy for insomnia (CBT-I) therapists evaluated responses from the LLM in a blind format. Experienced human CBT-I therapists assessed responses to 100 patient queries, comparing LLM-generated responses with appropriate and inappropriate responses crafted by experienced CBT-I therapists. Results showed that LLM responses received high ratings from the CBT-I therapists, often exceeding those of therapist-generated appropriate responses. This structured approach aims to integrate advanced LLM technology into medical applications, meeting regulatory requirements for establishing the safe and effective use of special purpose validated LLMs in medicine.</li>
</ul>

<h3>Title: Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet Extraction</h3>
<ul>
<li><strong>Authors: </strong>Kun Peng, Lei Jiang, Qian Li, Haoran Li, Xiaoyan Yu, Li Sun, Shuo Sun, Yanxian Bi, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21052">https://arxiv.org/abs/2407.21052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21052">https://arxiv.org/pdf/2407.21052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21052]] Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet Extraction(https://arxiv.org/abs/2407.21052)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract fine-grained sentiment elements from target domain sentences by leveraging the knowledge acquired from the source domain. Due to the absence of labeled data in the target domain, recent studies tend to rely on pre-trained language models to generate large amounts of synthetic data for training purposes. However, these approaches entail additional computational costs associated with the generation process. Different from them, we discover a striking resemblance between table-filling methods in ASTE and two-stage Object Detection (OD) in computer vision, which inspires us to revisit the cross-domain ASTE task and approach it from an OD standpoint. This allows the model to benefit from the OD extraction paradigm and region-level alignment. Building upon this premise, we propose a novel method named \textbf{T}able-\textbf{F}illing via \textbf{M}ean \textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the sentence into a 2D table to detect word relations, while TFMT treats the table as a feature map and utilizes a region consistency to enhance the quality of those generated pseudo labels. Additionally, considering the existence of the domain gap, a cross-domain consistency based on Maximum Mean Discrepancy is designed to alleviate domain shift problems. Our method achieves state-of-the-art performance with minimal parameters and computational costs, making it a strong baseline for cross-domain ASTE.</li>
</ul>

<h3>Title: Sentiment Reasoning for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Khai Le-Duc, Khai-Nguyen Nguyen, Bach Phan Tat, Duy Le, Jerry Ngo, Long Vo-Dang, Anh Totti Nguyen, Truong-Son Hy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21054">https://arxiv.org/abs/2407.21054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21054">https://arxiv.org/pdf/2407.21054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21054]] Sentiment Reasoning for Healthcare(https://arxiv.org/abs/2407.21054)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Transparency in AI decision-making is crucial in healthcare due to the severe consequences of errors, and this is important for building trust among AI and users in sentiment analysis task. Incorporating reasoning capabilities helps Large Language Models (LLMs) understand human emotions within broader contexts, handle nuanced and ambiguous language, and infer underlying sentiments that may not be explicitly stated. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, along with our proposed multimodal multitask framework and dataset. Our study showed that rationale-augmented training enhances model performance in sentiment classification across both human transcript and ASR settings. Also, we found that the generated rationales typically exhibit different vocabularies compared to human-generated rationales, but maintain similar semantics. All code, data (English-translated and Vietnamese) and models are published online: this https URL</li>
</ul>

<h3>Title: Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications</h3>
<ul>
<li><strong>Authors: </strong>Cui Long, Yongbin Liu, Chunping Ouyang, Ying Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21055">https://arxiv.org/abs/2407.21055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21055">https://arxiv.org/pdf/2407.21055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21055]] Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications(https://arxiv.org/abs/2407.21055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited remarkable proficiency in natural language understanding, prompting extensive exploration of their potential applications across diverse domains. In the medical domain, open-source LLMs have demonstrated moderate efficacy following domain-specific fine-tuning; however, they remain substantially inferior to proprietary models such as GPT-4 and GPT-3.5. These open-source models encounter limitations in the comprehensiveness of domain-specific knowledge and exhibit a propensity for 'hallucinations' during text generation. To mitigate these issues, researchers have implemented the Retrieval-Augmented Generation (RAG) approach, which augments LLMs with background information from external knowledge bases while preserving the model's internal parameters. However, document noise can adversely affect performance, and the application of RAG in the medical field remains in its nascent stages. This study presents the Bailicai framework: a novel integration of retrieval-augmented generation with large language models optimized for the medical domain. The Bailicai framework augments the performance of LLMs in medicine through the implementation of four sub-modules. Experimental results demonstrate that the Bailicai approach surpasses existing medical domain LLMs across multiple medical benchmarks and exceeds the performance of GPT-3.5. Furthermore, the Bailicai method effectively attenuates the prevalent issue of hallucinations in medical applications of LLMs and ameliorates the noise-related challenges associated with traditional RAG techniques when processing irrelevant or pseudo-relevant documents.</li>
</ul>

<h3>Title: What Matters in Explanations: Towards Explainable Fake Review Detection Focusing on Transformers</h3>
<ul>
<li><strong>Authors: </strong>Md Shajalal, Md Atabuzzaman, Alexander Boden, Gunnar Stevens, Delong Du</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21056">https://arxiv.org/abs/2407.21056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21056">https://arxiv.org/pdf/2407.21056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21056]] What Matters in Explanations: Towards Explainable Fake Review Detection Focusing on Transformers(https://arxiv.org/abs/2407.21056)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Customers' reviews and feedback play crucial role on electronic commerce~(E-commerce) platforms like Amazon, Zalando, and eBay in influencing other customers' purchasing decisions. However, there is a prevailing concern that sellers often post fake or spam reviews to deceive potential customers and manipulate their opinions about a product. Over the past decade, there has been considerable interest in using machine learning (ML) and deep learning (DL) models to identify such fraudulent reviews. Unfortunately, the decisions made by complex ML and DL models - which often function as \emph{black-boxes} - can be surprising and difficult for general users to comprehend. In this paper, we propose an explainable framework for detecting fake reviews with high precision in identifying fraudulent content with explanations and investigate what information matters most for explaining particular decisions by conducting empirical user evaluation. Initially, we develop fake review detection models using DL and transformer models including XLNet and DistilBERT. We then introduce layer-wise relevance propagation (LRP) technique for generating explanations that can map the contributions of words toward the predicted class. The experimental results on two benchmark fake review detection datasets demonstrate that our predictive models achieve state-of-the-art performance and outperform several existing methods. Furthermore, the empirical user evaluation of the generated explanations concludes which important information needs to be considered in generating explanations in the context of fake review identification.</li>
</ul>

<h3>Title: Multi-group Uncertainty Quantification for Long-form Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Terrance Liu, Zhiwei Steven Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21057">https://arxiv.org/abs/2407.21057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21057">https://arxiv.org/pdf/2407.21057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21057]] Multi-group Uncertainty Quantification for Long-form Text Generation(https://arxiv.org/abs/2407.21057)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models are rapidly moving towards consumer-facing applications, they are often still prone to factual errors and hallucinations. In order to reduce the potential harms that may come from these errors, it is important for users to know to what extent they can trust an LLM when it makes a factual claim. To this end, we study the problem of uncertainty quantification of factual correctness in long-form natural language generation. Given some output from a large language model, we study both uncertainty at the level of individual claims contained within the output (via calibration) and uncertainty across the entire output itself (via conformal prediction). Moreover, we invoke multicalibration and multivalid conformal prediction to ensure that such uncertainty guarantees are valid both marginally and across distinct groups of prompts. Using the task of biography generation, we demonstrate empirically that having access to and making use of additional group attributes for each prompt improves both overall and group-wise performance. As the problems of calibration, conformal prediction, and their multi-group counterparts have not been extensively explored previously in the context of long-form text generation, we consider these empirical results to form a benchmark for this setting.</li>
</ul>

<h3>Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21059">https://arxiv.org/abs/2407.21059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21059">https://arxiv.org/pdf/2407.21059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21059]] Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks(https://arxiv.org/abs/2407.21059)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of "retrieve-then-generate". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.</li>
</ul>

<h3>Title: Using Large Language Models for the Interpretation of Building Regulations</h3>
<ul>
<li><strong>Authors: </strong>Stefan Fuchs, Michael Witbrock, Johannes Dimyadi, Robert Amor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21060">https://arxiv.org/abs/2407.21060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21060">https://arxiv.org/pdf/2407.21060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21060]] Using Large Language Models for the Interpretation of Building Regulations(https://arxiv.org/abs/2407.21060)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Compliance checking is an essential part of a construction project. The recent rapid uptake of building information models (BIM) in the construction industry has created more opportunities for automated compliance checking (ACC). BIM enables sharing of digital building design data that can be used for compliance checking with legal requirements, which are conventionally conveyed in natural language and not intended for machine processing. Creating a computable representation of legal requirements suitable for ACC is complex, costly, and time-consuming. Large language models (LLMs) such as the generative pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT, can generate logically coherent text and source code responding to user prompts. This capability could be used to automate the conversion of building regulations into a semantic and computable representation. This paper evaluates the performance of LLMs in translating building regulations into LegalRuleML in a few-shot learning setup. By providing GPT-3.5 with only a few example translations, it can learn the basic structure of the format. Using a system prompt, we further specify the LegalRuleML representation and explore the existence of expert domain knowledge in the model. Such domain knowledge might be ingrained in GPT-3.5 through the broad pre-training but needs to be brought forth by careful contextualisation. Finally, we investigate whether strategies such as chain-of-thought reasoning and self-consistency could apply to this use case. As LLMs become more sophisticated, the increased common sense, logical coherence, and means to domain adaptation can significantly support ACC, leading to more efficient and effective checking processes.</li>
</ul>

<h3>Title: LawLLM: Law Large Language Model for the US Legal System</h3>
<ul>
<li><strong>Authors: </strong>Dong Shu, Haoran Zhao, Xukun Liu, David Demeter, Mengnan Du, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21065">https://arxiv.org/abs/2407.21065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21065">https://arxiv.org/pdf/2407.21065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21065]] LawLLM: Law Large Language Model for the US Legal System(https://arxiv.org/abs/2407.21065)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving field of legal analytics, finding relevant cases and accurately predicting judicial outcomes are challenging because of the complexity of legal language, which often includes specialized terminology, complex syntax, and historical context. Moreover, the subtle distinctions between similar and precedent cases require a deep understanding of legal knowledge. Researchers often conflate these concepts, making it difficult to develop specialized techniques to effectively address these nuanced tasks. In this paper, we introduce the Law Large Language Model (LawLLM), a multi-task model specifically designed for the US legal domain to address these challenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly distinguishing between precedent and similar cases, we provide essential clarity, guiding future research in developing specialized strategies for these tasks. We propose customized data preprocessing techniques for each task that transform raw legal data into a trainable format. Furthermore, we also use techniques such as in-context learning (ICL) and advanced information retrieval methods in LawLLM. The evaluation results demonstrate that LawLLM consistently outperforms existing baselines in both zero-shot and few-shot scenarios, offering unparalleled multi-task capabilities and filling critical gaps in the legal domain.</li>
</ul>

<h3>Title: ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech Processing Tasks</h3>
<ul>
<li><strong>Authors: </strong>Nakamasa Inoue, Shinta Otake, Takumi Hirose, Masanari Ohi, Rei Kawakami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21066">https://arxiv.org/abs/2407.21066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21066">https://arxiv.org/pdf/2407.21066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21066]] ELP-Adapters: Parameter Efficient Adapter Tuning for Various Speech Processing Tasks(https://arxiv.org/abs/2407.21066)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has emerged as a key approach for learning generic representations from speech data. Despite promising results in downstream tasks such as speech recognition, speaker verification, and emotion recognition, a significant number of parameters is required, which makes fine-tuning for each task memory-inefficient. To address this limitation, we introduce ELP-adapter tuning, a novel method for parameter-efficient fine-tuning using three types of adapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and a prompt adapter (P-adapter). The E-adapters are integrated into transformer-based encoder layers and help to learn fine-grained speech representations that are effective for speech recognition. The L-adapters create paths from each encoder layer to the downstream head and help to extract non-linguistic features from lower encoder layers that are effective for speaker verification and emotion recognition. The P-adapter appends pseudo features to CNN features to further improve effectiveness and efficiency. With these adapters, models can be quickly adapted to various speech processing tasks. Our evaluation across four downstream tasks using five backbone models demonstrated the effectiveness of the proposed method. With the WavLM backbone, its performance was comparable to or better than that of full fine-tuning on all tasks while requiring 90% fewer learnable parameters.</li>
</ul>

<h3>Title: Enhancing Adversarial Text Attacks on BERT Models with Projected Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Hetvi Waghela, Jaydip Sen, Sneha Rakshit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21073">https://arxiv.org/abs/2407.21073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21073">https://arxiv.org/pdf/2407.21073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21073]] Enhancing Adversarial Text Attacks on BERT Models with Projected Gradient Descent(https://arxiv.org/abs/2407.21073)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks against deep learning models represent a major threat to the security and reliability of natural language processing (NLP) systems. In this paper, we propose a modification to the BERT-Attack framework, integrating Projected Gradient Descent (PGD) to enhance its effectiveness and robustness. The original BERT-Attack, designed for generating adversarial examples against BERT-based models, suffers from limitations such as a fixed perturbation budget and a lack of consideration for semantic similarity. The proposed approach in this work, PGD-BERT-Attack, addresses these limitations by leveraging PGD to iteratively generate adversarial examples while ensuring both imperceptibility and semantic similarity to the original input. Extensive experiments are conducted to evaluate the performance of PGD-BERT-Attack compared to the original BERT-Attack and other baseline methods. The results demonstrate that PGD-BERT-Attack achieves higher success rates in causing misclassification while maintaining low perceptual changes. Furthermore, PGD-BERT-Attack produces adversarial instances that exhibit greater semantic resemblance to the initial input, enhancing their applicability in real-world scenarios. Overall, the proposed modification offers a more effective and robust approach to adversarial attacks on BERT-based models, thus contributing to the advancement of defense against attacks on NLP systems.</li>
</ul>

<h3>Title: Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Somshubra Majumdar, Vahid Noroozi, Sean Narenthiran, Aleksander Ficek, Jagadeesh Balam, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21077">https://arxiv.org/abs/2407.21077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21077">https://arxiv.org/pdf/2407.21077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21077]] Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models(https://arxiv.org/abs/2407.21077)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) rely on instruction samples for alignment, but creating these datasets poses challenges, particularly in expert-dependent tasks like coding, which can be cost-prohibitive. One approach to mitigate these challenges is synthesizing data using another LLM. In this paper, we introduce a scalable method for generating synthetic instructions to enhance the code generation capability of LLMs. The proposed algorithm, Genetic-Instruct, mimics evolutionary processes, utilizing self-instruction to create numerous synthetic samples from a limited number of seeds. Genetic-Instruct is designed for efficient scaling of the generation process. Fine-tuning multiple coding LLMs with the synthetic samples demonstrates a significant improvement in their code generation accuracy compared to the baselines.</li>
</ul>

<h3>Title: Accelerating Large Language Model Inference with Self-Supervised Early Exits</h3>
<ul>
<li><strong>Authors: </strong>Florian Valade</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21082">https://arxiv.org/abs/2407.21082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21082">https://arxiv.org/pdf/2407.21082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21082]] Accelerating Large Language Model Inference with Self-Supervised Early Exits(https://arxiv.org/abs/2407.21082)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel technique for accelerating inference in large, pre-trained language models (LLMs) by introducing early exits during inference. The computational demands of these models, used across a wide range of applications, can be substantial. By capitalizing on the inherent variability in token complexity, our approach enables selective acceleration of the inference process. Specifically, we propose the integration of early exit ''heads'' atop existing transformer layers, which facilitate conditional terminations based on a confidence metric. These heads are trained in a self-supervised manner using the model's own predictions as training data, thereby eliminating the need for additional annotated data. The confidence metric, established using a calibration set, ensures a desired level of accuracy while enabling early termination when confidence exceeds a predetermined threshold. Notably, our method preserves the original accuracy and reduces computational time on certain tasks, leveraging the existing knowledge of pre-trained LLMs without requiring extensive retraining. This lightweight, modular modification has the potential to greatly enhance the practical usability of LLMs, particularly in applications like real-time language processing in resource-constrained environments.</li>
</ul>

<h3>Title: Learning Optimal Signal Temporal Logic Decision Trees for Classification: A Max-Flow MILP Formulation</h3>
<ul>
<li><strong>Authors: </strong>Kaier Liang, Gustavo A. Cardona, Disha Kamale, Cristian-Ioan Vasile</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21090">https://arxiv.org/abs/2407.21090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21090">https://arxiv.org/pdf/2407.21090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21090]] Learning Optimal Signal Temporal Logic Decision Trees for Classification: A Max-Flow MILP Formulation(https://arxiv.org/abs/2407.21090)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper presents a novel framework for inferring timed temporal logic properties from data. The dataset comprises pairs of finite-time system traces and corresponding labels, denoting whether the traces demonstrate specific desired behaviors, e.g. whether the ship follows a safe route or not. Our proposed approach leverages decision-tree-based methods to infer Signal Temporal Logic classifiers using primitive formulae. We formulate the inference process as a mixed integer linear programming optimization problem, recursively generating constraints to determine both data classification and tree structure. Applying a max-flow algorithm on the resultant tree transforms the problem into a global optimization challenge, leading to improved classification rates compared to prior methodologies. Moreover, we introduce a technique to reduce the number of constraints by exploiting the symmetry inherent in STL primitives, which enhances the algorithm's time performance and interpretability. To assess our algorithm's effectiveness and classification performance, we conduct three case studies involving two-class, multi-class, and complex formula classification scenarios.</li>
</ul>

<h3>Title: Zero Shot Health Trajectory Prediction Using Transformer</h3>
<ul>
<li><strong>Authors: </strong>Pawel Renc, Yugang Jia, Anthony E. Samir, Jaroslaw Was, Quanzheng Li, David W. Bates, Arkadiusz Sitek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21124">https://arxiv.org/abs/2407.21124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21124">https://arxiv.org/pdf/2407.21124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21124]] Zero Shot Health Trajectory Prediction Using Transformer(https://arxiv.org/abs/2407.21124)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Integrating modern machine learning and clinical decision-making has great promise for mitigating healthcare's increasing cost and complexity. We introduce the Enhanced Transformer for Health Outcome Simulation (ETHOS), a novel application of the transformer deep-learning architecture for analyzing high-dimensional, heterogeneous, and episodic health data. ETHOS is trained using Patient Health Timelines (PHTs)-detailed, tokenized records of health events-to predict future health trajectories, leveraging a zero-shot learning approach. ETHOS represents a significant advancement in foundation model development for healthcare analytics, eliminating the need for labeled data and model fine-tuning. Its ability to simulate various treatment pathways and consider patient-specific factors positions ETHOS as a tool for care optimization and addressing biases in healthcare delivery. Future developments will expand ETHOS' capabilities to incorporate a wider range of data types and data sources. Our work demonstrates a pathway toward accelerated AI development and deployment in healthcare.</li>
</ul>

<h3>Title: Self-supervised Multi-future Occupancy Forecasting for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Bernard Lange, Masha Itkina, Jiachen Li, Mykel J. Kochenderfer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21126">https://arxiv.org/abs/2407.21126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21126">https://arxiv.org/pdf/2407.21126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21126]] Self-supervised Multi-future Occupancy Forecasting for Autonomous Driving(https://arxiv.org/abs/2407.21126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Environment prediction frameworks are critical for the safe navigation of autonomous vehicles (AVs) in dynamic settings. LiDAR-generated occupancy grid maps (L-OGMs) offer a robust bird's-eye view for the scene representation, enabling self-supervised joint scene predictions while exhibiting resilience to partial observability and perception detection failures. Prior approaches have focused on deterministic L-OGM prediction architectures within the grid cell space. While these methods have seen some success, they frequently produce unrealistic predictions and fail to capture the stochastic nature of the environment. Additionally, they do not effectively integrate additional sensor modalities present in AVs. Our proposed framework performs stochastic L-OGM prediction in the latent space of a generative architecture and allows for conditioning on RGB cameras, maps, and planned trajectories. We decode predictions using either a single-step decoder, which provides high-quality predictions in real-time, or a diffusion-based batch decoder, which can further refine the decoded frames to address temporal consistency issues and reduce compression losses. Our experiments on the nuScenes and Waymo Open datasets show that all variants of our approach qualitatively and quantitatively outperform prior approaches.</li>
</ul>

<h3>Title: FL-DECO-BC: A Privacy-Preserving, Provably Secure, and Provenance-Preserving Federated Learning Framework with Decentralized Oracles on Blockchain for VANETs</h3>
<ul>
<li><strong>Authors: </strong>Sathwik Narkedimilli, Rayachoti Arun Kumar, N. V. Saran Kumar, Ramapathruni Praneeth Reddy, Pavan Kumar C</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21141">https://arxiv.org/abs/2407.21141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21141">https://arxiv.org/pdf/2407.21141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21141]] FL-DECO-BC: A Privacy-Preserving, Provably Secure, and Provenance-Preserving Federated Learning Framework with Decentralized Oracles on Blockchain for VANETs(https://arxiv.org/abs/2407.21141)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Vehicular Ad-Hoc Networks (VANETs) hold immense potential for improving traffic safety and efficiency. However, traditional centralized approaches for machine learning in VANETs raise concerns about data privacy and security. Federated Learning (FL) offers a solution that enables collaborative model training without sharing raw data. This paper proposes FL-DECO-BC as a novel privacy-preserving, provably secure, and provenance-preserving federated learning framework specifically designed for VANETs. FL-DECO-BC leverages decentralized oracles on blockchain to securely access external data sources while ensuring data privacy through advanced techniques. The framework guarantees provable security through cryptographic primitives and formal verification methods. Furthermore, FL-DECO-BC incorporates a provenance-preserving design to track data origin and history, fostering trust and accountability. This combination of features empowers VANETs with secure and privacy-conscious machine-learning capabilities, paving the way for advanced traffic management and safety applications.</li>
</ul>

<h3>Title: PLANesT-3D: A new annotated dataset for segmentation of 3D plant point clouds</h3>
<ul>
<li><strong>Authors: </strong>Kerem Mertoğlu, Yusuf Şalk, Server Karahan Sarıkaya, Kaya Turgut, Yasemin Evrenesoğlu, Hakan Çevikalp, Ömer Nezih Gerek, Helin Dutağacı, David Rousseau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21150">https://arxiv.org/abs/2407.21150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21150">https://arxiv.org/pdf/2407.21150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21150]] PLANesT-3D: A new annotated dataset for segmentation of 3D plant point clouds(https://arxiv.org/abs/2407.21150)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Creation of new annotated public datasets is crucial in helping advances in 3D computer vision and machine learning meet their full potential for automatic interpretation of 3D plant models. In this paper, we introduce PLANesT-3D; a new annotated dataset of 3D color point clouds of plants. PLANesT-3D is composed of 34 point cloud models representing 34 real plants from three different plant species: \textit{Capsicum annuum}, \textit{Rosa kordana}, and \textit{Ribes rubrum}. Both semantic labels in terms of "leaf" and "stem", and organ instance labels were manually annotated for the full point clouds. As an additional contribution, SP-LSCnet, a novel semantic segmentation method that is a combination of unsupervised superpoint extraction and a 3D point-based deep learning approach is introduced and evaluated on the new dataset. Two existing deep neural network architectures, PointNet++ and RoseSegNet were also tested on the point clouds of PLANesT-3D for semantic segmentation.</li>
</ul>

<h3>Title: Private Collaborative Edge Inference via Over-the-Air Computation</h3>
<ul>
<li><strong>Authors: </strong>Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21151">https://arxiv.org/abs/2407.21151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21151">https://arxiv.org/pdf/2407.21151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21151]] Private Collaborative Edge Inference via Over-the-Air Computation(https://arxiv.org/abs/2407.21151)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We consider collaborative inference at the wireless edge, where each client's model is trained independently on their local datasets. Clients are queried in parallel to make an accurate decision collaboratively. In addition to maximizing the inference accuracy, we also want to ensure the privacy of local models. To this end, we leverage the superposition property of the multiple access channel to implement bandwidth-efficient multi-user inference methods. Specifically, we propose different methods for ensemble and multi-view classification that exploit over-the-air computation. We show that these schemes perform better than their orthogonal counterparts with statistically significant differences while using fewer resources and providing privacy guarantees. We also provide experimental results verifying the benefits of the proposed over-the-air multi-user inference approach and perform an ablation study to demonstrate the effectiveness of our design choices. We share the source code of the framework publicly on Github to facilitate further research and reproducibility.</li>
</ul>

<h3>Title: Event-Arguments Extraction Corpus and Modeling using BERT for Arabic</h3>
<ul>
<li><strong>Authors: </strong>Alaa Aljabari, Lina Duaibes, Mustafa Jarrar, Mohammed Khalilia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21153">https://arxiv.org/abs/2407.21153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21153">https://arxiv.org/pdf/2407.21153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21153]] Event-Arguments Extraction Corpus and Modeling using BERT for Arabic(https://arxiv.org/abs/2407.21153)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Event-argument extraction is a challenging task, particularly in Arabic due to sparse linguistic resources. To fill this gap, we introduce the \hadath corpus ($550$k tokens) as an extension of Wojood, enriched with event-argument annotations. We used three types of event arguments: $agent$, $location$, and $date$, which we annotated as relation types. Our inter-annotator agreement evaluation resulted in $82.23\%$ $Kappa$ score and $87.2\%$ $F_1$-score. Additionally, we propose a novel method for event relation extraction using BERT, in which we treat the task as text entailment. This method achieves an $F_1$-score of $94.01\%$. To further evaluate the generalization of our proposed method, we collected and annotated another out-of-domain corpus (about $80$k tokens) called \testNLI and used it as a second test set, on which our approach achieved promising results ($83.59\%$ $F_1$-score). Last but not least, we propose an end-to-end system for event-arguments extraction. This system is implemented as part of SinaTools, and both corpora are publicly available at {\small \url{this https URL}}</li>
</ul>

<h3>Title: Embedding Space Selection for Detecting Memorization and Fingerprinting in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jack He, Jianxing Zhao, Andrew Bai, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21159">https://arxiv.org/abs/2407.21159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21159">https://arxiv.org/pdf/2407.21159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21159]] Embedding Space Selection for Detecting Memorization and Fingerprinting in Generative Models(https://arxiv.org/abs/2407.21159)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of artificial intelligence, generative models such as Generative Adversarial Networks (GANs) and Diffusion Models have become cornerstone technologies, driving innovation in diverse fields from art creation to healthcare. Despite their potential, these models face the significant challenge of data memorization, which poses risks to privacy and the integrity of generated content. Among various metrics of memorization detection, our study delves into the memorization scores calculated from encoder layer embeddings, which involves measuring distances between samples in the embedding spaces. Particularly, we find that the memorization scores calculated from layer embeddings of Vision Transformers (ViTs) show an notable trend - the latter (deeper) the layer, the less the memorization measured. It has been found that the memorization scores from the early layers' embeddings are more sensitive to low-level memorization (e.g. colors and simple patterns for an image), while those from the latter layers are more sensitive to high-level memorization (e.g. semantic meaning of an image). We also observe that, for a specific model architecture, its degree of memorization on different levels of information is unique. It can be viewed as an inherent property of the architecture. Building upon this insight, we introduce a unique fingerprinting methodology. This method capitalizes on the unique distributions of the memorization score across different layers of ViTs, providing a novel approach to identifying models involved in generating deepfakes and malicious content. Our approach demonstrates a marked 30% enhancement in identification accuracy over existing baseline methods, offering a more effective tool for combating digital misinformation.</li>
</ul>

<h3>Title: Decomposed Prompting to Answer Questions on a Course Discussion Board</h3>
<ul>
<li><strong>Authors: </strong>Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21170">https://arxiv.org/abs/2407.21170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21170">https://arxiv.org/pdf/2407.21170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21170]] Decomposed Prompting to Answer Questions on a Course Discussion Board(https://arxiv.org/abs/2407.21170)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose and evaluate a question-answering system that uses decomposed prompting to classify and answer student questions on a course discussion board. Our system uses a large language model (LLM) to classify questions into one of four types: conceptual, homework, logistics, and not answerable. This enables us to employ a different strategy for answering questions that fall under different types. Using a variant of GPT-3, we achieve $81\%$ classification accuracy. We discuss our system's performance on answering conceptual questions from a machine learning course and various failure modes.</li>
</ul>

<h3>Title: AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning</h3>
<ul>
<li><strong>Authors: </strong>Maisha Binte Rashid, Pablo Rivas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21174">https://arxiv.org/abs/2407.21174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21174">https://arxiv.org/pdf/2407.21174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21174]] AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning(https://arxiv.org/abs/2407.21174)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversarial attacks. This paper presents an effective strategy to enhance the robustness of multimodal image captioning models against such attacks. By leveraging the Fast Gradient Sign Method (FGSM) to generate adversarial examples and incorporating adversarial training techniques, we demonstrate improved model robustness on two benchmark datasets: Flickr8k and COCO. Our findings indicate that selectively training only the text decoder of the multimodal architecture shows performance comparable to full adversarial training while offering increased computational efficiency. This targeted approach suggests a balance between robustness and training costs, facilitating the ethical deployment of multimodal AI systems across various domains.</li>
</ul>

<h3>Title: DKL-KAN: Scalable Deep Kernel Learning using Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Shrenik Zinage, Sudeepta Mondal, Soumalya Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21176">https://arxiv.org/abs/2407.21176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21176">https://arxiv.org/pdf/2407.21176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21176]] DKL-KAN: Scalable Deep Kernel Learning using Kolmogorov-Arnold Networks(https://arxiv.org/abs/2407.21176)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The need for scalable and expressive models in machine learning is paramount, particularly in applications requiring both structural depth and flexibility. Traditional deep learning methods, such as multilayer perceptrons (MLP), offer depth but lack ability to integrate structural characteristics of deep learning architectures with non-parametric flexibility of kernel methods. To address this, deep kernel learning (DKL) was introduced, where inputs to a base kernel are transformed using a deep learning architecture. These kernels can replace standard kernels, allowing both expressive power and scalability. The advent of Kolmogorov-Arnold Networks (KAN) has generated considerable attention and discussion among researchers in scientific domain. In this paper, we introduce a scalable deep kernel using KAN (DKL-KAN) as an effective alternative to DKL using MLP (DKL-MLP). Our approach involves simultaneously optimizing these kernel attributes using marginal likelihood within a Gaussian process framework. We analyze two variants of DKL-KAN for a fair comparison with DKL-MLP: one with same number of neurons and layers as DKL-MLP, and another with approximately same number of trainable parameters. To handle large datasets, we use kernel interpolation for scalable structured Gaussian processes (KISS-GP) for low-dimensional inputs and KISS-GP with product kernels for high-dimensional inputs. The efficacy of DKL-KAN is evaluated in terms of computational training time and test prediction accuracy across a wide range of applications. Additionally, the effectiveness of DKL-KAN is also examined in modeling discontinuities and accurately estimating prediction uncertainty. The results indicate that DKL-KAN outperforms DKL-MLP on datasets with a low number of observations. Conversely, DKL-MLP exhibits better scalability and higher test prediction accuracy on datasets with large number of observations.</li>
</ul>

<h3>Title: Amelia: A Large Model and Dataset for Airport Surface Movement Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Ingrid Navarro, Pablo Ortega-Kral, Jay Patrikar, Haichuan Wang, Zelin Ye, Jong Hoon Park, Jean Oh, Sebastian Scherer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21185">https://arxiv.org/abs/2407.21185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21185">https://arxiv.org/pdf/2407.21185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21185]] Amelia: A Large Model and Dataset for Airport Surface Movement Forecasting(https://arxiv.org/abs/2407.21185)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The growing demand for air travel requires technological advancements in air traffic management as well as mechanisms for monitoring and ensuring safe and efficient operations. In terminal airspaces, predictive models of future movements and traffic flows can help with proactive planning and efficient coordination; however, varying airport topologies, and interactions with other agents, among other factors, make accurate predictions challenging. Data-driven predictive models have shown promise for handling numerous variables to enable various downstream tasks, including collision risk assessment, taxi-out time prediction, departure metering, and emission estimations. While data-driven methods have shown improvements in these tasks, prior works lack large-scale curated surface movement datasets within the public domain and the development of generalizable trajectory forecasting models. In response to this, we propose two contributions: (1) Amelia-48, a large surface movement dataset collected using the System Wide Information Management (SWIM) Surface Movement Event Service (SMES). With data collection beginning in Dec 2022, the dataset provides more than a year's worth of SMES data (~30TB) and covers 48 airports within the US National Airspace System. In addition to releasing this data in the public domain, we also provide post-processing scripts and associated airport maps to enable research in the forecasting domain and beyond. (2) Amelia-TF model, a transformer-based next-token-prediction large multi-agent multi-airport trajectory forecasting model trained on 292 days or 9.4 billion tokens of position data encompassing 10 different airports with varying topology. The open-sourced model is validated on unseen airports with experiments showcasing the different prediction horizon lengths, ego-agent selection strategies, and training recipes to demonstrate the generalization capabilities.</li>
</ul>

<h3>Title: LFFR: Logistic Function For (multi-output) Regression</h3>
<ul>
<li><strong>Authors: </strong>John Chiang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21187">https://arxiv.org/abs/2407.21187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21187">https://arxiv.org/pdf/2407.21187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21187]] LFFR: Logistic Function For (multi-output) Regression(https://arxiv.org/abs/2407.21187)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust</a></li>
<li><strong>Abstract: </strong>In this manuscript, we extend our previous work on privacy-preserving regression to address multi-output regression problems using data encrypted under a fully homomorphic encryption scheme. We build upon the simplified fixed Hessian approach for linear and ridge regression and adapt our novel LFFR algorithm, initially designed for single-output logistic regression, to handle multiple outputs. We further refine the constant simplified Hessian method for the multi-output context, ensuring computational efficiency and robustness. Evaluations on multiple real-world datasets demonstrate the effectiveness of our multi-output LFFR algorithm, highlighting its capability to maintain privacy while achieving high predictive accuracy. Normalizing both data and target predictions remains essential for optimizing homomorphic encryption parameters, confirming the practicality of our approach for secure and efficient multi-output regression tasks.</li>
</ul>

<h3>Title: Diffusion-Based Generation of Neural Activity from Disentangled Latent Codes</h3>
<ul>
<li><strong>Authors: </strong>Jonathan D. McCart, Andrew R. Sedler, Christopher Versteeg, Domenick Mifsud, Mattia Rigotti-Thompson, Chethan Pandarinath</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21195">https://arxiv.org/abs/2407.21195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21195">https://arxiv.org/pdf/2407.21195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21195]] Diffusion-Based Generation of Neural Activity from Disentangled Latent Codes(https://arxiv.org/abs/2407.21195)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in recording technology have allowed neuroscientists to monitor activity from thousands of neurons simultaneously. Latent variable models are increasingly valuable for distilling these recordings into compact and interpretable representations. Here we propose a new approach to neural data analysis that leverages advances in conditional generative modeling to enable the unsupervised inference of disentangled behavioral variables from recorded neural activity. Our approach builds on InfoDiffusion, which augments diffusion models with a set of latent variables that capture important factors of variation in the data. We apply our model, called Generating Neural Observations Conditioned on Codes with High Information (GNOCCHI), to time series neural data and test its application to synthetic and biological recordings of neural activity during reaching. In comparison to a VAE-based sequential autoencoder, GNOCCHI learns higher-quality latent spaces that are more clearly structured and more disentangled with respect to key behavioral variables. These properties enable accurate generation of novel samples (unseen behavioral conditions) through simple linear traversal of the latent spaces produced by GNOCCHI. Our work demonstrates the potential of unsupervised, information-based models for the discovery of interpretable latent spaces from neural data, enabling researchers to generate high-quality samples from unseen conditions.</li>
</ul>

<h3>Title: NeuroSEM: A hybrid framework for simulating multiphysics problems by coupling PINNs and spectral elements</h3>
<ul>
<li><strong>Authors: </strong>Khemraj Shukla, Zongren Zou, Chi Hin Chan, Additi Pandey, Zhicheng Wang, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21217">https://arxiv.org/abs/2407.21217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21217">https://arxiv.org/pdf/2407.21217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21217]] NeuroSEM: A hybrid framework for simulating multiphysics problems by coupling PINNs and spectral elements(https://arxiv.org/abs/2407.21217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multiphysics problems that are characterized by complex interactions among fluid dynamics, heat transfer, structural mechanics, and electromagnetics, are inherently challenging due to their coupled nature. While experimental data on certain state variables may be available, integrating these data with numerical solvers remains a significant challenge. Physics-informed neural networks (PINNs) have shown promising results in various engineering disciplines, particularly in handling noisy data and solving inverse problems. However, their effectiveness in forecasting nonlinear phenomena in multiphysics regimes is yet to be fully established. This study introduces NeuroSEM, a hybrid framework integrating PINNs with the high-fidelity Spectral Element Method (SEM) solver, Nektar++. NeuroSEM leverages strengths of both PINNs and SEM, providing robust solutions for multiphysics problems. PINNs are trained to assimilate data and model physical phenomena in specific subdomains, which are then integrated into Nektar++. We demonstrate the efficiency and accuracy of NeuroSEM for thermal convection in cavity flow and flow past a cylinder. The framework effectively handles data assimilation by addressing those subdomains and state variables where data are available. We applied NeuroSEM to the Rayleigh-Bénard convection system, including cases with missing thermal boundary conditions. Our results indicate that NeuroSEM accurately models the physical phenomena and assimilates the data within the specified subdomains. The framework's plug-and-play nature facilitates its extension to other multiphysics or multiscale problems. Furthermore, NeuroSEM is optimized for an efficient execution on emerging integrated GPU-CPU architectures. This hybrid approach enhances the accuracy and efficiency of simulations, making it a powerful tool for tackling complex engineering challenges in various scientific domains.</li>
</ul>

<h3>Title: DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers</h3>
<ul>
<li><strong>Authors: </strong>C. A. Martínez-Mejía, J. Solano, J. Breier, D. Bucko, X. Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21220">https://arxiv.org/abs/2407.21220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21220">https://arxiv.org/pdf/2407.21220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21220]] DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers(https://arxiv.org/abs/2407.21220)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Machine Learning using neural networks has received prominent attention recently because of its success in solving a wide variety of computational tasks, in particular in the field of computer vision. However, several works have drawn attention to potential security risks involved with the training and implementation of such networks. In this work, we introduce DeepBaR, a novel approach that implants backdoors on neural networks by faulting their behavior at training, especially during fine-tuning. Our technique aims to generate adversarial samples by optimizing a custom loss function that mimics the implanted backdoors while adding an almost non-visible trigger in the image. We attack three popular convolutional neural network architectures and show that DeepBaR attacks have a success rate of up to 98.30\%. Furthermore, DeepBaR does not significantly affect the accuracy of the attacked networks after deployment when non-malicious inputs are given. Remarkably, DeepBaR allows attackers to choose an input that looks similar to a given class, from a human perspective, but that will be classified as belonging to an arbitrary target class.</li>
</ul>

<h3>Title: Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration</h3>
<ul>
<li><strong>Authors: </strong>Ngoc Son Nguyen, Van Son Nguyen, Tung Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21229">https://arxiv.org/abs/2407.21229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21229">https://arxiv.org/pdf/2407.21229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21229]] Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration(https://arxiv.org/abs/2407.21229)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) has recently emerged as a potential research domain, captivating the interest of many in the field of artificial intelligence and computer vision. Despite the prevalence of approaches in English, there is a notable lack of systems specifically developed for certain languages, particularly Vietnamese. This study aims to bridge this gap by conducting comprehensive experiments on the Vietnamese Visual Question Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed model. In response to community interest, we have developed a model that enhances image representation capabilities, thereby improving overall performance in the ViVQA system. Specifically, our model integrates the Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2) and the convolutional neural network EfficientNet to extract and process both local and global features from images. This integration leverages the strengths of transformer-based architectures for capturing comprehensive contextual information and convolutional networks for detailed local features. By freezing the parameters of these pre-trained models, we significantly reduce the computational cost and training time, while maintaining high performance. This approach significantly improves image representation and enhances the performance of existing VQA systems. We then leverage a multi-modal fusion module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse the information between visual and textual features. Our experimental findings demonstrate that our model surpasses competing baselines, achieving promising performance. This is particularly evident in its accuracy of $71.04\%$ on the test set of the ViVQA dataset, marking a significant advancement in our research area. The code is available at this https URL.</li>
</ul>

<h3>Title: GNUMAP: A Parameter-Free Approach to Unsupervised Dimensionality Reduction via Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jihee You, So Won Jeong, Claire Donnat</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21236">https://arxiv.org/abs/2407.21236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21236">https://arxiv.org/pdf/2407.21236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21236]] GNUMAP: A Parameter-Free Approach to Unsupervised Dimensionality Reduction via Graph Neural Networks(https://arxiv.org/abs/2407.21236)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the proliferation of Graph Neural Network (GNN) methods stemming from contrastive learning, unsupervised node representation learning for graph data is rapidly gaining traction across various fields, from biology to molecular dynamics, where it is often used as a dimensionality reduction tool. However, there remains a significant gap in understanding the quality of the low-dimensional node representations these methods produce, particularly beyond well-curated academic datasets. To address this gap, we propose here the first comprehensive benchmarking of various unsupervised node embedding techniques tailored for dimensionality reduction, encompassing a range of manifold learning tasks, along with various performance metrics. We emphasize the sensitivity of current methods to hyperparameter choices -- highlighting a fundamental issue as to their applicability in real-world settings where there is no established methodology for rigorous hyperparameter selection. Addressing this issue, we introduce GNUMAP, a robust and parameter-free method for unsupervised node representation learning that merges the traditional UMAP approach with the expressivity of the GNN framework. We show that GNUMAP consistently outperforms existing state-of-the-art GNN embedding methods in a variety of contexts, including synthetic geometric datasets, citation networks, and real-world biomedical data -- making it a simple but reliable dimensionality reduction tool.</li>
</ul>

<h3>Title: Informed Correctors for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yixiu Zhao, Jiaxin Shi, Lester Mackey, Scott Linderman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21243">https://arxiv.org/abs/2407.21243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21243">https://arxiv.org/pdf/2407.21243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21243]] Informed Correctors for Discrete Diffusion Models(https://arxiv.org/abs/2407.21243)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion modeling is a promising framework for modeling and generating data in discrete spaces. To sample from these models, different strategies present trade-offs between computation and sample quality. A predominant sampling strategy is predictor-corrector $\tau$-leaping, which simulates the continuous time generative process with discretized predictor steps and counteracts the accumulation of discretization error via corrector steps. However, for absorbing state diffusion, an important class of discrete diffusion models, the standard forward-backward corrector can be ineffective in fixing such errors, resulting in subpar sample quality. To remedy this problem, we propose a family of informed correctors that more reliably counteracts discretization error by leveraging information learned by the model. For further efficiency gains, we also propose $k$-Gillespie's, a sampling algorithm that better utilizes each model evaluation, while still enjoying the speed and flexibility of $\tau$-leaping. Across several real and synthetic datasets, we show that $k$-Gillespie's with informed correctors reliably produces higher quality samples at lower computational cost.</li>
</ul>

<h3>Title: Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens</h3>
<ul>
<li><strong>Authors: </strong>Anqi Zhang, Chaofeng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21248">https://arxiv.org/abs/2407.21248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21248">https://arxiv.org/pdf/2407.21248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21248]] Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens(https://arxiv.org/abs/2407.21248)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) are extensively used, there are raising concerns regarding privacy, security, and copyright due to their opaque training data, which brings the problem of detecting pre-training data on the table. Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is "certain but wrong", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data. The method can be applied without any access to the the pre-training data corpus or additional training like reference models. Our approach exhibits a consistent enhancement compared to existing methods in diverse experiments conducted on various benchmarks and models, achieving a maximum improvement of 29.5\%. We also introduce a new benchmark Dolma-Book developed upon a novel framework, which employs book data collected both before and after model training to provide further evaluation.</li>
</ul>

<h3>Title: Leveraging Adaptive Implicit Representation Mapping for Ultra High-Resolution Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Zhao, Xiaoguang Li, Pingping Cai, Canyu Zhang, Song Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21256">https://arxiv.org/abs/2407.21256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21256">https://arxiv.org/pdf/2407.21256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21256]] Leveraging Adaptive Implicit Representation Mapping for Ultra High-Resolution Image Segmentation(https://arxiv.org/abs/2407.21256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Implicit representation mapping (IRM) can translate image features to any continuous resolution, showcasing its potent capability for ultra-high-resolution image segmentation refinement. Current IRM-based methods for refining ultra-high-resolution image segmentation often rely on CNN-based encoders to extract image features and apply a Shared Implicit Representation Mapping Function (SIRMF) to convert pixel-wise features into segmented results. Hence, these methods exhibit two crucial limitations. Firstly, the CNN-based encoder may not effectively capture long-distance information, resulting in a lack of global semantic information in the pixel-wise features. Secondly, SIRMF is shared across all samples, which limits its ability to generalize and handle diverse inputs. To address these limitations, we propose a novel approach that leverages the newly proposed Adaptive Implicit Representation Mapping (AIRM) for ultra-high-resolution Image Segmentation. Specifically, the proposed method comprises two components: (1) the Affinity Empowered Encoder (AEE), a robust feature extractor that leverages the benefits of the transformer architecture and semantic affinity to model long-distance features effectively, and (2) the Adaptive Implicit Representation Mapping Function (AIRMF), which adaptively translates pixel-wise features without neglecting the global semantic information, allowing for flexible and precise feature translation. We evaluated our method on the commonly used ultra-high-resolution segmentation refinement datasets, i.e., BIG and PASCAL VOC 2012. The extensive experiments demonstrate that our method outperforms competitors by a large margin. The code is provided in supplementary material.</li>
</ul>

<h3>Title: Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21264">https://arxiv.org/abs/2407.21264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21264">https://arxiv.org/pdf/2407.21264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21264]] Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning(https://arxiv.org/abs/2407.21264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Model attribution for machine-generated disinformation poses a significant challenge in understanding its origins and mitigating its spread. This task is especially challenging because modern large language models (LLMs) produce disinformation with human-like quality. Additionally, the diversity in prompting methods used to generate disinformation complicates accurate source attribution. These methods introduce domain-specific features that can mask the fundamental characteristics of the models. In this paper, we introduce the concept of model attribution as a domain generalization problem, where each prompting method represents a unique domain. We argue that an effective attribution model must be invariant to these domain-specific features. It should also be proficient in identifying the originating models across all scenarios, reflecting real-world detection challenges. To address this, we introduce a novel approach based on Supervised Contrastive Learning. This method is designed to enhance the model's robustness to variations in prompts and focuses on distinguishing between different source LLMs. We evaluate our model through rigorous experiments involving three common prompting methods: ``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs: ``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the effectiveness of our approach in model attribution tasks, achieving state-of-the-art performance across diverse and unseen datasets.</li>
</ul>

<h3>Title: DDU-Net: A Domain Decomposition-based CNN on Multiple GPUs</h3>
<ul>
<li><strong>Authors: </strong>Corné Verburg, Alexander Heinlein, Eric C. Cyr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21266">https://arxiv.org/abs/2407.21266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21266">https://arxiv.org/pdf/2407.21266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21266]] DDU-Net: A Domain Decomposition-based CNN on Multiple GPUs(https://arxiv.org/abs/2407.21266)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The segmentation of ultra-high resolution images poses challenges such as loss of spatial information or computational inefficiency. In this work, a novel approach that combines encoder-decoder architectures with domain decomposition strategies to address these challenges is proposed. Specifically, a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which partitions input images into non-overlapping patches that can be processed independently on separate devices. A communication network is added to facilitate inter-patch information exchange to enhance the understanding of spatial context. Experimental validation is performed on a synthetic dataset that is designed to measure the effectiveness of the communication network. Then, the performance is tested on the DeepGlobe land cover classification dataset as a real-world benchmark data set. The results demonstrate that the approach, which includes inter-patch communication for images divided into $16\times16$ non-overlapping subimages, achieves a $2-3\,\%$ higher intersection over union (IoU) score compared to the same network without inter-patch communication. The performance of the network which includes communication is equivalent to that of a baseline U-Net trained on the full image, showing that our model provides an effective solution for segmenting ultra-high-resolution images while preserving spatial context. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net</h3>
<ul>
<li><strong>Authors: </strong>Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21273">https://arxiv.org/abs/2407.21273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21273">https://arxiv.org/pdf/2407.21273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21273]] Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net(https://arxiv.org/abs/2407.21273)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Efficient intravascular access in trauma and critical care significantly impacts patient outcomes. However, the availability of skilled medical personnel in austere environments is often limited. Autonomous robotic ultrasound systems can aid in needle insertion for medication delivery and support non-experts in such tasks. Despite advances in autonomous needle insertion, inaccuracies in vessel segmentation predictions pose risks. Understanding the uncertainty of predictive models in ultrasound imaging is crucial for assessing their reliability. We introduce MSU-Net, a novel multistage approach for training an ensemble of U-Nets to yield accurate ultrasound image segmentation maps. We demonstrate substantial improvements, 18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model transparency, and trustworthiness. By highlighting areas of model certainty, MSU-Net can guide safe needle insertions, empowering non-experts to accomplish such tasks.</li>
</ul>

<h3>Title: FedBChain: A Blockchain-enabled Federated Learning Framework for Improving DeepConvLSTM with Comparative Strategy Insights</h3>
<ul>
<li><strong>Authors: </strong>Gaoxuan Li, Chern Hong Lim, Qiyao Ma, Xinyu Tang, Hwa Hui Tew</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21282">https://arxiv.org/abs/2407.21282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21282">https://arxiv.org/pdf/2407.21282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21282]] FedBChain: A Blockchain-enabled Federated Learning Framework for Improving DeepConvLSTM with Comparative Strategy Insights(https://arxiv.org/abs/2407.21282)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Recent research in the field of Human Activity Recognition has shown that an improvement in prediction performance can be achieved by reducing the number of LSTM layers. However, this kind of enhancement is only significant on monolithic architectures, and when it runs on large-scale distributed training, data security and privacy issues will be reconsidered, and its prediction performance is unknown. In this paper, we introduce a novel framework: FedBChain, which integrates the federated learning paradigm based on a modified DeepConvLSTM architecture with a single LSTM layer. This framework performs comparative tests of prediction performance on three different real-world datasets based on three different hidden layer units (128, 256, and 512) combined with five different federated learning strategies, respectively. The results show that our architecture has significant improvements in Precision, Recall and F1-score compared to the centralized training approach on all datasets with all hidden layer units for all strategies: FedAvg strategy improves on average by 4.54%, FedProx improves on average by 4.57%, FedTrimmedAvg improves on average by 4.35%, Krum improves by 4.18% on average, and FedAvgM improves by 4.46% on average. Based on our results, it can be seen that FedBChain not only improves in performance, but also guarantees the security and privacy of user data compared to centralized training methods during the training process. The code for our experiments is publicly available (this https URL).</li>
</ul>

<h3>Title: Robust Box Prompt based SAM for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Huang, Xin Yang, Han Zhou, Yan Cao, Haoran Dou, Fajin Dong, Dong Ni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21284">https://arxiv.org/abs/2407.21284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21284">https://arxiv.org/pdf/2407.21284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21284]] Robust Box Prompt based SAM for Medical Image Segmentation(https://arxiv.org/abs/2407.21284)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) can achieve satisfactory segmentation performance under high-quality box prompts. However, SAM's robustness is compromised by the decline in box quality, limiting its practicality in clinical reality. In this study, we propose a novel Robust Box prompt based SAM (\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts with different qualities. Our contribution is three-fold. First, we propose a prompt refinement module to implicitly perceive the potential targets, and output the offsets to directly transform the low-quality box prompt into a high-quality one. We then provide an online iterative strategy for further prompt refinement. Second, we introduce a prompt enhancement module to automatically generate point prompts to assist the box-promptable segmentation effectively. Last, we build a self-information extractor to encode the prior information from the input image. These features can optimize the image embeddings and attention calculation, thus, the robustness of SAM can be further enhanced. Extensive experiments on the large medical segmentation dataset including 99,299 images, 5 modalities, and 25 organs/targets validated the efficacy of our proposed RoBox-SAM.</li>
</ul>

<h3>Title: Fine-grained Metrics for Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhuheng Lu, Ting Wu, Yuewei Dai, Weiqing Li, Zhiyong Su</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21289">https://arxiv.org/abs/2407.21289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21289">https://arxiv.org/pdf/2407.21289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21289]] Fine-grained Metrics for Point Cloud Semantic Segmentation(https://arxiv.org/abs/2407.21289)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Two forms of imbalances are commonly observed in point cloud semantic segmentation datasets: (1) category imbalances, where certain objects are more prevalent than others; and (2) size imbalances, where certain objects occupy more points than others. Because of this, the majority of categories and large objects are favored in the existing evaluation metrics. This paper suggests fine-grained mIoU and mAcc for a more thorough assessment of point cloud segmentation algorithms in order to address these issues. Richer statistical information is provided for models and datasets by these fine-grained metrics, which also lessen the bias of current semantic segmentation metrics towards large objects. The proposed metrics are used to train and assess various semantic segmentation algorithms on three distinct indoor and outdoor semantic segmentation datasets.</li>
</ul>

<h3>Title: TrackSorter: A Transformer-based sorting algorithm for track finding in High Energy Physics</h3>
<ul>
<li><strong>Authors: </strong>Yash Melkani, Xiangyang Ju</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21290">https://arxiv.org/abs/2407.21290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21290">https://arxiv.org/pdf/2407.21290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21290]] TrackSorter: A Transformer-based sorting algorithm for track finding in High Energy Physics(https://arxiv.org/abs/2407.21290)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Track finding in particle data is a challenging pattern recognition problem in High Energy Physics. It takes as inputs a point cloud of space points and labels them so that space points created by the same particle have the same label. The list of space points with the same label is a track candidate. We argue that this pattern recognition problem can be formulated as a sorting problem, of which the inputs are a list of space points sorted by their distances away from the collision points and the outputs are the space points sorted by their labels. In this paper, we propose the TrackSorter algorithm: a Transformer-based algorithm for pattern recognition in particle data. TrackSorter uses a simple tokenization scheme to convert space points into discrete tokens. It then uses the tokenized space points as inputs and sorts the input tokens into track candidates. TrackSorter is a novel end-to-end track finding algorithm that leverages Transformer-based models to solve pattern recognition problems. It is evaluated on the TrackML dataset and has good track finding performance.</li>
</ul>

<h3>Title: SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21293">https://arxiv.org/abs/2407.21293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21293">https://arxiv.org/pdf/2407.21293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21293]] SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving(https://arxiv.org/abs/2407.21293)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Many fields could benefit from the rapid development of the large language models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the typically fields facing new opportunities as the LLMs have supported more and more modalities. Here, by utilizing vision-language model (VLM), we proposed an e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided into four stages, which are perception, prediction, planning, and behavior. Each stage consists of several visual question answering (VQA) pairs and VQA pairs interconnect with each other constructing a graph called Graph VQA (GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our method could achieve e2e driving with language. In our method, vision transformers (ViT) models are employed to process nuScenes visual data, while VLM are utilized to interpret and reason about the information extracted from the visual inputs. In the perception stage, the system identifies and classifies objects from the driving environment. The prediction stage involves forecasting the potential movements of these objects. The planning stage utilizes the gathered information to develop a driving strategy, ensuring the safety and efficiency of the autonomous vehicle. Finally, the behavior stage translates the planned actions into executable commands for the vehicle. Our experiments demonstrate that SimpleLLM4AD achieves competitive performance in complex driving scenarios.</li>
</ul>

<h3>Title: A Vectorization Method Induced By Maximal Margin Classification For Persistent Diagrams</h3>
<ul>
<li><strong>Authors: </strong>An Wu, Yu Pan, Fuqi Zhou, Jinghui Yan, Chuanlu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21298">https://arxiv.org/abs/2407.21298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21298">https://arxiv.org/pdf/2407.21298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21298]] A Vectorization Method Induced By Maximal Margin Classification For Persistent Diagrams(https://arxiv.org/abs/2407.21298)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Persistent homology is an effective method for extracting topological information, represented as persistent diagrams, of spatial structure data. Hence it is well-suited for the study of protein structures. Attempts to incorporate Persistent homology in machine learning methods of protein function prediction have resulted in several techniques for vectorizing persistent diagrams. However, current vectorization methods are excessively artificial and cannot ensure the effective utilization of information or the rationality of the methods. To address this problem, we propose a more geometrical vectorization method of persistent diagrams based on maximal margin classification for Banach space, and additionaly propose a framework that utilizes topological data analysis to identify proteins with specific functions. We evaluated our vectorization method using a binary classification task on proteins and compared it with the statistical methods that exhibit the best performance among thirteen commonly used vectorization methods. The experimental results indicate that our approach surpasses the statistical methods in both robustness and precision.</li>
</ul>

<h3>Title: EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Ali Abedi, Q. M. Jonathan Wu, Ning Zhang, Farhad Pourpanah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21311">https://arxiv.org/abs/2407.21311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21311">https://arxiv.org/pdf/2407.21311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21311]] EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer(https://arxiv.org/abs/2407.21311)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue, where the distribution of training (source) data differs from that of testing (target) data. Many models have been developed to tackle this problem, and recently vision transformers (ViTs) have shown promising results. However, the complexity and large number of trainable parameters of ViTs restrict their deployment in practical applications. This underscores the need for an efficient model that not only reduces trainable parameters but also allows for adjustable complexity based on specific needs while delivering comparable performance. To achieve this, in this paper we introduce an Efficient Unsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which is a self-supervised ViT, as a feature extractor followed by a simplified bottleneck of fully connected layers to refine features for enhanced domain adaptation. Additionally, EUDA employs the synergistic domain alignment loss (SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD) losses, to balance adaptation by minimizing classification errors in the source domain while aligning the source and target domain distributions. The experimental results indicate the effectiveness of EUDA in producing comparable results as compared with other state-of-the-art methods in domain adaptation with significantly fewer trainable parameters, between 42% to 99.7% fewer. This showcases the ability to train the model in a resource-limited environment. The code of the model is available at: this https URL.</li>
</ul>

<h3>Title: State-observation augmented diffusion model for nonlinear assimilation</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyuan Li, Bin Dong, Pingwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21314">https://arxiv.org/abs/2407.21314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21314">https://arxiv.org/pdf/2407.21314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21314]] State-observation augmented diffusion model for nonlinear assimilation(https://arxiv.org/abs/2407.21314)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data assimilation has become a crucial technique aiming to combine physical models with observational data to estimate state variables. Traditional assimilation algorithms often face challenges of high nonlinearity brought by both the physical and observational models. In this work, we propose a novel data-driven assimilation algorithm based on generative models to address such concerns. Our State-Observation Augmented Diffusion (SOAD) model is designed to handle nonlinear physical and observational models more effectively. The marginal posterior associated with SOAD has been derived and then proved to match the real posterior under mild assumptions, which shows theoretical superiority over previous score-based assimilation works. Experimental results also indicate that our SOAD model may offer improved accuracy over existing data-driven methods.</li>
</ul>

<h3>Title: Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances</h3>
<ul>
<li><strong>Authors: </strong>Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21315">https://arxiv.org/abs/2407.21315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21315">https://arxiv.org/pdf/2407.21315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21315]] Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances(https://arxiv.org/abs/2407.21315)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to emotion detection in speech using Large Language Models (LLMs). We address the limitation of LLMs in processing audio inputs by translating speech characteristics into natural language descriptions. Our method integrates these descriptions into text prompts, enabling LLMs to perform multimodal emotion analysis without architectural modifications. We evaluate our approach on two datasets: IEMOCAP and MELD, demonstrating significant improvements in emotion recognition accuracy, particularly for high-quality audio data. Our experiments show that incorporating speech descriptions yields a 2 percentage point increase in weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare various LLM architectures and explore the effectiveness of different feature representations. Our findings highlight the potential of this approach in enhancing emotion detection capabilities of LLMs and underscore the importance of audio quality in speech-based emotion recognition tasks. We'll release the source code on Github.</li>
</ul>

<h3>Title: Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiang Hao, Xiao Jin, Hu Xiaoguang, Chen Tianyou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21316">https://arxiv.org/abs/2407.21316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21316">https://arxiv.org/pdf/2407.21316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21316]] Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion Models(https://arxiv.org/abs/2407.21316)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DM) represent one of the most advanced generative models today, yet recent studies suggest that DMs are vulnerable to backdoor attacks. Backdoor attacks establish hidden associations between particular input patterns and model behaviors, compromising model integrity by triggering undesirable actions with manipulated input data. This vulnerability poses substantial risks, including reputational damage to model owners and the dissemination of harmful content. To mitigate the threat of backdoor attacks, there have been some investigations on backdoor detection and model repair. However, previous work fails to purify the backdoored DMs created by state-of-the-art attacks, rendering the field much underexplored. To bridge this gap, we introduce \textbf{Diff-Cleanse}, a novel two-stage backdoor defense framework specifically designed for DMs. The first stage employs a innovative trigger inversion technique to detect the backdoor and reconstruct the trigger, and the second stage utilizes a structural pruning method to eliminate the backdoor. We evaluate our framework on hundreds of DMs attacked by 3 existing backdoor attack methods. Extensive experiments demonstrate that Diff-Cleanse achieves nearly 100\% detection accuracy and effectively mitigates backdoor impacts, preserving the model's benign performance with minimal compromise. Our code is avaliable at this https URL.</li>
</ul>

<h3>Title: Performance of Recent Large Language Models for a Low-Resourced Language</h3>
<ul>
<li><strong>Authors: </strong>Ravindu Jayakody, Gihan Dias</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21330">https://arxiv.org/abs/2407.21330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21330">https://arxiv.org/pdf/2407.21330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21330]] Performance of Recent Large Language Models for a Low-Resourced Language(https://arxiv.org/abs/2407.21330)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown significant advances in the past year. In addition to new versions of GPT and Llama, several other LLMs have been introduced recently. Some of these are open models available for download and modification. Although multilingual large language models have been available for some time, their performance on low-resourced languages such as Sinhala has been poor. We evaluated four recent LLMs on their performance directly in the Sinhala language, and by translation to and from English. We also evaluated their fine-tunability with a small amount of fine-tuning data. Claude and GPT 4o perform well out-of-the-box and do significantly better than previous versions. Llama and Mistral perform poorly but show some promise of improvement with fine tuning.</li>
</ul>

<h3>Title: Chat2Layout: Interactive 3D Furniture Layout with a Multimodal LLM</h3>
<ul>
<li><strong>Authors: </strong>Can Wang, Hongliang Zhong, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21333">https://arxiv.org/abs/2407.21333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21333">https://arxiv.org/pdf/2407.21333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21333]] Chat2Layout: Interactive 3D Furniture Layout with a Multimodal LLM(https://arxiv.org/abs/2407.21333)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic furniture layout is long desired for convenient interior design. Leveraging the remarkable visual reasoning capabilities of multimodal large language models (MLLMs), recent methods address layout generation in a static manner, lacking the feedback-driven refinement essential for interactive user engagement. We introduce Chat2Layout, a novel interactive furniture layout generation system that extends the functionality of MLLMs into the realm of interactive layout design. To achieve this, we establish a unified vision-question paradigm for in-context learning, enabling seamless communication with MLLMs to steer their behavior without altering model weights. Within this framework, we present a novel training-free visual prompting mechanism. This involves a visual-text prompting technique that assist MLLMs in reasoning about plausible layout plans, followed by an Offline-to-Online search (O2O-Search) method, which automatically identifies the minimal set of informative references to provide exemplars for visual-text prompting. By employing an agent system with MLLMs as the core controller, we enable bidirectional interaction. The agent not only comprehends the 3D environment and user requirements through linguistic and visual perception but also plans tasks and reasons about actions to generate and arrange furniture within the virtual space. Furthermore, the agent iteratively updates based on visual feedback from execution results. Experimental results demonstrate that our approach facilitates language-interactive generation and arrangement for diverse and complex 3D furniture.</li>
</ul>

<h3>Title: On-the-fly Point Feature Representation for Point Clouds Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiangyi Wang, Zhongyao Cheng, Na Zhao, Jun Cheng, Xulei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21335">https://arxiv.org/abs/2407.21335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21335">https://arxiv.org/pdf/2407.21335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21335]] On-the-fly Point Feature Representation for Point Clouds Analysis(https://arxiv.org/abs/2407.21335)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Point cloud analysis is challenging due to its unique characteristics of unorderness, sparsity and irregularity. Prior works attempt to capture local relationships by convolution operations or attention mechanisms, exploiting geometric information from coordinates implicitly. These methods, however, are insufficient to describe the explicit local geometry, e.g., curvature and orientation. In this paper, we propose On-the-fly Point Feature Representation (OPFR), which captures abundant geometric information explicitly through Curve Feature Generator module. This is inspired by Point Feature Histogram (PFH) from computer vision community. However, the utilization of vanilla PFH encounters great difficulties when applied to large datasets and dense point clouds, as it demands considerable time for feature generation. In contrast, we introduce the Local Reference Constructor module, which approximates the local coordinate systems based on triangle sets. Owing to this, our OPFR only requires extra 1.56ms for inference (65x faster than vanilla PFH) and 0.012M more parameters, and it can serve as a versatile plug-and-play module for various backbones, particularly MLP-based and Transformer-based backbones examined in this study. Additionally, we introduce the novel Hierarchical Sampling module aimed at enhancing the quality of triangle sets, thereby ensuring robustness of the obtained geometric features. Our proposed method improves overall accuracy (OA) on ModelNet40 from 90.7% to 94.5% (+3.8%) for classification, and OA on S3DIS Area-5 from 86.4% to 90.0% (+3.6%) for semantic segmentation, respectively, building upon PointNet++ backbone. When integrated with Point Transformer backbone, we achieve state-of-the-art results on both tasks: 94.8% OA on ModelNet40 and 91.7% OA on S3DIS Area-5.</li>
</ul>

<h3>Title: Differentially Private Block-wise Gradient Shuffle for Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>David Zagardo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21347">https://arxiv.org/abs/2407.21347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21347">https://arxiv.org/pdf/2407.21347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21347]] Differentially Private Block-wise Gradient Shuffle for Deep Learning(https://arxiv.org/abs/2407.21347)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction</a></li>
<li><strong>Abstract: </strong>Traditional Differentially Private Stochastic Gradient Descent (DP-SGD) introduces statistical noise on top of gradients drawn from a Gaussian distribution to ensure privacy. This paper introduces the novel Differentially Private Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning. BloGS builds off of existing private deep learning literature, but makes a definitive shift by taking a probabilistic approach to gradient noise introduction through shuffling modeled after information theoretic privacy analyses. The theoretical results presented in this paper show that the combination of shuffling, parameter-specific block size selection, batch layer clipping, and gradient accumulation allows DP-BloGS to achieve training times close to that of non-private training while maintaining similar privacy and utility guarantees to DP-SGD. DP-BloGS is found to be significantly more resistant to data extraction attempts than DP-SGD. The theoretical results are validated by the experimental findings.</li>
</ul>

<h3>Title: Small Object Few-shot Segmentation for Vision-based Industrial Inspection</h3>
<ul>
<li><strong>Authors: </strong>Zilong Zhang, Chang Niu, Zhibin Zhao, Xingwu Zhang, Xuefeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21351">https://arxiv.org/abs/2407.21351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21351">https://arxiv.org/pdf/2407.21351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21351]] Small Object Few-shot Segmentation for Vision-based Industrial Inspection(https://arxiv.org/abs/2407.21351)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vision-based industrial inspection (VII) aims to locate defects quickly and accurately. Supervised learning under a close-set setting and industrial anomaly detection, as two common paradigms in VII, face different problems in practical applications. The former is that various and sufficient defects are difficult to obtain, while the latter is that specific defects cannot be located. To solve these problems, in this paper, we focus on the few-shot semantic segmentation (FSS) method, which can locate unseen defects conditioned on a few annotations without retraining. Compared to common objects in natural images, the defects in VII are small. This brings two problems to current FSS methods: 1 distortion of target semantics and 2 many false positives for backgrounds. To alleviate these problems, we propose a small object few-shot segmentation (SOFS) model. The key idea for alleviating 1 is to avoid the resizing of the original image and correctly indicate the intensity of target semantics. SOFS achieves this idea via the non-resizing procedure and the prototype intensity downsampling of support annotations. To alleviate 2, we design an abnormal prior map in SOFS to guide the model to reduce false positives and propose a mixed normal Dice loss to preferentially prevent the model from predicting false positives. SOFS can achieve FSS and few-shot anomaly detection determined by support masks. Diverse experiments substantiate the superior performance of SOFS. Code is available at this https URL.</li>
</ul>

<h3>Title: GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yanxu Mao, Peipei Liu, Tiehan Cui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21384">https://arxiv.org/abs/2407.21384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21384">https://arxiv.org/pdf/2407.21384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21384]] GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction(https://arxiv.org/abs/2407.21384)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context. Currently, some studies are utilizing logical rules within evidence sentences to enhance the performance of DocRE. However, in the data without provided evidence sentences, researchers often obtain a list of evidence sentences for the entire document through evidence retrieval (ER). Therefore, DocRE suffers from two challenges: firstly, the relevance between evidence and entity pairs is weak; secondly, there is insufficient extraction of complex cross-relations between long-distance multi-entities. To overcome these challenges, we propose GEGA, a novel model for DocRE. The model leverages graph neural networks to construct multiple weight matrices, guiding attention allocation to evidence sentences. It also employs multi-scale representation aggregation to enhance ER. Subsequently, we integrate the most efficient evidence information to implement both fully supervised and weakly supervised training processes for the model. We evaluate the GEGA model on three widely used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The experimental results indicate that our model has achieved comprehensive improvements compared to the existing SOTA model.</li>
</ul>

<h3>Title: Fingerprint Theft Using Smart Padlocks: Droplock Exploits and Defenses</h3>
<ul>
<li><strong>Authors: </strong>Steve Kerrison</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21398">https://arxiv.org/abs/2407.21398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21398">https://arxiv.org/pdf/2407.21398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21398]] Fingerprint Theft Using Smart Padlocks: Droplock Exploits and Defenses(https://arxiv.org/abs/2407.21398)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>There is growing adoption of smart devices such as digital locks with remote control and sophisticated authentication mechanisms. However, a lack of attention to device security and user-awareness beyond the primary function of these IoT devices may be exposing users to invisible risks. This paper extends upon prior work that defined the "droplock", an attack whereby a smart lock is turned into a wireless fingerprint harvester. We perform a more in-depth analysis of a broader range of vulnerabilities and exploits that make a droplock attack easier to perform and harder to detect. Analysis is extended to a range of other smart lock models, and a threat model is used as the basis to recommend stronger security controls that may mitigate the risks of such as attack.</li>
</ul>

<h3>Title: Benchmarking AIGC Video Quality Assessment: A Dataset and Unified Model</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Zhang, Xinyue Li, Wei Sun, Jun Jia, Xiongkuo Min, Zicheng Zhang, Chunyi Li, Zijian Chen, Puyi Wang, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21408">https://arxiv.org/abs/2407.21408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21408">https://arxiv.org/pdf/2407.21408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21408]] Benchmarking AIGC Video Quality Assessment: A Dataset and Unified Model(https://arxiv.org/abs/2407.21408)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, artificial intelligence (AI) driven video generation has garnered significant attention due to advancements in stable diffusion and large language model techniques. Thus, there is a great demand for accurate video quality assessment (VQA) models to measure the perceptual quality of AI-generated content (AIGC) videos as well as optimize video generation techniques. However, assessing the quality of AIGC videos is quite challenging due to the highly complex distortions they exhibit (e.g., unnatural action, irrational objects, etc.). Therefore, in this paper, we try to systemically investigate the AIGC-VQA problem from both subjective and objective quality assessment perspectives. For the subjective perspective, we construct a Large-scale Generated Vdeo Quality assessment (LGVQ) dataset, consisting of 2,808 AIGC videos generated by 6 video generation models using 468 carefully selected text prompts. Unlike previous subjective VQA experiments, we evaluate the perceptual quality of AIGC videos from three dimensions: spatial quality, temporal quality, and text-to-video alignment, which hold utmost importance for current video generation techniques. For the objective perspective, we establish a benchmark for evaluating existing quality assessment metrics on the LGVQ dataset, which reveals that current metrics perform poorly on the LGVQ dataset. Thus, we propose a Unify Generated Video Quality assessment (UGVQ) model to comprehensively and accurately evaluate the quality of AIGC videos across three aspects using a unified model, which uses visual, textual and motion features of video and corresponding prompt, and integrates key features to enhance feature expression. We hope that our benchmark can promote the development of quality evaluation metrics for AIGC videos. The LGVQ dataset and the UGVQ metric will be publicly released.</li>
</ul>

<h3>Title: VIPeR: Visual Incremental Place Recognition with Adaptive Mining and Lifelong Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Ming, Minyang Xu, Xingrui Yang, Weicai Ye, Weihan Wang, Yong Peng, Weichen Dai, Wanzeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21416">https://arxiv.org/abs/2407.21416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21416">https://arxiv.org/pdf/2407.21416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21416]] VIPeR: Visual Incremental Place Recognition with Adaptive Mining and Lifelong Learning(https://arxiv.org/abs/2407.21416)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual place recognition (VPR) is an essential component of many autonomous and augmented/virtual reality systems. It enables the systems to robustly localize themselves in large-scale environments. Existing VPR methods demonstrate attractive performance at the cost of heavy pre-training and limited generalizability. When deployed in unseen environments, these methods exhibit significant performance drops. Targeting this issue, we present VIPeR, a novel approach for visual incremental place recognition with the ability to adapt to new environments while retaining the performance of previous environments. We first introduce an adaptive mining strategy that balances the performance within a single environment and the generalizability across multiple environments. Then, to prevent catastrophic forgetting in lifelong learning, we draw inspiration from human memory systems and design a novel memory bank for our VIPeR. Our memory bank contains a sensory memory, a working memory and a long-term memory, with the first two focusing on the current environment and the last one for all previously visited environments. Additionally, we propose a probabilistic knowledge distillation to explicitly safeguard the previously learned knowledge. We evaluate our proposed VIPeR on three large-scale datasets, namely Oxford Robotcar, Nordland, and TartanAir. For comparison, we first set a baseline performance with naive finetuning. Then, several more recent lifelong learning methods are compared. Our VIPeR achieves better performance in almost all aspects with the biggest improvement of 13.65% in average performance.</li>
</ul>

<h3>Title: Generalized Tampered Scene Text Detection in the era of Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Chenfan Qu, Yiwu Zhong, Fengjun Guo, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21422">https://arxiv.org/abs/2407.21422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21422">https://arxiv.org/pdf/2407.21422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21422]] Generalized Tampered Scene Text Detection in the era of Generative AI(https://arxiv.org/abs/2407.21422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancements of generative AI have fueled the potential of generative text image editing while simultaneously escalating the threat of misinformation spreading. However, existing forensics methods struggle to detect unseen forgery types that they have not been trained on, leaving the development of a model capable of generalized detection of tampered scene text as an unresolved issue. To tackle this, we propose a novel task: open-set tampered scene text detection, which evaluates forensics models on their ability to identify both seen and previously unseen forgery types. We have curated a comprehensive, high-quality dataset, featuring the texts tampered by eight text editing models, to thoroughly assess the open-set generalization capabilities. Further, we introduce a novel and effective pre-training paradigm that subtly alters the texture of selected texts within an image and trains the model to identify these regions. This approach not only mitigates the scarcity of high-quality training data but also enhances models' fine-grained perception and open-set generalization abilities. Additionally, we present DAF, a novel framework that improves open-set generalization by distinguishing between the features of authentic and tampered text, rather than focusing solely on the tampered text's features. Our extensive experiments validate the remarkable efficacy of our methods. For example, our zero-shot performance can even beat the previous state-of-the-art full-shot model by a large margin. Our dataset and code will be open-source.</li>
</ul>

<h3>Title: Cost-Effective Hallucination Detection for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Simon Valentin, Jinmiao Fu, Gianluca Detommaso, Shaoyuan Xu, Giovanni Zappella, Bryan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21424">https://arxiv.org/abs/2407.21424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21424">https://arxiv.org/pdf/2407.21424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21424]] Cost-Effective Hallucination Detection for LLMs(https://arxiv.org/abs/2407.21424)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can be prone to hallucinations - generating unreliable outputs that are unfaithful to their inputs, external facts or internally inconsistent. In this work, we address several challenges for post-hoc hallucination detection in production settings. Our pipeline for hallucination detection entails: first, producing a confidence score representing the likelihood that a generated answer is a hallucination; second, calibrating the score conditional on attributes of the inputs and candidate response; finally, performing detection by thresholding the calibrated score. We benchmark a variety of state-of-the-art scoring methods on different datasets, encompassing question answering, fact checking, and summarization tasks. We employ diverse LLMs to ensure a comprehensive assessment of performance. We show that calibrating individual scoring methods is critical for ensuring risk-aware downstream decision making. Based on findings that no individual score performs best in all situations, we propose a multi-scoring framework, which combines different scores and achieves top performance across all datasets. We further introduce cost-effective multi-scoring, which can match or even outperform more expensive detection methods, while significantly reducing computational overhead.</li>
</ul>

<h3>Title: A Plug-and-Play Method for Rare Human-Object Interactions Detection by Bridging Domain Gap</h3>
<ul>
<li><strong>Authors: </strong>Lijun Zhang, Wei Suo, Peng Wang, Yanning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21438">https://arxiv.org/abs/2407.21438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21438">https://arxiv.org/pdf/2407.21438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21438]] A Plug-and-Play Method for Rare Human-Object Interactions Detection by Bridging Domain Gap(https://arxiv.org/abs/2407.21438)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human-object interactions (HOI) detection aims at capturing human-object pairs in images and corresponding actions. It is an important step toward high-level visual reasoning and scene understanding. However, due to the natural bias from the real world, existing methods mostly struggle with rare human-object pairs and lead to sub-optimal results. Recently, with the development of the generative model, a straightforward approach is to construct a more balanced dataset based on a group of supplementary samples. Unfortunately, there is a significant domain gap between the generated data and the original data, and simply merging the generated images into the original dataset cannot significantly boost the performance. To alleviate the above problem, we present a novel model-agnostic framework called \textbf{C}ontext-\textbf{E}nhanced \textbf{F}eature \textbf{A}lignment (CEFA) module, which can effectively align the generated data with the original data at the feature level and bridge the domain gap. Specifically, CEFA consists of a feature alignment module and a context enhancement module. On one hand, considering the crucial role of human-object pairs information in HOI tasks, the feature alignment module aligns the human-object pairs by aggregating instance information. On the other hand, to mitigate the issue of losing important context information caused by the traditional discriminator-style alignment method, we employ a context-enhanced image reconstruction module to improve the model's learning ability of contextual cues. Extensive experiments have shown that our method can serve as a plug-and-play module to improve the detection performance of HOI models on rare categories\footnote{this https URL}.</li>
</ul>

<h3>Title: QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications</h3>
<ul>
<li><strong>Authors: </strong>Rivik Setty, Vinay Setty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21441">https://arxiv.org/abs/2407.21441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21441">https://arxiv.org/pdf/2407.21441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21441]] QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications(https://arxiv.org/abs/2407.21441)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Verifying fact-checking claims poses a significant challenge, even for humans. Recent approaches have demonstrated that decomposing claims into relevant questions to gather evidence enhances the efficiency of the fact-checking process. In this paper, we provide empirical evidence showing that this question decomposition can be effectively automated. We demonstrate that smaller generative models, fine-tuned for the question generation task using data augmentation from various datasets, outperform large language models by up to 8%. Surprisingly, in some cases, the evidence retrieved using machine-generated questions proves to be significantly more effective for fact-checking than that obtained from human-written questions. We also perform manual evaluation of the decomposed questions to assess the quality of the questions generated.</li>
</ul>

<h3>Title: Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency</h3>
<ul>
<li><strong>Authors: </strong>Taiji Li, Zhi Li, Yin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21443">https://arxiv.org/abs/2407.21443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21443">https://arxiv.org/pdf/2407.21443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21443]] Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency(https://arxiv.org/abs/2407.21443)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Despite large language models (LLMs) have demonstrated impressive performance in various tasks, they are still suffering from the factual inconsistency problem called hallucinations. For instance, LLMs occasionally generate content that diverges from source article, and prefer to extract information that appears at the beginning and end of the context, especially in long document summarization. Inspired by these findings, we propose to improve the faithfulness of LLMs in summarization by impelling them to process the entire article more fairly and faithfully. We present a novel summary generation strategy, namely SliSum, which exploits the ideas of sliding windows and self-consistency. Specifically, SliSum divides the source article into overlapping windows, and utilizes LLM to generate local summaries for the content in the windows. Finally, SliSum aggregates all local summaries using clustering and majority voting algorithm to produce more faithful summary of entire article. Extensive experiments demonstrate that SliSum significantly improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and GPT-3.5 in both short and long text summarization, while maintaining their fluency and informativeness and without additional fine-tuning and resources. We further conduct qualitative and quantitative studies to investigate why SliSum works and impacts of hyperparameters in SliSum on performance.</li>
</ul>

<h3>Title: TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors</h3>
<ul>
<li><strong>Authors: </strong>Zhaolan Huang, Adrien Tousnakhoff, Polina Kozyr, Roman Rehausen, Felix Bießmann, Robert Lachlan, Cedric Adjih, Emmanuel Baccelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD, eess.AS, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21453">https://arxiv.org/abs/2407.21453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21453">https://arxiv.org/pdf/2407.21453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21453]] TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors(https://arxiv.org/abs/2407.21453)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monitoring biodiversity at scale is challenging. Detecting and identifying species in fine grained taxonomies requires highly accurate machine learning (ML) methods. Training such models requires large high quality data sets. And deploying these models to low power devices requires novel compression techniques and model architectures. While species classification methods have profited from novel data sets and advances in ML methods, in particular neural networks, deploying these state of the art models to low power devices remains difficult. Here we present a comprehensive empirical comparison of various tinyML neural network architectures and compression techniques for species classification. We focus on the example of bird song detection, more concretely a data set curated for studying the corn bunting bird species. The data set is released along with all code and experiments of this study. In our experiments we compare predictive performance, memory and time complexity of classical spectrogram based methods and recent approaches operating on raw audio signal. Our results indicate that individual bird species can be robustly detected with relatively simple architectures that can be readily deployed to low power devices.</li>
</ul>

<h3>Title: StreetSurfaceVis: a dataset of crowdsourced street-level imagery with semi-automated annotations of road surface type and quality</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Kapp, Edith Hoffmann, Esther Weigmann, Helena Mihaljević</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21454">https://arxiv.org/abs/2407.21454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21454">https://arxiv.org/pdf/2407.21454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21454]] StreetSurfaceVis: a dataset of crowdsourced street-level imagery with semi-automated annotations of road surface type and quality(https://arxiv.org/abs/2407.21454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Road unevenness significantly impacts the safety and comfort of various traffic participants, especially vulnerable road users such as cyclists and wheelchair users. This paper introduces StreetSurfaceVis, a novel dataset comprising 9,122 street-level images collected from a crowdsourcing platform and manually annotated by road surface type and quality. The dataset is intended to train models for comprehensive surface assessments of road networks. Existing open datasets are constrained by limited geospatial coverage and camera setups, typically excluding cycleways and footways. By crafting a heterogeneous dataset, we aim to fill this gap and enable robust models that maintain high accuracy across diverse image sources. However, the frequency distribution of road surface types and qualities is highly imbalanced. We address the challenge of ensuring sufficient images per class while reducing manual annotation by proposing a sampling strategy that incorporates various external label prediction resources. More precisely, we estimate the impact of (1) enriching the image data with OpenStreetMap tags, (2) iterative training and application of a custom surface type classification model, (3) amplifying underrepresented classes through prompt-based classification with GPT-4o or similarity search using image embeddings. We show that utilizing a combination of these strategies effectively reduces manual annotation workload while ensuring sufficient class representation.</li>
</ul>

<h3>Title: MarvelOVD: Marrying Object Recognition and Vision-Language Models for Robust Open-Vocabulary Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Kuo Wang, Lechao Cheng, Weikai Chen, Pingping Zhang, Liang Lin, Fan Zhou, Guanbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21465">https://arxiv.org/abs/2407.21465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21465">https://arxiv.org/pdf/2407.21465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21465]] MarvelOVD: Marrying Object Recognition and Vision-Language Models for Robust Open-Vocabulary Object Detection(https://arxiv.org/abs/2407.21465)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning from pseudo-labels that generated with VLMs~(Vision Language Models) has been shown as a promising solution to assist open vocabulary detection (OVD) in recent studies. However, due to the domain gap between VLM and vision-detection tasks, pseudo-labels produced by the VLMs are prone to be noisy, while the training design of the detector further amplifies the bias. In this work, we investigate the root cause of VLMs' biased prediction under the OVD context. Our observations lead to a simple yet effective paradigm, coded MarvelOVD, that generates significantly better training targets and optimizes the learning procedure in an online manner by marrying the capability of the detector with the vision-language model. Our key insight is that the detector itself can act as a strong auxiliary guidance to accommodate VLM's inability of understanding both the ``background'' and the context of a proposal within the image. Based on it, we greatly purify the noisy pseudo-labels via Online Mining and propose Adaptive Reweighting to effectively suppress the biased training boxes that are not well aligned with the target object. In addition, we also identify a neglected ``base-novel-conflict'' problem and introduce stratified label assignments to prevent it. Extensive experiments on COCO and LVIS datasets demonstrate that our method outperforms the other state-of-the-arts by significant margins. Codes are available at this https URL</li>
</ul>

<h3>Title: Fine-gained Zero-shot Video Sampling</h3>
<ul>
<li><strong>Authors: </strong>Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21475">https://arxiv.org/abs/2407.21475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21475">https://arxiv.org/pdf/2407.21475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21475]] Fine-gained Zero-shot Video Sampling(https://arxiv.org/abs/2407.21475)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Incorporating a temporal dimension into pretrained image diffusion models for video generation is a prevalent approach. However, this method is computationally demanding and necessitates large-scale video datasets. More critically, the heterogeneity between image and video datasets often results in catastrophic forgetting of the image expertise. Recent attempts to directly extract video snippets from image diffusion models have somewhat mitigated these problems. Nevertheless, these methods can only generate brief video clips with simple movements and fail to capture fine-grained motion or non-grid deformation. In this paper, we propose a novel Zero-Shot video Sampling algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling high-quality video clips from existing image synthesis methods, such as Stable Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$ utilizes the dependency noise model and temporal momentum attention to ensure content consistency and animation coherence, respectively. This ability enables it to excel in related tasks, such as conditional and context-specialized video generation and instruction-guided video editing. Experimental results demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in zero-shot video generation, occasionally outperforming recent supervised methods. Homepage: \url{this https URL}.</li>
</ul>

<h3>Title: Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends</h3>
<ul>
<li><strong>Authors: </strong>Giuliano Martinelli, Edoardo Barba, Roberto Navigli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21489">https://arxiv.org/abs/2407.21489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21489">https://arxiv.org/pdf/2407.21489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21489]] Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends(https://arxiv.org/abs/2407.21489)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Large autoregressive generative models have emerged as the cornerstone for achieving the highest performance across several Natural Language Processing tasks. However, the urge to attain superior results has, at times, led to the premature replacement of carefully designed task-specific approaches without exhaustive experimentation. The Coreference Resolution task is no exception; all recent state-of-the-art solutions adopt large generative autoregressive models that outperform encoder-based discriminative systems. In this work,we challenge this recent trend by introducing Maverick, a carefully designed - yet simple - pipeline, which enables running a state-of-the-art Coreference Resolution system within the constraints of an academic budget, outperforming models with up to 13 billion parameters with as few as 500 million parameters. Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark, training with up to 0.006x the memory resources and obtaining a 170x faster inference compared to previous state-of-the-art systems. We extensively validate the robustness of the Maverick framework with an array of diverse experiments, reporting improvements over prior systems in data-scarce, long-document, and out-of-domain settings. We release our code and models for research purposes at this https URL.</li>
</ul>

<h3>Title: Generative Expressive Conversational Speech Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Yifan Hu, Ren Yi, Yin Xiang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21491">https://arxiv.org/abs/2407.21491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21491">https://arxiv.org/pdf/2407.21491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21491]] Generative Expressive Conversational Speech Synthesis(https://arxiv.org/abs/2407.21491)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conversational Speech Synthesis (CSS) aims to express a target utterance with the proper speaking style in a user-agent conversation setting. Existing CSS methods employ effective multi-modal context modeling techniques to achieve empathy understanding and expression. However, they often need to design complex network architectures and meticulously optimize the modules within them. In addition, due to the limitations of small-scale datasets containing scripted recording styles, they often fail to simulate real natural conversational styles. To address the above issues, we propose a novel generative expressive CSS system, termed GPT-Talker.We transform the multimodal information of the multi-turn dialogue history into discrete token sequences and seamlessly integrate them to form a comprehensive user-agent dialogue context. Leveraging the power of GPT, we predict the token sequence, that includes both semantic and style knowledge, of response for the agent. After that, the expressive conversational speech is synthesized by the conversation-enriched VITS to deliver feedback to the user.Furthermore, we propose a large-scale Natural CSS Dataset called NCSSD, that includes both naturally recorded conversational speech in improvised styles and dialogues extracted from TV shows. It encompasses both Chinese and English languages, with a total duration of 236 hours.We conducted comprehensive experiments on the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both subjective and objective evaluations demonstrate that our model outperforms other state-of-the-art CSS systems significantly in terms of naturalness and expressiveness. The Code, Dataset, and Pre-trained Model are available at: this https URL.</li>
</ul>

<h3>Title: Mitral Regurgitation Recogniton based on Unsupervised Out-of-Distribution Detection with Residual Diffusion Amplification</h3>
<ul>
<li><strong>Authors: </strong>Zhe Liu, Xiliang Zhu, Tong Han, Yuhao Huang, Jian Wang, Lian Liu, Fang Wang, Dong Ni, Zhongshan Gou, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21497">https://arxiv.org/abs/2407.21497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21497">https://arxiv.org/pdf/2407.21497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21497]] Mitral Regurgitation Recogniton based on Unsupervised Out-of-Distribution Detection with Residual Diffusion Amplification(https://arxiv.org/abs/2407.21497)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Mitral regurgitation (MR) is a serious heart valve disease. Early and accurate diagnosis of MR via ultrasound video is critical for timely clinical decision-making and surgical intervention. However, manual MR diagnosis heavily relies on the operator's experience, which may cause misdiagnosis and inter-observer variability. Since MR data is limited and has large intra-class variability, we propose an unsupervised out-of-distribution (OOD) detection method to identify MR rather than building a deep classifier. To our knowledge, we are the first to explore OOD in MR ultrasound videos. Our method consists of a feature extractor, a feature reconstruction model, and a residual accumulation amplification algorithm. The feature extractor obtains features from the video clips and feeds them into the feature reconstruction model to restore the original features. The residual accumulation amplification algorithm then iteratively performs noise feature reconstruction, amplifying the reconstructed error of OOD features. This algorithm is straightforward yet efficient and can seamlessly integrate as a plug-and-play component in reconstruction-based OOD detection methods. We validated the proposed method on a large ultrasound dataset containing 893 non-MR and 267 MR videos. Experimental results show that our OOD detection method can effectively identify MR samples.</li>
</ul>

<h3>Title: MaskUno: Switch-Split Block For Enhancing Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jawad Haidar, Marc Mouawad, Imad Elhajj, Daniel Asmar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21498">https://arxiv.org/abs/2407.21498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21498">https://arxiv.org/pdf/2407.21498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21498]] MaskUno: Switch-Split Block For Enhancing Instance Segmentation(https://arxiv.org/abs/2407.21498)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation is an advanced form of image segmentation which, beyond traditional segmentation, requires identifying individual instances of repeating objects in a scene. Mask R-CNN is the most common architecture for instance segmentation, and improvements to this architecture include steps such as benefiting from bounding box refinements, adding semantics, or backbone enhancements. In all the proposed variations to date, the problem of competing kernels (each class aims to maximize its own accuracy) persists when models try to synchronously learn numerous classes. In this paper, we propose mitigating this problem by replacing mask prediction with a Switch-Split block that processes refined ROIs, classifies them, and assigns them to specialized mask predictors. We name the method MaskUno and test it on various models from the literature, which are then trained on multiple classes using the benchmark COCO dataset. An increase in the mean Average Precision (mAP) of 2.03% was observed for the high-performing DetectoRS when trained on 80 classes. MaskUno proved to enhance the mAP of instance segmentation models regardless of the number and typ</li>
</ul>

<h3>Title: Root Cause Analysis Of Productivity Losses In Manufacturing Systems Utilizing Ensemble Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Jonas Gram, Brandon K. Sai, Thomas Bauernhansl</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21503">https://arxiv.org/abs/2407.21503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21503">https://arxiv.org/pdf/2407.21503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21503]] Root Cause Analysis Of Productivity Losses In Manufacturing Systems Utilizing Ensemble Machine Learning(https://arxiv.org/abs/2407.21503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In today's rapidly evolving landscape of automation and manufacturing systems, the efficient resolution of productivity losses is paramount. This study introduces a data-driven ensemble approach, utilizing the cyclic multivariate time series data from binary sensors and signals from Programmable Logic Controllers (PLCs) within these systems. The objective is to automatically analyze productivity losses per cycle and pinpoint their root causes by assigning the loss to a system element. The ensemble approach introduced in this publication integrates various methods, including information theory and machine learning behavior models, to provide a robust analysis for each production cycle. To expedite the resolution of productivity losses and ensure short response times, stream processing becomes a necessity. Addressing this, the approach is implemented as data-stream analysis and can be transferred to batch processing, seamlessly integrating into existing systems without the need for extensive historical data analysis. This method has two positive effects. Firstly, the result of the analysis ensures that the period of lower productivity is reduced by identifying the likely root cause of the productivity loss. Secondly, these results are more reliable due to the ensemble approach and therefore avoid dependency on technical experts. The approach is validated using a semi-automated welding manufacturing system, an injection molding automation system, and a synthetically generated test PLC dataset. The results demonstrate the method's efficacy in offering a data-driven understanding of process behavior and mark an advancement in autonomous manufacturing system analysis.</li>
</ul>

<h3>Title: A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging</h3>
<ul>
<li><strong>Authors: </strong>Miao Cao, Lishun Wang, Huan Wang, Xin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21517">https://arxiv.org/abs/2407.21517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21517">https://arxiv.org/pdf/2407.21517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21517]] A Simple Low-bit Quantization Framework for Video Snapshot Compressive Imaging(https://arxiv.org/abs/2407.21517)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Video Snapshot Compressive Imaging (SCI) aims to use a low-speed 2D camera to capture high-speed scene as snapshot compressed measurements, followed by a reconstruction algorithm to reconstruct the high-speed video frames. State-of-the-art (SOTA) deep learning-based algorithms have achieved impressive performance, yet with heavy computational workload. Network quantization is a promising way to reduce computational cost. However, a direct low-bit quantization will bring large performance drop. To address this challenge, in this paper, we propose a simple low-bit quantization framework (dubbed Q-SCI) for the end-to-end deep learning-based video SCI reconstruction methods which usually consist of a feature extraction, feature enhancement, and video reconstruction module. Specifically, we first design a high-quality feature extraction module and a precise video reconstruction module to extract and propagate high-quality features in the low-bit quantized model. In addition, to alleviate the information distortion of the Transformer branch in the quantized feature enhancement module, we introduce a shift operation on the query and key distributions to further bridge the performance gap. Comprehensive experimental results manifest that our Q-SCI framework can achieve superior performance, e.g., 4-bit quantized EfficientSCI-S derived by our Q-SCI framework can theoretically accelerate the real-valued EfficientSCI-S by 7.8X with only 2.3% performance gap on the simulation testing datasets. Code is available at this https URL.</li>
</ul>

<h3>Title: Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, Gang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21523">https://arxiv.org/abs/2407.21523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21523">https://arxiv.org/pdf/2407.21523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21523]] Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI(https://arxiv.org/abs/2407.21523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant high-quality tabular data for model training remains a significant obstacle. Numerous works have focused on tabular data augmentation (TDA) to enhance the original table with additional data, thereby improving downstream ML tasks. Recently, there has been a growing interest in leveraging the capabilities of generative AI for TDA. Therefore, we believe it is time to provide a comprehensive review of the progress and future prospects of TDA, with a particular emphasis on the trending generative AI. Specifically, we present an architectural view of the TDA pipeline, comprising three main procedures: pre-augmentation, augmentation, and post-augmentation. Pre-augmentation encompasses preparation tasks that facilitate subsequent TDA, including error handling, table annotation, table simplification, table representation, table indexing, table navigation, schema matching, and entity matching. Augmentation systematically analyzes current TDA methods, categorized into retrieval-based methods, which retrieve external data, and generation-based methods, which generate synthetic data. We further subdivide these methods based on the granularity of the augmentation process at the row, column, cell, and table levels. Post-augmentation focuses on the datasets, evaluation and optimization aspects of TDA. We also summarize current trends and future directions for TDA, highlighting promising opportunities in the era of generative AI. In addition, the accompanying papers and related resources are continuously updated and maintained in the GitHub repository at this https URL to reflect ongoing advancements in the field.</li>
</ul>

<h3>Title: ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Xiaoshuai Sun, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21534">https://arxiv.org/abs/2407.21534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21534">https://arxiv.org/pdf/2407.21534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21534]] ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models(https://arxiv.org/abs/2407.21534)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we propose a training-free method to inject visual referring into Multimodal Large Language Models (MLLMs) through learnable visual token optimization. We observe the relationship between text prompt tokens and visual tokens in MLLMs, where attention layers model the connection between them. Our approach involves adjusting visual tokens from the MLP output during inference, controlling which text prompt tokens attend to which visual tokens. We optimize a learnable visual token based on an energy function, enhancing the strength of referential regions in the attention map. This enables detailed region description and reasoning without the need for substantial training costs or model retraining. Our method offers a promising direction for integrating referential abilities into MLLMs. Our method support referring with box, mask, scribble and point. The results demonstrate that our method exhibits controllability and interpretability.</li>
</ul>

<h3>Title: Probabilistic Scoring Lists for Interpretable Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Jonas Hanselle, Stefan Heid, Johannes Fürnkranz, Eyke Hüllermeier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21535">https://arxiv.org/abs/2407.21535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21535">https://arxiv.org/pdf/2407.21535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21535]] Probabilistic Scoring Lists for Interpretable Machine Learning(https://arxiv.org/abs/2407.21535)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>A scoring system is a simple decision model that checks a set of features, adds a certain number of points to a total score for each feature that is satisfied, and finally makes a decision by comparing the total score to a threshold. Scoring systems have a long history of active use in safety-critical domains such as healthcare and justice, where they provide guidance for making objective and accurate decisions. Given their genuine interpretability, the idea of learning scoring systems from data is obviously appealing from the perspective of explainable AI. In this paper, we propose a practically motivated extension of scoring systems called probabilistic scoring lists (PSL), as well as a method for learning PSLs from data. Instead of making a deterministic decision, a PSL represents uncertainty in the form of probability distributions, or, more generally, probability intervals. Moreover, in the spirit of decision lists, a PSL evaluates features one by one and stops as soon as a decision can be made with enough confidence. To evaluate our approach, we conduct a case study in the medical domain.</li>
</ul>

<h3>Title: CXSimulator: A User Behavior Simulation using LLM Embeddings for Web-Marketing Campaign Assessment</h3>
<ul>
<li><strong>Authors: </strong>Akira Kasuga, Ryo Yonetani</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21553">https://arxiv.org/abs/2407.21553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21553">https://arxiv.org/pdf/2407.21553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21553]] CXSimulator: A User Behavior Simulation using LLM Embeddings for Web-Marketing Campaign Assessment(https://arxiv.org/abs/2407.21553)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents the Customer Experience (CX) Simulator, a novel framework designed to assess the effects of untested web-marketing campaigns through user behavior simulations. The proposed framework leverages large language models (LLMs) to represent various events in a user's behavioral history, such as viewing an item, applying a coupon, or purchasing an item, as semantic embedding vectors. We train a model to predict transitions between events from their LLM embeddings, which can even generalize to unseen events by learning from diverse training data. In web-marketing applications, we leverage this transition prediction model to simulate how users might react differently when new campaigns or products are presented to them. This allows us to eliminate the need for costly online testing and enhance the marketers' abilities to reveal insights. Our numerical evaluation and user study, utilizing BigQuery Public Datasets from the Google Merchandise Store, demonstrate the effectiveness of our framework.</li>
</ul>

<h3>Title: Conditioned Prompt-Optimization for Continual Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Francesco Laiti, Benedetta Liberatori, Thomas De Min, Elisa Ricci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21554">https://arxiv.org/abs/2407.21554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21554">https://arxiv.org/pdf/2407.21554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21554]] Conditioned Prompt-Optimization for Continual Deepfake Detection(https://arxiv.org/abs/2407.21554)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative models has significantly enhanced the realism and customization of digital content creation. The increasing power of these tools, coupled with their ease of access, fuels the creation of photorealistic fake content, termed deepfakes, that raises substantial concerns about their potential misuse. In response, there has been notable progress in developing detection mechanisms to identify content produced by these advanced systems. However, existing methods often struggle to adapt to the continuously evolving landscape of deepfake generation. This paper introduces Prompt2Guard, a novel solution for exemplar-free continual deepfake detection of images, that leverages Vision-Language Models (VLMs) and domain-specific multimodal prompts. Compared to previous VLM-based approaches that are either bounded by prompt selection accuracy or necessitate multiple forward passes, we leverage a prediction ensembling technique with read-only prompts. Read-only prompts do not interact with VLMs internal representation, mitigating the need for multiple forward passes. Thus, we enhance efficiency and accuracy in detecting generated content. Additionally, our method exploits a text-prompt conditioning tailored to deepfake detection, which we demonstrate is beneficial in our setting. We evaluate Prompt2Guard on CDDB-Hard, a continual deepfake detection benchmark composed of five deepfake detection datasets spanning multiple domains and generators, achieving a new state-of-the-art. Additionally, our results underscore the effectiveness of our approach in addressing the challenges posed by continual deepfake detection, paving the way for more robust and adaptable solutions in deepfake detection.</li>
</ul>

<h3>Title: Self-Sovereign Identity for Consented and Content-Based Access to Medical Records using Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Marie Tcholakian, Karolina Gorna, Maryline Laurent, Hella Kaffel Ben Ayed, Montassar Naghmouchi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21559">https://arxiv.org/abs/2407.21559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21559">https://arxiv.org/pdf/2407.21559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21559]] Self-Sovereign Identity for Consented and Content-Based Access to Medical Records using Blockchain(https://arxiv.org/abs/2407.21559)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Electronic Health Records (EHRs) and Medical Data are classified as personal data in every privacy law, meaning that any related service that includes processing such data must come with full security, confidentiality, privacy and accountability. Solutions for health data management, as in storing it, sharing and processing it, are emerging quickly and were significantly boosted by the Covid-19 pandemic that created a need to move things online. EHRs makes a crucial part of digital identity data, and the same digital identity trends -- as in self sovereign identity powered by decentralized ledger technologies like Blockchain, are being researched or implemented in contexts managing digital interactions between health facilities, patients and health professionals. In this paper, we propose a blockchain-based solution enabling secure exchange of EHRs between different parties powered by a self-sovereign identity (SSI) wallet and decentralized identifiers. We also make use of a consortium IPFS network for off-chain storage and attribute-based encryption (ABE) to ensure data confidentiality and integrity. Through our solution, we grant users full control over their medical data, and enable them to securely share it in total confidentiality over secure communication channels between user wallets using encryption. We also use DIDs for better user privacy and limit any possible correlations or identification by using pairwise DIDs. Overall, combining this set of technologies guarantees secure exchange of EHRs, secure storage and management along with by-design features inherited from the technological stack.</li>
</ul>

<h3>Title: Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jun Zhou, Dongyang Yu, Kamran Aziz, Fangfang Su, Qing Zhang, Fei Li, Donghong Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21560">https://arxiv.org/abs/2407.21560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21560">https://arxiv.org/pdf/2407.21560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21560]] Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding(https://arxiv.org/abs/2407.21560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-grained sentiment analysis involves extracting and organizing sentiment elements from textual data. However, existing approaches often overlook issues of category semantic inclusion and overlap, as well as inherent structural patterns within the target sequence. This study introduces a generative sentiment analysis model. To address the challenges related to category semantic inclusion and overlap, a latent category distribution variable is introduced. By reconstructing the input of a variational autoencoder, the model learns the intensity of the relationship between categories and text, thereby improving sequence generation. Additionally, a trie data structure and constrained decoding strategy are utilized to exploit structural patterns, which in turn reduces the search space and regularizes the generation process. Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets demonstrate a significant performance improvement compared to baseline models. Ablation experiments further confirm the effectiveness of latent category distribution and constrained decoding strategy.</li>
</ul>

<h3>Title: PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Min Jae Jung, JooHee Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21571">https://arxiv.org/abs/2407.21571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21571">https://arxiv.org/pdf/2407.21571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21571]] PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning(https://arxiv.org/abs/2407.21571)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) encounter significant challenges in continual learning due to catastrophic forgetting, where new information overwrites previously acquired knowledge. This limitation leads to substantial environmental and economic waste. In this study, we introduce the PMoE, Progressive Mixture of Experts with Asymmetric Transformer, which aims to minimize forgetting by utilizing an asymmetric design with shallow layers dedicated to general knowledge and deep layers for new knowledge. PMoE incorporates progressively added experts in deep layers and a router that allocates new knowledge to the appropriate experts efficiently. The router, positioned adjacent to the deep layers, utilizes deep features aggregating consolidated information. This enables the router to perform efficiently, allocating new knowledge to the appropriate experts, which progressively increase in the deep layers. Extensive experiments on TRACE datasets and general language understanding datasets demonstrate that the proposed PMoE outperforms previous state-of-the-art approaches.</li>
</ul>

<h3>Title: Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography</h3>
<ul>
<li><strong>Authors: </strong>Kit M. Bransby, Woo-jin Cho Kim, Jorge Oliveira, Alex Thorley, Arian Beqiri, Alberto Gomez, Agisilaos Chartsias</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21577">https://arxiv.org/abs/2407.21577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21577">https://arxiv.org/pdf/2407.21577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21577]] Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography(https://arxiv.org/abs/2407.21577)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Building an echocardiography view classifier that maintains performance in real-life cases requires diverse multi-site data, and frequent updates with newly available data to mitigate model drift. Simply fine-tuning on new datasets results in "catastrophic forgetting", and cannot adapt to variations of view labels between sites. Alternatively, collecting all data on a single server and re-training may not be feasible as data sharing agreements may restrict image transfer, or datasets may only become available at different times. Furthermore, time and cost associated with re-training grows with every new dataset. We propose a class-incremental learning method which learns an expert network for each dataset, and combines all expert networks with a score fusion model. The influence of ``unqualified experts'' is minimised by weighting each contribution with a learnt in-distribution score. These weights promote transparency as the contribution of each expert is known during inference. Instead of using the original images, we use learned features from each dataset, which are easier to share and raise fewer licensing and privacy concerns. We validate our work on six datasets from multiple sites, demonstrating significant reductions in training time while improving view classification performance.</li>
</ul>

<h3>Title: Voxel Scene Graph for Intracranial Hemorrhage</h3>
<ul>
<li><strong>Authors: </strong>Antoine P. Sanner, Nils F. Grauhan, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21580">https://arxiv.org/abs/2407.21580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21580">https://arxiv.org/pdf/2407.21580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21580]] Voxel Scene Graph for Intracranial Hemorrhage(https://arxiv.org/abs/2407.21580)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs to support the clinical decision-making. The majority of early work focuses on the detection and segmentation of ICH, but do not model the complex relations between ICH and adjacent brain structures. In this work, we design a tailored object detection method for ICH, which we unite with segmentation-grounded Scene Graph Generation (SGG) methods to learn a holistic representation of the clinical cerebral scene. To the best of our knowledge, this is the first application of SGG for 3D voxel images. We evaluate our method on two head-CT datasets and demonstrate that our model can recall up to 74% of clinically relevant relations. This work lays the foundation towards SGG for 3D voxel data. The generated Scene Graphs can already provide insights for the clinician, but are also valuable for all downstream tasks as a compact and interpretable representation.</li>
</ul>

<h3>Title: Adaptive Mix for Semi-Supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Shen, Peng Cao, Junming Su, Jinzhu Yang, Osmar R. Zaiane</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21586">https://arxiv.org/abs/2407.21586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21586">https://arxiv.org/pdf/2407.21586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21586]] Adaptive Mix for Semi-Supervised Medical Image Segmentation(https://arxiv.org/abs/2407.21586)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Mix-up is a key technique for consistency regularization-based semi-supervised learning methods, generating strong-perturbed samples for strong-weak pseudo-supervision. Existing mix-up operations are performed either randomly or with predefined rules, such as replacing low-confidence patches with high-confidence ones. The former lacks control over the perturbation degree, leading to overfitting on randomly perturbed samples, while the latter tends to generate images with trivial perturbations, both of which limit the effectiveness of consistency learning. This paper aims to answer the following question: How can image mix-up perturbation be adaptively performed during training? To this end, we propose an Adaptive Mix algorithm (AdaMix) for image mix-up in a self-paced learning manner. Given that, in general, a model's performance gradually improves during training, AdaMix is equipped with a self-paced curriculum that, in the initial training stage, provides relatively simple perturbed samples and then gradually increases the difficulty of perturbed images by adaptively controlling the perturbation degree based on the model's learning state estimated by a self-paced regularize. We develop three frameworks with our AdaMix, i.e., AdaMix-ST, AdaMix-MT, and AdaMix-CT, for semi-supervised medical image segmentation. Extensive experiments on three public datasets, including both 2D and 3D modalities, show that the proposed frameworks are capable of achieving superior performance. For example, compared with the state-of-the-art, AdaMix-CT achieves relative improvements of 2.62% in Dice and 48.25% in average surface distance on the ACDC dataset with 10% labeled data. The results demonstrate that mix-up operations with dynamically adjusted perturbation strength based on the segmentation model's state can significantly enhance the effectiveness of consistency regularization.</li>
</ul>

<h3>Title: Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality</h3>
<ul>
<li><strong>Authors: </strong>Steven N. Hart, Thomas E. Tavolara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21590">https://arxiv.org/abs/2407.21590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21590">https://arxiv.org/pdf/2407.21590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21590]] Measuring What Matters: Intrinsic Distance Preservation as a Robust Metric for Embedding Quality(https://arxiv.org/abs/2407.21590)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unsupervised embeddings are fundamental to numerous machine learning applications, yet their evaluation remains a challenging task. Traditional assessment methods often rely on extrinsic variables, such as performance in downstream tasks, which can introduce confounding factors and mask the true quality of embeddings. This paper introduces the Intrinsic Distance Preservation Evaluation (IDPE) method, a novel approach for assessing embedding quality based on the preservation of Mahalanobis distances between data points in the original and embedded spaces. We demonstrate the limitations of extrinsic evaluation methods through a simple example, highlighting how they can lead to misleading conclusions about embedding quality. IDPE addresses these issues by providing a task-independent measure of how well embeddings preserve the intrinsic structure of the original data. Our method leverages efficient similarity search techniques to make it applicable to large-scale datasets. We compare IDPE with established intrinsic metrics like trustworthiness and continuity, as well as extrinsic metrics such as Average Rank and Mean Reciprocal Rank. Our results show that IDPE offers a more comprehensive and reliable assessment of embedding quality across various scenarios. We evaluate PCA and t-SNE embeddings using IDPE, revealing insights into their performance that are not captured by traditional metrics. This work contributes to the field by providing a robust, efficient, and interpretable method for embedding evaluation. IDPE's focus on intrinsic properties offers a valuable tool for researchers and practitioners seeking to develop and assess high-quality embeddings for diverse machine learning applications.</li>
</ul>

<h3>Title: Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2</h3>
<ul>
<li><strong>Authors: </strong>Lv Tang, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21596">https://arxiv.org/abs/2407.21596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21596">https://arxiv.org/pdf/2407.21596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21596]] Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2(https://arxiv.org/abs/2407.21596)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM), introduced by Meta AI Research as a generic object segmentation model, quickly garnered widespread attention and significantly influenced the academic community. To extend its application to video, Meta further develops Segment Anything Model 2 (SAM2), a unified model capable of both video and image segmentation. SAM2 shows notable improvements over its predecessor in terms of applicable domains, promptable segmentation accuracy, and running speed. However, this report reveals a decline in SAM2's ability to perceive different objects in images without prompts in its auto mode, compared to SAM. Specifically, we employ the challenging task of camouflaged object detection to assess this performance decrease, hoping to inspire further exploration of the SAM model family by researchers. The results of this paper are provided in \url{this https URL}.</li>
</ul>

<h3>Title: MicroMIL: Graph-based Contextual Multiple Instance Learning for Patient Diagnosis Using Microscopy Images</h3>
<ul>
<li><strong>Authors: </strong>JongWoo Kim, Bryan Wong, YoungSin Ko, MunYong Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21604">https://arxiv.org/abs/2407.21604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21604">https://arxiv.org/pdf/2407.21604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21604]] MicroMIL: Graph-based Contextual Multiple Instance Learning for Patient Diagnosis Using Microscopy Images(https://arxiv.org/abs/2407.21604)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Current histopathology research has primarily focused on using whole-slide images (WSIs) produced by scanners with weakly-supervised multiple instance learning (MIL). However, WSIs are costly, memory-intensive, and require extensive analysis time. As an alternative, microscopy-based analysis offers cost and memory efficiency, though microscopy images face issues with unknown absolute positions and redundant images due to multiple captures from the subjective perspectives of pathologists. To this end, we introduce MicroMIL, a weakly-supervised MIL framework specifically built to address these challenges by dynamically clustering images using deep cluster embedding (DCE) and Gumbel Softmax for representative image extraction. Graph edges are then constructed from the upper triangular similarity matrix, with nodes connected to their most similar neighbors, and a graph neural network (GNN) is utilized to capture local and diverse areas of contextual information. Unlike existing graph-based MIL methods designed for WSIs that require absolute positions, MicroMIL efficiently handles the graph edges without this need. Extensive evaluations on real-world colon cancer (Seegene) and public BreakHis datasets demonstrate that MicroMIL outperforms state-of-the-art (SOTA) methods, offering a robust and efficient solution for patient diagnosis using microscopy images. The code is available at https://anonymous.4open.science/r/MicroMIL-6C7C</li>
</ul>

<h3>Title: Grid-Based Decompositions for Spatial Data under Local Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Berkay Kemal Balioglu, Alireza Khodaie, Ameer Taweel, Mehmet Emre Gursoy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21624">https://arxiv.org/abs/2407.21624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21624">https://arxiv.org/pdf/2407.21624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21624]] Grid-Based Decompositions for Spatial Data under Local Differential Privacy(https://arxiv.org/abs/2407.21624)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Local differential privacy (LDP) has recently emerged as a popular privacy standard. With the growing popularity of LDP, several recent works have applied LDP to spatial data, and grid-based decompositions have been a common building block in the collection and analysis of spatial data under DP and LDP. In this paper, we study three grid-based decomposition methods for spatial data under LDP: Uniform Grid (UG), PrivAG, and AAG. UG is a static approach that consists of equal-sized cells. To enable data-dependent decomposition, PrivAG was proposed by Yang et al. as the most recent adaptive grid method. To advance the state-of-the-art in adaptive grids, in this paper we propose the Advanced Adaptive Grid (AAG) method. For each grid cell, following the intuition that the cell's intra-cell density distribution will be affected by its neighbors, AAG performs uneven cell divisions depending on the neighboring cells' densities. We experimentally compare UG, PrivAG, and AAG using three real-world location datasets, varying privacy budgets, and query sizes. Results show that AAG provides higher utility than PrivAG, demonstrating the superiority of our proposed approach. Furthermore, UG's performance is heavily dependent on the choice of grid size. When the grid size is chosen optimally in UG, AAG still beats UG for small queries, but UG beats AAG for large (coarse-grained) queries.</li>
</ul>

<h3>Title: TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21630">https://arxiv.org/abs/2407.21630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21630">https://arxiv.org/pdf/2407.21630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21630]] TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods(https://arxiv.org/abs/2407.21630)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Authorship obfuscation aims to disguise the identity of an author within a text by altering the writing style, vocabulary, syntax, and other linguistic features associated with the text author. This alteration needs to balance privacy and utility. While strong obfuscation techniques can effectively hide the author's identity, they often degrade the quality and usefulness of the text for its intended purpose. Conversely, maintaining high utility tends to provide insufficient privacy, making it easier for an adversary to de-anonymize the author. Thus, achieving an optimal trade-off between these two conflicting objectives is crucial. In this paper, we propose TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship obfuscation method whose goal is to optimize the privacy-utility trade-off by regenerating the entire text considering its downstream utility. Our approach leverages policy optimization as a fine-tuning paradigm over small language models in order to rewrite texts by preserving author identity and downstream task utility. We show that our approach largely reduce the accuracy of attackers while preserving utility. We make our code and models publicly available.</li>
</ul>

<h3>Title: RoadFormer+: Delivering RGB-X Scene Parsing through Scale-Aware Information Decoupling and Advanced Heterogeneous Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jianxin Huang, Jiahang Li, Ning Jia, Yuxiang Sun, Chengju Liu, Qijun Chen, Rui Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21631">https://arxiv.org/abs/2407.21631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21631">https://arxiv.org/pdf/2407.21631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21631]] RoadFormer+: Delivering RGB-X Scene Parsing through Scale-Aware Information Decoupling and Advanced Heterogeneous Feature Fusion(https://arxiv.org/abs/2407.21631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Task-specific data-fusion networks have marked considerable achievements in urban scene parsing. Among these networks, our recently proposed RoadFormer successfully extracts heterogeneous features from RGB images and surface normal maps and fuses these features through attention mechanisms, demonstrating compelling efficacy in RGB-Normal road scene parsing. However, its performance significantly deteriorates when handling other types/sources of data or performing more universal, all-category scene parsing tasks. To overcome these limitations, this study introduces RoadFormer+, an efficient, robust, and adaptable model capable of effectively fusing RGB-X data, where ``X'', represents additional types/modalities of data such as depth, thermal, surface normal, and polarization. Specifically, we propose a novel hybrid feature decoupling encoder to extract heterogeneous features and decouple them into global and local components. These decoupled features are then fused through a dual-branch multi-scale heterogeneous feature fusion block, which employs parallel Transformer attentions and convolutional neural network modules to merge multi-scale features across different scales and receptive fields. The fused features are subsequently fed into a decoder to generate the final semantic predictions. Notably, our proposed RoadFormer+ ranks first on the KITTI Road benchmark and achieves state-of-the-art performance in mean intersection over union on the Cityscapes, MFNet, FMB, and ZJU datasets. Moreover, it reduces the number of learnable parameters by 65\% compared to RoadFormer. Our source code will be publicly available at mias.group/RoadFormerPlus.</li>
</ul>

<h3>Title: Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21633">https://arxiv.org/abs/2407.21633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21633">https://arxiv.org/pdf/2407.21633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21633]] Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation(https://arxiv.org/abs/2407.21633)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without manual annotation or extensive retraining. Prior research has approached this objective by embedding prompts into language models (LMs). Common methodologies include integrating prompts at the input layer or introducing learnable variables at each transformer layer. Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at the input layer risk underutilization, with their impact potentially diminishing across successive transformer layers. Conversely, the addition of learnable variables to each layer can complicate the training process and increase inference latency. To tackle the issues mentioned above, this paper proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank Adaptation (LoRA) components, targeting both dialogue context processing and prompt optimization, to ensure the comprehensive influence of prompts throughout the transformer model layers. This is achieved without incurring additional inference latency, showcasing an efficient integration into existing architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets, DualLoRA demonstrates notable improvements across multiple domains, outperforming traditional baseline methods in zero-shot settings. Our code is accessible at: \url{this https URL}.</li>
</ul>

<h3>Title: MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Seongju Lee, Junseok Lee, Yeonguk Yu, Taeri Kim, Kyoobin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21635">https://arxiv.org/abs/2407.21635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21635">https://arxiv.org/pdf/2407.21635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21635]] MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction(https://arxiv.org/abs/2407.21635)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multi-agent trajectory prediction is crucial to autonomous driving and understanding the surrounding environment. Learning-based approaches for multi-agent trajectory prediction, such as primarily relying on graph neural networks, graph transformers, and hypergraph neural networks, have demonstrated outstanding performance on real-world datasets in recent years. However, the hypergraph transformer-based method for trajectory prediction is yet to be explored. Therefore, we present a MultiscAle Relational Transformer (MART) network for multi-agent trajectory prediction. MART is a hypergraph transformer architecture to consider individual and group behaviors in transformer machinery. The core module of MART is the encoder, which comprises a Pair-wise Relational Transformer (PRT) and a Hyper Relational Transformer (HRT). The encoder extends the capabilities of a relational transformer by introducing HRT, which integrates hyperedge features into the transformer mechanism, promoting attention weights to focus on group-wise relations. In addition, we propose an Adaptive Group Estimator (AGE) designed to infer complex group relations in real-world environments. Extensive experiments on three real-world datasets (NBA, SDD, and ETH-UCY) demonstrate that our method achieves state-of-the-art performance, enhancing ADE/FDE by 3.9%/11.8% on the NBA dataset. Code is available at this https URL.</li>
</ul>

<h3>Title: Spatial Transformer Network YOLO Model for Agricultural Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21652">https://arxiv.org/abs/2407.21652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21652">https://arxiv.org/pdf/2407.21652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21652]] Spatial Transformer Network YOLO Model for Agricultural Object Detection(https://arxiv.org/abs/2407.21652)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Object detection plays a crucial role in the field of computer vision by autonomously identifying and locating objects of interest. The You Only Look Once (YOLO) model is an effective single-shot detector. However, YOLO faces challenges in cluttered or partially occluded scenes and can struggle with small, low-contrast objects. We propose a new method that integrates spatial transformer networks (STNs) into YOLO to improve performance. The proposed STN-YOLO aims to enhance the model's effectiveness by focusing on important areas of the image and improving the spatial invariance of the model before the detection process. Our proposed method improved object detection performance both qualitatively and quantitatively. We explore the impact of different localization networks within the STN module as well as the robustness of the model across different spatial transformations. We apply the STN-YOLO on benchmark datasets for Agricultural object detection as well as a new dataset from a state-of-the-art plant phenotyping greenhouse facility. Our code and dataset are publicly available.</li>
</ul>

<h3>Title: MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment</h3>
<ul>
<li><strong>Authors: </strong>Anurag Das, Xinting Hu, Li Jiang, Bernt Schiele</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21654">https://arxiv.org/abs/2407.21654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21654">https://arxiv.org/pdf/2407.21654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21654]] MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment(https://arxiv.org/abs/2407.21654)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent approaches have shown that large-scale vision-language models such as CLIP can improve semantic segmentation performance. These methods typically aim for pixel-level vision-language alignment, but often rely on low resolution image features from CLIP, resulting in class ambiguities along boundaries. Moreover, the global scene representations in CLIP text embeddings do not directly correlate with the local and detailed pixel-level features, making meaningful alignment more difficult. To address these limitations, we introduce MTA-CLIP, a novel framework employing mask-level vision-language alignment. Specifically, we first propose Mask-Text Decoder that enhances the mask representations using rich textual data with the CLIP language model. Subsequently, it aligns mask representations with text embeddings using Mask-to-Text Contrastive Learning. Furthermore, we introduce MaskText Prompt Learning, utilizing multiple context-specific prompts for text embeddings to capture diverse class representations across masks. Overall, MTA-CLIP achieves state-of-the-art, surpassing prior works by an average of 2.8% and 1.3% on on standard benchmark datasets, ADE20k and Cityscapes, respectively.</li>
</ul>

<h3>Title: Comgra: A Tool for Analyzing and Debugging Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Florian Dietz, Sophie Fellenz, Dietrich Klakow, Marius Kloft</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21656">https://arxiv.org/abs/2407.21656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21656">https://arxiv.org/pdf/2407.21656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21656]] Comgra: A Tool for Analyzing and Debugging Neural Networks(https://arxiv.org/abs/2407.21656)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Neural Networks are notoriously difficult to inspect. We introduce comgra, an open source python library for use with PyTorch. Comgra extracts data about the internal activations of a model and organizes it in a GUI (graphical user interface). It can show both summary statistics and individual data points, compare early and late stages of training, focus on individual samples of interest, and visualize the flow of the gradient through the network. This makes it possible to inspect the model's behavior from many different angles and save time by rapidly testing different hypotheses without having to rerun it. Comgra has applications for debugging, neural architecture design, and mechanistic interpretability. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at this https URL.</li>
</ul>

<h3>Title: Defending Jailbreak Attack in VLMs via Cross-modality Information Detector</h3>
<ul>
<li><strong>Authors: </strong>Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21659">https://arxiv.org/abs/2407.21659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21659">https://arxiv.org/pdf/2407.21659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21659]] Defending Jailbreak Attack in VLMs via Cross-modality Information Detector(https://arxiv.org/abs/2407.21659)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively understand vision information, achieving remarkable performance in many vision-centric tasks. Despite that, recent studies have shown that these models are susceptible to jailbreak attacks, which refer to an exploitative technique where malicious users can break the safety alignment of the target model and generate misleading and harmful answers. This potential threat is caused by both the inherent vulnerabilities of LLM and the larger attack scope introduced by vision input. To enhance the security of VLMs against jailbreak attacks, researchers have developed various defense techniques. However, these methods either require modifications to the model's internal structure or demand significant computational resources during the inference phase. Multimodal information is a double-edged sword. While it increases the risk of attacks, it also provides additional data that can enhance safeguards. Inspired by this, we propose $\underline{\textbf{C}}$ross-modality $\underline{\textbf{I}}$nformation $\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a plug-and-play jailbreaking detector designed to identify maliciously perturbed image inputs, utilizing the cross-modal similarity between harmful queries and adversarial images. This simple yet effective cross-modality information detector, $\textit{CIDER}$, is independent of the target VLMs and requires less computation cost. Extensive experimental results demonstrate the effectiveness and efficiency of $\textit{CIDER}$, as well as its transferability to both white-box and black-box VLMs.</li>
</ul>

<h3>Title: An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification</h3>
<ul>
<li><strong>Authors: </strong>Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21666">https://arxiv.org/abs/2407.21666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21666">https://arxiv.org/pdf/2407.21666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21666]] An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification(https://arxiv.org/abs/2407.21666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Early detection of drought stress is critical for taking timely measures for reducing crop loss before the drought impact becomes irreversible. The subtle phenotypical and physiological changes in response to drought stress are captured by non-invasive imaging techniques and these imaging data serve as valuable resource for machine learning methods to identify drought stress. While convolutional neural networks (CNNs) are in wide use, vision transformers (ViTs) present a promising alternative in capturing long-range dependencies and intricate spatial relationships, thereby enhancing the detection of subtle indicators of drought stress. We propose an explainable deep learning pipeline that leverages the power of ViTs for drought stress detection in potato crops using aerial imagery. We applied two distinct approaches: a synergistic combination of ViT and support vector machine (SVM), where ViT extracts intricate spatial features from aerial images, and SVM classifies the crops as stressed or healthy and an end-to-end approach using a dedicated classification layer within ViT to directly detect drought stress. Our key findings explain the ViT model's decision-making process by visualizing attention maps. These maps highlight the specific spatial features within the aerial images that the ViT model focuses as the drought stress signature. Our findings demonstrate that the proposed methods not only achieve high accuracy in drought stress identification but also shedding light on the diverse subtle plant features associated with drought stress. This offers a robust and interpretable solution for drought stress monitoring for farmers to undertake informed decisions for improved crop management.</li>
</ul>

<h3>Title: Synth-Empathy: Towards High-Quality Synthetic Empathy Data</h3>
<ul>
<li><strong>Authors: </strong>Hao Liang, Linzhuang Sun, Jingxuan Wei, Xijie Huang, Linkun Sun, Bihui Yu, Conghui He, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21669">https://arxiv.org/abs/2407.21669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21669">https://arxiv.org/pdf/2407.21669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21669]] Synth-Empathy: Towards High-Quality Synthetic Empathy Data(https://arxiv.org/abs/2407.21669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, with the rapid advancements in large language models (LLMs), achieving excellent empathetic response capabilities has become a crucial prerequisite. Consequently, managing and understanding empathetic datasets have gained increasing significance. However, empathetic data are typically human-labeled, leading to insufficient datasets and wasted human labor. In this work, we present Synth-Empathy, an LLM-based data generation and quality and diversity selection pipeline that automatically generates high-quality empathetic data while discarding low-quality data. With the data generated from a low empathetic model, we are able to further improve empathetic response performance and achieve state-of-the-art (SoTA) results across multiple benchmarks. Moreover, our model achieves SoTA performance on various human evaluation benchmarks, demonstrating its effectiveness and robustness in real-world applications. Furthermore, we show the trade-off between data quantity and quality, providing insights into empathetic data generation and selection.</li>
</ul>

<h3>Title: Dynamic Object Queries for Transformer-based Incremental Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jichuan Zhang, Wei Li, Shuang Cheng, Ya-Li Li, Shengjin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21687">https://arxiv.org/abs/2407.21687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21687">https://arxiv.org/pdf/2407.21687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21687]] Dynamic Object Queries for Transformer-based Incremental Object Detection(https://arxiv.org/abs/2407.21687)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Incremental object detection (IOD) aims to sequentially learn new classes, while maintaining the capability to locate and identify old ones. As the training data arrives with annotations only with new classes, IOD suffers from catastrophic forgetting. Prior methodologies mainly tackle the forgetting issue through knowledge distillation and exemplar replay, ignoring the conflict between limited model capacity and increasing knowledge. In this paper, we explore \textit{dynamic object queries} for incremental object detection built on Transformer architecture. We propose the \textbf{Dy}namic object \textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), which incrementally expands the model representation ability to achieve stability-plasticity tradeoff. First, a new set of learnable object queries are fed into the decoder to represent new classes. These new object queries are aggregated with those from previous phases to adapt both old and new knowledge well. Second, we propose the isolated bipartite matching for object queries in different phases, based on disentangled self-attention. The interaction among the object queries at different phases is eliminated to reduce inter-class confusion. Thanks to the separate supervision and computation over object queries, we further present the risk-balanced partial calibration for effective exemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly surpasses the state-of-the-art methods, with limited parameter overhead. Code will be made publicly available.</li>
</ul>

<h3>Title: Explainable Artificial Intelligence for Quantifying Interfering and High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom Environment Using Privacy-Preserving Video Analysis</h3>
<ul>
<li><strong>Authors: </strong>Barun Das, Conor Anderson, Tania Villavicencio, Johanna Lantz, Jenny Foster, Theresa Hamlin, Ali Bahrami Rad, Gari D. Clifford, Hyeokhyen Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21691">https://arxiv.org/abs/2407.21691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21691">https://arxiv.org/pdf/2407.21691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21691]] Explainable Artificial Intelligence for Quantifying Interfering and High-Risk Behaviors in Autism Spectrum Disorder in a Real-World Classroom Environment Using Privacy-Preserving Video Analysis(https://arxiv.org/abs/2407.21691)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Rapid identification and accurate documentation of interfering and high-risk behaviors in ASD, such as aggression, self-injury, disruption, and restricted repetitive behaviors, are important in daily classroom environments for tracking intervention effectiveness and allocating appropriate resources to manage care needs. However, having a staff dedicated solely to observing is costly and uncommon in most educational settings. Recently, multiple research studies have explored developing automated, continuous, and objective tools using machine learning models to quantify behaviors in ASD. However, the majority of the work was conducted under a controlled environment and has not been validated for real-world conditions. In this work, we demonstrate that the latest advances in video-based group activity recognition techniques can quantify behaviors in ASD in real-world activities in classroom environments while preserving privacy. Our explainable model could detect the episode of problem behaviors with a 77% F1-score and capture distinctive behavior features in different types of behaviors in ASD. To the best of our knowledge, this is the first work that shows the promise of objectively quantifying behaviors in ASD in a real-world environment, which is an important step toward the development of a practical tool that can ease the burden of data collection for classroom staff.</li>
</ul>

<h3>Title: Tora: Trajectory-oriented Diffusion Transformer for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21705">https://arxiv.org/abs/2407.21705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21705">https://arxiv.org/pdf/2407.21705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21705]] Tora: Trajectory-oriented Diffusion Transformer for Video Generation(https://arxiv.org/abs/2407.21705)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in Diffusion Transformer (DiT) have demonstrated remarkable proficiency in producing high-quality video content. Nonetheless, the potential of transformer-based diffusion models for effectively generating videos with controllable motion remains an area of limited exploration. This paper introduces Tora, the first trajectory-oriented DiT framework that integrates textual, visual, and trajectory conditions concurrently for video generation. Specifically, Tora consists of a Trajectory Extractor~(TE), a Spatial-Temporal DiT, and a Motion-guidance Fuser~(MGF). The TE encodes arbitrary trajectories into hierarchical spacetime motion patches with a 3D video compression network. The MGF integrates the motion patches into the DiT blocks to generate consistent videos following trajectories. Our design aligns seamlessly with DiT's scalability, allowing precise control of video content's dynamics with diverse durations, aspect ratios, and resolutions. Extensive experiments demonstrate Tora's excellence in achieving high motion fidelity, while also meticulously simulating the movement of the physical world. Page can be found at this https URL.</li>
</ul>

<h3>Title: Adaptive Retrieval-Augmented Generation for Conversational Systems</h3>
<ul>
<li><strong>Authors: </strong>Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21712">https://arxiv.org/abs/2407.21712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21712">https://arxiv.org/pdf/2407.21712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21712]] Adaptive Retrieval-Augmented Generation for Conversational Systems(https://arxiv.org/abs/2407.21712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation's confidence level and the relevance of the augmented knowledge.</li>
</ul>

<h3>Title: Detecting, Explaining, and Mitigating Memorization in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Wen, Yuchen Liu, Chen Chen, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21720">https://arxiv.org/abs/2407.21720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21720">https://arxiv.org/pdf/2407.21720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21720]] Detecting, Explaining, and Mitigating Memorization in Diffusion Models(https://arxiv.org/abs/2407.21720)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merely replications of training data. Such replications present potential legal challenges for model owners, especially when the generated content contains proprietary information. In this work, we introduce a straightforward yet effective method for detecting memorized prompts by inspecting the magnitude of text-conditional predictions. Our proposed method seamlessly integrates without disrupting sampling algorithms, and delivers high accuracy even at the first generation step, with a single generation per prompt. Building on our detection strategy, we unveil an explainable approach that shows the contribution of individual words or tokens to memorization. This offers an interactive medium for users to adjust their prompts. Moreover, we propose two strategies i.e., to mitigate memorization by leveraging the magnitude of text-conditional predictions, either through minimization during inference or filtering during training. These proposed strategies effectively counteract memorization while maintaining high-generation quality. Code is available at this https URL.</li>
</ul>

<h3>Title: A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, Karthik Nandakumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21739">https://arxiv.org/abs/2407.21739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21739">https://arxiv.org/pdf/2407.21739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21739]] A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation(https://arxiv.org/abs/2407.21739)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Adapting foundation models for medical image analysis requires finetuning them on a considerable amount of data because of extreme distribution shifts between natural (source) data used for pretraining and medical (target) data. However, collecting task-specific medical data for such finetuning at a central location raises many privacy concerns. Although Federated learning (FL) provides an effective means for training on private decentralized data, communication costs in federating large foundation models can quickly become a significant bottleneck, impacting the solution's scalability. In this work, we address this problem of efficient communication while ensuring effective learning in FL by combining the strengths of Parameter-Efficient Fine-tuning (PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA) in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical image segmentation. Unlike prior works that utilize LoRA and finetune the entire decoder, we critically analyze the contribution of each granular component of SAM on finetuning performance. Thus, we identify specific layers to be federated that are very efficient in terms of communication cost while producing on-par accuracy. Our experiments show that retaining the parameters of the SAM model (including most of the decoder) in their original state during adaptation is beneficial because fine-tuning on small datasets tends to distort the inherent capabilities of the underlying foundation model. On Fed-KiTS, our approach decreases communication cost (~48x) compared to full fine-tuning while increasing performance (~6% Dice score) in 3D segmentation tasks. Our approach performs similar to SAMed while achieving ~2.8x reduction in communication and parameters to be finetuned. We further validate our approach with experiments on Fed-IXI and Prostate MRI datasets.</li>
</ul>

<h3>Title: Contrastive Factor Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhibin Duan, Tiansheng Wen, Yifei Wang, Chen Zhu, Bo Chen, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21740">https://arxiv.org/abs/2407.21740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21740">https://arxiv.org/pdf/2407.21740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21740]] Contrastive Factor Analysis(https://arxiv.org/abs/2407.21740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Factor analysis, often regarded as a Bayesian variant of matrix factorization, offers superior capabilities in capturing uncertainty, modeling complex dependencies, and ensuring robustness. As the deep learning era arrives, factor analysis is receiving less and less attention due to their limited expressive ability. On the contrary, contrastive learning has emerged as a potent technique with demonstrated efficacy in unsupervised representational learning. While the two methods are different paradigms, recent theoretical analysis has revealed the mathematical equivalence between contrastive learning and matrix factorization, providing a potential possibility for factor analysis combined with contrastive learning. Motivated by the interconnectedness of contrastive learning, matrix factorization, and factor analysis, this paper introduces a novel Contrastive Factor Analysis framework, aiming to leverage factor analysis's advantageous properties within the realm of contrastive learning. To further leverage the interpretability properties of non-negative factor analysis, which can learn disentangled representations, contrastive factor analysis is extended to a non-negative version. Finally, extensive experimental validation showcases the efficacy of the proposed contrastive (non-negative) factor analysis methodology across multiple key properties, including expressiveness, robustness, interpretability, and accurate uncertainty estimation.</li>
</ul>

<h3>Title: HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Junwei He, Qianqian Xu, Yangbangyan Jiang, Zitai Wang, Yuchen Sun, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21742">https://arxiv.org/abs/2407.21742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21742">https://arxiv.org/pdf/2407.21742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21742]] HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection(https://arxiv.org/abs/2407.21742)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the progressive advancements in deep graph learning, out-of-distribution (OOD) detection for graph data has emerged as a critical challenge. While the efficacy of auxiliary datasets in enhancing OOD detection has been extensively studied for image and text data, such approaches have not yet been explored for graph data. Unlike Euclidean data, graph data exhibits greater diversity but lower robustness to perturbations, complicating the integration of outliers. To tackle these challenges, we propose the introduction of \textbf{H}ybrid External and Internal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE) to improve graph OOD detection performance. Our framework involves using realistic external graph data from various domains and synthesizing internal outliers within ID subgroups to address the poor robustness and presence of OOD samples within the ID class. Furthermore, we develop a boundary-aware OE loss that adaptively assigns weights to outliers, maximizing the use of high-quality OOD samples while minimizing the impact of low-quality ones. Our proposed HGOE framework is model-agnostic and designed to enhance the effectiveness of existing graph OOD detection models. Experimental results demonstrate that our HGOE framework can significantly improve the performance of existing OOD detection models across all 8 real datasets.</li>
</ul>

<h3>Title: Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Shi Liu, Kecheng Zheng, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21771">https://arxiv.org/abs/2407.21771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21771">https://arxiv.org/pdf/2407.21771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21771]] Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs(https://arxiv.org/abs/2407.21771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing Large Vision-Language Models (LVLMs) primarily align image features of vision encoder with Large Language Models (LLMs) to leverage their superior text generation capabilities. However, the scale disparity between vision encoder and language model may led to LLMs assuming a predominant role in multi-modal comprehension. This imbalance in LVLMs may result in the instances of hallucinatory. Concretely, LVLMs may generate consistent descriptions with or without visual input, indicating that certain outputs are influenced solely by context text. We refer to this phenomenon as "text inertia." To counteract this issue, we introduce a training-free algorithm to find an equilibrium point between image comprehension and language inference. Specifically, we adaptively involve adjusting and amplifying the attention weights assigned to image tokens, thereby granting greater prominence to visual elements. Meanwhile, we subtract the logits of multi-modal inputs from ones of pure text input, which can help LVLMs be not biased towards LLMs. By enhancing images tokens and reducing the stubborn output of LLM, we can let LVLM pay more attention to images, towards alleviating text inertia and reducing the hallucination in LVLMs. Our extensive experiments shows that this method substantially reduces the frequency of hallucinatory outputs in various LVLMs in terms of different metrics. Project page is available at this https URL.</li>
</ul>

<h3>Title: ShieldGemma: Generative AI Content Moderation Based on Gemma</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21772">https://arxiv.org/abs/2407.21772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21772">https://arxiv.org/pdf/2407.21772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21772]] ShieldGemma: Generative AI Content Moderation Based on Gemma(https://arxiv.org/abs/2407.21772)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>We present ShieldGemma, a comprehensive suite of LLM-based safety content moderation models built upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and LLM-generated output. By evaluating on both public and internal benchmarks, we demonstrate superior performance compared to existing models, such as Llama Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%). Additionally, we present a novel LLM-based data curation pipeline, adaptable to a variety of safety-related tasks and beyond. We have shown strong generalization performance for model trained mainly on synthetic data. By releasing ShieldGemma, we provide a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers.</li>
</ul>

<h3>Title: RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining</h3>
<ul>
<li><strong>Authors: </strong>Hongtao Wu, Yijun Yang, Huihui Xu, Weiming Wang, Jinni Zhou, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21773">https://arxiv.org/abs/2407.21773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21773">https://arxiv.org/pdf/2407.21773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21773]] RainMamba: Enhanced Locality Learning with State Space Models for Video Deraining(https://arxiv.org/abs/2407.21773)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The outdoor vision systems are frequently contaminated by rain streaks and raindrops, which significantly degenerate the performance of visual tasks and multimedia applications. The nature of videos exhibits redundant temporal cues for rain removal with higher stability. Traditional video deraining methods heavily rely on optical flow estimation and kernel-based manners, which have a limited receptive field. Yet, transformer architectures, while enabling long-term dependencies, bring about a significant increase in computational complexity. Recently, the linear-complexity operator of the state space models (SSMs) has contrarily facilitated efficient long-term temporal modeling, which is crucial for rain streaks and raindrops removal in videos. Unexpectedly, its uni-dimensional sequential process on videos destroys the local correlations across the spatio-temporal dimension by distancing adjacent pixels. To address this, we present an improved SSMs-based video deraining network (RainMamba) with a novel Hilbert scanning mechanism to better capture sequence-level local information. We also introduce a difference-guided dynamic contrastive locality learning strategy to enhance the patch-level self-similarity learning ability of the proposed network. Extensive experiments on four synthesized video deraining datasets and real-world rainy videos demonstrate the superiority of our network in the removal of rain streaks and raindrops.</li>
</ul>

<h3>Title: Vision-Language Model Based Handwriting Verification</h3>
<ul>
<li><strong>Authors: </strong>Mihir Chauhan, Abhishek Satbhai, Mohammad Abuzar Hashemi, Mir Basheer Ali, Bina Ramamurthy, Mingchen Gao, Siwei Lyu, Sargur Srihari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21788">https://arxiv.org/abs/2407.21788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21788">https://arxiv.org/pdf/2407.21788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21788]] Vision-Language Model Based Handwriting Verification(https://arxiv.org/abs/2407.21788)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Handwriting Verification is a critical in document forensics. Deep learning based approaches often face skepticism from forensic document examiners due to their lack of explainability and reliance on extensive training data and handcrafted features. This paper explores using Vision Language Models (VLMs), such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By leveraging their Visual Question Answering capabilities and 0-shot Chain-of-Thought (CoT) reasoning, our goal is to provide clear, human-understandable explanations for model decisions. Our experiments on the CEDAR handwriting dataset demonstrate that VLMs offer enhanced interpretability, reduce the need for large training datasets, and adapt better to diverse handwriting styles. However, results show that the CNN-based ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy: 71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings highlight the potential of VLMs in generating human-interpretable decisions while underscoring the need for further advancements to match the performance of specialized deep learning models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
