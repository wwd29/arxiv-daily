<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Blockchain-based decentralized voting system security Perspective: Safe and secure for digital voting system. (arXiv:2303.06306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06306">http://arxiv.org/abs/2303.06306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06306] Blockchain-based decentralized voting system security Perspective: Safe and secure for digital voting system](http://arxiv.org/abs/2303.06306) #secure</code></li>
<li>Summary: <p>This research study focuses primarily on Block-Chain-based voting systems,
which facilitate participation in and administration of voting for voters,
candidates, and officials. Because we used Block-Chain in the backend, which
enables everyone to trace vote fraud, our system is incredibly safe. This paper
approach any unique identification the Aadhar Card number or an OTP will be
generated then user can utilise the voting system to cast his/her vote. A
proposal for Bit-coin, a virtual currency system that is decided by a central
authority for producing money, transferring ownership, and validating
transactions, included the peer-to-peer network in a Block-Chain system, the
ledger is duplicated across several, identical databases which is hosted and
updated by a different process and all other nodes are updated concurrently if
changes made to one node and a transaction occurs, the records of the values
and assets are permanently exchanged, Only the user and the system need to be
verified no other authentication required. If any transaction carried out on a
block chain-based system would be settled in a matter of seconds while still
being safe, verifiable, and transparent. Although block-chain technology is the
foundation for Bitcoin and other digital currencies but also it may be applied
widely to greatly reduce difficulties in many other sectors, Voting is the
sector that is battling from a lack of security, centralized-authority,
management-issues, and many more despite the fact that transactions are kept in
a distributed and safe fashion.
</p></li>
</ul>

<h3>Title: Approaching Shannon's One-Time Pad: Metrics, Architectures, and Enabling Technologies. (arXiv:2303.06359v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06359">http://arxiv.org/abs/2303.06359</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06359] Approaching Shannon's One-Time Pad: Metrics, Architectures, and Enabling Technologies](http://arxiv.org/abs/2303.06359) #secure</code></li>
<li>Summary: <p>The rapid development of advanced computing technologies such as quantum
computing imposes new challenges to current wireless security mechanism which
is based on cryptographic approaches. To deal with various attacks and realize
long-lasting security, we are in urgent need of disruptive security solutions.
In this article, novel security transmission paradigms are proposed to approach
Shannon's one-time pad perfect secrecy. First, two metrics, termed as
Degree-of-Approaching (DoA) and Degree-of-Synchronous-Approaching (DoSA), are
developed to characterize the closeness between the achieved security strength
and perfect secrecy. These two metrics also serve as a guideline for secure
transmission protocol design. After that, we present two paths towards
Shannon's one-time pad, i.e., an explicit-encryption based approach and an
implicit-encryption based approach. For both of them, we discuss the
architecture design, enabling technologies, as well as preliminary performance
evaluation results. The techniques presented in this article provide promising
security-enhancing solutions for future wireless networks.
</p></li>
</ul>

<h2>security</h2>
<h2>privacy</h2>
<h3>Title: DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06171">http://arxiv.org/abs/2303.06171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06171] DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference](http://arxiv.org/abs/2303.06171) #privacy</code></li>
<li>Summary: <p>Bayesian inference provides a principled framework for learning from complex
data and reasoning under uncertainty. It has been widely applied in machine
learning tasks such as medical diagnosis, drug design, and policymaking. In
these common applications, the data can be highly sensitive. Differential
privacy (DP) offers data analysis tools with powerful worst-case privacy
guarantees and has been developed as the leading approach in privacy-preserving
data analysis. In this paper, we study Metropolis-Hastings (MH), one of the
most fundamental MCMC methods, for large-scale Bayesian inference under
differential privacy. While most existing private MCMC algorithms sacrifice
accuracy and efficiency to obtain privacy, we provide the first exact and fast
DP MH algorithm, using only a minibatch of data in most iterations. We further
reveal, for the first time, a three-way trade-off among privacy, scalability
(i.e. the batch size), and efficiency (i.e. the convergence rate),
theoretically characterizing how privacy affects the utility and computational
cost in Bayesian inference. We empirically demonstrate the effectiveness and
efficiency of our algorithm in various experiments.
</p></li>
</ul>

<h3>Title: Optimal and Private Learning from Human Response Data. (arXiv:2303.06234v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06234">http://arxiv.org/abs/2303.06234</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06234] Optimal and Private Learning from Human Response Data](http://arxiv.org/abs/2303.06234) #privacy</code></li>
<li>Summary: <p>Item response theory (IRT) is the study of how people make probabilistic
decisions, with diverse applications in education testing, recommendation
systems, among others. The Rasch model of binary response data, one of the most
fundamental models in IRT, remains an active area of research with important
practical significance. Recently, Nguyen and Zhang (2022) proposed a new
spectral estimation algorithm that is efficient and accurate. In this work, we
extend their results in two important ways. Firstly, we obtain a refined
entrywise error bound for the spectral algorithm, complementing the `average
error' $\ell_2$ bound in their work. Notably, under mild sampling conditions,
the spectral algorithm achieves the minimax optimal error bound (modulo a log
factor). Building on the refined analysis, we also show that the spectral
algorithm enjoys optimal sample complexity for top-$K$ recovery (e.g.,
identifying the best $K$ items from approval/disapproval response data),
explaining the empirical findings in the previous work. Our second contribution
addresses an important but understudied topic in IRT: privacy. Despite the
human-centric applications of IRT, there has not been any proposed
privacy-preserving mechanism in the literature. We develop a private extension
of the spectral algorithm, leveraging its unique Markov chain formulation and
the discrete Gaussian mechanism (Canonne et al., 2020). Experiments show that
our approach is significantly more accurate than the baselines in the
low-to-moderate privacy regime.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06280">http://arxiv.org/abs/2303.06280</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06280] Investigating Stateful Defenses Against Black-Box Adversarial Examples](http://arxiv.org/abs/2303.06280) #defense</code></li>
<li>Summary: <p>Defending machine-learning (ML) models against white-box adversarial attacks
has proven to be extremely difficult. Instead, recent work has proposed
stateful defenses in an attempt to defend against a more restricted black-box
attacker. These defenses operate by tracking a history of incoming model
queries, and rejecting those that are suspiciously similar. The current
state-of-the-art stateful defense Blacklight was proposed at USENIX Security
'22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and
ImageNet datasets. In this paper, we observe that an attacker can significantly
reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to
6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box
attack. Motivated by this surprising observation, since existing attacks were
evaluated by the Blacklight authors, we provide a systematization of stateful
defenses to understand why existing stateful defense models fail. Finally, we
propose a stronger evaluation strategy for stateful defenses comprised of
adaptive score and hard-label based black-box attacks. We use these attacks to
successfully reduce even reconfigured versions of Blacklight to as low as 0%
robust accuracy.
</p></li>
</ul>

<h3>Title: SHIELD: An Adaptive and Lightweight Defense against the Remote Power Side-Channel Attacks on Multi-tenant FPGAs. (arXiv:2303.06486v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06486">http://arxiv.org/abs/2303.06486</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06486] SHIELD: An Adaptive and Lightweight Defense against the Remote Power Side-Channel Attacks on Multi-tenant FPGAs](http://arxiv.org/abs/2303.06486) #defense</code></li>
<li>Summary: <p>Dynamic partial reconfiguration enables multi-tenancy in cloud-based FPGAs,
which presents security challenges for tenants, IPs, and data. Malicious users
can exploit FPGAs for remote side-channel attacks (SCAs), and shared on-chip
resources can be used for attacks. Logical separation can ensure design
integrity, but on-chip resources can still be exploited. Conventional SCA
mitigation can help, but it requires significant effort, and bitstream checking
techniques are not highly accurate. An active on-chip defense mechanism is
needed for tenant confidentiality. Toward this, we propose a lightweight
shielding technique utilizing ring oscillators (ROs) to protect applications
against remote power SCA. Unlike existing RO-based approaches, in our
methodology, an offline pre-processing stage is proposed to carefully configure
power monitors and an obfuscating circuit concerning the resource constraints
of the board. Detection of power fluctuations due to application execution
enables the obfuscating circuit to flatten the power consumption trace. To
evaluate the effectiveness of the proposed SHIELD, we implemented it on a
Xilinx Zynq-7000 FPGA board executing an RSA encryption algorithm. Due to the
SHIELD, the number of traces required to extract the encryption key is
increased by 166x, making an attack extremely hard at run-time. Note that the
proposed SHIELD does not require any modification in the target application.
Our methodology also shows up to 54% less power consumption and up to 26% less
area overhead than the state-of-the-art random noise-addition-based defense.
</p></li>
</ul>

<h3>Title: Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06302">http://arxiv.org/abs/2303.06302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06302] Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey](http://arxiv.org/abs/2303.06302) #defense</code></li>
<li>Summary: <p>Adversarial attacks and defenses in machine learning and deep neural network
have been gaining significant attention due to the rapidly growing applications
of deep learning in the Internet and relevant scenarios. This survey provides a
comprehensive overview of the recent advancements in the field of adversarial
attack and defense techniques, with a focus on deep neural network-based
classification models. Specifically, we conduct a comprehensive classification
of recent adversarial attack methods and state-of-the-art adversarial defense
techniques based on attack principles, and present them in visually appealing
tables and tree diagrams. This is based on a rigorous evaluation of the
existing works, including an analysis of their strengths and limitations. We
also categorize the methods into counter-attack detection and robustness
enhancement, with a specific focus on regularization-based methods for
enhancing robustness. New avenues of attack are also explored, including
search-based, decision-based, drop-based, and physical-world attacks, and a
hierarchical classification of the latest defense methods is provided,
highlighting the challenges of balancing training costs with performance,
maintaining clean accuracy, overcoming the effect of gradient masking, and
ensuring method transferability. At last, the lessons learned and open
challenges are summarized with future research opportunities recommended.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06241">http://arxiv.org/abs/2303.06241</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06241] Do we need entire training data for adversarial training?](http://arxiv.org/abs/2303.06241) #attack</code></li>
<li>Summary: <p>Deep Neural Networks (DNNs) are being used to solve a wide range of problems
in many domains including safety-critical domains like self-driving cars and
medical imagery. DNNs suffer from vulnerability against adversarial attacks. In
the past few years, numerous approaches have been proposed to tackle this
problem by training networks using adversarial training. Almost all the
approaches generate adversarial examples for the entire training dataset, thus
increasing the training time drastically. We show that we can decrease the
training time for any adversarial training algorithm by using only a subset of
training data for adversarial training. To select the subset, we filter the
adversarially-prone samples from the training data. We perform a simple
adversarial attack on all training examples to filter this subset. In this
attack, we add a small perturbation to each pixel and a few grid lines to the
input image.
</p></li>
</ul>

<p>We perform adversarial training on the adversarially-prone subset and mix it
with vanilla training performed on the entire dataset. Our results show that
when our method-agnostic approach is plugged into FGSM, we achieve a speedup of
3.52x on MNIST and 1.98x on the CIFAR-10 dataset with comparable robust
accuracy. We also test our approach on state-of-the-art Free adversarial
training and achieve a speedup of 1.2x in training time with a marginal drop in
robust accuracy on the ImageNet dataset.
</p>

<h3>Title: Hallucinated Heartbeats: Anomaly-Aware Remote Pulse Estimation. (arXiv:2303.06452v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06452">http://arxiv.org/abs/2303.06452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06452] Hallucinated Heartbeats: Anomaly-Aware Remote Pulse Estimation](http://arxiv.org/abs/2303.06452) #attack</code></li>
<li>Summary: <p>Camera-based physiological monitoring, especially remote photoplethysmography
(rPPG), is a promising tool for health diagnostics, and state-of-the-art pulse
estimators have shown impressive performance on benchmark datasets. We argue
that evaluations of modern solutions may be incomplete, as we uncover failure
cases for videos without a live person, or in the presence of severe noise. We
demonstrate that spatiotemporal deep learning models trained only with live
samples "hallucinate" a genuine-shaped pulse on anomalous and noisy videos,
which may have negative consequences when rPPG models are used by medical
personnel. To address this, we offer: (a) An anomaly detection model, built on
top of the predicted waveforms. We compare models trained in open-set (unknown
abnormal predictions) and closed-set (abnormal predictions known when training)
settings; (b) An anomaly-aware training regime that penalizes the model for
predicting periodic signals from anomalous videos. Extensive experimentation
with eight research datasets (rPPG-specific: DDPM, CDDPM, PURE, UBFC, ARPM;
deep fakes: DFDC; face presentation attack detection: HKBU-MARs; rPPG outlier:
KITTI) show better accuracy of anomaly detection for deep learning models
incorporating the proposed training (75.8%), compared to models trained
regularly (73.7%) and to hand-crafted rPPG methods (52-62%).
</p></li>
</ul>

<h3>Title: Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks. (arXiv:2303.06199v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06199">http://arxiv.org/abs/2303.06199</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06199] Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks](http://arxiv.org/abs/2303.06199) #attack</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have achieved state-of-the-art performance in
many graph learning tasks. However, recent studies show that GNNs are
vulnerable to both test-time evasion and training-time poisoning attacks that
perturb the graph structure. While existing attack methods have shown promising
attack performance, we would like to design an attack framework to further
enhance the performance. In particular, our attack framework is inspired by
certified robustness, which was originally used by defenders to defend against
adversarial attacks. We are the first, from the attacker perspective, to
leverage its properties to better attack GNNs. Specifically, we first derive
nodes' certified perturbation sizes against graph evasion and poisoning attacks
based on randomized smoothing, respectively. A larger certified perturbation
size of a node indicates this node is theoretically more robust to graph
perturbations. Such a property motivates us to focus more on nodes with smaller
certified perturbation sizes, as they are easier to be attacked after graph
perturbations. Accordingly, we design a certified robustness inspired attack
loss, when incorporated into (any) existing attacks, produces our certified
robustness inspired attack counterpart. We apply our framework to the existing
attacks and results show it can significantly enhance the existing base
attacks' performance.
</p></li>
</ul>

<h3>Title: Cloud Forensic: Issues, Challenges and Solution Models. (arXiv:2303.06313v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06313">http://arxiv.org/abs/2303.06313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06313] Cloud Forensic: Issues, Challenges and Solution Models](http://arxiv.org/abs/2303.06313) #attack</code></li>
<li>Summary: <p>Cloud computing is a web-based utility model that is becoming popular every
day with the emergence of 4th Industrial Revolution, therefore, cybercrimes
that affect web-based systems are also relevant to cloud computing. In order to
conduct a forensic investigation into a cyber-attack, it is necessary to
identify and locate the source of the attack as soon as possible. Although
significant study has been done in this domain on obstacles and its solutions,
research on approaches and strategies is still in its development stage. There
are barriers at every stage of cloud forensics, therefore, before we can come
up with a comprehensive way to deal with these problems, we must first
comprehend the cloud technology and its forensics environment. Although there
are articles that are linked to cloud forensics, there is not yet a paper that
accumulated the contemporary concerns and solutions related to cloud forensic.
Throughout this chapter, we have looked at the cloud environment, as well as
the threats and attacks that it may be subjected to. We have also looked at the
approaches that cloud forensics may take, as well as the various frameworks and
the practical challenges and limitations they may face when dealing with cloud
forensic investigations.
</p></li>
</ul>

<h3>Title: Detection of DDoS Attacks in Software Defined Networking Using Machine Learning Models. (arXiv:2303.06513v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06513">http://arxiv.org/abs/2303.06513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06513] Detection of DDoS Attacks in Software Defined Networking Using Machine Learning Models](http://arxiv.org/abs/2303.06513) #attack</code></li>
<li>Summary: <p>The concept of Software Defined Networking (SDN) represents a modern approach
to networking that separates the control plane from the data plane through
network abstraction, resulting in a flexible, programmable and dynamic
architecture compared to traditional networks. The separation of control and
data planes has led to a high degree of network resilience, but has also given
rise to new security risks, including the threat of distributed
denial-of-service (DDoS) attacks, which pose a new challenge in the SDN
environment. In this paper, the effectiveness of using machine learning
algorithms to detect distributed denial-of-service (DDoS) attacks in
software-defined networking (SDN) environments is investigated. Four
algorithms, including Random Forest, Decision Tree, Support Vector Machine, and
XGBoost, were tested on the CICDDoS2019 dataset, with the timestamp feature
dropped among others. Performance was assessed by measures of accuracy, recall,
accuracy, and F1 score, with the Random Forest algorithm having the highest
accuracy, at 68.9%. The results indicate that ML-based detection is a more
accurate and effective method for identifying DDoS attacks in SDN, despite the
computational requirements of non-parametric algorithms.
</p></li>
</ul>

<h3>Title: NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06151">http://arxiv.org/abs/2303.06151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06151] NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks](http://arxiv.org/abs/2303.06151) #attack</code></li>
<li>Summary: <p>Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various
domains. However, adversarial attacks can easily mislead a neural network and
lead to wrong decisions. Defense mechanisms are highly preferred in
safety-critical applications. In this paper, firstly, we use the gradient class
activation map (GradCAM) to analyze the behavior deviation of the VGG-16
network when its inputs are mixed with adversarial perturbation or Gaussian
noise. In particular, our method can locate vulnerable layers that are
sensitive to adversarial perturbation and Gaussian noise. We also show that the
behavior deviation of vulnerable layers can be used to detect adversarial
examples. Secondly, we propose a novel NoiseCAM algorithm that integrates
information from globally and pixel-level weighted class activation maps. Our
algorithm is susceptible to adversarial perturbations and will not respond to
Gaussian random noise mixed in the inputs. Third, we compare detecting
adversarial examples using both behavior deviation and NoiseCAM, and we show
that NoiseCAM outperforms behavior deviation modeling in its overall
performance. Our work could provide a useful tool to defend against certain
adversarial attacks on deep neural networks.
</p></li>
</ul>

<h3>Title: Anomaly Detection with Ensemble of Encoder and Decoder. (arXiv:2303.06431v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06431">http://arxiv.org/abs/2303.06431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06431] Anomaly Detection with Ensemble of Encoder and Decoder](http://arxiv.org/abs/2303.06431) #attack</code></li>
<li>Summary: <p>Hacking and false data injection from adversaries can threaten power grids'
everyday operations and cause significant economic loss. Anomaly detection in
power grids aims to detect and discriminate anomalies caused by cyber attacks
against the power system, which is essential for keeping power grids working
correctly and efficiently. Different methods have been applied for anomaly
detection, such as statistical methods and machine learning-based methods.
Usually, machine learning-based methods need to model the normal data
distribution. In this work, we propose a novel anomaly detection method by
modeling the data distribution of normal samples via multiple encoders and
decoders. Specifically, the proposed method maps input samples into a latent
space and then reconstructs output samples from latent vectors. The extra
encoder finally maps reconstructed samples to latent representations. During
the training phase, we optimize parameters by minimizing the reconstruction
loss and encoding loss. Training samples are re-weighted to focus more on
missed correlations between features of normal data. Furthermore, we employ the
long short-term memory model as encoders and decoders to test its
effectiveness. We also investigate a meta-learning-based framework for
hyper-parameter tuning of our approach. Experiment results on network intrusion
and power system datasets demonstrate the effectiveness of our proposed method,
where our models consistently outperform all baselines.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: A New Super-Resolution Measurement of Perceptual Quality and Fidelity. (arXiv:2303.06207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06207">http://arxiv.org/abs/2303.06207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06207] A New Super-Resolution Measurement of Perceptual Quality and Fidelity](http://arxiv.org/abs/2303.06207) #robust</code></li>
<li>Summary: <p>Super-resolution results are usually measured by full-reference image quality
metrics or human rating scores. However, these evaluation methods are general
image quality measurement, and do not account for the nature of the
super-resolution problem. In this work, we analyze the evaluation problem based
on the one-to-many mapping nature of super-resolution, and propose a novel
distribution-based metric for super-resolution. Starting from the distribution
distance, we derive the proposed metric to make it accessible and easy to
compute. Through a human subject study on super-resolution, we show that the
proposed metric is highly correlated with the human perceptual quality, and
better than most existing metrics. Moreover, the proposed metric has a higher
correlation with the fidelity measure compared to the perception-based metrics.
To understand the properties of the proposed metric, we conduct extensive
evaluation in terms of its design choices, and show that the metric is robust
to its design choices. Finally, we show that the metric can be used to train
super-resolution networks for better perceptual quality.
</p></li>
</ul>

<h3>Title: Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06296">http://arxiv.org/abs/2303.06296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06296] Stabilizing Transformer Training by Preventing Attention Entropy Collapse](http://arxiv.org/abs/2303.06296) #robust</code></li>
<li>Summary: <p>Training stability is of great importance to Transformers. In this work, we
investigate the training dynamics of Transformers by examining the evolution of
the attention layers. In particular, we track the attention entropy for each
attention head during the course of training, which is a proxy for model
sharpness. We identify a common pattern across different architectures and
tasks, where low attention entropy is accompanied by high training instability,
which can take the form of oscillating loss or divergence. We denote the
pathologically low attention entropy, corresponding to highly concentrated
attention scores, as $\textit{entropy collapse}$. As a remedy, we propose
$\sigma$Reparam, a simple and efficient solution where we reparametrize all
linear layers with spectral normalization and an additional learned scalar. We
demonstrate that the proposed reparameterization successfully prevents entropy
collapse in the attention layers, promoting more stable training. Additionally,
we prove a tight lower bound of the attention entropy, which decreases
exponentially fast with the spectral norm of the attention logits, providing
additional motivation for our approach. We conduct experiments with
$\sigma$Reparam on image classification, image self-supervised learning,
machine translation, automatic speech recognition, and language modeling tasks,
across Transformer architectures. We show that $\sigma$Reparam provides
stability and robustness with respect to the choice of hyperparameters, going
so far as enabling training (a) a Vision Transformer to competitive performance
without warmup, weight decay, layer normalization or adaptive optimizers; (b)
deep architectures in machine translation and (c) speech recognition to
competitive performance without warmup and adaptive optimizers.
</p></li>
</ul>

<h3>Title: PRSNet: A Masked Self-Supervised Learning Pedestrian Re-Identification Method. (arXiv:2303.06330v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06330">http://arxiv.org/abs/2303.06330</a></li>
<li>Code URL: <a href="https://github.com/zjiex/prsnet">https://github.com/zjiex/prsnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06330] PRSNet: A Masked Self-Supervised Learning Pedestrian Re-Identification Method](http://arxiv.org/abs/2303.06330) #robust</code></li>
<li>Summary: <p>In recent years, self-supervised learning has attracted widespread academic
debate and addressed many of the key issues of computer vision. The present
research focus is on how to construct a good agent task that allows for
improved network learning of advanced semantic information on images so that
model reasoning is accelerated during pre-training of the current task. In
order to solve the problem that existing feature extraction networks are
pre-trained on the ImageNet dataset and cannot extract the fine-grained
information in pedestrian images well, and the existing pre-task of contrast
self-supervised learning may destroy the original properties of pedestrian
images, this paper designs a pre-task of mask reconstruction to obtain a
pre-training model with strong robustness and uses it for the pedestrian
re-identification task. The training optimization of the network is performed
by improving the triplet loss based on the centroid, and the mask image is
added as an additional sample to the loss calculation, so that the network can
better cope with the pedestrian matching in practical applications after the
training is completed. This method achieves about 5% higher mAP on Marker1501
and CUHK03 data than existing self-supervised learning pedestrian
re-identification methods, and about 1% higher for Rank1, and ablation
experiments are conducted to demonstrate the feasibility of this method. Our
model code is located at https://github.com/ZJieX/prsnet.
</p></li>
</ul>

<h3>Title: Enhanced K-Radar: Optimal Density Reduction to Improve Detection Performance and Accessibility of 4D Radar Tensor-based Object Detection. (arXiv:2303.06342v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06342">http://arxiv.org/abs/2303.06342</a></li>
<li>Code URL: <a href="https://github.com/kaist-avelab/k-radar">https://github.com/kaist-avelab/k-radar</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06342] Enhanced K-Radar: Optimal Density Reduction to Improve Detection Performance and Accessibility of 4D Radar Tensor-based Object Detection](http://arxiv.org/abs/2303.06342) #robust</code></li>
<li>Summary: <p>Recent works have shown the superior robustness of four-dimensional (4D)
Radar-based three-dimensional (3D) object detection in adverse weather
conditions. However, processing 4D Radar data remains a challenge due to the
large data size, which require substantial amount of memory for computing and
storage. In previous work, an online density reduction is performed on the 4D
Radar Tensor (4DRT) to reduce the data size, in which the density reduction
level is chosen arbitrarily. However, the impact of density reduction on the
detection performance and memory consumption remains largely unknown. In this
paper, we aim to address this issue by conducting extensive hyperparamter
tuning on the density reduction level. Experimental results show that
increasing the density level from 0.01% to 50% of the original 4DRT density
level proportionally improves the detection performance, at a cost of memory
consumption. However, when the density level is increased beyond 5%, only the
memory consumption increases, while the detection performance oscillates below
the peak point. In addition to the optimized density hyperparameter, we also
introduce 4D Sparse Radar Tensor (4DSRT), a new representation for 4D Radar
data with offline density reduction, leading to a significantly reduced raw
data size. An optimized development kit for training the neural networks is
also provided, which along with the utilization of 4DSRT, improves training
speed by a factor of 17.1 compared to the state-of-the-art 4DRT-based neural
networks. All codes are available at: https://github.com/kaist-avelab/K-Radar.
</p></li>
</ul>

<h3>Title: Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination. (arXiv:2303.06380v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06380">http://arxiv.org/abs/2303.06380</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06380] Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination](http://arxiv.org/abs/2303.06380) #robust</code></li>
<li>Summary: <p>Enormous hand images with reliable annotations are collected through
marker-based MoCap. Unfortunately, degradations caused by markers limit their
application in hand appearance reconstruction. A clear appearance recovery
insight is an image-to-image translation trained with unpaired data. However,
most frameworks fail because there exists structure inconsistency from a
degraded hand to a bare one. The core of our approach is to first disentangle
the bare hand structure from those degraded images and then wrap the appearance
to this structure with a dual adversarial discrimination (DAD) scheme. Both
modules take full advantage of the semi-supervised learning paradigm: The
structure disentanglement benefits from the modeling ability of ViT, and the
translator is enhanced by the dual discrimination on both translation processes
and translation results. Comprehensive evaluations have been conducted to prove
that our framework can robustly recover photo-realistic hand appearance from
diverse marker-contained and even object-occluded datasets. It provides a novel
avenue to acquire bare hand appearance data for other downstream learning
problems.The codes will be publicly available at https://www.yangangwang.com
</p></li>
</ul>

<h3>Title: Improving the Robustness of Deep Convolutional Neural Networks Through Feature Learning. (arXiv:2303.06425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06425">http://arxiv.org/abs/2303.06425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06425] Improving the Robustness of Deep Convolutional Neural Networks Through Feature Learning](http://arxiv.org/abs/2303.06425) #robust</code></li>
<li>Summary: <p>Deep convolutional neural network (DCNN for short) models are vulnerable to
examples with small perturbations. Adversarial training (AT for short) is a
widely used approach to enhance the robustness of DCNN models by data
augmentation. In AT, the DCNN models are trained with clean examples and
adversarial examples (AE for short) which are generated using a specific attack
method, aiming to gain ability to defend themselves when facing the unseen AEs.
However, in practice, the trained DCNN models are often fooled by the AEs
generated by the novel attack methods. This naturally raises a question: can a
DCNN model learn certain features which are insensitive to small perturbations,
and further defend itself no matter what attack methods are presented. To
answer this question, this paper makes a beginning effort by proposing a
shallow binary feature module (SBFM for short), which can be integrated into
any popular backbone. The SBFM includes two types of layers, i.e., Sobel layer
and threshold layer. In Sobel layer, there are four parallel feature maps which
represent horizontal, vertical, and diagonal edge features, respectively. And
in threshold layer, it turns the edge features learnt by Sobel layer to the
binary features, which then are feeded into the fully connected layers for
classification with the features learnt by the backbone. We integrate SBFM into
VGG16 and ResNet34, respectively, and conduct experiments on multiple datasets.
Experimental results demonstrate, under FGSM attack with $\epsilon=8/255$, the
SBFM integrated models can achieve averagely 35\% higher accuracy than the
original ones, and in CIFAR-10 and TinyImageNet datasets, the SBFM integrated
models can achieve averagely 75\% classification accuracy. The work in this
paper shows it is promising to enhance the robustness of DCNN models through
feature learning.
</p></li>
</ul>

<h3>Title: Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06484">http://arxiv.org/abs/2303.06484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06484] Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap](http://arxiv.org/abs/2303.06484) #robust</code></li>
<li>Summary: <p>The neural collapse (NC) phenomenon describes an underlying geometric
symmetry for deep neural networks, where both deeply learned features and
classifiers converge to a simplex equiangular tight frame. It has been shown
that both cross-entropy loss and mean square error can provably lead to NC. We
remove NC's key assumption on the feature dimension and the number of classes,
and then present a generalized neural collapse (GNC) hypothesis that
effectively subsumes the original NC. Inspired by how NC characterizes the
training target of neural networks, we decouple GNC into two objectives:
minimal intra-class variability and maximal inter-class separability. We then
use hyperspherical uniformity (which characterizes the degree of uniformity on
the unit hypersphere) as a unified framework to quantify these two objectives.
Finally, we propose a general objective -- hyperspherical uniformity gap (HUG),
which is defined by the difference between inter-class and intra-class
hyperspherical uniformity. HUG not only provably converges to GNC, but also
decouples GNC into two separate objectives. Unlike cross-entropy loss that
couples intra-class compactness and inter-class separability, HUG enjoys more
flexibility and serves as a good alternative loss function. Empirical results
show that HUG works well in terms of generalization and robustness.
</p></li>
</ul>

<h3>Title: Robust Learning from Explanations. (arXiv:2303.06419v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06419">http://arxiv.org/abs/2303.06419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06419] Robust Learning from Explanations](http://arxiv.org/abs/2303.06419) #robust</code></li>
<li>Summary: <p>Machine learning from explanations (MLX) is an approach to learning that uses
human-provided annotations of relevant features for each input to ensure that
model predictions are right for the right reasons. Existing MLX approaches rely
heavily on a specific model interpretation approach and require strong
parameter regularization to align model and human explanations, leading to
sub-optimal performance. We recast MLX as an adversarial robustness problem,
where human explanations specify a lower dimensional manifold from which
perturbations can be drawn, and show both theoretically and empirically how
this approach alleviates the need for strong parameter regularization. We
consider various approaches to achieving robustness, leading to improved
performance over prior MLX methods. Finally, we combine robustness with an
earlier MLX method, yielding state-of-the-art results on both synthetic and
real-world benchmarks.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06180">http://arxiv.org/abs/2303.06180</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06180] Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels](http://arxiv.org/abs/2303.06180) #federate</code></li>
<li>Summary: <p>Numerous large-scale chest x-ray datasets have spearheaded expert-level
detection of abnormalities using deep learning. However, these datasets focus
on detecting a subset of disease labels that could be present, thus making them
distributed and non-iid with partial labels. Recent literature has indicated
the impact of batch normalization layers on the convergence of federated
learning due to domain shift associated with non-iid data with partial labels.
To that end, we propose FedFBN, a federated learning framework that draws
inspiration from transfer learning by using pretrained networks as the model
backend and freezing the batch normalization layers throughout the training
process. We evaluate FedFBN with current FL strategies using synthetic iid toy
datasets and large-scale non-iid datasets across scenarios with partial and
complete labels. Our results demonstrate that FedFBN outperforms current
aggregation strategies for training global models using distributed and non-iid
data with partial labels.
</p></li>
</ul>

<h3>Title: Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06155">http://arxiv.org/abs/2303.06155</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06155] Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning](http://arxiv.org/abs/2303.06155) #federate</code></li>
<li>Summary: <p>In this paper, to deal with the heterogeneity in federated learning (FL)
systems, a knowledge distillation (KD) driven training framework for FL is
proposed, where each user can select its neural network model on demand and
distill knowledge from a big teacher model using its own private dataset. To
overcome the challenge of train the big teacher model in resource limited user
devices, the digital twin (DT) is exploit in the way that the teacher model can
be trained at DT located in the server with enough computing resources. Then,
during model distillation, each user can update the parameters of its model at
either the physical entity or the digital agent. The joint problem of model
selection and training offloading and resource allocation for users is
formulated as a mixed integer programming (MIP) problem. To solve the problem,
Q-learning and optimization are jointly used, where Q-learning selects models
for users and determines whether to train locally or on the server, and
optimization is used to allocate resources for users based on the output of
Q-learning. Simulation results show the proposed DT-assisted KD framework and
joint optimization method can significantly improve the average accuracy of
users while reducing the total delay.
</p></li>
</ul>

<h3>Title: Papaya: Federated Learning, but Fully Decentralized. (arXiv:2303.06189v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06189">http://arxiv.org/abs/2303.06189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06189] Papaya: Federated Learning, but Fully Decentralized](http://arxiv.org/abs/2303.06189) #federate</code></li>
<li>Summary: <p>Federated Learning systems use a centralized server to aggregate model
updates. This is a bandwidth and resource-heavy constraint and exposes the
system to privacy concerns. We instead implement a peer to peer learning system
in which nodes train on their own data and periodically perform a weighted
average of their parameters with that of their peers according to a learned
trust matrix. So far, we have created a model client framework and have been
using this to run experiments on the proposed system using multiple virtual
nodes which in reality exist on the same computer. We used this strategy as
stated in Iteration 1 of our proposal to prove the concept of peer to peer
learning with shared parameters. We now hope to run more experiments and build
a more deployable real world system for the same.
</p></li>
</ul>

<h3>Title: Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06237">http://arxiv.org/abs/2303.06237</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06237] Complement Sparsification: Low-Overhead Model Pruning for Federated Learning](http://arxiv.org/abs/2303.06237) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) is a privacy-preserving distributed deep learning
paradigm that involves substantial communication and computation effort, which
is a problem for resource-constrained mobile and IoT devices. Model
pruning/sparsification develops sparse models that could solve this problem,
but existing sparsification solutions cannot satisfy at the same time the
requirements for low bidirectional communication overhead between the server
and the clients, low computation overhead at the clients, and good model
accuracy, under the FL assumption that the server does not have access to raw
data to fine-tune the pruned models. We propose Complement Sparsification (CS),
a pruning mechanism that satisfies all these requirements through a
complementary and collaborative pruning done at the server and the clients. At
each round, CS creates a global sparse model that contains the weights that
capture the general data distribution of all clients, while the clients create
local sparse models with the weights pruned from the global model to capture
the local trends. For improved model performance, these two types of
complementary sparse models are aggregated into a dense model in each round,
which is subsequently pruned in an iterative process. CS requires little
computation overhead on the top of vanilla FL for both the server and the
clients. We demonstrate that CS is an approximation of vanilla FL and, thus,
its models perform well. We evaluate CS experimentally with two popular FL
benchmark datasets. CS achieves substantial reduction in bidirectional
communication, while achieving performance comparable with vanilla FL. In
addition, CS outperforms baseline pruning mechanisms for FL.
</p></li>
</ul>

<h3>Title: Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06246">http://arxiv.org/abs/2303.06246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06246] Zone-based Federated Learning for Mobile Sensing Data](http://arxiv.org/abs/2303.06246) #federate</code></li>
<li>Summary: <p>Mobile apps, such as mHealth and wellness applications, can benefit from deep
learning (DL) models trained with mobile sensing data collected by smart phones
or wearable devices. However, currently there is no mobile sensing DL system
that simultaneously achieves good model accuracy while adapting to user
mobility behavior, scales well as the number of users increases, and protects
user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address
these requirements. ZoneFL divides the physical space into geographical zones
mapped to a mobile-edge-cloud system architecture for good model accuracy and
scalability. Each zone has a federated training model, called a zone model,
which adapts well to data and behaviors of users in that zone. Benefiting from
the FL design, the user data privacy is protected during the ZoneFL training.
We propose two novel zone-based federated training algorithms to optimize zone
models to user mobility behavior: Zone Merge and Split (ZMS) and Zone Gradient
Diffusion (ZGD). ZMS optimizes zone models by adapting the zone geographical
partitions through merging of neighboring zones or splitting of large zones
into smaller ones. Different from ZMS, ZGD maintains fixed zones and optimizes
a zone model by incorporating the gradients derived from neighboring zones'
data. ZGD uses a self-attention mechanism to dynamically control the impact of
one zone on its neighbors. Extensive analysis and experimental results
demonstrate that ZoneFL significantly outperforms traditional FL in two models
for heart rate prediction and human activity recognition. In addition, we
developed a ZoneFL system using Android phones and AWS cloud. The system was
used in a heart rate prediction field study with 63 users for 4 months, and we
demonstrated the feasibility of ZoneFL in real-life.
</p></li>
</ul>

<h3>Title: Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06314">http://arxiv.org/abs/2303.06314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06314] Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems](http://arxiv.org/abs/2303.06314) #federate</code></li>
<li>Summary: <p>Federated learning is an emerging technique for training deep models over
decentralized clients without exposing private data, which however suffers from
label distribution skew and usually results in slow convergence and degraded
model performance. This challenge could be more serious when the participating
clients are in unstable circumstances and dropout frequently. Previous work and
our empirical observations demonstrate that the classifier head for
classification task is more sensitive to label skew and the unstable
performance of FedAvg mainly lies in the imbalanced training samples across
different classes. The biased classifier head will also impact the learning of
feature representations. Therefore, maintaining a balanced classifier head is
of significant importance for building a better global model. To tackle this
issue, we propose a simple yet effective framework by introducing a
prior-calibrated softmax function for computing the cross-entropy loss and a
prototype-based feature augmentation scheme to re-balance the local training,
which are lightweight for edge devices and can facilitate the global model
aggregation. With extensive experiments performed on FashionMNIST and CIFAR-10
datasets, we demonstrate the improved model performance of our method over
existing baselines in the presence of non-IID data and client dropout.
</p></li>
</ul>

<h3>Title: FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06360">http://arxiv.org/abs/2303.06360</a></li>
<li>Code URL: <a href="https://github.com/zhuzzq/fedlp">https://github.com/zhuzzq/fedlp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06360] FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning](http://arxiv.org/abs/2303.06360) #federate</code></li>
<li>Summary: <p>Federated learning (FL) has prevailed as an efficient and privacy-preserved
scheme for distributed learning. In this work, we mainly focus on the
optimization of computation and communication in FL from a view of pruning. By
adopting layer-wise pruning in local training and federated updating, we
formulate an explicit FL pruning framework, FedLP (Federated Layer-wise
Pruning), which is model-agnostic and universal for different types of deep
learning models. Two specific schemes of FedLP are designed for scenarios with
homogeneous local models and heterogeneous ones. Both theoretical and
experimental evaluations are developed to verify that FedLP relieves the system
bottlenecks of communication and computation with marginal performance decay.
To the best of our knowledge, FedLP is the first framework that formally
introduces the layer-wise pruning into FL. Within the scope of federated
learning, more variants and combinations can be further designed based on
FedLP.
</p></li>
</ul>

<h3>Title: Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06530">http://arxiv.org/abs/2303.06530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06530] Making Batch Normalization Great in Federated Deep Learning](http://arxiv.org/abs/2303.06530) #federate</code></li>
<li>Summary: <p>Batch Normalization (BN) is commonly used in modern deep neural networks
(DNNs) to improve stability and speed up convergence during centralized
training. In federated learning (FL) with non-IID decentralized data, previous
works observed that training with BN could hinder performance due to the
mismatch of the BN statistics between training and testing. Group Normalization
(GN) is thus more often used in FL as an alternative to BN. However, from our
empirical study across various FL settings, we see no consistent winner between
BN and GN. This leads us to revisit the use of normalization layers in FL. We
find that with proper treatments, BN can be highly competitive across a wide
range of FL settings, and this requires no additional training or communication
costs. We hope that our study could serve as a valuable reference for future
practical usage and theoretical analysis in FL.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: No-regret Algorithms for Fair Resource Allocation. (arXiv:2303.06396v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06396">http://arxiv.org/abs/2303.06396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06396] No-regret Algorithms for Fair Resource Allocation](http://arxiv.org/abs/2303.06396) #fair</code></li>
<li>Summary: <p>We consider a fair resource allocation problem in the no-regret setting
against an unrestricted adversary. The objective is to allocate resources
equitably among several agents in an online fashion so that the difference of
the aggregate $\alpha$-fair utilities of the agents between an optimal static
clairvoyant allocation and that of the online policy grows sub-linearly with
time. The problem is challenging due to the non-additive nature of the
$\alpha$-fairness function. Previously, it was shown that no online policy can
exist for this problem with a sublinear standard regret. In this paper, we
propose an efficient online resource allocation policy, called Online
Proportional Fair (OPF), that achieves $c_\alpha$-approximate sublinear regret
with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$
for $0\leq \alpha < 1$. The upper bound to the $c_\alpha$-regret for this
problem exhibits a surprising phase transition phenomenon. The regret bound
changes from a power-law to a constant at the critical exponent
$\alpha=\frac{1}{2}.$ As a corollary, our result also resolves an open problem
raised by Even-Dar et al. [2009] on designing an efficient no-regret policy for
the online job scheduling problem in certain parameter regimes. The proof of
our results introduces new algorithmic and analytical techniques, including
greedy estimation of the future gradients for non-additive global reward
functions and bootstrapping adaptive regret bounds, which may be of independent
interest.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Interpretable Joint Event-Particle Reconstruction for Neutrino Physics at NOvA with Sparse CNNs and Transformers. (arXiv:2303.06201v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06201">http://arxiv.org/abs/2303.06201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06201] Interpretable Joint Event-Particle Reconstruction for Neutrino Physics at NOvA with Sparse CNNs and Transformers](http://arxiv.org/abs/2303.06201) #interpretability</code></li>
<li>Summary: <p>The complex events observed at the NOvA long-baseline neutrino oscillation
experiment contain vital information for understanding the most elusive
particles in the standard model. The NOvA detectors observe interactions of
neutrinos from the NuMI beam at Fermilab. Associating the particles produced in
these interaction events to their source particles, a process known as
reconstruction, is critical for accurately measuring key parameters of the
standard model. Events may contain several particles, each producing sparse
high-dimensional spatial observations, and current methods are limited to
evaluating individual particles. To accurately label these numerous,
high-dimensional observations, we present a novel neural network architecture
that combines the spatial learning enabled by convolutions with the contextual
learning enabled by attention. This joint approach, TransformerCVN,
simultaneously classifies each event and reconstructs every individual
particle's identity. TransformerCVN classifies events with 90\% accuracy and
improves the reconstruction of individual particles by 6\% over baseline
methods which lack the integrated architecture of TransformerCVN. In addition,
this architecture enables us to perform several interpretability studies which
provide insights into the network's predictions and show that TransformerCVN
discovers several fundamental principles that stem from the standard model.
</p></li>
</ul>

<h3>Title: Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06261">http://arxiv.org/abs/2303.06261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06261] Interpretable Outlier Summarization](http://arxiv.org/abs/2303.06261) #interpretability</code></li>
<li>Summary: <p>Outlier detection is critical in real applications to prevent financial
fraud, defend network intrusions, or detecting imminent device failures. To
reduce the human effort in evaluating outlier detection results and effectively
turn the outliers into actionable insights, the users often expect a system to
automatically produce interpretable summarizations of subgroups of outlier
detection results. Unfortunately, to date no such systems exist. To fill this
gap, we propose STAIR which learns a compact set of human understandable rules
to summarize and explain the anomaly detection results. Rather than use the
classical decision tree algorithms to produce these rules, STAIR proposes a new
optimization objective to produce a small number of rules with least
complexity, hence strong interpretability, to accurately summarize the
detection results. The learning algorithm of STAIR produces a rule set by
iteratively splitting the large rules and is optimal in maximizing this
objective in each iteration. Moreover, to effectively handle high dimensional,
highly complex data sets which are hard to summarize with simple rules, we
propose a localized STAIR approach, called L-STAIR. Taking data locality into
consideration, it simultaneously partitions data and learns a set of localized
rules for each partition. Our experimental study on many outlier benchmark
datasets shows that STAIR significantly reduces the complexity of the rules
required to summarize the outlier detection results, thus more amenable for
humans to understand and evaluate, compared to the decision tree methods.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image. (arXiv:2303.06371v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06371">http://arxiv.org/abs/2303.06371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06371] AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image](http://arxiv.org/abs/2303.06371) #diffusion</code></li>
<li>Summary: <p>Multiple Instance Learning (MIL), a powerful strategy for weakly supervised
learning, is able to perform various prediction tasks on gigapixel Whole Slide
Images (WSIs). However, the tens of thousands of patches in WSIs usually incur
a vast computational burden for image augmentation, limiting the MIL model's
improvement in performance. Currently, the feature augmentation-based MIL
framework is a promising solution, while existing methods such as Mixup often
produce unrealistic features. To explore a more efficient and practical
augmentation method, we introduce the Diffusion Model (DM) into MIL for the
first time and propose a feature augmentation framework called AugDiff.
Specifically, we employ the generation diversity of DM to improve the quality
of feature augmentation and the step-by-step generation property to control the
retention of semantic information. We conduct extensive experiments over three
distinct cancer datasets, two different feature extractors, and three prevalent
MIL algorithms to evaluate the performance of AugDiff. Ablation study and
visualization further verify the effectiveness. Moreover, we highlight
AugDiff's higher-quality augmented feature over image augmentation and its
superiority over self-supervised learning. The generalization over external
datasets indicates its broader applications.
</p></li>
</ul>

<h3>Title: Regularized Vector Quantization for Tokenized Image Synthesis. (arXiv:2303.06424v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06424">http://arxiv.org/abs/2303.06424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06424] Regularized Vector Quantization for Tokenized Image Synthesis](http://arxiv.org/abs/2303.06424) #diffusion</code></li>
<li>Summary: <p>Quantizing images into discrete representations has been a fundamental
problem in unified generative modeling. Predominant approaches learn the
discrete representation either in a deterministic manner by selecting the
best-matching token or in a stochastic manner by sampling from a predicted
distribution. However, deterministic quantization suffers from severe codebook
collapse and misalignment with inference stage while stochastic quantization
suffers from low codebook utilization and perturbed reconstruction objective.
This paper presents a regularized vector quantization framework that allows to
mitigate above issues effectively by applying regularization from two
perspectives. The first is a prior distribution regularization which measures
the discrepancy between a prior token distribution and the predicted token
distribution to avoid codebook collapse and low codebook utilization. The
second is a stochastic mask regularization that introduces stochasticity during
quantization to strike a good balance between inference stage misalignment and
unperturbed reconstruction objective. In addition, we design a probabilistic
contrastive loss which serves as a calibrated metric to further mitigate the
perturbed reconstruction objective. Extensive experiments show that the
proposed quantization framework outperforms prevailing vector quantization
methods consistently across different generative models including
auto-regressive models and diffusion models.
</p></li>
</ul>

<h3>Title: PARASOL: Parametric Style Control for Diffusion Image Synthesis. (arXiv:2303.06464v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06464">http://arxiv.org/abs/2303.06464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06464] PARASOL: Parametric Style Control for Diffusion Image Synthesis](http://arxiv.org/abs/2303.06464) #diffusion</code></li>
<li>Summary: <p>We propose PARASOL, a multi-modal synthesis model that enables disentangled,
parametric control of the visual style of the image by jointly conditioning
synthesis on both content and a fine-grained visual style embedding. We train a
latent diffusion model (LDM) using specific losses for each modality and adapt
the classifer-free guidance for encouraging disentangled control over
independent content and style modalities at inference time. We leverage
auxiliary semantic and style-based search to create training triplets for
supervision of the LDM, ensuring complementarity of content and style cues.
PARASOL shows promise for enabling nuanced control over visual style in
diffusion models for image creation and stylization, as well as generative
search where text-based search results may be adapted to more closely match
user intent by interpolating both content and style descriptors.
</p></li>
</ul>

<h3>Title: Diffusion-Based Hierarchical Multi-Label Object Detection to Analyze Panoramic Dental X-rays. (arXiv:2303.06500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.06500">http://arxiv.org/abs/2303.06500</a></li>
<li>Code URL: <a href="https://github.com/ibrahimethemhamamci/hierarchicaldet">https://github.com/ibrahimethemhamamci/hierarchicaldet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.06500] Diffusion-Based Hierarchical Multi-Label Object Detection to Analyze Panoramic Dental X-rays](http://arxiv.org/abs/2303.06500) #diffusion</code></li>
<li>Summary: <p>Due to the necessity for precise treatment planning, the use of panoramic
X-rays to identify different dental diseases has tremendously increased.
Although numerous ML models have been developed for the interpretation of
panoramic X-rays, there has not been an end-to-end model developed that can
identify problematic teeth with dental enumeration and associated diagnoses at
the same time. To develop such a model, we structure the three distinct types
of annotated data hierarchically following the FDI system, the first labeled
with only quadrant, the second labeled with quadrant-enumeration, and the third
fully labeled with quadrant-enumeration-diagnosis. To learn from all three
hierarchies jointly, we introduce a novel diffusion-based hierarchical
multi-label object detection framework by adapting a diffusion-based method
that formulates object detection as a denoising diffusion process from noisy
boxes to object boxes. Specifically, to take advantage of the hierarchically
annotated data, our method utilizes a novel noisy box manipulation technique by
adapting the denoising process in the diffusion network with the inference from
the previously trained model in hierarchical order. We also utilize a
multi-label object detection method to learn efficiently from partial
annotations and to give all the needed information about each abnormal tooth
for treatment planning. Experimental results show that our method significantly
outperforms state-of-the-art object detection methods, including RetinaNet,
Faster R-CNN, DETR, and DiffusionDet for the analysis of panoramic X-rays,
demonstrating the great potential of our method for hierarchically and
partially annotated datasets. The code and the data are available at:
https://github.com/ibrahimethemhamamci/HierarchicalDet.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
