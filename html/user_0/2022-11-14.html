<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Committed by Accident: Studying Prevention and Remediation Strategies Against Secret Leakage in Source Code Repositories. (arXiv:2211.06213v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06213">http://arxiv.org/abs/2211.06213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06213] Committed by Accident: Studying Prevention and Remediation Strategies Against Secret Leakage in Source Code Repositories](http://arxiv.org/abs/2211.06213)</code></li>
<li>Summary: <p>Version control systems for source code, such as Git, are key tools in modern
software development environments. Many developers use online services, such as
GitHub or GitLab, for collaborative software development. While software
projects often require code secrets to work, such as API keys or passwords,
they need to be handled securely within the project. Previous research and news
articles have illustrated that developers are blameworthy of committing code
secrets, such as private encryption keys, passwords, or API keys, accidentally
to public source code repositories. However, making secrets publicly available
might have disastrous consequences, such as leaving systems vulnerable to
attacks. In a mixed-methods study, we surveyed 109 developers and conducted 14
in-depth semi-structured interviews with developers which experienced secret
leakage in the past. We find that 30.3% of our participants have encountered
secret leakage in the past, and that developers are facing several challenges
with secret leakage prevention and remediation. Based on our findings, we
discuss challenges, e. g., estimating risks of leaked secrets, and needs of
developers in remediating and preventing code secret leaks, e. g., low adoption
requirements. We also give recommendations for developers and source code
platform providers to reduce the risk of secret leakage.
</p></li>
</ul>

<h3>Title: Secure Aggregation Is Not All You Need: Mitigating Privacy Attacks with Noise Tolerance in Federated Learning. (arXiv:2211.06324v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06324">http://arxiv.org/abs/2211.06324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06324] Secure Aggregation Is Not All You Need: Mitigating Privacy Attacks with Noise Tolerance in Federated Learning](http://arxiv.org/abs/2211.06324)</code></li>
<li>Summary: <p>Federated learning is a collaborative method that aims to preserve data
privacy while creating AI models. Current approaches to federated learning tend
to rely heavily on secure aggregation protocols to preserve data privacy.
However, to some degree, such protocols assume that the entity orchestrating
the federated learning process (i.e., the server) is not fully malicious or
dishonest. We investigate vulnerabilities to secure aggregation that could
arise if the server is fully malicious and attempts to obtain access to
private, potentially sensitive data. Furthermore, we provide a method to
further defend against such a malicious server, and demonstrate effectiveness
against known attacks that reconstruct data in a federated learning setting.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: An Integrity-Focused Threat Model for Software Development Pipelines. (arXiv:2211.06249v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06249">http://arxiv.org/abs/2211.06249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06249] An Integrity-Focused Threat Model for Software Development Pipelines](http://arxiv.org/abs/2211.06249)</code></li>
<li>Summary: <p>In recent years, there has been a growing concern with software integrity,
that is, the assurance that software has not been tampered with on the path
between developers and users. This path is represented by a software
development pipeline and plays a pivotal role in software supply chain
security. While there have been efforts to improve the security of development
pipelines, there is a lack of a comprehensive view of the threats affecting
them. We develop a systematic threat model for a generic software development
pipeline using the STRIDE framework and identify possible mitigations for each
threat. The pipeline adopted as a reference comprises five stages (integration,
continuous integration, infrastructure-as-code, deployment, and release), and
we review vulnerabilities and attacks in all stages reported in the literature.
We present a case study applying this threat model to a specific pipeline,
showing that the adaptation is straightforward and produces a list of relevant
threats.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: No Privacy in the Electronics Repair Industry. (arXiv:2211.05824v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05824">http://arxiv.org/abs/2211.05824</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05824] No Privacy in the Electronics Repair Industry](http://arxiv.org/abs/2211.05824)</code></li>
<li>Summary: <p>Electronics repair and service providers offer a range of services to
computing device owners across North America -- from software installation to
hardware repair. Device owners obtain these services and leave their device
along with their access credentials at the mercy of technicians, which leads to
privacy concerns for owners' personal data. We conduct a comprehensive
four-part study to measure the state of privacy in the electronics repair
industry. First, through a field study with 18 service providers, we uncover
that most service providers do not have any privacy policy or controls to
safeguard device owners' personal data from snooping by technicians. Second, we
drop rigged devices for repair at 16 service providers and collect data on
widespread privacy violations by technicians, including snooping on personal
data, copying data off the device, and removing tracks of snooping activities.
Third, we conduct an online survey (n=112) to collect data on customers'
experiences when getting devices repaired. Fourth, we invite a subset of survey
respondents (n=30) for semi-structured interviews to establish a deeper
understanding of their experiences and identify potential solutions to curtail
privacy violations by technicians. We apply our findings to discuss possible
controls and actions different stakeholders and regulatory agencies should take
to improve the state of privacy in the repair industry.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Remapped Cache Layout: Thwarting Cache-Based Side-Channel Attacks with a Hardware Defense. (arXiv:2211.06056v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06056">http://arxiv.org/abs/2211.06056</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06056] Remapped Cache Layout: Thwarting Cache-Based Side-Channel Attacks with a Hardware Defense](http://arxiv.org/abs/2211.06056)</code></li>
<li>Summary: <p>As cache-based side-channel attacks become serious security problems, various
defenses have been proposed and deployed in both software and hardware.
Consequently, cache-based side-channel attacks on processes co-residing on the
same core are becoming extremely difficult. Most of recent attacks then shift
their focus to the last-level cache (LLC). Although cache partitioning is
currently the most promising defense against the attacks abusing LLC, it is
ineffective in thwarting the side-channel attacks that automatically create
eviction sets or bypass the user address space layout randomization. In fact,
these attacks are largely undefended in current computer systems.
</p></li>
</ul>

<p>We propose Remapped Cache Layout (\textsf{RCL}) -- a pure hardware defense
against a broad range of conflict-based side-channel attacks. \textsf{RCL}
obfuscates the mapping from address to cache sets; therefore, an attacker
cannot accurately infer the location of her data in caches or using a cache set
to infer her victim's data. To our best knowledge, it is the first defense to
thwart the aforementioned largely undefended side-channel attacks .
\textsf{RCL} has been implemented in a superscalar processor and detailed
evaluation results show that \textsf{RCL} incurs only small costs in area,
frequency and execution time.
</p>

<h2>attack</h2>
<h2>robust</h2>
<h3>Title: Impact of Video Compression on the Performance of Object Detection Systems for Surveillance Applications. (arXiv:2211.05805v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05805">http://arxiv.org/abs/2211.05805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05805] Impact of Video Compression on the Performance of Object Detection Systems for Surveillance Applications](http://arxiv.org/abs/2211.05805)</code></li>
<li>Summary: <p>This study examines the relationship between H.264 video compression and the
performance of an object detection network (YOLOv5). We curated a set of 50
surveillance videos and annotated targets of interest (people, bikes, and
vehicles). Videos were encoded at 5 quality levels using Constant Rate Factor
(CRF) values in the set {22,32,37,42,47}. YOLOv5 was applied to compressed
videos and detection performance was analyzed at each CRF level. Test results
indicate that the detection performance is generally robust to moderate levels
of compression; using a CRF value of 37 instead of 22 leads to significantly
reduced bitrates/file sizes without adversely affecting detection performance.
However, detection performance degrades appreciably at higher compression
levels, especially in complex scenes with poor lighting and fast-moving
targets. Finally, retraining YOLOv5 on compressed imagery gives up to a 1%
improvement in F1 score when applied to highly compressed footage.
</p></li>
</ul>

<h3>Title: Casual Conversations v2: Designing a large consent-driven dataset to measure algorithmic bias and robustness. (arXiv:2211.05809v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05809">http://arxiv.org/abs/2211.05809</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05809] Casual Conversations v2: Designing a large consent-driven dataset to measure algorithmic bias and robustness](http://arxiv.org/abs/2211.05809)</code></li>
<li>Summary: <p>Developing robust and fair AI systems require datasets with comprehensive set
of labels that can help ensure the validity and legitimacy of relevant
measurements. Recent efforts, therefore, focus on collecting person-related
datasets that have carefully selected labels, including sensitive
characteristics, and consent forms in place to use those attributes for model
testing and development. Responsible data collection involves several stages,
including but not limited to determining use-case scenarios, selecting
categories (annotations) such that the data are fit for the purpose of
measuring algorithmic bias for subgroups and most importantly ensure that the
selected categories/subcategories are robust to regional diversities and
inclusive of as many subgroups as possible.
</p></li>
</ul>

<p>Meta, in a continuation of our efforts to measure AI algorithmic bias and
robustness
(https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set),
is working on collecting a large consent-driven dataset with a comprehensive
list of categories. This paper describes our proposed design of such categories
and subcategories for Casual Conversations v2.
</p>

<h3>Title: Open-Set Automatic Target Recognition. (arXiv:2211.05883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05883">http://arxiv.org/abs/2211.05883</a></li>
<li>Code URL: <a href="https://github.com/bardisafa/open-set-atr">https://github.com/bardisafa/open-set-atr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05883] Open-Set Automatic Target Recognition](http://arxiv.org/abs/2211.05883)</code></li>
<li>Summary: <p>Automatic Target Recognition (ATR) is a category of computer vision
algorithms which attempts to recognize targets on data obtained from different
sensors. ATR algorithms are extensively used in real-world scenarios such as
military and surveillance applications. Existing ATR algorithms are developed
for traditional closed-set methods where training and testing have the same
class distribution. Thus, these algorithms have not been robust to unknown
classes not seen during the training phase, limiting their utility in
real-world applications. To this end, we propose an Open-set Automatic Target
Recognition framework where we enable open-set recognition capability for ATR
algorithms. In addition, we introduce a plugin Category-aware Binary Classifier
(CBC) module to effectively tackle unknown classes seen during inference. The
proposed CBC module can be easily integrated with any existing ATR algorithms
and can be trained in an end-to-end manner. Experimental results show that the
proposed approach outperforms many open-set methods on the DSIAC and CIFAR-10
datasets. To the best of our knowledge, this is the first work to address the
open-set classification problem for ATR algorithms. Source code is available
at: https://github.com/bardisafa/Open-set-ATR.
</p></li>
</ul>

<h3>Title: Palm Vein Recognition via Multi-task Loss Function and Attention Layer. (arXiv:2211.05970v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05970">http://arxiv.org/abs/2211.05970</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05970] Palm Vein Recognition via Multi-task Loss Function and Attention Layer](http://arxiv.org/abs/2211.05970)</code></li>
<li>Summary: <p>With the improvement of arithmetic power and algorithm accuracy of personal
devices, biological features are increasingly widely used in personal
identification, and palm vein recognition has rich extractable features and has
been widely studied in recent years. However, traditional recognition methods
are poorly robust and susceptible to environmental influences such as
reflections and noise. In this paper, a convolutional neural network based on
VGG-16 transfer learning fused attention mechanism is used as the feature
extraction network on the infrared palm vein dataset. The palm vein
classification task is first trained using palmprint classification methods,
followed by matching using a similarity function, in which we propose the
multi-task loss function to improve the accuracy of the matching task. In order
to verify the robustness of the model, some experiments were carried out on
datasets from different sources. Then, we used K-means clustering to determine
the adaptive matching threshold and finally achieved an accuracy rate of 98.89%
on prediction set. At the same time, the matching is with high efficiency which
takes an average of 0.13 seconds per palm vein pair, and that means our method
can be adopted in practice.
</p></li>
</ul>

<h3>Title: LiDAL: Inter-frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation. (arXiv:2211.05997v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05997">http://arxiv.org/abs/2211.05997</a></li>
<li>Code URL: <a href="https://github.com/hzykent/lidal">https://github.com/hzykent/lidal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05997] LiDAL: Inter-frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation](http://arxiv.org/abs/2211.05997)</code></li>
<li>Summary: <p>We propose LiDAL, a novel active learning method for 3D LiDAR semantic
segmentation by exploiting inter-frame uncertainty among LiDAR frames. Our core
idea is that a well-trained model should generate robust results irrespective
of viewpoints for scene scanning and thus the inconsistencies in model
predictions across frames provide a very reliable measure of uncertainty for
active sample selection. To implement this uncertainty measure, we introduce
new inter-frame divergence and entropy formulations, which serve as the metrics
for active selection. Moreover, we demonstrate additional performance gains by
predicting and incorporating pseudo-labels, which are also selected using the
proposed inter-frame uncertainty measure. Experimental results validate the
effectiveness of LiDAL: we achieve 95% of the performance of fully supervised
learning with less than 5% of annotations on the SemanticKITTI and nuScenes
datasets, outperforming state-of-the-art active learning methods. Code release:
https://github.com/hzykent/LiDAL.
</p></li>
</ul>

<h3>Title: RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06108">http://arxiv.org/abs/2211.06108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06108] RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System](http://arxiv.org/abs/2211.06108)</code></li>
<li>Summary: <p>Radar, the only sensor that could provide reliable perception capability in
all weather conditions at an affordable cost, has been widely accepted as a key
supplement to camera and LiDAR in modern advanced driver assistance systems
(ADAS) and autonomous driving systems. Recent state-of-the-art works reveal
that fusion of radar and LiDAR can lead to robust detection in adverse weather,
such as fog. However, these methods still suffer from low accuracy of bounding
box estimations. This paper proposes a bird's-eye view (BEV) fusion learning
for an anchor box-free object detection system, which uses the feature derived
from the radar range-azimuth heatmap and the LiDAR point cloud to estimate the
possible objects. Different label assignment strategies have been designed to
facilitate the consistency between the classification of foreground or
background anchor points and the corresponding bounding box regressions.
Furthermore, the performance of the proposed object detector can be further
enhanced by employing a novel interactive transformer module. We demonstrated
the superior performance of the proposed methods in this paper using the
recently published Oxford Radar RobotCar (ORR) dataset. We showed that the
accuracy of our system significantly outperforms the other state-of-the-art
methods by a large margin.
</p></li>
</ul>

<h3>Title: CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class Classification. (arXiv:2211.05987v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05987">http://arxiv.org/abs/2211.05987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05987] CCPrompt: Counterfactual Contrastive Prompt-Tuning for Many-Class Classification](http://arxiv.org/abs/2211.05987)</code></li>
<li>Summary: <p>With the success of the prompt-tuning paradigm in Natural Language Processing
(NLP), various prompt templates have been proposed to further stimulate
specific knowledge for serving downstream tasks, e.g., machine translation,
text generation, relation extraction, and so on. Existing prompt templates are
mainly shared among all training samples with the information of task
description. However, training samples are quite diverse. The sharing task
description is unable to stimulate the unique task-related information in each
training sample, especially for tasks with the finite-label space. To exploit
the unique task-related information, we imitate the human decision process
which aims to find the contrastive attributes between the objective factual and
their potential counterfactuals. Thus, we propose the \textbf{C}ounterfactual
\textbf{C}ontrastive \textbf{Prompt}-Tuning (CCPrompt) approach for many-class
classification, e.g., relation classification, topic classification, and entity
typing. Compared with simple classification tasks, these tasks have more
complex finite-label spaces and are more rigorous for prompts. First of all, we
prune the finite label space to construct fact-counterfactual pairs. Then, we
exploit the contrastive attributes by projecting training instances onto every
fact-counterfactual pair. We further set up global prototypes corresponding
with all contrastive attributes for selecting valid contrastive attributes as
additional tokens in the prompt template. Finally, a simple Siamese
representation learning is employed to enhance the robustness of the model. We
conduct experiments on relation classification, topic classification, and
entity typing tasks in both fully supervised setting and few-shot setting. The
results indicate that our model outperforms former baselines.
</p></li>
</ul>

<h3>Title: Test-time adversarial detection and robustness for localizing humans using ultra wide band channel impulse responses. (arXiv:2211.05854v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05854">http://arxiv.org/abs/2211.05854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05854] Test-time adversarial detection and robustness for localizing humans using ultra wide band channel impulse responses](http://arxiv.org/abs/2211.05854)</code></li>
<li>Summary: <p>Keyless entry systems in cars are adopting neural networks for localizing its
operators. Using test-time adversarial defences equip such systems with the
ability to defend against adversarial attacks without prior training on
adversarial samples. We propose a test-time adversarial example detector which
detects the input adversarial example through quantifying the localized
intermediate responses of a pre-trained neural network and confidence scores of
an auxiliary softmax layer. Furthermore, in order to make the network robust,
we extenuate the non-relevant features by non-iterative input sample clipping.
Using our approach, mean performance over 15 levels of adversarial
perturbations is increased by 55.33% for the fast gradient sign method (FGSM)
and 6.3% for both the basic iterative method (BIM) and the projected gradient
method (PGD).
</p></li>
</ul>

<h3>Title: REVEL Framework to measure Local Linear Explanations for black-box models: Deep Learning Image Classification case of study. (arXiv:2211.06154v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06154">http://arxiv.org/abs/2211.06154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06154] REVEL Framework to measure Local Linear Explanations for black-box models: Deep Learning Image Classification case of study](http://arxiv.org/abs/2211.06154)</code></li>
<li>Summary: <p>Explainable artificial intelligence is proposed to provide explanations for
reasoning performed by an Artificial Intelligence. There is no consensus on how
to evaluate the quality of these explanations, since even the definition of
explanation itself is not clear in the literature. In particular, for the
widely known Local Linear Explanations, there are qualitative proposals for the
evaluation of explanations, although they suffer from theoretical
inconsistencies. The case of image is even more problematic, where a visual
explanation seems to explain a decision while detecting edges is what it really
does. There are a large number of metrics in the literature specialized in
quantitatively measuring different qualitative aspects so we should be able to
develop metrics capable of measuring in a robust and correct way the desirable
aspects of the explanations. In this paper, we propose a procedure called REVEL
to evaluate different aspects concerning the quality of explanations with a
theoretically coherent development. This procedure has several advances in the
state of the art: it standardizes the concepts of explanation and develops a
series of metrics not only to be able to compare between them but also to
obtain absolute information regarding the explanation itself. The experiments
have been carried out on image four datasets as benchmark where we show REVEL's
descriptive and analytical power.
</p></li>
</ul>

<h3>Title: Integrated Convolutional and Recurrent Neural Networks for Health Risk Prediction using Patient Journey Data with Many Missing Values. (arXiv:2211.06045v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06045">http://arxiv.org/abs/2211.06045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06045] Integrated Convolutional and Recurrent Neural Networks for Health Risk Prediction using Patient Journey Data with Many Missing Values](http://arxiv.org/abs/2211.06045)</code></li>
<li>Summary: <p>Predicting the health risks of patients using Electronic Health Records (EHR)
has attracted considerable attention in recent years, especially with the
development of deep learning techniques. Health risk refers to the probability
of the occurrence of a specific health outcome for a specific patient. The
predicted risks can be used to support decision-making by healthcare
professionals. EHRs are structured patient journey data. Each patient journey
contains a chronological set of clinical events, and within each clinical
event, there is a set of clinical/medical activities. Due to variations of
patient conditions and treatment needs, EHR patient journey data has an
inherently high degree of missingness that contains important information
affecting relationships among variables, including time. Existing deep
learning-based models generate imputed values for missing values when learning
the relationships. However, imputed data in EHR patient journey data may
distort the clinical meaning of the original EHR patient journey data,
resulting in classification bias. This paper proposes a novel end-to-end
approach to modeling EHR patient journey data with Integrated Convolutional and
Recurrent Neural Networks. Our model can capture both long- and short-term
temporal patterns within each patient journey and effectively handle the high
degree of missingness in EHR data without any imputation data generation.
Extensive experimental results using the proposed model on two real-world
datasets demonstrate robust performance as well as superior prediction accuracy
compared to existing state-of-the-art imputation-based prediction methods.
</p></li>
</ul>

<h3>Title: Comparison of Uncertainty Quantification with Deep Learning in Time Series Regression. (arXiv:2211.06233v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06233">http://arxiv.org/abs/2211.06233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06233] Comparison of Uncertainty Quantification with Deep Learning in Time Series Regression](http://arxiv.org/abs/2211.06233)</code></li>
<li>Summary: <p>Increasingly high-stakes decisions are made using neural networks in order to
make predictions. Specifically, meteorologists and hedge funds apply these
techniques to time series data. When it comes to prediction, there are certain
limitations for machine learning models (such as lack of expressiveness,
vulnerability of domain shifts and overconfidence) which can be solved using
uncertainty estimation. There is a set of expectations regarding how
uncertainty should ``behave". For instance, a wider prediction horizon should
lead to more uncertainty or the model's confidence should be proportional to
its accuracy. In this paper, different uncertainty estimation methods are
compared to forecast meteorological time series data and evaluate these
expectations. The results show how each uncertainty estimation method performs
on the forecasting task, which partially evaluates the robustness of predicted
uncertainty.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: An Improved End-to-End Multi-Target Tracking Method Based on Transformer Self-Attention. (arXiv:2211.06001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06001">http://arxiv.org/abs/2211.06001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06001] An Improved End-to-End Multi-Target Tracking Method Based on Transformer Self-Attention](http://arxiv.org/abs/2211.06001)</code></li>
<li>Summary: <p>This study proposes an improved end-to-end multi-target tracking algorithm
that adapts to multi-view multi-scale scenes based on the self-attentive
mechanism of the transformer's encoder-decoder structure. A multi-dimensional
feature extraction backbone network is combined with a self-built semantic
raster map, which is stored in the encoder for correlation and generates target
position encoding and multi-dimensional feature vectors. The decoder
incorporates four methods: spatial clustering and semantic filtering of
multi-view targets, dynamic matching of multi-dimensional features, space-time
logic-based multi-target tracking, and space-time convergence network
(STCN)-based parameter passing. Through the fusion of multiple decoding
methods, muti-camera targets are tracked in three dimensions: temporal logic,
spatial logic, and feature matching. For the MOT17 dataset, this study's method
significantly outperforms the current state-of-the-art method MiniTrackV2 [49]
by 2.2% to 0.836 on Multiple Object Tracking Accuracy(MOTA) metric.
Furthermore, this study proposes a retrospective mechanism for the first time,
and adopts a reverse-order processing method to optimise the historical
mislabeled targets for improving the Identification F1-score(IDF1). For the
self-built dataset OVIT-MOT01, the IDF1 improves from 0.948 to 0.967, and the
Multi-camera Tracking Accuracy(MCTA) improves from 0.878 to 0.909, which
significantly improves the continuous tracking accuracy and scene adaptation.
This research method introduces a new attentional tracking paradigm which is
able to achieve state-of-the-art performance on multi-target tracking (MOT17
and OVIT-MOT01) tasks.
</p></li>
</ul>

<h3>Title: Interactive Context-Aware Network for RGB-T Salient Object Detection. (arXiv:2211.06097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06097">http://arxiv.org/abs/2211.06097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06097] Interactive Context-Aware Network for RGB-T Salient Object Detection](http://arxiv.org/abs/2211.06097)</code></li>
<li>Summary: <p>Salient object detection (SOD) focuses on distinguishing the most conspicuous
objects in the scene. However, most related works are based on RGB images,
which lose massive useful information. Accordingly, with the maturity of
thermal technology, RGB-T (RGB-Thermal) multi-modality tasks attain more and
more attention. Thermal infrared images carry important information which can
be used to improve the accuracy of SOD prediction. To accomplish it, the
methods to integrate multi-modal information and suppress noises are critical.
In this paper, we propose a novel network called Interactive Context-Aware
Network (ICANet). It contains three modules that can effectively perform the
cross-modal and cross-scale fusions. We design a Hybrid Feature Fusion (HFF)
module to integrate the features of two modalities, which utilizes two types of
feature extraction. The Multi-Scale Attention Reinforcement (MSAR) and Upper
Fusion (UF) blocks are responsible for the cross-scale fusion that converges
different levels of features and generate the prediction maps. We also raise a
novel Context-Aware Multi-Supervised Network (CAMSNet) to calculate the content
loss between the prediction and the ground truth (GT). Experiments prove that
our network performs favorably against the state-of-the-art RGB-T SOD methods.
</p></li>
</ul>

<h3>Title: MEE: A Novel Multilingual Event Extraction Dataset. (arXiv:2211.05955v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05955">http://arxiv.org/abs/2211.05955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05955] MEE: A Novel Multilingual Event Extraction Dataset](http://arxiv.org/abs/2211.05955)</code></li>
<li>Summary: <p>Event Extraction (EE) is one of the fundamental tasks in Information
Extraction (IE) that aims to recognize event mentions and their arguments
(i.e., participants) from text. Due to its importance, extensive methods and
resources have been developed for Event Extraction. However, one limitation of
current research for EE involves the under-exploration for non-English
languages in which the lack of high-quality multilingual EE datasets for model
training and evaluation has been the main hindrance. To address this
limitation, we propose a novel Multilingual Event Extraction dataset (MEE) that
provides annotation for more than 50K event mentions in 8 typologically
different languages. MEE comprehensively annotates data for entity mentions,
event triggers and event arguments. We conduct extensive experiments on the
proposed dataset to reveal challenges and opportunities for multilingual EE.
</p></li>
</ul>

<h3>Title: Getting the Most out of Simile Recognition. (arXiv:2211.05984v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.05984">http://arxiv.org/abs/2211.05984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.05984] Getting the Most out of Simile Recognition](http://arxiv.org/abs/2211.05984)</code></li>
<li>Summary: <p>Simile recognition involves two subtasks: simile sentence classification that
discriminates whether a sentence contains simile, and simile component
extraction that locates the corresponding objects (i.e., tenors and vehicles).
Recent work ignores features other than surface strings. In this paper, we
explore expressive features for this task to achieve more effective data
utilization. Particularly, we study two types of features: 1) input-side
features that include POS tags, dependency trees and word definitions, and 2)
decoding features that capture the interdependence among various decoding
decisions. We further construct a model named HGSR, which merges the input-side
features as a heterogeneous graph and leverages decoding features via
distillation. Experiments show that HGSR significantly outperforms the current
state-of-the-art systems and carefully designed baselines, verifying the
effectiveness of introduced features. Our code is available at
https://github.com/DeepLearnXMU/HGSR.
</p></li>
</ul>

<h3>Title: Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction. (arXiv:2211.06014v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06014">http://arxiv.org/abs/2211.06014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06014] Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction](http://arxiv.org/abs/2211.06014)</code></li>
<li>Summary: <p>Information Extraction (IE) aims to extract structured information from
heterogeneous sources. IE from natural language texts include sub-tasks such as
Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction
(EE). Most IE systems require comprehensive understandings of sentence
structure, implied semantics, and domain knowledge to perform well; thus, IE
tasks always need adequate external resources and annotations. However, it
takes time and effort to obtain more human annotations. Low-Resource
Information Extraction (LRIE) strives to use unsupervised data, reducing the
required resources and human annotation. In practice, existing systems either
utilize self-training schemes to generate pseudo labels that will cause the
gradual drift problem, or leverage consistency regularization methods which
inevitably possess confirmation bias. To alleviate confirmation bias due to the
lack of feedback loops in existing LRIE learning paradigms, we develop a
Gradient Imitation Reinforcement Learning (GIRL) method to encourage
pseudo-labeled data to imitate the gradient descent direction on labeled data,
which can force pseudo-labeled data to achieve better optimization capabilities
similar to labeled data. Based on how well the pseudo-labeled data imitates the
instructive gradient descent direction obtained from labeled data, we design a
reward to quantify the imitation process and bootstrap the optimization
capability of pseudo-labeled data through trial and error. In addition to
learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage
GIRL to solve all IE sub-tasks (named entity recognition, relation extraction,
and event extraction) in low-resource settings (semi-supervised IE and few-shot
IE).
</p></li>
</ul>

<h3>Title: Towards automating Numerical Consistency Checks in Financial Reports. (arXiv:2211.06112v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06112">http://arxiv.org/abs/2211.06112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06112] Towards automating Numerical Consistency Checks in Financial Reports](http://arxiv.org/abs/2211.06112)</code></li>
<li>Summary: <p>We introduce KPI-Check, a novel system that automatically identifies and
cross-checks semantically equivalent key performance indicators (KPIs), e.g.
"revenue" or "total costs", in real-world German financial reports. It combines
a financial named entity and relation extraction module with a BERT-based
filtering and text pair classification component to extract KPIs from
unstructured sentences before linking them to synonymous occurrences in the
balance sheet and profit &amp; loss statement. The tool achieves a high matching
performance of $73.00$% micro F$_1$ on a hold out test set and is currently
being deployed for a globally operating major auditing firm to assist the
auditing procedure of financial statements.
</p></li>
</ul>

<h3>Title: Unimodal and Multimodal Representation Training for Relation Extraction. (arXiv:2211.06168v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06168">http://arxiv.org/abs/2211.06168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06168] Unimodal and Multimodal Representation Training for Relation Extraction](http://arxiv.org/abs/2211.06168)</code></li>
<li>Summary: <p>Multimodal integration of text, layout and visual information has achieved
SOTA results in visually rich document understanding (VrDU) tasks, including
relation extraction (RE). However, despite its importance, evaluation of the
relative predictive capacity of these modalities is less prevalent. Here, we
demonstrate the value of shared representations for RE tasks by conducting
experiments in which each data type is iteratively excluded during training. In
addition, text and layout data are evaluated in isolation. While a bimodal text
and layout approach performs best (F1=0.684), we show that text is the most
important single predictor of entity relations. Additionally, layout geometry
is highly predictive and may even be a feasible unimodal approach. Despite
being less effective, we highlight circumstances where visual information can
bolster performance. In total, our results demonstrate the efficacy of training
joint representations for RE.
</p></li>
</ul>

<h3>Title: What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives. (arXiv:2211.06009v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06009">http://arxiv.org/abs/2211.06009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06009] What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives](http://arxiv.org/abs/2211.06009)</code></li>
<li>Summary: <p>Intelligent mesh generation (IMG) refers to a technique to generate mesh by
machine learning, which is a relatively new and promising research field.
Within its short life span, IMG has greatly expanded the generalizability and
practicality of mesh generation techniques and brought many breakthroughs and
potential possibilities for mesh generation. However, there is a lack of
surveys focusing on IMG methods covering recent works. In this paper, we are
committed to a systematic and comprehensive survey describing the contemporary
IMG landscape. Focusing on 110 preliminary IMG methods, we conducted an
in-depth analysis and evaluation from multiple perspectives, including the core
technique and application scope of the algorithm, agent learning goals, data
types, targeting challenges, advantages and limitations. With the aim of
literature collection and classification based on content extraction, we
propose three different taxonomies from three views of key technique, output
mesh unit element, and applicable input data types. Finally, we highlight some
promising future research directions and challenges in IMG. To maximize the
convenience of readers, a project page of IMG is provided at
\url{https://github.com/xzb030/IMG_Survey}.
</p></li>
</ul>

<h3>Title: GeoAI for Knowledge Graph Construction: Identifying Causality Between Cascading Events to Support Environmental Resilience Research. (arXiv:2211.06011v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06011">http://arxiv.org/abs/2211.06011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06011] GeoAI for Knowledge Graph Construction: Identifying Causality Between Cascading Events to Support Environmental Resilience Research](http://arxiv.org/abs/2211.06011)</code></li>
<li>Summary: <p>Knowledge graph technology is considered a powerful and semantically enabled
solution to link entities, allowing users to derive new knowledge by reasoning
data according to various types of reasoning rules. However, in building such a
knowledge graph, events modeling, such as that of disasters, is often limited
to single, isolated events. The linkages among cascading events are often
missing in existing knowledge graphs. This paper introduces our GeoAI
(Geospatial Artificial Intelligence) solutions to identify causality among
events, in particular, disaster events, based on a set of spatially and
temporally-enabled semantic rules. Through a use case of causal disaster events
modeling, we demonstrated how these defined rules, including theme-based
identification of correlated events, spatiotemporal co-occurrence constraint,
and text mining of event metadata, enable the automatic extraction of causal
relationships between different events. Our solution enriches the event
knowledge base and allows for the exploration of linked cascading events in
large knowledge graphs, therefore empowering knowledge query and discovery.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Federated Approach to Predicting Emojis in Hindi Tweets. (arXiv:2211.06401v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06401">http://arxiv.org/abs/2211.06401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06401] A Federated Approach to Predicting Emojis in Hindi Tweets](http://arxiv.org/abs/2211.06401)</code></li>
<li>Summary: <p>The use of emojis affords a visual modality to, often private, textual
communication. The task of predicting emojis however provides a challenge for
machine learning as emoji use tends to cluster into the frequently used and the
rarely used emojis. Much of the machine learning research on emoji use has
focused on high resource languages and has conceptualised the task of
predicting emojis around traditional server-side machine learning approaches.
However, traditional machine learning approaches for private communication can
introduce privacy concerns, as these approaches require all data to be
transmitted to a central storage. In this paper, we seek to address the dual
concerns of emphasising high resource languages for emoji prediction and
risking the privacy of people's data. We introduce a new dataset of $118$k
tweets (augmented from $25$k unique tweets) for emoji prediction in Hindi, and
propose a modification to the federated learning algorithm, CausalFedGSD, which
aims to strike a balance between model performance and user privacy. We show
that our approach obtains comparative scores with more complex centralised
models while reducing the amount of data required to optimise the models and
minimising risks to user privacy.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Identifying, measuring, and mitigating individual unfairness for supervised learning models and application to credit risk models. (arXiv:2211.06106v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06106">http://arxiv.org/abs/2211.06106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06106] Identifying, measuring, and mitigating individual unfairness for supervised learning models and application to credit risk models](http://arxiv.org/abs/2211.06106)</code></li>
<li>Summary: <p>In the past few years, Artificial Intelligence (AI) has garnered attention
from various industries including financial services (FS). AI has made a
positive impact in financial services by enhancing productivity and improving
risk management. While AI can offer efficient solutions, it has the potential
to bring unintended consequences. One such consequence is the pronounced effect
of AI-related unfairness and attendant fairness-related harms. These
fairness-related harms could involve differential treatment of individuals; for
example, unfairly denying a loan to certain individuals or groups of
individuals. In this paper, we focus on identifying and mitigating individual
unfairness and leveraging some of the recently published techniques in this
domain, especially as applicable to the credit adjudication use case. We also
investigate the extent to which techniques for achieving individual fairness
are effective at achieving group fairness. Our main contribution in this work
is functionalizing a two-step training process which involves learning a fair
similarity metric from a group sense using a small portion of the raw data and
training an individually "fair" classifier using the rest of the data where the
sensitive features are excluded. The key characteristic of this two-step
technique is related to its flexibility, i.e., the fair metric obtained in the
first step can be used with any other individual fairness algorithms in the
second step. Furthermore, we developed a second metric (distinct from the fair
similarity metric) to determine how fairly a model is treating similar
individuals. We use this metric to compare a "fair" model against its baseline
model in terms of their individual fairness value. Finally, some experimental
results corresponding to the individual unfairness mitigation techniques are
presented.
</p></li>
</ul>

<h3>Title: Practical Approaches for Fair Learning with Multitype and Multivariate Sensitive Attributes. (arXiv:2211.06138v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06138">http://arxiv.org/abs/2211.06138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06138] Practical Approaches for Fair Learning with Multitype and Multivariate Sensitive Attributes](http://arxiv.org/abs/2211.06138)</code></li>
<li>Summary: <p>It is important to guarantee that machine learning algorithms deployed in the
real world do not result in unfairness or unintended social consequences. Fair
ML has largely focused on the protection of single attributes in the simpler
setting where both attributes and target outcomes are binary. However, the
practical application in many a real-world problem entails the simultaneous
protection of multiple sensitive attributes, which are often not simply binary,
but continuous or categorical. To address this more challenging task, we
introduce FairCOCCO, a fairness measure built on cross-covariance operators on
reproducing kernel Hilbert Spaces. This leads to two practical tools: first,
the FairCOCCO Score, a normalised metric that can quantify fairness in settings
with single or multiple sensitive attributes of arbitrary type; and second, a
subsequent regularisation term that can be incorporated into arbitrary learning
objectives to obtain fair predictors. These contributions address crucial gaps
in the algorithmic fairness literature, and we empirically demonstrate
consistent improvements against state-of-the-art techniques in balancing
predictive power and fairness on real-world datasets.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Spatial Temporal Graph Convolution with Graph Structure Self-learning for Early MCI Detection. (arXiv:2211.06161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06161">http://arxiv.org/abs/2211.06161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06161] Spatial Temporal Graph Convolution with Graph Structure Self-learning for Early MCI Detection](http://arxiv.org/abs/2211.06161)</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have been successfully applied to early mild
cognitive impairment (EMCI) detection, with the usage of elaborately designed
features constructed from blood oxygen level-dependent (BOLD) time series.
However, few works explored the feasibility of using BOLD signals directly as
features. Meanwhile, existing GNN-based methods primarily rely on hand-crafted
explicit brain topology as the adjacency matrix, which is not optimal and
ignores the implicit topological organization of the brain. In this paper, we
propose a spatial temporal graph convolutional network with a novel graph
structure self-learning mechanism for EMCI detection. The proposed spatial
temporal graph convolution block directly exploits BOLD time series as input
features, which provides an interesting view for rsfMRI-based preclinical AD
diagnosis. Moreover, our model can adaptively learn the optimal topological
structure and refine edge weights with the graph structure self-learning
mechanism. Results on the Alzheimer's Disease Neuroimaging Initiative (ADNI)
database show that our method outperforms state-of-the-art approaches.
Biomarkers consistent with previous studies can be extracted from the model,
proving the reliable interpretability of our method.
</p></li>
</ul>

<h3>Title: Rethinking Log Odds: Linear Probability Modelling and Expert Advice in Interpretable Machine Learning. (arXiv:2211.06360v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.06360">http://arxiv.org/abs/2211.06360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.06360] Rethinking Log Odds: Linear Probability Modelling and Expert Advice in Interpretable Machine Learning](http://arxiv.org/abs/2211.06360)</code></li>
<li>Summary: <p>We introduce a family of interpretable machine learning models, with two
broad additions: Linearised Additive Models (LAMs) which replace the ubiquitous
logistic link function in General Additive Models (GAMs); and SubscaleHedge, an
expert advice algorithm for combining base models trained on subsets of
features called subscales. LAMs can augment any additive binary classification
model equipped with a sigmoid link function. Moreover, they afford direct
global and local attributions of additive components to the model output in
probability space. We argue that LAMs and SubscaleHedge improve the
interpretability of their base algorithms. Using rigorous null-hypothesis
significance testing on a broad suite of financial modelling data, we show that
our algorithms do not suffer from large performance penalties in terms of
ROC-AUC and calibration.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
