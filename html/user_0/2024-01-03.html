<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-03</h1>
<h2>secure</h2>
<h3>Title: CCA-Secure Hybrid Encryption in Correlated Randomness Model and KEM Combiners. (arXiv:2401.00983v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00983">http://arxiv.org/abs/2401.00983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00983]] CCA-Secure Hybrid Encryption in Correlated Randomness Model and KEM Combiners(http://arxiv.org/abs/2401.00983)</code></li>
<li>Summary: <p>A hybrid encryption (HE) system is an efficient public key encryption system
for arbitrarily long messages. An HE system consists of a public key component
called key encapsulation mechanism (KEM), and a symmetric key component called
data encapsulation mechanism (DEM). The HE encryption algorithm uses a KEM
generated key k to encapsulate the message using DEM, and send the ciphertext
together with the encapsulaton of k, to the decryptor who decapsulates k and
uses it to decapsulate the message using the corresponding KEM and DEM
components. The KEM/DEM composition theorem proves that if KEM and DEM satisfy
well-defined security notions, then HE will be secure with well defined
security. We introduce HE in correlated randomness model where the encryption
and decryption algorithms have samples of correlated random variables that are
partially leaked to the adversary. Security of the new KEM/DEM paradigm is
defined against computationally unbounded or polynomially bounded adversaries.
We define iKEM and cKEM with respective information theoretic computational
security, and prove a composition theorem for them and a computationally secure
DEM, resulting in secure HEs with proved computational security (CPA and CCA)
and without any computational assumption. We construct two iKEMs that provably
satisfy the required security notions of the composition theorem. The iKEMs are
used to construct two efficient quantum-resistant HEs when used with an AES
based DEM. We also define and construct combiners with proved security that
combine the new KEM/DEM paradigm of HE with the traditional public key based
paradigm of HE.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing. (arXiv:2401.01102v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01102">http://arxiv.org/abs/2401.01102</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01102]] Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing(http://arxiv.org/abs/2401.01102)</code></li>
<li>Summary: <p>Face recognition systems have raised concerns due to their vulnerability to
different presentation attacks, and system security has become an increasingly
critical concern. Although many face anti-spoofing (FAS) methods perform well
in intra-dataset scenarios, their generalization remains a challenge. To
address this issue, some methods adopt domain adversarial training (DAT) to
extract domain-invariant features. However, the competition between the encoder
and the domain discriminator can cause the network to be difficult to train and
converge. In this paper, we propose a domain adversarial attack (DAA) method to
mitigate the training instability problem by adding perturbations to the input
images, which makes them indistinguishable across domains and enables domain
alignment. Moreover, since models trained on limited data and types of attacks
cannot generalize well to unknown attacks, we propose a dual perceptual and
generative knowledge distillation framework for face anti-spoofing that
utilizes pre-trained face-related models containing rich face priors.
Specifically, we adopt two different face-related models as teachers to
transfer knowledge to the target student model. The pre-trained teacher models
are not from the task of face anti-spoofing but from perceptual and generative
tasks, respectively, which implicitly augment the data. By combining both DAA
and dual-teacher knowledge distillation, we develop a dual teacher knowledge
distillation with domain alignment framework (DTDA) for face anti-spoofing. The
advantage of our proposed method has been verified through extensive ablation
studies and comparison with state-of-the-art methods on public datasets across
multiple protocols.
</p></li>
</ul>

<h3>Title: Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise. (arXiv:2401.01216v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01216">http://arxiv.org/abs/2401.01216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01216]] Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise(http://arxiv.org/abs/2401.01216)</code></li>
<li>Summary: <p>Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.
</p></li>
</ul>

<h3>Title: Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs. (arXiv:2401.00905v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00905">http://arxiv.org/abs/2401.00905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00905]] Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs(http://arxiv.org/abs/2401.00905)</code></li>
<li>Summary: <p>The emergence of large language models (LLMs) has significantly accelerated
the development of a wide range of applications across various fields. There is
a growing trend in the construction of specialized platforms based on LLMs,
such as the newly introduced custom GPTs by OpenAI. While custom GPTs provide
various functionalities like web browsing and code execution, they also
introduce significant security threats. In this paper, we conduct a
comprehensive analysis of the security and privacy issues arising from the
custom GPT platform. Our systematic examination categorizes potential attack
scenarios into three threat models based on the role of the malicious actor,
and identifies critical data exchange channels in custom GPTs. Utilizing the
STRIDE threat modeling framework, we identify 26 potential attack vectors, with
19 being partially or fully validated in real-world settings. Our findings
emphasize the urgent need for robust security and privacy measures in the
custom GPT ecosystem, especially in light of the forthcoming launch of the
official GPT store by OpenAI.
</p></li>
</ul>

<h3>Title: An Interdisciplinary Survey on Information Flows in Supply Chains. (arXiv:2401.01022v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01022">http://arxiv.org/abs/2401.01022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01022]] An Interdisciplinary Survey on Information Flows in Supply Chains(http://arxiv.org/abs/2401.01022)</code></li>
<li>Summary: <p>Supply chains form the backbone of modern economies and therefore require
reliable information flows. In practice, however, supply chains face severe
technical challenges, especially regarding security and privacy. In this work,
we consolidate studies from supply chain management, information systems, and
computer science from 2010-2021 in an interdisciplinary meta-survey to make
this topic holistically accessible to interdisciplinary research. In
particular, we identify a significant potential for computer scientists to
remedy technical challenges and improve the robustness of information flows. We
subsequently present a concise information flow-focused taxonomy for supply
chains before discussing future research directions to provide possible entry
points.
</p></li>
</ul>

<h3>Title: Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00867">http://arxiv.org/abs/2401.00867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00867]] Tensor Networks for Explainable Machine Learning in Cybersecurity(http://arxiv.org/abs/2401.00867)</code></li>
<li>Summary: <p>In this paper we show how tensor networks help in developing explainability
of machine learning algorithms. Specifically, we develop an unsupervised
clustering algorithm based on Matrix Product States (MPS) and apply it in the
context of a real use-case of adversary-generated threat intelligence. Our
investigation proves that MPS rival traditional deep learning models such as
autoencoders and GANs in terms of performance, while providing much richer
model interpretability. Our approach naturally facilitates the extraction of
feature-wise probabilities, Von Neumann Entropy, and mutual information,
offering a compelling narrative for classification of anomalies and fostering
an unprecedented level of transparency and interpretability, something
fundamental to understand the rationale behind artificial intelligence
decisions.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition. (arXiv:2401.00964v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00964">http://arxiv.org/abs/2401.00964</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00964]] Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition(http://arxiv.org/abs/2401.00964)</code></li>
<li>Summary: <p>The recognition of human activities based on WiFi Channel State Information
(CSI) enables contactless and visual privacy-preserving sensing in indoor
environments. However, poor model generalization, due to varying environmental
conditions and sensing hardware, is a well-known problem in this space. To
address this issue, in this work, data augmentation techniques commonly used in
image-based learning are applied to WiFi CSI to investigate their effects on
model generalization performance in cross-scenario and cross-system settings.
In particular, we focus on the generalization between line-of-sight (LOS) and
non-line-of-sight (NLOS) through-wall scenarios, as well as on the
generalization between different antenna systems, which remains under-explored.
We collect and make publicly available a dataset of CSI amplitude spectrograms
of human activities. Utilizing this data, an ablation study is conducted in
which activity recognition models based on the EfficientNetV2 architecture are
trained, allowing us to assess the effects of each augmentation on model
generalization performance. The gathered results show that specific
combinations of simple data augmentation techniques applied to CSI amplitude
data can significantly improve cross-scenario and cross-system generalization.
</p></li>
</ul>

<h3>Title: Teach Large Language Models to Forget Privacy. (arXiv:2401.00870v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00870">http://arxiv.org/abs/2401.00870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00870]] Teach Large Language Models to Forget Privacy(http://arxiv.org/abs/2401.00870)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have proven powerful, but the risk of privacy
leakage remains a significant concern. Traditional privacy-preserving methods,
such as Differential Privacy and Homomorphic Encryption, are inadequate for
black-box API-only settings, demanding either model transparency or heavy
computational resources. We propose Prompt2Forget (P2F), the first framework
designed to tackle the LLM local privacy challenge by teaching LLM to forget.
The method involves decomposing full questions into smaller segments,
generating fabricated answers, and obfuscating the model's memory of the
original input. A benchmark dataset was crafted with questions containing
privacy-sensitive information from diverse fields. P2F achieves zero-shot
generalization, allowing adaptability across a wide range of use cases without
manual adjustments. Experimental results indicate P2F's robust capability to
obfuscate LLM's memory, attaining a forgetfulness score of around 90\% without
any utility loss. This represents an enhancement of up to 63\% when contrasted
with the naive direct instruction technique, highlighting P2F's efficacy in
mitigating memory retention of sensitive information within LLMs. Our findings
establish the first benchmark in the novel field of the LLM forgetting task,
representing a meaningful advancement in privacy preservation in the emerging
LLM domain.
</p></li>
</ul>

<h3>Title: Facebook Report on Privacy of fNIRS data. (arXiv:2401.00973v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00973">http://arxiv.org/abs/2401.00973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00973]] Facebook Report on Privacy of fNIRS data(http://arxiv.org/abs/2401.00973)</code></li>
<li>Summary: <p>The primary goal of this project is to develop privacy-preserving machine
learning model training techniques for fNIRS data. This project will build a
local model in a centralized setting with both differential privacy (DP) and
certified robustness. It will also explore collaborative federated learning to
train a shared model between multiple clients without sharing local fNIRS
datasets. To prevent unintentional private information leakage of such clients'
private datasets, we will also implement DP in the federated learning setting.
</p></li>
</ul>

<h3>Title: PPBFL: A Privacy Protected Blockchain-based Federated Learning Model. (arXiv:2401.01204v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01204">http://arxiv.org/abs/2401.01204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01204]] PPBFL: A Privacy Protected Blockchain-based Federated Learning Model(http://arxiv.org/abs/2401.01204)</code></li>
<li>Summary: <p>With the rapid development of machine learning and growing concerns about
data privacy, federated learning has become an increasingly prominent focus.
However, challenges such as attacks on model parameters and the lack of
incentive mechanisms hinder the effectiveness of federated learning. Therefore,
we propose a Privacy Protected Blockchain-based Federated Learning Model
(PPBFL) to enhance the security of federated learning and promote the active
participation of nodes in model training. Blockchain ensures that model
parameters stored in the InterPlanetary File System (IPFS) remain unaltered. A
novel adaptive differential privacy addition algorithm is simultaneously
applied to local and global models, preserving the privacy of local models and
preventing a decrease in the security of the global model due to the presence
of numerous local models in federated learning. Additionally, we introduce a
new mix transactions mechanism to better protect the identity privacy of local
training clients. Security analysis and experimental results demonstrate that
PPBFL outperforms baseline methods in both model performance and security.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants. (arXiv:2401.00994v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00994">http://arxiv.org/abs/2401.00994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00994]] Detection and Defense Against Prominent Attacks on Preconditioned LLM-Integrated Virtual Assistants(http://arxiv.org/abs/2401.00994)</code></li>
<li>Summary: <p>The emergence of LLM (Large Language Model) integrated virtual assistants has
brought about a rapid transformation in communication dynamics. During virtual
assistant development, some developers prefer to leverage the system message,
also known as an initial prompt or custom prompt, for preconditioning purposes.
However, it is important to recognize that an excessive reliance on this
functionality raises the risk of manipulation by malicious actors who can
exploit it with carefully crafted prompts. Such malicious manipulation poses a
significant threat, potentially compromising the accuracy and reliability of
the virtual assistant's responses. Consequently, safeguarding the virtual
assistants with detection and defense mechanisms becomes of paramount
importance to ensure their safety and integrity. In this study, we explored
three detection and defense mechanisms aimed at countering attacks that target
the system message. These mechanisms include inserting a reference key,
utilizing an LLM evaluator, and implementing a Self-Reminder. To showcase the
efficacy of these mechanisms, they were tested against prominent attack
techniques. Our findings demonstrate that the investigated mechanisms are
capable of accurately identifying and counteracting the attacks. The
effectiveness of these mechanisms underscores their potential in safeguarding
the integrity and reliability of virtual assistants, reinforcing the importance
of their implementation in real-world scenarios. By prioritizing the security
of virtual assistants, organizations can maintain user trust, preserve the
integrity of the application, and uphold the high standards expected in this
era of transformative technologies.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example. (arXiv:2401.01199v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01199">http://arxiv.org/abs/2401.01199</a></li>
<li>Code URL: <a href="https://github.com/guowei-cn/JMA--A-General-Close-to-Optimal-Targeted-Adversarial-Attack-with-Improved-Efficiency">https://github.com/guowei-cn/JMA--A-General-Close-to-Optimal-Targeted-Adversarial-Attack-with-Improved-Efficiency</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01199]] JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example(http://arxiv.org/abs/2401.01199)</code></li>
<li>Summary: <p>Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.
</p></li>
</ul>

<h3>Title: A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models. (arXiv:2401.00991v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00991">http://arxiv.org/abs/2401.00991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00991]] A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models(http://arxiv.org/abs/2401.00991)</code></li>
<li>Summary: <p>Prompt injection attacks exploit vulnerabilities in large language models
(LLMs) to manipulate the model into unintended actions or generate malicious
content. As LLM integrated applications gain wider adoption, they face growing
susceptibility to such attacks. This study introduces a novel evaluation
framework for quantifying the resilience of applications. The framework
incorporates innovative techniques designed to ensure representativeness,
interpretability, and robustness. To ensure the representativeness of simulated
attacks on the application, a meticulous selection process was employed,
resulting in 115 carefully chosen attacks based on coverage and relevance. For
enhanced interpretability, a second LLM was utilized to evaluate the responses
generated from these simulated attacks. Unlike conventional malicious content
classifiers that provide only a confidence score, the LLM-based evaluation
produces a score accompanied by an explanation, thereby enhancing
interpretability. Subsequently, a resilience score is computed by assigning
higher weights to attacks with greater impact, thus providing a robust
measurement of the application resilience. To assess the framework's efficacy,
it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that
Llama2, the newer model exhibited higher resilience compared to ChatGLM. This
finding substantiates the effectiveness of the framework, aligning with the
prevailing notion that newer models tend to possess greater resilience.
Moreover, the framework exhibited exceptional versatility, requiring only
minimal adjustments to accommodate emerging attack techniques and
classifications, thereby establishing itself as an effective and practical
solution. Overall, the framework offers valuable insights that empower
organizations to make well-informed decisions to fortify their applications
against potential threats from prompt injection.
</p></li>
</ul>

<h3>Title: Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control. (arXiv:2401.01085v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01085">http://arxiv.org/abs/2401.01085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01085]] Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control(http://arxiv.org/abs/2401.01085)</code></li>
<li>Summary: <p>Revolutionized by the transformer architecture, natural language processing
(NLP) has received unprecedented attention. While advancements in NLP models
have led to extensive research into their backdoor vulnerabilities, the
potential for these advancements to introduce new backdoor threats remains
unexplored. This paper proposes Imperio, which harnesses the language
understanding capabilities of NLP models to enrich backdoor attacks. Imperio
provides a new model control experience. It empowers the adversary to control
the victim model with arbitrary output through language-guided instructions.
This is achieved using a language model to fuel a conditional trigger
generator, with optimizations designed to extend its language understanding
capabilities to backdoor instruction interpretation and execution. Our
experiments across three datasets, five attacks, and nine defenses confirm
Imperio's effectiveness. It can produce contextually adaptive triggers from
text descriptions and control the victim model with desired outputs, even in
scenarios not encountered during training. The attack maintains a high success
rate across complex datasets without compromising the accuracy of clean inputs
and also exhibits resilience against representative defenses. The source code
is available at \url{https://khchow.com/Imperio}.
</p></li>
</ul>

<h3>Title: Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2401.01304v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01304">http://arxiv.org/abs/2401.01304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01304]] Experimental Validation of Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles(http://arxiv.org/abs/2401.01304)</code></li>
<li>Summary: <p>In this paper, we validate the performance of the a sensor fusion-based
Global Navigation Satellite System (GNSS) spoofing attack detection framework
for Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS
receiver, along with Inertial Measurement Unit (IMU) is used. The detection
framework incorporates two strategies: The first strategy involves comparing
the predicted location shift, which is the distance traveled between two
consecutive timestamps, with the inertial sensor-based location shift. For this
purpose, data from low-cost in-vehicle inertial sensors such as the
accelerometer and gyroscope sensor are fused and fed into a long short-term
memory (LSTM) neural network. The second strategy employs a Random-Forest
supervised machine learning model to detect and classify turns, distinguishing
between left and right turns using the output from the steering angle sensor.
In experiments, two types of spoofing attack models: turn-by-turn and wrong
turn are simulated. These spoofing attacks are modeled as SQL injection
attacks, where, upon successful implementation, the navigation system perceives
injected spoofed location information as legitimate while being unable to
detect legitimate GNSS signals. Importantly, the IMU data remains uncompromised
throughout the spoofing attack. To test the effectiveness of the detection
framework, experiments are conducted in Tuscaloosa, AL, mimicking urban road
structures. The results demonstrate the framework's ability to detect various
sophisticated GNSS spoofing attacks, even including slow position drifting
attacks. Overall, the experimental results showcase the robustness and efficacy
of the sensor fusion-based spoofing attack detection approach in safeguarding
AVs against GNSS spoofing threats.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00897">http://arxiv.org/abs/2401.00897</a></li>
<li>Code URL: <a href="https://github.com/lupin1998/awesome-mim">https://github.com/lupin1998/awesome-mim</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00897]] Masked Modeling for Self-supervised Representation Learning on Vision and Beyond(http://arxiv.org/abs/2401.00897)</code></li>
<li>Summary: <p>As the deep learning revolution marches on, self-supervised learning has
garnered increasing attention in recent years thanks to its remarkable
representation learning ability and the low dependence on labeled data. Among
these varied self-supervised techniques, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training. This paradigm enables deep models to
learn robust representations and has demonstrated exceptional performance in
the context of computer vision, natural language processing, and other
modalities. In this survey, we present a comprehensive review of the masked
modeling framework and its methodology. We elaborate on the details of
techniques within masked modeling, including diverse masking strategies,
recovering targets, network architectures, and more. Then, we systematically
investigate its wide-ranging applications across domains. Furthermore, we also
explore the commonalities and differences between masked modeling methods in
different fields. Toward the end of this paper, we conclude by discussing the
limitations of current techniques and point out several potential avenues for
advancing masked modeling research. A paper list project with this survey is
available at \url{https://github.com/Lupin1998/Awesome-MIM}.
</p></li>
</ul>

<h3>Title: Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models. (arXiv:2401.00988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00988">http://arxiv.org/abs/2401.00988</a></li>
<li>Code URL: <a href="https://github.com/xmed-lab/nuinstruct">https://github.com/xmed-lab/nuinstruct</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00988]] Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models(http://arxiv.org/abs/2401.00988)</code></li>
<li>Summary: <p>The rise of multimodal large language models (MLLMs) has spurred interest in
language-based driving tasks. However, existing research typically focuses on
limited tasks and often omits key multi-view and temporal information which is
crucial for robust autonomous driving. To bridge these gaps, we introduce
NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17
subtasks, where each task demands holistic information (e.g., temporal,
multi-view, and spatial), significantly elevating the challenge level. To
obtain NuInstruct, we propose a novel SQL-based method to generate
instruction-response pairs automatically, which is inspired by the driving
logical progression of humans. We further present BEV-InMLLM, an end-to-end
method for efficiently deriving instruction-aware Bird's-Eye-View (BEV)
features, language-aligned for large language models. BEV-InMLLM integrates
multi-view, spatial awareness, and temporal semantics to enhance MLLMs'
capabilities on NuInstruct tasks. Moreover, our proposed BEV injection module
is a plug-and-play method for existing MLLMs. Our experiments on NuInstruct
demonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.
around 9% improvement on various tasks. We plan to release our NuInstruct for
future research development.
</p></li>
</ul>

<h3>Title: Exploring Hyperspectral Anomaly Detection with Human Vision: A Small Target Aware Detector. (arXiv:2401.01093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01093">http://arxiv.org/abs/2401.01093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01093]] Exploring Hyperspectral Anomaly Detection with Human Vision: A Small Target Aware Detector(http://arxiv.org/abs/2401.01093)</code></li>
<li>Summary: <p>Hyperspectral anomaly detection (HAD) aims to localize pixel points whose
spectral features differ from the background. HAD is essential in scenarios of
unknown or camouflaged target features, such as water quality monitoring, crop
growth monitoring and camouflaged target detection, where prior information of
targets is difficult to obtain. Existing HAD methods aim to objectively detect
and distinguish background and anomalous spectra, which can be achieved almost
effortlessly by human perception. However, the underlying processes of human
visual perception are thought to be quite complex. In this paper, we analyze
hyperspectral image (HSI) features under human visual perception, and transfer
the solution process of HAD to the more robust feature space for the first
time. Specifically, we propose a small target aware detector (STAD), which
introduces saliency maps to capture HSI features closer to human visual
perception. STAD not only extracts more anomalous representations, but also
reduces the impact of low-confidence regions through a proposed small target
filter (STF). Furthermore, considering the possibility of HAD algorithms being
applied to edge devices, we propose a full connected network to convolutional
network knowledge distillation strategy. It can learn the spectral and spatial
features of the HSI while lightening the network. We train the network on the
HAD100 training set and validate the proposed method on the HAD100 test set.
Our method provides a new solution space for HAD that is closer to human visual
perception with high confidence. Sufficient experiments on real HSI with
multiple method comparisons demonstrate the excellent performance and unique
potential of the proposed method. The code is available at
https://github.com/majitao-xd/STAD-HAD.
</p></li>
</ul>

<h3>Title: Robust single-particle cryo-EM image denoising and restoration. (arXiv:2401.01097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01097">http://arxiv.org/abs/2401.01097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01097]] Robust single-particle cryo-EM image denoising and restoration(http://arxiv.org/abs/2401.01097)</code></li>
<li>Summary: <p>Cryo-electron microscopy (cryo-EM) has achieved near-atomic level resolution
of biomolecules by reconstructing 2D micrographs. However, the resolution and
accuracy of the reconstructed particles are significantly reduced due to the
extremely low signal-to-noise ratio (SNR) and complex noise structure of
cryo-EM images. In this paper, we introduce a diffusion model with
post-processing framework to effectively denoise and restore single particle
cryo-EM images. Our method outperforms the state-of-the-art (SOTA) denoising
methods by effectively removing structural noise that has not been addressed
before. Additionally, more accurate and high-resolution three-dimensional
reconstruction structures can be obtained from denoised cryo-EM images.
</p></li>
</ul>

<h3>Title: Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01179">http://arxiv.org/abs/2401.01179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01179]] Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training(http://arxiv.org/abs/2401.01179)</code></li>
<li>Summary: <p>Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.
</p></li>
</ul>

<h3>Title: Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label Classification. (arXiv:2401.01181v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01181">http://arxiv.org/abs/2401.01181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01181]] Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label Classification(http://arxiv.org/abs/2401.01181)</code></li>
<li>Summary: <p>Identifying labels that did not appear during training, known as multi-label
zero-shot learning, is a non-trivial task in computer vision. To this end,
recent studies have attempted to explore the multi-modal knowledge of
vision-language pre-training (VLP) models by knowledge distillation, allowing
to recognize unseen labels in an open-vocabulary manner. However, experimental
evidence shows that knowledge distillation is suboptimal and provides limited
performance gain in unseen label prediction. In this paper, a novel query-based
knowledge sharing paradigm is proposed to explore the multi-modal knowledge
from the pretrained VLP model for open-vocabulary multi-label classification.
Specifically, a set of learnable label-agnostic query tokens is trained to
extract critical vision knowledge from the input image, and further shared
across all labels, allowing them to select tokens of interest as visual clues
for recognition. Besides, we propose an effective prompt pool for robust label
embedding, and reformulate the standard ranking learning into a form of
classification to allow the magnitude of feature vectors for matching, which
both significantly benefit label recognition. Experimental results show that
our framework significantly outperforms state-of-the-art methods on zero-shot
task by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.
</p></li>
</ul>

<h3>Title: Robust Meta-Model for Predicting the Need for Blood Transfusion in Non-traumatic ICU Patients. (arXiv:2401.00972v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00972">http://arxiv.org/abs/2401.00972</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00972]] Robust Meta-Model for Predicting the Need for Blood Transfusion in Non-traumatic ICU Patients(http://arxiv.org/abs/2401.00972)</code></li>
<li>Summary: <p>Objective: Blood transfusions, crucial in managing anemia and coagulopathy in
ICU settings, require accurate prediction for effective resource allocation and
patient risk assessment. However, existing clinical decision support systems
have primarily targeted a particular patient demographic with unique medical
conditions and focused on a single type of blood transfusion. This study aims
to develop an advanced machine learning-based model to predict the probability
of transfusion necessity over the next 24 hours for a diverse range of
non-traumatic ICU patients.
</p>
<p>Methods: We conducted a retrospective cohort study on 72,072 adult
non-traumatic ICU patients admitted to a high-volume US metropolitan academic
hospital between 2016 and 2020. We developed a meta-learner and various machine
learning models to serve as predictors, training them annually with four-year
data and evaluating on the fifth, unseen year, iteratively over five years.
</p>
<p>Results: The experimental results revealed that the meta-model surpasses the
other models in different development scenarios. It achieved notable
performance metrics, including an Area Under the Receiver Operating
Characteristic (AUROC) curve of 0.97, an accuracy rate of 0.93, and an F1-score
of 0.89 in the best scenario.
</p>
<p>Conclusion: This study pioneers the use of machine learning models for
predicting blood transfusion needs in a diverse cohort of critically ill
patients. The findings of this evaluation confirm that our model not only
predicts transfusion requirements effectively but also identifies key
biomarkers for making transfusion decisions.
</p></li>
</ul>

<h3>Title: Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning. (arXiv:2401.01013v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01013">http://arxiv.org/abs/2401.01013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01013]] Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning(http://arxiv.org/abs/2401.01013)</code></li>
<li>Summary: <p>Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU)
has revealed that traditional machine learning methods, such as semi-supervised
label propagation and K-nearest neighbors, outperform Transformer-based models
in artifact detection from PPG signals, mainly when data is limited. This study
addresses the underutilization of abundant unlabeled data by employing
self-supervised learning (SSL) to extract latent features from these data,
followed by fine-tuning on labeled data. Our experiments demonstrate that SSL
significantly enhances the Transformer model's ability to learn
representations, improving its robustness in artifact classification tasks.
Among various SSL techniques, including masking, contrastive learning, and DINO
(self-distillation with no labels)-contrastive learning exhibited the most
stable and superior performance in small PPG datasets. Further, we delve into
optimizing contrastive loss functions, which are crucial for contrastive SSL.
Inspired by InfoNCE, we introduce a novel contrastive loss function that
facilitates smoother training and better convergence, thereby enhancing
performance in artifact classification. In summary, this study establishes the
efficacy of SSL in leveraging unlabeled data, particularly in enhancing the
capabilities of the Transformer model. This approach holds promise for broader
applications in PICU environments, where annotated data is often limited.
</p></li>
</ul>

<h3>Title: Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01100">http://arxiv.org/abs/2401.01100</a></li>
<li>Code URL: <a href="https://github.com/zpguigroupwhu/scml">https://github.com/zpguigroupwhu/scml</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01100]] Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding(http://arxiv.org/abs/2401.01100)</code></li>
<li>Summary: <p>As a pivotal approach in machine learning and data science, manifold learning
aims to uncover the intrinsic low-dimensional structure within complex
nonlinear manifolds in high-dimensional space. By exploiting the manifold
hypothesis, various techniques for nonlinear dimension reduction have been
developed to facilitate visualization, classification, clustering, and gaining
key insights. Although existing manifold learning methods have achieved
remarkable successes, they still suffer from extensive distortions incurred in
the global structure, which hinders the understanding of underlying patterns.
Scalability issues also limit their applicability for handling large-scale
data. Here, we propose a scalable manifold learning (scML) method that can
manipulate large-scale and high-dimensional data in an efficient manner. It
starts by seeking a set of landmarks to construct the low-dimensional skeleton
of the entire data and then incorporates the non-landmarks into the landmark
space based on the constrained locally linear embedding (CLLE). We empirically
validated the effectiveness of scML on synthetic datasets and real-world
benchmarks of different types, and applied it to analyze the single-cell
transcriptomics and detect anomalies in electrocardiogram (ECG) signals. scML
scales well with increasing data sizes and exhibits promising performance in
preserving the global structure. The experiments demonstrate notable robustness
in embedding quality as the sample rate decreases.
</p></li>
</ul>

<h3>Title: Reinforcement Learning for SAR View Angle Inversion with Differentiable SAR Renderer. (arXiv:2401.01165v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01165">http://arxiv.org/abs/2401.01165</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01165]] Reinforcement Learning for SAR View Angle Inversion with Differentiable SAR Renderer(http://arxiv.org/abs/2401.01165)</code></li>
<li>Summary: <p>The electromagnetic inverse problem has long been a research hotspot. This
study aims to reverse radar view angles in synthetic aperture radar (SAR)
images given a target model. Nonetheless, the scarcity of SAR data, combined
with the intricate background interference and imaging mechanisms, limit the
applications of existing learning-based approaches. To address these
challenges, we propose an interactive deep reinforcement learning (DRL)
framework, where an electromagnetic simulator named differentiable SAR render
(DSR) is embedded to facilitate the interaction between the agent and the
environment, simulating a human-like process of angle prediction. Specifically,
DSR generates SAR images at arbitrary view angles in real-time. And the
differences in sequential and semantic aspects between the view
angle-corresponding images are leveraged to construct the state space in DRL,
which effectively suppress the complex background interference, enhance the
sensitivity to temporal variations, and improve the capability to capture
fine-grained information. Additionally, in order to maintain the stability and
convergence of our method, a series of reward mechanisms, such as memory
difference, smoothing and boundary penalty, are utilized to form the final
reward function. Extensive experiments performed on both simulated and real
datasets demonstrate the effectiveness and robustness of our proposed method.
When utilized in the cross-domain area, the proposed method greatly mitigates
inconsistency between simulated and real domains, outperforming reference
methods significantly.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans. (arXiv:2401.01201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01201">http://arxiv.org/abs/2401.01201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01201]] Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans(http://arxiv.org/abs/2401.01201)</code></li>
<li>Summary: <p>The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images. In this
paper, we introduce a paradigm shift that attains human-level performance in
biometric measurement by aggregating automatically extracted biometrics from
every frame across an entire scan, with no need for operator intervention. We
use a convolutional neural network to classify each frame of an ultrasound
video recording. We then measure fetal biometrics in every frame where
appropriate anatomy is visible. We use a Bayesian method to estimate the true
value of each biometric from a large number of measurements and
probabilistically reject outliers. We performed a retrospective experiment on
1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,
estimated fetal biometrics in those scans and compared our estimates to the
measurements sonographers took during the scan. Our method achieves human-level
performance in estimating fetal biometrics and estimates well-calibrated
credible intervals in which the true biometric value is expected to lie.
</p></li>
</ul>

<h3>Title: IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01227">http://arxiv.org/abs/2401.01227</a></li>
<li>Code URL: <a href="https://github.com/MahmoudRabea13/IdentiFace">https://github.com/MahmoudRabea13/IdentiFace</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01227]] IdentiFace : A VGG Based Multimodal Facial Biometric System(http://arxiv.org/abs/2401.01227)</code></li>
<li>Summary: <p>The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there's always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce "IdentiFace" which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00926">http://arxiv.org/abs/2401.00926</a></li>
<li>Code URL: <a href="https://github.com/justlfc03/mfds-detr">https://github.com/justlfc03/mfds-detr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00926]] Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases(http://arxiv.org/abs/2401.00926)</code></li>
<li>Summary: <p>In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</p></li>
</ul>

<h3>Title: BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01065">http://arxiv.org/abs/2401.01065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01065]] BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving(http://arxiv.org/abs/2401.01065)</code></li>
<li>Summary: <p>The demand for the retrieval of complex scene data in autonomous driving is
increasing, especially as passenger vehicles have been equipped with the
ability to navigate urban settings, with the imperative to address long-tail
scenarios. Meanwhile, under the pre-existing two dimensional image retrieval
method, some problems may arise with scene retrieval, such as lack of global
feature representation and subpar text retrieval ability. To address these
issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-Eye
View(BEV) retrieval methodology that utilizes descriptive text as an input to
retrieve corresponding scenes. This methodology applies the semantic feature
extraction abilities of a large language model (LLM) to facilitate zero-shot
retrieval of extensive text descriptions, and incorporates semi-structured
information from a knowledge graph to improve the semantic richness and variety
of the language embedding. Our experiments result in 87.66% accuracy on
NuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases in
our paper support that our retrieval method is also indicated to be effective
in identifying certain long-tail corner scenes.
</p></li>
</ul>

<h3>Title: GBSS:a global building semantic segmentation dataset for large-scale remote sensing building extraction. (arXiv:2401.01178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01178">http://arxiv.org/abs/2401.01178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01178]] GBSS:a global building semantic segmentation dataset for large-scale remote sensing building extraction(http://arxiv.org/abs/2401.01178)</code></li>
<li>Summary: <p>Semantic segmentation techniques for extracting building footprints from
high-resolution remote sensing images have been widely used in many fields such
as urban planning. However, large-scale building extraction demands higher
diversity in training samples. In this paper, we construct a Global Building
Semantic Segmentation (GBSS) dataset (The dataset will be released), which
comprises 116.9k pairs of samples (about 742k buildings) from six continents.
There are significant variations of building samples in terms of size and
style, so the dataset can be a more challenging benchmark for evaluating the
generalization and robustness of building semantic segmentation models. We
validated through quantitative and qualitative comparisons between different
datasets, and further confirmed the potential application in the field of
transfer learning by conducting experiments on subsets.
</p></li>
</ul>

<h3>Title: Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms. (arXiv:2401.01200v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01200">http://arxiv.org/abs/2401.01200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01200]] Skin cancer diagnosis using NIR spectroscopy data of skin lesions in vivo using machine learning algorithms(http://arxiv.org/abs/2401.01200)</code></li>
<li>Summary: <p>Skin lesions are classified in benign or malignant. Among the malignant,
melanoma is a very aggressive cancer and the major cause of deaths. So, early
diagnosis of skin cancer is very desired. In the last few years, there is a
growing interest in computer aided diagnostic (CAD) using most image and
clinical data of the lesion. These sources of information present limitations
due to their inability to provide information of the molecular structure of the
lesion. NIR spectroscopy may provide an alternative source of information to
automated CAD of skin lesions. The most commonly used techniques and
classification algorithms used in spectroscopy are Principal Component Analysis
(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support
Vector Machines (SVM). Nonetheless, there is a growing interest in applying the
modern techniques of machine and deep learning (MDL) to spectroscopy. One of
the main limitations to apply MDL to spectroscopy is the lack of public
datasets. Since there is no public dataset of NIR spectral data to skin
lesions, as far as we know, an effort has been made and a new dataset named
NIR-SC-UFES, has been collected, annotated and analyzed generating the
gold-standard for classification of NIR spectral data to skin cancer. Next, the
machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional
neural network (1D-CNN) were investigated to classify cancer and non-cancer
skin lesions. Experimental results indicate the best performance obtained by
LightGBM with pre-processing using standard normal variate (SNV), feature
extraction providing values of 0.839 for balanced accuracy, 0.851 for recall,
0.852 for precision, and 0.850 for F-score. The obtained results indicate the
first steps in CAD of skin lesions aiming the automated triage of patients with
skin lesions in vivo using NIR spectral data.
</p></li>
</ul>

<h3>Title: FGENet: Fine-Grained Extraction Network for Congested Crowd Counting. (arXiv:2401.01208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01208">http://arxiv.org/abs/2401.01208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01208]] FGENet: Fine-Grained Extraction Network for Congested Crowd Counting(http://arxiv.org/abs/2401.01208)</code></li>
<li>Summary: <p>Crowd counting has gained significant popularity due to its practical
applications. However, mainstream counting methods ignore precise individual
localization and suffer from annotation noise because of counting from
estimating density maps. Additionally, they also struggle with high-density
images.To address these issues, we propose an end-to-end model called
Fine-Grained Extraction Network (FGENet). Different from methods estimating
density maps, FGENet directly learns the original coordinate points that
represent the precise localization of individuals.This study designs a fusion
module, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature
maps extracted by the backbone of FGENet. The fused features are then passed to
both regression and classification heads, where the former provides predicted
point coordinates for a given image, and the latter determines the confidence
level for each predicted point being an individual. At the end, FGENet
establishes correspondences between prediction points and ground truth points
by employing the Hungarian algorithm. For training FGENet, we design a robust
loss function, named Three-Task Combination (TTC), to mitigate the impact of
annotation noise. Extensive experiments are conducted on four widely used crowd
counting datasets. Experimental results demonstrate the effectiveness of
FGENet. Notably, our method achieves a remarkable improvement of 3.14 points in
Mean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its
superiority over the existing state-of-the-art methods. Even more impressively,
FGENet surpasses previous benchmarks on the UCF\_CC\_50 dataset with an
astounding enhancement of 30.16 points in MAE.
</p></li>
</ul>

<h3>Title: Temporal Adaptive RGBT Tracking with Modality Prompt. (arXiv:2401.01244v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01244">http://arxiv.org/abs/2401.01244</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01244]] Temporal Adaptive RGBT Tracking with Modality Prompt(http://arxiv.org/abs/2401.01244)</code></li>
<li>Summary: <p>RGBT tracking has been widely used in various fields such as robotics,
surveillance processing, and autonomous driving. Existing RGBT trackers fully
explore the spatial information between the template and the search region and
locate the target based on the appearance matching results. However, these RGBT
trackers have very limited exploitation of temporal information, either
ignoring temporal information or exploiting it through online sampling and
training. The former struggles to cope with the object state changes, while the
latter neglects the correlation between spatial and temporal information. To
alleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking
framework, named as TATrack. TATrack has a spatio-temporal two-stream structure
and captures temporal information by an online updated template, where the
two-stream structure refers to the multi-modal feature extraction and
cross-modal interaction for the initial template and the online update template
respectively. TATrack contributes to comprehensively exploit spatio-temporal
information and multi-modal information for target localization. In addition,
we design a spatio-temporal interaction (STI) mechanism that bridges two
branches and enables cross-modal interaction to span longer time scales.
Extensive experiments on three popular RGBT tracking benchmarks show that our
method achieves state-of-the-art performance, while running at real-time speed.
</p></li>
</ul>

<h3>Title: CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation. (arXiv:2401.01275v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01275">http://arxiv.org/abs/2401.01275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01275]] CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation(http://arxiv.org/abs/2401.01275)</code></li>
<li>Summary: <p>Recently, the advent of large language models (LLMs) has revolutionized
generative agents. Among them, Role-Playing Conversational Agents (RPCAs)
attract considerable attention due to their ability to emotionally engage
users. However, the absence of a comprehensive benchmark impedes progress in
this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark
for comprehensive RPCA assessment, complemented by a tailored high-quality
dataset. The dataset comprises 1,785 multi-turn role-playing dialogues,
encompassing 23,020 examples and featuring 77 characters derived from Chinese
novels and scripts. It was carefully constructed, beginning with initial
dialogue extraction via GPT-4, followed by rigorous human-led quality control,
and enhanced with in-depth character profiles sourced from Baidu Baike.
CharacterEval employs a multifaceted evaluation approach, encompassing thirteen
targeted metrics on four dimensions. Comprehensive experiments on CharacterEval
demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in
Chinese role-playing conversation. Source code, data source and reward model
will be publicly accessible at https://github.com/morecry/CharacterEval.
</p></li>
</ul>

<h3>Title: An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01326">http://arxiv.org/abs/2401.01326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01326]] An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction(http://arxiv.org/abs/2401.01326)</code></li>
<li>Summary: <p>In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Balanced Multi-modal Federated Learning via Cross-Modal Infiltration. (arXiv:2401.00894v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00894">http://arxiv.org/abs/2401.00894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00894]] Balanced Multi-modal Federated Learning via Cross-Modal Infiltration(http://arxiv.org/abs/2401.00894)</code></li>
<li>Summary: <p>Federated learning (FL) underpins advancements in privacy-preserving
distributed computing by collaboratively training neural networks without
exposing clients' raw data. Current FL paradigms primarily focus on uni-modal
data, while exploiting the knowledge from distributed multimodal data remains
largely unexplored. Existing multimodal FL (MFL) solutions are mainly designed
for statistical or modality heterogeneity from the input side, however, have
yet to solve the fundamental issue,"modality imbalance", in distributed
conditions, which can lead to inadequate information exploitation and
heterogeneous knowledge aggregation on different modalities.In this paper, we
propose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework
that effectively alleviates modality imbalance and knowledge heterogeneity via
knowledge transfer from the global dominant modality. To avoid the loss of
information in the weak modality due to merely imitating the behavior of
dominant modality, we design the two-projector module to integrate the
knowledge from dominant modality while still promoting the local feature
exploitation of weak modality. In addition, we introduce a class-wise
temperature adaptation scheme to achieve fair performance across different
classes. Extensive experiments over popular datasets are conducted and give us
a gratifying confirmation of the proposed framework for fully exploring the
information of each modality in MFL.
</p></li>
</ul>

<h3>Title: FedQV: Leveraging Quadratic Voting in Federated Learning. (arXiv:2401.01168v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01168">http://arxiv.org/abs/2401.01168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01168]] FedQV: Leveraging Quadratic Voting in Federated Learning(http://arxiv.org/abs/2401.01168)</code></li>
<li>Summary: <p>Federated Learning (FL) permits different parties to collaboratively train a
global model without disclosing their respective local labels. A crucial step
of FL, that of aggregating local models to produce the global one, shares many
similarities with public decision-making, and elections in particular. In that
context, a major weakness of FL, namely its vulnerability to poisoning attacks,
can be interpreted as a consequence of the one person one vote (henceforth
1p1v) principle underpinning most contemporary aggregation rules. In this
paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic
voting scheme, recently proposed as a better alternative to 1p1v-based
elections. Our theoretical analysis establishes that FedQV is a truthful
mechanism in which bidding according to one's true valuation is a dominant
strategy that achieves a convergence rate that matches those of
state-of-the-art methods. Furthermore, our empirical analysis using multiple
real-world datasets validates the superior performance of FedQV against
poisoning attacks. It also shows that combining FedQV with unequal voting
``budgets'' according to a reputation score increases its performance benefits
even further. Finally, we show that FedQV can be easily combined with
Byzantine-robust privacy-preserving mechanisms to enhance its robustness
against both poisoning and privacy attacks.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01262">http://arxiv.org/abs/2401.01262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01262]] Fairness Certification for Natural Language Processing and Large Language Models(http://arxiv.org/abs/2401.01262)</code></li>
<li>Summary: <p>Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</p></li>
</ul>

<h3>Title: Evaluating the Fairness of the MIMIC-IV Dataset and a Baseline Algorithm: Application to the ICU Length of Stay Prediction. (arXiv:2401.00902v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00902">http://arxiv.org/abs/2401.00902</a></li>
<li>Code URL: <a href="https://github.com/akakadiaris/fairnessofmimiciv">https://github.com/akakadiaris/fairnessofmimiciv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00902]] Evaluating the Fairness of the MIMIC-IV Dataset and a Baseline Algorithm: Application to the ICU Length of Stay Prediction(http://arxiv.org/abs/2401.00902)</code></li>
<li>Summary: <p>This paper uses the MIMIC-IV dataset to examine the fairness and bias in an
XGBoost binary classification model predicting the Intensive Care Unit (ICU)
length of stay (LOS). Highlighting the critical role of the ICU in managing
critically ill patients, the study addresses the growing strain on ICU
capacity. It emphasizes the significance of LOS prediction for resource
allocation. The research reveals class imbalances in the dataset across
demographic attributes and employs data preprocessing and feature extraction.
While the XGBoost model performs well overall, disparities across race and
insurance attributes reflect the need for tailored assessments and continuous
monitoring. The paper concludes with recommendations for fairness-aware machine
learning techniques for mitigating biases and the need for collaborative
efforts among healthcare professionals and data scientists.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Explainable Adaptive Tree-based Model Selection for Time Series Forecasting. (arXiv:2401.01124v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01124">http://arxiv.org/abs/2401.01124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01124]] Explainable Adaptive Tree-based Model Selection for Time Series Forecasting(http://arxiv.org/abs/2401.01124)</code></li>
<li>Summary: <p>Tree-based models have been successfully applied to a wide variety of tasks,
including time series forecasting. They are increasingly in demand and widely
accepted because of their comparatively high level of interpretability.
However, many of them suffer from the overfitting problem, which limits their
application in real-world decision-making. This problem becomes even more
severe in online-forecasting settings where time series observations are
incrementally acquired, and the distributions from which they are drawn may
keep changing over time. In this context, we propose a novel method for the
online selection of tree-based models using the TreeSHAP explainability method
in the task of time series forecasting. We start with an arbitrary set of
different tree-based models. Then, we outline a performance-based ranking with
a coherent design to make TreeSHAP able to specialize the tree-based
forecasters across different regions in the input time series. In this
framework, adequate model selection is performed online, adaptively following
drift detection in the time series. In addition, explainability is supported on
three levels, namely online input importance, model selection, and model output
explanation. An extensive empirical study on various real-world datasets
demonstrates that our method achieves excellent or on-par results in comparison
to the state-of-the-art approaches as well as several baselines.
</p></li>
</ul>

<h3>Title: Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01259">http://arxiv.org/abs/2401.01259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01259]] Do Concept Bottleneck Models Obey Locality?(http://arxiv.org/abs/2401.01259)</code></li>
<li>Summary: <p>Concept-based learning improves a deep learning model's interpretability by
explaining its predictions via human-understandable concepts. Deep learning
models trained under this paradigm heavily rely on the assumption that neural
networks can learn to predict the presence or absence of a given concept
independently of other concepts. Recent work, however, strongly suggests that
this assumption may fail to hold in Concept Bottleneck Models (CBMs), a
quintessential family of concept-based interpretable architectures. In this
paper, we investigate whether CBMs correctly capture the degree of conditional
independence across concepts when such concepts are localised both spatially,
by having their values entirely defined by a fixed subset of features, and
semantically, by having their values correlated with only a fixed subset of
predefined concepts. To understand locality, we analyse how changes to features
outside of a concept's spatial or semantic locality impact concept predictions.
Our results suggest that even in well-defined scenarios where the presence of a
concept is localised to a fixed feature subspace, or whose semantics are
correlated to a small subset of other concepts, CBMs fail to learn this
locality. These results cast doubt upon the quality of concept representations
learnt by CBMs and strongly suggest that concept-based explanations may be
fragile to changes outside their localities.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: FlashVideo: A Framework for Swift Inference in Text-to-Video Generation. (arXiv:2401.00869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00869">http://arxiv.org/abs/2401.00869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00869]] FlashVideo: A Framework for Swift Inference in Text-to-Video Generation(http://arxiv.org/abs/2401.00869)</code></li>
<li>Summary: <p>In the evolving field of machine learning, video generation has witnessed
significant advancements with autoregressive-based transformer models and
diffusion models, known for synthesizing dynamic and realistic scenes. However,
these models often face challenges with prolonged inference times, even for
generating short video clips such as GIFs. This paper introduces FlashVideo, a
novel framework tailored for swift Text-to-Video generation. FlashVideo
represents the first successful adaptation of the RetNet architecture for video
generation, bringing a unique approach to the field. Leveraging the
RetNet-based architecture, FlashVideo reduces the time complexity of inference
from $\mathcal{O}(L^2)$ to $\mathcal{O}(L)$ for a sequence of length $L$,
significantly accelerating inference speed. Additionally, we adopt a
redundant-free frame interpolation method, enhancing the efficiency of frame
interpolation. Our comprehensive experiments demonstrate that FlashVideo
achieves a $\times9.17$ efficiency improvement over a traditional
autoregressive-based transformer model, and its inference speed is of the same
order of magnitude as that of BERT-based transformer models.
</p></li>
</ul>

<h3>Title: TrailBlazer: Trajectory Control for Diffusion-Based Video Generation. (arXiv:2401.00896v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00896">http://arxiv.org/abs/2401.00896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00896]] TrailBlazer: Trajectory Control for Diffusion-Based Video Generation(http://arxiv.org/abs/2401.00896)</code></li>
<li>Summary: <p>Within recent approaches to text-to-video (T2V) generation, achieving
controllability in the synthesized video is often a challenge. Typically, this
issue is addressed by providing low-level per-frame guidance in the form of
edge maps, depth maps, or an existing video to be altered. However, the process
of obtaining such guidance can be labor-intensive. This paper focuses on
enhancing controllability in video synthesis by employing straightforward
bounding boxes to guide the subject in various ways, all without the need for
neural network training, finetuning, optimization at inference time, or the use
of pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a
pre-trained (T2V) model, and easy to implement. The subject is directed by a
bounding box through the proposed spatial and temporal attention map editing.
Moreover, we introduce the concept of keyframing, allowing the subject
trajectory and overall appearance to be guided by both a moving bounding box
and corresponding prompts, without the need to provide a detailed mask. The
method is efficient, with negligible additional computation relative to the
underlying pre-trained model. Despite the simplicity of the bounding box
guidance, the resulting motion is surprisingly natural, with emergent effects
including perspective and movement toward the virtual camera as the box size
increases.
</p></li>
</ul>

<h3>Title: Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01008">http://arxiv.org/abs/2401.01008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01008]] Fast Inference Through The Reuse Of Attention Maps In Diffusion Models(http://arxiv.org/abs/2401.01008)</code></li>
<li>Summary: <p>Text-to-image diffusion models have demonstrated unprecedented abilities at
flexible and realistic image synthesis. However, the iterative process required
to produce a single image is costly and incurs a high latency, prompting
researchers to further investigate its efficiency. Typically, improvements in
latency have been achieved in two ways: (1) training smaller models through
knowledge distillation (KD); and (2) adopting techniques from ODE-theory to
facilitate larger step sizes. In contrast, we propose a training-free approach
that does not alter the step-size of the sampler. Specifically, we find the
repeated calculation of attention maps to be both costly and redundant;
therefore, we propose a structured reuse of attention maps during sampling. Our
initial reuse policy is motivated by rudimentary ODE-theory, which suggests
that reuse is most suitable late in the sampling procedure. After noting a
number of limitations in this theoretical approach, we empirically search for a
better policy. Unlike methods that rely on KD, our reuse policies can easily be
adapted to a variety of setups in a plug-and-play manner. Furthermore, when
applied to Stable Diffusion-1.5, our reuse policies reduce latency with minimal
repercussions on sample quality.
</p></li>
</ul>

<h3>Title: Joint Generative Modeling of Scene Graphs and Images via Diffusion Models. (arXiv:2401.01130v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01130">http://arxiv.org/abs/2401.01130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01130]] Joint Generative Modeling of Scene Graphs and Images via Diffusion Models(http://arxiv.org/abs/2401.01130)</code></li>
<li>Summary: <p>In this paper, we present a novel generative task: joint scene graph - image
generation. While previous works have explored image generation conditioned on
scene graphs or layouts, our task is distinctive and important as it involves
generating scene graphs themselves unconditionally from noise, enabling
efficient and interpretable control for image generation. Our task is
challenging, requiring the generation of plausible scene graphs with
heterogeneous attributes for nodes (objects) and edges (relations among
objects), including continuous object bounding boxes and discrete object and
relation categories. We introduce a novel diffusion model, DiffuseSG, that
jointly models the adjacency matrix along with heterogeneous node and edge
attributes. We explore various types of encodings for the categorical data,
relaxing it into a continuous space. With a graph transformer being the
denoiser, DiffuseSG successively denoises the scene graph representation in a
continuous space and discretizes the final representation to generate the clean
scene graph. Additionally, we introduce an IoU regularization to enhance the
empirical performance. Our model significantly outperforms existing methods in
scene graph generation on the Visual Genome and COCO-Stuff datasets, both on
standard and newly introduced metrics that better capture the problem
complexity. Moreover, we demonstrate the additional benefits of our model in
two downstream applications: 1) excelling in a series of scene graph completion
tasks, and 2) improving scene graph detection models by using extra training
samples generated from DiffuseSG.
</p></li>
</ul>

<h3>Title: Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation. (arXiv:2401.01207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01207">http://arxiv.org/abs/2401.01207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01207]] Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation(http://arxiv.org/abs/2401.01207)</code></li>
<li>Summary: <p>In human-centric content generation, the pre-trained text-to-image models
struggle to produce user-wanted portrait images, which retain the identity of
individuals while exhibiting diverse expressions. This paper introduces our
efforts towards personalized face generation. To this end, we propose a novel
multi-modal face generation framework, capable of simultaneous
identity-expression control and more fine-grained expression synthesis. Our
expression control is so sophisticated that it can be specialized by the
fine-grained emotional vocabulary. We devise a novel diffusion model that can
undertake the task of simultaneously face swapping and reenactment. Due to the
entanglement of identity and expression, it's nontrivial to separately and
precisely control them in one framework, thus has not been explored yet. To
overcome this, we propose several innovative designs in the conditional
diffusion model, including balancing identity and expression encoder, improved
midpoint sampling, and explicitly background conditioning. Extensive
experiments have demonstrated the controllability and scalability of the
proposed framework, in comparison with state-of-the-art text-to-image, face
swapping, and face reenactment methods.
</p></li>
</ul>

<h3>Title: VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM. (arXiv:2401.01256v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01256">http://arxiv.org/abs/2401.01256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01256]] VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM(http://arxiv.org/abs/2401.01256)</code></li>
<li>Summary: <p>The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoDrafter, for content-consistent multi-scene video
generation. Technically, VideoDrafter leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoDrafter identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoDrafter outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoDrafter outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: 3D Human Pose Perception from Egocentric Stereo Videos. (arXiv:2401.00889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00889">http://arxiv.org/abs/2401.00889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00889]] 3D Human Pose Perception from Egocentric Stereo Videos(http://arxiv.org/abs/2401.00889)</code></li>
<li>Summary: <p>While head-mounted devices are becoming more compact, they provide egocentric
views with significant self-occlusions of the device user. Hence, existing
methods often fail to accurately estimate complex 3D poses from egocentric
views. In this work, we propose a new transformer-based framework to improve
egocentric stereo 3D human pose estimation, which leverages the scene
information and temporal context of egocentric stereo videos. Specifically, we
utilize 1) depth features from our 3D scene reconstruction module with
uniformly sampled windows of egocentric stereo frames, and 2) human joint
queries enhanced by temporal features of the video inputs. Our method is able
to accurately estimate human poses even in challenging scenarios, such as
crouching and sitting. Furthermore, we introduce two new benchmark datasets,
i.e., UnrealEgo2 and UnrealEgo-RW (RealWorld). The proposed datasets offer a
much larger number of egocentric stereo views with a wider variety of human
motions than the existing datasets, allowing comprehensive evaluation of
existing and upcoming methods. Our extensive experiments show that the proposed
approach significantly outperforms previous methods. We will release
UnrealEgo2, UnrealEgo-RW, and trained models on our project page.
</p></li>
</ul>

<h3>Title: ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention. (arXiv:2401.00912v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00912">http://arxiv.org/abs/2401.00912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00912]] ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention(http://arxiv.org/abs/2401.00912)</code></li>
<li>Summary: <p>Window-based transformers have demonstrated strong ability in large-scale
point cloud understanding by capturing context-aware representations with
affordable attention computation in a more localized manner. However, because
of the sparse nature of point clouds, the number of voxels per window varies
significantly. Current methods partition the voxels in each window into
multiple subsets of equal size, which cost expensive overhead in sorting and
padding the voxels, making them run slower than sparse convolution based
methods. In this paper, we present ScatterFormer, which, for the first time to
our best knowledge, could directly perform attention on voxel sets with
variable length. The key of ScatterFormer lies in the innovative Scatter Linear
Attention (SLA) module, which leverages the linear attention mechanism to
process in parallel all voxels scattered in different windows. Harnessing the
hierarchical computation units of the GPU and matrix blocking algorithm, we
reduce the latency of the proposed SLA module to less than 1 ms on moderate
GPUs. Besides, we develop a cross-window interaction module to simultaneously
enhance the local representation and allow the information flow across windows,
eliminating the need for window shifting. Our proposed ScatterFormer
demonstrates 73 mAP (L2) on the large-scale Waymo Open Dataset and 70.5 NDS on
the NuScenes dataset, running at an outstanding detection rate of 28 FPS. Code
is available at https://github.com/skyhehe123/ScatterFormer
</p></li>
</ul>

<h3>Title: Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence. (arXiv:2401.00921v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00921">http://arxiv.org/abs/2401.00921</a></li>
<li>Code URL: <a href="https://github.com/ruizhuo-xu/skeleton2vec">https://github.com/ruizhuo-xu/skeleton2vec</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00921]] Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence(http://arxiv.org/abs/2401.00921)</code></li>
<li>Summary: <p>Self-supervised pre-training paradigms have been extensively explored in the
field of skeleton-based action recognition. In particular, methods based on
masked prediction have pushed the performance of pre-training to a new height.
However, these methods take low-level features, such as raw joint coordinates
or temporal motion, as prediction targets for the masked regions, which is
suboptimal. In this paper, we show that using high-level contextualized
features as prediction targets can achieve superior performance. Specifically,
we propose Skeleton2vec, a simple and efficient self-supervised 3D action
representation learning framework, which utilizes a transformer-based teacher
encoder taking unmasked training samples as input to create latent
contextualized representations as prediction targets. Benefiting from the
self-attention mechanism, the latent representations generated by the teacher
encoder can incorporate the global context of the entire training samples,
leading to a richer training task. Additionally, considering the high temporal
correlations in skeleton sequences, we propose a motion-aware tube masking
strategy which divides the skeleton sequence into several tubes and performs
persistent masking within each tube based on motion priors, thus forcing the
model to build long-range spatio-temporal connections and focus on
action-semantic richer regions. Extensive experiments on NTU-60, NTU-120, and
PKU-MMD datasets demonstrate that our proposed Skeleton2vec outperforms
previous methods and achieves state-of-the-art results.
</p></li>
</ul>

<h3>Title: AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis. (arXiv:2401.01074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01074">http://arxiv.org/abs/2401.01074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01074]] AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided Diagnosis(http://arxiv.org/abs/2401.01074)</code></li>
<li>Summary: <p>Medical data collected for making a diagnostic decision are typically
multi-modal and provide complementary perspectives of a subject. A
computer-aided diagnosis system welcomes multi-modal inputs; however, how to
effectively fuse such multi-modal data is a challenging task and attracts a lot
of attention in the medical research field. In this paper, we propose a
transformer-based framework, called Alifuse, for aligning and fusing
multi-modal medical data. Specifically, we convert images and unstructured and
structured texts into vision and language tokens, and use intramodal and
intermodal attention mechanisms to learn holistic representations of all
imaging and non-imaging data for classification. We apply Alifuse to classify
Alzheimer's disease and obtain state-of-the-art performance on five public
datasets, by outperforming eight baselines. The source code will be available
online later.
</p></li>
</ul>

<h3>Title: Unifying Structured Data as Graph for Data-to-Text Pre-Training. (arXiv:2401.01183v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01183">http://arxiv.org/abs/2401.01183</a></li>
<li>Code URL: <a href="https://github.com/alibabaresearch/damo-convai">https://github.com/alibabaresearch/damo-convai</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01183]] Unifying Structured Data as Graph for Data-to-Text Pre-Training(http://arxiv.org/abs/2401.01183)</code></li>
<li>Summary: <p>Data-to-text (D2T) generation aims to transform structured data into natural
language text. Data-to-text pre-training has proved to be powerful in enhancing
D2T generation and yields impressive performances. However, previous
pre-training methods either oversimplified structured data into a sequence
without considering input structures or designed training objectives tailored
for a specific data structure (e.g., table or knowledge graph). In this paper,
we unify different types of structured data (i.e., table, key-value data,
knowledge graph) into the graph format and cast different data-to-text
generation tasks as graph-to-text generation. To effectively exploit the
structural information of the input graph, we propose a structure-enhanced
pre-training method for D2T generation by designing a structure-enhanced
Transformer. Concretely, we devise a position matrix for the Transformer,
encoding relative positional information of connected nodes in the input graph.
In addition, we propose a new attention matrix to incorporate graph structures
into the original Transformer by taking the available explicit connectivity
structure into account. Extensive experiments on six benchmark datasets show
the effectiveness of our model. Our source codes are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.
</p></li>
</ul>

<h3>Title: Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems. (arXiv:2401.01192v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01192">http://arxiv.org/abs/2401.01192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01192]] Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems(http://arxiv.org/abs/2401.01192)</code></li>
<li>Summary: <p>In many recent works, the potential of Exploratory Landscape Analysis (ELA)
features to numerically characterize, in particular, single-objective
continuous optimization problems has been demonstrated. These numerical
features provide the input for all kinds of machine learning tasks on
continuous optimization problems, ranging, i.a., from High-level Property
Prediction to Automated Algorithm Selection and Automated Algorithm
Configuration. Without ELA features, analyzing and understanding the
characteristics of single-objective continuous optimization problems would be
impossible.
</p>
<p>Yet, despite their undisputed usefulness, ELA features suffer from several
drawbacks. These include, in particular, (1.) a strong correlation between
multiple features, as well as (2.) its very limited applicability to
multi-objective continuous optimization problems. As a remedy, recent works
proposed deep learning-based approaches as alternatives to ELA. In these works,
e.g., point-cloud transformers were used to characterize an optimization
problem's fitness landscape. However, these approaches require a large amount
of labeled training data.
</p>
<p>Within this work, we propose a hybrid approach, Deep-ELA, which combines (the
benefits of) deep learning and ELA features. Specifically, we pre-trained four
transformers on millions of randomly generated optimization problems to learn
deep representations of the landscapes of continuous single- and
multi-objective optimization problems. Our proposed framework can either be
used out-of-the-box for analyzing single- and multi-objective continuous
optimization problems, or subsequently fine-tuned to various tasks focussing on
algorithm behavior and problem understanding.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00873">http://arxiv.org/abs/2401.00873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00873]] A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models(http://arxiv.org/abs/2401.00873)</code></li>
<li>Summary: <p>Self-supervised learning is a popular and powerful method for utilizing large
amounts of unlabeled data, for which a wide variety of training objectives have
been proposed in the literature. In this study, we perform a Bayesian analysis
of state-of-the-art self-supervised learning objectives, elucidating the
underlying probabilistic graphical models in each class and presenting a
standardized methodology for their derivation from first principles. The
analysis also indicates a natural means of integrating self-supervised learning
with likelihood-based generative models. We instantiate this concept within the
realm of cluster-based self-supervised learning and energy models, introducing
a novel lower bound which is proven to reliably penalize the most important
failure modes. Furthermore, this newly proposed lower bound enables the
training of a standard backbone architecture without the necessity for
asymmetric elements such as stop gradients, momentum encoders, or specialized
clustering layers - typically introduced to avoid learning trivial solutions.
Our theoretical findings are substantiated through experiments on synthetic and
real-world data, including SVHN, CIFAR10, and CIFAR100, thus showing that our
objective function allows to outperform existing self-supervised learning
strategies in terms of clustering, generation and out-of-distribution detection
performance by a wide margin. We also demonstrate that GEDI can be integrated
into a neural-symbolic framework to mitigate the reasoning shortcut problem and
to learn higher quality symbolic representations thanks to the enhanced
classification performance.
</p></li>
</ul>

<h3>Title: En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data. (arXiv:2401.01173v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01173">http://arxiv.org/abs/2401.01173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01173]] En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data(http://arxiv.org/abs/2401.01173)</code></li>
<li>Summary: <p>We present En3D, an enhanced generative scheme for sculpting high-quality 3D
human avatars. Unlike previous works that rely on scarce 3D datasets or limited
2D collections with imbalanced viewing angles and imprecise pose priors, our
approach aims to develop a zero-shot 3D generative scheme capable of producing
visually realistic, geometrically accurate and content-wise diverse 3D humans
without relying on pre-existing 3D or 2D assets. To address this challenge, we
introduce a meticulously crafted workflow that implements accurate physical
modeling to learn the enhanced 3D generative model from synthetic 2D data.
During inference, we integrate optimization modules to bridge the gap between
realistic appearances and coarse 3D shapes. Specifically, En3D comprises three
modules: a 3D generator that accurately models generalizable 3D humans with
realistic appearance from synthesized balanced, diverse, and structured human
images; a geometry sculptor that enhances shape quality using multi-view normal
constraints for intricate human anatomy; and a texturing module that
disentangles explicit texture maps with fidelity and editability, leveraging
semantical UV partitioning and a differentiable rasterizer. Experimental
results show that our approach significantly outperforms prior works in terms
of image quality, geometry accuracy and content diversity. We also showcase the
applicability of our generated avatars for animation and editing, as well as
the scalability of our approach for content-style free adaptation.
</p></li>
</ul>

<h3>Title: MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic Communication. (arXiv:2401.01272v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01272">http://arxiv.org/abs/2401.01272</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01272]] MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic Communication(http://arxiv.org/abs/2401.01272)</code></li>
<li>Summary: <p>Vector quantization-based image semantic communication systems have
successfully boosted transmission efficiency, but face a challenge with
conflicting requirements between codebook design and digital constellation
modulation. Traditional codebooks need a wide index range, while modulation
favors few discrete states. To address this, we propose a multilevel generative
semantic communication system with a two-stage training framework. In the first
stage, we train a high-quality codebook, using a multi-head octonary codebook
(MOC) to compress the index range. We also integrate a residual vector
quantization (RVQ) mechanism for effective multilevel communication. In the
second stage, a noise reduction block (NRB) based on Swin Transformer is
introduced, coupled with the multilevel codebook from the first stage, serving
as a high-quality semantic knowledge base (SKB) for generative feature
restoration. Experimental results highlight MOC-RVQ's superior performance over
methods like BPG or JPEG, even without channel error correction coding.
</p></li>
</ul>

<h3>Title: DocLLM: A layout-aware generative language model for multimodal document understanding. (arXiv:2401.00908v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00908">http://arxiv.org/abs/2401.00908</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00908]] DocLLM: A layout-aware generative language model for multimodal document understanding(http://arxiv.org/abs/2401.00908)</code></li>
<li>Summary: <p>Enterprise documents such as forms, invoices, receipts, reports, contracts,
and other similar records, often carry rich semantics at the intersection of
textual and spatial modalities. The visual cues offered by their complex
layouts play a crucial role in comprehending these documents effectively. In
this paper, we present DocLLM, a lightweight extension to traditional large
language models (LLMs) for reasoning over visual documents, taking into account
both textual semantics and spatial layout. Our model differs from existing
multimodal LLMs by avoiding expensive image encoders and focuses exclusively on
bounding box information to incorporate the spatial layout structure.
Specifically, the cross-alignment between text and spatial modalities is
captured by decomposing the attention mechanism in classical transformers to a
set of disentangled matrices. Furthermore, we devise a pre-training objective
that learns to infill text segments. This approach allows us to address
irregular layouts and heterogeneous content frequently encountered in visual
documents. The pre-trained model is fine-tuned using a large-scale instruction
dataset, covering four core document intelligence tasks. We demonstrate that
our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks,
and generalizes well to 4 out of 5 previously unseen datasets.
</p></li>
</ul>

<h3>Title: Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective. (arXiv:2401.00965v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00965">http://arxiv.org/abs/2401.00965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00965]] Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective(http://arxiv.org/abs/2401.00965)</code></li>
<li>Summary: <p>Exploring generative model training for synthetic tabular data, specifically
in sequential contexts such as credit card transaction data, presents
significant challenges. This paper addresses these challenges, focusing on
attaining both high fidelity to actual data and optimal utility for machine
learning tasks. We introduce five pre-processing schemas to enhance the
training of the Conditional Probabilistic Auto-Regressive Model (CPAR),
demonstrating incremental improvements in the synthetic data's fidelity and
utility. Upon achieving satisfactory fidelity levels, our attention shifts to
training fraud detection models tailored for time-series data, evaluating the
utility of the synthetic data. Our findings offer valuable insights and
practical guidelines for synthetic data practitioners in the finance sector,
transitioning from real to synthetic datasets for training purposes, and
illuminating broader methodologies for synthesizing credit card transaction
time series.
</p></li>
</ul>

<h3>Title: Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models. (arXiv:2401.00974v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00974">http://arxiv.org/abs/2401.00974</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00974]] Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models(http://arxiv.org/abs/2401.00974)</code></li>
<li>Summary: <p>Devising procedures for downstream task-oriented generative model selections
is an unresolved problem of practical importance. Existing studies focused on
the utility of a single family of generative models. They provided limited
insights on how synthetic data practitioners select the best family generative
models for synthetic training tasks given a specific combination of machine
learning model class and performance metric. In this paper, we approach the
downstream task-oriented generative model selections problem in the case of
training fraud detection models and investigate the best practice given
different combinations of model interpretability and model performance
constraints. Our investigation supports that, while both Neural
Network(NN)-based and Bayesian Network(BN)-based generative models are both
good to complete synthetic training task under loose model interpretability
constrain, the BN-based generative models is better than NN-based when
synthetic training fraud detection model under strict model interpretability
constrain. Our results provides practical guidance for machine learning
practitioner who is interested in replacing their training dataset from real to
synthetic, and shed lights on more general downstream task-oriented generative
model selection problems.
</p></li>
</ul>

<h3>Title: Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning. (arXiv:2401.01232v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01232">http://arxiv.org/abs/2401.01232</a></li>
<li>Code URL: <a href="https://github.com/riemanngraph/motifrgc">https://github.com/riemanngraph/motifrgc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01232]] Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning(http://arxiv.org/abs/2401.01232)</code></li>
<li>Summary: <p>Graphs are typical non-Euclidean data of complex structures. In recent years,
Riemannian graph representation learning has emerged as an exciting alternative
to Euclidean ones. However, Riemannian methods are still in an early stage:
most of them present a single curvature (radius) regardless of structural
complexity, suffer from numerical instability due to the
exponential/logarithmic map, and lack the ability to capture motif regularity.
In light of the issues above, we propose the problem of \emph{Motif-aware
Riemannian Graph Representation Learning}, seeking a numerically stable encoder
to capture motif regularity in a diverse-curvature manifold without labels. To
this end, we present a novel Motif-aware Riemannian model with
Generative-Contrastive learning (MotifRGC), which conducts a minmax game in
Riemannian manifold in a self-supervised manner. First, we propose a new type
of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold
by a product layer with the diversified factor, and replace the
exponential/logarithmic map by a stable kernel layer. Second, we introduce a
motif-aware Riemannian generative-contrastive learning to capture motif
regularity in the constructed manifold and learn motif-aware node
representation without external labels. Empirical results show the superiority
of MofitRGC.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: SSP: A Simple and Safe automatic Prompt engineering method towards realistic image synthesis on LVM. (arXiv:2401.01128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01128">http://arxiv.org/abs/2401.01128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01128]] SSP: A Simple and Safe automatic Prompt engineering method towards realistic image synthesis on LVM(http://arxiv.org/abs/2401.01128)</code></li>
<li>Summary: <p>Recently, text-to-image (T2I) synthesis has undergone significant
advancements, particularly with the emergence of Large Language Models (LLM)
and their enhancement in Large Vision Models (LVM), greatly enhancing the
instruction-following capabilities of traditional T2I models. Nevertheless,
previous methods focus on improving generation quality but introduce unsafe
factors into prompts. We explore that appending specific camera descriptions to
prompts can enhance safety performance. Consequently, we propose a simple and
safe prompt engineering method (SSP) to improve image generation quality by
providing optimal camera descriptions. Specifically, we create a dataset from
multi-datasets as original prompts. To select the optimal camera, we design an
optimal camera matching approach and implement a classifier for original
prompts capable of automatically matching. Appending camera descriptions to
original prompts generates optimized prompts for further LVM image generation.
Experiments demonstrate that SSP improves semantic consistency by an average of
16% compared to others and safety metrics by 48.9%.
</p></li>
</ul>

<h3>Title: A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01286">http://arxiv.org/abs/2401.01286</a></li>
<li>Code URL: <a href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01286]] A Comprehensive Study of Knowledge Editing for Large Language Models(http://arxiv.org/abs/2401.01286)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</p></li>
</ul>

<h3>Title: LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00907">http://arxiv.org/abs/2401.00907</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00907]] LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models(http://arxiv.org/abs/2401.00907)</code></li>
<li>Summary: <p>Fine-tuning Large Language Models (LLMs) adapts a trained model to specific
downstream tasks, significantly improving task-specific performance. Supervised
Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce
desired answers. However, LLMs trained with SFT sometimes make simple mistakes
and result in hallucinations on reasoning tasks such as question-answering.
Without external feedback, it is difficult for SFT to learn a good mapping
between the question and the desired answer, especially with a small dataset.
This paper introduces an alternative to SFT called Natural Language Feedback
for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they
will receive from an annotator. We find that requiring such reflection can
significantly improve the accuracy in in-domain question-answering tasks,
providing a promising direction for the application of natural language
feedback in the realm of SFT LLMs. Additional ablation studies show that the
portion of human-annotated data in the annotated datasets affects the
fine-tuning performance.
</p></li>
</ul>

<h3>Title: Quokka: An Open-source Large Language Model ChatBot for Material Science. (arXiv:2401.01089v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01089">http://arxiv.org/abs/2401.01089</a></li>
<li>Code URL: <a href="https://github.com/xianjun-yang/quokka">https://github.com/xianjun-yang/quokka</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01089]] Quokka: An Open-source Large Language Model ChatBot for Material Science(http://arxiv.org/abs/2401.01089)</code></li>
<li>Summary: <p>This paper presents the development of a specialized chatbot for materials
science, leveraging the Llama-2 language model, and continuing pre-training on
the expansive research articles in the materials science domain from the S2ORC
dataset. The methodology involves an initial pretraining phase on over one
million domain-specific papers, followed by an instruction-tuning process to
refine the chatbot's capabilities. The chatbot is designed to assist
researchers, educators, and students by providing instant, context-aware
responses to queries in the field of materials science. We make the four
trained checkpoints (7B, 13B, with or without chat ability) freely available to
the research community at https://github.com/Xianjun-Yang/Quokka.
</p></li>
</ul>

<h3>Title: Uncertainty Resolution in Misinformation Detection. (arXiv:2401.01197v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01197">http://arxiv.org/abs/2401.01197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01197]] Uncertainty Resolution in Misinformation Detection(http://arxiv.org/abs/2401.01197)</code></li>
<li>Summary: <p>Misinformation poses a variety of risks, such as undermining public trust and
distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been
shown effective in mitigating misinformation, particularly in handling
statements where enough context is provided. However, they struggle to assess
ambiguous or context-deficient statements accurately. This work introduces a
new method to resolve uncertainty in such statements. We propose a framework to
categorize missing information and publish category labels for the LIAR-New
dataset, which is adaptable to cross-domain content with missing information.
We then leverage this framework to generate effective user queries for missing
context. Compared to baselines, our method improves the rate at which generated
questions are answerable by the user by 38 percentage points and classification
performance by over 10 percentage points macro F1. Thus, this approach may
provide a valuable component for future misinformation mitigation pipelines.
</p></li>
</ul>

<h3>Title: Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01218">http://arxiv.org/abs/2401.01218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01218]] Zero-Shot Position Debiasing for Large Language Models(http://arxiv.org/abs/2401.01218)</code></li>
<li>Summary: <p>Fine-tuning has been demonstrated to be an effective method to improve the
domain performance of large language models (LLMs). However, LLMs might fit the
dataset bias and shortcuts for prediction, leading to poor generation
performance. Experimental result shows that LLMs are prone to exhibit position
bias, i.e., leveraging information positioned at the beginning or end, or
specific positional cues within the input. Existing works on mitigating
position bias require external bias knowledge or annotated non-biased samples,
which is unpractical in reality. In this work, we propose a zero-shot position
debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages
unsupervised responses from pre-trained LLMs for debiasing, thus without any
external knowledge or datasets. To improve the quality of unsupervised
responses, we propose a master-slave alignment (MSA) module to prune these
responses. Experiments on eight datasets and five tasks show that ZOE
consistently outperforms existing methods in mitigating four types of position
biases. Besides, ZOE achieves this by sacrificing only a small performance on
biased samples, which is simple and effective.
</p></li>
</ul>

<h3>Title: Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01301">http://arxiv.org/abs/2401.01301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01301]] Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models(http://arxiv.org/abs/2401.01301)</code></li>
<li>Summary: <p>Large language models (LLMs) have the potential to transform the practice of
law, but this potential is threatened by the presence of legal hallucinations
-- responses from these models that are not consistent with legal facts. We
investigate the extent of these hallucinations using an original suite of legal
queries, comparing LLMs' responses to structured legal metadata and examining
their consistency. Our work makes four key contributions: (1) We develop a
typology of legal hallucinations, providing a conceptual framework for future
research in this area. (2) We find that legal hallucinations are alarmingly
prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with
Llama 2, when these models are asked specific, verifiable questions about
random federal court cases. (3) We illustrate that LLMs often fail to correct a
user's incorrect legal assumptions in a contra-factual question setup. (4) We
provide evidence that LLMs cannot always predict, or do not always know, when
they are producing legal hallucinations. Taken together, these findings caution
against the rapid and unsupervised integration of popular LLMs into legal
tasks. Even experienced lawyers must remain wary of legal hallucinations, and
the risks are highest for those who stand to benefit from LLMs the most -- pro
se litigants or those without access to traditional legal resources.
</p></li>
</ul>

<h3>Title: A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01313">http://arxiv.org/abs/2401.01313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01313]] A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models(http://arxiv.org/abs/2401.01313)</code></li>
<li>Summary: <p>As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</p></li>
</ul>

<h3>Title: LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01325">http://arxiv.org/abs/2401.01325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01325]] LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning(http://arxiv.org/abs/2401.01325)</code></li>
<li>Summary: <p>This work elicits LLMs' inherent ability to handle long contexts without
fine-tuning. The limited length of the training sequence during training may
limit the application of Large Language Models (LLMs) on long input sequences
for inference. In this work, we argue that existing LLMs themselves have
inherent capabilities for handling long contexts. Based on this argument, we
suggest extending LLMs' context window by themselves to fully utilize the
inherent ability.We propose Self-Extend to stimulate LLMs' long context
handling potential. The basic idea is to construct bi-level attention
information: the group level and the neighbor level. The two levels are
computed by the original model's self-attention, which means the proposed does
not require any training. With only four lines of code modification, the
proposed method can effortlessly extend existing LLMs' context window without
any fine-tuning. We conduct comprehensive experiments and the results show that
the proposed method can effectively extend existing LLMs' context window's
length.
</p></li>
</ul>

<h3>Title: Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01335">http://arxiv.org/abs/2401.01335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01335]] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models(http://arxiv.org/abs/2401.01335)</code></li>
<li>Summary: <p>Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM's performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents.
</p></li>
</ul>

<h3>Title: LLbezpeky: Leveraging Large Language Models for Vulnerability Detection. (arXiv:2401.01269v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01269">http://arxiv.org/abs/2401.01269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01269]] LLbezpeky: Leveraging Large Language Models for Vulnerability Detection(http://arxiv.org/abs/2401.01269)</code></li>
<li>Summary: <p>Despite the continued research and progress in building secure systems,
Android applications continue to be ridden with vulnerabilities, necessitating
effective detection methods. Current strategies involving static and dynamic
analysis tools come with limitations like overwhelming number of false
positives and limited scope of analysis which make either difficult to adopt.
Over the past years, machine learning based approaches have been extensively
explored for vulnerability detection, but its real-world applicability is
constrained by data requirements and feature engineering challenges. Large
Language Models (LLMs), with their vast parameters, have shown tremendous
potential in understanding semnatics in human as well as programming languages.
We dive into the efficacy of LLMs for detecting vulnerabilities in the context
of Android security. We focus on building an AI-driven workflow to assist
developers in identifying and rectifying vulnerabilities. Our experiments show
that LLMs outperform our expectations in finding issues within applications
correctly flagging insecure apps in 91.67% of cases in the Ghera benchmark. We
use inferences from our experiments towards building a robust and actionable
vulnerability detection system and demonstrate its effectiveness. Our
experiments also shed light on how different various simple configurations can
affect the True Positive (TP) and False Positive (FP) rates.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.00910">http://arxiv.org/abs/2401.00910</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.00910]] WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge(http://arxiv.org/abs/2401.00910)</code></li>
<li>Summary: <p>Motion segmentation is a complex yet indispensable task in autonomous
driving. The challenges introduced by the ego-motion of the cameras, radial
distortion in fisheye lenses, and the need for temporal consistency make the
task more complicated, rendering traditional and standard Convolutional Neural
Network (CNN) approaches less effective. The consequent laborious data
labeling, representation of diverse and uncommon scenarios, and extensive data
capture requirements underscore the imperative of synthetic data for improving
machine learning model performance. To this end, we employ the PD-WoodScape
synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye
dataset. Thus, we present the WoodScape fisheye motion segmentation challenge
for autonomous driving, held as part of the CVPR 2023 Workshop on
Omnidirectional Computer Vision (OmniCV). As one of the first competitions
focused on fisheye motion segmentation, we aim to explore and evaluate the
potential and impact of utilizing synthetic data in this domain. In this paper,
we provide a detailed analysis on the competition which attracted the
participation of 112 global teams and a total of 234 submissions. This study
delineates the complexities inherent in the task of motion segmentation,
emphasizes the significance of fisheye datasets, articulate the necessity for
synthetic datasets and the resultant domain gap they engender, outlining the
foundational blueprint for devising successful solutions. Subsequently, we
delve into the details of the baseline experiments and winning methods
evaluating their qualitative and quantitative results, providing with useful
insights.
</p></li>
</ul>

<h3>Title: Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt. (arXiv:2401.01010v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01010">http://arxiv.org/abs/2401.01010</a></li>
<li>Code URL: <a href="https://github.com/shirowalker/ucad">https://github.com/shirowalker/ucad</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01010]] Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt(http://arxiv.org/abs/2401.01010)</code></li>
<li>Summary: <p>Unsupervised Anomaly Detection (UAD) with incremental training is crucial in
industrial manufacturing, as unpredictable defects make obtaining sufficient
labeled data infeasible. However, continual learning methods primarily rely on
supervised annotations, while the application in UAD is limited due to the
absence of supervision. Current UAD methods train separate models for different
classes sequentially, leading to catastrophic forgetting and a heavy
computational burden. To address this issue, we introduce a novel Unsupervised
Continual Anomaly Detection framework called UCAD, which equips the UAD with
continual learning capability through contrastively-learned prompts. In the
proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a
concise key-prompt-knowledge memory bank to guide task-invariant `anomaly'
model predictions using task-specific `normal' knowledge. Moreover,
Structure-based Contrastive Learning (SCL) is designed with the Segment
Anything Model (SAM) to improve prompt learning and anomaly segmentation
results. Specifically, by treating SAM's masks as structure, we draw features
within the same mask closer and push others apart for general feature
representations. We conduct comprehensive experiments and set the benchmark on
unsupervised continual anomaly detection and segmentation, demonstrating that
our method is significantly better than anomaly detection methods, even with
rehearsal training. The code will be available at
https://github.com/shirowalker/UCAD.
</p></li>
</ul>

<h3>Title: Online Continual Domain Adaptation for Semantic Image Segmentation Using Internal Representations. (arXiv:2401.01035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01035">http://arxiv.org/abs/2401.01035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01035]] Online Continual Domain Adaptation for Semantic Image Segmentation Using Internal Representations(http://arxiv.org/abs/2401.01035)</code></li>
<li>Summary: <p>Semantic segmentation models trained on annotated data fail to generalize
well when the input data distribution changes over extended time period,
leading to requiring re-training to maintain performance. Classic Unsupervised
domain adaptation (UDA) attempts to address a similar problem when there is
target domain with no annotated data points through transferring knowledge from
a source domain with annotated data. We develop an online UDA algorithm for
semantic segmentation of images that improves model generalization on
unannotated domains in scenarios where source data access is restricted during
adaptation. We perform model adaptation is by minimizing the distributional
distance between the source latent features and the target features in a shared
embedding space. Our solution promotes a shared domain-agnostic latent feature
space between the two domains, which allows for classifier generalization on
the target dataset. To alleviate the need of access to source samples during
adaptation, we approximate the source latent feature distribution via an
appropriate surrogate distribution, in this case a Gassian mixture model (GMM).
We evaluate our approach on well established semantic segmentation datasets and
demonstrate it compares favorably against state-of-the-art (SOTA) UDA semantic
segmentation methods.
</p></li>
</ul>

<h3>Title: DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in Nighttime Semantic Segmentation. (arXiv:2401.01066v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01066">http://arxiv.org/abs/2401.01066</a></li>
<li>Code URL: <a href="https://github.com/hf618/dtbs">https://github.com/hf618/dtbs</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01066]] DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in Nighttime Semantic Segmentation(http://arxiv.org/abs/2401.01066)</code></li>
<li>Summary: <p>Due to the poor illumination and the difficulty in annotating, nighttime
conditions pose a significant challenge for autonomous vehicle perception
systems. Unsupervised domain adaptation (UDA) has been widely applied to
semantic segmentation on such images to adapt models from normal conditions to
target nighttime-condition domains. Self-training (ST) is a paradigm in UDA,
where a momentum teacher is utilized for pseudo-label prediction, but a
confirmation bias issue exists. Because the one-directional knowledge transfer
from a single teacher is insufficient to adapt to a large domain shift. To
mitigate this issue, we propose to alleviate domain gap by incrementally
considering style influence and illumination change. Therefore, we introduce a
one-stage Dual-Teacher Bi-directional Self-training (DTBS) framework for smooth
knowledge transfer and feedback. Based on two teacher models, we present a
novel pipeline to respectively decouple style and illumination shift. In
addition, we propose a new Re-weight exponential moving average (EMA) to merge
the knowledge of style and illumination factors, and provide feedback to the
student model. In this way, our method can be embedded in other UDA methods to
enhance their performance. For example, the Cityscapes to ACDC night task
yielded 53.8 mIoU (\%), which corresponds to an improvement of +5\% over the
previous state-of-the-art. The code is available at
\url{https://github.com/hf618/DTBS}.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
