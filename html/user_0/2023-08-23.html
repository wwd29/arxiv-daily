<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Blockchain-Powered Supply Chain Management for Kidney Organ Preservation. (arXiv:2308.11169v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11169">http://arxiv.org/abs/2308.11169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11169]] Blockchain-Powered Supply Chain Management for Kidney Organ Preservation(http://arxiv.org/abs/2308.11169)</code></li>
<li>Summary: <p>Due to the shortage of available kidney organs for transplants, handling
every donor kidney with utmost care is crucial to preserve the organ's health,
especially during the organ supply chain where kidneys are prone to
deterioration during transportation. Therefore, this research proposes a
blockchain platform to aid in managing kidneys in the supply chain. This
framework establishes a secure system that meticulously tracks the organ's
location and handling, safeguarding the health from donor to recipient.
Additionally, a machine-learning algorithm is embedded to monitor organ health
in real-time against various metrics for prompt detection of possible kidney
damage.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Unlocking Hardware Security Assurance: The Potential of LLMs. (arXiv:2308.11042v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11042">http://arxiv.org/abs/2308.11042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11042]] Unlocking Hardware Security Assurance: The Potential of LLMs(http://arxiv.org/abs/2308.11042)</code></li>
<li>Summary: <p>System-on-Chips (SoCs) form the crux of modern computing systems. SoCs enable
high-level integration through the utilization of multiple Intellectual
Property (IP) cores. However, the integration of multiple IP cores also
presents unique challenges owing to their inherent vulnerabilities, thereby
compromising the security of the entire system. Hence, it is imperative to
perform hardware security validation to address these concerns. The efficiency
of this validation procedure is contingent on the quality of the SoC security
properties provided. However, generating security properties with traditional
approaches often requires expert intervention and is limited to a few IPs,
thereby resulting in a time-consuming and non-robust process. To address this
issue, we, for the first time, propose a novel and automated Natural Language
Processing (NLP)-based Security Property Generator (NSPG). Specifically, our
approach utilizes hardware documentation in order to propose the first hardware
security-specific language model, HS-BERT, for extracting security properties
dedicated to hardware design. To evaluate our proposed technique, we trained
the HS-BERT model using sentences from RISC-V, OpenRISC, MIPS, OpenSPARC, and
OpenTitan SoC documentation. When assessedb on five untrained OpenTitan
hardware IP documents, NSPG was able to extract 326 security properties from
1723 sentences. This, in turn, aided in identifying eight security bugs in the
OpenTitan SoC design presented in the hardware hacking competition, Hack@DAC
2022.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection. (arXiv:2308.11072v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11072">http://arxiv.org/abs/2308.11072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11072]] TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection(http://arxiv.org/abs/2308.11072)</code></li>
<li>Summary: <p>Video anomaly detection (VAD) without human monitoring is a complex computer
vision task that can have a positive impact on society if implemented
successfully. While recent advances have made significant progress in solving
this task, most existing approaches overlook a critical real-world concern:
privacy. With the increasing popularity of artificial intelligence
technologies, it becomes crucial to implement proper AI ethics into their
development. Privacy leakage in VAD allows models to pick up and amplify
unnecessary biases related to people's personal information, which may lead to
undesirable decision making. In this paper, we propose TeD-SPAD, a
privacy-aware video anomaly detection framework that destroys visual private
information in a self-supervised manner. In particular, we propose the use of a
temporally-distinct triplet loss to promote temporally discriminative features,
which complements current weakly-supervised VAD methods. Using TeD-SPAD, we
achieve a positive trade-off between privacy protection and utility anomaly
detection performance on three popular weakly supervised VAD datasets:
UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model
reduces private attribute prediction by 32.25% while only reducing frame-level
ROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page:
https://joefioresi718.github.io/TeD-SPAD_webpage/
</p></li>
</ul>

<h3>Title: LDP-Feat: Image Features with Local Differential Privacy. (arXiv:2308.11223v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11223">http://arxiv.org/abs/2308.11223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11223]] LDP-Feat: Image Features with Local Differential Privacy(http://arxiv.org/abs/2308.11223)</code></li>
<li>Summary: <p>Modern computer vision services often require users to share raw feature
descriptors with an untrusted server. This presents an inherent privacy risk,
as raw descriptors may be used to recover the source images from which they
were extracted. To address this issue, researchers recently proposed
privatizing image features by embedding them within an affine subspace
containing the original feature as well as adversarial feature samples. In this
paper, we propose two novel inversion attacks to show that it is possible to
(approximately) recover the original image features from these embeddings,
allowing us to recover privacy-critical image content. In light of such
successes and the lack of theoretical privacy guarantees afforded by existing
visual privacy methods, we further propose the first method to privatize image
features via local differential privacy, which, unlike prior approaches,
provides a guaranteed bound for privacy leakage regardless of the strength of
the attacks. In addition, our method yields strong performance in visual
localization as a downstream task while enjoying the privacy guarantee.
</p></li>
</ul>

<h3>Title: Semantic RGB-D Image Synthesis. (arXiv:2308.11356v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11356">http://arxiv.org/abs/2308.11356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11356]] Semantic RGB-D Image Synthesis(http://arxiv.org/abs/2308.11356)</code></li>
<li>Summary: <p>Collecting diverse sets of training images for RGB-D semantic image
segmentation is not always possible. In particular, when robots need to operate
in privacy-sensitive areas like homes, the collection is often limited to a
small set of locations. As a consequence, the annotated images lack diversity
in appearance and approaches for RGB-D semantic image segmentation tend to
overfit the training data. In this paper, we thus introduce semantic RGB-D
image synthesis to address this problem. It requires synthesising a
realistic-looking RGB-D image for a given semantic label map. Current
approaches, however, are uni-modal and cannot cope with multi-modal data.
Indeed, we show that extending uni-modal approaches to multi-modal data does
not perform well. In this paper, we therefore propose a generator for
multi-modal data that separates modal-independent information of the semantic
layout from the modal-dependent information that is needed to generate an RGB
and a depth image, respectively. Furthermore, we propose a discriminator that
ensures semantic consistency between the label maps and the generated images
and perceptual similarity between the real and generated images. Our
comprehensive experiments demonstrate that the proposed method outperforms
previous uni-modal methods by a large margin and that the accuracy of an
approach for RGB-D semantic segmentation can be significantly improved by
mixing real and generated images during training.
</p></li>
</ul>

<h3>Title: Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics. (arXiv:2308.11027v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11027">http://arxiv.org/abs/2308.11027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11027]] Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics(http://arxiv.org/abs/2308.11027)</code></li>
<li>Summary: <p>Deep learning continues to rapidly evolve and is now demonstrating remarkable
potential for numerous medical prediction tasks. However, realizing deep
learning models that generalize across healthcare organizations is challenging.
This is due, in part, to the inherent siloed nature of these organizations and
patient privacy requirements. To address this problem, we illustrate how split
learning can enable collaborative training of deep learning models across
disparate and privately maintained health datasets, while keeping the original
records and model parameters private. We introduce a new privacy-preserving
distributed learning framework that offers a higher level of privacy compared
to conventional federated learning. We use several biomedical imaging and
electronic health record (EHR) datasets to show that deep learning models
trained via split learning can achieve highly similar performance to their
centralized and federated counterparts while greatly improving computational
efficiency and reducing privacy risks.
</p></li>
</ul>

<h3>Title: A novel analysis of utility in privacy pipelines, using Kronecker products and quantitative information flow. (arXiv:2308.11110v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11110">http://arxiv.org/abs/2308.11110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11110]] A novel analysis of utility in privacy pipelines, using Kronecker products and quantitative information flow(http://arxiv.org/abs/2308.11110)</code></li>
<li>Summary: <p>We combine Kronecker products, and quantitative information flow, to give a
novel formal analysis for the fine-grained verification of utility in complex
privacy pipelines. The combination explains a surprising anomaly in the
behaviour of utility of privacy-preserving pipelines -- that sometimes a
reduction in privacy results also in a decrease in utility. We use the standard
measure of utility for Bayesian analysis, introduced by Ghosh at al., to
produce tractable and rigorous proofs of the fine-grained statistical behaviour
leading to the anomaly. More generally, we offer the prospect of
formal-analysis tools for utility that complement extant formal analyses of
privacy. We demonstrate our results on a number of common privacy-preserving
designs.
</p></li>
</ul>

<h3>Title: Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment. (arXiv:2308.11220v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11220">http://arxiv.org/abs/2308.11220</a></li>
<li>Code URL: https://github.com/toriqiu/fl-pcos</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11220]] Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment(http://arxiv.org/abs/2308.11220)</code></li>
<li>Summary: <p>The field of women's endocrinology has trailed behind data-driven medical
solutions, largely due to concerns over the privacy of patient data. Valuable
datapoints about hormone levels or menstrual cycling could expose patients who
suffer from comorbidities or terminate a pregnancy, violating their privacy. We
explore the application of Federated Learning (FL) to predict the optimal drug
for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal
disorder impacting millions of women worldwide, yet it's poorly understood and
its research is stunted by a lack of patient data. We demonstrate that a
variety of FL approaches succeed on a synthetic PCOS patient dataset. Our
proposed FL models are a tool to access massive quantities of diverse data and
identify the most effective treatment option while providing PCOS patients with
privacy guarantees.
</p></li>
</ul>

<h3>Title: Up-to-date Threat Modelling for Soft Privacy on Smart Cars. (arXiv:2308.11273v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11273">http://arxiv.org/abs/2308.11273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11273]] Up-to-date Threat Modelling for Soft Privacy on Smart Cars(http://arxiv.org/abs/2308.11273)</code></li>
<li>Summary: <p>Physical persons playing the role of car drivers consume data that is sourced
from the Internet and, at the same time, themselves act as sources of relevant
data. It follows that citizens' privacy is potentially at risk while they
drive, hence the need to model privacy threats in this application domain. This
paper addresses the privacy threats by updating a recent threat-modelling
methodology and by tailoring it specifically to the soft privacy target
property, which ensures citizens' full control on their personal data. The
methodology now features the sources of documentation as an explicit variable
that is to be considered. It is demonstrated by including a new version of the
de-facto standard LINDDUN methodology as well as an additional source by ENISA
which is found to be relevant to soft privacy. The main findings are a set of
23 domain-independent threats, 43 domain-specific assets and 525
domain-dependent threats for the target property in the automotive domain.
While these exceed their previous versions, their main value is to offer
self-evident support to at least two arguments. One is that LINDDUN has evolved
much the way our original methodology already advocated because a few of our
previously suggested extensions are no longer outstanding. The other one is
that ENISA's treatment of privacy aboard smart cars should be extended
considerably because our 525 threats fall in the same scope.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation. (arXiv:2308.11333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11333">http://arxiv.org/abs/2308.11333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11333]] Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation(http://arxiv.org/abs/2308.11333)</code></li>
<li>Summary: <p>As a distributed machine learning paradigm, Federated Learning (FL) enables
large-scale clients to collaboratively train a model without sharing their raw
data. However, due to the lack of data auditing for untrusted clients, FL is
vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned
data for local training or directly changing the model parameters, attackers
can easily inject backdoors into the model, which can trigger the model to make
misclassification of targeted patterns in images. To address these issues, we
propose a novel data-free trigger-generation-based defense approach based on
the two characteristics of backdoor attacks: i) triggers are learned faster
than normal knowledge, and ii) trigger patterns have a greater effect on image
classification than normal class patterns. Our approach generates the images
with newly learned knowledge by identifying the differences between the old and
new global models, and filters trigger images by evaluating the effect of these
generated images. By using these trigger images, our approach eliminates
poisoned models to ensure the updated global model is benign. Comprehensive
experiments demonstrate that our approach can defend against almost all the
existing types of backdoor attacks and outperform all the seven
state-of-the-art defense methods with both IID and non-IID scenarios.
Especially, our approach can successfully defend against the backdoor attack
even when 80\% of the clients are malicious.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (arXiv:2308.11406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11406">http://arxiv.org/abs/2308.11406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11406]] Designing an attack-defense game: how to increase robustness of financial transaction models via a competition(http://arxiv.org/abs/2308.11406)</code></li>
<li>Summary: <p>Given the escalating risks of malicious attacks in the finance sector and the
consequential severe damage, a thorough understanding of adversarial strategies
and robust defense mechanisms for machine learning models is critical. The
threat becomes even more severe with the increased adoption in banks more
accurate, but potentially fragile neural networks. We aim to investigate the
current state and dynamics of adversarial attacks and defenses for neural
network models that use sequential financial data as the input.
</p>
<p>To achieve this goal, we have designed a competition that allows realistic
and detailed investigation of problems in modern financial transaction data.
The participants compete directly against each other, so possible attacks and
defenses are examined in close-to-real-life conditions. Our main contributions
are the analysis of the competition dynamics that answers the questions on how
important it is to conceal a model from malicious users, how long does it take
to break it, and what techniques one should use to make it more robust, and
introduction additional way to attack models or increase their robustness.
</p>
<p>Our analysis continues with a meta-study on the used approaches with their
power, numerical experiments, and accompanied ablations studies. We show that
the developed attacks and defenses outperform existing alternatives from the
literature while being practical in terms of execution, proving the validity of
the competition as a tool for uncovering vulnerabilities of machine learning
models and mitigating them in various domains.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11070">http://arxiv.org/abs/2308.11070</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11070]] Temporal-Distributed Backdoor Attack Against Video Based Action Recognition(http://arxiv.org/abs/2308.11070)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have achieved tremendous success in various
applications including video action recognition, yet remain vulnerable to
backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to
the target class chosen by the attacker when a test instance (from a non-target
class) is embedded with a specific trigger, while maintaining high accuracy on
attack-free instances. Although there are extensive studies on backdoor attacks
against image data, the susceptibility of video-based systems under backdoor
attacks remains largely unexplored. Current studies are direct extensions of
approaches proposed for image data, e.g., the triggers are
\textbf{independently} embedded within the frames, which tend to be detectable
by existing defenses. In this paper, we introduce a \textit{simple} yet
\textit{effective} backdoor attack against video data. Our proposed attack,
adding perturbations in a transformed domain, plants an \textbf{imperceptible,
temporally distributed} trigger across the video frames, and is shown to be
resilient to existing defensive strategies. The effectiveness of the proposed
attack is demonstrated by extensive experiments with various well-known models
on two video recognition benchmarks, UCF101 and HMDB51, and a sign language
recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the
impact of several influential factors on our proposed attack and identify an
intriguing effect termed "collateral damage" through extensive studies.
</p></li>
</ul>

<h3>Title: SDeMorph: Towards Better Facial De-morphing from Single Morph. (arXiv:2308.11442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11442">http://arxiv.org/abs/2308.11442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11442]] SDeMorph: Towards Better Facial De-morphing from Single Morph(http://arxiv.org/abs/2308.11442)</code></li>
<li>Summary: <p>Face Recognition Systems (FRS) are vulnerable to morph attacks. A face morph
is created by combining multiple identities with the intention to fool FRS and
making it match the morph with multiple identities. Current Morph Attack
Detection (MAD) can detect the morph but are unable to recover the identities
used to create the morph with satisfactory outcomes. Existing work in
de-morphing is mostly reference-based, i.e. they require the availability of
one identity to recover the other. Sudipta et al. \cite{ref9} proposed a
reference-free de-morphing technique but the visual realism of outputs produced
were feeble. In this work, we propose SDeMorph (Stably Diffused De-morpher), a
novel de-morphing method that is reference-free and recovers the identities of
bona fides. Our method produces feature-rich outputs that are of significantly
high quality in terms of definition and facial fidelity. Our method utilizes
Denoising Diffusion Probabilistic Models (DDPM) by destroying the input morphed
signal and then reconstructing it back using a branched-UNet. Experiments on
ASML, FRLL-FaceMorph, FRLL-MorDIFF, and SMDD datasets support the effectiveness
of the proposed method.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Debiasing Counterfactuals In the Presence of Spurious Correlations. (arXiv:2308.10984v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10984">http://arxiv.org/abs/2308.10984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10984]] Debiasing Counterfactuals In the Presence of Spurious Correlations(http://arxiv.org/abs/2308.10984)</code></li>
<li>Summary: <p>Deep learning models can perform well in complex medical imaging
classification tasks, even when basing their conclusions on spurious
correlations (i.e. confounders), should they be prevalent in the training
dataset, rather than on the causal image markers of interest. This would
thereby limit their ability to generalize across the population. Explainability
based on counterfactual image generation can be used to expose the confounders
but does not provide a strategy to mitigate the bias. In this work, we
introduce the first end-to-end training framework that integrates both (i)
popular debiasing classifiers (e.g. distributionally robust optimization (DRO))
to avoid latching onto the spurious correlations and (ii) counterfactual image
generation to unveil generalizable imaging markers of relevance to the task.
Additionally, we propose a novel metric, Spurious Correlation Latching Score
(SCLS), to quantify the extent of the classifier reliance on the spurious
correlation as exposed by the counterfactual images. Through comprehensive
experiments on two public datasets (with the simulated and real visual
artifacts), we demonstrate that the debiasing method: (i) learns generalizable
markers across the population, and (ii) successfully ignores spurious
correlations and focuses on the underlying disease pathology.
</p></li>
</ul>

<h3>Title: Long-Term Prediction of Natural Video Sequences with Robust Video Predictors. (arXiv:2308.11079v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11079">http://arxiv.org/abs/2308.11079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11079]] Long-Term Prediction of Natural Video Sequences with Robust Video Predictors(http://arxiv.org/abs/2308.11079)</code></li>
<li>Summary: <p>Predicting high dimensional video sequences is a curiously difficult problem.
The number of possible futures for a given video sequence grows exponentially
over time due to uncertainty. This is especially evident when trying to predict
complicated natural video scenes from a limited snapshot of the world. The
inherent uncertainty accumulates the further into the future you predict making
long-term prediction very difficult. In this work we introduce a number of
improvements to existing work that aid in creating Robust Video Predictors
(RoViPs). We show that with a combination of deep Perceptual and
uncertainty-based reconstruction losses we are able to create high quality
short-term predictions. Attention-based skip connections are utilised to allow
for long range spatial movement of input features to further improve
performance. Finally, we show that by simply making the predictor robust to its
own prediction errors, it is possible to produce very long, realistic natural
video sequences using an iterated single-step prediction task.
</p></li>
</ul>

<h3>Title: Domain Generalization via Rationale Invariance. (arXiv:2308.11158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11158">http://arxiv.org/abs/2308.11158</a></li>
<li>Code URL: https://github.com/liangchen527/ridg</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11158]] Domain Generalization via Rationale Invariance(http://arxiv.org/abs/2308.11158)</code></li>
<li>Summary: <p>This paper offers a new perspective to ease the challenge of domain
generalization, which involves maintaining robust results even in unseen
environments. Our design focuses on the decision-making process in the final
classifier layer. Specifically, we propose treating the element-wise
contributions to the final results as the rationale for making a decision and
representing the rationale for each sample as a matrix. For a well-generalized
model, we suggest the rationale matrices for samples belonging to the same
category should be similar, indicating the model relies on domain-invariant
clues to make decisions, thereby ensuring robust results. To implement this
idea, we introduce a rationale invariance loss as a simple regularization
technique, requiring only a few lines of code. Our experiments demonstrate that
the proposed approach achieves competitive results across various datasets,
despite its simplicity. Code is available at
\url{https://github.com/liangchen527/RIDG}.
</p></li>
</ul>

<h3>Title: Decoupled Contrastive Multi-view Clustering with High-order Random Walks. (arXiv:2308.11164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11164">http://arxiv.org/abs/2308.11164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11164]] Decoupled Contrastive Multi-view Clustering with High-order Random Walks(http://arxiv.org/abs/2308.11164)</code></li>
<li>Summary: <p>In recent, some robust contrastive multi-view clustering (MvC) methods have
been proposed, which construct data pairs from neighborhoods to alleviate the
false negative issue, i.e., some intra-cluster samples are wrongly treated as
negative pairs. Although promising performance has been achieved by these
methods, the false negative issue is still far from addressed and the false
positive issue emerges because all in- and out-of-neighborhood samples are
simply treated as positive and negative, respectively. To address the issues,
we propose a novel robust method, dubbed decoupled contrastive multi-view
clustering with high-order random walks (DIVIDE). In brief, DIVIDE leverages
random walks to progressively identify data pairs in a global instead of local
manner. As a result, DIVIDE could identify in-neighborhood negatives and
out-of-neighborhood positives. Moreover, DIVIDE embraces a novel MvC
architecture to perform inter- and intra-view contrastive learning in different
embedding spaces, thus boosting clustering performance and embracing the
robustness against missing views. To verify the efficacy of DIVIDE, we carry
out extensive experiments on four benchmark datasets comparing with nine
state-of-the-art MvC methods in both complete and incomplete MvC settings.
</p></li>
</ul>

<h3>Title: Video BagNet: short temporal receptive fields increase robustness in long-term action recognition. (arXiv:2308.11249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11249">http://arxiv.org/abs/2308.11249</a></li>
<li>Code URL: https://github.com/ombretta/videobagnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11249]] Video BagNet: short temporal receptive fields increase robustness in long-term action recognition(http://arxiv.org/abs/2308.11249)</code></li>
<li>Summary: <p>Previous work on long-term video action recognition relies on deep
3D-convolutional models that have a large temporal receptive field (RF). We
argue that these models are not always the best choice for temporal modeling in
videos. A large temporal receptive field allows the model to encode the exact
sub-action order of a video, which causes a performance decrease when testing
videos have a different sub-action order. In this work, we investigate whether
we can improve the model robustness to the sub-action order by shrinking the
temporal receptive field of action recognition models. For this, we design
Video BagNet, a variant of the 3D ResNet-50 model with the temporal receptive
field size limited to 1, 9, 17 or 33 frames. We analyze Video BagNet on
synthetic and real-world video datasets and experimentally compare models with
varying temporal receptive fields. We find that short receptive fields are
robust to sub-action order changes, while larger temporal receptive fields are
sensitive to the sub-action order.
</p></li>
</ul>

<h3>Title: Towards Clip-Free Quantized Super-Resolution Networks: How to Tame Representative Images. (arXiv:2308.11365v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11365">http://arxiv.org/abs/2308.11365</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11365]] Towards Clip-Free Quantized Super-Resolution Networks: How to Tame Representative Images(http://arxiv.org/abs/2308.11365)</code></li>
<li>Summary: <p>Super-resolution (SR) networks have been investigated for a while, with their
mobile and lightweight versions gaining noticeable popularity recently.
Quantization, the procedure of decreasing the precision of network parameters
(mostly FP32 to INT8), is also utilized in SR networks for establishing mobile
compatibility. This study focuses on a very important but mostly overlooked
post-training quantization (PTQ) step: representative dataset (RD), which
adjusts the quantization range for PTQ. We propose a novel pipeline (clip-free
quantization pipeline, CFQP) backed up with extensive experimental
justifications to cleverly augment RD images by only using outputs of the FP32
model. Using the proposed pipeline for RD, we can successfully eliminate
unwanted clipped activation layers, which nearly all mobile SR methods utilize
to make the model more robust to PTQ in return for a large overhead in runtime.
Removing clipped activations with our method significantly benefits overall
increased stability, decreased inference runtime up to 54% on some SR models,
better visual quality results compared to INT8 clipped models - and outperforms
even some FP32 non-quantized models, both in runtime and visual quality,
without the need for retraining with clipped activation.
</p></li>
</ul>

<h3>Title: Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging. (arXiv:2308.11443v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11443">http://arxiv.org/abs/2308.11443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11443]] Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging(http://arxiv.org/abs/2308.11443)</code></li>
<li>Summary: <p>Fast Adversarial Training (FAT) not only improves the model robustness but
also reduces the training cost of standard adversarial training. However, fast
adversarial training often suffers from Catastrophic Overfitting (CO), which
results in poor robustness performance. Catastrophic Overfitting describes the
phenomenon of a sudden and significant decrease in robust accuracy during the
training of fast adversarial training. Many effective techniques have been
developed to prevent Catastrophic Overfitting and improve the model robustness
from different perspectives. However, these techniques adopt inconsistent
training settings and require different training costs, i.e, training time and
memory costs, leading to unfair comparisons. In this paper, we conduct a
comprehensive study of over 10 fast adversarial training methods in terms of
adversarial robustness and training costs. We revisit the effectiveness and
efficiency of fast adversarial training techniques in preventing Catastrophic
Overfitting from the perspective of model local nonlinearity and propose an
effective Lipschitz regularization method for fast adversarial training.
Furthermore, we explore the effect of data augmentation and weight averaging in
fast adversarial training and propose a simple yet effective auto weight
averaging method to improve robustness further. By assembling these techniques,
we propose a FGSM-based fast adversarial training method equipped with
Lipschitz regularization and Auto Weight averaging, abbreviated as FGSM-LAW.
Experimental evaluations on four benchmark databases demonstrate the
superiority of the proposed method over state-of-the-art fast adversarial
training methods and the advanced standard adversarial training methods.
</p></li>
</ul>

<h3>Title: Free Lunch for Gait Recognition: A Novel Relation Descriptor. (arXiv:2308.11487v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11487">http://arxiv.org/abs/2308.11487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11487]] Free Lunch for Gait Recognition: A Novel Relation Descriptor(http://arxiv.org/abs/2308.11487)</code></li>
<li>Summary: <p>Gait recognition is to seek correct matches for query individuals by their
unique walking patterns at a long distance. However, current methods focus
solely on individual gait features, disregarding inter-personal relationships.
In this paper, we reconsider gait representation, asserting that gait is not
just an aggregation of individual features, but also the relationships among
different subjects' gait features once reference gaits are established. From
this perspective, we redefine classifier weights as reference-anchored gaits,
allowing each person's gait to be described by their relationship with these
references. In our work, we call this novel descriptor Relationship Descriptor
(RD). This Relationship Descriptor offers two benefits: emphasizing meaningful
features and enhancing robustness. To be specific, The normalized dot product
between gait features and classifier weights signifies a similarity relation,
where each dimension indicates the similarity between the test sample and each
training ID's gait prototype, respectively. Despite its potential, the direct
use of relationship descriptors poses dimensionality challenges since the
dimension of RD depends on the training set's identity count. To address this,
we propose a Farthest Anchored gaits Selection algorithm and a dimension
reduction method to boost gait recognition performance. Our method can be built
on top of off-the-shelf pre-trained classification-based models without extra
parameters. We show that RD achieves higher recognition performance than
directly using extracted features. We evaluate the effectiveness of our method
on the popular GREW, Gait3D, CASIA-B, and OU-MVLP, showing that our method
consistently outperforms the baselines and achieves state-of-the-art
performances.
</p></li>
</ul>

<h3>Title: Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>Code URL: https://github.com/gengyuanmax/mevtr</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11551]] Multi-event Video-Text Retrieval(http://arxiv.org/abs/2308.11551)</code></li>
<li>Summary: <p>Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive
video-text data on the Internet. A plethora of work characterized by using a
two-stream Vision-Language model architecture that learns a joint
representation of video-text pairs has become a prominent approach for the VTR
task. However, these models operate under the assumption of bijective
video-text correspondences and neglect a more practical scenario where video
content usually encompasses multiple events, while texts like user queries or
webpage metadata tend to be specific and correspond to single events. This
establishes a gap between the previous training objective and real-world
applications, leading to the potential performance degradation of earlier
models during inference. In this study, we introduce the Multi-event Video-Text
Retrieval (MeVTR) task, addressing scenarios in which each video contains
multiple different events, as a niche scenario of the conventional Video-Text
Retrieval Task. We present a simple model, Me-Retriever, which incorporates key
event video representation and a new MeVTR loss for the MeVTR task.
Comprehensive experiments show that this straightforward framework outperforms
other models in the Video-to-Text and Text-to-Video tasks, effectively
establishing a robust baseline for the MeVTR task. We believe this work serves
as a strong foundation for future studies. Code is available at
https://github.com/gengyuanmax/MeVTR.
</p></li>
</ul>

<h3>Title: G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model. (arXiv:2308.11573v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11573">http://arxiv.org/abs/2308.11573</a></li>
<li>Code URL: https://github.com/hkust-aerial-robotics/g3reg</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11573]] G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid Model(http://arxiv.org/abs/2308.11573)</code></li>
<li>Summary: <p>This study introduces a novel framework, G3Reg, for fast and robust global
registration of LiDAR point clouds. In contrast to conventional complex
keypoints and descriptors, we extract fundamental geometric primitives
including planes, clusters, and lines (PCL) from the raw point cloud to obtain
low-level semantic segments. Each segment is formulated as a unified Gaussian
Ellipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground
truth centers are encompassed with a certain degree of probability. Utilizing
these GEMs, we then present a distrust-and-verify scheme based on a Pyramid
Compatibility Graph for Global Registration (PAGOR). Specifically, we establish
an upper bound, which can be traversed based on the confidence level for
compatibility testing to construct the pyramid graph. Gradually, we solve
multiple maximum cliques (MAC) for each level of the graph, generating numerous
transformation candidates. In the verification phase, we adopt a precise and
efficient metric for point cloud alignment quality, founded on geometric
primitives, to identify the optimal candidate. The performance of the algorithm
is extensively validated on three publicly available datasets and a
self-collected multi-session dataset, without changing any parameter settings
in the experimental evaluation. The results exhibit superior robustness and
real-time performance of the G3Reg framework compared to state-of-the-art
methods. Furthermore, we demonstrate the potential for integrating individual
GEM and PAGOR components into other algorithmic frameworks to enhance their
efficacy. To advance further research and promote community understanding, we
have publicly shared the source code.
</p></li>
</ul>

<h3>Title: Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11490">http://arxiv.org/abs/2308.11490</a></li>
<li>Code URL: https://github.com/llnl/luar</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11490]] Can Authorship Representation Learning Capture Stylistic Features?(http://arxiv.org/abs/2308.11490)</code></li>
<li>Summary: <p>Automatically disentangling an author's style from the content of their
writing is a longstanding and possibly insurmountable problem in computational
linguistics. At the same time, the availability of large text corpora furnished
with author labels has recently enabled learning authorship representations in
a purely data-driven manner for authorship attribution, a task that ostensibly
depends to a greater extent on encoding writing style than encoding content.
However, success on this surrogate task does not ensure that such
representations capture writing style since authorship could also be correlated
with other latent variables, such as topic. In an effort to better understand
the nature of the information these representations convey, and specifically to
validate the hypothesis that they chiefly encode writing style, we
systematically probe these representations through a series of targeted
experiments. The results of these experiments suggest that representations
learned for the surrogate authorship prediction task are indeed sensitive to
writing style. As a consequence, authorship representations may be expected to
be robust to certain kinds of data shift, such as topic drift over time.
Additionally, our findings may open the door to downstream applications that
require stylistic representations, such as style transfer.
</p></li>
</ul>

<h3>Title: SeamlessM4T-Massively Multilingual & Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11596">http://arxiv.org/abs/2308.11596</a></li>
<li>Code URL: https://github.com/facebookresearch/seamless_communication</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11596]] SeamlessM4T-Massively Multilingual & Multimodal Machine Translation(http://arxiv.org/abs/2308.11596)</code></li>
<li>Summary: <p>What does it take to create the Babel Fish, a tool that can help individuals
translate speech between any two languages? While recent breakthroughs in
text-based models have pushed machine translation coverage beyond 200
languages, unified speech-to-speech translation models have yet to achieve
similar strides. More specifically, conventional speech-to-speech translation
systems rely on cascaded systems that perform translation progressively,
putting high-performing unified systems out of reach. To address these gaps, we
introduce SeamlessM4T, a single model that supports speech-to-speech
translation, speech-to-text translation, text-to-speech translation,
text-to-text translation, and automatic speech recognition for up to 100
languages. To build this, we used 1 million hours of open speech audio data to
learn self-supervised speech representations with w2v-BERT 2.0. Subsequently,
we created a multimodal corpus of automatically aligned speech translations.
Filtered and combined with human-labeled and pseudo-labeled data, we developed
the first multilingual system capable of translating from and into English for
both speech and text. On FLEURS, SeamlessM4T sets a new standard for
translations into multiple target languages, achieving an improvement of 20%
BLEU over the previous SOTA in direct speech-to-text translation. Compared to
strong cascaded models, SeamlessM4T improves the quality of into-English
translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in
speech-to-speech. Tested for robustness, our system performs better against
background noises and speaker variations in speech-to-text tasks compared to
the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and
added toxicity to assess translation safety. Finally, all contributions in this
work are open-sourced at this https
https://github.com/facebookresearch/seamless_communication.
</p></li>
</ul>

<h3>Title: Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11197">http://arxiv.org/abs/2308.11197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11197]] Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation(http://arxiv.org/abs/2308.11197)</code></li>
<li>Summary: <p>This study's first purpose is to provide quantitative evidence that would
incentivize researchers to instead use the more robust method of nested
cross-validation. The second purpose is to present methods and MATLAB codes for
doing power analysis for ML-based analysis during the design of a study. Monte
Carlo simulations were used to quantify the interactions between the employed
cross-validation method, the discriminative power of features, the
dimensionality of the feature space, and the dimensionality of the model. Four
different cross-validations (single holdout, 10-fold, train-validation-test,
and nested 10-fold) were compared based on the statistical power and
statistical confidence of the ML models. Distributions of the null and
alternative hypotheses were used to determine the minimum required sample size
for obtaining a statistically significant outcome ({\alpha}=0.05,
1-\b{eta}=0.8). Statistical confidence of the model was defined as the
probability of correct features being selected and hence being included in the
final model. Our analysis showed that the model generated based on the single
holdout method had very low statistical power and statistical confidence and
that it significantly overestimated the accuracy. Conversely, the nested
10-fold cross-validation resulted in the highest statistical confidence and the
highest statistical power, while providing an unbiased estimate of the
accuracy. The required sample size with a single holdout could be 50% higher
than what would be needed if nested cross-validation were used. Confidence in
the model based on nested cross-validation was as much as four times higher
than the confidence in the single holdout-based model. A computational model,
MATLAB codes, and lookup tables are provided to assist researchers with
estimating the sample size during the design of their future studies.
</p></li>
</ul>

<h3>Title: Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11267">http://arxiv.org/abs/2308.11267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11267]] Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes(http://arxiv.org/abs/2308.11267)</code></li>
<li>Summary: <p>The robust constrained Markov decision process (RCMDP) is a recent
task-modelling framework for reinforcement learning that incorporates
behavioural constraints and that provides robustness to errors in the
transition dynamics model through the use of an uncertainty set. Simulating
RCMDPs requires computing the worst-case dynamics based on value estimates for
each state, an approach which has previously been used in the Robust
Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG
such as not robustifying the full constrained objective and the lack of
incremental learning, this paper introduces two algorithms, called RCPG with
Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies
RCPG by taking the worst-case dynamics based on the Lagrangian rather than
either the value or the constraint. Adversarial RCPG also formulates the
worst-case dynamics based on the Lagrangian but learns this directly and
incrementally as an adversarial policy through gradient descent rather than
indirectly and abruptly through constrained optimisation on a sorted value
list. A theoretical analysis first derives the Lagrangian policy gradient for
the policy optimisation of both proposed algorithms and then the adversarial
policy gradient to learn the adversary for Adversarial RCPG. Empirical
experiments injecting perturbations in inventory management and safe navigation
tasks demonstrate the competitive performance of both algorithms compared to
traditional RCPG variants as well as non-robust and non-constrained ablations.
In particular, Adversarial RCPG ranks among the top two performing algorithms
on all tests.
</p></li>
</ul>

<h3>Title: Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models. (arXiv:2308.11511v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11511">http://arxiv.org/abs/2308.11511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11511]] Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models(http://arxiv.org/abs/2308.11511)</code></li>
<li>Summary: <p>We explore element-wise convex combinations of two permutation-aligned neural
network parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conduct
extensive experiments by examining various distributions of such model
combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its
vicinity. Our findings reveal that broad regions of the hypercube form surfaces
of low loss values, indicating that the notion of linear mode connectivity
extends to a more general phenomenon which we call mode combinability. We also
make several novel observations regarding linear mode connectivity and model
re-basin. We demonstrate a transitivity property: two models re-based to a
common third model are also linear mode connected, and a robustness property:
even with significant perturbations of the neuron matchings the resulting
combinations continue to form a working model. Moreover, we analyze the
functional and weight similarity of model combinations and show that such
combinations are non-vacuous in the sense that there are significant functional
differences between the resulting models.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm. (arXiv:2308.10990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10990">http://arxiv.org/abs/2308.10990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10990]] Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm(http://arxiv.org/abs/2308.10990)</code></li>
<li>Summary: <p>Pore-network models (PNMs) have become an important tool in the study of
fluid flow in porous media over the last few decades, and the accuracy of their
results highly depends on the extraction of pore networks. Traditional methods
of pore-network extraction are based on pixels and require images with high
quality. Here, a pixel-free method called the flashlight search medial axis
(FSMA) algorithm is proposed for pore-network extraction in a continuous space.
The search domain in a two-dimensional space is a line, whereas a surface
domain is searched in a three-dimensional scenario. Thus, the FSMA algorithm
follows the dimensionality reduction idea; the medial axis can be identified
using only a few points instead of calculating every point in the void space.
In this way, computational complexity of this method is greatly reduced
compared to that of traditional pixel-based extraction methods, thus enabling
large-scale pore-network extraction. Based on cases featuring two- and
three-dimensional porous media, the FSMA algorithm performs well regardless of
the topological structure of the pore network or the positions of the pore and
throat centers. This algorithm can also be used to examine both closed- and
open-boundary cases. Finally, the FSMA algorithm can search dead-end pores,
which is of great significance in the study of multiphase flow in porous media.
</p></li>
</ul>

<h3>Title: Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding. (arXiv:2308.11448v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11448">http://arxiv.org/abs/2308.11448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11448]] Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding(http://arxiv.org/abs/2308.11448)</code></li>
<li>Summary: <p>Self-supervised pretraining (SSP) has emerged as a popular technique in
machine learning, enabling the extraction of meaningful feature representations
without labelled data. In the realm of computer vision, pretrained vision
transformers (ViTs) have played a pivotal role in advancing transfer learning.
Nonetheless, the escalating cost of finetuning these large models has posed a
challenge due to the explosion of model size. This study endeavours to evaluate
the effectiveness of pure self-supervised learning (SSL) techniques in computer
vision tasks, obviating the need for finetuning, with the intention of
emulating human-like capabilities in generalisation and recognition of unseen
objects. To this end, we propose an evaluation protocol for zero-shot
segmentation based on a prompting patch. Given a point on the target object as
a prompt, the algorithm calculates the similarity map between the selected
patch and other patches, upon that, a simple thresholding is applied to segment
the target. Another evaluation is intra-object and inter-object similarity to
gauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentation
from prompting and discriminatory abilities of SSP led to the design of a
simple SSP approach, termed MMC. This approaches combines Masked image
modelling for encouraging similarity of local features, Momentum based
self-distillation for transferring semantics from global to local features, and
global Contrast for promoting semantics of global features, to enhance
discriminative representations of SSP ViTs. Consequently, our proposed method
significantly reduces the overlap of intra-object and inter-object
similarities, thereby facilitating effective object segmentation within an
image. Our experiments reveal that MMC delivers top-tier results in zero-shot
semantic segmentation across various datasets.
</p></li>
</ul>

<h3>Title: Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm. (arXiv:2308.11411v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11411">http://arxiv.org/abs/2308.11411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11411]] Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm(http://arxiv.org/abs/2308.11411)</code></li>
<li>Summary: <p>Extracting relational triples (subject, predicate, object) from text enables
the transformation of unstructured text data into structured knowledge. The
named entity recognition (NER) and the relation extraction (RE) are two
foundational subtasks in this knowledge generation pipeline. The integration of
subtasks poses a considerable challenge due to their disparate nature. This
paper presents a novel approach that converts the triple extraction task into a
graph labeling problem, capitalizing on the structural information of
dependency parsing and graph recursive neural networks (GRNNs). To integrate
subtasks, this paper proposes a dynamic feedback forest algorithm that connects
the representations of subtasks by inference operations during model training.
Experimental results demonstrate the effectiveness of the proposed method.
</p></li>
</ul>

<h3>Title: BELB: a Biomedical Entity Linking Benchmark. (arXiv:2308.11537v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11537">http://arxiv.org/abs/2308.11537</a></li>
<li>Code URL: https://github.com/sg-wbi/belb-exp</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11537]] BELB: a Biomedical Entity Linking Benchmark(http://arxiv.org/abs/2308.11537)</code></li>
<li>Summary: <p>Biomedical entity linking (BEL) is the task of grounding entity mentions to a
knowledge base. It plays a vital role in information extraction pipelines for
the life sciences literature. We review recent work in the field and find that,
as the task is absent from existing benchmarks for biomedical text mining,
different studies adopt different experimental setups making comparisons based
on published numbers problematic. Furthermore, neural systems are tested
primarily on instances linked to the broad coverage knowledge base UMLS,
leaving their performance to more specialized ones, e.g. genes or variants,
understudied. We therefore developed BELB, a Biomedical Entity Linking
Benchmark, providing access in a unified format to 11 corpora linked to 7
knowledge bases and spanning six entity types: gene, disease, chemical,
species, cell line and variant. BELB greatly reduces preprocessing overhead in
testing BEL systems on multiple corpora offering a standardized testbed for
reproducible experiments. Using BELB we perform an extensive evaluation of six
rule-based entity-specific systems and three recent neural approaches
leveraging pre-trained language models. Our results reveal a mixed picture
showing that neural approaches fail to perform consistently across entity
types, highlighting the need of further studies towards entity-agnostic models.
</p></li>
</ul>

<h3>Title: Semantic Multi-Resolution Communications. (arXiv:2308.11604v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11604">http://arxiv.org/abs/2308.11604</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11604]] Semantic Multi-Resolution Communications(http://arxiv.org/abs/2308.11604)</code></li>
<li>Summary: <p>Deep learning based joint source-channel coding (JSCC) has demonstrated
significant advancements in data reconstruction compared to separate
source-channel coding (SSCC). This superiority arises from the suboptimality of
SSCC when dealing with finite block-length data. Moreover, SSCC falls short in
reconstructing data in a multi-user and/or multi-resolution fashion, as it only
tries to satisfy the worst channel and/or the highest quality data. To overcome
these limitations, we propose a novel deep learning multi-resolution JSCC
framework inspired by the concept of multi-task learning (MTL). This proposed
framework excels at encoding data for different resolutions through
hierarchical layers and effectively decodes it by leveraging both current and
past layers of encoded data. Moreover, this framework holds great potential for
semantic communication, where the objective extends beyond data reconstruction
to preserving specific semantic attributes throughout the communication
process. These semantic features could be crucial elements such as class
labels, essential for classification tasks, or other key attributes that
require preservation. Within this framework, each level of encoded data can be
carefully designed to retain specific data semantics. As a result, the
precision of a semantic classifier can be progressively enhanced across
successive layers, emphasizing the preservation of targeted semantics
throughout the encoding and decoding stages. We conduct experiments on MNIST
and CIFAR10 dataset. The experiment with both datasets illustrates that our
proposed method is capable of surpassing the SSCC method in reconstructing data
with different resolutions, enabling the extraction of semantic features with
heightened confidence in successive layers. This capability is particularly
advantageous for prioritizing and preserving more crucial semantic features
within the datasets.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11217">http://arxiv.org/abs/2308.11217</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11217]] Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models(http://arxiv.org/abs/2308.11217)</code></li>
<li>Summary: <p>Multimodal data, which can comprehensively perceive and recognize the
physical world, has become an essential path towards general artificial
intelligence. However, multimodal large models trained on public datasets often
underperform in specific industrial domains. This paper proposes a multimodal
federated learning framework that enables multiple enterprises to utilize
private domain data to collaboratively train large models for vertical domains,
achieving intelligent services across scenarios. The authors discuss in-depth
the strategic transformation of federated learning in terms of intelligence
foundation and objectives in the era of big model, as well as the new
challenges faced in heterogeneous data, model aggregation, performance and cost
trade-off, data privacy, and incentive mechanism. The paper elaborates a case
study of leading enterprises contributing multimodal data and expert knowledge
to city safety operation management , including distributed deployment and
efficient coordination of the federated learning platform, technical
innovations on data quality improvement based on large model capabilities and
efficient joint fine-tuning approaches. Preliminary experiments show that
enterprises can enhance and accumulate intelligent capabilities through
multimodal model federated learning, thereby jointly creating an smart city
model that provides high-quality intelligent services covering energy
infrastructure safety, residential community security, and urban operation
management. The established federated learning cooperation ecosystem is
expected to further aggregate industry, academia, and research resources,
realize large models in multiple vertical domains, and promote the large-scale
industrial application of artificial intelligence and cutting-edge research on
multimodal federated learning.
</p></li>
</ul>

<h3>Title: Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11464">http://arxiv.org/abs/2308.11464</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11464]] Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning(http://arxiv.org/abs/2308.11464)</code></li>
<li>Summary: <p>Federated learning (FL) inevitably confronts the challenge of system
heterogeneity in practical scenarios. To enhance the capabilities of most
model-homogeneous FL methods in handling system heterogeneity, we propose a
training scheme that can extend their capabilities to cope with this challenge.
In this paper, we commence our study with a detailed exploration of homogeneous
and heterogeneous FL settings and discover three key observations: (1) a
positive correlation between client performance and layer similarities, (2)
higher similarities in the shallow layers in contrast to the deep layers, and
(3) the smoother gradients distributions indicate the higher layer
similarities. Building upon these observations, we propose InCo Aggregation
that leverags internal cross-layer gradients, a mixture of gradients from
shallow and deep layers within a server model, to augment the similarity in the
deep layers without requiring additional communication between clients.
Furthermore, our methods can be tailored to accommodate model-homogeneous FL
methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their
capabilities to handle the system heterogeneity. Copious experimental results
validate the effectiveness of InCo Aggregation, spotlighting internal
cross-layer gradients as a promising avenue to enhance the performance in
heterogenous FL.
</p></li>
</ul>

<h3>Title: EM for Mixture of Linear Regression with Clustered Data. (arXiv:2308.11518v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11518">http://arxiv.org/abs/2308.11518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11518]] EM for Mixture of Linear Regression with Clustered Data(http://arxiv.org/abs/2308.11518)</code></li>
<li>Summary: <p>Modern data-driven and distributed learning frameworks deal with diverse
massive data generated by clients spread across heterogeneous environments.
Indeed, data heterogeneity is a major bottleneck in scaling up many distributed
learning paradigms. In many settings however, heterogeneous data may be
generated in clusters with shared structures, as is the case in several
applications such as federated learning where a common latent variable governs
the distribution of all the samples generated by a client. It is therefore
natural to ask how the underlying clustered structures in distributed data can
be exploited to improve learning schemes. In this paper, we tackle this
question in the special case of estimating $d$-dimensional parameters of a
two-component mixture of linear regressions problem where each of $m$ nodes
generates $n$ samples with a shared latent variable. We employ the well-known
Expectation-Maximization (EM) method to estimate the maximum likelihood
parameters from $m$ batches of dependent samples each containing $n$
measurements. Discarding the clustered structure in the mixture model, EM is
known to require $O(\log(mn/d))$ iterations to reach the statistical accuracy
of $O(\sqrt{d/(mn)})$. In contrast, we show that if initialized properly, EM on
the structured data requires only $O(1)$ iterations to reach the same
statistical accuracy, as long as $m$ grows up as $e^{o(n)}$. Our analysis
establishes and combines novel asymptotic optimization and generalization
guarantees for population and empirical EM with dependent samples, which may be
of independent interest.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Addressing Fairness and Explainability in Image Classification Using Optimal Transport. (arXiv:2308.11090v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11090">http://arxiv.org/abs/2308.11090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11090]] Addressing Fairness and Explainability in Image Classification Using Optimal Transport(http://arxiv.org/abs/2308.11090)</code></li>
<li>Summary: <p>Algorithmic Fairness and the explainability of potentially unfair outcomes
are crucial for establishing trust and accountability of Artificial
Intelligence systems in domains such as healthcare and policing. Though
significant advances have been made in each of the fields separately, achieving
explainability in fairness applications remains challenging, particularly so in
domains where deep neural networks are used. At the same time, ethical
data-mining has become ever more relevant, as it has been shown countless times
that fairness-unaware algorithms result in biased outcomes. Current approaches
focus on mitigating biases in the outcomes of the model, but few attempts have
been made to try to explain \emph{why} a model is biased. To bridge this gap,
we propose a comprehensive approach that leverages optimal transport theory to
uncover the causes and implications of biased regions in images, which easily
extends to tabular data as well. Through the use of Wasserstein barycenters, we
obtain scores that are independent of a sensitive variable but keep their
marginal orderings. This step ensures predictive accuracy but also helps us to
recover the regions most associated with the generation of the biases. Our
findings hold significant implications for the development of trustworthy and
unbiased AI systems, fostering transparency, accountability, and fairness in
critical decision-making scenarios across diverse domains.
</p></li>
</ul>

<h3>Title: Targeted Data Augmentation for bias mitigation. (arXiv:2308.11386v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11386">http://arxiv.org/abs/2308.11386</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11386]] Targeted Data Augmentation for bias mitigation(http://arxiv.org/abs/2308.11386)</code></li>
<li>Summary: <p>The development of fair and ethical AI systems requires careful consideration
of bias mitigation, an area often overlooked or ignored. In this study, we
introduce a novel and efficient approach for addressing biases called Targeted
Data Augmentation (TDA), which leverages classical data augmentation techniques
to tackle the pressing issue of bias in data and models. Unlike the laborious
task of removing biases, our method proposes to insert biases instead,
resulting in improved performance. To identify biases, we annotated two diverse
datasets: a dataset of clinical skin lesions and a dataset of male and female
faces. These bias annotations are published for the first time in this study,
providing a valuable resource for future research. Through Counterfactual Bias
Insertion, we discovered that biases associated with the frame, ruler, and
glasses had a significant impact on models. By randomly introducing biases
during training, we mitigated these biases and achieved a substantial decrease
in bias measures, ranging from two-fold to more than 50-fold, while maintaining
a negligible increase in the error rate.
</p></li>
</ul>

<h3>Title: Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11531">http://arxiv.org/abs/2308.11531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11531]] Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law(http://arxiv.org/abs/2308.11531)</code></li>
<li>Summary: <p>Our project aims at helping and supporting stakeholders in refugee status
adjudications, such as lawyers, judges, governing bodies, and claimants, in
order to make better decisions through data-driven intelligence and increase
the understanding and transparency of the refugee application process for all
involved parties. This PhD project has two primary objectives: (1) to retrieve
past cases, and (2) to analyze legal decision-making processes on a dataset of
Canadian cases. In this paper, we present the current state of our work, which
includes a completed experiment on part (1) and ongoing efforts related to part
(2). We believe that NLP-based solutions are well-suited to address these
challenges, and we investigate the feasibility of automating all steps
involved. In addition, we introduce a novel benchmark for future NLP research
in refugee law. Our methodology aims to be inclusive to all end-users and
stakeholders, with expected benefits including reduced time-to-decision, fairer
and more transparent outcomes, and improved decision quality.
</p></li>
</ul>

<h3>Title: A survey on bias in machine learning research. (arXiv:2308.11254v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11254">http://arxiv.org/abs/2308.11254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11254]] A survey on bias in machine learning research(http://arxiv.org/abs/2308.11254)</code></li>
<li>Summary: <p>Current research on bias in machine learning often focuses on fairness, while
overlooking the roots or causes of bias. However, bias was originally defined
as a "systematic error," often caused by humans at different stages of the
research process. This article aims to bridge the gap between past literature
on bias in research by providing taxonomy for potential sources of bias and
errors in data and models. The paper focus on bias in machine learning
pipelines. Survey analyses over forty potential sources of bias in the machine
learning (ML) pipeline, providing clear examples for each. By understanding the
sources and consequences of bias in machine learning, better methods can be
developed for its detecting and mitigating, leading to fairer, more
transparent, and more accurate ML models.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11257">http://arxiv.org/abs/2308.11257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11257]] HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge(http://arxiv.org/abs/2308.11257)</code></li>
<li>Summary: <p>The semantic parsing-based method is an important research branch for
knowledge-based question answering. It usually generates executable programs
lean upon the question and then conduct them to reason answers over a knowledge
base. Benefit from this inherent mechanism, it has advantages in the
performance and the interpretability. However,traditional semantic parsing
methods usually generate a complete program before executing it, which
struggles with multi-hop question answering over heterogeneous knowledge.
Firstly,a complete multi-hop program relies on multiple heterogeneous
supporting facts, and it is difficult for models to receive these facts
simultaneously. Secondly,these methods ignore the interaction information
between the previous-hop execution result and the current-hop program
generation. To alleviate these challenges, we propose a self-iterative
framework for multi-hop program generation (HopPG) over heterogeneous
knowledge, which leverages the previous-hop execution results to retrieve
supporting facts and generate subsequent programs iteratively. We evaluate our
model on MMQA-T^2. The experimental results show that HopPG outperforms
existing semantic-parsing-based baselines, especially on the multi-hop
questions.
</p></li>
</ul>

<h3>Title: Hamiltonian GAN. (arXiv:2308.11216v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11216">http://arxiv.org/abs/2308.11216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11216]] Hamiltonian GAN(http://arxiv.org/abs/2308.11216)</code></li>
<li>Summary: <p>A growing body of work leverages the Hamiltonian formalism as an inductive
bias for physically plausible neural network based video generation. The
structure of the Hamiltonian ensures conservation of a learned quantity (e.g.,
energy) and imposes a phase-space interpretation on the low-dimensional
manifold underlying the input video. While this interpretation has the
potential to facilitate the integration of learned representations in
downstream tasks, existing methods are limited in their applicability as they
require a structural prior for the configuration space at design time. In this
work, we present a GAN-based video generation pipeline with a learned
configuration space map and Hamiltonian neural network motion model, to learn a
representation of the configuration space from data. We train our model with a
physics-inspired cyclic-coordinate loss function which encourages a minimal
representation of the configuration space and improves interpretability. We
demonstrate the efficacy and advantages of our approach on the Hamiltonian
Dynamics Suite Toy Physics dataset.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks. (arXiv:2308.11235v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11235">http://arxiv.org/abs/2308.11235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11235]] Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks(http://arxiv.org/abs/2308.11235)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) has found wide application, but also poses risks
due to unintentional or malicious tampering during deployment. Regular checks
are therefore necessary to detect and prevent such risks. Fragile watermarking
is a technique used to identify tampering in AI models. However, previous
methods have faced challenges including risks of omission, additional
information transmission, and inability to locate tampering precisely. In this
paper, we propose a method for detecting tampered parameters and bits, which
can be used to detect, locate, and restore parameters that have been tampered
with. We also propose an adaptive embedding method that maximizes information
capacity while maintaining model accuracy. Our approach was tested on multiple
neural networks subjected to attacks that modified weight parameters, and our
results demonstrate that our method achieved great recovery performance when
the modification rate was below 20%. Furthermore, for models where watermarking
significantly affected accuracy, we utilized an adaptive bit technique to
recover more than 15% of the accuracy loss of the model.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: Diffusion Model as Representation Learner. (arXiv:2308.10916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10916">http://arxiv.org/abs/2308.10916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10916]] Diffusion Model as Representation Learner(http://arxiv.org/abs/2308.10916)</code></li>
<li>Summary: <p>Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive
results on various generative tasks.Despite its promises, the learned
representations of pre-trained DPMs, however, have not been fully understood.
In this paper, we conduct an in-depth investigation of the representation power
of DPMs, and propose a novel knowledge transfer method that leverages the
knowledge acquired by generative DPMs for recognition tasks. Our study begins
by examining the feature space of DPMs, revealing that DPMs are inherently
denoising autoencoders that balance the representation learning with
regularizing model capacity. To this end, we introduce a novel knowledge
transfer paradigm named RepFusion. Our paradigm extracts representations at
different time steps from off-the-shelf DPMs and dynamically employs them as
supervision for student networks, in which the optimal time is determined
through reinforcement learning. We evaluate our approach on several image
classification, semantic segmentation, and landmark detection benchmarks, and
demonstrate that it outperforms state-of-the-art methods. Our results uncover
the potential of DPMs as a powerful tool for representation learning and
provide insights into the usefulness of generative models beyond sample
generation. The code is available at
\url{https://github.com/Adamdad/Repfusion}.
</p></li>
</ul>

<h3>Title: DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment. (arXiv:2308.11206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11206">http://arxiv.org/abs/2308.11206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11206]] DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment(http://arxiv.org/abs/2308.11206)</code></li>
<li>Summary: <p>Cross-modal garment synthesis and manipulation will significantly benefit the
way fashion designers generate garments and modify their designs via flexible
linguistic interfaces.Current approaches follow the general text-to-image
paradigm and mine cross-modal relations via simple cross-attention modules,
neglecting the structural correspondence between visual and textual
representations in the fashion design domain. In this work, we instead
introduce DiffCloth, a diffusion-based pipeline for cross-modal garment
synthesis and manipulation, which empowers diffusion models with flexible
compositionality in the fashion domain by structurally aligning the cross-modal
semantics. Specifically, we formulate the part-level cross-modal alignment as a
bipartite matching problem between the linguistic Attribute-Phrases (AP) and
the visual garment parts which are obtained via constituency parsing and
semantic segmentation, respectively. To mitigate the issue of attribute
confusion, we further propose a semantic-bundled cross-attention to preserve
the spatial structure similarities between the attention maps of attribute
adjectives and part nouns in each AP. Moreover, DiffCloth allows for
manipulation of the generated results by simply replacing APs in the text
prompts. The manipulation-irrelevant regions are recognized by blended masks
obtained from the bundled attention maps of the APs and kept unchanged.
Extensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth
both yields state-of-the-art garment synthesis results by leveraging the
inherent structural information and supports flexible manipulation with region
consistency.
</p></li>
</ul>

<h3>Title: MatFuse: Controllable Material Generation with Diffusion Models. (arXiv:2308.11408v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11408">http://arxiv.org/abs/2308.11408</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11408]] MatFuse: Controllable Material Generation with Diffusion Models(http://arxiv.org/abs/2308.11408)</code></li>
<li>Summary: <p>Creating high quality and realistic materials in computer graphics is a
challenging and time-consuming task, which requires great expertise. In this
paper, we present MatFuse, a novel unified approach that harnesses the
generative power of diffusion models (DM) to simplify the creation of SVBRDF
maps. Our DM-based pipeline integrates multiple sources of conditioning, such
as color palettes, sketches, and pictures, enabling fine-grained control and
flexibility in material synthesis. This design allows for the combination of
diverse information sources (e.g., sketch + image embedding), enhancing
creative possibilities in line with the principle of compositionality. We
demonstrate the generative capabilities of the proposed method under various
conditioning settings; on the SVBRDF estimation task, we show that our method
yields performance comparable to state-of-the-art approaches, both
qualitatively and quantitatively.
</p></li>
</ul>

<h3>Title: IT3D: Improved Text-to-3D Generation with Explicit View Synthesis. (arXiv:2308.11473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11473">http://arxiv.org/abs/2308.11473</a></li>
<li>Code URL: https://github.com/buaacyw/it3d-text-to-3d</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11473]] IT3D: Improved Text-to-3D Generation with Explicit View Synthesis(http://arxiv.org/abs/2308.11473)</code></li>
<li>Summary: <p>Recent strides in Text-to-3D techniques have been propelled by distilling
knowledge from powerful large text-to-image diffusion models (LDMs).
Nonetheless, existing Text-to-3D approaches often grapple with challenges such
as over-saturation, inadequate detailing, and unrealistic outputs. This study
presents a novel strategy that leverages explicitly synthesized multi-view
images to address these issues. Our approach involves the utilization of
image-to-image pipelines, empowered by LDMs, to generate posed high-quality
images based on the renderings of coarse 3D models. Although the generated
images mostly alleviate the aforementioned issues, challenges such as view
inconsistency and significant content variance persist due to the inherent
generative nature of large diffusion models, posing extensive difficulties in
leveraging these images effectively. To overcome this hurdle, we advocate
integrating a discriminator alongside a novel Diffusion-GAN dual training
strategy to guide the training of 3D models. For the incorporated
discriminator, the synthesized multi-view images are considered real data,
while the renderings of the optimized 3D models function as fake data. We
conduct a comprehensive set of experiments that demonstrate the effectiveness
of our method over baseline approaches.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images. (arXiv:2308.11015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11015">http://arxiv.org/abs/2308.11015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11015]] Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images(http://arxiv.org/abs/2308.11015)</code></li>
<li>Summary: <p>We propose a novel transformer-based framework that reconstructs two high
fidelity hands from multi-view RGB images. Unlike existing hand pose estimation
methods, where one typically trains a deep network to regress hand model
parameters from single RGB image, we consider a more challenging problem
setting where we directly regress the absolute root poses of two-hands with
extended forearm at high resolution from egocentric view. As existing datasets
are either infeasible for egocentric viewpoints or lack background variations,
we create a large-scale synthetic dataset with diverse scenarios and collect a
real dataset from multi-calibrated camera setup to verify our proposed
multi-view image feature fusion strategy. To make the reconstruction physically
plausible, we propose two strategies: (i) a coarse-to-fine spectral graph
convolution decoder to smoothen the meshes during upsampling and (ii) an
optimisation-based refinement stage at inference to prevent self-penetrations.
Through extensive quantitative and qualitative evaluations, we show that our
framework is able to produce realistic two-hand reconstructions and demonstrate
the generalisation of synthetic-trained models to real data, as well as
real-time AR/VR applications.
</p></li>
</ul>

<h3>Title: Video OWL-ViT: Temporally-consistent open-world localization in video. (arXiv:2308.11093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11093">http://arxiv.org/abs/2308.11093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11093]] Video OWL-ViT: Temporally-consistent open-world localization in video(http://arxiv.org/abs/2308.11093)</code></li>
<li>Summary: <p>We present an architecture and a training recipe that adapts pre-trained
open-world image models to localization in videos. Understanding the open
visual world (without being constrained by fixed label spaces) is crucial for
many real-world vision tasks. Contrastive pre-training on large image-text
datasets has recently led to significant improvements for image-level tasks.
For more structured tasks involving object localization applying pre-trained
models is more challenging. This is particularly true for video tasks, where
task-specific data is limited. We show successful transfer of open-world models
by building on the OWL-ViT open-vocabulary detection model and adapting it to
video by adding a transformer decoder. The decoder propagates object
representations recurrently through time by using the output tokens for one
frame as the object queries for the next. Our model is end-to-end trainable on
video data and enjoys improved temporal consistency compared to
tracking-by-detection baselines, while retaining the open-world capabilities of
the backbone detector. We evaluate our model on the challenging TAO-OW
benchmark and demonstrate that open-world capabilities, learned from
large-scale image-text pre-training, can be transferred successfully to
open-world localization across diverse videos.
</p></li>
</ul>

<h3>Title: SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection. (arXiv:2308.11159v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11159">http://arxiv.org/abs/2308.11159</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11159]] SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection(http://arxiv.org/abs/2308.11159)</code></li>
<li>Summary: <p>Among the current mainstream change detection networks, transformer is
deficient in the ability to capture accurate low-level details, while
convolutional neural network (CNN) is wanting in the capacity to understand
global information and establish remote spatial relationships. Meanwhile, both
of the widely used early fusion and late fusion frameworks are not able to well
learn complete change features. Therefore, based on swin transformer V2 (Swin
V2) and VGG16, we propose an end-to-end compounded dense network SwinV2DNet to
inherit the advantages of both transformer and CNN and overcome the
shortcomings of existing networks in feature learning. Firstly, it captures the
change relationship features through the densely connected Swin V2 backbone,
and provides the low-level pre-changed and post-changed features through a CNN
branch. Based on these three change features, we accomplish accurate change
detection results. Secondly, combined with transformer and CNN, we propose
mixed feature pyramid (MFP) which provides inter-layer interaction information
and intra-layer multi-scale information for complete feature learning. MFP is a
plug and play module which is experimentally proven to be also effective in
other change detection networks. Further more, we impose a self-supervision
strategy to guide a new CNN branch, which solves the untrainable problem of the
CNN branch and provides the semantic change information for the features of
encoder. The state-of-the-art (SOTA) change detection scores and fine-grained
change maps were obtained compared with other advanced methods on four commonly
used public remote sensing datasets. The code is available at
https://github.com/DalongZ/SwinV2DNet.
</p></li>
</ul>

<h3>Title: Improving Misaligned Multi-modality Image Fusion with One-stage Progressive Dense Registration. (arXiv:2308.11165v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11165">http://arxiv.org/abs/2308.11165</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11165]] Improving Misaligned Multi-modality Image Fusion with One-stage Progressive Dense Registration(http://arxiv.org/abs/2308.11165)</code></li>
<li>Summary: <p>Misalignments between multi-modality images pose challenges in image fusion,
manifesting as structural distortions and edge ghosts. Existing efforts
commonly resort to registering first and fusing later, typically employing two
cascaded stages for registration,i.e., coarse registration and fine
registration. Both stages directly estimate the respective target deformation
fields. In this paper, we argue that the separated two-stage registration is
not compact, and the direct estimation of the target deformation fields is not
accurate enough. To address these challenges, we propose a Cross-modality
Multi-scale Progressive Dense Registration (C-MPDR) scheme, which accomplishes
the coarse-to-fine registration exclusively using a one-stage optimization,
thus improving the fusion performance of misaligned multi-modality images.
Specifically, two pivotal components are involved, a dense Deformation Field
Fusion (DFF) module and a Progressive Feature Fine (PFF) module. The DFF
aggregates the predicted multi-scale deformation sub-fields at the current
scale, while the PFF progressively refines the remaining misaligned features.
Both work together to accurately estimate the final deformation fields. In
addition, we develop a Transformer-Conv-based Fusion (TCF) subnetwork that
considers local and long-range feature dependencies, allowing us to capture
more informative features from the registered infrared and visible images for
the generation of high-quality fused images. Extensive experimental analysis
demonstrates the superiority of the proposed method in the fusion of misaligned
cross-modality images.
</p></li>
</ul>

<h3>Title: ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11199">http://arxiv.org/abs/2308.11199</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11199]] ConcatPlexer: Additional Dim1 Batching for Faster ViTs(http://arxiv.org/abs/2308.11199)</code></li>
<li>Summary: <p>Transformers have demonstrated tremendous success not only in the natural
language processing (NLP) domain but also the field of computer vision,
igniting various creative approaches and applications. Yet, the superior
performance and modeling flexibility of transformers came with a severe
increase in computation costs, and hence several works have proposed methods to
reduce this burden. Inspired by a cost-cutting method originally proposed for
language models, Data Multiplexing (DataMUX), we propose a novel approach for
efficient visual recognition that employs additional dim1 batching (i.e.,
concatenation) that greatly improves the throughput with little compromise in
the accuracy. We first introduce a naive adaptation of DataMux for vision
models, Image Multiplexer, and devise novel components to overcome its
weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between
inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and
CIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B/16 with 69.5% and
83.4% validation accuracy, respectively.
</p></li>
</ul>

<h3>Title: Exemplar-Free Continual Transformer with Convolutions. (arXiv:2308.11357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11357">http://arxiv.org/abs/2308.11357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11357]] Exemplar-Free Continual Transformer with Convolutions(http://arxiv.org/abs/2308.11357)</code></li>
<li>Summary: <p>Continual Learning (CL) involves training a machine learning model in a
sequential manner to learn new information while retaining previously learned
tasks without the presence of previous training data. Although there has been
significant interest in CL, most recent CL approaches in computer vision have
focused on convolutional architectures only. However, with the recent success
of vision transformers, there is a need to explore their potential for CL.
Although there have been some recent CL approaches for vision transformers,
they either store training instances of previous tasks or require a task
identifier during test time, which can be limiting. This paper proposes a new
exemplar-free approach for class/task incremental learning called ConTraCon,
which does not require task-id to be explicitly present during inference and
avoids the need for storing previous training instances. The proposed approach
leverages the transformer architecture and involves re-weighting the key,
query, and value weights of the multi-head self-attention layers of a
transformer trained on a similar task. The re-weighting is done using
convolution, which enables the approach to maintain low parameter requirements
per task. Additionally, an image augmentation-based entropic task
identification approach is used to predict tasks without requiring task-ids
during inference. Experiments on four benchmark datasets demonstrate that the
proposed approach outperforms several competitive approaches while requiring
fewer parameters.
</p></li>
</ul>

<h3>Title: TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. (arXiv:2308.11421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11421">http://arxiv.org/abs/2308.11421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11421]] TurboViT: Generating Fast Vision Transformers via Generative Architecture Search(http://arxiv.org/abs/2308.11421)</code></li>
<li>Summary: <p>Vision transformers have shown unprecedented levels of performance in
tackling various visual perception tasks in recent years. However, the
architectural and computational complexity of such network architectures have
made them challenging to deploy in real-world applications with
high-throughput, low-memory requirements. As such, there has been significant
research recently on the design of efficient vision transformer architectures.
In this study, we explore the generation of fast vision transformer
architecture designs via generative architecture search (GAS) to achieve a
strong balance between accuracy and architectural and computational efficiency.
Through this generative architecture search process, we create TurboViT, a
highly efficient hierarchical vision transformer architecture design that is
generated around mask unit attention and Q-pooling design patterns. The
resulting TurboViT architecture design achieves significantly lower
architectural computational complexity (&gt;2.47$\times$ smaller than FasterViT-0
while achieving same accuracy) and computational complexity (&gt;3.4$\times$ fewer
FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other
state-of-the-art efficient vision transformer network architecture designs
within a similar range of accuracy on the ImageNet-1K dataset. Furthermore,
TurboViT demonstrated strong inference latency and throughput in both
low-latency and batch processing scenarios (&gt;3.21$\times$ lower latency and
&gt;3.18$\times$ higher throughput compared to FasterViT-0 for low-latency
scenario). These promising results demonstrate the efficacy of leveraging
generative architecture search for generating efficient transformer
architecture designs for high-throughput scenarios.
</p></li>
</ul>

<h3>Title: SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation. (arXiv:2308.11509v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11509">http://arxiv.org/abs/2308.11509</a></li>
<li>Code URL: https://github.com/lxq1000/swinface</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11509]] SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation(http://arxiv.org/abs/2308.11509)</code></li>
<li>Summary: <p>In recent years, vision transformers have been introduced into face
recognition and analysis and have achieved performance breakthroughs. However,
most previous methods generally train a single model or an ensemble of models
to perform the desired task, which ignores the synergy among different tasks
and fails to achieve improved prediction accuracy, increased data efficiency,
and reduced training time. This paper presents a multi-purpose algorithm for
simultaneous face recognition, facial expression recognition, age estimation,
and face attribute estimation (40 attributes including gender) based on a
single Swin Transformer. Our design, the SwinFace, consists of a single shared
backbone together with a subnet for each set of related tasks. To address the
conflicts among multiple tasks and meet the different demands of tasks, a
Multi-Level Channel Attention (MLCA) module is integrated into each
task-specific analysis subnet, which can adaptively select the features from
optimal levels and channels to perform the desired tasks. Extensive experiments
show that the proposed model has a better understanding of the face and
achieves excellent performance for all tasks. Especially, it achieves 90.97%
accuracy on RAF-DB and 0.22 $\epsilon$-error on CLAP2015, which are
state-of-the-art results on facial expression recognition and age estimation
respectively. The code and models will be made publicly available at
https://github.com/lxq1000/SwinFace.
</p></li>
</ul>

<h3>Title: Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation. (arXiv:2308.11561v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11561">http://arxiv.org/abs/2308.11561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11561]] Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation(http://arxiv.org/abs/2308.11561)</code></li>
<li>Summary: <p>This report details the method of the winning entry of the AVDN Challenge in
ICCV 2023. The competition addresses the Aerial Navigation from Dialog History
(ANDH) task, which requires a drone agent to associate dialog history with
aerial observations to reach the destination. For better cross-modal grounding
abilities of the drone agent, we propose a Target-Grounded Graph-Aware
Transformer (TG-GAT) framework. Concretely, TG-GAT first leverages a
graph-aware transformer to capture spatiotemporal dependency, which is
beneficial for navigation state tracking and robust action planning. TG-GAT
first leverages a graph-aware transformer to capture spatiotemporal
dependencies for more robust action planning. In addition, an auxiliary visual
grounding task is devised to boost the agent's awareness of referred landmarks.
Moreover, a hybrid augmentation strategy based on large language models is
utilized to mitigate data scarcity limitations. Our TG-GAT framework won the
AVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the baseline
on SPL and SR metrics, respectively. The code is available at
https://github.com/yifeisu/avdn-challenge.
</p></li>
</ul>

<h3>Title: Delving into Motion-Aware Matching for Monocular 3D Object Tracking. (arXiv:2308.11607v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11607">http://arxiv.org/abs/2308.11607</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11607]] Delving into Motion-Aware Matching for Monocular 3D Object Tracking(http://arxiv.org/abs/2308.11607)</code></li>
<li>Summary: <p>Recent advances of monocular 3D object detection facilitate the 3D
multi-object tracking task based on low-cost camera sensors. In this paper, we
find that the motion cue of objects along different time frames is critical in
3D multi-object tracking, which is less explored in existing monocular-based
approaches. In this paper, we propose a motion-aware framework for monocular 3D
MOT. To this end, we propose MoMA-M3T, a framework that mainly consists of
three motion-aware components. First, we represent the possible movement of an
object related to all object tracklets in the feature space as its motion
features. Then, we further model the historical object tracklet along the time
frame in a spatial-temporal perspective via a motion transformer. Finally, we
propose a motion-aware matching module to associate historical object tracklets
and current observations as final tracking results. We conduct extensive
experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T
achieves competitive performance against state-of-the-art methods. Moreover,
the proposed tracker is flexible and can be easily plugged into existing
image-based 3D object detectors without re-training. Code and models are
available at https://github.com/kuanchihhuang/MoMA-M3T.
</p></li>
</ul>

<h3>Title: Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers. (arXiv:2308.11519v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11519">http://arxiv.org/abs/2308.11519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11519]] Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers(http://arxiv.org/abs/2308.11519)</code></li>
<li>Summary: <p>Customer reviews play a crucial role in assessing customer satisfaction,
gathering feedback, and driving improvements for businesses. Analyzing these
reviews provides valuable insights into customer sentiments, including
compliments, comments, and suggestions. Text classification techniques enable
businesses to categorize customer reviews into distinct categories,
facilitating a better understanding of customer feedback. However, challenges
such as overfitting and bias limit the effectiveness of a single classifier in
ensuring optimal prediction. This study proposes a novel approach to address
these challenges by introducing a stacking ensemble-based multi-text
classification method that leverages transformer models. By combining multiple
single transformers, including BERT, ELECTRA, and DistilBERT, as base-level
classifiers, and a meta-level classifier based on RoBERTa, an optimal
predictive model is generated. The proposed stacking ensemble-based multi-text
classification method aims to enhance the accuracy and robustness of customer
review analysis. Experimental evaluations conducted on a real-world customer
review dataset demonstrate the effectiveness and superiority of the proposed
approach over traditional single classifier models. The stacking ensemble-based
multi-text classification method using transformers proves to be a promising
solution for businesses seeking to extract valuable insights from customer
reviews and make data-driven decisions to enhance customer satisfaction and
drive continuous improvement.
</p></li>
</ul>

<h3>Title: BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11527">http://arxiv.org/abs/2308.11527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11527]] BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction(http://arxiv.org/abs/2308.11527)</code></li>
<li>Summary: <p>Although deep pre-trained language models have shown promising benefit in a
large set of industrial scenarios, including Click-Through-Rate (CTR)
prediction, how to integrate pre-trained language models that handle only
textual signals into a prediction pipeline with non-textual features is
challenging.
</p>
<p>Up to now two directions have been explored to integrate multi-modal inputs
in fine-tuning of pre-trained language models. One consists of fusing the
outcome of language models and non-textual features through an aggregation
layer, resulting into ensemble framework, where the cross-information between
textual and non-textual inputs are only learned in the aggregation layer. The
second one consists of splitting non-textual features into fine-grained
fragments and transforming the fragments to new tokens combined with textual
ones, so that they can be fed directly to transformer layers in language
models. However, this approach increases the complexity of the learning and
inference because of the numerous additional tokens.
</p>
<p>To address these limitations, we propose in this work a novel framework
BERT4CTR, with the Uni-Attention mechanism that can benefit from the
interactions between non-textual and textual features while maintaining low
time-costs in training and inference through a dimensionality reduction.
Comprehensive experiments on both public and commercial data demonstrate that
BERT4CTR can outperform significantly the state-of-the-art frameworks to handle
multi-modal inputs and be applicable to CTR prediction.
</p></li>
</ul>

<h3>Title: Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11129">http://arxiv.org/abs/2308.11129</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11129]] Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances(http://arxiv.org/abs/2308.11129)</code></li>
<li>Summary: <p>Graph transformers need strong inductive biases to derive meaningful
attention scores. Yet, current proposals rarely address methods capturing
longer ranges, hierarchical structures, or community structures, as they appear
in various graphs such as molecules, social networks, and citation networks. In
this paper, we propose a hierarchy-distance structural encoding (HDSE), which
models a hierarchical distance between the nodes in a graph focusing on its
multi-level, hierarchical nature. In particular, this yields a framework which
can be flexibly integrated with existing graph transformers, allowing for
simultaneous application with other positional representations. Through
extensive experiments on 12 real-world datasets, we demonstrate that our HDSE
method successfully enhances various types of baseline transformers, achieving
state-of-the-art empirical performances on 10 benchmark datasets.
</p></li>
</ul>

<h3>Title: SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting. (arXiv:2308.11200v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11200">http://arxiv.org/abs/2308.11200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11200]] SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting(http://arxiv.org/abs/2308.11200)</code></li>
<li>Summary: <p>RNN-based methods have faced challenges in the Long-term Time Series
Forecasting (LTSF) domain when dealing with excessively long look-back windows
and forecast horizons. Consequently, the dominance in this domain has shifted
towards Transformer, MLP, and CNN approaches. The substantial number of
recurrent iterations are the fundamental reasons behind the limitations of RNNs
in LTSF. To address these issues, we propose two novel strategies to reduce the
number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and
Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies,
namely SegRNN, significantly reduce the required recurrent iterations for LTSF,
resulting in notable improvements in forecast accuracy and inference speed.
Extensive experiments demonstrate that SegRNN not only outperforms SOTA
Transformer-based models but also reduces runtime and memory usage by more than
78%. These achievements provide strong evidence that RNNs continue to excel in
LTSF tasks and encourage further exploration of this domain with more RNN-based
approaches. The source code is coming soon.
</p></li>
</ul>

<h3>Title: Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices. (arXiv:2308.11295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11295">http://arxiv.org/abs/2308.11295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11295]] Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices(http://arxiv.org/abs/2308.11295)</code></li>
<li>Summary: <p>Determining the degree of confidence of deep learning model in its prediction
is an open problem in the field of natural language processing. Most of the
classical methods for uncertainty estimation are quite weak for text
classification models. We set the task of obtaining an uncertainty estimate for
neural networks based on the Transformer architecture. A key feature of such
mo-dels is the attention mechanism, which supports the information flow between
the hidden representations of tokens in the neural network. We explore the
formed relationships between internal representations using Topological Data
Analysis methods and utilize them to predict model's confidence. In this paper,
we propose a method for uncertainty estimation based on the topological
properties of the attention mechanism and compare it with classical methods. As
a result, the proposed algorithm surpasses the existing methods in quality and
opens up a new area of application of the attention mechanism, but requires the
selection of topological features.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10997]] SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models(http://arxiv.org/abs/2308.10997)</code></li>
<li>Summary: <p>Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running inference multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
This method is shown to work in conjunction with the recently proposed Muse
model. The MRF encodes the compatibility among image tokens at different
spatial locations and enables us to significantly reduce the required number of
Muse prediction steps. Inference with the MRF is significantly cheaper, and its
parameters can be quickly learned through back-propagation by modeling MRF
inference as a differentiable neural-network layer. Our full model, SPEGTI,
uses this proposed MRF model to speed up Muse by 1.5X with no loss in output
image quality.
</p></li>
</ul>

<h3>Title: Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11480">http://arxiv.org/abs/2308.11480</a></li>
<li>Code URL: https://github.com/servicenow/broad-openood</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11480]] Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection(http://arxiv.org/abs/2308.11480)</code></li>
<li>Summary: <p>Improving the reliability of deployed machine learning systems often involves
developing methods to detect out-of-distribution (OOD) inputs. However,
existing research often narrowly focuses on samples from classes that are
absent from the training set, neglecting other types of plausible distribution
shifts. This limitation reduces the applicability of these methods in
real-world scenarios, where systems encounter a wide variety of anomalous
inputs. In this study, we categorize five distinct types of distribution shifts
and critically evaluate the performance of recent OOD detection methods on each
of them. We publicly release our benchmark under the name BROAD (Benchmarking
Resilience Over Anomaly Diversity). Our findings reveal that while these
methods excel in detecting unknown classes, their performance is inconsistent
when encountering other types of distribution shifts. In other words, they only
reliably detect unexpected inputs that they have been specifically designed to
expect. As a first step toward broad OOD detection, we learn a generative model
of existing detection scores with a Gaussian mixture. By doing so, we present
an ensemble approach that offers a more consistent and comprehensive solution
for broad OOD detection, demonstrating superior performance compared to
existing methods. Our code to download BROAD and reproduce our experiments is
publicly available.
</p></li>
</ul>

<h3>Title: Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11584">http://arxiv.org/abs/2308.11584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11584]] Building Emotional Support Chatbots in the Era of LLMs(http://arxiv.org/abs/2308.11584)</code></li>
<li>Summary: <p>The integration of emotional support into various conversational scenarios
presents profound societal benefits, such as social interactions, mental health
counseling, and customer service. However, there are unsolved challenges that
hinder real-world applications in this field, including limited data
availability and the absence of well-accepted model training paradigms. This
work endeavors to navigate these challenges by harnessing the capabilities of
Large Language Models (LLMs). We introduce an innovative methodology that
synthesizes human insights with the computational prowess of LLMs to curate an
extensive emotional support dialogue dataset. Our approach is initiated with a
meticulously designed set of dialogues spanning diverse scenarios as generative
seeds. By utilizing the in-context learning potential of ChatGPT, we
recursively generate an ExTensible Emotional Support dialogue dataset, named
ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,
examining the impact of diverse training strategies, ultimately yielding an LLM
meticulously optimized for emotional support interactions. An exhaustive
assessment of the resultant model showcases its proficiency in offering
emotional support, marking a pivotal step in the realm of emotional support
bots and paving the way for subsequent research and implementations.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11103">http://arxiv.org/abs/2308.11103</a></li>
<li>Code URL: https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11103]] Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models(http://arxiv.org/abs/2308.11103)</code></li>
<li>Summary: <p>Anonymity of both natural and legal persons in court rulings is a critical
aspect of privacy protection in the European Union and Switzerland. With the
advent of LLMs, concerns about large-scale re-identification of anonymized
persons are growing. In accordance with the Federal Supreme Court of
Switzerland, we explore the potential of LLMs to re-identify individuals in
court rulings by constructing a proof-of-concept using actual legal data from
the Swiss federal supreme court. Following the initial experiment, we
constructed an anonymized Wikipedia dataset as a more rigorous testing ground
to further investigate the findings. With the introduction and application of
the new task of re-identifying people in texts, we also introduce new metrics
to measure performance. We systematically analyze the factors that influence
successful re-identifications, identifying model size, input length, and
instruction tuning among the most critical determinants. Despite high
re-identification rates on Wikipedia, even the best LLMs struggled with court
decisions. The complexity is attributed to the lack of test datasets, the
necessity for substantial training resources, and data sparsity in the
information used for re-identification. In conclusion, this study demonstrates
that re-identification using LLMs may not be feasible for now, but as the
proof-of-concept on Wikipedia showed, it might become possible in the future.
We hope that our system can help enhance the confidence in the security of
anonymized decisions, thus leading to the courts being more confident to
publish decisions.
</p></li>
</ul>

<h3>Title: Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11189">http://arxiv.org/abs/2308.11189</a></li>
<li>Code URL: https://github.com/lab-v2/diversity_measures</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11189]] Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries(http://arxiv.org/abs/2308.11189)</code></li>
<li>Summary: <p>Error prediction in large language models often relies on domain-specific
information. In this paper, we present measures for quantification of error in
the response of a large language model based on the diversity of responses to a
given prompt - hence independent of the underlying application. We describe how
three such measures - based on entropy, Gini impurity, and centroid distance -
can be employed. We perform a suite of experiments on multiple datasets and
temperature settings to demonstrate that these measures strongly correlate with
the probability of failure. Additionally, we present empirical results
demonstrating how these measures can be applied to few-shot prompting,
chain-of-thought reasoning, and error detection.
</p></li>
</ul>

<h3>Title: LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11462">http://arxiv.org/abs/2308.11462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11462]] LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models(http://arxiv.org/abs/2308.11462)</code></li>
<li>Summary: <p>The advent of large language models (LLMs) and their adoption by the legal
community has given rise to the question: what types of legal reasoning can
LLMs perform? To enable greater study of this question, we present LegalBench:
a collaboratively constructed legal reasoning benchmark consisting of 162 tasks
covering six different types of legal reasoning. LegalBench was built through
an interdisciplinary process, in which we collected tasks designed and
hand-crafted by legal professionals. Because these subject matter experts took
a leading role in construction, tasks either measure legal reasoning
capabilities that are practically useful, or measure reasoning skills that
lawyers find interesting. To enable cross-disciplinary conversations about LLMs
in the law, we additionally show how popular legal frameworks for describing
legal reasoning -- which distinguish between its many forms -- correspond to
LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary.
This paper describes LegalBench, presents an empirical evaluation of 20
open-source and commercial LLMs, and illustrates the types of research
explorations LegalBench enables.
</p></li>
</ul>

<h3>Title: Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11483">http://arxiv.org/abs/2308.11483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11483]] Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions(http://arxiv.org/abs/2308.11483)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable capabilities in
various NLP tasks. However, previous works have shown these models are
sensitive towards prompt wording, and few-shot demonstrations and their order,
posing challenges to fair assessment of these models. As these models become
more powerful, it becomes imperative to understand and address these
limitations. In this paper, we focus on LLMs robustness on the task of
multiple-choice questions -- commonly adopted task to study reasoning and
fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs
towards the order of options in multiple-choice questions, we demonstrate a
considerable performance gap of approximately 13% to 75% in LLMs on different
benchmarks, when answer options are reordered, even when using demonstrations
in a few-shot setting. Through a detailed analysis, we conjecture that this
sensitivity arises when LLMs are uncertain about the prediction between the
top-2/3 choices, and specific options placements may favor certain prediction
between those top choices depending on the question caused by positional bias.
We also identify patterns in top-2 choices that amplify or mitigate the model's
bias toward option placement. We found that for amplifying bias, the optimal
strategy involves positioning the top two choices as the first and last
options. Conversely, to mitigate bias, we recommend placing these choices among
the adjacent options. To validate our conjecture, we conduct various
experiments and adopt two approaches to calibrate LLMs' predictions, leading to
up to 8 percentage points improvement across different models and benchmarks.
</p></li>
</ul>

<h3>Title: Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11521">http://arxiv.org/abs/2308.11521</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11521]] Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models(http://arxiv.org/abs/2308.11521)</code></li>
<li>Summary: <p>Large language models (LLMs), such as ChatGPT, have emerged with astonishing
capabilities approaching artificial general intelligence. While providing
convenience for various societal needs, LLMs have also lowered the cost of
generating harmful content. Consequently, LLM developers have deployed
semantic-level defenses to recognize and reject prompts that may lead to
inappropriate content. Unfortunately, these defenses are not foolproof, and
some attackers have crafted "jailbreak" prompts that temporarily hypnotize the
LLM into forgetting content defense rules and answering any improper questions.
To date, there is no clear explanation of the principles behind these
semantic-level attacks and defenses in both industry and academia.
</p>
<p>This paper investigates the LLM jailbreak problem and proposes an automatic
jailbreak method for the first time. We propose the concept of a semantic
firewall and provide three technical implementation approaches. Inspired by the
attack that penetrates traditional firewalls through reverse tunnels, we
introduce a "self-deception" attack that can bypass the semantic firewall by
inducing LLM to generate prompts that facilitate jailbreak. We generated a
total of 2,520 attack payloads in six languages (English, Russian, French,
Spanish, Chinese, and Arabic) across seven virtual scenarios, targeting the
three most common types of violations: violence, hate, and pornography. The
experiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The
success rates on the two models were 86.2% and 67%, while the failure rates
were 4.7% and 2.2%, respectively. This highlighted the effectiveness of the
proposed attack method. All experimental code and raw data will be released as
open-source to inspire future research. We believe that manipulating AI
behavior through carefully crafted prompts will become an important research
direction in the future.
</p></li>
</ul>

<h3>Title: Learning Representations on Logs for AIOps. (arXiv:2308.11526v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11526">http://arxiv.org/abs/2308.11526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11526]] Learning Representations on Logs for AIOps(http://arxiv.org/abs/2308.11526)</code></li>
<li>Summary: <p>AI for IT Operations (AIOps) is a powerful platform that Site Reliability
Engineers (SREs) use to automate and streamline operational workflows with
minimal human intervention. Automated log analysis is a critical task in AIOps
as it provides key insights for SREs to identify and address ongoing faults.
Tasks such as log format detection, log classification, and log parsing are key
components of automated log analysis. Most of these tasks require supervised
learning; however, there are multiple challenges due to limited labelled log
data and the diverse nature of log data. Large Language Models (LLMs) such as
BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled
data. These models provide generalized representations that can be effectively
used for various downstream tasks with limited labelled data. Motivated by the
success of LLMs in specific domains like science and biology, this paper
introduces a LLM for log data which is trained on public and proprietary log
data. The results of our experiments demonstrate that the proposed LLM
outperforms existing models on multiple downstream tasks. In summary, AIOps
powered by LLMs offers an efficient and effective solution for automating log
analysis tasks and enabling SREs to focus on higher-level tasks. Our proposed
LLM, trained on public and proprietary log data, offers superior performance on
multiple downstream tasks, making it a valuable addition to the AIOps platform.
</p></li>
</ul>

<h3>Title: Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11534">http://arxiv.org/abs/2308.11534</a></li>
<li>Code URL: https://github.com/FreedomIntelligence/ReaLM</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11534]] Large Language Model as a User Simulator(http://arxiv.org/abs/2308.11534)</code></li>
<li>Summary: <p>The unparalleled performance of closed-sourced ChatGPT has sparked efforts
towards its democratization, with notable strides made by leveraging real user
and ChatGPT conversations, as evidenced by Vicuna. However, while current
endeavors like Baize and UltraChat aim to auto-generate conversational data due
to challenges in gathering human participation, they primarily rely on ChatGPT
to simulate human behaviors based on directives rather than genuine human
learning. This results in a limited scope, diminished diversity, and an absence
of genuine multi-round conversational dynamics. To address the above issues, we
innovatively target human questions extracted from genuine human-machine
conversations as a learning goal and train a user simulator, UserGPT, to
produce a high-quality human-centric synthetic conversation dataset, RealChat.
Subsequently, this dataset trains our assistant model, ReaLM. Experimentally,
ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
comparison when considering equivalent training set sizes, and manual
evaluation also shows that our model is highly competitive. Impressively, when
fine-tuned with the latest LLaMA 2 model, ReaLM secured a leading score of 6.33
in the MT-Bench, outshining the contemporary same-scale models, including the
LLaMA-2-7B-chat model. Further in-depth analysis demonstrates the scalability
and transferability of our approach. A preliminary exploration into the
interplay between training set data quality and resultant model performance is
also undertaken, laying a robust groundwork for future investigations.
</p></li>
</ul>

<h3>Title: Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11578">http://arxiv.org/abs/2308.11578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11578]] Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models(http://arxiv.org/abs/2308.11578)</code></li>
<li>Summary: <p>After the inception of emotion recognition or affective computing, it has
increasingly become an active research topic due to its broad applications.
Over the past couple of decades, emotion recognition models have gradually
migrated from statistically shallow models to neural network-based deep models,
which can significantly boost the performance of emotion recognition models and
consistently achieve the best results on different benchmarks. Therefore, in
recent years, deep models have always been considered the first option for
emotion recognition. However, the debut of large language models (LLMs), such
as ChatGPT, has remarkably astonished the world due to their emerged
capabilities of zero/few-shot learning, in-context learning, chain-of-thought,
and others that are never shown in previous deep models. In the present paper,
we comprehensively investigate how the LLMs perform in emotion recognition in
terms of diverse aspects, including in-context learning, few-short learning,
accuracy, generalisation, and explanation. Moreover, we offer some insights and
pose other potential challenges, hoping to ignite broader discussions about
enhancing emotion recognition in the new era of advanced and generalised large
models.
</p></li>
</ul>

<h3>Title: Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11601]] Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model(http://arxiv.org/abs/2308.11601)</code></li>
<li>Summary: <p>The introduction of the transformer architecture and the self-attention
mechanism has led to an explosive production of language models trained on
specific downstream tasks and data domains. With over 200, 000 models in the
Hugging Face ecosystem, users grapple with selecting and optimizing models to
suit multifaceted workflows and data domains while addressing computational,
security, and recency concerns. There is an urgent need for machine learning
frameworks that can eliminate the burden of model selection and customization
and unleash the incredible power of the vast emerging model library for end
users. Here, we propose a context-aware routing system, Tryage, that leverages
a language model router for optimal selection of expert models from a model
library based on analysis of individual input prompts. Inspired by the thalamic
router in the brain, Tryage employs a perceptive router to predict down-stream
model performance on prompts and, then, makes a routing decision using an
objective function that integrates performance predictions with user goals and
constraints that are incorporated through flags (e.g., model size, model
recency). Tryage allows users to explore a Pareto front and automatically
trade-off between task accuracy and secondary goals including minimization of
model size, recency, security, verbosity, and readability. Across heterogeneous
data sets that include code, text, clinical data, and patents, the Tryage
framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection
identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by
GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how
routing models can be applied to program and control the behavior of
multi-model LLM systems to maximize efficient use of the expanding and evolving
language model ecosystem.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation. (arXiv:2308.11052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11052">http://arxiv.org/abs/2308.11052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11052]] Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation(http://arxiv.org/abs/2308.11052)</code></li>
<li>Summary: <p>In recent years, several Weakly Supervised Semantic Segmentation (WS3)
methods have been proposed that use class activation maps (CAMs) generated by a
classifier to produce pseudo-ground truths for training segmentation models.
While CAMs are good at highlighting discriminative regions (DR) of an image,
they are known to disregard regions of the object that do not contribute to the
classifier's prediction, termed non-discriminative regions (NDR). In contrast,
attribution methods such as saliency maps provide an alternative approach for
assigning a score to every pixel based on its contribution to the
classification prediction. This paper provides a comprehensive comparison
between saliencies and CAMs for WS3. Our study includes multiple perspectives
on understanding their similarities and dissimilarities. Moreover, we provide
new evaluation metrics that perform a comprehensive assessment of WS3
performance of alternative methods w.r.t. CAMs. We demonstrate the
effectiveness of saliencies in addressing the limitation of CAMs through our
empirical studies on benchmark datasets. Furthermore, we propose random
cropping as a stochastic aggregation technique that improves the performance of
saliency, making it a strong alternative to CAM for WS3.
</p></li>
</ul>

<h3>Title: UnLoc: A Unified Framework for Video Localization Tasks. (arXiv:2308.11062v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11062">http://arxiv.org/abs/2308.11062</a></li>
<li>Code URL: https://github.com/google-research/scenic</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11062]] UnLoc: A Unified Framework for Video Localization Tasks(http://arxiv.org/abs/2308.11062)</code></li>
<li>Summary: <p>While large-scale image-text pretrained models such as CLIP have been used
for multiple video-level tasks on trimmed videos, their use for temporal
localization in untrimmed videos is still a relatively unexplored task. We
design a new approach for this called UnLoc, which uses pretrained image and
text towers, and feeds tokens to a video-text fusion model. The output of the
fusion module are then used to construct a feature pyramid in which each level
connects to a head to predict a per-frame relevancy score and start/end time
displacements. Unlike previous works, our architecture enables Moment
Retrieval, Temporal Localization, and Action Segmentation with a single stage
model, without the need for action proposals, motion based pretrained features
or representation masking. Unlike specialized models, we achieve state of the
art results on all three different localization tasks with a unified approach.
Code will be available at: \url{https://github.com/google-research/scenic}.
</p></li>
</ul>

<h3>Title: Exploring Unsupervised Cell Recognition with Prior Self-activation Maps. (arXiv:2308.11144v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11144">http://arxiv.org/abs/2308.11144</a></li>
<li>Code URL: https://github.com/cpystan/psm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11144]] Exploring Unsupervised Cell Recognition with Prior Self-activation Maps(http://arxiv.org/abs/2308.11144)</code></li>
<li>Summary: <p>The success of supervised deep learning models on cell recognition tasks
relies on detailed annotations. Many previous works have managed to reduce the
dependency on labels. However, considering the large number of cells contained
in a patch, costly and inefficient labeling is still inevitable. To this end,
we explored label-free methods for cell recognition. Prior self-activation maps
(PSM) are proposed to generate pseudo masks as training targets. To be
specific, an activation network is trained with self-supervised learning. The
gradient information in the shallow layers of the network is aggregated to
generate prior self-activation maps. Afterward, a semantic clustering module is
then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo
masks for downstream tasks. We evaluated our method on two histological
datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection).
Compared with other fully-supervised and weakly-supervised methods, our method
can achieve competitive performance without any manual annotations. Our simple
but effective framework can also achieve multi-class cell detection which can
not be done by existing unsupervised methods. The results show the potential of
PSMs that might inspire other research to deal with the hunger for labels in
medical area.
</p></li>
</ul>

<h3>Title: Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation. (arXiv:2308.11166v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11166">http://arxiv.org/abs/2308.11166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11166]] Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation(http://arxiv.org/abs/2308.11166)</code></li>
<li>Summary: <p>Impressive performance on point cloud semantic segmentation has been achieved
by fully-supervised methods with large amounts of labelled data. As it is
labour-intensive to acquire large-scale point cloud data with point-wise
labels, many attempts have been made to explore learning 3D point cloud
segmentation with limited annotations. Active learning is one of the effective
strategies to achieve this purpose but is still under-explored. The most recent
methods of this kind measure the uncertainty of each pre-divided region for
manual labelling but they suffer from redundant information and require
additional efforts for region division. This paper aims at addressing this
issue by developing a hierarchical point-based active learning strategy.
Specifically, we measure the uncertainty for each point by a hierarchical
minimum margin uncertainty module which considers the contextual information at
multiple levels. Then, a feature-distance suppression strategy is designed to
select important and representative points for manual labelling. Besides, to
better exploit the unlabelled data, we build a semi-supervised segmentation
framework based on our active strategy. Extensive experiments on the S3DIS and
ScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and
100% performance of fully-supervised baseline with only 0.07% and 0.1% training
data, respectively, outperforming the state-of-the-art weakly-supervised and
active learning methods. The code will be available at
https://github.com/SmiletoE/HPAL.
</p></li>
</ul>

<h3>Title: A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology. (arXiv:2308.11179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11179">http://arxiv.org/abs/2308.11179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11179]] A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology(http://arxiv.org/abs/2308.11179)</code></li>
<li>Summary: <p>Simultaneous segmentation and classification of nuclei in digital histology
play an essential role in computer-assisted cancer diagnosis; however, it
remains challenging. The highest achieved binary and multi-class Panoptic
Quality (PQ) remains as low as 0.68 bPQ and 0.49 mPQ, respectively. It is due
to the higher staining variability, variability across the tissue, rough
clinical conditions, overlapping nuclei, and nuclear class imbalance. The
generic deep-learning methods usually rely on end-to-end models, which fail to
address these problems associated explicitly with digital histology. In our
previous work, DAN-NucNet, we resolved these issues for semantic segmentation
with an end-to-end model. This work extends our previous model to simultaneous
instance segmentation and classification. We introduce additional decoder heads
with independent weighted losses, which produce semantic segmentation, edge
proposals, and classification maps. We use the outputs from the three-head
model to apply post-processing to produce the final segmentation and
classification. Our multi-stage approach utilizes edge proposals and semantic
segmentations compared to direct segmentation and classification strategies
followed by most state-of-the-art methods. Due to this, we demonstrate a
significant performance improvement in producing high-quality instance
segmentation and nuclei classification. We have achieved a 0.841 Dice score for
semantic segmentation, 0.713 bPQ scores for instance segmentation, and 0.633
mPQ for nuclei classification. Our proposed framework is generalized across 19
types of tissues. Furthermore, the framework is less complex compared to the
state-of-the-art.
</p></li>
</ul>

<h3>Title: MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation. (arXiv:2308.11185v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11185">http://arxiv.org/abs/2308.11185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11185]] MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation(http://arxiv.org/abs/2308.11185)</code></li>
<li>Summary: <p>Previous research has studied the task of segmenting cinematic videos into
scenes and into narrative acts. However, these studies have overlooked the
essential task of multimodal alignment and fusion for effectively and
efficiently processing long-form videos (&gt;60min). In this paper, we introduce
Multimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematic
long-video segmentation. MEGA tackles the challenge by leveraging multiple
media modalities. The method coarsely aligns inputs of variable lengths and
different modalities with alignment positional encoding. To maintain temporal
synchronization while reducing computation, we further introduce an enhanced
bottleneck fusion layer which uses temporal alignment. Additionally, MEGA
employs a novel contrastive loss to synchronize and transfer labels across
modalities, enabling act segmentation from labeled synopsis sentences on video
shots. Our experimental results show that MEGA outperforms state-of-the-art
methods on MovieNet dataset for scene segmentation (with an Average Precision
improvement of +1.19%) and on TRIPOD dataset for act segmentation (with a Total
Agreement improvement of +5.51%)
</p></li>
</ul>

<h3>Title: Masked Cross-image Encoding for Few-shot Segmentation. (arXiv:2308.11201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11201">http://arxiv.org/abs/2308.11201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11201]] Masked Cross-image Encoding for Few-shot Segmentation(http://arxiv.org/abs/2308.11201)</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) is a dense prediction task that aims to infer the
pixel-wise labels of unseen classes using only a limited number of annotated
images. The key challenge in FSS is to classify the labels of query pixels
using class prototypes learned from the few labeled support exemplars. Prior
approaches to FSS have typically focused on learning class-wise descriptors
independently from support images, thereby ignoring the rich contextual
information and mutual dependencies among support-query features. To address
this limitation, we propose a joint learning method termed Masked Cross-Image
Encoding (MCE), which is designed to capture common visual properties that
describe object details and to learn bidirectional inter-image dependencies
that enhance feature interaction. MCE is more than a visual representation
enrichment module; it also considers cross-image mutual dependencies and
implicit guidance. Experiments on FSS benchmarks PASCAL-$5^i$ and COCO-$20^i$
demonstrate the advanced meta-learning ability of the proposed method.
</p></li>
</ul>

<h3>Title: Affordance segmentation of hand-occluded containers from exocentric images. (arXiv:2308.11233v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11233">http://arxiv.org/abs/2308.11233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11233]] Affordance segmentation of hand-occluded containers from exocentric images(http://arxiv.org/abs/2308.11233)</code></li>
<li>Summary: <p>Visual affordance segmentation identifies the surfaces of an object an agent
can interact with. Common challenges for the identification of affordances are
the variety of the geometry and physical properties of these surfaces as well
as occlusions. In this paper, we focus on occlusions of an object that is
hand-held by a person manipulating it. To address this challenge, we propose an
affordance segmentation model that uses auxiliary branches to process the
object and hand regions separately. The proposed model learns affordance
features under hand-occlusion by weighting the feature map through hand and
object segmentation. To train the model, we annotated the visual affordances of
an existing dataset with mixed-reality images of hand-held containers in
third-person (exocentric) images. Experiments on both real and mixed-reality
images show that our model achieves better affordance segmentation and
generalisation than existing models.
</p></li>
</ul>

<h3>Title: LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training. (arXiv:2308.11239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11239">http://arxiv.org/abs/2308.11239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11239]] LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training(http://arxiv.org/abs/2308.11239)</code></li>
<li>Summary: <p>Learning object segmentation in image and video datasets without human
supervision is a challenging problem. Humans easily identify moving salient
objects in videos using the gestalt principle of common fate, which suggests
that what moves together belongs together. Building upon this idea, we propose
a self-supervised object discovery approach that leverages motion and
appearance information to produce high-quality object segmentation masks.
Specifically, we redesign the traditional graph cut on images to include motion
information in a linear combination with appearance information to produce edge
weights. Remarkably, this step produces object segmentation masks comparable to
the current state-of-the-art on multiple benchmarks. To further improve
performance, we bootstrap a segmentation network trained on these preliminary
masks as pseudo-ground truths to learn from its own outputs via self-training.
We demonstrate the effectiveness of our approach, named LOCATE, on multiple
standard video object segmentation, image saliency detection, and object
segmentation benchmarks, achieving results on par with and, in many cases
surpassing state-of-the-art methods. We also demonstrate the transferability of
our approach to novel domains through a qualitative study on in-the-wild
images. Additionally, we present extensive ablation analysis to support our
design choices and highlight the contribution of each component of our proposed
method.
</p></li>
</ul>

<h3>Title: Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation. (arXiv:2308.11291v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11291">http://arxiv.org/abs/2308.11291</a></li>
<li>Code URL: https://github.com/jeremyfix/icvs2023</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11291]] Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation(http://arxiv.org/abs/2308.11291)</code></li>
<li>Summary: <p>The quality of a wood log in the wood industry depends heavily on the
presence of both outer and inner defects, including inner knots that are a
result of the growth of tree branches. Today, locating the inner knots require
the use of expensive equipment such as X-ray scanners. In this paper, we
address the task of predicting the location of inner defects from the outer
shape of the logs. The dataset is built by extracting both the contours and the
knots with X-ray measurements. We propose to solve this binary segmentation
task by leveraging convolutional recurrent neural networks. Once the neural
network is trained, inference can be performed from the outer shape measured
with cheap devices such as laser profilers. We demonstrate the effectiveness of
our approach on fir and spruce tree species and perform ablation on the
recurrence to demonstrate its importance.
</p></li>
</ul>

<h3>Title: BHSD: A 3D Multi-Class Brain Hemorrhage Segmentation Dataset. (arXiv:2308.11298v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11298">http://arxiv.org/abs/2308.11298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11298]] BHSD: A 3D Multi-Class Brain Hemorrhage Segmentation Dataset(http://arxiv.org/abs/2308.11298)</code></li>
<li>Summary: <p>Intracranial hemorrhage (ICH) is a pathological condition characterized by
bleeding inside the skull or brain, which can be attributed to various factors.
Identifying, localizing and quantifying ICH has important clinical
implications, in a bleed-dependent manner. While deep learning techniques are
widely used in medical image segmentation and have been applied to the ICH
segmentation task, existing public ICH datasets do not support the multi-class
segmentation problem. To address this, we develop the Brain Hemorrhage
Segmentation Dataset (BHSD), which provides a 3D multi-class ICH dataset
containing 192 volumes with pixel-level annotations and 2200 volumes with
slice-level annotations across five categories of ICH. To demonstrate the
utility of the dataset, we formulate a series of supervised and semi-supervised
ICH segmentation tasks. We provide experimental results with state-of-the-art
models as reference benchmarks for further model developments and evaluations
on this dataset.
</p></li>
</ul>

<h3>Title: How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11358">http://arxiv.org/abs/2308.11358</a></li>
<li>Code URL: https://github.com/ltcontext/ltcontext</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11358]] How Much Temporal Long-Term Context is Needed for Action Segmentation?(http://arxiv.org/abs/2308.11358)</code></li>
<li>Summary: <p>Modeling long-term context in videos is crucial for many fine-grained tasks
including temporal action segmentation. An interesting question that is still
open is how much long-term temporal context is needed for optimal performance.
While transformers can model the long-term context of a video, this becomes
computationally prohibitive for long videos. Recent works on temporal action
segmentation thus combine temporal convolutional networks with self-attentions
that are computed only for a local temporal window. While these approaches show
good results, their performance is limited by their inability to capture the
full context of a video. In this work, we try to answer how much long-term
temporal context is required for temporal action segmentation by introducing a
transformer-based model that leverages sparse attention to capture the full
context of a video. We compare our model with the current state of the art on
three datasets for temporal action segmentation, namely 50Salads, Breakfast,
and Assembly101. Our experiments show that modeling the full context of a video
is necessary to obtain the best performance for temporal action segmentation.
</p></li>
</ul>

<h3>Title: Boundary-RL: Reinforcement Learning for Weakly-Supervised Prostate Segmentation in TRUS Images. (arXiv:2308.11376v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11376">http://arxiv.org/abs/2308.11376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11376]] Boundary-RL: Reinforcement Learning for Weakly-Supervised Prostate Segmentation in TRUS Images(http://arxiv.org/abs/2308.11376)</code></li>
<li>Summary: <p>We propose Boundary-RL, a novel weakly supervised segmentation method that
utilises only patch-level labels for training. We envision the segmentation as
a boundary detection problem, rather than a pixel-level classification as in
previous works. This outlook on segmentation may allow for boundary delineation
under challenging scenarios such as where noise artefacts may be present within
the region-of-interest (ROI) boundaries, where traditional pixel-level
classification-based weakly supervised methods may not be able to effectively
segment the ROI. Particularly of interest, ultrasound images, where intensity
values represent acoustic impedance differences between boundaries, may also
benefit from the boundary delineation approach. Our method uses reinforcement
learning to train a controller function to localise boundaries of ROIs using a
reward derived from a pre-trained boundary-presence classifier. The classifier
indicates when an object boundary is encountered within a patch, as the
controller modifies the patch location in a sequential Markov decision process.
The classifier itself is trained using only binary patch-level labels of object
presence, which are the only labels used during training of the entire boundary
delineation framework, and serves as a weak signal to inform the boundary
delineation. The use of a controller function ensures that a sliding window
over the entire image is not necessary. It also prevents possible
false-positive or -negative cases by minimising number of patches passed to the
boundary-presence classifier. We evaluate our proposed approach for a
clinically relevant task of prostate gland segmentation on trans-rectal
ultrasound images. We show improved performance compared to other tested weakly
supervised methods, using the same labels e.g., multiple instance learning.
</p></li>
</ul>

<h3>Title: Food Image Classification and Segmentation with Attention-based Multiple Instance Learning. (arXiv:2308.11452v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11452">http://arxiv.org/abs/2308.11452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11452]] Food Image Classification and Segmentation with Attention-based Multiple Instance Learning(http://arxiv.org/abs/2308.11452)</code></li>
<li>Summary: <p>The demand for accurate food quantification has increased in the recent
years, driven by the needs of applications in dietary monitoring. At the same
time, computer vision approaches have exhibited great potential in automating
tasks within the food domain. Traditionally, the development of machine
learning models for these problems relies on training data sets with
pixel-level class annotations. However, this approach introduces challenges
arising from data collection and ground truth generation that quickly become
costly and error-prone since they must be performed in multiple settings and
for thousands of classes. To overcome these challenges, the paper presents a
weakly supervised methodology for training food image classification and
semantic segmentation models without relying on pixel-level annotations. The
proposed methodology is based on a multiple instance learning approach in
combination with an attention-based mechanism. At test time, the models are
used for classification and, concurrently, the attention mechanism generates
semantic heat maps which are used for food class segmentation. In the paper, we
conduct experiments on two meta-classes within the FoodSeg103 data set to
verify the feasibility of the proposed approach and we explore the functioning
properties of the attention mechanism.
</p></li>
</ul>

<h3>Title: LCCo: Lending CLIP to Co-Segmentation. (arXiv:2308.11506v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11506">http://arxiv.org/abs/2308.11506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11506]] LCCo: Lending CLIP to Co-Segmentation(http://arxiv.org/abs/2308.11506)</code></li>
<li>Summary: <p>This paper studies co-segmenting the common semantic object in a set of
images. Existing works either rely on carefully engineered networks to mine the
implicit semantic information in visual features or require extra data (i.e.,
classification labels) for training. In this paper, we leverage the contrastive
language-image pre-training framework (CLIP) for the task. With a backbone
segmentation network that independently processes each image from the set, we
introduce semantics from CLIP into the backbone features, refining them in a
coarse-to-fine manner with three key modules: i) an image set feature
correspondence module, encoding global consistent semantic information of the
image set; ii) a CLIP interaction module, using CLIP-mined common semantics of
the image set to refine the backbone feature; iii) a CLIP regularization
module, drawing CLIP towards this co-segmentation task, identifying the best
CLIP semantic and using it to regularize the backbone feature. Experiments on
four standard co-segmentation benchmark datasets show that the performance of
our method outperforms state-of-the-art methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
