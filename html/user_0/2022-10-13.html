<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: PSNet: Parallel Symmetric Network for Video Salient Object Detection. (arXiv:2210.05912v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05912">http://arxiv.org/abs/2210.05912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05912] PSNet: Parallel Symmetric Network for Video Salient Object Detection](http://arxiv.org/abs/2210.05912)</code></li>
<li>Summary: <p>For the video salient object detection (VSOD) task, how to excavate the
information from the appearance modality and the motion modality has always
been a topic of great concern. The two-stream structure, including an RGB
appearance stream and an optical flow motion stream, has been widely used as a
typical pipeline for VSOD tasks, but the existing methods usually only use
motion features to unidirectionally guide appearance features or adaptively but
blindly fuse two modality features. However, these methods underperform in
diverse scenarios due to the uncomprehensive and unspecific learning schemes.
In this paper, following a more secure modeling philosophy, we deeply
investigate the importance of appearance modality and motion modality in a more
comprehensive way and propose a VSOD network with up and down parallel
symmetry, named PSNet. Two parallel branches with different dominant modalities
are set to achieve complete video saliency decoding with the cooperation of the
Gather Diffusion Reinforcement (GDR) module and Cross-modality Refinement and
Complement (CRC) module. Finally, we use the Importance Perception Fusion (IPF)
module to fuse the features from two parallel branches according to their
different importance in different scenarios. Experiments on four dataset
benchmarks demonstrate that our method achieves desirable and competitive
performance.
</p></li>
</ul>

<h3>Title: Privacy of federated QR decomposition using additive secure multiparty computation. (arXiv:2210.06163v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06163">http://arxiv.org/abs/2210.06163</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06163] Privacy of federated QR decomposition using additive secure multiparty computation](http://arxiv.org/abs/2210.06163)</code></li>
<li>Summary: <p>Federated learning (FL) is a privacy-aware data mining strategy keeping the
private data on the owners' machine and thereby confidential. The clients
compute local models and send them to an aggregator which computes a global
model. In hybrid FL, the local parameters are additionally masked using secure
aggregation, such that only the global aggregated statistics become available
in clear text, not the client specific updates. Federated QR decomposition has
not been studied extensively in the context of cross-silo federated learning.
In this article, we investigate the suitability of three QR decomposition
algorithms for cross-silo FL and suggest a privacy-aware QR decomposition
scheme based on the Gram-Schmidt algorithm which does not blatantly leak raw
data. We apply the algorithm to compute linear regression in a federated
manner.
</p></li>
</ul>

<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Momentum Aggregation for Private Non-convex ERM. (arXiv:2210.06328v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06328">http://arxiv.org/abs/2210.06328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06328] Momentum Aggregation for Private Non-convex ERM](http://arxiv.org/abs/2210.06328)</code></li>
<li>Summary: <p>We introduce new algorithms and convergence guarantees for privacy-preserving
non-convex Empirical Risk Minimization (ERM) on smooth $d$-dimensional
objectives. We develop an improved sensitivity analysis of stochastic gradient
descent on smooth objectives that exploits the recurrence of examples in
different epochs. By combining this new approach with recent analysis of
momentum with private aggregation techniques, we provide an
$(\epsilon,\delta)$-differential private algorithm that finds a gradient of
norm $\tilde O\left(\frac{d^{1/3}}{(\epsilon N)^{2/3}}\right)$ in
$O\left(\frac{N^{7/3}\epsilon^{4/3}}{d^{2/3}}\right)$ gradient evaluations,
improving the previous best gradient bound of $\tilde
O\left(\frac{d^{1/4}}{\sqrt{\epsilon N}}\right)$.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Statistical Modeling of Soft Error Influence on Neural Networks. (arXiv:2210.05876v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05876">http://arxiv.org/abs/2210.05876</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05876] Statistical Modeling of Soft Error Influence on Neural Networks](http://arxiv.org/abs/2210.05876)</code></li>
<li>Summary: <p>Soft errors in large VLSI circuits pose dramatic influence on computing- and
memory-intensive neural network (NN) processing. Understanding the influence of
soft errors on NNs is critical to protect against soft errors for reliable NN
processing. Prior work mainly rely on fault simulation to analyze the influence
of soft errors on NN processing. They are accurate but usually specific to
limited configurations of errors and NN models due to the prohibitively slow
simulation speed especially for large NN models and datasets. With the
observation that the influence of soft errors propagates across a large number
of neurons and accumulates as well, we propose to characterize the soft error
induced data disturbance on each neuron with normal distribution model
according to central limit theorem and develop a series of statistical models
to analyze the behavior of NN models under soft errors in general. The
statistical models reveal not only the correlation between soft errors and NN
model accuracy, but also how NN parameters such as quantization and
architecture affect the reliability of NNs. The proposed models are compared
with fault simulation and verified comprehensively. In addition, we observe
that the statistical models that characterize the soft error influence can also
be utilized to predict fault simulation results in many cases and we explore
the use of the proposed statistical models to accelerate fault simulations of
NNs. According to our experiments, the accelerated fault simulation shows
almost two orders of magnitude speedup with negligible simulation accuracy loss
over the baseline fault simulations.
</p></li>
</ul>

<h3>Title: FCT-GAN: Enhancing Table Synthesis via Fourier Transform. (arXiv:2210.06239v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06239">http://arxiv.org/abs/2210.06239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06239] FCT-GAN: Enhancing Table Synthesis via Fourier Transform](http://arxiv.org/abs/2210.06239)</code></li>
<li>Summary: <p>Synthetic tabular data emerges as an alternative for sharing knowledge while
adhering to restrictive data access regulations, e.g., European General Data
Protection Regulation (GDPR). Mainstream state-of-the-art tabular data
synthesizers draw methodologies from Generative Adversarial Networks (GANs),
which are composed of a generator and a discriminator. While convolution neural
networks are shown to be a better architecture than fully connected networks
for tabular data synthesizing, two key properties of tabular data are
overlooked: (i) the global correlation across columns, and (ii) invariant
synthesizing to column permutations of input data. To address the above
problems, we propose a Fourier conditional tabular generative adversarial
network (FCT-GAN). We introduce feature tokenization and Fourier networks to
construct a transformer-style generator and discriminator, and capture both
local and global dependencies across columns. The tokenizer captures local
spatial features and transforms original data into tokens. Fourier networks
transform tokens to frequency domains and element-wisely multiply a learnable
filter. Extensive evaluation on benchmarks and real-world data shows that
FCT-GAN can synthesize tabular data with high machine learning utility (up to
27.8% better than state-of-the-art baselines) and high statistical similarity
to the original data (up to 26.5% better), while maintaining the global
correlation across columns, especially on high dimensional dataset.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Synthetic Text Detection: Systemic Literature Review. (arXiv:2210.06336v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06336">http://arxiv.org/abs/2210.06336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06336] Synthetic Text Detection: Systemic Literature Review](http://arxiv.org/abs/2210.06336)</code></li>
<li>Summary: <p>Within the text analysis and processing fields, generated text attacks have
been made easier to create than ever before. To combat these attacks open
sourcing models and datasets have become a major trend to create automated
detection algorithms in defense of authenticity. For this purpose, synthetic
text detection has become an increasingly viable topic of research. This review
is written for the purpose of creating a snapshot of the state of current
literature and easing the barrier to entry for future authors. Towards that
goal, we identified few research trends and challenges in this field.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Curved Representation Space of Vision Transformers. (arXiv:2210.05742v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05742">http://arxiv.org/abs/2210.05742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05742] Curved Representation Space of Vision Transformers](http://arxiv.org/abs/2210.05742)</code></li>
<li>Summary: <p>Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin
have emerged as a better alternative to traditional convolutional neural
networks (CNNs) for computer vision tasks. However, our understanding of how
the new architecture works is still limited. In this paper, we focus on the
phenomenon that Transformers show higher robustness against corruptions than
CNNs, while not being overconfident (in fact, we find Transformers are actually
underconfident). This is contrary to the intuition that robustness increases
with confidence. We resolve this contradiction by investigating how the output
of the penultimate layer moves in the representation space as the input data
moves within a small area. In particular, we show the following. (1) While CNNs
exhibit fairly linear relationship between the input and output movements,
Transformers show nonlinear relationship for some data. For those data, the
output of Transformers moves in a curved trajectory as the input moves
linearly. (2) When a data is located in a curved region, it is hard to move it
out of the decision region since the output moves along a curved trajectory
instead of a straight line to the decision boundary, resulting in high
robustness of Transformers. (3) If a data is slightly modified to jump out of
the curved region, the movements afterwards become linear and the output goes
to the decision boundary directly. Thus, Transformers can be attacked easily
after a small random jump and the perturbation in the final attacked data
remains imperceptible, i.e., there does exist a decision boundary near the
data. This also explains the underconfident prediction of Transformers. (4) The
curved regions in the representation space start to form at an early training
stage and grow throughout the training course. Some data are trapped in the
regions, obstructing Transformers from reducing the training loss.
</p></li>
</ul>

<h3>Title: Deep Learning for Iris Recognition: A Survey. (arXiv:2210.05866v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05866">http://arxiv.org/abs/2210.05866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05866] Deep Learning for Iris Recognition: A Survey](http://arxiv.org/abs/2210.05866)</code></li>
<li>Summary: <p>In this survey, we provide a comprehensive review of more than 200 papers,
technical reports, and GitHub repositories published over the last 10 years on
the recent developments of deep learning techniques for iris recognition,
covering broad topics on algorithm designs, open-source tools, open challenges,
and emerging research. First, we conduct a comprehensive analysis of deep
learning techniques developed for two main sub-tasks in iris biometrics:
segmentation and recognition. Second, we focus on deep learning techniques for
the robustness of iris recognition systems against presentation attacks and via
human-machine pairing. Third, we delve deep into deep learning techniques for
forensic application, especially in post-mortem iris recognition. Fourth, we
review open-source resources and tools in deep learning techniques for iris
recognition. Finally, we highlight the technical challenges, emerging research
trends, and outlook for the future of deep learning in iris recognition.
</p></li>
</ul>

<h3>Title: Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation. (arXiv:2210.05968v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05968">http://arxiv.org/abs/2210.05968</a></li>
<li>Code URL: <a href="https://github.com/Alan-Qin/Transfer_attack_RAP">https://github.com/Alan-Qin/Transfer_attack_RAP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05968] Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation](http://arxiv.org/abs/2210.05968)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
examples, which can produce erroneous predictions by injecting imperceptible
perturbations. In this work, we study the transferability of adversarial
examples, which is significant due to its threat to real-world applications
where model architecture or parameters are usually unknown. Many existing works
reveal that the adversarial examples are likely to overfit the surrogate model
that they are generated from, limiting its transfer attack performance against
different target models. To mitigate the overfitting of the surrogate model, we
propose a novel attack method, dubbed reverse adversarial perturbation (RAP).
Specifically, instead of minimizing the loss of a single adversarial point, we
advocate seeking adversarial example located at a region with unified low loss
value, by injecting the worst-case perturbation (the reverse adversarial
perturbation) for each step of the optimization procedure. The adversarial
attack with RAP is formulated as a min-max bi-level optimization problem. By
integrating RAP into the iterative process for attacks, our method can find
more stable adversarial examples which are less sensitive to the changes of
decision boundary, mitigating the overfitting of the surrogate model.
Comprehensive experimental comparisons demonstrate that RAP can significantly
boost adversarial transferability. Furthermore, RAP can be naturally combined
with many existing black-box attack techniques, to further boost the
transferability. When attacking a real-world image recognition system, Google
Cloud Vision API, we obtain 22% performance improvement of targeted attacks
over the compared method. Our codes are available at
https://github.com/SCLBD/Transfer_attack_RAP.
</p></li>
</ul>

<h3>Title: Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning. (arXiv:2210.05927v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05927">http://arxiv.org/abs/2210.05927</a></li>
<li>Code URL: <a href="https://github.com/umd-huang-lab/wocar-rl">https://github.com/umd-huang-lab/wocar-rl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05927] Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning](http://arxiv.org/abs/2210.05927)</code></li>
<li>Summary: <p>Recent studies reveal that a well-trained deep reinforcement learning (RL)
policy can be particularly vulnerable to adversarial perturbations on input
observations. Therefore, it is crucial to train RL agents that are robust
against any attacks with a bounded budget. Existing robust training methods in
deep RL either treat correlated steps separately, ignoring the robustness of
long-term rewards, or train the agents and RL-based attacker together, doubling
the computational burden and sample complexity of the training process. In this
work, we propose a strong and efficient robust training framework for RL, named
Worst-case-aware Robust RL (WocaR-RL) that directly estimates and optimizes the
worst-case reward of a policy under bounded l_p attacks without requiring extra
samples for learning an attacker. Experiments on multiple environments show
that WocaR-RL achieves state-of-the-art performance under various strong
attacks, and obtains significantly higher training efficiency than prior
state-of-the-art robust training methods. The code of this work is available at
https://github.com/umd-huang-lab/WocaR-RL.
</p></li>
</ul>

<h3>Title: Few-shot Backdoor Attacks via Neural Tangent Kernels. (arXiv:2210.05929v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05929">http://arxiv.org/abs/2210.05929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05929] Few-shot Backdoor Attacks via Neural Tangent Kernels](http://arxiv.org/abs/2210.05929)</code></li>
<li>Summary: <p>In a backdoor attack, an attacker injects corrupted examples into the
training set. The goal of the attacker is to cause the final trained model to
predict the attacker's desired target label when a predefined trigger is added
to test inputs. Central to these attacks is the trade-off between the success
rate of the attack and the number of corrupted training examples injected. We
pose this attack as a novel bilevel optimization problem: construct strong
poison examples that maximize the attack success rate of the trained model. We
use neural tangent kernels to approximate the training dynamics of the model
being attacked and automatically learn strong poison examples. We experiment on
subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt
architectures on periodic and patch trigger attacks and show that NTBA-designed
poisoned examples achieve, for example, an attack success rate of 90% with ten
times smaller number of poison examples injected compared to the baseline. We
provided an interpretation of the NTBA-designed attacks using the analysis of
kernel linear regression. We further demonstrate a vulnerability in
overparametrized deep neural networks, which is revealed by the shape of the
neural tangent kernel.
</p></li>
</ul>

<h3>Title: Betting the system: Using lineups to predict football scores. (arXiv:2210.06327v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06327">http://arxiv.org/abs/2210.06327</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06327] Betting the system: Using lineups to predict football scores](http://arxiv.org/abs/2210.06327)</code></li>
<li>Summary: <p>This paper aims to reduce randomness in football by analysing the role of
lineups in final scores using machine learning prediction models we have
developed. Football clubs invest millions of dollars on lineups and knowing how
individual statistics translate to better outcomes can optimise investments.
Moreover, sports betting is growing exponentially and being able to predict the
future is profitable and desirable. We use machine learning models and
historical player data from English Premier League (2020-2022) to predict
scores and to understand how individual performance can improve the outcome of
a match. We compared different prediction techniques to maximise the
possibility of finding useful models. We created heuristic and machine learning
models predicting football scores to compare different techniques. We used
different sets of features and shown goalkeepers stats are more important than
attackers stats to predict goals scored. We applied a broad evaluation process
to assess the efficacy of the models in real world applications. We managed to
predict correctly all relegated teams after forecast 100 consecutive matches.
We show that Support Vector Regression outperformed other techniques predicting
final scores and that lineups do improve predictions. Finally, our model was
profitable (42% return) when emulating a betting system using real world odds
data.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robustify Transformers with Robust Kernel Density Estimation. (arXiv:2210.05794v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05794">http://arxiv.org/abs/2210.05794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05794] Robustify Transformers with Robust Kernel Density Estimation](http://arxiv.org/abs/2210.05794)</code></li>
<li>Summary: <p>Recent advances in Transformer architecture have empowered its empirical
success in various tasks across different domains. However, existing works
mainly focus on improving the standard accuracy and computational cost, without
considering the robustness of contaminated samples. Existing work has shown
that the self-attention mechanism, which is the center of the Transformer
architecture, can be viewed as a non-parametric estimator based on the
well-known kernel density estimation (KDE). This motivates us to leverage the
robust kernel density estimation (RKDE) in the self-attention mechanism, to
alleviate the issue of the contamination of data by down-weighting the weight
of bad samples in the estimation process. The modified self-attention mechanism
can be incorporated into different Transformer variants. Empirical results on
language modeling and image classification tasks demonstrate the effectiveness
of this approach.
</p></li>
</ul>

<h3>Title: Point Cloud Scene Completion with Joint Color and Semantic Estimation from Single RGB-D Image. (arXiv:2210.05891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05891">http://arxiv.org/abs/2210.05891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05891] Point Cloud Scene Completion with Joint Color and Semantic Estimation from Single RGB-D Image](http://arxiv.org/abs/2210.05891)</code></li>
<li>Summary: <p>We present a deep reinforcement learning method of progressive view
inpainting for colored semantic point cloud scene completion under volume
guidance, achieving high-quality scene reconstruction from only a single RGB-D
image with severe occlusion. Our approach is end-to-end, consisting of three
modules: 3D scene volume reconstruction, 2D RGB-D and segmentation image
inpainting, and multi-view selection for completion. Given a single RGB-D
image, our method first predicts its semantic segmentation map and goes through
the 3D volume branch to obtain a volumetric scene reconstruction as a guide to
the next view inpainting step, which attempts to make up the missing
information; the third step involves projecting the volume under the same view
of the input, concatenating them to complete the current view RGB-D and
segmentation map, and integrating all RGB-D and segmentation maps into the
point cloud. Since the occluded areas are unavailable, we resort to a A3C
network to glance around and pick the next best view for large hole completion
progressively until a scene is adequately reconstructed while guaranteeing
validity. All steps are learned jointly to achieve robust and consistent
results. We perform qualitative and quantitative evaluations with extensive
experiments on the 3D-FUTURE data, obtaining better results than
state-of-the-arts.
</p></li>
</ul>

<h3>Title: Common Corruption Robustness of Point Cloud Detectors: Benchmark and Enhancement. (arXiv:2210.05896v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05896">http://arxiv.org/abs/2210.05896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05896] Common Corruption Robustness of Point Cloud Detectors: Benchmark and Enhancement](http://arxiv.org/abs/2210.05896)</code></li>
<li>Summary: <p>Object detection through LiDAR-based point cloud has recently been important
in autonomous driving. Although achieving high accuracy on public benchmarks,
the state-of-the-art detectors may still go wrong and cause a heavy loss due to
the widespread corruptions in the real world like rain, snow, sensor noise,
etc. Nevertheless, there is a lack of a large-scale dataset covering diverse
scenes and realistic corruption types with different severities to develop
practical and robust point cloud detectors, which is challenging due to the
heavy collection costs. To alleviate the challenge and start the first step for
robust point cloud detection, we propose the physical-aware simulation methods
to generate degraded point clouds under different real-world common
corruptions. Then, for the first attempt, we construct a benchmark based on the
physical-aware common corruptions for point cloud detectors, which contains a
total of 1,122,150 examples covering 7,481 scenes, 25 common corruption types,
and 6 severities. With such a novel benchmark, we conduct extensive empirical
studies on 8 state-of-the-art detectors that contain 6 different detection
frameworks. Thus we get several insight observations revealing the
vulnerabilities of the detectors and indicating the enhancement directions.
Moreover, we further study the effectiveness of existing robustness enhancement
methods based on data augmentation and data denoising. The benchmark can
potentially be a new platform for evaluating point cloud detectors, opening a
door for developing novel robustness enhancement methods.
</p></li>
</ul>

<h3>Title: Robust Models are less Over-Confident. (arXiv:2210.05938v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05938">http://arxiv.org/abs/2210.05938</a></li>
<li>Code URL: <a href="https://github.com/gejulia/robustness_confidences_evaluation">https://github.com/gejulia/robustness_confidences_evaluation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05938] Robust Models are less Over-Confident](http://arxiv.org/abs/2210.05938)</code></li>
<li>Summary: <p>Despite the success of convolutional neural networks (CNNs) in many academic
benchmarks for computer vision tasks, their application in the real-world is
still facing fundamental challenges. One of these open problems is the inherent
lack of robustness, unveiled by the striking effectiveness of adversarial
attacks. Current attack methods are able to manipulate the network's prediction
by adding specific but small amounts of noise to the input. In turn,
adversarial training (AT) aims to achieve robustness against such attacks and
ideally a better model generalization ability by including adversarial samples
in the trainingset. However, an in-depth analysis of the resulting robust
models beyond adversarial robustness is still pending. In this paper, we
empirically analyze a variety of adversarially trained models that achieve high
robust accuracies when facing state-of-the-art attacks and we show that AT has
an interesting side-effect: it leads to models that are significantly less
overconfident with their decisions, even on clean data than non-robust models.
Further, our analysis of robust models shows that not only AT but also the
model's building blocks (like activation functions and pooling) have a strong
influence on the models' prediction confidences. Data &amp; Project website:
https://github.com/GeJulia/robustness_confidences_evaluation
</p></li>
</ul>

<h3>Title: Estimating the Pose of a Euro Pallet with an RGB Camera based on Synthetic Training Data. (arXiv:2210.06001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06001">http://arxiv.org/abs/2210.06001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06001] Estimating the Pose of a Euro Pallet with an RGB Camera based on Synthetic Training Data](http://arxiv.org/abs/2210.06001)</code></li>
<li>Summary: <p>Estimating the pose of a pallet and other logistics objects is crucial for
various use cases, such as automatized material handling or tracking.
Innovations in computer vision, computing power, and machine learning open up
new opportunities for device-free localization based on cameras and neural
networks. Large image datasets with annotated poses are required for training
the network. Manual annotation, especially of 6D poses, is an extremely
labor-intensive process. Hence, newer approaches often leverage synthetic
training data to automatize the process of generating annotated image datasets.
In this work, the generation of synthetic training data for 6D pose estimation
of pallets is presented. The data is then used to train the Deep Object Pose
Estimation (DOPE) algorithm. The experimental validation of the algorithm
proves that the 6D pose estimation of a standardized Euro pallet with a
Red-Green-Blue (RGB) camera is feasible. The comparison of the results from
three varying datasets under different lighting conditions shows the relevance
of an appropriate dataset design to achieve an accurate and robust
localization. The quantitative evaluation shows an average position error of
less than 20 cm for the preferred dataset. The validated training dataset and a
photorealistic model of a Euro pallet are publicly provided.
</p></li>
</ul>

<h3>Title: Gotcha: A Challenge-Response System for Real-Time Deepfake Detection. (arXiv:2210.06186v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06186">http://arxiv.org/abs/2210.06186</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06186] Gotcha: A Challenge-Response System for Real-Time Deepfake Detection](http://arxiv.org/abs/2210.06186)</code></li>
<li>Summary: <p>The integrity of online video interactions is threatened by the widespread
rise of AI-enabled high-quality deepfakes that are now deployable in real-time.
This paper presents Gotcha, a real-time deepfake detection system for live
video interactions. The core principle underlying Gotcha is the presentation of
a specially chosen cascade of both active and passive challenges to video
conference participants. Active challenges include inducing changes in face
occlusion, face expression, view angle, and ambiance; passive challenges
include digital manipulation of the webcam feed. The challenges are designed to
target vulnerabilities in the structure of modern deepfake generators and
create perceptible artifacts for the human eye while inducing robust signals
for ML-based automatic deepfake detectors. We present a comprehensive taxonomy
of a large set of challenge tasks, which reveals a natural hierarchy among
different challenges. Our system leverages this hierarchy by cascading
progressively more demanding challenges to a suspected deepfake. We evaluate
our system on a novel dataset of live users emulating deepfakes and show that
our system provides consistent, measurable degradation of deepfake quality,
showcasing its promise for robust real-time deepfake detection when deployed in
the wild.
</p></li>
</ul>

<h3>Title: Pose-Guided Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2210.06192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06192">http://arxiv.org/abs/2210.06192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06192] Pose-Guided Graph Convolutional Networks for Skeleton-Based Action Recognition](http://arxiv.org/abs/2210.06192)</code></li>
<li>Summary: <p>Graph convolutional networks (GCNs), which can model the human body skeletons
as spatial and temporal graphs, have shown remarkable potential in
skeleton-based action recognition. However, in the existing GCN-based methods,
graph-structured representation of the human skeleton makes it difficult to be
fused with other modalities, especially in the early stages. This may limit
their scalability and performance in action recognition tasks. In addition, the
pose information, which naturally contains informative and discriminative clues
for action recognition, is rarely explored together with skeleton data in
existing methods. In this work, we propose pose-guided GCN (PG-GCN), a
multi-modal framework for high-performance human action recognition. In
particular, a multi-stream network is constructed to simultaneously explore the
robust features from both the pose and skeleton data, while a dynamic attention
module is designed for early-stage feature fusion. The core idea of this module
is to utilize a trainable graph to aggregate features from the skeleton stream
with that of the pose stream, which leads to a network with more robust feature
representation ability. Extensive experiments show that the proposed PG-GCN can
achieve state-of-the-art performance on the NTU RGB+D 60 and NTU RGB+D 120
datasets.
</p></li>
</ul>

<h3>Title: What can we learn about a generated image corrupting its latent representation?. (arXiv:2210.06257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06257">http://arxiv.org/abs/2210.06257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06257] What can we learn about a generated image corrupting its latent representation?](http://arxiv.org/abs/2210.06257)</code></li>
<li>Summary: <p>Generative adversarial networks (GANs) offer an effective solution to the
image-to-image translation problem, thereby allowing for new possibilities in
medical imaging. They can translate images from one imaging modality to another
at a low cost. For unpaired datasets, they rely mostly on cycle loss. Despite
its effectiveness in learning the underlying data distribution, it can lead to
a discrepancy between input and output data. The purpose of this work is to
investigate the hypothesis that we can predict image quality based on its
latent representation in the GANs bottleneck. We achieve this by corrupting the
latent representation with noise and generating multiple outputs. The degree of
differences between them is interpreted as the strength of the representation:
the more robust the latent representation, the fewer changes in the output
image the corruption causes. Our results demonstrate that our proposed method
has the ability to i) predict uncertain parts of synthesized images, and ii)
identify samples that may not be reliable for downstream tasks, e.g., liver
segmentation task.
</p></li>
</ul>

<h3>Title: Visual Prompting for Adversarial Robustness. (arXiv:2210.06284v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06284">http://arxiv.org/abs/2210.06284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06284] Visual Prompting for Adversarial Robustness](http://arxiv.org/abs/2210.06284)</code></li>
<li>Summary: <p>In this work, we leverage visual prompting (VP) to improve adversarial
robustness of a fixed, pre-trained model at testing time. Compared to
conventional adversarial defenses, VP allows us to design universal (i.e.,
data-agnostic) input prompting templates, which have plug-and-play capabilities
at testing time to achieve desired model performance without introducing much
computation overhead. Although VP has been successfully applied to improving
model generalization, it remains elusive whether and how it can be used to
defend against adversarial attacks. We investigate this problem and show that
the vanilla VP approach is not effective in adversarial defense since a
universal input prompt lacks the capacity for robust learning against
sample-specific adversarial perturbations. To circumvent it, we propose a new
VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate
class-wise visual prompts so as to not only leverage the strengths of ensemble
prompts but also optimize their interrelations to improve model robustness. Our
experiments show that C-AVP outperforms the conventional VP method, with 2.1X
standard accuracy gain and 2X robust accuracy gain. Compared to classical
test-time defenses, C-AVP also yields a 42X inference time speedup.
</p></li>
</ul>

<h3>Title: Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers. (arXiv:2210.06313v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06313">http://arxiv.org/abs/2210.06313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06313] Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers](http://arxiv.org/abs/2210.06313)</code></li>
<li>Summary: <p>This paper studies the curious phenomenon for machine learning models with
Transformer architectures that their activation maps are sparse. By activation
map we refer to the intermediate output of the multi-layer perceptrons (MLPs)
after a ReLU activation function, and by "sparse" we mean that on average very
few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each
input to MLP. Moreover, larger Transformers with more layers and wider MLP
hidden dimensions are sparser as measured by the percentage of nonzero entries.
Through extensive experiments we demonstrate that the emergence of sparsity is
a prevalent phenomenon that occurs for both natural language processing and
vision tasks, on both training and evaluation data, for Transformers of various
configurations, at layers of all depth levels, as well as for other
architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also
emerges using training datasets with random labels, or with random inputs, or
with infinite amount of data, demonstrating that sparsity is not a result of a
specific family of datasets. We discuss how sparsity immediately implies a way
to significantly reduce the FLOP count and improve efficiency for Transformers.
Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser
activation via Top-k thresholding with a small value of k brings a collection
of desired but missing properties for Transformers, namely less sensitivity to
noisy training data, more robustness to input corruptions, and better
calibration for their prediction confidence.
</p></li>
</ul>

<h3>Title: Vote'n'Rank: Revision of Benchmarking with Social Choice Theory. (arXiv:2210.05769v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05769">http://arxiv.org/abs/2210.05769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05769] Vote'n'Rank: Revision of Benchmarking with Social Choice Theory](http://arxiv.org/abs/2210.05769)</code></li>
<li>Summary: <p>The development of state-of-the-art systems in different applied areas of
machine learning (ML) is driven by benchmarks, which have shaped the paradigm
of evaluating generalisation capabilities from multiple perspectives. Although
the paradigm is shifting towards more fine-grained evaluation across diverse
tasks, the delicate question of how to aggregate the performances has received
particular interest in the community. In general, benchmarks follow the
unspoken utilitarian principles, where the systems are ranked based on their
mean average score over task-specific metrics. Such aggregation procedure has
been viewed as a sub-optimal evaluation protocol, which may have created the
illusion of progress. This paper proposes Vote'n'Rank, a framework for ranking
systems in multi-task benchmarks under the principles of the social choice
theory. We demonstrate that our approach can be efficiently utilised to draw
new insights on benchmarking in several ML sub-fields and identify the
best-performing systems in research and development case studies. The
Vote'n'Rank's procedures are more robust than the mean average while being able
to handle missing performance scores and determine conditions under which the
system becomes the winner.
</p></li>
</ul>

<h3>Title: AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning. (arXiv:2210.05883v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05883">http://arxiv.org/abs/2210.05883</a></li>
<li>Code URL: <a href="https://github.com/taoyang225/ad-drop">https://github.com/taoyang225/ad-drop</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05883] AD-DROP: Attribution-Driven Dropout for Robust Language Model Fine-Tuning](http://arxiv.org/abs/2210.05883)</code></li>
<li>Summary: <p>Fine-tuning large pre-trained language models on downstream tasks is apt to
suffer from overfitting when limited training data is available. While dropout
proves to be an effective antidote by randomly dropping a proportion of units,
existing research has not examined its effect on the self-attention mechanism.
In this paper, we investigate this problem through self-attention attribution
and find that dropping attention positions with low attribution scores can
accelerate training and increase the risk of overfitting. Motivated by this
observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly
discards some high-attribution positions to encourage the model to make
predictions by relying more on low-attribution positions to reduce overfitting.
We also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to
avoid dropping high-attribution positions excessively. Extensive experiments on
various benchmarks show that AD-DROP yields consistent improvements over
baselines. Analysis further confirms that AD-DROP serves as a strategic
regularizer to prevent overfitting during fine-tuning.
</p></li>
</ul>

<h3>Title: Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05740">http://arxiv.org/abs/2210.05740</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05740] Stochastic Constrained DRO with a Complexity Independent of Sample Size](http://arxiv.org/abs/2210.05740)</code></li>
<li>Summary: <p>Distributionally Robust Optimization (DRO), as a popular method to train
robust models against distribution shift between training and test sets, has
received tremendous attention in recent years. In this paper, we propose and
analyze stochastic algorithms that apply to both non-convex and convex losses
for solving Kullback Leibler divergence constrained DRO problem. Compared with
existing methods solving this problem, our stochastic algorithms not only enjoy
competitive if not better complexity independent of sample size but also just
require a constant batch size at every iteration, which is more practical for
broad applications. We establish a nearly optimal complexity bound for finding
an $\epsilon$ stationary solution for non-convex losses and an optimal
complexity for finding an $\epsilon$ optimal solution for convex losses.
Empirical studies demonstrate the effectiveness of the proposed algorithms for
solving non-convex and convex constrained DRO problems.
</p></li>
</ul>

<h3>Title: C-Mixup: Improving Generalization in Regression. (arXiv:2210.05775v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05775">http://arxiv.org/abs/2210.05775</a></li>
<li>Code URL: <a href="https://github.com/huaxiuyao/c-mixup">https://github.com/huaxiuyao/c-mixup</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05775] C-Mixup: Improving Generalization in Regression](http://arxiv.org/abs/2210.05775)</code></li>
<li>Summary: <p>Improving the generalization of deep networks is an important open challenge,
particularly in domains without plentiful data. The mixup algorithm improves
generalization by linearly interpolating a pair of examples and their
corresponding labels. These interpolated examples augment the original training
set. Mixup has shown promising results in various classification tasks, but
systematic analysis of mixup in regression remains underexplored. Using mixup
directly on regression labels can result in arbitrarily incorrect labels. In
this paper, we propose a simple yet powerful algorithm, C-Mixup, to improve
generalization on regression tasks. In contrast with vanilla mixup, which picks
training examples for mixing with uniform probability, C-Mixup adjusts the
sampling probability based on the similarity of the labels. Our theoretical
analysis confirms that C-Mixup with label similarity obtains a smaller mean
square error in supervised regression and meta-regression than vanilla mixup
and using feature similarity. Another benefit of C-Mixup is that it can improve
out-of-distribution robustness, where the test distribution is different from
the training distribution. By selectively interpolating examples with similar
labels, it mitigates the effects of domain-associated information and yields
domain-invariant representations. We evaluate C-Mixup on eleven datasets,
ranging from tabular to video data. Compared to the best prior approach,
C-Mixup achieves 6.56%, 4.76%, 5.82% improvements in in-distribution
generalization, task generalization, and out-of-distribution robustness,
respectively. Code is released at https://github.com/huaxiuyao/C-Mixup.
</p></li>
</ul>

<h3>Title: Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity. (arXiv:2210.06077v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06077">http://arxiv.org/abs/2210.06077</a></li>
<li>Code URL: <a href="https://github.com/andrew-cullen/doublebubble">https://github.com/andrew-cullen/doublebubble</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06077] Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity](http://arxiv.org/abs/2210.06077)</code></li>
<li>Summary: <p>In response to subtle adversarial examples flipping classifications of neural
network models, recent research has promoted certified robustness as a
solution. There, invariance of predictions to all norm-bounded attacks is
achieved through randomised smoothing of network inputs. Today's
state-of-the-art certifications make optimal use of the class output scores at
the input instance under test: no better radius of certification (under the
$L_2$ norm) is possible given only these score. However, it is an open question
as to whether such lower bounds can be improved using local information around
the instance under test. In this work, we demonstrate how today's "optimal"
certificates can be improved by exploiting both the transitivity of
certifications, and the geometry of the input space, giving rise to what we
term Geometrically-Informed Certified Robustness. By considering the smallest
distance to points on the boundary of a set of certifications this approach
improves certifications for more than $80\%$ of Tiny-Imagenet instances,
yielding an on average $5 \%$ increase in the associated certification. When
incorporating training time processes that enhance the certified radius, our
technique shows even more promising results, with a uniform $4$ percentage
point increase in the achieved certified radius.
</p></li>
</ul>

<h3>Title: When are Local Queries Useful for Robust Learning?. (arXiv:2210.06089v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06089">http://arxiv.org/abs/2210.06089</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06089] When are Local Queries Useful for Robust Learning?](http://arxiv.org/abs/2210.06089)</code></li>
<li>Summary: <p>Distributional assumptions have been shown to be necessary for the robust
learnability of concept classes when considering the exact-in-the-ball robust
risk and access to random examples by Gourdeau et al. (2019). In this paper, we
study learning models where the learner is given more power through the use of
local queries, and give the first distribution-free algorithms that perform
robust empirical risk minimization (ERM) for this notion of robustness. The
first learning model we consider uses local membership queries (LMQ), where the
learner can query the label of points near the training sample. We show that,
under the uniform distribution, LMQs do not increase the robustness threshold
of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced
with this negative result, we introduce the local equivalence query (LEQ)
oracle, which returns whether the hypothesis and target concept agree in the
perturbation region around a point in the training sample, as well as a
counterexample if it exists. We show a separation result: on one hand, if the
query radius $\lambda$ is strictly smaller than the adversary's perturbation
budget $\rho$, then distribution-free robust learning is impossible for a wide
variety of concept classes; on the other hand, the setting $\lambda=\rho$
allows us to develop robust ERM algorithms. We then bound the query complexity
of these algorithms based on online learning guarantees and further improve
these bounds for the special case of conjunctions. We finish by giving robust
learning algorithms for halfspaces with margins on both ${0,1}^n$ and
$\mathbb{R}^n$.
</p></li>
</ul>

<h3>Title: Probabilistic Inverse Modeling: An Application in Hydrology. (arXiv:2210.06213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06213">http://arxiv.org/abs/2210.06213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06213] Probabilistic Inverse Modeling: An Application in Hydrology](http://arxiv.org/abs/2210.06213)</code></li>
<li>Summary: <p>The astounding success of these methods has made it imperative to obtain more
explainable and trustworthy estimates from these models. In hydrology, basin
characteristics can be noisy or missing, impacting streamflow prediction. For
solving inverse problems in such applications, ensuring explainability is
pivotal for tackling issues relating to data bias and large search space. We
propose a probabilistic inverse model framework that can reconstruct robust
hydrology basin characteristics from dynamic input weather driver and
streamflow response data. We address two aspects of building more explainable
inverse models, uncertainty estimation and robustness. This can help improve
the trust of water managers, handling of noisy data and reduce costs. We
propose uncertainty based learning method that offers 6\% improvement in $R^2$
for streamflow prediction (forward modeling) from inverse model inferred basin
characteristic estimates, 17\% reduction in uncertainty (40\% in presence of
noise) and 4\% higher coverage rate for basin characteristics.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: VCSE: Time-Domain Visual-Contextual Speaker Extraction Network. (arXiv:2210.06177v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06177">http://arxiv.org/abs/2210.06177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06177] VCSE: Time-Domain Visual-Contextual Speaker Extraction Network](http://arxiv.org/abs/2210.06177)</code></li>
<li>Summary: <p>Speaker extraction seeks to extract the target speech in a multi-talker
scenario given an auxiliary reference. Such reference can be auditory, i.e., a
pre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic
sequence. References in different modalities provide distinct and complementary
information that could be fused to form top-down attention on the target
speaker. Previous studies have introduced visual and contextual modalities in a
single model. In this paper, we propose a two-stage time-domain
visual-contextual speaker extraction network named VCSE, which incorporates
visual and self-enrolled contextual cues stage by stage to take full advantage
of every modality. In the first stage, we pre-extract a target speech with
visual cues and estimate the underlying phonetic sequence. In the second stage,
we refine the pre-extracted target speech with the self-enrolled contextual
cues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)
database demonstrate that our proposed VCSE network consistently outperforms
other state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: Explore Contextual Information for 3D Scene Graph Generation. (arXiv:2210.06240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06240">http://arxiv.org/abs/2210.06240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06240] Explore Contextual Information for 3D Scene Graph Generation](http://arxiv.org/abs/2210.06240)</code></li>
<li>Summary: <p>3D scene graph generation (SGG) has been of high interest in computer vision.
Although the accuracy of 3D SGG on coarse classification and single relation
label has been gradually improved, the performance of existing works is still
far from being perfect for fine-grained and multi-label situations. In this
paper, we propose a framework fully exploring contextual information for the 3D
SGG task, which attempts to satisfy the requirements of fine-grained entity
class, multiple relation labels, and high accuracy simultaneously. Our proposed
approach is composed of a Graph Feature Extraction module and a Graph
Contextual Reasoning module, achieving appropriate information-redundancy
feature extraction, structured organization, and hierarchical inferring. Our
approach achieves superior or competitive performance over previous methods on
the 3DSSG dataset, especially on the relationship prediction sub-task.
</p></li>
</ul>

<h3>Title: MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score. (arXiv:2210.05875v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05875">http://arxiv.org/abs/2210.05875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05875] MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and Contextualized Masked Language Model Score](http://arxiv.org/abs/2210.05875)</code></li>
<li>Summary: <p>This paper proposes a new natural language processing (NLP) application for
identifying medical jargon terms potentially difficult for patients to
comprehend from electronic health record (EHR) notes. We first present a novel
and publicly available dataset with expert-annotated medical jargon terms from
18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon
extraction ($MedJEx$) model which has been shown to outperform existing
state-of-the-art NLP models. First, MedJEx improved the overall performance
when it was trained on an auxiliary Wikipedia hyperlink span dataset, where
hyperlink spans provide additional Wikipedia articles to explain the spans (or
terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that
a contextualized masked language model score was beneficial for detecting
domain-specific unfamiliar jargon terms. Moreover, our results show that
training on the auxiliary Wikipedia hyperlink span datasets improved six out of
eight biomedical named entity recognition benchmark datasets. Both MedJ and
MedJEx are publicly available.
</p></li>
</ul>

<h3>Title: ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding. (arXiv:2210.06155v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06155">http://arxiv.org/abs/2210.06155</a></li>
<li>Code URL: <a href="https://github.com/PaddlePaddle/PaddleNLP">https://github.com/PaddlePaddle/PaddleNLP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06155] ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding](http://arxiv.org/abs/2210.06155)</code></li>
<li>Summary: <p>Recent years have witnessed the rise and success of pre-training techniques
in visually-rich document understanding. However, most existing methods lack
the systematic mining and utilization of layout-centered knowledge, leading to
sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel
document pre-training solution with layout knowledge enhancement in the whole
workflow, to learn better representations that combine the features from text,
layout, and image. Specifically, we first rearrange input sequences in the
serialization stage, and then present a correlative pre-training task, reading
order prediction, to learn the proper reading order of documents. To improve
the layout awareness of the model, we integrate a spatial-aware disentangled
attention into the multi-modal transformer and a replaced regions prediction
task into the pre-training phase. Experimental results show that ERNIE-Layout
achieves superior performance on various downstream tasks, setting new
state-of-the-art on key information extraction, document image classification,
and document question answering datasets. The code and models are publicly
available at
<a href="http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.">this http URL</a>
</p></li>
</ul>

<h3>Title: Russian Web Tables: A Public Corpus of Web Tables for Russian Language Based on Wikipedia. (arXiv:2210.06353v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06353">http://arxiv.org/abs/2210.06353</a></li>
<li>Code URL: <a href="https://gitlab.com/unidata-labs/ru-wiki-tables-frontend">https://gitlab.com/unidata-labs/ru-wiki-tables-frontend</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06353] Russian Web Tables: A Public Corpus of Web Tables for Russian Language Based on Wikipedia](http://arxiv.org/abs/2210.06353)</code></li>
<li>Summary: <p>Corpora that contain tabular data such as WebTables are a vital resource for
the academic community. Essentially, they are the backbone of any modern
research in information management. They are used for various tasks of data
extraction, knowledge base construction, question answering, column semantic
type detection and many other. Such corpora are useful not only as a source of
data, but also as a base for building test datasets. So far, there were no such
corpora for the Russian language and this seriously hindered research in the
aforementioned areas.
</p></li>
</ul>

<p>In this paper, we present the first corpus of Web tables created specifically
out of Russian language material. It was built via a special toolkit we have
developed to crawl the Russian Wikipedia. Both the corpus and the toolkit are
open-source and publicly available. Finally, we present a short study that
describes Russian Wikipedia tables and their statistics.
</p>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Continual Learning for Text Classification via Selective Inter-client Transfer. (arXiv:2210.06101v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06101">http://arxiv.org/abs/2210.06101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06101] Federated Continual Learning for Text Classification via Selective Inter-client Transfer](http://arxiv.org/abs/2210.06101)</code></li>
<li>Summary: <p>In this work, we combine the two paradigms: Federated Learning (FL) and
Continual Learning (CL) for text classification task in cloud-edge continuum.
The objective of Federated Continual Learning (FCL) is to improve deep learning
models over life time at each client by (relevant and efficient) knowledge
transfer without sharing data. Here, we address challenges in minimizing
inter-client interference while knowledge sharing due to heterogeneous tasks
across clients in FCL setup. In doing so, we propose a novel framework,
Federated Selective Inter-client Transfer (FedSeIT) which selectively combines
model parameters of foreign clients. To further maximize knowledge transfer, we
assess domain overlap and select informative tasks from the sequence of
historical tasks at each foreign client while preserving privacy. Evaluating
against the baselines, we show improved performance, a gain of (average) 12.4\%
in text classification over a sequence of tasks using five datasets from
diverse domains. To the best of our knowledge, this is the first work that
applies FCL to NLP.
</p></li>
</ul>

<h3>Title: Question Answering Over Biological Knowledge Graph via Amazon Alexa. (arXiv:2210.06040v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06040">http://arxiv.org/abs/2210.06040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06040] Question Answering Over Biological Knowledge Graph via Amazon Alexa](http://arxiv.org/abs/2210.06040)</code></li>
<li>Summary: <p>Structured and unstructured data and facts about drugs, genes, protein,
viruses, and their mechanism are spread across a huge number of scientific
articles. These articles are a large-scale knowledge source and can have a huge
impact on disseminating knowledge about the mechanisms of certain biological
processes. A knowledge graph (KG) can be constructed by integrating such facts
and data and be used for data integration, exploration, and federated queries.
However, exploration and querying large-scale KGs is tedious for certain groups
of users due to a lack of knowledge about underlying data assets or semantic
technologies. A question-answering (QA) system allows the answer of natural
language questions over KGs automatically using triples contained in a KG.
Recently, the use and adaption of digital assistants are getting wider owing to
their capability at enabling users to voice commands to control smart systems
or devices. This paper is about using Amazon Alexa's voice-enabled interface
for QA over KGs. As a proof-of-concept, we use the well-known DisgeNET KG,
which contains knowledge covering 1.13 million gene-disease associations
between 21,671 genes and 30,170 diseases, disorders, and clinical or abnormal
human phenotypes. Our study shows how Alex could be of help to find facts about
certain biological entities from large-scale knowledge bases.
</p></li>
</ul>

<h3>Title: Aergia: Leveraging Heterogeneity in Federated Learning Systems. (arXiv:2210.06154v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06154">http://arxiv.org/abs/2210.06154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06154] Aergia: Leveraging Heterogeneity in Federated Learning Systems](http://arxiv.org/abs/2210.06154)</code></li>
<li>Summary: <p>Federated Learning (FL) is a popular approach for distributed deep learning
that prevents the pooling of large amounts of data in a central server. FL
relies on clients to update a global model using their local datasets.
Classical FL algorithms use a central federator that, for each training round,
waits for all clients to send their model updates before aggregating them. In
practical deployments, clients might have different computing powers and
network capabilities, which might lead slow clients to become performance
bottlenecks. Previous works have suggested to use a deadline for each learning
round so that the federator ignores the late updates of slow clients, or so
that clients send partially trained models before the deadline. To speed up the
training process, we instead propose Aergia, a novel approach where slow
clients (i) freeze the part of their model that is the most computationally
intensive to train; (ii) train the unfrozen part of their model; and (iii)
offload the training of the frozen part of their model to a faster client that
trains it using its own dataset. The offloading decisions are orchestrated by
the federator based on the training speed that clients report and on the
similarities between their datasets, which are privately evaluated thanks to a
trusted execution environment. We show through extensive experiments that
Aergia maintains high accuracy and significantly reduces the training time
under heterogeneous settings by up to 27% and 53% compared to FedAvg and TiFL,
respectively.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05892">http://arxiv.org/abs/2210.05892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05892] Perplexity from PLM Is Unreliable for Evaluating Text Quality](http://arxiv.org/abs/2210.05892)</code></li>
<li>Summary: <p>Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality
of the generated text. They suppose that if the value of PPL is smaller, the
quality(i.e. fluency) of the text to be evaluated is better. However, we find
that the PPL referee is unqualified and it cannot evaluate the generated text
fairly for the following reasons: (i) The PPL of short text is larger than long
text, which goes against common sense, (ii) The repeated text span could damage
the performance of PPL, and (iii) The punctuation marks could affect the
performance of PPL heavily. Experiments show that the PPL is unreliable for
evaluating the quality of given text. Last, we discuss the key problems with
evaluating text quality using language models.
</p></li>
</ul>

<h3>Title: A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter. (arXiv:2210.06351v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06351">http://arxiv.org/abs/2210.06351</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06351] A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter](http://arxiv.org/abs/2210.06351)</code></li>
<li>Summary: <p>Harmful content detection models tend to have higher false positive rates for
content from marginalized groups. In the context of marginal abuse modeling on
Twitter, such disproportionate penalization poses the risk of reduced
visibility, where marginalized communities lose the opportunity to voice their
opinion on the platform. Current approaches to algorithmic harm mitigation, and
bias detection for NLP models are often very ad hoc and subject to human bias.
We make two main contributions in this paper. First, we design a novel
methodology, which provides a principled approach to detecting and measuring
the severity of potential harms associated with a text-based model. Second, we
apply our methodology to audit Twitter's English marginal abuse model, which is
used for removing amplification eligibility of marginally abusive content.
Without utilizing demographic labels or dialect classifiers, we are still able
to detect and measure the severity of issues related to the over-penalization
of the speech of marginalized communities, such as the use of reclaimed speech,
counterspeech, and identity related terms. In order to mitigate the associated
harms, we experiment with adding additional true negative examples and find
that doing so provides improvements to our fairness metrics without large
degradations in model performance.
</p></li>
</ul>

<h3>Title: Equal Experience in Recommender Systems. (arXiv:2210.05936v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05936">http://arxiv.org/abs/2210.05936</a></li>
<li>Code URL: <a href="https://github.com/cjw2525/fairrec">https://github.com/cjw2525/fairrec</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05936] Equal Experience in Recommender Systems](http://arxiv.org/abs/2210.05936)</code></li>
<li>Summary: <p>We explore the fairness issue that arises in recommender systems. Biased data
due to inherent stereotypes of particular groups (e.g., male students' average
rating on mathematics is often higher than that on humanities, and vice versa
for females) may yield a limited scope of suggested items to a certain group of
users. Our main contribution lies in the introduction of a novel fairness
notion (that we call equal experience), which can serve to regulate such
unfairness in the presence of biased data. The notion captures the degree of
the equal experience of item recommendations across distinct groups. We propose
an optimization framework that incorporates the fairness notion as a
regularization term, as well as introduce computationally-efficient algorithms
that solve the optimization. Experiments on synthetic and benchmark real
datasets demonstrate that the proposed framework can indeed mitigate such
unfairness while exhibiting a minor degradation of recommendation accuracy.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features. (arXiv:2210.05916v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05916">http://arxiv.org/abs/2210.05916</a></li>
<li>Code URL: <a href="https://github.com/gokulkarthik/hateclipper">https://github.com/gokulkarthik/hateclipper</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05916] Hate-CLIPper: Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features](http://arxiv.org/abs/2210.05916)</code></li>
<li>Summary: <p>Hateful memes are a growing menace on social media. While the image and its
corresponding text in a meme are related, they do not necessarily convey the
same meaning when viewed individually. Hence, detecting hateful memes requires
careful consideration of both visual and textual information. Multimodal
pre-training can be beneficial for this task because it effectively captures
the relationship between the image and the text by representing them in a
similar feature space. Furthermore, it is essential to model the interactions
between the image and text features through intermediate fusion. Most existing
methods either employ multimodal pre-training or intermediate fusion, but not
both. In this work, we propose the Hate-CLIPper architecture, which explicitly
models the cross-modal interactions between the image and text representations
obtained using Contrastive Language-Image Pre-training (CLIP) encoders via a
feature interaction matrix (FIM). A simple classifier based on the FIM
representation is able to achieve state-of-the-art performance on the Hateful
Memes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the
human performance of 82.65. Experiments on other meme datasets such as
Propaganda Memes and TamilMemes also demonstrate the generalizability of the
proposed approach. Finally, we analyze the interpretability of the FIM
representation and show that cross-modal interactions can indeed facilitate the
learning of meaningful concepts. The code for this work is available at
https://github.com/gokulkarthik/hateclipper.
</p></li>
</ul>

<h3>Title: Decoupled Context Processing for Context Augmented Language Modeling. (arXiv:2210.05758v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05758">http://arxiv.org/abs/2210.05758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05758] Decoupled Context Processing for Context Augmented Language Modeling](http://arxiv.org/abs/2210.05758)</code></li>
<li>Summary: <p>Language models can be augmented with a context retriever to incorporate
knowledge from large external databases. By leveraging retrieved context, the
neural network does not have to memorize the massive amount of world knowledge
within its internal parameters, leading to better parameter efficiency,
interpretability and modularity. In this paper we examined a simple yet
effective architecture for incorporating external context into language models
based on decoupled Encoder Decoder architecture. We showed that such a simple
architecture achieves competitive results on auto-regressive language modeling
and open domain question answering tasks. We also analyzed the behavior of the
proposed model which performs grounded context transfer. Finally we discussed
the computational implications of such retrieval augmented models.
</p></li>
</ul>

<h3>Title: Quasi-symbolic explanatory NLI via disentanglement: A geometrical examination. (arXiv:2210.06230v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.06230">http://arxiv.org/abs/2210.06230</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.06230] Quasi-symbolic explanatory NLI via disentanglement: A geometrical examination](http://arxiv.org/abs/2210.06230)</code></li>
<li>Summary: <p>Disentangling the encodings of neural models is a fundamental aspect for
improving interpretability, semantic control, and understanding downstream task
performance in Natural Language Processing. The connection points between
disentanglement and downstream tasks, however, remains underexplored from a
explanatory standpoint. This work presents a methodology for assessment of
geometrical properties of the resulting latent space w.r.t. vector operations
and semantic disentanglement in quantitative and qualitative terms, based on a
VAE-based supervised framework. Empirical results indicate that the
role-contents of explanations, such as \textit{ARG0-animal}, are disentangled
in the latent space, which provides us a chance for controlling the explanation
generation by manipulating the traversal of vector over latent space.
</p></li>
</ul>

<h3>Title: Classification by estimating the cumulative distribution function for small data. (arXiv:2210.05953v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.05953">http://arxiv.org/abs/2210.05953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.05953] Classification by estimating the cumulative distribution function for small data](http://arxiv.org/abs/2210.05953)</code></li>
<li>Summary: <p>In this paper, we study the classification problem by estimating the
conditional probability function of the given data. Different from the
traditional expected risk estimation theory on empirical data, we calculate the
probability via Fredholm equation, this leads to estimate the distribution of
the data. Based on the Fredholm equation, a new expected risk estimation theory
by estimating the cumulative distribution function is presented. The main
characteristics of the new expected risk estimation is to measure the risk on
the distribution of the input space. The corresponding empirical risk
estimation is also presented, and an $\varepsilon$-insensitive $L_{1}$
cumulative support vector machines ($\varepsilon$-$L_{1}$VSVM) is proposed by
introducing an insensitive loss. It is worth mentioning that the classification
models and the classification evaluation indicators based on the new mechanism
are different from the traditional one. Experimental results show the
effectiveness of the proposed $\varepsilon$-$L_{1}$VSVM and the corresponding
cumulative distribution function indicator on validity and interpretability of
small data classification.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
