<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-01-15</h1>
<h3>Title: Multi-task Domain Adaptation for Computation Offloading in Edge-intelligence Networks</h3>
<ul>
<li><strong>Authors: </strong>Runxin Han (1), Bo Yang (1), Zhiwen Yu (1), Xuelin Cao (2), George C. Alexandropoulos (3,4), Chau Yuen (5) ((1) School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China, (2) School of Cyber Engineering, Xidian University, Xi'an, Shaanxi, China, (3) Department of Informatics and Telecommunications, National and Kapodistrian University of Athens, Athens, Greece, (4) Department of Electrical and Computer Engineering, University of Illinois Chicago, IL, USA, (5) School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07585">https://arxiv.org/abs/2501.07585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07585">https://arxiv.org/pdf/2501.07585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07585]] Multi-task Domain Adaptation for Computation Offloading in Edge-intelligence Networks(https://arxiv.org/abs/2501.07585)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In the field of multi-access edge computing (MEC), efficient computation offloading is crucial for improving resource utilization and reducing latency in dynamically changing environments. This paper introduces a new approach, termed as Multi-Task Domain Adaptation (MTDA), aiming to enhance the ability of computational offloading models to generalize in the presence of domain shifts, i.e., when new data in the target environment significantly differs from the data in the source domain. The proposed MTDA model incorporates a teacher-student architecture that allows continuous adaptation without necessitating access to the source domain data during inference, thereby maintaining privacy and reducing computational overhead. Utilizing a multi-task learning framework that simultaneously manages offloading decisions and resource allocation, the proposed MTDA approach outperforms benchmark methods regarding mean squared error and accuracy, particularly in environments with increasing numbers of users. It is observed by means of computer simulation that the proposed MTDA model maintains high performance across various scenarios, demonstrating its potential for practical deployment in emerging MEC applications.</li>
</ul>

<h3>Title: A Multi-Layer CNN-GRUSKIP model based on transformer for spatial TEMPORAL traffic flow prediction</h3>
<ul>
<li><strong>Authors: </strong>Karimeh Ibrahim Mohammad Ata, Mohd Khair Hassan, Ayad Ghany Ismaeel, Syed Abdul Rahman Al-Haddad, Thamer Alquthami, Sameer Alani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07593">https://arxiv.org/abs/2501.07593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07593">https://arxiv.org/pdf/2501.07593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07593]] A Multi-Layer CNN-GRUSKIP model based on transformer for spatial TEMPORAL traffic flow prediction(https://arxiv.org/abs/2501.07593)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Traffic flow prediction remains a cornerstone for intelligent transportation systems ITS, influencing both route optimization and environmental efforts. While Recurrent Neural Networks RNN and traditional Convolutional Neural Networks CNN offer some insights into the spatial temporal dynamics of traffic data, they are often limited when navigating sparse and extended spatial temporal patterns. In response, the CNN-GRUSKIP model emerges as a pioneering approach. Notably, it integrates the GRU-SKIP mechanism, a hybrid model that leverages the Gate Recurrent Unit of GRU capabilities to process sequences with the SKIP feature of ability to bypass and connect longer temporal dependencies, making it especially potent for traffic flow predictions with erratic and extended patterns. Another distinctive aspect is its non-standard 6-layer CNN, meticulously designed for in-depth spatiotemporal correlation extraction. The model comprises (1) the specialized CNN feature extraction, (2) the GRU-SKIP enhanced long-temporal module adept at capturing extended patterns, (3) a transformer module employing encoder-decoder and multi-attention mechanisms to hone prediction accuracy and trim model complexity, and (4) a bespoke prediction module. When tested against real-world datasets from California of Caltrans Performance Measurement System PeMS, specifically PeMS districts 4 and 8, the CNN-GRUSKIP consistently outperformed established models such as ARIMA, Graph Wave Net, HA, LSTM, STGCN, and APTN. With its potent predictive prowess and adaptive architecture, the CNN-GRUSKIP model stands to redefine ITS applications, especially where nuanced traffic dynamics are in play.</li>
</ul>

<h3>Title: Analyzing Spatio-Temporal Dynamics of Dissolved Oxygen for the River Thames using Superstatistical Methods and Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Hankun He, Takuya Boehringer, Benjamin Sch√§fer, Kate Heppell, Christian Beck</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07599">https://arxiv.org/abs/2501.07599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07599">https://arxiv.org/pdf/2501.07599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07599]] Analyzing Spatio-Temporal Dynamics of Dissolved Oxygen for the River Thames using Superstatistical Methods and Machine Learning(https://arxiv.org/abs/2501.07599)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>By employing superstatistical methods and machine learning, we analyze time series data of water quality indicators for the River Thames, with a specific focus on the dynamics of dissolved oxygen. After detrending, the probability density functions of dissolved oxygen fluctuations exhibit heavy tails that are effectively modeled using $q$-Gaussian distributions. Our findings indicate that the multiplicative Empirical Mode Decomposition method stands out as the most effective detrending technique, yielding the highest log-likelihood in nearly all fittings. We also observe that the optimally fitted width parameter of the $q$-Gaussian shows a negative correlation with the distance to the sea, highlighting the influence of geographical factors on water quality dynamics. In the context of same-time prediction of dissolved oxygen, regression analysis incorporating various water quality indicators and temporal features identify the Light Gradient Boosting Machine as the best model. SHapley Additive exPlanations reveal that temperature, pH, and time of year play crucial roles in the predictions. Furthermore, we use the Transformer to forecast dissolved oxygen concentrations. For long-term forecasting, the Informer model consistently delivers superior performance, achieving the lowest MAE and SMAPE with the 192 historical time steps that we used. This performance is attributed to the Informer's ProbSparse self-attention mechanism, which allows it to capture long-range dependencies in time-series data more effectively than other machine learning models. It effectively recognizes the half-life cycle of dissolved oxygen, with particular attention to key intervals. Our findings provide valuable insights for policymakers involved in ecological health assessments, aiding in accurate predictions of river water quality and the maintenance of healthy aquatic ecosystems.</li>
</ul>

<h3>Title: Impact of Data Breadth and Depth on Performance of Siamese Neural Network Model: Experiments with Three Keystroke Dynamic Datasets</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Anu Wahab, Daqing Hou, Nadia Cheng, Parker Huntley, Charles Devlen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07600">https://arxiv.org/abs/2501.07600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07600">https://arxiv.org/pdf/2501.07600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07600]] Impact of Data Breadth and Depth on Performance of Siamese Neural Network Model: Experiments with Three Keystroke Dynamic Datasets(https://arxiv.org/abs/2501.07600)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Deep learning models, such as the Siamese Neural Networks (SNN), have shown great potential in capturing the intricate patterns in behavioral data. However, the impacts of dataset breadth (i.e., the number of subjects) and depth (e.g., the amount of training samples per subject) on the performance of these models is often informally assumed, and remains under-explored. To this end, we have conducted extensive experiments using the concepts of "feature space" and "density" to guide and gain deeper understanding on the impact of dataset breadth and depth on three publicly available keystroke datasets (Aalto, CMU and Clarkson II). Through varying the number of training subjects, number of samples per subject, amount of data in each sample, and number of triplets used in training, we found that when feasible, increasing dataset breadth enables the training of a well-trained model that effectively captures more inter-subject variability. In contrast, we find that the extent of depth's impact from a dataset depends on the nature of the dataset. Free-text datasets are influenced by all three depth-wise factors; inadequate samples per subject, sequence length, training triplets and gallery sample size, which may all lead to an under-trained model. Fixed-text datasets are less affected by these factors, and as such make it easier to create a well-trained model. These findings shed light on the importance of dataset breadth and depth in training deep learning models for behavioral biometrics and provide valuable insights for designing more effective authentication systems.</li>
</ul>

<h3>Title: Kolmogorov-Arnold Networks and Evolutionary Game Theory for More Personalized Cancer Treatment</h3>
<ul>
<li><strong>Authors: </strong>Sepinoud Azimi, Louise Spekking, Kate≈ôina Sta≈àkov√°</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07611">https://arxiv.org/abs/2501.07611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07611">https://arxiv.org/pdf/2501.07611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07611]] Kolmogorov-Arnold Networks and Evolutionary Game Theory for More Personalized Cancer Treatment(https://arxiv.org/abs/2501.07611)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Personalized cancer treatment is revolutionizing oncology by leveraging precision medicine and advanced computational techniques to tailor therapies to individual patients. Despite its transformative potential, challenges such as limited generalizability, interpretability, and reproducibility of predictive models hinder its integration into clinical practice. Current methodologies often rely on black-box machine learning models, which, while accurate, lack the transparency needed for clinician trust and real-world application. This paper proposes the development of an innovative framework that bridges Kolmogorov-Arnold Networks (KANs) and Evolutionary Game Theory (EGT) to address these limitations. Inspired by the Kolmogorov-Arnold representation theorem, KANs offer interpretable, edge-based neural architectures capable of modeling complex biological systems with unprecedented adaptability. Their integration into the EGT framework enables dynamic modeling of cancer progression and treatment responses. By combining KAN's computational precision with EGT's mechanistic insights, this hybrid approach promises to enhance predictive accuracy, scalability, and clinical usability.</li>
</ul>

<h3>Title: GPT as a Monte Carlo Language Tree: A Probabilistic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Kun-Peng Ning, Jia-Yu Yao, Yu-Yang Liu, Mu-Nan Ning, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07641">https://arxiv.org/abs/2501.07641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07641">https://arxiv.org/pdf/2501.07641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07641]] GPT as a Monte Carlo Language Tree: A Probabilistic Perspective(https://arxiv.org/abs/2501.07641)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as GPT, are considered to learn the latent distributions within large-scale web-crawl datasets and accomplish natural language processing (NLP) tasks by predicting the next token. However, this mechanism of latent distribution modeling lacks quantitative understanding and analysis. In this paper, we propose a novel perspective that any language dataset can be represented by a Monte Carlo Language Tree (abbreviated as ``Data-Tree''), where each node denotes a token, each edge denotes a token transition probability, and each sequence has a unique path. Any GPT-like language model can also be flattened into another Monte Carlo Language Tree (abbreviated as ``GPT-Tree''). Our experiments show that different GPT models trained on the same dataset exhibit significant structural similarity in GPT-Tree visualization, and larger models converge more closely to the Data-Tree. More than 87\% GPT output tokens can be recalled by Data-Tree. These findings may confirm that the reasoning process of LLMs is more likely to be probabilistic pattern-matching rather than formal reasoning, as each model inference seems to find a context pattern with maximum probability from the Data-Tree. Furthermore, we provide deeper insights into issues such as hallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs.</li>
</ul>

<h3>Title: BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations</h3>
<ul>
<li><strong>Authors: </strong>Weixi Feng, Chao Liu, Sifei Liu, William Yang Wang, Arash Vahdat, Weili Nie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07647">https://arxiv.org/abs/2501.07647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07647">https://arxiv.org/pdf/2501.07647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07647]] BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations(https://arxiv.org/abs/2501.07647)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing video generation models struggle to follow complex text prompts and synthesize multiple objects, raising the need for additional grounding input for improved controllability. In this work, we propose to decompose videos into visual primitives - blob video representation, a general representation for controllable video generation. Based on blob conditions, we develop a blob-grounded video diffusion model named BlobGEN-Vid that allows users to control object motions and fine-grained object appearance. In particular, we introduce a masked 3D attention module that effectively improves regional consistency across frames. In addition, we introduce a learnable module to interpolate text embeddings so that users can control semantics in specific frames and obtain smooth object transitions. We show that our framework is model-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video diffusion models. Extensive experimental results show that BlobGEN-Vid achieves superior zero-shot video generation ability and state-of-the-art layout controllability on multiple benchmarks. When combined with an LLM for layout planning, our framework even outperforms proprietary text-to-video generators in terms of compositional accuracy.</li>
</ul>

<h3>Title: Enhancing Talent Employment Insights Through Feature Extraction with LLM Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Karishma Thakrar, Nick Young</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07663">https://arxiv.org/abs/2501.07663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07663">https://arxiv.org/pdf/2501.07663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07663]] Enhancing Talent Employment Insights Through Feature Extraction with LLM Finetuning(https://arxiv.org/abs/2501.07663)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the application of large language models (LLMs) to extract nuanced and complex job features from unstructured job postings. Using a dataset of 1.2 million job postings provided by AdeptID, we developed a robust pipeline to identify and classify variables such as remote work availability, remuneration structures, educational requirements, and work experience preferences. Our methodology combines semantic chunking, retrieval-augmented generation (RAG), and fine-tuning DistilBERT models to overcome the limitations of traditional parsing tools. By leveraging these techniques, we achieved significant improvements in identifying variables often mislabeled or overlooked, such as non-salary-based compensation and inferred remote work categories. We present a comprehensive evaluation of our fine-tuned models and analyze their strengths, limitations, and potential for scaling. This work highlights the promise of LLMs in labor market analytics, providing a foundation for more accurate and actionable insights into job data.</li>
</ul>

<h3>Title: A Survey of Early Exit Deep Neural Networks in NLP</h3>
<ul>
<li><strong>Authors: </strong>Divya Jyoti Bajpai, Manjesh Kumar Hanawal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07670">https://arxiv.org/abs/2501.07670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07670">https://arxiv.org/pdf/2501.07670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07670]] A Survey of Early Exit Deep Neural Networks in NLP(https://arxiv.org/abs/2501.07670)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) have grown increasingly large in size to achieve state of the art performance across a wide range of tasks. However, their high computational requirements make them less suitable for resource-constrained applications. Also, real-world datasets often consist of a mixture of easy and complex samples, necessitating adaptive inference mechanisms that account for sample difficulty. Early exit strategies offer a promising solution by enabling adaptive inference, where simpler samples are classified using the initial layers of the DNN, thereby accelerating the overall inference process. By attaching classifiers at different layers, early exit methods not only reduce inference latency but also improve the model robustness against adversarial attacks. This paper presents a comprehensive survey of early exit methods and their applications in NLP.</li>
</ul>

<h3>Title: Dataset Distillation as Pushforward Optimal Quantization</h3>
<ul>
<li><strong>Authors: </strong>Hong Ye Tan, Emma Slade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07681">https://arxiv.org/abs/2501.07681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07681">https://arxiv.org/pdf/2501.07681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07681]] Dataset Distillation as Pushforward Optimal Quantization(https://arxiv.org/abs/2501.07681)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose a simple extension of the state-of-the-art data distillation method D4M, achieving better performance on the ImageNet-1K dataset with trivial additional computation, and state-of-the-art performance in higher image-per-class settings.</li>
</ul>

<h3>Title: A Review on the Security Vulnerabilities of the IoMT against Malware Attacks and DDoS</h3>
<ul>
<li><strong>Authors: </strong>Lily Dzamesi, Nelly Elsayed</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07703">https://arxiv.org/abs/2501.07703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07703">https://arxiv.org/pdf/2501.07703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07703]] A Review on the Security Vulnerabilities of the IoMT against Malware Attacks and DDoS(https://arxiv.org/abs/2501.07703)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>The Internet of Medical Things (IoMT) has transformed the healthcare industry by connecting medical devices in monitoring treatment outcomes of patients. This increased connectivity has resulted to significant security vulnerabilities in the case of malware and Distributed Denial of Service (DDoS) attacks. This literature review examines the vulnerabilities of IoMT devices, focusing on critical threats and exploring mitigation strategies. We conducted a comprehensive search across leading databases such as ACM Digital Library, IEEE Xplore, and Elsevier to analyze peer-reviewed studies published within the last five years (from 2019 to 2024). The review shows that inadequate encryption protocols, weak authentication methods, and irregular firmware updates are the main causes of risks associated with IoMT devices. We have identified emerging solutions like machine learning algorithms, blockchain technology, and edge computing as promising approaches to enhance IoMT security. This review emphasizes the pressing need to develop lightweight security measures and standardized protocols to protect patient data and ensure the integrity of healthcare services.</li>
</ul>

<h3>Title: Pedestrian Trajectory Prediction Based on Social Interactions Learning With Random Weights</h3>
<ul>
<li><strong>Authors: </strong>Jiajia Xie, Sheng Zhang, Beihao Xia, Zhu Xiao, Hongbo Jiang, Siwang Zhou, Zheng Qin, Hongyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07711">https://arxiv.org/abs/2501.07711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07711">https://arxiv.org/pdf/2501.07711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07711]] Pedestrian Trajectory Prediction Based on Social Interactions Learning With Random Weights(https://arxiv.org/abs/2501.07711)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pedestrian trajectory prediction is a critical technology in the evolution of self-driving cars toward complete artificial intelligence. Over recent years, focusing on the trajectories of pedestrians to model their social interactions has surged with great interest in more accurate trajectory predictions. However, existing methods for modeling pedestrian social interactions rely on pre-defined rules, struggling to capture non-explicit social interactions. In this work, we propose a novel framework named DTGAN, which extends the application of Generative Adversarial Networks (GANs) to graph sequence data, with the primary objective of automatically capturing implicit social interactions and achieving precise predictions of pedestrian trajectory. DTGAN innovatively incorporates random weights within each graph to eliminate the need for pre-defined interaction rules. We further enhance the performance of DTGAN by exploring diverse task loss functions during adversarial training, which yields improvements of 16.7\% and 39.3\% on metrics ADE and FDE, respectively. The effectiveness and accuracy of our framework are verified on two public datasets. The experimental results show that our proposed DTGAN achieves superior performance and is well able to understand pedestrians' intentions.</li>
</ul>

<h3>Title: Testing Human-Hand Segmentation on In-Distribution and Out-of-Distribution Data in Human-Robot Interactions Using a Deep Ensemble Model</h3>
<ul>
<li><strong>Authors: </strong>Reza Jalayer, Yuxin Chen, Masoud Jalayer, Carlotta Orsenigo, Masayoshi Tomizuka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07713">https://arxiv.org/abs/2501.07713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07713">https://arxiv.org/pdf/2501.07713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07713]] Testing Human-Hand Segmentation on In-Distribution and Out-of-Distribution Data in Human-Robot Interactions Using a Deep Ensemble Model(https://arxiv.org/abs/2501.07713)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Reliable detection and segmentation of human hands are critical for enhancing safety and facilitating advanced interactions in human-robot collaboration. Current research predominantly evaluates hand segmentation under in-distribution (ID) data, which reflects the training data of deep learning (DL) models. However, this approach fails to address out-of-distribution (OOD) scenarios that often arise in real-world human-robot interactions. In this study, we present a novel approach by evaluating the performance of pre-trained DL models under both ID data and more challenging OOD scenarios. To mimic realistic industrial scenarios, we designed a diverse dataset featuring simple and cluttered backgrounds with industrial tools, varying numbers of hands (0 to 4), and hands with and without gloves. For OOD scenarios, we incorporated unique and rare conditions such as finger-crossing gestures and motion blur from fast-moving hands, addressing both epistemic and aleatoric uncertainties. To ensure multiple point of views (PoVs), we utilized both egocentric cameras, mounted on the operator's head, and static cameras to capture RGB images of human-robot interactions. This approach allowed us to account for multiple camera perspectives while also evaluating the performance of models trained on existing egocentric datasets as well as static-camera datasets. For segmentation, we used a deep ensemble model composed of UNet and RefineNet as base learners. Performance evaluation was conducted using segmentation metrics and uncertainty quantification via predictive entropy. Results revealed that models trained on industrial datasets outperformed those trained on non-industrial datasets, highlighting the importance of context-specific training. Although all models struggled with OOD scenarios, those trained on industrial datasets demonstrated significantly better generalization.</li>
</ul>

<h3>Title: Benchmarking Abstractive Summarisation: A Dataset of Human-authored Summaries of Norwegian News Articles</h3>
<ul>
<li><strong>Authors: </strong>Samia Touileb, Vladislav Mikhailov, Marie Kroka, Lilja √òvrelid, Erik Velldal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07718">https://arxiv.org/abs/2501.07718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07718">https://arxiv.org/pdf/2501.07718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07718]] Benchmarking Abstractive Summarisation: A Dataset of Human-authored Summaries of Norwegian News Articles(https://arxiv.org/abs/2501.07718)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a dataset of high-quality human-authored summaries of news articles in Norwegian. The dataset is intended for benchmarking the abstractive summarisation capabilities of generative language models. Each document in the dataset is provided with three different candidate gold-standard summaries written by native Norwegian speakers, and all summaries are provided in both of the written variants of Norwegian -- Bokm√•l and Nynorsk. The paper describes details on the data creation effort as well as an evaluation of existing open LLMs for Norwegian on the dataset. We also provide insights from a manual human evaluation, comparing human-authored to model-generated summaries. Our results indicate that the dataset provides a challenging LLM benchmark for Norwegian summarisation capabilities</li>
</ul>

<h3>Title: LLMic: Romanian Foundation Language Model</h3>
<ul>
<li><strong>Authors: </strong>Vlad-Andrei BƒÉdoiu, Mihai-Valentin Dumitru, Alexandru M. Gherghescu, Alexandru Agache, Costin Raiciu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07721">https://arxiv.org/abs/2501.07721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07721">https://arxiv.org/pdf/2501.07721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07721]] LLMic: Romanian Foundation Language Model(https://arxiv.org/abs/2501.07721)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks with commercial models leading the way. While open models usually operate at a smaller scale, they maintain competitiveness through specialization and fine-tuning. However, a significant challenge persists: open models often underperform in low-resource languages due to limited representation in the training corpus. In this paper, we present LLMic, a bilingual foundation language model designed specifically for the Romanian Language. We document the complete process of pretraining a foundation model for a low-resource language, including corpus construction, architecture selection, and hyper-parameter optimization. Our evaluation demonstrates that LLMic can be specialized for tasks in the target language, achieving results comparable to other much larger open models. We show that fine-tuning LLMic for language translation after the initial pretraining phase outperforms existing solutions in English-to-Romanian translation tasks. This opens the path for efficient large-scale processing for the Romanian language community, using the much smaller LLMic model</li>
</ul>

<h3>Title: ESURF: Simple and Effective EDU Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Sediqin, Shlomo Engelson Argamon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07723">https://arxiv.org/abs/2501.07723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07723">https://arxiv.org/pdf/2501.07723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07723]] ESURF: Simple and Effective EDU Segmentation(https://arxiv.org/abs/2501.07723)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segmenting text into Elemental Discourse Units (EDUs) is a fundamental task in discourse parsing. We present a new simple method for identifying EDU boundaries, and hence segmenting them, based on lexical and character n-gram features, using random forest classification. We show that the method, despite its simplicity, outperforms other methods both for segmentation and within a state of the art discourse parser. This indicates the importance of such features for identifying basic discourse elements, pointing towards potentially more training-efficient methods for discourse analysis.</li>
</ul>

<h3>Title: Exploring the encoding of linguistic representations in the Fully-Connected Layer of generative CNNs for Speech</h3>
<ul>
<li><strong>Authors: </strong>Bruno Ferenc ≈†egedin, Gasper Begu≈°</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07726">https://arxiv.org/abs/2501.07726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07726">https://arxiv.org/pdf/2501.07726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07726]] Exploring the encoding of linguistic representations in the Fully-Connected Layer of generative CNNs for Speech(https://arxiv.org/abs/2501.07726)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Interpretability work on the convolutional layers of CNNs has primarily focused on computer vision, but some studies also explore correspondences between the latent space and the output in the audio domain. However, it has not been thoroughly examined how acoustic and linguistic information is represented in the fully connected (FC) layer that bridges the latent space and convolutional layers. The current study presents the first exploration of how the FC layer of CNNs for speech synthesis encodes linguistically relevant information. We propose two techniques for exploration of the fully connected layer. In Experiment 1, we use weight matrices as inputs into convolutional layers. In Experiment 2, we manipulate the FC layer to explore how symbolic-like representations are encoded in CNNs. We leverage the fact that the FC layer outputs a feature map and that variable-specific weight matrices are temporally structured to (1) demonstrate how the distribution of learned weights varies between latent variables in systematic ways and (2) demonstrate how manipulating the FC layer while holding constant subsequent model parameters affects the output. We ultimately present an FC manipulation that can output a single segment. Using this technique, we show that lexically specific latent codes in generative CNNs (ciwGAN) have shared lexically invariant sublexical representations in the FC-layer weights, showing that ciwGAN encodes lexical information in a linguistically principled manner.</li>
</ul>

<h3>Title: Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens</h3>
<ul>
<li><strong>Authors: </strong>Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, Liang-Chieh Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07730">https://arxiv.org/abs/2501.07730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07730">https://arxiv.org/pdf/2501.07730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07730]] Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens(https://arxiv.org/abs/2501.07730)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models.</li>
</ul>

<h3>Title: HyperQuery: Beyond Binary Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Sepideh Maleki, Josh Vekhter, Keshav Pingali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07731">https://arxiv.org/abs/2501.07731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07731">https://arxiv.org/pdf/2501.07731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07731]] HyperQuery: Beyond Binary Link Prediction(https://arxiv.org/abs/2501.07731)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Groups with complex set intersection relations are a natural way to model a wide array of data, from the formation of social groups to the complex protein interactions which form the basis of biological life. One approach to representing such higher order relationships is as a hypergraph. However, efforts to apply machine learning techniques to hypergraph structured datasets have been limited thus far. In this paper, we address the problem of link prediction in knowledge hypergraphs as well as simple hypergraphs and develop a novel, simple, and effective optimization architecture that addresses both tasks. Additionally, we introduce a novel feature extraction technique using node level clustering and we show how integrating data from node-level labels can improve system performance. Our self-supervised approach achieves significant improvement over state of the art baselines on several hyperedge prediction and knowledge hypergraph completion benchmarks.</li>
</ul>

<h3>Title: Advancing Student Writing Through Automated Syntax Feedback</h3>
<ul>
<li><strong>Authors: </strong>Kamyar Zeinalipour, Mehak Mehak, Fatemeh Parsamotamed, Marco Maggini, Marco Gori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07740">https://arxiv.org/abs/2501.07740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07740">https://arxiv.org/pdf/2501.07740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07740]] Advancing Student Writing Through Automated Syntax Feedback(https://arxiv.org/abs/2501.07740)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study underscores the pivotal role of syntax feedback in augmenting the syntactic proficiency of students. Recognizing the challenges faced by learners in mastering syntactic nuances, we introduce a specialized dataset named Essay-Syntax-Instruct designed to enhance the understanding and application of English syntax among these students. Leveraging the capabilities of Large Language Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a comprehensive fine-tuning process tailored to the syntax improvement task. Through meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit a marked improvement in addressing syntax-related challenges, thereby serving as a potent tool for students to identify and rectify their syntactic errors. The findings not only highlight the effectiveness of the proposed dataset in elevating the performance of LLMs for syntax enhancement but also illuminate a promising path for utilizing advanced language models to support language acquisition efforts. This research contributes to the broader field of language learning technology by showcasing the potential of LLMs in facilitating the linguistic development of Students.</li>
</ul>

<h3>Title: Scaling Up ESM2 Architectures for Long Protein Sequences Analysis: Long and Quantized Approaches</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Bianchin de Oliveira, Helio Pedrini, Zanoni Dias</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07747">https://arxiv.org/abs/2501.07747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07747">https://arxiv.org/pdf/2501.07747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07747]] Scaling Up ESM2 Architectures for Long Protein Sequences Analysis: Long and Quantized Approaches(https://arxiv.org/abs/2501.07747)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Various approaches utilizing Transformer architectures have achieved state-of-the-art results in Natural Language Processing (NLP). Based on this success, numerous architectures have been proposed for other types of data, such as in biology, particularly for protein sequences. Notably among these are the ESM2 architectures, pre-trained on billions of proteins, which form the basis of various state-of-the-art approaches in the field. However, the ESM2 architectures have a limitation regarding input size, restricting it to 1,022 amino acids, which necessitates the use of preprocessing techniques to handle sequences longer than this limit. In this paper, we present the long and quantized versions of the ESM2 architectures, doubling the input size limit to 2,048 amino acids.</li>
</ul>

<h3>Title: Boosting Sclera Segmentation through Semi-supervised Learning with Fewer Labels</h3>
<ul>
<li><strong>Authors: </strong>Guanjun Wang, Lu Wang, Ning Niu, Qiaoyi Yao, Yixuan Wang, Sufen Ren, Shengchao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07750">https://arxiv.org/abs/2501.07750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07750">https://arxiv.org/pdf/2501.07750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07750]] Boosting Sclera Segmentation through Semi-supervised Learning with Fewer Labels(https://arxiv.org/abs/2501.07750)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Sclera segmentation is crucial for developing automatic eye-related medical computer-aided diagnostic systems, as well as for personal identification and verification, because the sclera contains distinct personal features. Deep learning-based sclera segmentation has achieved significant success compared to traditional methods that rely on hand-crafted features, primarily because it can autonomously extract critical output-related features without the need to consider potential physical constraints. However, achieving accurate sclera segmentation using these methods is challenging due to the scarcity of high-quality, fully labeled datasets, which depend on costly, labor-intensive medical acquisition and expertise. To address this challenge, this paper introduces a novel sclera segmentation framework that excels with limited labeled samples. Specifically, we employ a semi-supervised learning method that integrates domain-specific improvements and image-based spatial transformations to enhance segmentation performance. Additionally, we have developed a real-world eye diagnosis dataset to enrich the evaluation process. Extensive experiments on our dataset and two additional public datasets demonstrate the effectiveness and superiority of our proposed method, especially with significantly fewer labeled samples.</li>
</ul>

<h3>Title: PSReg: Prior-guided Sparse Mixture of Experts for Point Cloud Registration</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshui Huang, Zhou Huang, Yifan Zuo, Yongshun Gong, Chengdong Zhang, Deyang Liu, Yuming Fang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07762">https://arxiv.org/abs/2501.07762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07762">https://arxiv.org/pdf/2501.07762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07762]] PSReg: Prior-guided Sparse Mixture of Experts for Point Cloud Registration(https://arxiv.org/abs/2501.07762)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The discriminative feature is crucial for point cloud registration. Recent methods improve the feature discriminative by distinguishing between non-overlapping and overlapping region points. However, they still face challenges in distinguishing the ambiguous structures in the overlapping regions. Therefore, the ambiguous features they extracted resulted in a significant number of outlier matches from overlapping regions. To solve this problem, we propose a prior-guided SMoE-based registration method to improve the feature distinctiveness by dispatching the potential correspondences to the same experts. Specifically, we propose a prior-guided SMoE module by fusing prior overlap and potential correspondence embeddings for routing, assigning tokens to the most suitable experts for processing. In addition, we propose a registration framework by a specific combination of Transformer layer and prior-guided SMoE module. The proposed method not only pays attention to the importance of locating the overlapping areas of point clouds, but also commits to finding more accurate correspondences in overlapping areas. Our extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art registration recall (95.7\%/79.3\%) on the 3DMatch/3DLoMatch benchmark. Moreover, we also test the performance on ModelNet40 and demonstrate excellent performance.</li>
</ul>

<h3>Title: Deep Learning for Disease Outbreak Prediction: A Robust Early Warning Signal for Transcritical Bifurcations</h3>
<ul>
<li><strong>Authors: </strong>Reza Miry, Amit K. Chakraborty, Russell Greiner, Mark A. Lewis, Hao Wang, Tianyu Guan, Pouria Ramazi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07764">https://arxiv.org/abs/2501.07764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07764">https://arxiv.org/pdf/2501.07764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07764]] Deep Learning for Disease Outbreak Prediction: A Robust Early Warning Signal for Transcritical Bifurcations(https://arxiv.org/abs/2501.07764)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early Warning Signals (EWSs) are vital for implementing preventive measures before a disease turns into a pandemic. While new diseases exhibit unique behaviors, they often share fundamental characteristics from a dynamical systems perspective. Moreover, measurements during disease outbreaks are often corrupted by different noise sources, posing challenges for Time Series Classification (TSC) tasks. In this study, we address the problem of having a robust EWS for disease outbreak prediction using a best-performing deep learning model in the domain of TSC. We employed two simulated datasets to train the model: one representing generated dynamical systems with randomly selected polynomial terms to model new disease behaviors, and another simulating noise-induced disease dynamics to account for noisy measurements. The model's performance was analyzed using both simulated data from different disease models and real-world data, including influenza and COVID-19. Results demonstrate that the proposed model outperforms previous models, effectively providing EWSs of impending outbreaks across various scenarios. This study bridges advancements in deep learning with the ability to provide robust early warning signals in noisy environments, making it highly applicable to real-world crises involving emerging disease outbreaks.</li>
</ul>

<h3>Title: PINN-FEM: A Hybrid Approach for Enforcing Dirichlet Boundary Conditions in Physics-Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Nahil Sobh, Rini Jasmine Gladstone, Hadi Meidani</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07765">https://arxiv.org/abs/2501.07765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07765">https://arxiv.org/pdf/2501.07765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07765]] PINN-FEM: A Hybrid Approach for Enforcing Dirichlet Boundary Conditions in Physics-Informed Neural Networks(https://arxiv.org/abs/2501.07765)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Physics-Informed Neural Networks (PINNs) solve partial differential equations (PDEs) by embedding governing equations and boundary/initial conditions into the loss function. However, enforcing Dirichlet boundary conditions accurately remains challenging, often leading to soft enforcement that compromises convergence and reliability in complex domains. We propose a hybrid approach, PINN-FEM, which combines PINNs with finite element methods (FEM) to impose strong Dirichlet boundary conditions via domain decomposition. This method incorporates FEM-based representations near the boundary, ensuring exact enforcement without compromising convergence. Through six experiments of increasing complexity, PINN-FEM outperforms standard PINN models, showcasing superior accuracy and robustness. While distance functions and similar techniques have been proposed for boundary condition enforcement, they lack generality for real-world applications. PINN-FEM bridges this gap by leveraging FEM near boundaries, making it well-suited for industrial and scientific problems.</li>
</ul>

<h3>Title: Large Language Models for Knowledge Graph Embedding Techniques, Methods, and Challenges: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Bingchen Liu, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07766">https://arxiv.org/abs/2501.07766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07766">https://arxiv.org/pdf/2501.07766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07766]] Large Language Models for Knowledge Graph Embedding Techniques, Methods, and Challenges: A Survey(https://arxiv.org/abs/2501.07766)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have attracted a lot of attention in various fields due to their superior performance, aiming to train hundreds of millions or more parameters on large amounts of text data to understand and generate natural language. As the superior performance of LLMs becomes apparent, they are increasingly being applied to knowledge graph embedding (KGE) related tasks to improve the processing results. As a deep learning model in the field of Natural Language Processing (NLP), it learns a large amount of textual data to predict the next word or generate content related to a given text. However, LLMs have recently been invoked to varying degrees in different types of KGE related scenarios such as multi-modal KGE and open KGE according to their task characteristics. In this paper, we investigate a wide range of approaches for performing LLMs-related tasks in different types of KGE scenarios. To better compare the various approaches, we summarize each KGE scenario in a classification. In addition to the categorization methods, we provide a tabular overview of the methods and their source code links for a more direct comparison. In the article we also discuss the applications in which the methods are mainly used and suggest several forward-looking directions for the development of this new research area.</li>
</ul>

<h3>Title: Symmetry-Aware Generative Modeling through Learned Canonicalization</h3>
<ul>
<li><strong>Authors: </strong>Kusha Sareen, Daniel Levy, Arnab Kumar Mondal, S√©kou-Oumar Kaba, Tara Akhound-Sadegh, Siamak Ravanbakhsh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07773">https://arxiv.org/abs/2501.07773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07773">https://arxiv.org/pdf/2501.07773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07773]] Symmetry-Aware Generative Modeling through Learned Canonicalization(https://arxiv.org/abs/2501.07773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling of symmetric densities has a range of applications in AI for science, from drug discovery to physics simulations. The existing generative modeling paradigm for invariant densities combines an invariant prior with an equivariant generative process. However, we observe that this technique is not necessary and has several drawbacks resulting from the limitations of equivariant networks. Instead, we propose to model a learned slice of the density so that only one representative element per orbit is learned. To accomplish this, we learn a group-equivariant canonicalization network that maps training samples to a canonical pose and train a non-equivariant generative model over these canonicalized samples. We implement this idea in the context of diffusion models. Our preliminary experimental results on molecular modeling are promising, demonstrating improved sample quality and faster inference time.</li>
</ul>

<h3>Title: Transforming Indoor Localization: Advanced Transformer Architecture for NLOS Dominated Wireless Environments with Distributed Sensors</h3>
<ul>
<li><strong>Authors: </strong>Saad Masrur, Jung-Fu (Thomas)Cheng, Atieh R. Khamesi, Ismail Guvenc</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07774">https://arxiv.org/abs/2501.07774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07774">https://arxiv.org/pdf/2501.07774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07774]] Transforming Indoor Localization: Advanced Transformer Architecture for NLOS Dominated Wireless Environments with Distributed Sensors(https://arxiv.org/abs/2501.07774)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Indoor localization in challenging non-line-of-sight (NLOS) environments often leads to mediocre accuracy with traditional approaches. Deep learning (DL) has been applied to tackle these challenges; however, many DL approaches overlook computational complexity, especially for floating-point operations (FLOPs), making them unsuitable for resource-limited devices. Transformer-based models have achieved remarkable success in natural language processing (NLP) and computer vision (CV) tasks, motivating their use in wireless applications. However, their use in indoor localization remains nascent, and directly applying Transformers for indoor localization can be both computationally intensive and exhibit limitations in accuracy. To address these challenges, in this work, we introduce a novel tokenization approach, referred to as Sensor Snapshot Tokenization (SST), which preserves variable-specific representations of power delay profile (PDP) and enhances attention mechanisms by effectively capturing multi-variate correlation. Complementing this, we propose a lightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer) model, designed to reduce computational complexity without compromising localization accuracy. Together, these contributions mitigate the computational burden and dependency on large datasets, making Transformer models more efficient and suitable for resource-constrained scenarios. The proposed tokenization method enables the Vanilla Transformer to achieve a 90th percentile positioning error of 0.388 m in a highly NLOS indoor factory, surpassing conventional tokenization methods. The L-SwiGLU ViT further reduces the error to 0.355 m, achieving an 8.51% improvement. Additionally, the proposed model outperforms a 14.1 times larger model with a 46.13% improvement, underscoring its computational efficiency.</li>
</ul>

<h3>Title: Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07783">https://arxiv.org/abs/2501.07783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07783">https://arxiv.org/pdf/2501.07783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07783]] Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding(https://arxiv.org/abs/2501.07783)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at this https URL.</li>
</ul>

<h3>Title: Linearly Convergent Mixup Learning</h3>
<ul>
<li><strong>Authors: </strong>Gakuto Obi, Ayato Saito, Yuto Sasaki, Tsuyoshi Kato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07794">https://arxiv.org/abs/2501.07794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07794">https://arxiv.org/pdf/2501.07794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07794]] Linearly Convergent Mixup Learning(https://arxiv.org/abs/2501.07794)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Learning in the reproducing kernel Hilbert space (RKHS) such as the support vector machine has been recognized as a promising technique. It continues to be highly effective and competitive in numerous prediction tasks, particularly in settings where there is a shortage of training data or computational limitations exist. These methods are especially valued for their ability to work with small datasets and their interpretability. To address the issue of limited training data, mixup data augmentation, widely used in deep learning, has remained challenging to apply to learning in RKHS due to the generation of intermediate class labels. Although gradient descent methods handle these labels effectively, dual optimization approaches are typically not directly applicable. In this study, we present two novel algorithms that extend to a broader range of binary classification models. Unlike gradient-based approaches, our algorithms do not require hyperparameters like learning rates, simplifying their implementation and optimization. Both the number of iterations to converge and the computational cost per iteration scale linearly with respect to the dataset size. The numerical experiments demonstrate that our algorithms achieve faster convergence to the optimal solution compared to gradient descent approaches, and that mixup data augmentation consistently improves the predictive performance across various loss functions.</li>
</ul>

<h3>Title: BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Farnoosh Koleini, Muhammad Usama Saleem, Pu Wang, Hongfei Xue, Ahmed Helmy, Abbey Fenwick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07800">https://arxiv.org/abs/2501.07800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07800">https://arxiv.org/pdf/2501.07800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07800]] BioPose: Biomechanically-accurate 3D Pose Estimation from Monocular Videos(https://arxiv.org/abs/2501.07800)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D human pose estimation from single-camera images and videos have relied on parametric models, like SMPL. However, these models oversimplify anatomical structures, limiting their accuracy in capturing true joint locations and movements, which reduces their applicability in biomechanics, healthcare, and robotics. Biomechanically accurate pose estimation, on the other hand, typically requires costly marker-based motion capture systems and optimization techniques in specialized labs. To bridge this gap, we propose BioPose, a novel learning-based framework for predicting biomechanically accurate 3D human pose directly from monocular videos. BioPose includes three key components: a Multi-Query Human Mesh Recovery model (MQ-HMR), a Neural Inverse Kinematics (NeurIK) model, and a 2D-informed pose refinement technique. MQ-HMR leverages a multi-query deformable transformer to extract multi-scale fine-grained image features, enabling precise human mesh recovery. NeurIK treats the mesh vertices as virtual markers, applying a spatial-temporal network to regress biomechanically accurate 3D poses under anatomical constraints. To further improve 3D pose estimations, a 2D-informed refinement step optimizes the query tokens during inference by aligning the 3D structure with 2D pose observations. Experiments on benchmark datasets demonstrate that BioPose significantly outperforms state-of-the-art methods. Project website: \url{this https URL}.</li>
</ul>

<h3>Title: A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security</h3>
<ul>
<li><strong>Authors: </strong>Osvaldo Arreche, Mustafa Abdallah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07801">https://arxiv.org/abs/2501.07801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07801">https://arxiv.org/pdf/2501.07801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07801]] A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security(https://arxiv.org/abs/2501.07801)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>New research focuses on creating artificial intelligence (AI) solutions for network intrusion detection systems (NIDS), drawing its inspiration from the ever-growing number of intrusions on networked systems, increasing its complexity and intelligibility. Hence, the use of explainable AI (XAI) techniques in real-world intrusion detection systems comes from the requirement to comprehend and elucidate black-box AI models to security analysts. In an effort to meet such requirements, this paper focuses on applying and evaluating White-Box XAI techniques (particularly LRP, IG, and DeepLift) for NIDS via an end-to-end framework for neural network models, using three widely used network intrusion datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), assessing its global and local scopes, and examining six distinct assessment measures (descriptive accuracy, sparsity, stability, robustness, efficiency, and completeness). We also compare the performance of white-box XAI methods with black-box XAI methods. The results show that using White-box XAI techniques scores high in robustness and completeness, which are crucial metrics for IDS. Moreover, the source codes for the programs developed for our XAI evaluation framework are available to be improved and used by the research community.</li>
</ul>

<h3>Title: Balance Divergence for Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yafei Qi, Chen Wang, Zhaoning Zhang, Yaping Liu, Yongmin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07804">https://arxiv.org/abs/2501.07804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07804">https://arxiv.org/pdf/2501.07804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07804]] Balance Divergence for Knowledge Distillation(https://arxiv.org/abs/2501.07804)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Knowledge distillation has been widely adopted in computer vision task processing, since it can effectively enhance the performance of lightweight student networks by leveraging the knowledge transferred from cumbersome teacher networks. Most existing knowledge distillation methods utilize Kullback-Leibler divergence to mimic the logit output probabilities between the teacher network and the student network. Nonetheless, these methods may neglect the negative parts of the teacher's ''dark knowledge'' because the divergence calculations may ignore the effect of the minute probabilities from the teacher's logit output. This deficiency may lead to suboptimal performance in logit mimicry during the distillation process and result in an imbalance of information acquired by the student network. In this paper, we investigate the impact of this imbalance and propose a novel method, named Balance Divergence Distillation. By introducing a compensatory operation using reverse Kullback-Leibler divergence, our method can improve the modeling of the extremely small values in the negative from the teacher and preserve the learning capacity for the positive. Furthermore, we test the impact of different temperature coefficients adjustments, which may conducted to further balance for knowledge transferring. We evaluate the proposed method on several computer vision tasks, including image classification and semantic segmentation. The evaluation results show that our method achieves an accuracy improvement of 1%~3% for lightweight students on both CIFAR-100 and ImageNet dataset, and a 4.55% improvement in mIoU for PSP-ResNet18 on the Cityscapes dataset. The experiments show that our method is a simple yet highly effective solution that can be smoothly applied to different knowledge distillation methods.</li>
</ul>

<h3>Title: How Far are App Secrets from Being Stolen? A Case Study on Android</h3>
<ul>
<li><strong>Authors: </strong>Lili Wei, Heqing Huang, Shing-Chi Cheung, Kevin Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07805">https://arxiv.org/abs/2501.07805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07805">https://arxiv.org/pdf/2501.07805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07805]] How Far are App Secrets from Being Stolen? A Case Study on Android(https://arxiv.org/abs/2501.07805)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Android apps can hold secret strings of themselves such as cloud service credentials or encryption keys. Leakage of such secret strings can induce unprecedented consequences like monetary losses or leakage of user private information. In practice, various security issues were reported because many apps failed to protect their secrets. However, little is known about the types, usages, exploitability, and consequences of app secret leakage issues. While a large body of literature has been devoted to studying user private information leakage, there is no systematic study characterizing app secret leakage issues. How far are Android app secrets from being stolen? To bridge this gap, we conducted the first systematic study to characterize app secret leakage issues in Android apps based on 575 potential app secrets sampled from 14,665 popular Android apps on Google Play. We summarized the common categories of leaked app secrets, assessed their security impacts and disclosed app bad practices in storing app secrets. We devised a text mining strategy using regular expressions and demonstrated that numerous app secrets can be easily stolen, even from the highly popular Android apps on Google. In a follow-up study, we harvested 3,711 distinct exploitable app secrets through automatic analysis. Our findings highlight the prevalence of this problem and call for greater attention to app secret protection.</li>
</ul>

<h3>Title: Learning Motion and Temporal Cues for Unsupervised Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yunzhi Zhuge, Hongyu Gu, Lu Zhang, Jinqing Qi, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07806">https://arxiv.org/abs/2501.07806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07806">https://arxiv.org/pdf/2501.07806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07806]] Learning Motion and Temporal Cues for Unsupervised Video Object Segmentation(https://arxiv.org/abs/2501.07806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenges in unsupervised video object segmentation (UVOS) by proposing an efficient algorithm, termed MTNet, which concurrently exploits motion and temporal cues. Unlike previous methods that focus solely on integrating appearance with motion or on modeling temporal relations, our method combines both aspects by integrating them within a unified framework. MTNet is devised by effectively merging appearance and motion features during the feature extraction process within encoders, promoting a more complementary representation. To capture the intricate long-range contextual dynamics and information embedded within videos, a temporal transformer module is introduced, facilitating efficacious inter-frame interactions throughout a video clip. Furthermore, we employ a cascade of decoders all feature levels across all feature levels to optimally exploit the derived features, aiming to generate increasingly precise segmentation masks. As a result, MTNet provides a strong and compact framework that explores both temporal and cross-modality knowledge to robustly localize and track the primary object accurately in various challenging scenarios efficiently. Extensive experiments across diverse benchmarks conclusively show that our method not only attains state-of-the-art performance in unsupervised video object segmentation but also delivers competitive results in video salient object detection. These findings highlight the method's robust versatility and its adeptness in adapting to a range of segmentation tasks. Source code is available on this https URL.</li>
</ul>

<h3>Title: AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sitong Gong, Yunzhi Zhuge, Lu Zhang, Yifan Wang, Pingping Zhang, Lijun Wang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07810">https://arxiv.org/abs/2501.07810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07810">https://arxiv.org/pdf/2501.07810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07810]] AVS-Mamba: Exploring Temporal and Multi-modal Mamba for Audio-Visual Segmentation(https://arxiv.org/abs/2501.07810)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The essence of audio-visual segmentation (AVS) lies in locating and delineating sound-emitting objects within a video stream. While Transformer-based methods have shown promise, their handling of long-range dependencies struggles due to quadratic computational costs, presenting a bottleneck in complex scenarios. To overcome this limitation and facilitate complex multi-modal comprehension with linear complexity, we introduce AVS-Mamba, a selective state space model to address the AVS task. Our framework incorporates two key components for video understanding and cross-modal learning: Temporal Mamba Block for sequential video processing and Vision-to-Audio Fusion Block for advanced audio-vision integration. Building on this, we develop the Multi-scale Temporal Encoder, aimed at enhancing the learning of visual features across scales, facilitating the perception of intra- and inter-frame information. To perform multi-modal fusion, we propose the Modality Aggregation Decoder, leveraging the Vision-to-Audio Fusion Block to integrate visual features into audio features across both frame and temporal levels. Further, we adopt the Contextual Integration Pyramid to perform audio-to-vision spatial-temporal context collaboration. Through these innovative contributions, our approach achieves new state-of-the-art results on the AVSBench-object and AVSBench-semantic datasets. Our source code and model weights are available at AVS-Mamba.</li>
</ul>

<h3>Title: A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kaustubh D. Dhole</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07818">https://arxiv.org/abs/2501.07818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07818">https://arxiv.org/pdf/2501.07818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07818]] A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models(https://arxiv.org/abs/2501.07818)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Among parameter-efficient fine-tuning methods, freezing has emerged as a popular strategy for speeding up training, reducing catastrophic forgetting, and improving downstream performance. We investigate the impact of freezing the decoder in a multi-task setup comprising diverse natural language tasks, aiming to reduce deployment overhead and enhance portability to novel tasks. Our experiments, conducted by fine-tuning both individual and multi-task setups on the AlexaTM model, reveal that freezing decoders is highly effective for tasks with natural language outputs and mitigates catastrophic forgetting in multilingual tasks. However, we find that pairing frozen decoders with a larger model can effectively maintain or even enhance performance in structured and QA tasks, making it a viable strategy for a broader range of task types.</li>
</ul>

<h3>Title: 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Xiong, Yunzhi Zhuge, Jiawen Zhu, Lu Zhang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07819">https://arxiv.org/abs/2501.07819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07819">https://arxiv.org/pdf/2501.07819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07819]] 3UR-LLM: An End-to-End Multimodal Large Language Model for 3D Scene Understanding(https://arxiv.org/abs/2501.07819)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal Large Language Models (MLLMs) exhibit impressive capabilities in 2D tasks, yet encounter challenges in discerning the spatial positions, interrelations, and causal logic in scenes when transitioning from 2D to 3D representations. We find that the limitations mainly lie in: i) the high annotation cost restricting the scale-up of volumes of 3D scene data, and ii) the lack of a straightforward and effective way to perceive 3D information which results in prolonged training durations and complicates the streamlined framework. To this end, we develop pipeline based on open-source 2D MLLMs and LLMs to generate high-quality 3D-text pairs and construct 3DS-160K , to enhance the pre-training process. Leveraging this high-quality pre-training data, we introduce the 3UR-LLM model, an end-to-end 3D MLLM designed for precise interpretation of 3D scenes, showcasing exceptional capability in navigating the complexities of the physical world. 3UR-LLM directly receives 3D point cloud as input and project 3D features fused with text instructions into a manageable set of tokens. Considering the computation burden derived from these hybrid tokens, we design a 3D compressor module to cohesively compress the 3D spatial cues and textual narrative. 3UR-LLM achieves promising performance with respect to the previous SOTAs, for instance, 3UR-LLM exceeds its counterparts by 7.1\% CIDEr on ScanQA, while utilizing fewer training resources. The code and model weights for 3UR-LLM and the 3DS-160K benchmark are available at 3UR-LLM.</li>
</ul>

<h3>Title: Real-time Verification and Refinement of Language Model Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Joonho Ko, Jinheon Baek, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07824">https://arxiv.org/abs/2501.07824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07824">https://arxiv.org/pdf/2501.07824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07824]] Real-time Verification and Refinement of Language Model Text Generation(https://arxiv.org/abs/2501.07824)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.</li>
</ul>

<h3>Title: Prediction Interval Construction Method for Electricity Prices</h3>
<ul>
<li><strong>Authors: </strong>Xin Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07827">https://arxiv.org/abs/2501.07827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07827">https://arxiv.org/pdf/2501.07827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07827]] Prediction Interval Construction Method for Electricity Prices(https://arxiv.org/abs/2501.07827)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate prediction of electricity prices plays an essential role in the electricity market. To reflect the uncertainty of electricity prices, price intervals are predicted. This paper proposes a novel prediction interval construction method. A conditional generative adversarial network is first presented to generate electricity price scenarios, with which the prediction intervals can be constructed. Then, different generated scenarios are stacked to obtain the probability densities, which can be applied to accurately reflect the uncertainty of electricity prices. Furthermore, a reinforced prediction mechanism based on the volatility level of weather factors is introduced to address the spikes or volatile prices. A case study is conducted to verify the effectiveness of the proposed novel prediction interval construction method. The method can also provide the probability density of each price scenario within the prediction interval and has the superiority to address the volatile prices and price spikes with a reinforced prediction mechanism.</li>
</ul>

<h3>Title: Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Han, Yaochen Xie, Hui Liu, Xianfeng Tang, Sreyashi Nag, William Headden, Hui Liu, Yang Li, Chen Luo, Shuiwang Ji, Qi He, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07845">https://arxiv.org/abs/2501.07845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07845">https://arxiv.org/pdf/2501.07845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07845]] Reasoning with Graphs: Structuring Implicit Knowledge to Enhance LLMs Reasoning(https://arxiv.org/abs/2501.07845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable success across a wide range of tasks; however, they still encounter challenges in reasoning tasks that require understanding and inferring relationships between distinct pieces of information within text sequences. This challenge is particularly pronounced in tasks involving multi-step processes, such as logical reasoning and multi-hop question answering, where understanding implicit relationships between entities and leveraging multi-hop connections in the given context are crucial. Graphs, as fundamental data structures, explicitly represent pairwise relationships between entities, thereby offering the potential to enhance LLMs' reasoning capabilities. External graphs have proven effective in supporting LLMs across multiple tasks. However, in many reasoning tasks, no pre-existing graph structure is provided. Can we structure implicit knowledge derived from context into graphs to assist LLMs in reasoning? In this paper, we propose Reasoning with Graphs (RwG) by first constructing explicit graphs from the context and then leveraging these graphs to enhance LLM reasoning performance on reasoning tasks. Extensive experiments demonstrate the effectiveness of the proposed method in improving both logical reasoning and multi-hop question answering tasks.</li>
</ul>

<h3>Title: Optimizing Language Models for Grammatical Acceptability: A Comparative Study of Fine-Tuning Techniques</h3>
<ul>
<li><strong>Authors: </strong>Shobhit Ratan, Farley Knight, Ghada Jerfel, Sze Chung Ho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07853">https://arxiv.org/abs/2501.07853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07853">https://arxiv.org/pdf/2501.07853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07853]] Optimizing Language Models for Grammatical Acceptability: A Comparative Study of Fine-Tuning Techniques(https://arxiv.org/abs/2501.07853)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This study explores the fine-tuning (FT) of the Open Pre-trained Transformer (OPT-125M) for grammatical acceptability tasks using the CoLA dataset. By comparing Vanilla-Fine-Tuning (VFT), Pattern-Based-Fine-Tuning (PBFT), and Parameter-Efficient Fine-Tuning techniques (PEFT) like Low-Rank Adaptation (LoRA), we demonstrate significant improvements in computational efficiency while maintaining high accuracy. Our experiments reveal that while VFT achieves the highest accuracy (81.2%), LoRA enhancing FT by reducing memory usage and iteration time by more than 50%, and increases accuracy in PBFT case. Context Distillation (CD), though computationally efficient, underperformed with accuracy around 31%. Our findings contribute to democratizing access to large language models (LLM) by reducing computational barriers.</li>
</ul>

<h3>Title: State-of-the-Art Transformer Models for Image Super-Resolution: Techniques, Challenges, and Applications</h3>
<ul>
<li><strong>Authors: </strong>Debasish Dutta, Deepjyoti Chetia, Neeharika Sonowal, Sanjib Kr Kalita</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07855">https://arxiv.org/abs/2501.07855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07855">https://arxiv.org/pdf/2501.07855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07855]] State-of-the-Art Transformer Models for Image Super-Resolution: Techniques, Challenges, and Applications(https://arxiv.org/abs/2501.07855)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image Super-Resolution (SR) aims to recover a high-resolution image from its low-resolution counterpart, which has been affected by a specific degradation process. This is achieved by enhancing detail and visual quality. Recent advancements in transformer-based methods have remolded image super-resolution by enabling high-quality reconstructions surpassing previous deep-learning approaches like CNN and GAN-based. This effectively addresses the limitations of previous methods, such as limited receptive fields, poor global context capture, and challenges in high-frequency detail recovery. Additionally, the paper reviews recent trends and advancements in transformer-based SR models, exploring various innovative techniques and architectures that combine transformers with traditional networks to balance global and local contexts. These neoteric methods are critically analyzed, revealing promising yet unexplored gaps and potential directions for future research. Several visualizations of models and techniques are included to foster a holistic understanding of recent trends. This work seeks to offer a structured roadmap for researchers at the forefront of deep learning, specifically exploring the impact of transformers on super-resolution techniques.</li>
</ul>

<h3>Title: ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding</h3>
<ul>
<li><strong>Authors: </strong>Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, Han Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07861">https://arxiv.org/abs/2501.07861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07861">https://arxiv.org/pdf/2501.07861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07861]] ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding(https://arxiv.org/abs/2501.07861)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) hold promise in knowledge-intensive tasks but face limitations in complex multi-step reasoning. While recent methods have integrated RAG with chain-of-thought reasoning or test-time search using Process Reward Models (PRMs), these approaches encounter challenges such as a lack of explanations, bias in PRM training data, early-step bias in PRM scores, and insufficient post-training optimization of reasoning potential. To address these issues, we propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding (ReARTeR), a framework that enhances RAG systems' reasoning capabilities through post-training and test-time scaling. At test time, ReARTeR introduces Trustworthy Process Rewarding via a Process Reward Model for accurate scalar scoring and a Process Explanation Model (PEM) for generating natural language explanations, enabling step refinement. During post-training, it utilizes Monte Carlo Tree Search guided by Trustworthy Process Rewarding to collect high-quality step-level preference data, optimized through Iterative Preference Optimization. ReARTeR addresses three core challenges: (1) misalignment between PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM training data, mitigated by balanced annotation methods and stronger annotations for challenging examples; and (3) early-step bias in PRM, resolved through a temporal-difference-based look-ahead search strategy. Experimental results on multi-step reasoning benchmarks demonstrate significant improvements, underscoring ReARTeR's potential to advance the reasoning capabilities of RAG systems.</li>
</ul>

<h3>Title: PUFBind: PUF-Enabled Lightweight Program Binary Authentication for FPGA-based Embedded Systems</h3>
<ul>
<li><strong>Authors: </strong>Sneha Swaroopa, Venkata Sreekanth Balijabudda, Rajat Subhra Chakraborty, Indrajit Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07868">https://arxiv.org/abs/2501.07868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07868">https://arxiv.org/pdf/2501.07868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07868]] PUFBind: PUF-Enabled Lightweight Program Binary Authentication for FPGA-based Embedded Systems(https://arxiv.org/abs/2501.07868)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Field Programmable Gate Array (FPGA)-based embedded systems have become mainstream in the last decade, often in security-sensitive applications. However, even with an authenticated hardware platform, compromised software can severely jeopardize the overall system security, making hardware protection insufficient if the software itself is malicious. In this paper, we propose a novel low-overhead hardware-software co-design solution that utilizes Physical Unclonable Functions (PUFs) to ensure the authenticity of program binaries for microprocessors/microcontrollers mapped on the FPGA. Our technique binds a program binary to a specific target FPGA through a PUF signature, performs runtime authentication for the program binary, and allows execution of the binary only after successful authentication. The proposed scheme is platform-agnostic and capable of operating in a "bare metal'' mode (no system software requirement) for maximum flexibility. Our scheme also does not require any modification of the original hardware design or program binary. We demonstrate a successful prototype implementation using the open-source PicoBlaze microcontroller on AMD/Xilinx FPGA, comparing its hardware resource footprint and performance with other existing solutions of a similar nature.</li>
</ul>

<h3>Title: Make-A-Character 2: Animatable 3D Character Generation From a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Lin Liu, Yutong Wang, Jiahao Chen, Jianfang Li, Tangli Xue, Longlong Li, Jianqiang Ren, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07870">https://arxiv.org/abs/2501.07870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07870">https://arxiv.org/pdf/2501.07870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07870]] Make-A-Character 2: Animatable 3D Character Generation From a Single Image(https://arxiv.org/abs/2501.07870)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This report introduces Make-A-Character 2, an advanced system for generating high-quality 3D characters from single portrait photographs, ideal for game development and digital human applications. Make-A-Character 2 builds upon its predecessor by incorporating several significant improvements for image-based head generation. We utilize the IC-Light method to correct non-ideal illumination in input photos and apply neural network-based color correction to harmonize skin tones between the photos and game engine renders. We also employ the Hierarchical Representation Network to capture high-frequency facial structures and conduct adaptive skeleton calibration for accurate and expressive facial animations. The entire image-to-3D-character generation process takes less than 2 minutes. Furthermore, we leverage transformer architecture to generate co-speech facial and gesture actions, enabling real-time conversation with the generated character. These technologies have been integrated into our conversational AI avatar products.</li>
</ul>

<h3>Title: MD-Syn: Synergistic drug combination prediction based on the multidimensional feature fusion method and attention mechanisms</h3>
<ul>
<li><strong>Authors: </strong>XinXin Ge, Yi-Ting Lee, Shan-Ju Yeh</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07884">https://arxiv.org/abs/2501.07884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07884">https://arxiv.org/pdf/2501.07884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07884]] MD-Syn: Synergistic drug combination prediction based on the multidimensional feature fusion method and attention mechanisms(https://arxiv.org/abs/2501.07884)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Drug combination therapies have shown promising therapeutic efficacy in complex diseases and have demonstrated the potential to reduce drug resistance. However, the huge number of possible drug combinations makes it difficult to screen them all in traditional experiments. In this study, we proposed MD-Syn, a computational framework, which is based on the multidimensional feature fusion method and multi-head attention mechanisms. Given drug pair-cell line triplets, MD-Syn considers one-dimensional and two-dimensional feature spaces simultaneously. It consists of a one-dimensional feature embedding module (1D-FEM), a two-dimensional feature embedding module (2D-FEM), and a deep neural network-based classifier for synergistic drug combination prediction. MD-Syn achieved the AUROC of 0.919 in 5-fold cross-validation, outperforming the state-of-the-art methods. Further, MD-Syn showed comparable results over two independent datasets. In addition, the multi-head attention mechanisms not only learn embeddings from different feature aspects but also focus on essential interactive feature elements, improving the interpretability of MD-Syn. In summary, MD-Syn is an interpretable framework to prioritize synergistic drug combination pairs with chemicals and cancer cell line gene expression profiles. To facilitate broader community access to this model, we have developed a web portal (this https URL) that enables customized predictions of drug combination synergy effects based on user-specified compounds.</li>
</ul>

<h3>Title: Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal Modeling</h3>
<ul>
<li><strong>Authors: </strong>Min Sik Byun, Wendy Wan Yee Hui, Wai Kwong Lau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07885">https://arxiv.org/abs/2501.07885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07885">https://arxiv.org/pdf/2501.07885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07885]] Mitigating Algorithmic Bias in Multiclass CNN Classifications Using Causal Modeling(https://arxiv.org/abs/2501.07885)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This study describes a procedure for applying causal modeling to detect and mitigate algorithmic bias in a multiclass classification problem. The dataset was derived from the FairFace dataset, supplemented with emotional labels generated by the DeepFace pre-trained model. A custom Convolutional Neural Network (CNN) was developed, consisting of four convolutional blocks, followed by fully connected layers and dropout layers to mitigate overfitting. Gender bias was identified in the CNN model's classifications: Females were more likely to be classified as "happy" or "sad," while males were more likely to be classified as "neutral." To address this, the one-vs-all (OvA) technique was applied. A causal model was constructed for each emotion class to adjust the CNN model's predicted class probabilities. The adjusted probabilities for the various classes were then aggregated by selecting the class with the highest probability. The resulting debiased classifications demonstrated enhanced gender fairness across all classes, with negligible impact--or even a slight improvement--on overall accuracy. This study highlights that algorithmic fairness and accuracy are not necessarily trade-offs. All data and code for this study are publicly available for download.</li>
</ul>

<h3>Title: Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07888">https://arxiv.org/abs/2501.07888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07888">https://arxiv.org/pdf/2501.07888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07888]] Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding(https://arxiv.org/abs/2501.07888)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8\% over GPT-4o and 5.8\% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\% performance advantage over GPT-4o and +24.9\% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model.</li>
</ul>

<h3>Title: Demographic Variability in Face Image Quality Measures</h3>
<ul>
<li><strong>Authors: </strong>Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07898">https://arxiv.org/abs/2501.07898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07898">https://arxiv.org/pdf/2501.07898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07898]] Demographic Variability in Face Image Quality Measures(https://arxiv.org/abs/2501.07898)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Face image quality assessment (FIQA) algorithms are being integrated into online identity management applications. These applications allow users to upload a face image as part of their document issuance process, where the image is then run through a quality assessment process to make sure it meets the quality and compliance requirements. Concerns about demographic bias have been raised about biometric systems, given the societal implications this may cause. It is therefore important that demographic variability in FIQA algorithms is assessed such that mitigation measures can be created. In this work, we study the demographic variability of all face image quality measures included in the ISO/IEC 29794-5 international standard across three demographic variables: age, gender, and skin tone. The results are rather promising and show no clear bias toward any specific demographic group for most measures. Only two quality measures are found to have considerable variations in their outcomes for different groups on the skin tone variable.</li>
</ul>

<h3>Title: VENOM: Text-driven Unrestricted Adversarial Example Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hui Kuurila-Zhang, Haoyu Chen, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07922">https://arxiv.org/abs/2501.07922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07922">https://arxiv.org/pdf/2501.07922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07922]] VENOM: Text-driven Unrestricted Adversarial Example Generation with Diffusion Models(https://arxiv.org/abs/2501.07922)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Adversarial attacks have proven effective in deceiving machine learning models by subtly altering input images, motivating extensive research in recent years. Traditional methods constrain perturbations within $l_p$-norm bounds, but advancements in Unrestricted Adversarial Examples (UAEs) allow for more complex, generative-model-based manipulations. Diffusion models now lead UAE generation due to superior stability and image quality over GANs. However, existing diffusion-based UAE methods are limited to using reference images and face challenges in generating Natural Adversarial Examples (NAEs) directly from random noise, often producing uncontrolled or distorted outputs. In this work, we introduce VENOM, the first text-driven framework for high-quality unrestricted adversarial examples generation through diffusion models. VENOM unifies image content generation and adversarial synthesis into a single reverse diffusion process, enabling high-fidelity adversarial examples without sacrificing attack success rate (ASR). To stabilize this process, we incorporate an adaptive adversarial guidance strategy with momentum, ensuring that the generated adversarial examples $x^*$ align with the distribution $p(x)$ of natural images. Extensive experiments demonstrate that VENOM achieves superior ASR and image quality compared to prior methods, marking a significant advancement in adversarial example generation and providing insights into model vulnerabilities for improved defense development.</li>
</ul>

<h3>Title: Gandalf the Red: Adaptive Security for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Niklas Pfister, V√°clav Volhejn, Manuel Knott, Santiago Arias, Julia Bazi≈Ñska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Dami√°n Pascual-Ortiz, Jakub Podolak, Adri√† Romero-L√≥pez, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07927">https://arxiv.org/abs/2501.07927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07927">https://arxiv.org/pdf/2501.07927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07927]] Gandalf the Red: Adaptive Security for LLMs(https://arxiv.org/abs/2501.07927)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and rigorously expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack datasets. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications. Code is available at \href{this https URL}{\texttt{this https URL}}.</li>
</ul>

<h3>Title: Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral Representation</h3>
<ul>
<li><strong>Authors: </strong>Chia-Ming Lee, Yu-Fan Lin, Li-Wei Kang, Chih-Chung Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07953">https://arxiv.org/abs/2501.07953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07953">https://arxiv.org/pdf/2501.07953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07953]] Robust Hyperspectral Image Panshapring via Sparse Spatial-Spectral Representation(https://arxiv.org/abs/2501.07953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>High-resolution hyperspectral imaging plays a crucial role in various remote sensing applications, yet its acquisition often faces fundamental limitations due to hardware constraints. This paper introduces S$^{3}$RNet, a novel framework for hyperspectral image pansharpening that effectively combines low-resolution hyperspectral images (LRHSI) with high-resolution multispectral images (HRMSI) through sparse spatial-spectral representation. The core of S$^{3}$RNet is the Multi-Branch Fusion Network (MBFN), which employs parallel branches to capture complementary features at different spatial and spectral scales. Unlike traditional approaches that treat all features equally, our Spatial-Spectral Attention Weight Block (SSAWB) dynamically adjusts feature weights to maintain sparse representation while suppressing noise and redundancy. To enhance feature propagation, we incorporate the Dense Feature Aggregation Block (DFAB), which efficiently aggregates inputted features through dense connectivity patterns. This integrated design enables S$^{3}$RNet to selectively emphasize the most informative features from differnt scale while maintaining computational efficiency. Comprehensive experiments demonstrate that S$^{3}$RNet achieves state-of-the-art performance across multiple evaluation metrics, showing particular strength in maintaining high reconstruction quality even under challenging noise conditions. The code will be made publicly available.</li>
</ul>

<h3>Title: Differentially Private Distance Query with Asymmetric Noise</h3>
<ul>
<li><strong>Authors: </strong>Weihong Sheng, Jiajun Chen, Chunqiang Hu, Bin Cai, Meng Han, Jiguo Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07955">https://arxiv.org/abs/2501.07955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07955">https://arxiv.org/pdf/2501.07955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07955]] Differentially Private Distance Query with Asymmetric Noise(https://arxiv.org/abs/2501.07955)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>With the growth of online social services, social information graphs are becoming increasingly complex. Privacy issues related to analyzing or publishing on social graphs are also becoming increasingly serious. Since the shortest paths play an important role in graphs, privately publishing the shortest paths or distances has attracted the attention of researchers. Differential privacy (DP) is an excellent standard for preserving privacy. However, existing works to answer the distance query with the guarantee of DP were almost based on the weight private graph assumption, not on the paths themselves. In this paper, we consider edges as privacy and propose distance publishing mechanisms based on edge DP. To address the issue of utility damage caused by large global sensitivities, we revisit studies related to asymmetric neighborhoods in DP with the observation that the distance query is monotonic in asymmetric neighborhoods. We formally give the definition of asymmetric neighborhoods and propose Individual Asymmetric Differential Privacy with higher privacy guarantees in combination with smooth sensitivity. Then, we introduce two methods to efficiently compute the smooth sensitivity of distance queries in asymmetric neighborhoods. Finally, we validate our scheme using both real-world and synthetic datasets, which can reduce the error to $0.0862$.</li>
</ul>

<h3>Title: SkipClick: Combining Quick Responses and Low-Level Features for Interactive Segmentation in Winter Sports Contexts</h3>
<ul>
<li><strong>Authors: </strong>Robin Sch√∂n, Julian Lorenz, Daniel Kienzle, Rainer Lienhart</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07960">https://arxiv.org/abs/2501.07960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07960">https://arxiv.org/pdf/2501.07960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07960]] SkipClick: Combining Quick Responses and Low-Level Features for Interactive Segmentation in Winter Sports Contexts(https://arxiv.org/abs/2501.07960)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel architecture for interactive segmentation in winter sports contexts. The field of interactive segmentation deals with the prediction of high-quality segmentation masks by informing the network about the objects position with the help of user guidance. In our case the guidance consists of click prompts. For this task, we first present a baseline architecture which is specifically geared towards quickly responding after each click. Afterwards, we motivate and describe a number of architectural modifications which improve the performance when tasked with segmenting winter sports equipment on the WSESeg dataset. With regards to the average NoC@85 metric on the WSESeg classes, we outperform SAM and HQ-SAM by 2.336 and 7.946 clicks, respectively. When applied to the HQSeg-44k dataset, our system delivers state-of-the-art results with a NoC@90 of 6.00 and NoC@95 of 9.89. In addition to that, we test our model on a novel dataset containing masks for humans during skiing.</li>
</ul>

<h3>Title: Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression Perception and Contextual Awareness</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Zhao, Boyuan Sun, Xiang Chen, Xihan Wei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07978">https://arxiv.org/abs/2501.07978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07978">https://arxiv.org/pdf/2501.07978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07978]] Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression Perception and Contextual Awareness(https://arxiv.org/abs/2501.07978)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Facial expression captioning has found widespread application across various domains. Recently, the emergence of video Multimodal Large Language Models (MLLMs) has shown promise in general video understanding tasks. However, describing facial expressions within videos poses two major challenges for these models: (1) the lack of adequate datasets and benchmarks, and (2) the limited visual token capacity of video MLLMs. To address these issues, this paper introduces a new instruction-following dataset tailored for dynamic facial expression caption. The dataset comprises 5,033 high-quality video clips annotated manually, containing over 700,000 tokens. Its purpose is to improve the capability of video MLLMs to discern subtle facial nuances. Furthermore, we propose FaceTrack-MM, which leverages a limited number of tokens to encode the main character's face. This model demonstrates superior performance in tracking faces and focusing on the facial expressions of the main characters, even in intricate multi-person scenarios. Additionally, we introduce a novel evaluation metric combining event extraction, relation classification, and the longest common subsequence (LCS) algorithm to assess the content consistency and temporal sequence consistency of generated text. Moreover, we present FEC-Bench, a benchmark designed to assess the performance of existing video MLLMs in this specific task. All data and source code will be made publicly available.</li>
</ul>

<h3>Title: V-Trans4Style: Visual Transition Recommendation for Video Production Style Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Pooja Guhan, Tsung-Wei Huang, Guan-Ming Su, Subhadra Gopalakrishnan, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07983">https://arxiv.org/abs/2501.07983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07983">https://arxiv.org/pdf/2501.07983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07983]] V-Trans4Style: Visual Transition Recommendation for Video Production Style Adaptation(https://arxiv.org/abs/2501.07983)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce V-Trans4Style, an innovative algorithm tailored for dynamic video content editing needs. It is designed to adapt videos to different production styles like documentaries, dramas, feature films, or a specific YouTube channel's video-making technique. Our algorithm recommends optimal visual transitions to help achieve this flexibility using a more bottom-up approach. We first employ a transformer-based encoder-decoder network to learn recommending temporally consistent and visually seamless sequences of visual transitions using only the input videos. We then introduce a style conditioning module that leverages this model to iteratively adjust the visual transitions obtained from the decoder through activation maximization. We demonstrate the efficacy of our method through experiments conducted on our newly introduced AutoTransition++ dataset. It is a 6k video version of AutoTransition Dataset that additionally categorizes its videos into different production style categories. Our encoder-decoder model outperforms the state-of-the-art transition recommendation method, achieving improvements of 10% to 80% in Recall@K and mean rank values over baseline. Our style conditioning module results in visual transitions that improve the capture of the desired video production style characteristics by an average of around 12% in comparison to other methods when measured with similarity metrics. We hope that our work serves as a foundation for exploring and understanding video production styles further.</li>
</ul>

<h3>Title: Threshold Attention Network for Semantic Segmentation of Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Wei Long, Yongjun Zhang, Zhongwei Cui, Yujie Xu, Xuexue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07984">https://arxiv.org/abs/2501.07984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07984">https://arxiv.org/pdf/2501.07984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07984]] Threshold Attention Network for Semantic Segmentation of Remote Sensing Images(https://arxiv.org/abs/2501.07984)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation of remote sensing images is essential for various applications, including vegetation monitoring, disaster management, and urban planning. Previous studies have demonstrated that the self-attention mechanism (SA) is an effective approach for designing segmentation networks that can capture long-range pixel dependencies. SA enables the network to model the global dependencies between the input features, resulting in improved segmentation outcomes. However, the high density of attentional feature maps used in this mechanism causes exponential increases in computational complexity. Additionally, it introduces redundant information that negatively impacts the feature representation. Inspired by traditional threshold segmentation algorithms, we propose a novel threshold attention mechanism (TAM). This mechanism significantly reduces computational effort while also better modeling the correlation between different regions of the feature map. Based on TAM, we present a threshold attention network (TANet) for semantic segmentation. TANet consists of an attentional feature enhancement module (AFEM) for global feature enhancement of shallow features and a threshold attention pyramid pooling module (TAPP) for acquiring feature information at different scales for deep features. We have conducted extensive experiments on the ISPRS Vaihingen and Potsdam datasets. The results demonstrate the validity and superiority of our proposed TANet compared to the most state-of-the-art models.</li>
</ul>

<h3>Title: GAC-Net_Geometric and attention-based Network for Depth Completion</h3>
<ul>
<li><strong>Authors: </strong>Kuang Zhu, Xingli Gan, Min Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.07988">https://arxiv.org/abs/2501.07988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.07988">https://arxiv.org/pdf/2501.07988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.07988]] GAC-Net_Geometric and attention-based Network for Depth Completion(https://arxiv.org/abs/2501.07988)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth completion is a key task in autonomous driving, aiming to complete sparse LiDAR depth measurements into high-quality dense depth maps through image guidance. However, existing methods usually treat depth maps as an additional channel of color images, or directly perform convolution on sparse data, failing to fully exploit the 3D geometric information in depth maps, especially with limited performance in complex boundaries and sparse areas. To address these issues, this paper proposes a depth completion network combining channel attention mechanism and 3D global feature perception (CGA-Net). The main innovations include: 1) Utilizing PointNet++ to extract global 3D geometric features from sparse depth maps, enhancing the scene perception ability of low-line LiDAR data; 2) Designing a channel-attention-based multimodal feature fusion module to efficiently integrate sparse depth, RGB images, and 3D geometric features; 3) Combining residual learning with CSPN++ to optimize the depth refinement stage, further improving the completion quality in edge areas and complex scenes. Experiments on the KITTI depth completion dataset show that CGA-Net can significantly improve the prediction accuracy of dense depth maps, achieving a new state-of-the-art (SOTA), and demonstrating strong robustness to sparse and complex scenes.</li>
</ul>

<h3>Title: Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based Model Poisoning</h3>
<ul>
<li><strong>Authors: </strong>Marios Aristodemou, Xiaolan Liu, Yuan Wang, Konstantinos G. Kyriakopoulos, Sangarapillai Lambotharan, Qingsong Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08002">https://arxiv.org/abs/2501.08002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08002">https://arxiv.org/pdf/2501.08002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08002]] Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based Model Poisoning(https://arxiv.org/abs/2501.08002)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate, generative</a></li>
<li><strong>Abstract: </strong>As we transition from Narrow Artificial Intelligence towards Artificial Super Intelligence, users are increasingly concerned about their privacy and the trustworthiness of machine learning (ML) technology. A common denominator for the metrics of trustworthiness is the quantification of uncertainty inherent in DL algorithms, and specifically in the model parameters, input data, and model predictions. One of the common approaches to address privacy-related issues in DL is to adopt distributed learning such as federated learning (FL), where private raw data is not shared among users. Despite the privacy-preserving mechanisms in FL, it still faces challenges in trustworthiness. Specifically, the malicious users, during training, can systematically create malicious model parameters to compromise the models predictive and generative capabilities, resulting in high uncertainty about their reliability. To demonstrate malicious behaviour, we propose a novel model poisoning attack method named Delphi which aims to maximise the uncertainty of the global model output. We achieve this by taking advantage of the relationship between the uncertainty and the model parameters of the first hidden layer of the local model. Delphi employs two types of optimisation , Bayesian Optimisation and Least Squares Trust Region, to search for the optimal poisoned model parameters, named as Delphi-BO and Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise the distance of the predictive probability distribution towards an uncertain distribution of model output. Furthermore, we establish a mathematical proof for the attack effectiveness demonstrated in FL. Numerical results demonstrate that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR highlighting vulnerability of FL systems to model poisoning attacks.</li>
</ul>

<h3>Title: TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yao Liang, Yuwei Wang, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08008">https://arxiv.org/abs/2501.08008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08008">https://arxiv.org/pdf/2501.08008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08008]] TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2501.08008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The fine-tuning of Large Language Models (LLMs) is pivotal for achieving optimal performance across diverse downstream tasks. However, while full fine-tuning delivers superior results, it entails significant computational and resource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, address these challenges by reducing the number of trainable parameters, but they often struggle with rank adjustment efficiency and task-specific adaptability. We propose Triangular Adaptive Low-Rank Adaptation (TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles, which dynamically optimizes the allocation of trainable parameters. TriAdaptLoRA introduces three key innovations: 1) a triangular split of transformation matrices into lower and upper triangular components to maximize parameter utilization, 2) a parameter importance metric based on normalized Frobenius norms for efficient adaptation, and 3) an adaptive rank-growth strategy governed by dynamic thresholds, allowing flexible parameter allocation across training steps. Experiments conducted on a variety of natural language understanding and generation tasks demonstrate that TriAdaptLoRA consistently outperforms existing PEFT methods. It achieves superior performance, enhanced stability, and reduced computational overhead, particularly under linear threshold-driven rank growth. These results highlight its efficacy as a scalable and resource-efficient solution for fine-tuning LLMs.</li>
</ul>

<h3>Title: Entropy Mixing Networks: Enhancing Pseudo-Random Number Generators with Lightweight Dynamic Entropy Injection</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Aly Bouke, Omar Imhemmed Alramli, Azizol Abdullah, Nur Izura Udzir, Normalia Samian, Mohamed Othman, Zurina Mohd Hanapi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08031">https://arxiv.org/abs/2501.08031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08031">https://arxiv.org/pdf/2501.08031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08031]] Entropy Mixing Networks: Enhancing Pseudo-Random Number Generators with Lightweight Dynamic Entropy Injection(https://arxiv.org/abs/2501.08031)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Random number generation plays a vital role in cryptographic systems and computational applications, where uniformity, unpredictability, and robustness are essential. This paper presents the Entropy Mixing Network (EMN), a novel hybrid random number generator designed to enhance randomness quality by combining deterministic pseudo-random generation with periodic entropy injection. To evaluate its effectiveness, we propose a comprehensive assessment framework that integrates statistical tests, advanced metrics, and visual analyses, providing a holistic view of randomness quality, predictability, and computational efficiency. The results demonstrate that EMN outperforms Python's SystemRandom and MersenneTwister in critical metrics, achieving the highest Chi-squared p-value (0.9430), entropy (7.9840), and lowest predictability (-0.0286). These improvements come with a trade-off in computational performance, as EMN incurs a higher generation time (0.2602 seconds). Despite this, its superior randomness quality makes it particularly suitable for cryptographic applications where security is prioritized over speed.</li>
</ul>

<h3>Title: READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data</h3>
<ul>
<li><strong>Authors: </strong>Rohit Sharma, Shanu Kumar, Avinash Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08035">https://arxiv.org/abs/2501.08035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08035">https://arxiv.org/pdf/2501.08035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08035]] READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data(https://arxiv.org/abs/2501.08035)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pre-trained transformer models such as BERT have shown massive gains across many text classification tasks. However, these models usually need enormous labeled data to achieve impressive performances. Obtaining labeled data is often expensive and time-consuming, whereas collecting unlabeled data using some heuristics is relatively much cheaper for any task. Therefore, this paper proposes a method that encapsulates reinforcement learning-based text generation and semi-supervised adversarial learning approaches in a novel way to improve the model's performance. Our method READ, Reinforcement-based Adversarial learning, utilizes an unlabeled dataset to generate diverse synthetic text through reinforcement learning, improving the model's generalization capability using adversarial learning. Our experimental results show that READ outperforms the existing state-of-art methods on multiple datasets.</li>
</ul>

<h3>Title: Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I Networks</h3>
<ul>
<li><strong>Authors: </strong>Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08037">https://arxiv.org/abs/2501.08037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08037">https://arxiv.org/pdf/2501.08037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08037]] Enhanced SPS Velocity-adaptive Scheme: Access Fariness in 5G NR V2I Networks(https://arxiv.org/abs/2501.08037)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Vehicle-to-Infrastructure (V2I) technology enables information exchange between vehicles and road infrastructure. Specifically, when a vehicle approaches a roadside unit (RSU), it can exchange information with the RSU to obtain accurate data that assists in driving. With the release of the 3rd Generation Partnership Project (3GPP) Release 16, which includes the 5G New Radio (NR) Vehicle-to-Everything (V2X) standards, vehicles typically adopt mode-2 communication using sensing-based semi-persistent scheduling (SPS) for resource allocation. In this approach, vehicles identify candidate resources within a selection window and exclude ineligible resources based on information from a sensing window. However, vehicles often drive at different speeds, resulting in varying amounts of data transmission with RSUs as they pass by, which leads to unfair access. Therefore, it is essential to design an access scheme that accounts for different vehicle speeds to achieve fair access across the network. This paper formulates an optimization problem for vehicular networks and proposes a multi-objective optimization scheme to address it by adjusting the selection window in the SPS mechanism of 5G NR V2I mode-2. Simulation results demonstrate the effectiveness of the proposed scheme</li>
</ul>

<h3>Title: Robust Low-Light Human Pose Estimation through Illumination-Texture Modulation</h3>
<ul>
<li><strong>Authors: </strong>Feng Zhang, Ze Li, Xiatian Zhu, Lei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08038">https://arxiv.org/abs/2501.08038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08038">https://arxiv.org/pdf/2501.08038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08038]] Robust Low-Light Human Pose Estimation through Illumination-Texture Modulation(https://arxiv.org/abs/2501.08038)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As critical visual details become obscured, the low visibility and high ISO noise in extremely low-light images pose a significant challenge to human pose estimation. Current methods fail to provide high-quality representations due to reliance on pixel-level enhancements that compromise semantics and the inability to effectively handle extreme low-light conditions for robust feature learning. In this work, we propose a frequency-based framework for low-light human pose estimation, rooted in the "divide-and-conquer" principle. Instead of uniformly enhancing the entire image, our method focuses on task-relevant information. By applying dynamic illumination correction to the low-frequency components and low-rank denoising to the high-frequency components, we effectively enhance both the semantic and texture information essential for accurate pose estimation. As a result, this targeted enhancement method results in robust, high-quality representations, significantly improving pose estimation performance. Extensive experiments demonstrating its superiority over state-of-the-art methods in various challenging low-light scenarios.</li>
</ul>

<h3>Title: Exploring visual language models as a powerful tool in the diagnosis of Ewing Sarcoma</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Pastor-Naranjo, Pablo Meseguer, Roc√≠o del Amor, Jose Antonio Lopez-Guerrero, Samuel Navarro, Katia Scotlandi, Antonio Llombart-Bosch, Isidro Machado, Valery Naranjo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08042">https://arxiv.org/abs/2501.08042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08042">https://arxiv.org/pdf/2501.08042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08042]] Exploring visual language models as a powerful tool in the diagnosis of Ewing Sarcoma(https://arxiv.org/abs/2501.08042)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Ewing's sarcoma (ES), characterized by a high density of small round blue cells without structural organization, presents a significant health concern, particularly among adolescents aged 10 to 19. Artificial intelligence-based systems for automated analysis of histopathological images are promising to contribute to an accurate diagnosis of ES. In this context, this study explores the feature extraction ability of different pre-training strategies for distinguishing ES from other soft tissue or bone sarcomas with similar morphology in digitized tissue microarrays for the first time, as far as we know. Vision-language supervision (VLS) is compared to fully-supervised ImageNet pre-training within a multiple instance learning paradigm. Our findings indicate a substantial improvement in diagnostic accuracy with the adaption of VLS using an in-domain dataset. Notably, these models not only enhance the accuracy of predicted classes but also drastically reduce the number of trainable parameters and computational costs.</li>
</ul>

<h3>Title: UFGraphFR: An attempt at a federated recommendation system based on user text characteristics</h3>
<ul>
<li><strong>Authors: </strong>Xudong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08044">https://arxiv.org/abs/2501.08044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08044">https://arxiv.org/pdf/2501.08044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08044]] UFGraphFR: An attempt at a federated recommendation system based on user text characteristics(https://arxiv.org/abs/2501.08044)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate, transformer</a></li>
<li><strong>Abstract: </strong>Federated learning has become an important research area in 'private computing' due to the 'useable invisibility' of data during training. Inspired by Federated learning, the federated recommendation system has gradually become a new recommendation service architecture that can protect users' privacy. The use of user diagrams to enhance federated recommendations is a promising topic. How to use user diagrams to enhance federated recommendations is a promising research topic. However, it's a great challenge to construct a user diagram without compromising privacy in a federated learning scenario. Inspired by the simple idea that similar users often have the same attribute characteristics, we propose a personalized federated recommendation algorithm based on the user relationship graph constructed by the user text characteristics(Graph Federation Recommendation System based on User Text description Features, UFGraphFR). The method uses the embedding layer weight of the user's text feature description to construct the user relationship graph. It introduces the Transformer mechanism to capture the sequence modeling of the user's historical interaction sequence. Without access to user history interactions and specific user attributes, the federal learning privacy protection of data 'useable invisibility' is embodied. Preliminary experiments on some benchmark datasets demonstrate the superior performance of UFGraphFR. Our experiments show that this model can protect user privacy to some extent without affecting the performance of the recommendation system. The code will be easily available on this https URL.</li>
</ul>

<h3>Title: Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT</h3>
<ul>
<li><strong>Authors: </strong>Awritrojit Banerjee, Achim Schilling, Patrick Krauss</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08053">https://arxiv.org/abs/2501.08053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08053">https://arxiv.org/pdf/2501.08053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08053]] Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT(https://arxiv.org/abs/2501.08053)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the internal mechanisms of BERT, a transformer-based large language model, with a focus on its ability to cluster narrative content and authorial style across its layers. Using a dataset of narratives developed via GPT-4, featuring diverse semantic content and stylistic variations, we analyze BERT's layerwise activations to uncover patterns of localized neural processing. Through dimensionality reduction techniques such as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that BERT exhibits strong clustering based on narrative content in its later layers, with progressively compact and distinct clusters. While strong stylistic clustering might occur when narratives are rephrased into different text types (e.g., fables, sci-fi, kids' stories), minimal clustering is observed for authorial style specific to individual writers. These findings highlight BERT's prioritization of semantic content over stylistic features, offering insights into its representational capabilities and processing hierarchy. This study contributes to understanding how transformer models like BERT encode linguistic information, paving the way for future interdisciplinary research in artificial intelligence and cognitive neuroscience.</li>
</ul>

<h3>Title: Skeleton and Font Generation Network for Zero-shot Chinese Character Generation</h3>
<ul>
<li><strong>Authors: </strong>Mobai Xue, Jun Du, Zhenrong Zhang, Jiefeng Ma, Qikai Chang, Pengfei Hu, Jianshu Zhang, Yu Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08062">https://arxiv.org/abs/2501.08062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08062">https://arxiv.org/pdf/2501.08062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08062]] Skeleton and Font Generation Network for Zero-shot Chinese Character Generation(https://arxiv.org/abs/2501.08062)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automatic font generation remains a challenging research issue, primarily due to the vast number of Chinese characters, each with unique and intricate structures. Our investigation of previous studies reveals inherent bias capable of causing structural changes in characters. Specifically, when generating a Chinese character similar to, but different from, those in the training samples, the bias is prone to either correcting or ignoring these subtle variations. To address this concern, we propose a novel Skeleton and Font Generation Network (SFGN) to achieve a more robust Chinese character font generation. Our approach includes a skeleton builder and font generator. The skeleton builder synthesizes content features using low-resource text input, enabling our technique to realize font generation independently of content image inputs. Unlike previous font generation methods that treat font style as a global embedding, we introduce a font generator to align content and style features on the radical level, which is a brand-new perspective for font generation. Except for common characters, we also conduct experiments on misspelled characters, a substantial portion of which slightly differs from the common ones. Our approach visually demonstrates the efficacy of generated images and outperforms current state-of-the-art font generation methods. Moreover, we believe that misspelled character generation have significant pedagogical implications and verify such supposition through experiments. We used generated misspelled characters as data augmentation in Chinese character error correction tasks, simulating the scenario where students learn handwritten Chinese characters with the help of misspelled characters. The significantly improved performance of error correction tasks demonstrates the effectiveness of our proposed approach and the value of misspelled character generation.</li>
</ul>

<h3>Title: Optimal Policy Adaptation under Covariate Shift</h3>
<ul>
<li><strong>Authors: </strong>Xueqing Liu, Qinwei Yang, Zhaoqing Tian, Ruocheng Guo, Peng Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08067">https://arxiv.org/abs/2501.08067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08067">https://arxiv.org/pdf/2501.08067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08067]] Optimal Policy Adaptation under Covariate Shift(https://arxiv.org/abs/2501.08067)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transfer learning of prediction models has been extensively studied, while the corresponding policy learning approaches are rarely discussed. In this paper, we propose principled approaches for learning the optimal policy in the target domain by leveraging two datasets: one with full information from the source domain and the other from the target domain with only covariates. First, under the setting of covariate shift, we formulate the problem from a perspective of causality and present the identifiability assumptions for the reward induced by a given policy. Then, we derive the efficient influence function and the semiparametric efficiency bound for the reward. Based on this, we construct a doubly robust and semiparametric efficient estimator for the reward and then learn the optimal policy by optimizing the estimated reward. Moreover, we theoretically analyze the bias and the generalization error bound for the learned policy. Furthermore, in the presence of both covariate and concept shifts, we propose a novel sensitivity analysis method to evaluate the robustness of the proposed policy learning approach. Extensive experiments demonstrate that the approach not only estimates the reward more accurately but also yields a policy that closely approximates the theoretically optimal policy.</li>
</ul>

<h3>Title: Benchmarking Vision Foundation Models for Input Monitoring in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Nert Keser, Halil Ibrahim Orhan, Niki Amini-Naieni, Gesina Schwalbe, Alois Knoll, Matthias Rottmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08083">https://arxiv.org/abs/2501.08083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08083">https://arxiv.org/pdf/2501.08083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08083]] Benchmarking Vision Foundation Models for Input Monitoring in Autonomous Driving(https://arxiv.org/abs/2501.08083)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) remain challenged by distribution shifts in complex open-world domains like automated driving (AD): Absolute robustness against yet unknown novel objects (semantic shift) or styles like lighting conditions (covariate shift) cannot be guaranteed. Hence, reliable operation-time monitors for identification of out-of-training-data-distribution (OOD) scenarios are imperative. Current approaches for OOD classification are untested for complex domains like AD, are limited in the kinds of shifts they detect, or even require supervision with OOD samples. To prepare for unanticipated shifts, we instead establish a framework around a principled, unsupervised, and model-agnostic method that unifies detection of all kinds of shifts: Find a full model of the training data's feature distribution, to then use its density at new points as in-distribution (ID) score. To implement this, we propose to combine the newly available Vision Foundation Models (VFM) as feature extractors with one of four alternative density modeling techniques. In an extensive benchmark of 4 VFMs against 20 baselines, we show the superior performance of VFM feature encodings compared to shift-specific OOD monitors. Additionally, we find that sophisticated architectures outperform larger latent space dimensionality; and our method identifies samples with higher risk of errors on downstream tasks, despite being model-agnostic. This suggests that VFMs are promising to realize model-agnostic, unsupervised, reliable safety monitors in complex vision tasks.</li>
</ul>

<h3>Title: Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention for Enabled Classification</h3>
<ul>
<li><strong>Authors: </strong>Hui Lee, Singh Suniljit, Yong Siang Ong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08085">https://arxiv.org/abs/2501.08085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08085">https://arxiv.org/pdf/2501.08085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08085]] Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention for Enabled Classification(https://arxiv.org/abs/2501.08085)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper explores the development of a multimodal sentiment analysis model that integrates text, audio, and visual data to enhance sentiment classification. The goal is to improve emotion detection by capturing the complex interactions between these modalities, thereby enabling more accurate and nuanced sentiment interpretation. The study evaluates three feature fusion strategies -- late stage fusion, early stage fusion, and multi-headed attention -- within a transformer-based architecture. Experiments were conducted using the CMU-MOSEI dataset, which includes synchronized text, audio, and visual inputs labeled with sentiment scores. Results show that early stage fusion significantly outperforms late stage fusion, achieving an accuracy of 71.87\%, while the multi-headed attention approach offers marginal improvement, reaching 72.39\%. The findings suggest that integrating modalities early in the process enhances sentiment classification, while attention mechanisms may have limited impact within the current framework. Future work will focus on refining feature fusion techniques, incorporating temporal data, and exploring dynamic feature weighting to further improve model performance.</li>
</ul>

<h3>Title: Consistency of Responses and Continuations Generated by Large Language Models on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08102">https://arxiv.org/abs/2501.08102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08102">https://arxiv.org/pdf/2501.08102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08102]] Consistency of Responses and Continuations Generated by Large Language Models on Social Media(https://arxiv.org/abs/2501.08102)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.</li>
</ul>

<h3>Title: EarthView: A Large Scale Remote Sensing Dataset for Self-Supervision</h3>
<ul>
<li><strong>Authors: </strong>Diego Velazquez, Pau Rodriguez L√≥pez, Sergio Alonso, Josep M. Gonfaus, Jordi Gonzalez, Gerardo Richarte, Javier Marin, Yoshua Bengio, Alexandre Lacoste</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08111">https://arxiv.org/abs/2501.08111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08111">https://arxiv.org/pdf/2501.08111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08111]] EarthView: A Large Scale Remote Sensing Dataset for Self-Supervision(https://arxiv.org/abs/2501.08111)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents EarthView, a comprehensive dataset specifically designed for self-supervision on remote sensing data, intended to enhance deep learning applications on Earth monitoring tasks. The dataset spans 15 tera pixels of global remote-sensing data, combining imagery from a diverse range of sources, including NEON, Sentinel, and a novel release of 1m spatial resolution data from Satellogic. Our dataset provides a wide spectrum of image data with varying resolutions, harnessed from different sensors and organized coherently into an accessible HuggingFace dataset in parquet format. This data spans five years, from 2017 to 2022. Accompanying the dataset, we introduce EarthMAE, a tailored Masked Autoencoder, developed to tackle the distinct challenges of remote sensing data. Trained in a self-supervised fashion, EarthMAE effectively processes different data modalities such as hyperspectral, multispectral, topographical data, segmentation maps, and temporal structure. This model helps us show that pre-training on Satellogic data improves performance on downstream tasks. While there is still a gap to fill in MAE for heterogeneous data, we regard this innovative combination of an expansive, diverse dataset and a versatile model adapted for self-supervised learning as a stride forward in deep learning for Earth monitoring.</li>
</ul>

<h3>Title: Change Captioning in Remote Sensing: Evolution to SAT-Cap -- A Single-Stage Transformer Approach</h3>
<ul>
<li><strong>Authors: </strong>Yuduo Wang, Weikang Yu, Pedram Ghamisi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08114">https://arxiv.org/abs/2501.08114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08114">https://arxiv.org/pdf/2501.08114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08114]] Change Captioning in Remote Sensing: Evolution to SAT-Cap -- A Single-Stage Transformer Approach(https://arxiv.org/abs/2501.08114)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Change captioning has become essential for accurately describing changes in multi-temporal remote sensing data, providing an intuitive way to monitor Earth's dynamics through natural language. However, existing change captioning methods face two key challenges: high computational demands due to multistage fusion strategy, and insufficient detail in object descriptions due to limited semantic extraction from individual images. To solve these challenges, we propose SAT-Cap based on the transformers model with a single-stage feature fusion for remote sensing change captioning. In particular, SAT-Cap integrates a Spatial-Channel Attention Encoder, a Difference-Guided Fusion module, and a Caption Decoder. Compared to typical models that require multi-stage fusion in transformer encoder and fusion module, SAT-Cap uses only a simple cosine similarity-based fusion module for information integration, reducing the complexity of the model architecture. By jointly modeling spatial and channel information in Spatial-Channel Attention Encoder, our approach significantly enhances the model's ability to extract semantic information from objects in multi-temporal remote sensing images. Extensive experiments validate the effectiveness of SAT-Cap, achieving CIDEr scores of 140.23% on the LEVIR-CC dataset and 97.74% on the DUBAI-CC dataset, surpassing current state-of-the-art methods. The code and pre-trained models will be available online.</li>
</ul>

<h3>Title: RoHan: Robust Hand Detection in Operation Room</h3>
<ul>
<li><strong>Authors: </strong>Roi Papo, Sapir Gershov, Tom Friedman, Itay Or, Gil Bolotin, Shlomi Laufer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08115">https://arxiv.org/abs/2501.08115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08115">https://arxiv.org/pdf/2501.08115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08115]] RoHan: Robust Hand Detection in Operation Room(https://arxiv.org/abs/2501.08115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hand-specific localization has garnered significant interest within the computer vision community. Although there are numerous datasets with hand annotations from various angles and settings, domain transfer techniques frequently struggle in surgical environments. This is mainly due to the limited availability of gloved hand instances and the unique challenges of operating rooms (ORs). Thus, hand-detection models tailored to OR settings require extensive training and expensive annotation processes. To overcome these challenges, we present "RoHan" - a novel approach for robust hand detection in the OR, leveraging advanced semi-supervised domain adaptation techniques to tackle the challenges of varying recording conditions, diverse glove colors, and occlusions common in surgical settings. Our methodology encompasses two main stages: (1) data augmentation strategy that utilizes "Artificial Gloves," a method for augmenting publicly available hand datasets with synthetic images of hands-wearing gloves; (2) semi-supervised domain adaptation pipeline that improves detection performance in real-world OR settings through iterative prediction refinement and efficient frame filtering. We evaluate our method using two datasets: simulated enterotomy repair and saphenous vein graft harvesting. "RoHan" substantially reduces the need for extensive labeling and model training, paving the way for the practical implementation of hand detection technologies in medical settings.</li>
</ul>

<h3>Title: Revisiting Birds Eye View Perception Models with Frozen Foundation Models: DINOv2 and Metric3Dv2</h3>
<ul>
<li><strong>Authors: </strong>Seamie Hayes, Ganesh Sistu, Ciar√°n Eising</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08118">https://arxiv.org/abs/2501.08118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08118">https://arxiv.org/pdf/2501.08118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08118]] Revisiting Birds Eye View Perception Models with Frozen Foundation Models: DINOv2 and Metric3Dv2(https://arxiv.org/abs/2501.08118)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Birds Eye View perception models require extensive data to perform and generalize effectively. While traditional datasets often provide abundant driving scenes from diverse locations, this is not always the case. It is crucial to maximize the utility of the available training data. With the advent of large foundation models such as DINOv2 and Metric3Dv2, a pertinent question arises: can these models be integrated into existing model architectures to not only reduce the required training data but surpass the performance of current models? We choose two model architectures in the vehicle segmentation domain to alter: Lift-Splat-Shoot, and Simple-BEV. For Lift-Splat-Shoot, we explore the implementation of frozen DINOv2 for feature extraction and Metric3Dv2 for depth estimation, where we greatly exceed the baseline results by 7.4 IoU while utilizing only half the training data and iterations. Furthermore, we introduce an innovative application of Metric3Dv2's depth information as a PseudoLiDAR point cloud incorporated into the Simple-BEV architecture, replacing traditional LiDAR. This integration results in a +3 IoU improvement compared to the Camera-only model.</li>
</ul>

<h3>Title: Bootstrapping Corner Cases: High-Resolution Inpainting for Safety Critical Detect and Avoid for Automated Flying</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Lyhs, Lars Hinneburg, Michael Fischer, Florian √ñlsner, Stefan Milz, Jeremy Tschirner, Patrick M√§der</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08142">https://arxiv.org/abs/2501.08142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08142">https://arxiv.org/pdf/2501.08142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08142]] Bootstrapping Corner Cases: High-Resolution Inpainting for Safety Critical Detect and Avoid for Automated Flying(https://arxiv.org/abs/2501.08142)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern machine learning techniques have shown tremendous potential, especially for object detection on camera images. For this reason, they are also used to enable safety-critical automated processes such as autonomous drone flights. We present a study on object detection for Detect and Avoid, a safety critical function for drones that detects air traffic during automated flights for safety reasons. An ill-posed problem is the generation of good and especially large data sets, since detection itself is the corner case. Most models suffer from limited ground truth in raw data, \eg recorded air traffic or frontal flight with a small aircraft. It often leads to poor and critical detection rates. We overcome this problem by using inpainting methods to bootstrap the dataset such that it explicitly contains the corner cases of the raw data. We provide an overview of inpainting methods and generative models and present an example pipeline given a small annotated dataset. We validate our method by generating a high-resolution dataset, which we make publicly available and present it to an independent object detector that was fully trained on real data.</li>
</ul>

<h3>Title: Refusal Behavior in Large Language Models: A Nonlinear Perspective</h3>
<ul>
<li><strong>Authors: </strong>Fabian Hildebrandt, Andreas Maier, Patrick Krauss, Achim Schilling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08145">https://arxiv.org/abs/2501.08145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08145">https://arxiv.org/pdf/2501.08145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08145]] Refusal Behavior in Large Language Models: A Nonlinear Perspective(https://arxiv.org/abs/2501.08145)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Refusal behavior in large language models (LLMs) enables them to decline responding to harmful, unethical, or inappropriate prompts, ensuring alignment with ethical standards. This paper investigates refusal behavior across six LLMs from three architectural families. We challenge the assumption of refusal as a linear phenomenon by employing dimensionality reduction techniques, including PCA, t-SNE, and UMAP. Our results reveal that refusal mechanisms exhibit nonlinear, multidimensional characteristics that vary by model architecture and layer. These findings highlight the need for nonlinear interpretability to improve alignment research and inform safer AI deployment strategies.</li>
</ul>

<h3>Title: Energy Backdoor Attack to Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, Olivier D√©forges, Kassem Kallas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08152">https://arxiv.org/abs/2501.08152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08152">https://arxiv.org/pdf/2501.08152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08152]] Energy Backdoor Attack to Deep Neural Networks(https://arxiv.org/abs/2501.08152)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>The rise of deep learning (DL) has increased computing complexity and energy use, prompting the adoption of application specific integrated circuits (ASICs) for energy-efficient edge and mobile deployment. However, recent studies have demonstrated the vulnerability of these accelerators to energy attacks. Despite the development of various inference time energy attacks in prior research, backdoor energy attacks remain unexplored. In this paper, we design an innovative energy backdoor attack against deep neural networks (DNNs) operating on sparsity-based accelerators. Our attack is carried out in two distinct phases: backdoor injection and backdoor stealthiness. Experimental results using ResNet-18 and MobileNet-V2 models trained on CIFAR-10 and Tiny ImageNet datasets show the effectiveness of our proposed attack in increasing energy consumption on trigger samples while preserving the model's performance for clean/regular inputs. This demonstrates the vulnerability of DNNs to energy backdoor attacks. The source code of our attack is available at: this https URL.</li>
</ul>

<h3>Title: FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification</h3>
<ul>
<li><strong>Authors: </strong>Nurit Cohen-Inger, Lior Rokach, Bracha Shapira, Seffi Cohen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08155">https://arxiv.org/abs/2501.08155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08155">https://arxiv.org/pdf/2501.08155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08155]] FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification(https://arxiv.org/abs/2501.08155)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, fair</a></li>
<li><strong>Abstract: </strong>Algorithmic decision-making has become deeply ingrained in many domains, yet biases in machine learning models can still produce discriminatory outcomes, often harming unprivileged groups. Achieving fair classification is inherently challenging, requiring a careful balance between predictive performance and ethical considerations. We present FairTTTS, a novel post-processing bias mitigation method inspired by the Tree Test Time Simulation (TTTS) method. Originally developed to enhance accuracy and robustness against adversarial inputs through probabilistic decision-path adjustments, TTTS serves as the foundation for FairTTTS. By building on this accuracy-enhancing technique, FairTTTS mitigates bias and improves predictive performance. FairTTTS uses a distance-based heuristic to adjust decisions at protected attribute nodes, ensuring fairness for unprivileged samples. This fairness-oriented adjustment occurs as a post-processing step, allowing FairTTTS to be applied to pre-trained models, diverse datasets, and various fairness metrics without retraining. Extensive evaluation on seven benchmark datasets shows that FairTTTS outperforms traditional methods in fairness improvement, achieving a 20.96% average increase over the baseline compared to 18.78% for related work, and further enhances accuracy by 0.55%. In contrast, competing methods typically reduce accuracy by 0.42%. These results confirm that FairTTTS effectively promotes more equitable decision-making while simultaneously improving predictive performance.</li>
</ul>

<h3>Title: Potential and Perils of Large Language Models as Judges of Unstructured Textual Data</h3>
<ul>
<li><strong>Authors: </strong>Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08167">https://arxiv.org/abs/2501.08167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08167">https://arxiv.org/pdf/2501.08167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08167]] Potential and Perils of Large Language Models as Judges of Unstructured Textual Data(https://arxiv.org/abs/2501.08167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases.</li>
</ul>

<h3>Title: Revolutionizing Communication with Deep Learning and XAI for Enhanced Arabic Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mazen Balat, Rewaa Awaad, Ahmed B. Zaky, Salah A. Aly</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08169">https://arxiv.org/abs/2501.08169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08169">https://arxiv.org/pdf/2501.08169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08169]] Revolutionizing Communication with Deep Learning and XAI for Enhanced Arabic Sign Language Recognition(https://arxiv.org/abs/2501.08169)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This study introduces an integrated approach to recognizing Arabic Sign Language (ArSL) using state-of-the-art deep learning models such as MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with EfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively. Key innovations include sophisticated data augmentation methods to mitigate class imbalance, implementation of stratified 5-fold cross-validation for better generalization, and the use of Grad-CAM for clear model decision transparency. The proposed system not only sets new benchmarks in recognition accuracy but also emphasizes interpretability, making it suitable for applications in healthcare, education, and inclusive communication technologies.</li>
</ul>

<h3>Title: D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models</h3>
<ul>
<li><strong>Authors: </strong>Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08180">https://arxiv.org/abs/2501.08180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08180">https://arxiv.org/pdf/2501.08180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08180]] D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models(https://arxiv.org/abs/2501.08180)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved cutting-edge performance in image generation. However, their lengthy denoising process and computationally intensive score estimation network impede their scalability in low-latency and resource-constrained scenarios. Post-training quantization (PTQ) compresses and accelerates diffusion models without retraining, but it inevitably introduces additional quantization noise, resulting in mean and variance deviations. In this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely mitigating the adverse effects of quantization noise on the noise estimation network. Specifically, we first unravel the impact of quantization noise on the sampling equation into two components: the mean deviation and the variance deviation. The mean deviation alters the drift coefficient of the sampling equation, influencing the trajectory trend, while the variance deviation magnifies the diffusion coefficient, impacting the convergence of the sampling trajectory. The proposed D2-DPM is thus devised to denoise the quantization noise at each time step, and then denoise the noisy sample through the inverse diffusion iterations. Experimental results demonstrate that D2-DPM achieves superior generation quality, yielding a 1.42 lower FID than the full-precision model while achieving 3.99x compression and 11.67x bit-operation acceleration.</li>
</ul>

<h3>Title: A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE, cs.HC, cs.LG, q-bio.CB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08187">https://arxiv.org/abs/2501.08187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08187">https://arxiv.org/pdf/2501.08187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08187]] A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following(https://arxiv.org/abs/2501.08187)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language of cellular biology", capturing intricate gene expression patterns at the single-cell level. However, interacting with this "language" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.</li>
</ul>

<h3>Title: A Critical Synthesis of Uncertainty Quantification and Foundation Models in Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Steven Landgraf, Rongjun Qin, Markus Ulrich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08188">https://arxiv.org/abs/2501.08188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08188">https://arxiv.org/pdf/2501.08188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08188]] A Critical Synthesis of Uncertainty Quantification and Foundation Models in Monocular Depth Estimation(https://arxiv.org/abs/2501.08188)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, segmentation</a></li>
<li><strong>Abstract: </strong>While recent foundation models have enabled significant breakthroughs in monocular depth estimation, a clear path towards safe and reliable deployment in the real-world remains elusive. Metric depth estimation, which involves predicting absolute distances, poses particular challenges, as even the most advanced foundation models remain prone to critical errors. Since quantifying the uncertainty has emerged as a promising endeavor to address these limitations and enable trustworthy deployment, we fuse five different uncertainty quantification methods with the current state-of-the-art DepthAnythingV2 foundation model. To cover a wide range of metric depth domains, we evaluate their performance on four diverse datasets. Our findings identify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as a particularly promising approach, offering reliable uncertainty estimates while maintaining predictive performance and computational efficiency on par with the baseline, encompassing both training and inference time. By fusing uncertainty quantification and foundation models within the context of monocular depth estimation, this paper lays a critical foundation for future research aimed at improving not only model performance but also its explainability. Extending this critical synthesis of uncertainty quantification and foundation models into other crucial tasks, such as semantic segmentation and pose estimation, presents exciting opportunities for safer and more reliable machine vision systems.</li>
</ul>

<h3>Title: Modeling Quantum Machine Learning for Genomic Data Analysis</h3>
<ul>
<li><strong>Authors: </strong>Navneet Singh, Shiva Raj Pokhrel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08193">https://arxiv.org/abs/2501.08193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08193">https://arxiv.org/pdf/2501.08193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08193]] Modeling Quantum Machine Learning for Genomic Data Analysis(https://arxiv.org/abs/2501.08193)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Quantum Machine Learning (QML) continues to evolve, unlocking new opportunities for diverse applications. In this study, we investigate and evaluate the applicability of QML models for binary classification of genome sequence data by employing various feature mapping techniques. We present an open-source, independent Qiskit-based implementation to conduct experiments on a benchmark genomic dataset. Our simulations reveal that the interplay between feature mapping techniques and QML algorithms significantly influences performance. Notably, the Pegasos Quantum Support Vector Classifier (Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall metrics, while Quantum Neural Networks (QNN) achieve the highest training accuracy across all feature maps. However, the pronounced variability in classifier performance, dependent on feature mapping, highlights the risk of overfitting to localized output distributions in certain scenarios. This work underscores the transformative potential of QML for genomic data classification while emphasizing the need for continued advancements to enhance the robustness and accuracy of these methodologies.</li>
</ul>

<h3>Title: OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training</h3>
<ul>
<li><strong>Authors: </strong>Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, Ji Pei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08197">https://arxiv.org/abs/2501.08197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08197">https://arxiv.org/pdf/2501.08197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08197]] OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training(https://arxiv.org/abs/2501.08197)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities, but their success heavily relies on the quality of pretraining corpora. For Chinese LLMs, the scarcity of high-quality Chinese datasets presents a significant challenge, often limiting their performance. To address this issue, we propose the OpenCSG Chinese Corpus, a series of high-quality datasets specifically designed for LLM pretraining, post-training, and fine-tuning. This corpus includes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and Smoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets focus on filtered, high-quality content derived from diverse Chinese web sources; Cosmopedia-chinese provides synthetic, textbook-style data for knowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and diverse chat-format data. The OpenCSG Chinese Corpus is characterized by its high-quality text, diverse coverage across domains, and scalable, reproducible data curation processes. Additionally, we conducted extensive experimental analyses, including evaluations on smaller parameter models, which demonstrated significant performance improvements in tasks such as C-Eval, showcasing the effectiveness of the corpus for training Chinese LLMs.</li>
</ul>

<h3>Title: EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yassine El Boudouri, Amine Bohi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08199">https://arxiv.org/abs/2501.08199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08199">https://arxiv.org/pdf/2501.08199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08199]] EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition(https://arxiv.org/abs/2501.08199)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Facial expressions play a crucial role in human communication serving as a powerful and impactful means to express a wide range of emotions. With advancements in artificial intelligence and computer vision, deep neural networks have emerged as effective tools for facial emotion recognition. In this paper, we propose EmoNeXt, a novel deep learning framework for facial expression recognition based on an adapted ConvNeXt architecture network. We integrate a Spatial Transformer Network (STN) to focus on feature-rich regions of the face and Squeeze-and-Excitation blocks to capture channel-wise dependencies. Moreover, we introduce a self-attention regularization term, encouraging the model to generate compact feature vectors. We demonstrate the superiority of our model over existing state-of-the-art deep learning models on the FER2013 dataset regarding emotion classification accuracy.</li>
</ul>

<h3>Title: ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, Akbar Karimi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08203">https://arxiv.org/abs/2501.08203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08203">https://arxiv.org/pdf/2501.08203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08203]] ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving(https://arxiv.org/abs/2501.08203)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have shown impressive capabilities in math problem-solving tasks, their robustness to noisy inputs is not well-studied. In this work, we propose ArithmAttack to examine how robust the LLMs are when they encounter noisy prompts that contain extra noise in the form of punctuation marks. While being easy to implement, ArithmAttack does not cause any information loss since words are not added or deleted from the context. We evaluate the robustness of seven LLMs, including LLama3, Mistral, and Mathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest that all the studied models show vulnerability to such noise, with more noise leading to poorer performances.</li>
</ul>

<h3>Title: Modeling Feature Maps for Quantum Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Navneet Singh, Shiva Raj Pokhrel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08205">https://arxiv.org/abs/2501.08205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08205">https://arxiv.org/pdf/2501.08205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08205]] Modeling Feature Maps for Quantum Machine Learning(https://arxiv.org/abs/2501.08205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Quantum Machine Learning (QML) offers significant potential for complex tasks like genome sequence classification, but quantum noise on Noisy Intermediate-Scale Quantum (NISQ) devices poses practical challenges. This study systematically evaluates how various quantum noise models including dephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and phase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature mapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results indicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are more sensitive, particularly to depolarizing and amplitude-damping noise. The PauliFeatureMap is especially vulnerable, highlighting difficulties in maintaining accurate classification under noisy conditions. These findings underscore the critical importance of feature map selection and noise mitigation strategies in optimizing QML for genomic classification, with promising implications for personalized medicine.</li>
</ul>

<h3>Title: ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08208">https://arxiv.org/abs/2501.08208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08208">https://arxiv.org/pdf/2501.08208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08208]] ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems(https://arxiv.org/abs/2501.08208)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.</li>
</ul>

<h3>Title: Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings</h3>
<ul>
<li><strong>Authors: </strong>Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08219">https://arxiv.org/abs/2501.08219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08219">https://arxiv.org/pdf/2501.08219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08219]] Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings(https://arxiv.org/abs/2501.08219)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown significant improvements in many natural language processing (NLP) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of LLMs has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of LLMs. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of LLMs during inference and examine their trade-offs. First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking LLMs such as Falcon-7B, Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling (DVFS), on the models' latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient LLM inference systems.</li>
</ul>

<h3>Title: FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Yabo Zhang, Xinpeng Zhou, Yihan Zeng, Hang Xu, Hui Li, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08225">https://arxiv.org/abs/2501.08225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08225">https://arxiv.org/pdf/2501.08225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08225]] FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors(https://arxiv.org/abs/2501.08225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Interactive image editing allows users to modify images through visual interaction operations such as drawing, clicking, and dragging. Existing methods construct such supervision signals from videos, as they capture how objects change with various physical interactions. However, these models are usually built upon text-to-image diffusion models, so necessitate (i) massive training samples and (ii) an additional reference encoder to learn real-world dynamics and visual consistency. In this paper, we reformulate this task as an image-to-video generation problem, so that inherit powerful video diffusion priors to reduce training costs and ensure temporal consistency. Specifically, we introduce FramePainter as an efficient instantiation of this formulation. Initialized with Stable Video Diffusion, it only uses a lightweight sparse control encoder to inject editing signals. Considering the limitations of temporal attention in handling large motion between two frames, we further propose matching attention to enlarge the receptive field while encouraging dense correspondence between edited and source image tokens. We highlight the effectiveness and efficiency of FramePainter across various of editing signals: it domainantly outperforms previous state-of-the-art methods with far less training data, achieving highly seamless and coherent editing of images, \eg, automatically adjust the reflection of the cup. Moreover, FramePainter also exhibits exceptional generalization in scenarios not present in real-world videos, \eg, transform the clownfish into shark-like shape. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth Models</h3>
<ul>
<li><strong>Authors: </strong>Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08226">https://arxiv.org/abs/2501.08226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08226">https://arxiv.org/pdf/2501.08226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08226]] Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth Models(https://arxiv.org/abs/2501.08226)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Glioblastoma, a highly aggressive brain tumor, poses major challenges due to its poor prognosis and high morbidity rates. Partial differential equation-based models offer promising potential to enhance therapeutic outcomes by simulating patient-specific tumor behavior for improved radiotherapy planning. However, model calibration remains a bottleneck due to the high computational demands of optimization methods like Monte Carlo sampling and evolutionary algorithms. To address this, we recently introduced an approach leveraging a neural forward solver with gradient-based optimization to significantly reduce calibration time. This approach requires a highly accurate and fully differentiable forward model. We investigate multiple architectures, including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a 3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best overall results, excelling in both tumor outline matching and voxel-level prediction of tumor cell concentration. It halved the MSE relative to the baseline model and achieved the highest Dice score across all tumor cell concentration thresholds. Our study demonstrates significant enhancement in forward solver performance and outlines important future research directions.</li>
</ul>

<h3>Title: Privacy-Preserving Model and Preprocessing Verification for Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08236">https://arxiv.org/abs/2501.08236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08236">https://arxiv.org/pdf/2501.08236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08236]] Privacy-Preserving Model and Preprocessing Verification for Machine Learning(https://arxiv.org/abs/2501.08236)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>This paper presents a framework for privacy-preserving verification of machine learning models, focusing on models trained on sensitive data. Integrating Local Differential Privacy (LDP) with model explanations from LIME and SHAP, our framework enables robust verification without compromising individual privacy. It addresses two key tasks: binary classification, to verify if a target model was trained correctly by applying the appropriate preprocessing steps, and multi-class classification, to identify specific preprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult, and Student Record-demonstrate that while the ML-based approach is particularly effective in binary tasks, the threshold-based method performs comparably in multi-class tasks. Results indicate that although verification accuracy varies across datasets and noise levels, the framework provides effective detection of preprocessing errors, strong privacy guarantees, and practical applicability for safeguarding sensitive data.</li>
</ul>

<h3>Title: Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture for Context Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08245">https://arxiv.org/abs/2501.08245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08245">https://arxiv.org/pdf/2501.08245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08245]] Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture for Context Adaptation(https://arxiv.org/abs/2501.08245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Deep Learning for medical imaging faces challenges in adapting and generalizing to new contexts. Additionally, it often lacks sufficient labeled data for specific tasks requiring significant annotation effort. Continual Learning (CL) tackles adaptability and generalizability by enabling lifelong learning from a data stream while mitigating forgetting of previously learned knowledge. Active Learning (AL) reduces the number of required annotations for effective training. This work explores both approaches (CAL) to develop a novel framework for robust medical image analysis. Based on the automatic recognition of shifts in image characteristics, Replay-Base Architecture for Context Adaptation (RBACA) employs a CL rehearsal method to continually learn from diverse contexts, and an AL component to select the most informative instances for annotation. A novel approach to evaluate CAL methods is established using a defined metric denominated IL-Score, which allows for the simultaneous assessment of transfer learning, forgetting, and final model performance. We show that RBACA works in domain and class-incremental learning scenarios, by assessing its IL-Score on the segmentation and diagnosis of cardiac images. The results show that RBACA outperforms a baseline framework without CAL, and a state-of-the-art CAL method across various memory sizes and annotation budgets. Our code is available in this https URL .</li>
</ul>

<h3>Title: Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jonathan N√∂ther, Adish Singla, Goran Radanoviƒá</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08246">https://arxiv.org/abs/2501.08246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08246">https://arxiv.org/pdf/2501.08246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08246]] Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints(https://arxiv.org/abs/2501.08246)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red-teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this paper, we study red-teaming strategies that enable a targeted security assessment. We propose an optimization framework for red-teaming with proximity constraints, where the discovered prompts must be similar to reference prompts from a given dataset. This dataset serves as a template for the discovered prompts, anchoring the search for test-cases to specific topics, writing styles, or types of harmful behavior. We show that established auto-regressive model architectures do not perform well in this setting. We therefore introduce a black-box red-teaming method inspired by text-diffusion models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the reference prompt by perturbing it in the embedding space, directly controlling the amount of change introduced. We systematically evaluate our method by comparing its effectiveness with established methods based on model fine-tuning and zero- and few-shot prompting. Our results show that DART is significantly more effective at discovering harmful inputs in close proximity to the reference prompt.</li>
</ul>

<h3>Title: Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08248">https://arxiv.org/abs/2501.08248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08248">https://arxiv.org/pdf/2501.08248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08248]] Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models(https://arxiv.org/abs/2501.08248)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and perform retrieval and reasoning directly -- a capability we define as In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like LOFT often overestimate LCLM performance by providing overly simplified contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs in more realistic scenarios by including confounding passages retrieved with strong retrievers. We then propose three methods to enhance LCLM performance: (1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which uses attention heads to filter and de-noise long contexts during decoding, and (3) joint retrieval head training alongside the generation head. Our evaluation of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks despite being a much smaller model.</li>
</ul>

<h3>Title: Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World</h3>
<ul>
<li><strong>Authors: </strong>Dudi Biton, Jacob Shams, Koda Satoru, Asaf Shabtai, Yuval Elovici, Ben Nassi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08258">https://arxiv.org/abs/2501.08258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08258">https://arxiv.org/pdf/2501.08258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08258]] Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World(https://arxiv.org/abs/2501.08258)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The traditional learning process of patch-based adversarial attacks, conducted in the digital domain and then applied in the physical domain (e.g., via printed stickers), may suffer from reduced performance due to adversarial patches' limited transferability from the digital domain to the physical domain. Given that previous studies have considered using projectors to apply adversarial attacks, we raise the following question: can adversarial learning (i.e., patch generation) be performed entirely in the physical domain with a projector? In this work, we propose the Physical-domain Adversarial Patch Learning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework that converts adversarial learning from the digital domain to the physical domain using a projector. We evaluate PAPLA across multiple scenarios, including controlled laboratory settings and realistic outdoor environments, demonstrating its ability to ensure attack success compared to conventional digital learning-physical application (DL-PA) methods. We also analyze the impact of environmental factors, such as projection surface color, projector strength, ambient light, distance, and angle of the target object relative to the camera, on the effectiveness of projected patches. Finally, we demonstrate the feasibility of the attack against a parked car and a stop sign in a real-world outdoor environment. Our results show that under specific conditions, E2E adversarial learning in the physical domain eliminates the transferability issue and ensures evasion by object detectors. Finally, we provide insights into the challenges and opportunities of applying adversarial learning in the physical domain and explain where such an approach is more effective than using a sticker.</li>
</ul>

<h3>Title: Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</h3>
<ul>
<li><strong>Authors: </strong>TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08263">https://arxiv.org/abs/2501.08263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08263">https://arxiv.org/pdf/2501.08263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08263]] Multiplayer Federated Learning: Reaching Equilibrium with Less Communication(https://arxiv.org/abs/2501.08263)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Traditional Federated Learning (FL) approaches assume collaborative clients with aligned objectives working towards a shared global model. However, in many real-world scenarios, clients act as rational players with individual objectives and strategic behaviors, a concept that existing FL frameworks are not equipped to adequately address. To bridge this gap, we introduce Multiplayer Federated Learning (MpFL), a novel framework that models the clients in the FL environment as players in a game-theoretic context, aiming to reach an equilibrium. In this scenario, each player tries to optimize their own utility function, which may not align with the collective goal. Within MpFL, we propose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm in which each player/client performs local updates independently and periodically communicates with other players. We theoretically analyze PEARL-SGD and prove that it reaches a neighborhood of equilibrium with less communication in the stochastic setup compared to its non-local counterpart. Finally, we verify our theoretical findings through numerical experiments.</li>
</ul>

<h3>Title: AI Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Sanjida Afrin Mou (1), Tasfia Noor Chowdhury (2), Adib Ibn Mannan (3), Sadia Nourin Mim (4), Lubana Tarannum (5), Tasrin Noman (6), Jamal Uddin Ahamed ((1) Department of Mechatronics &amp; Industrial Engineering, Chittagong University of Engineering &amp; Technology (CUET), Chattogram, Bangladesh (2) Department of Mechanical Engineering, Chittagong University of Engineering &amp; Technology (CUET), Chattogram, Bangladesh)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08266">https://arxiv.org/abs/2501.08266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08266">https://arxiv.org/pdf/2501.08266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08266]] AI Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring(https://arxiv.org/abs/2501.08266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Flooding is a major natural hazard causing significant fatalities and economic losses annually, with increasing frequency due to climate change. Rapid and accurate flood detection and monitoring are crucial for mitigating these impacts. This study compares the performance of three deep learning models UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid in flood detection, utilizing images from drones, in field observations, and social media. This study involves creating a new dataset that augments wellknown benchmark datasets with flood-specific images, enhancing the robustness of the models. The UNet, ResNet, and DeepLab v3 architectures are tested to determine their effectiveness in various environmental conditions and geographical locations, and the strengths and limitations of each model are also discussed here, providing insights into their applicability in different scenarios by predicting image segmentation masks. This fully automated approach allows these models to isolate flooded areas in images, significantly reducing processing time compared to traditional semi-automated methods. The outcome of this study is to predict segmented masks for each image effected by a flood disaster and the validation accuracy of these models. This methodology facilitates timely and continuous flood monitoring, providing vital data for emergency response teams to reduce loss of life and economic damages. It offers a significant reduction in the time required to generate flood maps, cutting down the manual processing time. Additionally, we present avenues for future research, including the integration of multimodal data sources and the development of robust deep learning architectures tailored specifically for flood detection tasks. Overall, our work contributes to the advancement of flood management strategies through innovative use of deep learning technologies.</li>
</ul>

<h3>Title: Comparative Analysis of Efficient Adapter-Based Fine-Tuning of State-of-the-Art Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Saad Mashkoor Siddiqui, Mohammad Ali Sheikh, Muhammad Aleem, Kajol R Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08271">https://arxiv.org/abs/2501.08271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08271">https://arxiv.org/pdf/2501.08271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08271]] Comparative Analysis of Efficient Adapter-Based Fine-Tuning of State-of-the-Art Transformer Models(https://arxiv.org/abs/2501.08271)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the efficacy of various adapter architectures on supervised binary classification tasks from the SuperGLUE benchmark as well as a supervised multi-class news category classification task from Kaggle. Specifically, we compare classification performance and time complexity of three transformer models, namely DistilBERT, ELECTRA, and BART, using conventional fine-tuning as well as nine state-of-the-art (SoTA) adapter architectures. Our analysis reveals performance differences across adapter architectures, highlighting their ability to achieve comparable or better performance relative to fine-tuning at a fraction of the training time. Similar results are observed on the new classification task, further supporting our findings and demonstrating adapters as efficient and flexible alternatives to fine-tuning. This study provides valuable insights and guidelines for selecting and implementing adapters in diverse natural language processing (NLP) applications.</li>
</ul>

<h3>Title: Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing</h3>
<ul>
<li><strong>Authors: </strong>Pulkit Arora, Akbar Karimi, Lucie Flek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08276">https://arxiv.org/abs/2501.08276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08276">https://arxiv.org/pdf/2501.08276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08276]] Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing(https://arxiv.org/abs/2501.08276)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive performance in various NLP tasks. However, there are concerns about their reliability in different domains of linguistic variations. Many works have proposed robustness evaluation measures for local adversarial attacks, but we need globally robust models unbiased to different language styles. We take a broader approach to explore a wider range of variations across sociodemographic dimensions to perform structured reliability tests on the reasoning capacity of language models. We extend the SocialIQA dataset to create diverse paraphrased sets conditioned on sociodemographic styles. The assessment aims to provide a deeper understanding of LLMs in (a) their capability of generating demographic paraphrases with engineered prompts and (b) their reasoning capabilities in real-world, complex language scenarios. We also explore measures such as perplexity, explainability, and ATOMIC performance of paraphrases for fine-grained reliability analysis of LLMs on these sets. We find that demographic-specific paraphrasing significantly impacts the performance of language models, indicating that the subtleties of language variations remain a significant challenge. The code and dataset will be made available for reproducibility and future research.</li>
</ul>

<h3>Title: SmartEraser: Remove Anything from Images using Masked-Region Guidance</h3>
<ul>
<li><strong>Authors: </strong>Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08279">https://arxiv.org/abs/2501.08279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08279">https://arxiv.org/pdf/2501.08279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08279]] SmartEraser: Remove Anything from Images using Masked-Region Guidance(https://arxiv.org/abs/2501.08279)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Object removal has so far been dominated by the mask-and-inpaint paradigm, where the masked region is excluded from the input, leaving models relying on unmasked areas to inpaint the missing region. However, this approach lacks contextual information for the masked area, often resulting in unstable performance. In this work, we introduce SmartEraser, built with a new removing paradigm called Masked-Region Guidance. This paradigm retains the masked region in the input, using it as guidance for the removal process. It offers several distinct advantages: (a) it guides the model to accurately identify the object to be removed, preventing its regeneration in the output; (b) since the user mask often extends beyond the object itself, it aids in preserving the surrounding context in the final result. Leveraging this new paradigm, we present Syn4Removal, a large-scale object removal dataset, where instance segmentation data is used to copy and paste objects onto images as removal targets, with the original images serving as ground truths. Experimental results demonstrate that SmartEraser significantly outperforms existing methods, achieving superior performance in object removal, especially in complex scenes with intricate compositions.</li>
</ul>

<h3>Title: Decoding Interpretable Logic Rules from Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08281">https://arxiv.org/abs/2501.08281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08281">https://arxiv.org/pdf/2501.08281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08281]] Decoding Interpretable Logic Rules from Neural Networks(https://arxiv.org/abs/2501.08281)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>As deep neural networks continue to excel across various domains, their black-box nature has raised concerns about transparency and trust. In particular, interpretability has become increasingly essential for applications that demand high safety and knowledge rigor, such as drug discovery, autonomous driving, and genomics. However, progress in understanding even the simplest deep neural networks - such as fully connected networks - has been limited, despite their role as foundational elements in state-of-the-art models like ResNet and Transformer. In this paper, we address this challenge by introducing NeuroLogic, a novel approach for decoding interpretable logic rules from neural networks. NeuroLogic leverages neural activation patterns to capture the model's critical decision-making processes, translating them into logical rules represented by hidden predicates. Thanks to its flexible design in the grounding phase, NeuroLogic can be adapted to a wide range of neural networks. For simple fully connected neural networks, hidden predicates can be grounded in certain split patterns of original input features to derive decision-tree-like rules. For large, complex vision neural networks, NeuroLogic grounds hidden predicates into high-level visual concepts that are understandable to humans. Our empirical study demonstrates that NeuroLogic can extract global and interpretable rules from state-of-the-art models such as ResNet, a task at which existing work struggles. We believe NeuroLogic can help pave the way for understanding the black-box nature of neural networks.</li>
</ul>

<h3>Title: LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, Si Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08282">https://arxiv.org/abs/2501.08282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08282">https://arxiv.org/pdf/2501.08282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08282]] LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding(https://arxiv.org/abs/2501.08282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal large language models (MLLMs) have shown promising results, yet existing approaches struggle to effectively handle both temporal and spatial localization simultaneously. This challenge stems from two key issues: first, incorporating spatial-temporal localization introduces a vast number of coordinate combinations, complicating the alignment of linguistic and visual coordinate representations; second, encoding fine-grained temporal and spatial information during video feature compression is inherently difficult. To address these issues, we propose LLaVA-ST, a MLLM for fine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we propose Language-Aligned Positional Embedding, which embeds the textual coordinate special token into the visual space, simplifying the alignment of fine-grained spatial-temporal correspondences. Additionally, we design the Spatial-Temporal Packer, which decouples the feature compression of temporal and spatial resolutions into two distinct point-to-region attention processing streams. Furthermore, we propose ST-Align dataset with 4.3M training samples for fine-grained spatial-temporal multimodal understanding. With ST-align, we present a progressive training pipeline that aligns the visual and textual feature through sequential coarse-to-fine this http URL, we introduce an ST-Align benchmark to evaluate spatial-temporal interleaved fine-grained understanding tasks, which include Spatial-Temporal Video Grounding (STVG) , Event Localization and Captioning (ELC) and Spatial Video Grounding (SVG). LLaVA-ST achieves outstanding performance on 11 benchmarks requiring fine-grained temporal, spatial, or spatial-temporal interleaving multimodal understanding. Our code, data and benchmark will be released at Our code, data and benchmark will be released at this https URL .</li>
</ul>

<h3>Title: HALoGEN: Fantastic LLM Hallucinations and Where to Find Them</h3>
<ul>
<li><strong>Authors: </strong>Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08292">https://arxiv.org/abs/2501.08292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08292">https://arxiv.org/pdf/2501.08292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08292]] HALoGEN: Fantastic LLM Hallucinations and Where to Find Them(https://arxiv.org/abs/2501.08292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models.</li>
</ul>

<h3>Title: LayerAnimate: Layer-specific Control for Animation</h3>
<ul>
<li><strong>Authors: </strong>Yuxue Yang, Lue Fan, Zuzen Lin, Feng Wang, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08295">https://arxiv.org/abs/2501.08295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08295">https://arxiv.org/pdf/2501.08295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08295]] LayerAnimate: Layer-specific Control for Animation(https://arxiv.org/abs/2501.08295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Animated video separates foreground and background elements into layers, with distinct processes for sketching, refining, coloring, and in-betweening. Existing video generation methods typically treat animation as a monolithic data domain, lacking fine-grained control over individual layers. In this paper, we introduce LayerAnimate, a novel architectural approach that enhances fine-grained control over individual animation layers within a video diffusion model, allowing users to independently manipulate foreground and background elements in distinct layers. To address the challenge of limited layer-specific data, we propose a data curation pipeline that features automated element segmentation, motion-state hierarchical merging, and motion coherence refinement. Through quantitative and qualitative comparisons, and user study, we demonstrate that LayerAnimate outperforms current methods in terms of animation quality, control precision, and usability, making it an ideal tool for both professional animators and amateur enthusiasts. This framework opens up new possibilities for layer-specific animation applications and creative flexibility. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Survey on Pedophile Attribution Techniques for Online Platforms</h3>
<ul>
<li><strong>Authors: </strong>Hiba Fallatah, Ching Suen, Olga Ormandjieva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08296">https://arxiv.org/abs/2501.08296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08296">https://arxiv.org/pdf/2501.08296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08296]] A Survey on Pedophile Attribution Techniques for Online Platforms(https://arxiv.org/abs/2501.08296)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Reliance on anonymity in social media has increased its popularity on these platforms among all ages. The availability of public Wi-Fi networks has facilitated a vast variety of online content, including social media applications. Although anonymity and ease of access can be a convenient means of communication for their users, it is difficult to manage and protect its vulnerable users against sexual predators. Using an automated identification system that can attribute predators to their text would make the solution more attainable. In this survey, we provide a review of the methods of pedophile attribution used in social media platforms. We examine the effect of the size of the suspect set and the length of the text on the task of attribution. Moreover, we review the most-used datasets, features, classification techniques and performance measures for attributing sexual predators. We found that few studies have proposed tools to mitigate the risk of online sexual predators, but none of them can provide suspect attribution. Finally, we list several open research problems.</li>
</ul>

<h3>Title: Polynomial Threshold Functions of Bounded Tree-Width: Some Explainability and Complexity Aspects</h3>
<ul>
<li><strong>Authors: </strong>Karine Chubarian, Johnny Joyce, Gyorgy Turan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08297">https://arxiv.org/abs/2501.08297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08297">https://arxiv.org/pdf/2501.08297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08297]] Polynomial Threshold Functions of Bounded Tree-Width: Some Explainability and Complexity Aspects(https://arxiv.org/abs/2501.08297)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The tree-width of a multivariate polynomial is the tree-width of the hypergraph with hyperedges corresponding to its terms. Multivariate polynomials of bounded tree-width have been studied by Makowsky and Meer as a new sparsity condition that allows for polynomial solvability of problems which are intractable in general. We consider a variation on this theme for Boolean variables. A representation of a Boolean function as the sign of a polynomial is called a polynomial threshold representation. We discuss Boolean functions representable as polynomial threshold functions of bounded tree-width and present two applications to Bayesian network classifiers, a probabilistic graphical model. Both applications are in Explainable Artificial Intelligence (XAI), the research area dealing with the black-box nature of many recent machine learning models. We also give a separation result between the representational power of positive and general polynomial threshold functions.</li>
</ul>

<h3>Title: Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers</h3>
<ul>
<li><strong>Authors: </strong>Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08303">https://arxiv.org/abs/2501.08303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08303">https://arxiv.org/pdf/2501.08303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08303]] Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers(https://arxiv.org/abs/2501.08303)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic future prediction is important for autonomous systems navigating dynamic environments. This paper introduces FUTURIST, a method for multimodal future semantic prediction that uses a unified and efficient visual sequence transformer architecture. Our approach incorporates a multimodal masked visual modeling objective and a novel masking mechanism designed for multimodal training. This allows the model to effectively integrate visible information from various modalities, improving prediction accuracy. Additionally, we propose a VAE-free hierarchical tokenization process, which reduces computational complexity, streamlines the training pipeline, and enables end-to-end training with high-resolution, multimodal inputs. We validate FUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performance in future semantic segmentation for both short- and mid-term forecasting. We provide the implementation code at this https URL .</li>
</ul>

<h3>Title: Benchmarking Graph Representations and Graph Neural Networks for Multivariate Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Wennuo Yang, Shiling Wu, Yuzhi Zhou, Weicheng Xie, Linlin Shen, Siyang Song</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08305">https://arxiv.org/abs/2501.08305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08305">https://arxiv.org/pdf/2501.08305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08305]] Benchmarking Graph Representations and Graph Neural Networks for Multivariate Time Series Classification(https://arxiv.org/abs/2501.08305)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Multivariate Time Series Classification (MTSC) enables the analysis if complex temporal data, and thus serves as a cornerstone in various real-world applications, ranging from healthcare to finance. Since the relationship among variables in MTS usually contain crucial cues, a large number of graph-based MTSC approaches have been proposed, as the graph topology and edges can explicitly represent relationships among variables (channels), where not only various MTS graph representation learning strategies but also different Graph Neural Networks (GNNs) have been explored. Despite such progresses, there is no comprehensive study that fairly benchmarks and investigates the performances of existing widely-used graph representation learning strategies/GNN classifiers in the application of different MTSC tasks. In this paper, we present the first benchmark which systematically investigates the effectiveness of the widely-used three node feature definition strategies, four edge feature learning strategies and five GNN architecture, resulting in 60 different variants for graph-based MTSC. These variants are developed and evaluated with a standardized data pipeline and training/validation/testing strategy on 26 widely-used suspensor MTSC datasets. Our experiments highlight that node features significantly influence MTSC performance, while the visualization of edge features illustrates why adaptive edge learning outperforms other edge feature learning methods. The code of the proposed benchmark is publicly available at \url{this https URL}.</li>
</ul>

<h3>Title: Diffusion Adversarial Post-Training for One-Step Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08316">https://arxiv.org/abs/2501.08316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08316">https://arxiv.org/pdf/2501.08316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08316]] Diffusion Adversarial Post-Training for One-Step Video Generation(https://arxiv.org/abs/2501.08316)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.</li>
</ul>

<h3>Title: Enhancing Automated Interpretability with Output-Centric Feature Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08319">https://arxiv.org/abs/2501.08319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08319">https://arxiv.org/pdf/2501.08319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08319]] Enhancing Automated Interpretability with Output-Centric Feature Descriptions(https://arxiv.org/abs/2501.08319)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary "unembedding" head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be "dead".</li>
</ul>

<h3>Title: Exploring Robustness of Multilingual LLMs on Real-World Noisy Data</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Aliakbarzadeh, Lucie Flek, Akbar Karimi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08322">https://arxiv.org/abs/2501.08322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08322">https://arxiv.org/pdf/2501.08322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08322]] Exploring Robustness of Multilingual LLMs on Real-World Noisy Data(https://arxiv.org/abs/2501.08322)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on Web data that might contain spelling errors made by humans. But do they become robust to similar real-world noise? In this paper, we investigate the effect of real-world spelling mistakes on the performance of 9 language models, with parameters ranging from 0.2B to 13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), Name Entity Recognition (NER), and Intent Classification (IC). We perform our experiments on 6 different languages and build a dictionary of real-world noise for them using the Wikipedia edit history. We show that the performance gap of the studied models on the clean and noisy test data averaged across all the datasets and languages ranges from 2.3 to 4.3 absolute percentage points. In addition, mT5 models, in general, show more robustness compared to BLOOM, Falcon, and BERT-like models. In particular, mT5 (13B), was the most robust on average overall, across the 3 tasks, and in 4 of the 6 languages.</li>
</ul>

<h3>Title: GameFactory: Creating New Games with Generative Interactive Videos</h3>
<ul>
<li><strong>Authors: </strong>Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08325">https://arxiv.org/abs/2501.08325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08325">https://arxiv.org/pdf/2501.08325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08325]] GameFactory: Creating New Games with Generative Interactive Videos(https://arxiv.org/abs/2501.08325)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative game engines have the potential to revolutionize game development by autonomously creating new content and reducing manual workload. However, existing video-based game generation methods fail to address the critical challenge of scene generalization, limiting their applicability to existing games with fixed styles and scenes. In this paper, we present GameFactory, a framework focused on exploring scene generalization in game video generation. To enable the creation of entirely new and diverse games, we leverage pre-trained video diffusion models trained on open-domain video data. To bridge the domain gap between open-domain priors and small-scale game dataset, we propose a multi-phase training strategy that decouples game style learning from action control, preserving open-domain generalization while achieving action controllability. Using Minecraft as our data source, we release GF-Minecraft, a high-quality and diversity action-annotated video dataset for research. Furthermore, we extend our framework to enable autoregressive action-controllable game video generation, allowing the production of unlimited-length interactive game videos. Experimental results demonstrate that GameFactory effectively generates open-domain, diverse, and action-controllable game videos, representing a significant step forward in AI-driven game generation. Our dataset and project page are publicly available at \url{this https URL}.</li>
</ul>

<h3>Title: Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks</h3>
<ul>
<li><strong>Authors: </strong>Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08326">https://arxiv.org/abs/2501.08326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08326">https://arxiv.org/pdf/2501.08326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08326]] Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks(https://arxiv.org/abs/2501.08326)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present Omni-RGPT, a multimodal large language model designed to facilitate region-level comprehension for both images and videos. To achieve consistent region representation across spatio-temporal dimensions, we introduce Token Mark, a set of tokens highlighting the target regions within the visual feature space. These tokens are directly embedded into spatial regions using region prompts (e.g., boxes or masks) and simultaneously incorporated into the text prompt to specify the target, establishing a direct connection between visual and text tokens. To further support robust video understanding without requiring tracklets, we introduce an auxiliary task that guides Token Mark by leveraging the consistency of the tokens, enabling stable region interpretation across the video. Additionally, we introduce a large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT achieves state-of-the-art results on image and video-based commonsense reasoning benchmarks while showing strong performance in captioning and referring expression comprehension tasks.</li>
</ul>

<h3>Title: PokerBench: Training Large Language Models to become Professional Poker Players</h3>
<ul>
<li><strong>Authors: </strong>Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08328">https://arxiv.org/abs/2501.08328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08328">https://arxiv.org/pdf/2501.08328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08328]] PokerBench: Training Large Language Models to become Professional Poker Players(https://arxiv.org/abs/2501.08328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. The dataset and code will be made available at: \url{this https URL}.</li>
</ul>

<h3>Title: Predicting 4D Hand Trajectory from Monocular Videos</h3>
<ul>
<li><strong>Authors: </strong>Yufei Ye, Yao Feng, Omid Taheri, Haiwen Feng, Shubham Tulsiani, Michael J. Black</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08329">https://arxiv.org/abs/2501.08329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08329">https://arxiv.org/pdf/2501.08329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08329]] Predicting 4D Hand Trajectory from Monocular Videos(https://arxiv.org/abs/2501.08329)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present HaPTIC, an approach that infers coherent 4D hand trajectories from monocular videos. Current video-based hand pose reconstruction methods primarily focus on improving frame-wise 3D pose using adjacent frames rather than studying consistent 4D hand trajectories in space. Despite the additional temporal cues, they generally underperform compared to image-based methods due to the scarcity of annotated video data. To address these issues, we repurpose a state-of-the-art image-based transformer to take in multiple frames and directly predict a coherent trajectory. We introduce two types of lightweight attention layers: cross-view self-attention to fuse temporal information, and global cross-attention to bring in larger spatial context. Our method infers 4D hand trajectories similar to the ground truth while maintaining strong 2D reprojection alignment. We apply the method to both egocentric and allocentric videos. It significantly outperforms existing methods in global trajectory accuracy while being comparable to the state-of-the-art in single-image pose estimation. Project website: this https URL</li>
</ul>

<h3>Title: Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</h3>
<ul>
<li><strong>Authors: </strong>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08331">https://arxiv.org/abs/2501.08331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08331">https://arxiv.org/pdf/2501.08331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08331]] Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise(https://arxiv.org/abs/2501.08331)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: this https URL source code and model checkpoints are available on GitHub: this https URL.</li>
</ul>

<h3>Title: MangaNinja: Line Art Colorization with Precise Reference Following</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Liu, Ka Leong Cheng, Xi Chen, Jie Xiao, Hao Ouyang, Kai Zhu, Yu Liu, Yujun Shen, Qifeng Chen, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08332">https://arxiv.org/abs/2501.08332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08332">https://arxiv.org/pdf/2501.08332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08332]] MangaNinja: Line Art Colorization with Precise Reference Following(https://arxiv.org/abs/2501.08332)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Derived from diffusion models, MangaNinjia specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases, cross-character colorization, multi-reference harmonization, beyond the reach of existing algorithms.</li>
</ul>

<h3>Title: DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hyeonwoo Kim, Sangwon Beak, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.08333">https://arxiv.org/abs/2501.08333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.08333">https://arxiv.org/pdf/2501.08333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.08333]] DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models(https://arxiv.org/abs/2501.08333)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Understanding the ability of humans to use objects is crucial for AI to improve daily life. Existing studies for learning such ability focus on human-object patterns (e.g., contact, spatial relation, orientation) in static situations, and learning Human-Object Interaction (HOI) patterns over time (i.e., movement of human and object) is relatively less explored. In this paper, we introduce a novel type of affordance named Dynamic Affordance. For a given input 3D object mesh, we learn dynamic affordance which models the distribution of both (1) human motion and (2) human-guided object pose during interactions. As a core idea, we present a method to learn the 3D dynamic affordance from synthetically generated 2D videos, leveraging a pre-trained video diffusion model. Specifically, we propose a pipeline that first generates 2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOI samples. Once we generate diverse 4D HOI samples on various target objects, we train our DAViD, where we present a method based on the Low-Rank Adaptation (LoRA) module for pre-trained human motion diffusion model (MDM) and an object pose diffusion model with human pose guidance. Our motion diffusion model is extended for multi-object interactions, demonstrating the advantage of our pipeline with LoRA for combining the concepts of object usage. Through extensive experiments, we demonstrate our DAViD outperforms the baselines in generating human motion with HOIs.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
