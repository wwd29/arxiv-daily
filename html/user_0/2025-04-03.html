<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-03</h1>
<h3>Title: Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yufei He, Xucong Zhang, Arno H. A. Stienen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01024">https://arxiv.org/abs/2504.01024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01024">https://arxiv.org/pdf/2504.01024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01024]] Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks(https://arxiv.org/abs/2504.01024)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Human intention detection with hand motion prediction is critical to drive the upper-extremity assistive robots in neurorehabilitation applications. However, the traditional methods relying on physiological signal measurement are restrictive and often lack environmental context. We propose a novel approach that predicts future sequences of both hand poses and joint positions. This method integrates gaze information, historical hand motion sequences, and environmental object data, adapting dynamically to the assistive needs of the patient without prior knowledge of the intended object for grasping. Specifically, we use a vector-quantized variational autoencoder for robust hand pose encoding with an autoregressive generative transformer for effective hand motion sequence prediction. We demonstrate the usability of these novel techniques in a pilot study with healthy subjects. To train and evaluate the proposed method, we collect a dataset consisting of various types of grasp actions on different objects from multiple subjects. Through extensive experiments, we demonstrate that the proposed method can successfully predict sequential hand movement. Especially, the gaze information shows significant enhancements in prediction capabilities, particularly with fewer input frames, highlighting the potential of the proposed method for real-world applications.</li>
</ul>

<h3>Title: Coarse-to-Fine Learning for Multi-Pipette Localisation in Robot-Assisted In Vivo Patch-Clamp</h3>
<ul>
<li><strong>Authors: </strong>Lan Wei, Gema Vera Gonzalez, Phatsimo Kgwarae, Alexander Timms, Denis Zahorovsky, Simon Schultz, Dandan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01044">https://arxiv.org/abs/2504.01044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01044">https://arxiv.org/pdf/2504.01044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01044]] Coarse-to-Fine Learning for Multi-Pipette Localisation in Robot-Assisted In Vivo Patch-Clamp(https://arxiv.org/abs/2504.01044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>In vivo image-guided multi-pipette patch-clamp is essential for studying cellular interactions and network dynamics in neuroscience. However, current procedures mainly rely on manual expertise, which limits accessibility and scalability. Robotic automation presents a promising solution, but achieving precise real-time detection of multiple pipettes remains a challenge. Existing methods focus on ex vivo experiments or single pipette use, making them inadequate for in vivo multi-pipette scenarios. To address these challenges, we propose a heatmap-augmented coarse-to-fine learning technique to facilitate multi-pipette real-time localisation for robot-assisted in vivo patch-clamp. More specifically, we introduce a Generative Adversarial Network (GAN)-based module to remove background noise and enhance pipette visibility. We then introduce a two-stage Transformer model that starts with predicting the coarse heatmap of the pipette tips, followed by the fine-grained coordination regression module for precise tip localisation. To ensure robust training, we use the Hungarian algorithm for optimal matching between the predicted and actual locations of tips. Experimental results demonstrate that our method achieved > 98% accuracy within 10 {\mu}m, and > 89% accuracy within 5 {\mu}m for the localisation of multi-pipette tips. The average MSE is 2.52 {\mu}m.</li>
</ul>

<h3>Title: Predicting Movie Production Years through Facial Recognition of Actors with Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Asraa Muayed Abdalah, Noor Redha Alkazaz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01047">https://arxiv.org/abs/2504.01047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01047">https://arxiv.org/pdf/2504.01047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01047]] Predicting Movie Production Years through Facial Recognition of Actors with Machine Learning(https://arxiv.org/abs/2504.01047)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This study used machine learning algorithms to identify actors and extract the age of actors from images taken randomly from movies. The use of images taken from Arab movies includes challenges such as non-uniform lighting, different and multiple poses for the actors and multiple elements with the actor or a group of actors. Additionally, the use of make-up, wigs, beards, and wearing different accessories and costumes made it difficult for the system to identify the personality of the same actor. The Arab Actors Dataset-AAD comprises 574 images sourced from various movies, encompassing both black and white as well as color compositions. The images depict complete scenes or fragments thereof. Multiple models were employed for feature extraction, and diverse machine learning algorithms were utilized during the classification and prediction stages to determine the most effective algorithm for handling such image types. The study demonstrated the effectiveness of the Logistic Regression model exhibited the best performance compared to other models in the training phase, as evidenced by its AUC, precision, CA and F1score values of 99%, 86%, 85.5% and 84.2% respectively. The findings of this study can be used to improve the precision and reliability of facial recognition technology for various uses as with movies search services, movie suggestion algorithms, and genre classification of movies.</li>
</ul>

<h3>Title: How does Watermarking Affect Visual Language Models in Document Understanding?</h3>
<ul>
<li><strong>Authors: </strong>Chunxue Xu, Yiwei Wang, Bryan Hooi, Yujun Cai, Songze Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01048">https://arxiv.org/abs/2504.01048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01048">https://arxiv.org/pdf/2504.01048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01048]] How does Watermarking Affect Visual Language Models in Document Understanding?(https://arxiv.org/abs/2504.01048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>Visual Language Models (VLMs) have become foundational models for document understanding tasks, widely used in the processing of complex multimodal documents across domains such as finance, law, and academia. However, documents often contain noise-like information, such as watermarks, which inevitably leads us to inquire: \emph{Do watermarks degrade the performance of VLMs in document understanding?} To address this, we propose a novel evaluation framework to investigate the effect of visible watermarks on VLMs performance. We takes into account various factors, including different types of document data, the positions of watermarks within documents and variations in watermark content. Our experimental results reveal that VLMs performance can be significantly compromised by watermarks, with performance drop rates reaching up to 36\%. We discover that \emph{scattered} watermarks cause stronger interference than centralized ones, and that \emph{semantic contents} in watermarks creates greater disruption than simple visual occlusion. Through attention mechanism analysis and embedding similarity examination, we find that the performance drops are mainly attributed to that watermarks 1) force widespread attention redistribution, and 2) alter semantic representation in the embedding space. Our research not only highlights significant challenges in deploying VLMs for document understanding, but also provides insights towards developing robust inference mechanisms on watermarked documents.</li>
</ul>

<h3>Title: SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Bingxin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01049">https://arxiv.org/abs/2504.01049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01049">https://arxiv.org/pdf/2504.01049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01049]] SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual Question Answering(https://arxiv.org/abs/2504.01049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Multimodal models integrating speech and vision hold significant potential for advancing human-computer interaction, particularly in Speech-Based Visual Question Answering (SBVQA) where spoken questions about images require direct audio-visual understanding. Existing approaches predominantly focus on text-visual integration, leaving speech-visual modality gaps underexplored due to their inherent heterogeneity. To this end, we introduce SViQA, a unified speech-vision model that directly processes spoken questions without text transcription. Building upon the LLaVA architecture, our framework bridges auditory and visual modalities through two key innovations: (1) end-to-end speech feature extraction eliminating intermediate text conversion, and (2) cross-modal alignment optimization enabling effective fusion of speech signals with visual content. Extensive experimental results on the SBVQA benchmark demonstrate the proposed SViQA's state-of-the-art performance, achieving 75.62% accuracy, and competitive multimodal generalization. Leveraging speech-text mixed input boosts performance to 78.85%, a 3.23% improvement over pure speech input, highlighting SViQA's enhanced robustness and effective cross-modal attention alignment.</li>
</ul>

<h3>Title: ShieldGemma 2: Robust and Tractable Image Content Moderation</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Zeng, Dana Kurniawan, Ryan Mullins, Yuchi Liu, Tamoghna Saha, Dirichi Ike-Njoku, Jindong Gu, Yiwen Song, Cai Xu, Jingjing Zhou, Aparna Joshi, Shravan Dheep, Mani Malek, Hamid Palangi, Joon Baek, Rick Pereira, Karthik Narasimhan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01081">https://arxiv.org/abs/2504.01081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01081">https://arxiv.org/pdf/2504.01081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01081]] ShieldGemma 2: Robust and Tractable Image Content Moderation(https://arxiv.org/abs/2504.01081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce ShieldGemma 2, a 4B parameter image content moderation model built on Gemma 3. This model provides robust safety risk predictions across the following key harm categories: Sexually Explicit, Violence \& Gore, and Dangerous Content for synthetic images (e.g. output of any image generation model) and natural images (e.g. any image input to a Vision-Language Model). We evaluated on both internal and external benchmarks to demonstrate state-of-the-art performance compared to LlavaGuard \citep{helff2024llavaguard}, GPT-4o mini \citep{hurst2024gpt}, and the base Gemma 3 model \citep{gemma_2025} based on our policies. Additionally, we present a novel adversarial data generation pipeline which enables a controlled, diverse, and robust image generation. ShieldGemma 2 provides an open image moderation tool to advance multimodal safety and responsible AI development.</li>
</ul>

<h3>Title: MPCritic: A plug-and-play MPC architecture for reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Nathan P. Lawrence, Thomas Banker, Ali Mesbah</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01086">https://arxiv.org/abs/2504.01086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01086">https://arxiv.org/pdf/2504.01086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01086]] MPCritic: A plug-and-play MPC architecture for reinforcement learning(https://arxiv.org/abs/2504.01086)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The reinforcement learning (RL) and model predictive control (MPC) communities have developed vast ecosystems of theoretical approaches and computational tools for solving optimal control problems. Given their conceptual similarities but differing strengths, there has been increasing interest in synergizing RL and MPC. However, existing approaches tend to be limited for various reasons, including computational cost of MPC in an RL algorithm and software hurdles towards seamless integration of MPC and RL tools. These challenges often result in the use of "simple" MPC schemes or RL algorithms, neglecting the state-of-the-art in both areas. This paper presents MPCritic, a machine learning-friendly architecture that interfaces seamlessly with MPC tools. MPCritic utilizes the loss landscape defined by a parameterized MPC problem, focusing on "soft" optimization over batched training steps; thereby updating the MPC parameters while avoiding costly minimization and parametric sensitivities. Since the MPC structure is preserved during training, an MPC agent can be readily used for online deployment, where robust constraint satisfaction is paramount. We demonstrate the versatility of MPCritic, in terms of MPC architectures and RL algorithms that it can accommodate, on classic control benchmarks.</li>
</ul>

<h3>Title: Hard-constraining Neumann boundary conditions in physics-informed neural networks via Fourier feature embeddings</h3>
<ul>
<li><strong>Authors: </strong>Christopher Straub, Philipp Brendel, Vlad Medvedev, Andreas Rosskopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01093">https://arxiv.org/abs/2504.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01093">https://arxiv.org/pdf/2504.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01093]] Hard-constraining Neumann boundary conditions in physics-informed neural networks via Fourier feature embeddings(https://arxiv.org/abs/2504.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel approach to hard-constrain Neumann boundary conditions in physics-informed neural networks (PINNs) using Fourier feature embeddings. Neumann boundary conditions are used to described critical processes in various application, yet they are more challenging to hard-constrain in PINNs than Dirichlet conditions. Our method employs specific Fourier feature embeddings to directly incorporate Neumann boundary conditions into the neural network's architecture instead of learning them. The embedding can be naturally extended by high frequency modes to better capture high frequency phenomena. We demonstrate the efficacy of our approach through experiments on a diffusion problem, for which our method outperforms existing hard-constraining methods and classical PINNs, particularly in multiscale and high frequency scenarios.</li>
</ul>

<h3>Title: Efficient State Estimation of a Networked FlipIt Model</h3>
<ul>
<li><strong>Authors: </strong>Brandon Collins, Thomas Gherna, Keith Paarporn, Shouhuai Xu, Philip N. Brown</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01096">https://arxiv.org/abs/2504.01096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01096">https://arxiv.org/pdf/2504.01096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01096]] Efficient State Estimation of a Networked FlipIt Model(https://arxiv.org/abs/2504.01096)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>The Boolean Kalman Filter and associated Boolean Dynamical System Theory have been proposed to study the spread of infection on computer networks. Such models feature a network where attacks propagate through, an intrusion detection system that provides noisy signals of the true state of the network, and the capability of the defender to clean a subset of computers at any time. The Boolean Kalman Filter has been used to solve the optimal estimation problem, by estimating the hidden true state given the attack-defense dynamics and noisy observations. However, this algorithm is infeasible because it runs in exponential time and space with respect to the network size. We address this feasibility problem by proposing a mean-field estimation approach, which is inspired by the epidemic modeling literature. Although our approach is heuristic, we prove that our estimator exactly matches the optimal estimator in certain non-trivial cases. We conclude by using simulations to show both the run-time improvement and estimation accuracy of our approach.</li>
</ul>

<h3>Title: ffstruc2vec: Flat, Flexible and Scalable Learning of Node Representations from Structural Identities</h3>
<ul>
<li><strong>Authors: </strong>Mario Heidrich, Jeffrey Heidemann, Rüdiger Buchkremer, Gonzalo Wandosell Fernández de Bobadilla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01122">https://arxiv.org/abs/2504.01122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01122">https://arxiv.org/pdf/2504.01122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01122]] ffstruc2vec: Flat, Flexible and Scalable Learning of Node Representations from Structural Identities(https://arxiv.org/abs/2504.01122)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Node embedding refers to techniques that generate low-dimensional vector representations of nodes in a graph while preserving specific properties of the nodes. A key challenge in the field is developing scalable methods that can preserve structural properties suitable for the required types of structural patterns of a given downstream application task. While most existing methods focus on preserving node proximity, those that do preserve structural properties often lack the flexibility to preserve various types of structural patterns required by downstream application tasks. This paper introduces ffstruc2vec, a scalable deep-learning framework for learning node embedding vectors that preserve structural identities. Its flat, efficient architecture allows high flexibility in capturing diverse types of structural patterns, enabling broad adaptability to various downstream application tasks. The proposed framework significantly outperforms existing approaches across diverse unsupervised and supervised tasks in practical applications. Moreover, ffstruc2vec enables explainability by quantifying how individual structural patterns influence task outcomes, providing actionable interpretation. To our knowledge, no existing framework combines this level of flexibility, scalability, and structural interpretability, underscoring its unique capabilities.</li>
</ul>

<h3>Title: Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Liu, Priyanka Dey, Zhenyu Zhao, Jen-tse Huang, Rahul Gupta, Yang Liu, Jieyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01127">https://arxiv.org/abs/2504.01127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01127">https://arxiv.org/pdf/2504.01127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01127]] Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Metacognitive Cultural Intelligence with CQ-Bench(https://arxiv.org/abs/2504.01127)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Cultural Intelligence (CQ) refers to the ability to understand unfamiliar cultural contexts-a crucial skill for large language models (LLMs) to effectively engage with globally diverse users. While existing research often focuses on explicitly stated cultural norms, such approaches fail to capture the subtle, implicit values that underlie real-world conversations. To address this gap, we introduce CQ-Bench, a benchmark specifically designed to assess LLMs' capability to infer implicit cultural values from natural conversational contexts. We generate a multi-character conversation-based stories dataset using values from the World Value Survey and GlobalOpinions datasets, with topics including ethical, religious, social, and political. Our dataset construction pipeline includes rigorous validation procedures-incorporation, consistency, and implicitness checks-using GPT-4o, with 98.2% human-model agreement in the final validation. Our benchmark consists of three tasks of increasing complexity: attitude detection, value selection, and value extraction. We find that while o1 and Deepseek-R1 models reach human-level performance in value selection (0.809 and 0.814), they still fall short in nuanced attitude detection, with F1 scores of 0.622 and 0.635, respectively. In the value extraction task, GPT-4o-mini and o3-mini score 0.602 and 0.598, highlighting the difficulty of open-ended cultural reasoning. Notably, fine-tuning smaller models (e.g., LLaMA-3.2-3B) on only 500 culturally rich examples improves performance by over 10%, even outperforming stronger baselines (o3-mini) in some cases. Using CQ-Bench, we provide insights into the current challenges in LLMs' CQ research and suggest practical pathways for enhancing LLMs' cross-cultural reasoning abilities.</li>
</ul>

<h3>Title: RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety</h3>
<ul>
<li><strong>Authors: </strong>Andrei Dumitriu, Florin Tatui, Florin Miron, Aakash Ralhan, Radu Tudor Ionescu, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01128">https://arxiv.org/abs/2504.01128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01128">https://arxiv.org/pdf/2504.01128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01128]] RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety(https://arxiv.org/abs/2504.01128)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Rip currents are strong, localized and narrow currents of water that flow outwards into the sea, causing numerous beach-related injuries and fatalities worldwide. Accurate identification of rip currents remains challenging due to their amorphous nature and the lack of annotated data, which often requires expert knowledge. To address these issues, we present RipVIS, a large-scale video instance segmentation benchmark explicitly designed for rip current segmentation. RipVIS is an order of magnitude larger than previous datasets, featuring $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$ frames) are with rip currents, collected from various sources, including drones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse visual contexts, such as wave-breaking patterns, sediment flows, and water color variations, across multiple global locations, including USA, Mexico, Costa Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New Zealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic scenarios, supplemented by an additional $34$ videos ($48,800$ frames) without rip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade Mask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip current segmentation. Results are reported in terms of multiple metrics, with a particular focus on the $F_2$ score to prioritize recall and reduce false negatives. To enhance segmentation performance, we introduce a novel post-processing step based on Temporal Confidence Aggregation (TCA). RipVIS aims to set a new standard for rip current segmentation, contributing towards safer beach environments. We offer a benchmark website to share data, models, and results with the research community, encouraging ongoing collaboration and future contributions, at this https URL.</li>
</ul>

<h3>Title: Performative Drift Resistant Classification Using Generative Domain Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Maciej Makowski, Brandon Gower-Winter, Georg Krempl</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01135">https://arxiv.org/abs/2504.01135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01135">https://arxiv.org/pdf/2504.01135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01135]] Performative Drift Resistant Classification Using Generative Domain Adversarial Networks(https://arxiv.org/abs/2504.01135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Performative Drift is a special type of Concept Drift that occurs when a model's predictions influence the future instances the model will encounter. In these settings, retraining is not always feasible. In this work, we instead focus on drift understanding as a method for creating drift-resistant classifiers. To achieve this, we introduce the Generative Domain Adversarial Network (GDAN) which combines both Domain and Generative Adversarial Networks. Using GDAN, domain-invariant representations of incoming data are created and a generative network is used to reverse the effects of performative drift. Using semi-real and synthetic data generators, we empirically evaluate GDAN's ability to provide drift-resistant classification. Initial results are promising with GDAN limiting performance degradation over several timesteps. Additionally, GDAN's generative network can be used in tandem with other models to limit their performance degradation in the presence of performative drift. Lastly, we highlight the relationship between model retraining and the unpredictability of performative drift, providing deeper insights into the challenges faced when using traditional Concept Drift mitigation strategies in the performative setting.</li>
</ul>

<h3>Title: Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Guy Kaplan, Michael Toker, Yuval Reif, Yonatan Belinkov, Roy Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01137">https://arxiv.org/abs/2504.01137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01137">https://arxiv.org/pdf/2504.01137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01137]] Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models(https://arxiv.org/abs/2504.01137)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) models often suffer from issues such as semantic leakage, incorrect feature binding, and omissions of key concepts in the generated image. This work studies these phenomena by looking into the role of information flow between textual token representations. To this end, we generate images by applying the diffusion component on a subset of contextual token representations in a given prompt and observe several interesting phenomena. First, in many cases, a word or multiword expression is fully represented by one or two tokens, while other tokens are redundant. For example, in "San Francisco's Golden Gate Bridge", the token "gate" alone captures the full expression. We demonstrate the redundancy of these tokens by removing them after textual encoding and generating an image from the resulting representation. Surprisingly, we find that this process not only maintains image generation performance but also reduces errors by 21\% compared to standard generation. We then show that information can also flow between different expressions in a sentence, which often leads to semantic leakage. Based on this observation, we propose a simple, training-free method to mitigate semantic leakage: replacing the leaked item's representation after the textual encoding with its uncontextualized representation. Remarkably, this simple approach reduces semantic leakage by 85\%. Overall, our work provides a comprehensive analysis of information flow across textual tokens in T2I models, offering both novel insights and practical benefits.</li>
</ul>

<h3>Title: MaLAware: Automating the Comprehension of Malicious Software Behaviours using Large Language Models (LLMs)</h3>
<ul>
<li><strong>Authors: </strong>Bikash Saha, Nanda Rani, Sandeep Kumar Shukla</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01145">https://arxiv.org/abs/2504.01145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01145">https://arxiv.org/pdf/2504.01145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01145]] MaLAware: Automating the Comprehension of Malicious Software Behaviours using Large Language Models (LLMs)(https://arxiv.org/abs/2504.01145)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current malware (malicious software) analysis tools focus on detection and family classification but fail to provide clear and actionable narrative insights into the malignant activity of the malware. Therefore, there is a need for a tool that translates raw malware data into human-readable descriptions. Developing such a tool accelerates incident response, reduces malware analysts' cognitive load, and enables individuals having limited technical expertise to understand malicious software behaviour. With this objective, we present MaLAware, which automatically summarizes the full spectrum of malicious activity of malware executables. MaLAware processes Cuckoo Sandbox-generated reports using large language models (LLMs) to correlate malignant activities and generate concise summaries explaining malware behaviour. We evaluate the tool's performance on five open-source LLMs. The evaluation uses the human-written malware behaviour description dataset as ground truth. The model's performance is measured using 11 extensive performance metrics, which boosts the confidence of MaLAware's effectiveness. The current version of the tool, i.e., MaLAware, supports Qwen2.5-7B, Llama2-7B, Llama3.1-8B, Mistral-7B, and Falcon-7B, along with the quantization feature for resource-constrained environments. MaLAware lays a foundation for future research in malware behavior explanation, and its extensive evaluation demonstrates LLMs' ability to narrate malware behavior in an actionable and comprehensive manner.</li>
</ul>

<h3>Title: Efficient n-body simulations using physics informed graph neural networks</h3>
<ul>
<li><strong>Authors: </strong>Víctor Ramos-Osuna, Alberto Díaz-Álvarez, Raúl Lara-Cabrera</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01169">https://arxiv.org/abs/2504.01169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01169">https://arxiv.org/pdf/2504.01169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01169]] Efficient n-body simulations using physics informed graph neural networks(https://arxiv.org/abs/2504.01169)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach for accelerating n-body simulations by integrating a physics-informed graph neural networks (GNN) with traditional numerical methods. Our method implements a leapfrog-based simulation engine to generate datasets from diverse astrophysical scenarios which are then transformed into graph representations. A custom-designed GNN is trained to predict particle accelerations with high precision. Experiments, conducted on 60 training and 6 testing simulations spanning from 3 to 500 bodies over 1000 time steps, demonstrate that the proposed model achieves extremely low prediction errors-loss values while maintaining robust long-term stability, with accumulated errors in position, velocity, and acceleration remaining insignificant. Furthermore, our method yields a modest speedup of approximately 17% over conventional simulation techniques. These results indicate that the integration of deep learning with traditional physical simulation methods offers a promising pathway to significantly enhance computational efficiency without compromising accuracy.</li>
</ul>

<h3>Title: Neural Approaches to SAT Solving: Design Choices and Interpretability</h3>
<ul>
<li><strong>Authors: </strong>David Mojžíšek, Jan Hůla, Ziwei Li, Ziyu Zhou, Mikoláš Janota</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01173">https://arxiv.org/abs/2504.01173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01173">https://arxiv.org/pdf/2504.01173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01173]] Neural Approaches to SAT Solving: Design Choices and Interpretability(https://arxiv.org/abs/2504.01173)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>In this contribution, we provide a comprehensive evaluation of graph neural networks applied to Boolean satisfiability problems, accompanied by an intuitive explanation of the mechanisms enabling the model to generalize to different instances. We introduce several training improvements, particularly a novel closest assignment supervision method that dynamically adapts to the model's current state, significantly enhancing performance on problems with larger solution spaces. Our experiments demonstrate the suitability of variable-clause graph representations with recurrent neural network updates, which achieve good accuracy on SAT assignment prediction while reducing computational demands. We extend the base graph neural network into a diffusion model that facilitates incremental sampling and can be effectively combined with classical techniques like unit propagation. Through analysis of embedding space patterns and optimization trajectories, we show how these networks implicitly perform a process very similar to continuous relaxations of MaxSAT, offering an interpretable view of their reasoning process. This understanding guides our design choices and explains the ability of recurrent architectures to scale effectively at inference time beyond their training distribution, which we demonstrate with test-time scaling experiments.</li>
</ul>

<h3>Title: $μ$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zian Su, Ziyang Huang, Kaiyuan Zhang, Xiangyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01196">https://arxiv.org/abs/2504.01196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01196">https://arxiv.org/pdf/2504.01196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01196]] $μ$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models(https://arxiv.org/abs/2504.01196)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful knowledge bases yet are limited by static training data, leading to issues such as hallucinations and safety risks. Editing a model's internal knowledge through the locate-and-edit paradigm has proven a cost-effective alternative to retraining, though current unstructured approaches, especially window-based autoregressive methods, often disrupt the causal dependency between early memory updates and later output tokens. In this work, we first theoretically analyze these limitations and then introduce Matryoshka Unstructured Knowledge Editing ($\mu$KE), a novel memory update mechanism that preserves such dependencies via a Matryoshka-style objective and adaptive loss coefficients. Empirical evaluations on two models across four benchmarks demonstrate that $\mu$KE improves edit efficacy by up to 12.33% over state-of-the-art methods, and remain robust when applied to diverse formatted edits, underscoring its potential for effective unstructured knowledge editing in LLMs.</li>
</ul>

<h3>Title: Medical large language models are easily distracted</h3>
<ul>
<li><strong>Authors: </strong>Krithik Vishwanath, Anton Alyakin, Daniel Alexander Alber, Jin Vivian Lee, Douglas Kondziolka, Eric Karl Oermann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01201">https://arxiv.org/abs/2504.01201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01201">https://arxiv.org/pdf/2504.01201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01201]] Medical large language models are easily distracted(https://arxiv.org/abs/2504.01201)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.</li>
</ul>

<h3>Title: Global explainability of a deep abstaining classifier</h3>
<ul>
<li><strong>Authors: </strong>Sayera Dhaubhadel (1 and 2), Jamaludin Mohd-Yusof (1), Benjamin H. McMahon (1), Trilce Estrada (2), Kumkum Ganguly (1), Adam Spannaus (3), John P. Gounley (3), Xiao-Cheng Wu (4), Eric B. Durbin (5), Heidi A. Hanson (3), Tanmoy Bhattacharya (1) ((1) Los Alamos National Laboratory, (2) University of New Mexico, (3) Oak Ridge National Laboratory, (4) Louisiana Tumor Registry, (5) Kentucky Cancer Registry)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01202">https://arxiv.org/abs/2504.01202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01202">https://arxiv.org/pdf/2504.01202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01202]] Global explainability of a deep abstaining classifier(https://arxiv.org/abs/2504.01202)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>We present a global explainability method to characterize sources of errors in the histology prediction task of our real-world multitask convolutional neural network (MTCNN)-based deep abstaining classifier (DAC), for automated annotation of cancer pathology reports from NCI-SEER registries. Our classifier was trained and evaluated on 1.04 million hand-annotated samples and makes simultaneous predictions of cancer site, subsite, histology, laterality, and behavior for each report. The DAC framework enables the model to abstain on ambiguous reports and/or confusing classes to achieve a target accuracy on the retained (non-abstained) samples, but at the cost of decreased coverage. Requiring 97% accuracy on the histology task caused our model to retain only 22% of all samples, mostly the less ambiguous and common classes. Local explainability with the GradInp technique provided a computationally efficient way of obtaining contextual reasoning for thousands of individual predictions. Our method, involving dimensionality reduction of approximately 13000 aggregated local explanations, enabled global identification of sources of errors as hierarchical complexity among classes, label noise, insufficient information, and conflicting evidence. This suggests several strategies such as exclusion criteria, focused annotation, and reduced penalties for errors involving hierarchically related classes to iteratively improve our DAC in this complex real-world implementation.</li>
</ul>

<h3>Title: GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection</h3>
<ul>
<li><strong>Authors: </strong>Banafsheh Adami, Nima Karimian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01213">https://arxiv.org/abs/2504.01213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01213">https://arxiv.org/pdf/2504.01213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01213]] GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection(https://arxiv.org/abs/2504.01213)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Although contactless fingerprints offer user comfort, they are more vulnerable to spoofing. The current solution for anti-spoofing in the area of contactless fingerprints relies on domain adaptation learning, limiting their generalization and scalability. To address these limitations, we introduce GRU-AUNet, a domain adaptation approach that integrates a Swin Transformer-based UNet architecture with GRU-enhanced attention mechanisms, a Dynamic Filter Network in the bottleneck, and a combined Focal and Contrastive Loss function. Trained in both genuine and spoof fingerprint images, GRU-AUNet demonstrates robust resilience against presentation attacks, achieving an average BPCER of 0.09\% and APCER of 1.2\% in the CLARKSON, COLFISPOOF, and IIITD datasets, outperforming state-of-the-art domain adaptation methods.</li>
</ul>

<h3>Title: Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Feng Chen, Dror Ben-Zeev, Gillian Sparks, Arya Kadakia, Trevor Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01216">https://arxiv.org/abs/2504.01216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01216">https://arxiv.org/pdf/2504.01216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01216]] Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models(https://arxiv.org/abs/2504.01216)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical settings, presenting opportunities for automated detection to identify patients. This study evaluates natural language processing approaches for detecting PTSD from clinical interview transcripts. We compared general and mental health-specific transformer models (BERT/RoBERTa), embedding-based methods (SentenceBERT/LLaMA), and large language model prompting strategies (zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset. Domain-specific models significantly outperformed general models (Mental-RoBERTa F1=0.643 vs. RoBERTa-base 0.485). LLaMA embeddings with neural networks achieved the highest performance (F1=0.700). Zero-shot prompting using DSM-5 criteria yielded competitive results without training data (F1=0.657). Performance varied significantly across symptom severity and comorbidity status, with higher accuracy for severe PTSD cases and patients with comorbid depression. Our findings highlight the potential of domain-adapted embeddings and LLMs for scalable screening while underscoring the need for improved detection of nuanced presentations and offering insights for developing clinically viable AI tools for PTSD assessment.</li>
</ul>

<h3>Title: Prompting Forgetting: Unlearning in GANs via Textual Guidance</h3>
<ul>
<li><strong>Authors: </strong>Piyush Nagasubramaniam (1), Neeraj Karamchandani (1), Chen Wu (2), Sencun Zhu (1) ((1) The Pennsylvania State University, (2) Meta)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01218">https://arxiv.org/abs/2504.01218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01218">https://arxiv.org/pdf/2504.01218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01218]] Prompting Forgetting: Unlearning in GANs via Textual Guidance(https://arxiv.org/abs/2504.01218)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior.</li>
</ul>

<h3>Title: Gradient-free Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Grzegorz Rypeść</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01219">https://arxiv.org/abs/2504.01219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01219">https://arxiv.org/pdf/2504.01219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01219]] Gradient-free Continual Learning(https://arxiv.org/abs/2504.01219)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Continual learning (CL) presents a fundamental challenge in training neural networks on sequential tasks without experiencing catastrophic forgetting. Traditionally, the dominant approach in CL has been gradient-based optimization, where updates to the network parameters are performed using stochastic gradient descent (SGD) or its variants. However, a major limitation arises when previous data is no longer accessible, as is often assumed in CL settings. In such cases, there is no gradient information available for past data, leading to uncontrolled parameter changes and consequently severe forgetting of previously learned tasks. By shifting focus from data availability to gradient availability, this work opens up new avenues for addressing forgetting in CL. We explore the hypothesis that gradient-free optimization methods can provide a robust alternative to conventional gradient-based continual learning approaches. We discuss the theoretical underpinnings of such method, analyze their potential advantages and limitations, and present empirical evidence supporting their effectiveness. By reconsidering the fundamental cause of forgetting, this work aims to contribute a fresh perspective to the field of continual learning and inspire novel research directions.</li>
</ul>

<h3>Title: rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator</h3>
<ul>
<li><strong>Authors: </strong>Banafsheh Adami, Nima Karimian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01220">https://arxiv.org/abs/2504.01220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01220">https://arxiv.org/pdf/2504.01220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01220]] rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator(https://arxiv.org/abs/2504.01220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Remote photoplethysmography (rPPG) offers a novel approach to noninvasive monitoring of vital signs, such as respiratory rate, utilizing a camera. Although several supervised and self-supervised methods have been proposed, they often fail to accurately reconstruct the PPG signal, particularly in distinguishing between systolic and diastolic components. Their primary focus tends to be solely on extracting heart rate, which may not accurately represent the complete PPG signal. To address this limitation, this paper proposes a novel deep learning architecture using Generative Adversarial Networks by introducing multi-discriminators to extract rPPG signals from facial videos. These discriminators focus on the time domain, the frequency domain, and the second derivative of the original time domain signal. The discriminator integrates four loss functions: variance loss to mitigate local minima caused by noise; dynamic time warping loss to address local minima induced by alignment and sequences of variable lengths; Sparsity Loss for heart rate adjustment, and Variance Loss to ensure a uniform distribution across the desired frequency domain and time interval between systolic and diastolic phases of the PPG signal.</li>
</ul>

<h3>Title: AutoML Benchmark with shorter time constraints and early stopping</h3>
<ul>
<li><strong>Authors: </strong>Israel Campero Jurado, Pieter Gijsbers, Joaquin Vanschoren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01222">https://arxiv.org/abs/2504.01222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01222">https://arxiv.org/pdf/2504.01222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01222]] AutoML Benchmark with shorter time constraints and early stopping(https://arxiv.org/abs/2504.01222)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Automated Machine Learning (AutoML) automatically builds machine learning (ML) models on data. The de facto standard for evaluating new AutoML frameworks for tabular data is the AutoML Benchmark (AMLB). AMLB proposed to evaluate AutoML frameworks using 1- and 4-hour time budgets across 104 tasks. We argue that shorter time constraints should be considered for the benchmark because of their practical value, such as when models need to be retrained with high frequency, and to make AMLB more accessible. This work considers two ways in which to reduce the overall computation used in the benchmark: smaller time constraints and the use of early stopping. We conduct evaluations of 11 AutoML frameworks on 104 tasks with different time constraints and find the relative ranking of AutoML frameworks is fairly consistent across time constraints, but that using early-stopping leads to a greater variety in model performance.</li>
</ul>

<h3>Title: Explainable post-training bias mitigation with distribution-based fairness metrics</h3>
<ul>
<li><strong>Authors: </strong>Ryan Franks, Alexey Miroshnikov</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01223">https://arxiv.org/abs/2504.01223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01223">https://arxiv.org/pdf/2504.01223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01223]] Explainable post-training bias mitigation with distribution-based fairness metrics(https://arxiv.org/abs/2504.01223)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We develop a novel optimization framework with distribution-based fairness constraints for efficiently producing demographically blind, explainable models across a wide range of fairness levels. This is accomplished through post-processing, avoiding the need for retraining. Our framework, which is based on stochastic gradient descent, can be applied to a wide range of model types, with a particular emphasis on the post-processing of gradient-boosted decision trees. Additionally, we design a broad class of interpretable global bias metrics compatible with our method by building on previous work. We empirically test our methodology on a variety of datasets and compare it to other methods.</li>
</ul>

<h3>Title: TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification</h3>
<ul>
<li><strong>Authors: </strong>Kimia haghjooei, Mansoor Rezghi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01228">https://arxiv.org/abs/2504.01228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01228">https://arxiv.org/pdf/2504.01228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01228]] TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification(https://arxiv.org/abs/2504.01228)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Deep learning models have achieved remarkable success in computer vision but remain vulnerable to adversarial attacks, particularly in black-box settings where model details are unknown. Existing adversarial attack methods(even those works with key frames) often treat video data as simple vectors, ignoring their inherent multi-dimensional structure, and require a large number of queries, making them inefficient and detectable. In this paper, we propose \textbf{TenAd}, a novel tensor-based low-rank adversarial attack that leverages the multi-dimensional properties of video data by representing videos as fourth-order tensors. By exploiting low-rank attack, our method significantly reduces the search space and the number of queries needed to generate adversarial examples in black-box settings. Experimental results on standard video classification datasets demonstrate that \textbf{TenAd} effectively generates imperceptible adversarial perturbations while achieving higher attack success rates and query efficiency compared to state-of-the-art methods. Our approach outperforms existing black-box adversarial attacks in terms of success rate, query efficiency, and perturbation imperceptibility, highlighting the potential of tensor-based methods for adversarial attacks on video models.</li>
</ul>

<h3>Title: Highway to Hull: An Algorithm for Solving the General Matrix Code Equivalence Problem</h3>
<ul>
<li><strong>Authors: </strong>Alain Couvreur, Christophe Levrat</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01230">https://arxiv.org/abs/2504.01230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01230">https://arxiv.org/pdf/2504.01230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01230]] Highway to Hull: An Algorithm for Solving the General Matrix Code Equivalence Problem(https://arxiv.org/abs/2504.01230)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The matrix code equivalence problem consists, given two matrix spaces $\mathcal{C},\mathcal{D}\subset \mathbb{F}_q^{m\times n}$ of dimension $k$, in finding invertible matrices $P\in\mathrm{GL}_m(\mathbb{F}_q)$ and $Q\in\mathrm{GL}_n(\mathbb{F}_q)$ such that $\mathcal{D}=P\mathcal{C} Q^{-1}$. Recent signature schemes such as MEDS and ALTEQ relate their security to the hardness of this problem. Naranayan et. al. recently published an algorithm solving this problem in the case $k = n =m$ in $\widetilde{O}(q^{\frac k 2})$ operations. We present a different algorithm which solves the problem in the general case. Our approach consists in reducing the problem to the matrix code conjugacy problem, i.e. the case $P=Q$. For the latter problem, similarly to the permutation code equivalence problem in Hamming metric, a natural invariant based on the Hull of the code can be used. Next, the equivalence of codes can be deduced using a usual list collision argument. For $k=m=n$, our algorithm achieves the same complexity as in the aforementioned reference. However, it extends to a much broader range of parameters.</li>
</ul>

<h3>Title: Towards Resilient Federated Learning in CyberEdge Networks: Recent Advances and Future Trends</h3>
<ul>
<li><strong>Authors: </strong>Kai Li, Zhengyang Zhang, Azadeh Pourkabirian, Wei Ni, Falko Dressler, Ozgur B. Akan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01240">https://arxiv.org/abs/2504.01240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01240">https://arxiv.org/pdf/2504.01240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01240]] Towards Resilient Federated Learning in CyberEdge Networks: Recent Advances and Future Trends(https://arxiv.org/abs/2504.01240)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, federate, large language model</a></li>
<li><strong>Abstract: </strong>In this survey, we investigate the most recent techniques of resilient federated learning (ResFL) in CyberEdge networks, focusing on joint training with agglomerative deduction and feature-oriented security mechanisms. We explore adaptive hierarchical learning strategies to tackle non-IID data challenges, improving scalability and reducing communication overhead. Fault tolerance techniques and agglomerative deduction mechanisms are studied to detect unreliable devices, refine model updates, and enhance convergence stability. Unlike existing FL security research, we comprehensively analyze feature-oriented threats, such as poisoning, inference, and reconstruction attacks that exploit model features. Moreover, we examine resilient aggregation techniques, anomaly detection, and cryptographic defenses, including differential privacy and secure multi-party computation, to strengthen FL security. In addition, we discuss the integration of 6G, large language models (LLMs), and interoperable learning frameworks to enhance privacy-preserving and decentralized cross-domain training. These advancements offer ultra-low latency, artificial intelligence (AI)-driven network management, and improved resilience against adversarial attacks, fostering the deployment of secure ResFL in CyberEdge networks.</li>
</ul>

<h3>Title: Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks</h3>
<ul>
<li><strong>Authors: </strong>Naimul Haque</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01241">https://arxiv.org/abs/2504.01241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01241">https://arxiv.org/pdf/2504.01241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01241]] Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks(https://arxiv.org/abs/2504.01241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significantly advanced Natural Language Processing (NLP), particularly in Natural Language Understanding (NLU) tasks. As we progress toward an agentic world where LLM-based agents autonomously handle specialized tasks, it becomes crucial for these models to adapt to new tasks without forgetting previously learned information - a challenge known as catastrophic forgetting. This study evaluates the continual fine-tuning of various open-source LLMs with different parameter sizes (specifically models under 10 billion parameters) on key NLU tasks from the GLUE benchmark, including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and task-specific adjustments, we assess and compare the models' abilities to retain prior knowledge while learning new tasks. Our results indicate that models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong learning capabilities, making them well-suited for continual learning environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate impressive learning abilities and overall performance after fine-tuning. This work contributes to understanding catastrophic forgetting in LLMs and highlights prompting engineering to optimize model performance for continual learning scenarios.</li>
</ul>

<h3>Title: Dynamic Graph Structure Estimation for Learning Multivariate Point Process using Spiking Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Biswadeep Chakraborty, Hemant Kumawat, Beomseok Kang, Saibal Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01246">https://arxiv.org/abs/2504.01246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01246">https://arxiv.org/pdf/2504.01246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01246]] Dynamic Graph Structure Estimation for Learning Multivariate Point Process using Spiking Neural Networks(https://arxiv.org/abs/2504.01246)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Modeling and predicting temporal point processes (TPPs) is critical in domains such as neuroscience, epidemiology, finance, and social sciences. We introduce the Spiking Dynamic Graph Network (SDGN), a novel framework that leverages the temporal processing capabilities of spiking neural networks (SNNs) and spike-timing-dependent plasticity (STDP) to dynamically estimate underlying spatio-temporal functional graphs. Unlike existing methods that rely on predefined or static graph structures, SDGN adapts to any dataset by learning dynamic spatio-temporal dependencies directly from the event data, enhancing generalizability and robustness. While SDGN offers significant improvements over prior methods, we acknowledge its limitations in handling dense graphs and certain non-Gaussian dependencies, providing opportunities for future refinement. Our evaluations, conducted on both synthetic and real-world datasets including NYC Taxi, 911, Reddit, and Stack Overflow, demonstrate that SDGN achieves superior predictive accuracy while maintaining computational efficiency. Furthermore, we include ablation studies to highlight the contributions of its core components.</li>
</ul>

<h3>Title: Automated Factual Benchmarking for In-Car Conversational Systems using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rafael Giebisch, Ken E. Friedl, Lev Sorokin, Andrea Stocco</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01248">https://arxiv.org/abs/2504.01248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01248">https://arxiv.org/pdf/2504.01248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01248]] Automated Factual Benchmarking for In-Car Conversational Systems using Large Language Models(https://arxiv.org/abs/2504.01248)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-car conversational systems bring the promise to improve the in-vehicle user experience. Modern conversational systems are based on Large Language Models (LLMs), which makes them prone to errors such as hallucinations, i.e., inaccurate, fictitious, and therefore factually incorrect information. In this paper, we present an LLM-based methodology for the automatic factual benchmarking of in-car conversational systems. We instantiate our methodology with five LLM-based methods, leveraging ensembling techniques and diverse personae to enhance agreement and minimize hallucinations. We use our methodology to evaluate CarExpert, an in-car retrieval-augmented conversational question answering system, with respect to the factual correctness to a vehicle's manual. We produced a novel dataset specifically created for the in-car domain, and tested our methodology against an expert evaluation. Our results show that the combination of GPT-4 with the Input Output Prompting achieves over 90 per cent factual correctness agreement rate with expert evaluations, other than being the most efficient approach yielding an average response time of 4.5s. Our findings suggest that LLM-based testing constitutes a viable approach for the validation of conversational systems regarding their factual correctness.</li>
</ul>

<h3>Title: R2DN: Scalable Parameterization of Contracting and Lipschitz Recurrent Deep Networks</h3>
<ul>
<li><strong>Authors: </strong>Nicholas H. Barbara, Ruigang Wang, Ian R. Manchester</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01250">https://arxiv.org/abs/2504.01250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01250">https://arxiv.org/pdf/2504.01250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01250]] R2DN: Scalable Parameterization of Contracting and Lipschitz Recurrent Deep Networks(https://arxiv.org/abs/2504.01250)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents the Robust Recurrent Deep Network (R2DN), a scalable parameterization of robust recurrent neural networks for machine learning and data-driven control. We construct R2DNs as a feedback interconnection of a linear time-invariant system and a 1-Lipschitz deep feedforward network, and directly parameterize the weights so that our models are stable (contracting) and robust to small input perturbations (Lipschitz) by design. Our parameterization uses a structure similar to the previously-proposed recurrent equilibrium networks (RENs), but without the requirement to iteratively solve an equilibrium layer at each time-step. This speeds up model evaluation and backpropagation on GPUs, and makes it computationally feasible to scale up the network size, batch size, and input sequence length in comparison to RENs. We compare R2DNs to RENs on three representative problems in nonlinear system identification, observer design, and learning-based feedback control and find that training and inference are both up to an order of magnitude faster with similar test set performance, and that training/inference times scale more favorably with respect to model expressivity.</li>
</ul>

<h3>Title: Grade Guard: A Smart System for Short Answer Automated Grading</h3>
<ul>
<li><strong>Authors: </strong>Niharika Dadu, Harsh Vardhan Singh, Romi Banerjee (Indian Institute of Technology Jodhpur)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01253">https://arxiv.org/abs/2504.01253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01253">https://arxiv.org/pdf/2504.01253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01253]] Grade Guard: A Smart System for Short Answer Automated Grading(https://arxiv.org/abs/2504.01253)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) in the education sector has provided impetus to automate grading short answer questions. LLMs make evaluating short answers very efficient, thus addressing issues like staff shortage. However, in the task of Automated Short Answer Grading (ASAG), LLM responses are influenced by diverse perspectives in their training dataset, leading to inaccuracies in evaluating nuanced or partially correct answers. To address this challenge, we propose a novel framework, Grade Guard. 1. To enhance the task-based specialization of the LLMs, the temperature parameter has been fine-tuned using Root Mean Square Error (RMSE). 2. Unlike traditional approaches, LLMs in Grade Guard compute an Indecisiveness Score (IS) along with the grade to reflect uncertainty in predicted grades. 3. Introduced Confidence-Aware Loss (CAL) to generate an optimized Indecisiveness Score (IS). 4. To improve reliability, self-reflection based on the optimized IS has been introduced into the framework, enabling human re-evaluation to minimize incorrect grade assignments. Our experimentation shows that the best setting of Grade Guard outperforms traditional methods by 19.16% RMSE in Upstage Solar Pro, 23.64% RMSE in Upstage Solar Mini, 4.00% RMSE in Gemini 1.5 Flash, and 10.20% RMSE in GPT 4-o Mini. Future work includes improving interpretability by generating rationales for grades to enhance accuracy. Expanding benchmark datasets and annotating them with domain-specific nuances will enhance grading accuracy. Finally, analyzing feedback to enhance confidence in predicted grades, reduce biases, optimize grading criteria, and personalize learning while supporting multilingual grading systems will make the solution more accurate, adaptable, fair, and inclusive.</li>
</ul>

<h3>Title: Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding</h3>
<ul>
<li><strong>Authors: </strong>Sakhinana Sagar Srinivas, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01281">https://arxiv.org/abs/2504.01281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01281">https://arxiv.org/pdf/2504.01281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01281]] Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding(https://arxiv.org/abs/2504.01281)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.</li>
</ul>

<h3>Title: Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing</h3>
<ul>
<li><strong>Authors: </strong>Jihyun Janice Ahn, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01282">https://arxiv.org/abs/2504.01282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01282">https://arxiv.org/pdf/2504.01282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01282]] Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing(https://arxiv.org/abs/2504.01282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While the inconsistency of LLMs is not a novel topic, prior research has predominantly addressed two types of generative inconsistencies: i) Randomness Inconsistency: running the same LLM multiple trials, yielding varying responses; ii) Paraphrase Inconsistency: paraphrased prompts result in different responses from the same LLM. Randomness Inconsistency arises from the inherent randomness due to stochastic sampling in generative models, while Paraphrase Inconsistency is a consequence of the language modeling objectives, where paraphrased prompts alter the distribution of vocabulary logits. This research discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM self-inconsistency: given a question and a couple of LLM-generated answer candidates, the LLM often has conflicting responses when prompted "Which are correct answers?" and "Which are incorrect answers?". PRIN poses a big concern as it undermines the credibility of LLM-as-a-judge, and suggests a challenge for LLMs to adhere to basic logical rules. We conduct a series of experiments to investigate PRIN, examining the extent of PRIN across different LLMs, methods to mitigate it, potential applications, and its relationship with Randomness Inconsistency and Paraphrase Inconsistency. As the first study to explore PRIN, our findings offer valuable insights into the inner workings of LLMs and contribute to advancing trustworthy AI.</li>
</ul>

<h3>Title: A Novel Framework To Assess Cybersecurity Capability Maturity</h3>
<ul>
<li><strong>Authors: </strong>Lasini Liyanage, Nalin Archchilage, Giovanni Russello</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01305">https://arxiv.org/abs/2504.01305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01305">https://arxiv.org/pdf/2504.01305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01305]] A Novel Framework To Assess Cybersecurity Capability Maturity(https://arxiv.org/abs/2504.01305)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In today's rapidly evolving digital landscape, organisations face escalating cyber threats that can disrupt operations, compromise sensitive data, and inflict financial and reputational harm. A key reason for this lies in the organisations' lack of a clear understanding of their cybersecurity capabilities, leading to ineffective defences. To address this gap, Cybersecurity Capability Maturity Models (CCMMs) provide a systematic approach to assessing and enhancing an organisation's cybersecurity posture by focusing on capability maturity rather than merely implementing controls. However, their limitations, such as rigid structures, one-size-fits-all approach, complexity, gaps in security scope (i.e., technological, organisational, and human aspects) and lack of quantitative metrics, hinder their effectiveness. It makes implementing CCMMs in varying contexts challenging and results in fragmented, incomprehensive assessments. Therefore, we propose a novel Cybersecurity Capability Maturity Framework that is holistic, flexible, and measurable to provide organisations with a more relevant and impactful assessment to enhance their cybersecurity posture.</li>
</ul>

<h3>Title: Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01308">https://arxiv.org/abs/2504.01308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01308">https://arxiv.org/pdf/2504.01308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01308]] Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks(https://arxiv.org/abs/2504.01308)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Adaptive Rectification Sampling for Test-Time Compute Scaling</h3>
<ul>
<li><strong>Authors: </strong>Zhendong Tan, Xingjun Zhang, Chaoyi Hu, Yancheng Pan, Shaoxun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01317">https://arxiv.org/abs/2504.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01317">https://arxiv.org/pdf/2504.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01317]] Adaptive Rectification Sampling for Test-Time Compute Scaling(https://arxiv.org/abs/2504.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time scaling can significantly improve model performance, especially in complex tasks such as logical reasoning. Common test-time scaling methods involve generating more chain of thoughts (CoTs) or longer CoTs with self-correction. However, while self-correction can improve performance, it may lead to significant token waste and reduce readability of the CoT if the reasoning steps are already correct. To demonstrate that large language models (LLMs) can rectify errors at a more fine-grained level, we propose Adaptive Rectification Sampling (AR-Sampling), which can guide the LLMs to self-correction at the appropriate step. AR-Sampling leverages a process-supervised reward model (PRM) as a verifier and constructed trigger sentences to guide the model in adaptive step-level rethinking. Through the experiments on GSM8K and MATH500, it indicate that our approach enables the models to rethink in more fine-grained level, improving the accuracy of solutions, while generating a reasonable number of additional tokens.</li>
</ul>

<h3>Title: COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01321">https://arxiv.org/abs/2504.01321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01321">https://arxiv.org/pdf/2504.01321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01321]] COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking(https://arxiv.org/abs/2504.01321)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available.</li>
</ul>

<h3>Title: CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jin Lian, Zhongyu Wan, Ming Gao, JunFeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01326">https://arxiv.org/abs/2504.01326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01326">https://arxiv.org/pdf/2504.01326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01326]] CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection(https://arxiv.org/abs/2504.01326)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cross-layer feature pyramid networks (CFPNs) have achieved notable progress in multi-scale feature fusion and boundary detail preservation for salient object detection. However, traditional CFPNs still suffer from two core limitations: (1) a computational bottleneck caused by complex feature weighting operations, and (2) degraded boundary accuracy due to feature blurring in the upsampling process. To address these challenges, we propose CFMD, a novel cross-layer feature pyramid network that introduces two key innovations. First, we design a context-aware feature aggregation module (CFLMA), which incorporates the state-of-the-art Mamba architecture to construct a dynamic weight distribution mechanism. This module adaptively adjusts feature importance based on image context, significantly improving both representation efficiency and generalization. Second, we introduce an adaptive dynamic upsampling unit (CFLMD) that preserves spatial details during resolution recovery. By adjusting the upsampling range dynamically and initializing with a bilinear strategy, the module effectively reduces feature overlap and maintains fine-grained boundary structures. Extensive experiments on three standard benchmarks using three mainstream backbone networks demonstrate that CFMD achieves substantial improvements in pixel-level accuracy and boundary segmentation quality, especially in complex scenes. The results validate the effectiveness of CFMD in jointly enhancing computational efficiency and segmentation performance, highlighting its strong potential in salient object detection tasks.</li>
</ul>

<h3>Title: Slow-Fast Architecture for Video Multi-Modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Min Shi, Shihao Wang, Chieh-Yun Chen, Jitesh Jain, Kai Wang, Junjun Xiong, Guilin Liu, Zhiding Yu, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01328">https://arxiv.org/abs/2504.01328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01328">https://arxiv.org/pdf/2504.01328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01328]] Slow-Fast Architecture for Video Multi-Modal Large Language Models(https://arxiv.org/abs/2504.01328)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) "fast" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) "slow" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.</li>
</ul>

<h3>Title: Flexible and Explainable Graph Analysis for EEG-based Alzheimer's Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>Jing Wang, Jun-En Ding, Feng Liu, Elisa Kallioniemi, Shuqiang Wang, Wen-Xiang Tsai, Albert C. Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01329">https://arxiv.org/abs/2504.01329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01329">https://arxiv.org/pdf/2504.01329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01329]] Flexible and Explainable Graph Analysis for EEG-based Alzheimer's Disease Classification(https://arxiv.org/abs/2504.01329)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease is a progressive neurological disorder that is one of the most common forms of dementia. It leads to a decline in memory, reasoning ability, and behavior, especially in older people. The cause of Alzheimer's Disease is still under exploration and there is no all-inclusive theory that can explain the pathologies in each individual patient. Nevertheless, early intervention has been found to be effective in managing symptoms and slowing down the disease's progression. Recent research has utilized electroencephalography (EEG) data to identify biomarkers that distinguish Alzheimer's Disease patients from healthy individuals. Prior studies have used various machine learning methods, including deep learning and graph neural networks, to examine electroencephalography-based signals for identifying Alzheimer's Disease patients. In our research, we proposed a Flexible and Explainable Gated Graph Convolutional Network (GGCN) with Multi-Objective Tree-Structured Parzen Estimator (MOTPE) hyperparameter tuning. This provides a flexible solution that efficiently identifies the optimal number of GGCN blocks to achieve the optimized precision, specificity, and recall outcomes, as well as the optimized area under the Receiver Operating Characteristic (AUC). Our findings demonstrated a high efficacy with an over 0.9 Receiver Operating Characteristic score, alongside precision, specificity, and recall scores in distinguishing health control with Alzheimer's Disease patients in Moderate to Severe Dementia using the power spectrum density (PSD) of electroencephalography signals across various frequency bands. Moreover, our research enhanced the interpretability of the embedded adjacency matrices, revealing connectivity differences in frontal and parietal brain regions between Alzheimer's patients and healthy individuals.</li>
</ul>

<h3>Title: Foundations and Evaluations in NLP</h3>
<ul>
<li><strong>Authors: </strong>Jungyeul Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01342">https://arxiv.org/abs/2504.01342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01342">https://arxiv.org/pdf/2504.01342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01342]] Foundations and Evaluations in NLP(https://arxiv.org/abs/2504.01342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This memoir explores two fundamental aspects of Natural Language Processing (NLP): the creation of linguistic resources and the evaluation of NLP system performance. Over the past decade, my work has focused on developing a morpheme-based annotation scheme for the Korean language that captures linguistic properties from morphology to semantics. This approach has achieved state-of-the-art results in various NLP tasks, including part-of-speech tagging, dependency parsing, and named entity recognition. Additionally, this work provides a comprehensive analysis of segmentation granularity and its critical impact on NLP system performance. In parallel with linguistic resource development, I have proposed a novel evaluation framework, the jp-algorithm, which introduces an alignment-based method to address challenges in preprocessing tasks like tokenization and sentence boundary detection (SBD). Traditional evaluation methods assume identical tokenization and sentence lengths between gold standards and system outputs, limiting their applicability to real-world data. The jp-algorithm overcomes these limitations, enabling robust end-to-end evaluations across a variety of NLP tasks. It enhances accuracy and flexibility by incorporating linear-time alignment while preserving the complexity of traditional evaluation metrics. This memoir provides key insights into the processing of morphologically rich languages, such as Korean, while offering a generalizable framework for evaluating diverse end-to-end NLP systems. My contributions lay the foundation for future developments, with broader implications for multilingual resource development and system evaluation.</li>
</ul>

<h3>Title: Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for Targeted Misclassification</h3>
<ul>
<li><strong>Authors: </strong>Akil Raj Subedi, Taniya Shah, Aswani Kumar Cherukuri, Thanos Vasilakos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01345">https://arxiv.org/abs/2504.01345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01345">https://arxiv.org/pdf/2504.01345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01345]] Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for Targeted Misclassification(https://arxiv.org/abs/2504.01345)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, transformer</a></li>
<li><strong>Abstract: </strong>Social media platforms like Twitter have increasingly relied on Natural Language Processing NLP techniques to analyze and understand the sentiments expressed in the user generated content. One such state of the art NLP model is Bidirectional Encoder Representations from Transformers BERT which has been widely adapted in sentiment analysis. BERT is susceptible to adversarial attacks. This paper aims to scrutinize the inherent vulnerabilities of such models in Twitter sentiment analysis. It aims to formulate a framework for constructing targeted adversarial texts capable of deceiving these models, while maintaining stealth. In contrast to conventional methodologies, such as Importance Reweighting, this framework core idea resides in its reliance on gradients to prioritize the importance of individual words within the text. It uses a whitebox approach to attain fine grained sensitivity, pinpointing words that exert maximal influence on the classification outcome. This paper is organized into three interdependent phases. It starts with fine-tuning a pre-trained BERT model on Twitter data. It then analyzes gradients of the model to rank words on their importance, and iteratively replaces those with feasible candidates until an acceptable solution is found. Finally, it evaluates the effectiveness of the adversarial text against the custom trained sentiment classification model. This assessment would help in gauging the capacity of the adversarial text to successfully subvert classification without raising any alarm.</li>
</ul>

<h3>Title: Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Yuji Nozawa, Yu-Chieh Lin, Kazumoto Nakamura, Youyang Ng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01348">https://arxiv.org/abs/2504.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01348">https://arxiv.org/pdf/2504.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01348]] Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval(https://arxiv.org/abs/2504.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The goal of this paper is to enhance pretrained Vision Transformer (ViT) models for focus-oriented image retrieval with visual prompting. In real-world image retrieval scenarios, both query and database images often exhibit complexity, with multiple objects and intricate backgrounds. Users often want to retrieve images with specific object, which we define as the Focus-Oriented Image Retrieval (FOIR) task. While a standard image encoder can be employed to extract image features for similarity matching, it may not perform optimally in the multi-object-based FOIR task. This is because each image is represented by a single global feature vector. To overcome this, a prompt-based image retrieval solution is required. We propose an approach called Prompt-guided attention Head Selection (PHS) to leverage the head-wise potential of the multi-head attention mechanism in ViT in a promptable manner. PHS selects specific attention heads by matching their attention maps with user's visual prompts, such as a point, box, or segmentation. This empowers the model to focus on specific object of interest while preserving the surrounding visual context. Notably, PHS does not necessitate model re-training and avoids any image alteration. Experimental results show that PHS substantially improves performance on multiple datasets, offering a practical and training-free solution to enhance model performance in the FOIR task.</li>
</ul>

<h3>Title: xML-workFlow: an end-to-end explainable scikit-learn workflow for rapid biomedical experimentation</h3>
<ul>
<li><strong>Authors: </strong>Khoa A. Tran, John V. Pearson, Nicola Waddell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01356">https://arxiv.org/abs/2504.01356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01356">https://arxiv.org/pdf/2504.01356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01356]] xML-workFlow: an end-to-end explainable scikit-learn workflow for rapid biomedical experimentation(https://arxiv.org/abs/2504.01356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Motivation: Building and iterating machine learning models is often a resource-intensive process. In biomedical research, scientific codebases can lack scalability and are not easily transferable to work beyond what they were intended. xML-workFlow addresses this issue by providing a rapid, robust, and traceable end-to-end workflow that can be adapted to any ML project with minimal code rewriting. Results: We show a practical, end-to-end workflow that integrates scikit-learn, MLflow, and SHAP. This template significantly reduces the time and effort required to build and iterate on ML models, addressing the common challenges of scalability and reproducibility in biomedical research. Adapting our template may save bioinformaticians time in development and enables biomedical researchers to deploy ML projects. Availability and implementation: xML-workFlow is available at this https URL.</li>
</ul>

<h3>Title: LITE: LLM-Impelled efficient Taxonomy Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Lin Zhang, Zhouhong Gu, Suhang Zheng, Tao Wang, Tianyu Li, Hongwei Feng, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01369">https://arxiv.org/abs/2504.01369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01369">https://arxiv.org/pdf/2504.01369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01369]] LITE: LLM-Impelled efficient Taxonomy Evaluation(https://arxiv.org/abs/2504.01369)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This paper presents LITE, an LLM-based evaluation method designed for efficient and flexible assessment of taxonomy quality. To address challenges in large-scale taxonomy evaluation, such as efficiency, fairness, and consistency, LITE adopts a top-down hierarchical evaluation strategy, breaking down the taxonomy into manageable substructures and ensuring result reliability through cross-validation and standardized input formats. LITE also introduces a penalty mechanism to handle extreme cases and provides both quantitative performance analysis and qualitative insights by integrating evaluation metrics closely aligned with task objectives. Experimental results show that LITE demonstrates high reliability in complex evaluation tasks, effectively identifying semantic errors, logical contradictions, and structural flaws in taxonomies, while offering directions for improvement. Code is available at this https URL .</li>
</ul>

<h3>Title: UniFault: A Fault Diagnosis Foundation Model from Bearing Data</h3>
<ul>
<li><strong>Authors: </strong>Emadeldeen Eldele, Mohamed Ragab, Xu Qing, Edward, Zhenghua Chen, Min Wu, Xiaoli Li, Jay Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01373">https://arxiv.org/abs/2504.01373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01373">https://arxiv.org/pdf/2504.01373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01373]] UniFault: A Fault Diagnosis Foundation Model from Bearing Data(https://arxiv.org/abs/2504.01373)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Machine fault diagnosis (FD) is a critical task for predictive maintenance, enabling early fault detection and preventing unexpected failures. Despite its importance, existing FD models are operation-specific with limited generalization across diverse datasets. Foundation models (FM) have demonstrated remarkable potential in both visual and language domains, achieving impressive generalization capabilities even with minimal data through few-shot or zero-shot learning. However, translating these advances to FD presents unique hurdles. Unlike the large-scale, cohesive datasets available for images and text, FD datasets are typically smaller and more heterogeneous, with significant variations in sampling frequencies and the number of channels across different systems and applications. This heterogeneity complicates the design of a universal architecture capable of effectively processing such diverse data while maintaining robust feature extraction and learning capabilities. In this paper, we introduce UniFault, a foundation model for fault diagnosis that systematically addresses these issues. Specifically, the model incorporates a comprehensive data harmonization pipeline featuring two key innovations. First, a unification scheme transforms multivariate inputs into standardized univariate sequences while retaining local inter-channel relationships. Second, a novel cross-domain temporal fusion strategy mitigates distribution shifts and enriches sample diversity and count, improving the model generalization across varying conditions. UniFault is pretrained on over 9 billion data points spanning diverse FD datasets, enabling superior few-shot performance. Extensive experiments on real-world FD datasets demonstrate that UniFault achieves SoTA performance, setting a new benchmark for fault diagnosis models and paving the way for more scalable and robust predictive maintenance solutions.</li>
</ul>

<h3>Title: FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores</h3>
<ul>
<li><strong>Authors: </strong>Zhe Jiang, Sam Ainsworth, Timothy Jones</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01380">https://arxiv.org/abs/2504.01380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01380">https://arxiv.org/pdf/2504.01380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01380]] FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores(https://arxiv.org/abs/2504.01380)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>High-performance security guarantees rely on hardware support. Generic programmable support for fine-grained instruction analysis has gained broad interest in the literature as a fundamental building block for the security of future processors. Yet, implementation in real out-of-order (OoO) superscalar processors presents tough challenges that cannot be explored in highly abstract simulators. We detail the challenges of implementing complex programmable pathways without critical paths or contention. We then introduce FireGuard, the first implementation of fine-grained instruction analysis on a real OoO superscalar processor. We establish an end-to-end system, including microarchitecture, SoC, ISA and programming model. Experiments show that our solution simultaneously ensures both security and performance of the system, with parallel scalability. We examine the feasibility of building FireGuard into modern SoCs: Apple's M1-Pro, Huawei's Kirin-960, and Intel's i7-12700F, where less than 1% silicon area is introduced. The Repo. of FireGuard's source code: this https URL.</li>
</ul>

<h3>Title: v-CLR: View-Consistent Learning for Open-World Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chang-Bin Zhang, Jinhong Ni, Yujie Zhong, Kai Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01383">https://arxiv.org/abs/2504.01383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01383">https://arxiv.org/pdf/2504.01383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01383]] v-CLR: View-Consistent Learning for Open-World Instance Segmentation(https://arxiv.org/abs/2504.01383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, \eg texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called view-Consistent LeaRning (v-CLR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In v-CLR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the image's underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our method on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance. Project page: this https URL</li>
</ul>

<h3>Title: De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Junyu Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01389">https://arxiv.org/abs/2504.01389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01389">https://arxiv.org/pdf/2504.01389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01389]] De Novo Molecular Design Enabled by Direct Preference Optimization and Curriculum Learning(https://arxiv.org/abs/2504.01389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>De novo molecular design has extensive applications in drug discovery and materials science. The vast chemical space renders direct molecular searches computationally prohibitive, while traditional experimental screening is both time- and labor-intensive. Efficient molecular generation and screening methods are therefore essential for accelerating drug discovery and reducing costs. Although reinforcement learning (RL) has been applied to optimize molecular properties via reward mechanisms, its practical utility is limited by issues in training efficiency, convergence, and stability. To address these challenges, we adopt Direct Preference Optimization (DPO) from NLP, which uses molecular score-based sample pairs to maximize the likelihood difference between high- and low-quality molecules, effectively guiding the model toward better compounds. Moreover, integrating curriculum learning further boosts training efficiency and accelerates convergence. A systematic evaluation of the proposed method on the GuacaMol Benchmark yielded excellent scores. For instance, the method achieved a score of 0.883 on the Perindopril MPO task, representing a 6\% improvement over competing models. And subsequent target protein binding experiments confirmed its practical efficacy. These results demonstrate the strong potential of DPO for molecular design tasks and highlight its effectiveness as a robust and efficient solution for data-driven drug discovery.</li>
</ul>

<h3>Title: From Easy to Hard: Building a Shortcut for Differentially Private Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Kecen Li, Chen Gong, Xiaochen Li, Yuzhong Zhao, Xinwen Hou, Tianhao Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01395">https://arxiv.org/abs/2504.01395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01395">https://arxiv.org/pdf/2504.01395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01395]] From Easy to Hard: Building a Shortcut for Differentially Private Image Synthesis(https://arxiv.org/abs/2504.01395)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Differentially private (DP) image synthesis aims to generate synthetic images from a sensitive dataset, alleviating the privacy leakage concerns of organizations sharing and utilizing synthetic images. Although previous methods have significantly progressed, especially in training diffusion models on sensitive images with DP Stochastic Gradient Descent (DP-SGD), they still suffer from unsatisfactory performance. In this work, inspired by curriculum learning, we propose a two-stage DP image synthesis framework, where diffusion models learn to generate DP synthetic images from easy to hard. Unlike existing methods that directly use DP-SGD to train diffusion models, we propose an easy stage in the beginning, where diffusion models learn simple features of the sensitive images. To facilitate this easy stage, we propose to use `central images', simply aggregations of random samples of the sensitive dataset. Intuitively, although those central images do not show details, they demonstrate useful characteristics of all images and only incur minimal privacy costs, thus helping early-phase model training. We conduct experiments to present that on the average of four investigated image datasets, the fidelity and utility metrics of our synthetic images are 33.1% and 2.1% better than the state-of-the-art method.</li>
</ul>

<h3>Title: All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning</h3>
<ul>
<li><strong>Authors: </strong>Zheng Yang, Ruoxin Chen, Zhiyuan Yan, Ke-Yue Zhang, Xinghe Fu, Shuang Wu, Xiujun Shu, Taiping Yao, Junchi Yan, Shouhong Ding, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01396">https://arxiv.org/abs/2504.01396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01396">https://arxiv.org/pdf/2504.01396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01396]] All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning(https://arxiv.org/abs/2504.01396)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The exponential growth of AI-generated images (AIGIs) underscores the urgent need for robust and generalizable detection methods. In this paper, we establish two key principles for AIGI detection through systematic analysis: \textbf{(1) All Patches Matter:} Unlike conventional image classification where discriminative features concentrate on object-centric regions, each patch in AIGIs inherently contains synthetic artifacts due to the uniform generation process, suggesting that every patch serves as an important artifact source for detection. \textbf{(2) More Patches Better}: Leveraging distributed artifacts across more patches improves detection robustness by capturing complementary forensic evidence and reducing over-reliance on specific patches, thereby enhancing robustness and generalization. However, our counterfactual analysis reveals an undesirable phenomenon: naively trained detectors often exhibit a \textbf{Few-Patch Bias}, discriminating between real and synthetic images based on minority patches. We identify \textbf{Lazy Learner} as the root cause: detectors preferentially learn conspicuous artifacts in limited patches while neglecting broader artifact distributions. To address this bias, we propose the \textbf{P}anoptic \textbf{P}atch \textbf{L}earning (PPL) framework, involving: (1) Random Patch Replacement that randomly substitutes synthetic patches with real counterparts to compel models to identify artifacts in underutilized regions, encouraging the broader use of more patches; (2) Patch-wise Contrastive Learning that enforces consistent discriminative capability across all patches, ensuring uniform utilization of all patches. Extensive experiments across two different settings on several benchmarks verify the effectiveness of our approach.</li>
</ul>

<h3>Title: Leveraging Generalizability of Image-to-Image Translation for Enhanced Adversarial Defense</h3>
<ul>
<li><strong>Authors: </strong>Haibo Zhang, Zhihua Yao, Kouichi Sakurai, Takeshi Saitoh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01399">https://arxiv.org/abs/2504.01399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01399">https://arxiv.org/pdf/2504.01399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01399]] Leveraging Generalizability of Image-to-Image Translation for Enhanced Adversarial Defense(https://arxiv.org/abs/2504.01399)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving field of artificial intelligence, machine learning emerges as a key technology characterized by its vast potential and inherent risks. The stability and reliability of these models are important, as they are frequent targets of security threats. Adversarial attacks, first rigorously defined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability: they can trick machine learning models into making incorrect predictions by applying nearly invisible perturbations to images. Although many studies have focused on constructing sophisticated defensive mechanisms to mitigate such attacks, they often overlook the substantial time and computational costs of training and maintaining these models. Ideally, a defense method should be able to generalize across various, even unseen, adversarial attacks with minimal overhead. Building on our previous work on image-to-image translation-based defenses, this study introduces an improved model that incorporates residual blocks to enhance generalizability. The proposed method requires training only a single model, effectively defends against diverse attack types, and is well-transferable between different target models. Experiments show that our model can restore the classification accuracy from near zero to an average of 72\% while maintaining competitive performance compared to state-of-the-art methods.</li>
</ul>

<h3>Title: ToolACE-R: Tool Learning with Adaptive Self-Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xingshan Zeng, Weiwen Liu, Xu Huang, Zezhong Wang, Lingzhi Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruiming Tang, Qun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01400">https://arxiv.org/abs/2504.01400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01400">https://arxiv.org/pdf/2504.01400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01400]] ToolACE-R: Tool Learning with Adaptive Self-Refinement(https://arxiv.org/abs/2504.01400)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tool learning, which allows Large Language Models (LLMs) to leverage external tools for solving complex user tasks, has emerged as a promising avenue for extending model capabilities. However, current approaches primarily focus on data synthesis for fine-tuning LLMs to invoke tools effectively, largely ignoring how to fully stimulate the potential of the model. In this paper, we propose ToolACE-R, a novel method that introduces adaptive self-refinement for tool invocations. Our approach features a model-aware iterative training procedure that progressively incorporates more training samples based on the model's evolving capabilities. Additionally, it allows LLMs to iteratively refine their tool calls, optimizing performance without requiring external feedback. To further enhance computational efficiency, we integrate an adaptive mechanism when scaling the inference time, enabling the model to autonomously determine when to stop the refinement process. We conduct extensive experiments across several benchmark datasets, showing that ToolACE-R achieves competitive performance compared to advanced API-based models, even without any refinement. Furthermore, its performance can be further improved efficiently through adaptive self-refinement. Our results demonstrate the effectiveness of the proposed method, which is compatible with base models of various sizes, offering a promising direction for more efficient tool learning.</li>
</ul>

<h3>Title: FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations</h3>
<ul>
<li><strong>Authors: </strong>Athena Wen, Tanush Patil, Ansh Saxena, Yicheng Fu, Sean O'Brien, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01420">https://arxiv.org/abs/2504.01420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01420">https://arxiv.org/pdf/2504.01420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01420]] FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations(https://arxiv.org/abs/2504.01420)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>In an era where AI-driven hiring is transforming recruitment practices, concerns about fairness and bias have become increasingly important. To explore these issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume Evaluation), to test for racial and gender bias in large language models (LLMs) used to evaluate resumes across different industries. We use two methods-direct scoring and ranking-to measure how model performance changes when resumes are slightly altered to reflect different racial or gender identities. Our findings reveal that while every model exhibits some degree of bias, the magnitude and direction vary considerably. This benchmark provides a clear way to examine these differences and offers valuable insights into the fairness of AI-based hiring tools. It highlights the urgent need for strategies to reduce bias in AI-driven recruitment. Our benchmark code and dataset are open-sourced at our repository: this https URL.</li>
</ul>

<h3>Title: Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics</h3>
<ul>
<li><strong>Authors: </strong>Zhaoxing Li, Xiaoming Zhang, Haifeng Zhang, Chengxiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01429">https://arxiv.org/abs/2504.01429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01429">https://arxiv.org/pdf/2504.01429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01429]] Refining Interactions: Enhancing Anisotropy in Graph Neural Networks with Language Semantics(https://arxiv.org/abs/2504.01429)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) with Graph Neural Networks (GNNs) has recently been explored to enhance the capabilities of Text Attribute Graphs (TAGs). Most existing methods feed textual descriptions of the graph structure or neighbouring nodes' text directly into LLMs. However, these approaches often cause LLMs to treat structural information simply as general contextual text, thus limiting their effectiveness in graph-related tasks. In this paper, we introduce LanSAGNN (Language Semantic Anisotropic Graph Neural Network), a framework that extends the concept of anisotropic GNNs to the natural language level. This model leverages LLMs to extract tailor-made semantic information for node pairs, effectively capturing the unique interactions within node relationships. In addition, we propose an efficient dual-layer LLMs finetuning architecture to better align LLMs' outputs with graph tasks. Experimental results demonstrate that LanSAGNN significantly enhances existing LLM-based methods without increasing complexity while also exhibiting strong robustness against interference.</li>
</ul>

<h3>Title: Solving Time-Fractional Partial Integro-Differential Equations Using Tensor Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Zhongshuo Lin, Qingkui Ma, Hehu Xie, Xiaobo Yin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01440">https://arxiv.org/abs/2504.01440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01440">https://arxiv.org/pdf/2504.01440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01440]] Solving Time-Fractional Partial Integro-Differential Equations Using Tensor Neural Networks(https://arxiv.org/abs/2504.01440)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel machine learning method based on adaptive tensor neural network subspace to solve linear time-fractional diffusion-wave equations and nonlinear time-fractional partial integro-differential equations. In this framework, the tensor neural network and Gauss-Jacobi quadrature are effectively combined to construct a universal numerical scheme for the temporal Caputo derivative with orders spanning $ (0,1)$ and $(1,2)$. Specifically, in order to effectively utilize Gauss-Jacobi quadrature to discretize Caputo derivatives, we design the tensor neural network function multiplied by the function $t^{\mu}$ where the power $\mu$ is selected according to the parameters of the equations at hand. Finally, some numerical examples are provided to validate the efficiency and accuracy of the proposed tensor neural network-based machine learning method.</li>
</ul>

<h3>Title: PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization</h3>
<ul>
<li><strong>Authors: </strong>Aofan Liu, Lulu Tang, Ting Pan, Yuguo Yin, Bin Wang, Ao Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01444">https://arxiv.org/abs/2504.01444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01444">https://arxiv.org/pdf/2504.01444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01444]] PiCo: Jailbreaking Multimodal Large Language Models via $\textbf{Pi}$ctorial $\textbf{Co}$de Contextualization(https://arxiv.org/abs/2504.01444)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs), which integrate vision and other modalities into Large Language Models (LLMs), significantly enhance AI capabilities but also introduce new security vulnerabilities. By exploiting the vulnerabilities of the visual modality and the long-tail distribution characteristic of code training data, we present PiCo, a novel jailbreaking framework designed to progressively bypass multi-tiered defense mechanisms in advanced MLLMs. PiCo employs a tier-by-tier jailbreak strategy, using token-level typographic attacks to evade input filtering and embedding harmful intent within programming context instructions to bypass runtime monitoring. To comprehensively assess the impact of attacks, a new evaluation metric is further proposed to assess both the toxicity and helpfulness of model outputs post-attack. By embedding harmful intent within code-style visual instructions, PiCo achieves an average Attack Success Rate (ASR) of 84.13% on Gemini-Pro Vision and 52.66% on GPT-4, surpassing previous methods. Experimental results highlight the critical gaps in current defenses, underscoring the need for more robust strategies to secure advanced MLLMs.</li>
</ul>

<h3>Title: Multimodal Point Cloud Semantic Segmentation With Virtual Point Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Zaipeng Duan, Xuzhong Hu, Pei An, Jie Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01449">https://arxiv.org/abs/2504.01449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01449">https://arxiv.org/pdf/2504.01449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01449]] Multimodal Point Cloud Semantic Segmentation With Virtual Point Enhancement(https://arxiv.org/abs/2504.01449)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR-based 3D point cloud recognition has been proven beneficial in various applications. However, the sparsity and varying density pose a significant challenge in capturing intricate details of objects, particularly for medium-range and small targets. Therefore, we propose a multi-modal point cloud semantic segmentation method based on Virtual Point Enhancement (VPE), which integrates virtual points generated from images to address these issues. These virtual points are dense but noisy, and directly incorporating them can increase computational burden and degrade performance. Therefore, we introduce a spatial difference-driven adaptive filtering module that selectively extracts valuable pseudo points from these virtual points based on density and distance, enhancing the density of medium-range targets. Subsequently, we propose a noise-robust sparse feature encoder that incorporates noise-robust feature extraction and fine-grained feature enhancement. Noise-robust feature extraction exploits the 2D image space to reduce the impact of noisy points, while fine-grained feature enhancement boosts sparse geometric features through inner-voxel neighborhood point aggregation and downsampled voxel aggregation. The results on the SemanticKITTI and nuScenes, two large-scale benchmark data sets, have validated effectiveness, significantly improving 2.89\% mIoU with the introduction of 7.7\% virtual points on nuScenes.</li>
</ul>

<h3>Title: BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models</h3>
<ul>
<li><strong>Authors: </strong>Encheng Su, Hu Cao, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01452">https://arxiv.org/abs/2504.01452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01452">https://arxiv.org/pdf/2504.01452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01452]] BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models(https://arxiv.org/abs/2504.01452)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of polyps and skin lesions is essential for diagnosing colorectal and skin cancers. While various segmentation methods for polyps and skin lesions using fully supervised deep learning techniques have been developed, the pixel-level annotation of medical images by doctors is both time-consuming and costly. Foundational vision models like the Segment Anything Model (SAM) have demonstrated superior performance; however, directly applying SAM to medical segmentation may not yield satisfactory results due to the lack of domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a SAM-guided weakly supervised prompting and boundary refinement network for the segmentation of polyps and skin lesions. Specifically, we fine-tune SAM combined with a CNN module to learn local features. We introduce a WeakBox with two functions: automatically generating box prompts for the SAM model and using our proposed Multi-choice Mask-to-Box (MM2B) transformation for rough mask-to-box conversion, addressing the mismatch between coarse labels and precise predictions. Additionally, we apply scale consistency (SC) loss for prediction scale alignment. Our DetailRefine module enhances boundary precision and segmentation accuracy by refining coarse predictions using a limited amount of ground truth labels. This comprehensive approach enables BiSeg-SAM to achieve excellent multi-task segmentation performance. Our method demonstrates significant superiority over state-of-the-art (SOTA) methods when tested on five polyp datasets and one skin cancer dataset.</li>
</ul>

<h3>Title: Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker</h3>
<ul>
<li><strong>Authors: </strong>Ting Meng, Chunyun Fu, Xiangyan Yan, Zheng Liang, Pan Ji, Jianwen Wang, Tao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01457">https://arxiv.org/abs/2504.01457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01457">https://arxiv.org/pdf/2504.01457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01457]] Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker(https://arxiv.org/abs/2504.01457)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Multi-object tracking plays a crucial role in various applications, such as autonomous driving and security surveillance. This study introduces Deep LG-Track, a novel multi-object tracker that incorporates three key enhancements to improve the tracking accuracy and robustness. First, an adaptive Kalman filter is developed to dynamically update the covariance of measurement noise based on detection confidence and trajectory disappearance. Second, a novel cost matrix is formulated to adaptively fuse motion and appearance information, leveraging localization confidence and detection confidence as weighting factors. Third, a dynamic appearance feature updating strategy is introduced, adjusting the relative weighting of historical and current appearance features based on appearance clarity and localization accuracy. Comprehensive evaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep LG-Track consistently outperforms state-of-the-art trackers across multiple performance metrics, highlighting its effectiveness in multi-object tracking tasks.</li>
</ul>

<h3>Title: A Prefixed Patch Time Series Transformer for Two-Point Boundary Value Problems in Three-Body Problems</h3>
<ul>
<li><strong>Authors: </strong>Akira Hatakeyama, Shota Ito, Toshihiko Yanase, Naoya Ozaki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01464">https://arxiv.org/abs/2504.01464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01464">https://arxiv.org/pdf/2504.01464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01464]] A Prefixed Patch Time Series Transformer for Two-Point Boundary Value Problems in Three-Body Problems(https://arxiv.org/abs/2504.01464)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Two-point boundary value problems for cislunar trajectories present significant challenges in circler restricted three body problem, making traditional analytical methods like Lambert's problem inapplicable. This study proposes a novel approach using a prefixed patch time series Transformer model that automates the solution of two-point boundary value problems from lunar flyby to arbitrary terminal conditions. Using prefix tokens of terminal conditions in our deep generative model enables solving boundary value problems in three-body dynamics. The training dataset consists of trajectories obtained through forward propagation rather than solving boundary value problems directly. The model demonstrates potential practical utility for preliminary trajectory design in cislunar mission scenarios.</li>
</ul>

<h3>Title: Detecting Lip-Syncing Deepfakes: Vision Temporal Transformer for Analyzing Mouth Inconsistencies</h3>
<ul>
<li><strong>Authors: </strong>Soumyya Kanti Datta, Shan Jia, Siwei Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01470">https://arxiv.org/abs/2504.01470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01470">https://arxiv.org/pdf/2504.01470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01470]] Detecting Lip-Syncing Deepfakes: Vision Temporal Transformer for Analyzing Mouth Inconsistencies(https://arxiv.org/abs/2504.01470)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deepfakes are AI-generated media in which the original content is digitally altered to create convincing but manipulated images, videos, or audio. Among the various types of deepfakes, lip-syncing deepfakes are one of the most challenging deepfakes to detect. In these videos, a person's lip movements are synthesized to match altered or entirely new audio using AI models. Therefore, unlike other types of deepfakes, the artifacts in lip-syncing deepfakes are confined to the mouth region, making them more subtle and, thus harder to discern. In this paper, we propose LIPINC-V2, a novel detection framework that leverages a combination of vision temporal transformer with multihead cross-attention to detect lip-syncing deepfakes by identifying spatiotemporal inconsistencies in the mouth region. These inconsistencies appear across adjacent frames and persist throughout the video. Our model can successfully capture both short-term and long-term variations in mouth movement, enhancing its ability to detect these inconsistencies. Additionally, we created a new lip-syncing deepfake dataset, LipSyncTIMIT, which was generated using five state-of-the-art lip-syncing models to simulate real-world scenarios. Extensive experiments on our proposed LipSyncTIMIT dataset and two other benchmark deepfake datasets demonstrate that our model achieves state-of-the-art performance. The code and the dataset are available at this https URL .</li>
</ul>

<h3>Title: ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction</h3>
<ul>
<li><strong>Authors: </strong>Yuejiao Su, Yi Wang, Qiongyang Hu, Chuang Yang, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01472">https://arxiv.org/abs/2504.01472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01472">https://arxiv.org/pdf/2504.01472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01472]] ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction(https://arxiv.org/abs/2504.01472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Egocentric interaction perception is one of the essential branches in investigating human-environment interaction, which lays the basis for developing next-generation intelligent systems. However, existing egocentric interaction understanding methods cannot yield coherent textual and pixel-level responses simultaneously according to user queries, which lacks flexibility for varying downstream application requirements. To comprehend egocentric interactions exhaustively, this paper presents a novel task named Egocentric Interaction Reasoning and pixel Grounding (Ego-IRG). Taking an egocentric image with the query as input, Ego-IRG is the first task that aims to resolve the interactions through three crucial steps: analyzing, answering, and pixel grounding, which results in fluent textual and fine-grained pixel-level responses. Another challenge is that existing datasets cannot meet the conditions for the Ego-IRG task. To address this limitation, this paper creates the Ego-IRGBench dataset based on extensive manual efforts, which includes over 20k egocentric images with 1.6 million queries and corresponding multimodal responses about interactions. Moreover, we design a unified ANNEXE model to generate text- and pixel-level outputs utilizing multimodal large language models, which enables a comprehensive interpretation of egocentric interactions. The experiments on the Ego-IRGBench exhibit the effectiveness of our ANNEXE model compared with other works.</li>
</ul>

<h3>Title: Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Junlong Ren, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01476">https://arxiv.org/abs/2504.01476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01476">https://arxiv.org/pdf/2504.01476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01476]] Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction(https://arxiv.org/abs/2504.01476)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-modal 3D retrieval is a critical yet challenging task, aiming to achieve bi-directional retrieval between 3D and text modalities. Current methods predominantly rely on a certain 3D representation (e.g., point cloud), with few exploiting the 2D-3D consistency and complementary relationships, which constrains their performance. To bridge this gap, we propose to adopt multi-view images and point clouds to jointly represent 3D shapes, facilitating tri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D retrieval. Notably, we introduce tri-modal reconstruction to improve the generalization ability of encoders. Given point features, we reconstruct image features under the guidance of text features, and vice versa. With well-aligned point cloud and multi-view image features, we aggregate them as multimodal embeddings through fine-grained 2D-3D fusion to enhance geometric and semantic understanding. Recognizing the significant noise in current datasets where many 3D shapes and texts share similar semantics, we employ hard negative contrastive training to emphasize harder negatives with greater significance, leading to robust discriminative embeddings. Extensive experiments on the Text2Shape dataset demonstrate that our method significantly outperforms previous state-of-the-art methods in both shape-to-text and text-to-shape retrieval tasks by a substantial margin.</li>
</ul>

<h3>Title: Identifying Obfuscated Code through Graph-Based Semantic Analysis of Binary Code</h3>
<ul>
<li><strong>Authors: </strong>Roxane Cohen (LAMSADE), Robin David, Florian Yger (LITIS), Fabrice Rossi (CEREMADE)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01481">https://arxiv.org/abs/2504.01481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01481">https://arxiv.org/pdf/2504.01481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01481]] Identifying Obfuscated Code through Graph-Based Semantic Analysis of Binary Code(https://arxiv.org/abs/2504.01481)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Protecting sensitive program content is a critical issue in various situations, ranging from legitimate use cases to unethical contexts. Obfuscation is one of the most used techniques to ensure such protection. Consequently, attackers must first detect and characterize obfuscation before launching any attack against it. This paper investigates the problem of function-level obfuscation detection using graph-based approaches, comparing algorithms, from elementary baselines to promising techniques like GNN (Graph Neural Networks), on different feature choices. We consider various obfuscation types and obfuscators, resulting in two complex datasets. Our findings demonstrate that GNNs need meaningful features that capture aspects of function semantics to outperform baselines. Our approach shows satisfactory results, especially in a challenging 11-class classification task and in a practical malware analysis example.</li>
</ul>

<h3>Title: A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown Lévy Process Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Qihao Ye, Xiaochuan Tian, Yuhua Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01482">https://arxiv.org/abs/2504.01482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01482">https://arxiv.org/pdf/2504.01482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01482]] A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown Lévy Process Dynamics(https://arxiv.org/abs/2504.01482)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper develops a model-based framework for continuous-time policy evaluation (CTPE) in reinforcement learning, incorporating both Brownian and Lévy noise to model stochastic dynamics influenced by rare and extreme events. Our approach formulates the policy evaluation problem as solving a partial integro-differential equation (PIDE) for the value function with unknown coefficients. A key challenge in this setting is accurately recovering the unknown coefficients in the stochastic dynamics, particularly when driven by Lévy processes with heavy tail effects. To address this, we propose a robust numerical approach that effectively handles both unbiased and censored trajectory datasets. This method combines maximum likelihood estimation with an iterative tail correction mechanism, improving the stability and accuracy of coefficient recovery. Additionally, we establish a theoretical bound for the policy evaluation error based on coefficient recovery error. Through numerical experiments, we demonstrate the effectiveness and robustness of our method in recovering heavy-tailed Lévy dynamics and verify the theoretical error analysis in policy evaluation.</li>
</ul>

<h3>Title: Approximate Agreement Algorithms for Byzantine Collaborative Learning</h3>
<ul>
<li><strong>Authors: </strong>Tijana Milentijević, Mélanie Cambus, Darya Melnyk, Stefan Schmid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01504">https://arxiv.org/abs/2504.01504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01504">https://arxiv.org/pdf/2504.01504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01504]] Approximate Agreement Algorithms for Byzantine Collaborative Learning(https://arxiv.org/abs/2504.01504)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In Byzantine collaborative learning, $n$ clients in a peer-to-peer network collectively learn a model without sharing their data by exchanging and aggregating stochastic gradient estimates. Byzantine clients can prevent others from collecting identical sets of gradient estimates. The aggregation step thus needs to be combined with an efficient (approximate) agreement subroutine to ensure convergence of the training process. In this work, we study the geometric median aggregation rule for Byzantine collaborative learning. We show that known approaches do not provide theoretical guarantees on convergence or gradient quality in the agreement subroutine. To satisfy these theoretical guarantees, we present a hyperbox algorithm for geometric median aggregation. We practically evaluate our algorithm in both centralized and decentralized settings under Byzantine attacks on non-i.i.d. data. We show that our geometric median-based approaches can tolerate sign-flip attacks better than known mean-based approaches from the literature.</li>
</ul>

<h3>Title: PROPHET: An Inferable Future Forecasting Benchmark with Causal Intervened Likelihood Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhengwei Tao, Zhi Jin, Bincheng Li, Xiaoying Bai, Haiyan Zhao, Chengfeng Dou, Xiancai Chen, Jia Li, Linyu Li, Chongyang Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01509">https://arxiv.org/abs/2504.01509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01509">https://arxiv.org/pdf/2504.01509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01509]] PROPHET: An Inferable Future Forecasting Benchmark with Causal Intervened Likelihood Estimation(https://arxiv.org/abs/2504.01509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Predicting future events stands as one of the ultimate aspirations of artificial intelligence. Recent advances in large language model (LLM)-based systems have shown remarkable potential in forecasting future events, thereby garnering significant interest in the research community. Currently, several benchmarks have been established to evaluate the forecasting capabilities by formalizing the event prediction as a retrieval-augmented generation (RAG) and reasoning task. In these benchmarks, each prediction question is answered with relevant retrieved news articles. However, because there is no consideration on whether the questions can be supported by valid or sufficient supporting rationales, some of the questions in these benchmarks may be inherently noninferable. To address this issue, we introduce a new benchmark, PROPHET, which comprises inferable forecasting questions paired with relevant news for retrieval. To ensure the inferability of the benchmark, we propose Causal Intervened Likelihood (CIL), a statistical measure that assesses inferability through causal inference. In constructing this benchmark, we first collected recent trend forecasting questions and then filtered the data using CIL, resulting in an inferable benchmark for event prediction. Through extensive experiments, we first demonstrate the validity of CIL and in-depth investigations into event prediction with the aid of CIL. Subsequently, we evaluate several representative prediction systems on PROPHET, drawing valuable insights for future directions.</li>
</ul>

<h3>Title: High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01512">https://arxiv.org/abs/2504.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01512">https://arxiv.org/pdf/2504.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01512]] High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model(https://arxiv.org/abs/2504.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency.</li>
</ul>

<h3>Title: Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Wang, Duo Peng, Feng Chen, Yuwei Yang, Yinjie Lei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01515">https://arxiv.org/abs/2504.01515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01515">https://arxiv.org/pdf/2504.01515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01515]] Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis(https://arxiv.org/abs/2504.01515)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Conditional image synthesis is a crucial task with broad applications, such as artistic creation and virtual reality. However, current generative methods are often task-oriented with a narrow scope, handling a restricted condition with constrained applicability. In this paper, we propose a novel approach that treats conditional image synthesis as the modular combination of diverse fundamental condition units. Specifically, we divide conditions into three primary units: text, layout, and drag. To enable effective control over these conditions, we design a dedicated alignment module for each. For the text condition, we introduce a Dense Concept Alignment (DCA) module, which achieves dense visual-text alignment by drawing on diverse textual concepts. For the layout condition, we propose a Dense Geometry Alignment (DGA) module to enforce comprehensive geometric constraints that preserve the spatial configuration. For the drag condition, we introduce a Dense Motion Alignment (DMA) module to apply multi-level motion regularization, ensuring that each pixel follows its desired trajectory without visual artifacts. By flexibly inserting and combining these alignment modules, our framework enhances the model's adaptability to diverse conditional generation tasks and greatly expands its application range. Extensive experiments demonstrate the superior performance of our framework across a variety of conditions, including textual description, segmentation mask (bounding box), drag manipulation, and their combinations. Code is available at this https URL.</li>
</ul>

<h3>Title: Chain of Correction for Full-text Speech Recognition with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Tang, Dong Wang, Zhikai Zhou, Yong Liu, Shen Huang, Shidong Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01519">https://arxiv.org/abs/2504.01519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01519">https://arxiv.org/pdf/2504.01519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01519]] Chain of Correction for Full-text Speech Recognition with Large Language Models(https://arxiv.org/abs/2504.01519)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Full-text error correction with Large Language Models (LLMs) for Automatic Speech Recognition (ASR) has gained increased attention due to its potential to correct errors across long contexts and address a broader spectrum of error types, including punctuation restoration and inverse text normalization. Nevertheless, many challenges persist, including issues related to stability, controllability, completeness, and fluency. To mitigate these challenges, this paper proposes the Chain of Correction (CoC) for full-text error correction with LLMs, which corrects errors segment by segment using pre-recognized text as guidance within a regular multi-turn chat format. The CoC also uses pre-recognized full text for context, allowing the model to better grasp global semantics and maintain a comprehensive overview of the entire content. Utilizing the open-sourced full-text error correction dataset ChFT, we fine-tune a pre-trained LLM to evaluate the performance of the CoC framework. Experimental results demonstrate that the CoC effectively corrects errors in full-text ASR outputs, significantly outperforming baseline and benchmark systems. We further analyze how to set the correction threshold to balance under-correction and over-rephrasing, extrapolate the CoC model on extremely long ASR outputs, and investigate whether other types of information can be employed to guide the error correction process.</li>
</ul>

<h3>Title: Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jincheng Zhong, Xiangcheng Zhang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01521">https://arxiv.org/abs/2504.01521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01521">https://arxiv.org/pdf/2504.01521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01521]] Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model(https://arxiv.org/abs/2504.01521)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have revolutionized generative modeling. However, the impressive and vivid outputs they produce often come at the cost of significant model scaling and increased computational demands. Consequently, building personalized diffusion models based on off-the-shelf models has emerged as an appealing alternative. In this paper, we introduce a novel perspective on conditional generation for transferring a pre-trained model. From this viewpoint, we propose *Domain Guidance*, a straightforward transfer approach that leverages pre-trained knowledge to guide the sampling process toward the target domain. Domain Guidance shares a formulation similar to advanced classifier-free guidance, facilitating better domain alignment and higher-quality generations. We provide both empirical and theoretical analyses of the mechanisms behind Domain Guidance. Our experimental results demonstrate its substantial effectiveness across various transfer benchmarks, achieving over a 19.6% improvement in FID and a 23.4% improvement in FD$_\text{DINOv2}$ compared to standard fine-tuning. Notably, existing fine-tuned models can seamlessly integrate Domain Guidance to leverage these benefits, without additional training.</li>
</ul>

<h3>Title: Beyond Nearest Neighbor Interpolation in Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Olivier Rukundo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01527">https://arxiv.org/abs/2504.01527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01527">https://arxiv.org/pdf/2504.01527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01527]] Beyond Nearest Neighbor Interpolation in Data Augmentation(https://arxiv.org/abs/2504.01527)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Avoiding the risk of undefined categorical labels using nearest neighbor interpolation overlooks the risk of exacerbating pixel level annotation errors in data augmentation. To simultaneously avoid these risks, the author modified convolutional neural networks data transformation functions by incorporating a modified geometric transformation function to improve the quality of augmented data by removing the reliance on nearest neighbor interpolation and integrating a mean based class filtering mechanism to handle undefined categorical labels with alternative interpolation algorithms. Experiments on semantic segmentation tasks using three medical image datasets demonstrated both qualitative and quantitative improvements with alternative interpolation algorithms.</li>
</ul>

<h3>Title: LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Yang, Jie Peng, Zhen Tan, Tianlong Chen, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01533">https://arxiv.org/abs/2504.01533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01533">https://arxiv.org/pdf/2504.01533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01533]] LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution(https://arxiv.org/abs/2504.01533)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for defending against jailbreak attacks are primarily based on auxiliary models. These strategies, however, often require extensive data collection or training. We propose LightDefense, a lightweight defense mechanism targeted at white-box models, which utilizes a safety-oriented direction to adjust the probabilities of tokens in the vocabulary, making safety disclaimers appear among the top tokens after sorting tokens by probability in descending order. We further innovatively leverage LLM's uncertainty about prompts to measure their harmfulness and adaptively adjust defense strength, effectively balancing safety and helpfulness. The effectiveness of LightDefense in defending against 5 attack methods across 2 target LLMs, without compromising helpfulness to benign user queries, highlights its potential as a novel and lightweight defense mechanism, enhancing security of LLMs.</li>
</ul>

<h3>Title: Context-Aware Toxicity Detection in Multiplayer Games: Integrating Domain-Adaptive Pretraining and Match Metadata</h3>
<ul>
<li><strong>Authors: </strong>Adrien Schurger-Foy, Rafal Dariusz Kocielnik, Caglar Gulcehre, R. Michael Alvarez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01534">https://arxiv.org/abs/2504.01534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01534">https://arxiv.org/pdf/2504.01534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01534]] Context-Aware Toxicity Detection in Multiplayer Games: Integrating Domain-Adaptive Pretraining and Match Metadata(https://arxiv.org/abs/2504.01534)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>The detrimental effects of toxicity in competitive online video games are widely acknowledged, prompting publishers to monitor player chat conversations. This is challenging due to the context-dependent nature of toxicity, often spread across multiple messages or informed by non-textual interactions. Traditional toxicity detectors focus on isolated messages, missing the broader context needed for accurate moderation. This is especially problematic in video games, where interactions involve specialized slang, abbreviations, and typos, making it difficult for standard models to detect toxicity, especially given its rarity. We adapted RoBERTa LLM to support moderation tailored to video games, integrating both textual and non-textual context. By enhancing pretrained embeddings with metadata and addressing the unique slang and language quirks through domain adaptive pretraining, our method better captures the nuances of player interactions. Using two gaming datasets - from Defense of the Ancients 2 (DOTA 2) and Call of Duty$^\circledR$: Modern Warfare$^\circledR$III (MWIII) we demonstrate which sources of context (metadata, prior interactions...) are most useful, how to best leverage them to boost performance, and the conditions conducive to doing so. This work underscores the importance of context-aware and domain-specific approaches for proactive moderation.</li>
</ul>

<h3>Title: From Smør-re-brød to Subwords: Training LLMs on Danish, One Morpheme at a Time</h3>
<ul>
<li><strong>Authors: </strong>Mikkel Wildner Kildeberg, Emil Allerslev Schledermann, Nicolaj Larsen, Rob van der Goot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01540">https://arxiv.org/abs/2504.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01540">https://arxiv.org/pdf/2504.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01540]] From Smør-re-brød to Subwords: Training LLMs on Danish, One Morpheme at a Time(https://arxiv.org/abs/2504.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, segmentation</a></li>
<li><strong>Abstract: </strong>The best performing transformer-based language models use subword tokenization techniques, such as Byte-Pair-Encoding (BPE). However, these approaches often overlook linguistic principles, such as morphological segmentation, which we believe is fundamental for understanding language-specific word structure. In this study, we leverage an annotated Danish morphological dataset to train a semisupervised model for morphological segmentation, enabling the development of tokenizers optimized for Danish morphology. We evaluate four distinct tokenizers, including two custom morphological tokenizers, by analyzing their performance in morphologically segmenting Danish words. Additionally, we train two generative transformer models, \textit{CerebrasGPT-111M} and \textit{LLaMA-3.2 1B}, using these tokenizers and evaluate their downstream performance. Our findings reveal that our custom-developed tokenizers substantially enhance morphological segmentation, achieving an F1 score of 58.84, compared to 39.28 achieved by a Danish BPE tokenizer. In downstream tasks, models trained with our morphological tokenizers outperform those using BPE tokenizers across different evaluation metrics. These results highlight that incorporating Danish morphological segmentation strategies into tokenizers leads to improved performance in generative transformer models on Danish language</li>
</ul>

<h3>Title: Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation</h3>
<ul>
<li><strong>Authors: </strong>Amanda Myntti, Erik Henriksson, Veronika Laippala, Sampo Pyysalo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01542">https://arxiv.org/abs/2504.01542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01542">https://arxiv.org/pdf/2504.01542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01542]] Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation(https://arxiv.org/abs/2504.01542)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pretraining data curation is a cornerstone in Large Language Model (LLM) development, leading to growing research on quality filtering of large web corpora. From statistical quality flags to LLM-based labeling systems, datasets are divided into categories, frequently reducing to a binary: those passing the filters deemed as valuable examples, others discarded as useless or detrimental. However, a more detailed understanding of the contribution of different kinds of texts to model performance is still largely lacking. In this article, we present the first study utilizing registers (also known as genres) - a widely used standard in corpus linguistics to model linguistic variation - to curate pretraining datasets and investigate the effect of register on the performance of LLMs. We perform comparative studies by training models with register classified data and evaluating them using standard benchmarks, and show that the register of pretraining data substantially affects model performance. We uncover surprising relationships between the pretraining material and the resulting models: using the News register results in subpar performance, and on the contrary, including the Opinion class, covering texts such as reviews and opinion blogs, is highly beneficial. While a model trained on the entire unfiltered dataset outperforms those trained on datasets limited to a single register, combining well-performing registers like How-to-Instructions, Informational Description, and Opinion leads to major improvements. Furthermore, analysis of individual benchmark results reveals key differences in the strengths and drawbacks of specific register classes as pretraining data. These findings show that register is an important explainer of model variation and can facilitate more deliberate future data selection practices.</li>
</ul>

<h3>Title: Semi-Supervised Biomedical Image Segmentation via Diffusion Models and Teacher-Student Co-Training</h3>
<ul>
<li><strong>Authors: </strong>Luca Ciampi, Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01547">https://arxiv.org/abs/2504.01547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01547">https://arxiv.org/pdf/2504.01547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01547]] Semi-Supervised Biomedical Image Segmentation via Diffusion Models and Teacher-Student Co-Training(https://arxiv.org/abs/2504.01547)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Supervised deep learning for semantic segmentation has achieved excellent results in accurately identifying anatomical and pathological structures in medical images. However, it often requires large annotated training datasets, which limits its scalability in clinical settings. To address this challenge, semi-supervised learning is a well-established approach that leverages both labeled and unlabeled data. In this paper, we introduce a novel semi-supervised teacher-student framework for biomedical image segmentation, inspired by the recent success of generative models. Our approach leverages denoising diffusion probabilistic models (DDPMs) to generate segmentation masks by progressively refining noisy inputs conditioned on the corresponding images. The teacher model is first trained in an unsupervised manner using a cycle-consistency constraint based on noise-corrupted image reconstruction, enabling it to generate informative semantic masks. Subsequently, the teacher is integrated into a co-training process with a twin-student network. The student learns from ground-truth labels when available and from teacher-generated pseudo-labels otherwise, while the teacher continuously improves its pseudo-labeling capabilities. Finally, to further enhance performance, we introduce a multi-round pseudo-label generation strategy that iteratively improves the pseudo-labeling process. We evaluate our approach on multiple biomedical imaging benchmarks, spanning multiple imaging modalities and segmentation tasks. Experimental results show that our method consistently outperforms state-of-the-art semi-supervised techniques, highlighting its effectiveness in scenarios with limited annotated data. The code to replicate our experiments can be found at this https URL</li>
</ul>

<h3>Title: Representation Bending for Large Language Model Safety</h3>
<ul>
<li><strong>Authors: </strong>Ashkan Yousefpour, Taeheon Kim, Ryan S. Kwon, Seungbeen Lee, Wonje Jeung, Seungju Han, Alvin Wan, Harrison Ngan, Youngjae Yu, Jonghyun Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01550">https://arxiv.org/abs/2504.01550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01550">https://arxiv.org/pdf/2504.01550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01550]] Representation Bending for Large Language Model Safety(https://arxiv.org/abs/2504.01550)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools, but their inherent safety risks - ranging from harmful content generation to broader societal harms - pose significant challenges. These risks can be amplified by the recent adversarial attacks, fine-tuning vulnerabilities, and the increasing deployment of LLMs in high-stakes environments. Existing safety-enhancing techniques, such as fine-tuning with human feedback or adversarial training, are still vulnerable as they address specific threats and often fail to generalize across unseen attacks, or require manual system-level defenses. This paper introduces RepBend, a novel approach that fundamentally disrupts the representations underlying harmful behaviors in LLMs, offering a scalable solution to enhance (potentially inherent) safety. RepBend brings the idea of activation steering - simple vector arithmetic for steering model's behavior during inference - to loss-based fine-tuning. Through extensive evaluation, RepBend achieves state-of-the-art performance, outperforming prior methods such as Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success rates across diverse jailbreak benchmarks, all with negligible reduction in model usability and general capabilities.</li>
</ul>

<h3>Title: DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image</h3>
<ul>
<li><strong>Authors: </strong>Jijun Xiang, Xuan Zhu, Xianqi Wang, Yu Wang, Hong Zhang, Fei Guo, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01596">https://arxiv.org/abs/2504.01596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01596">https://arxiv.org/pdf/2504.01596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01596]] DEPTHOR: Depth Enhancement from a Practical Light-Weight dToF Sensor and RGB Image(https://arxiv.org/abs/2504.01596)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Depth enhancement, which uses RGB images as guidance to convert raw signals from dToF into high-precision, dense depth maps, is a critical task in computer vision. Although existing super-resolution-based methods show promising results on public datasets, they often rely on idealized assumptions like accurate region correspondences and reliable dToF inputs, overlooking calibration errors that cause misalignment and anomaly signals inherent to dToF imaging, limiting real-world applicability. To address these challenges, we propose a novel completion-based method, named DEPTHOR, featuring advances in both the training strategy and model architecture. First, we propose a method to simulate real-world dToF data from the accurate ground truth in synthetic datasets to enable noise-robust training. Second, we design a novel network that incorporates monocular depth estimation (MDE), leveraging global depth relationships and contextual information to improve prediction in challenging regions. On the ZJU-L5 dataset, our training strategy significantly enhances depth completion models, achieving results comparable to depth super-resolution methods, while our model achieves state-of-the-art results, improving Rel and RMSE by 27% and 18%, respectively. On a more challenging set of dToF samples we collected, our method outperforms SOTA methods on preliminary stereo-based GT, improving Rel and RMSE by 23% and 22%, respectively. Our Code is available at this https URL</li>
</ul>

<h3>Title: A topology-preserving three-stage framework for fully-connected coronary artery extraction</h3>
<ul>
<li><strong>Authors: </strong>Yuehui Qiu, Dandan Shan, Yining Wang, Pei Dong, Dijia Wu, Xinnian Yang, Qingqi Hong, Dinggang Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01597">https://arxiv.org/abs/2504.01597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01597">https://arxiv.org/pdf/2504.01597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01597]] A topology-preserving three-stage framework for fully-connected coronary artery extraction(https://arxiv.org/abs/2504.01597)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Coronary artery extraction is a crucial prerequisite for computer-aided diagnosis of coronary artery disease. Accurately extracting the complete coronary tree remains challenging due to several factors, including presence of thin distal vessels, tortuous topological structures, and insufficient contrast. These issues often result in over-segmentation and under-segmentation in current segmentation methods. To address these challenges, we propose a topology-preserving three-stage framework for fully-connected coronary artery extraction. This framework includes vessel segmentation, centerline reconnection, and missing vessel reconstruction. First, we introduce a new centerline enhanced loss in the segmentation process. Second, for the broken vessel segments, we further propose a regularized walk algorithm to integrate distance, probabilities predicted by a centerline classifier, and directional cosine similarity, for reconnecting the centerlines. Third, we apply implicit neural representation and implicit modeling, to reconstruct the geometric model of the missing vessels. Experimental results show that our proposed framework outperforms existing methods, achieving Dice scores of 88.53\% and 85.07\%, with Hausdorff Distances (HD) of 1.07mm and 1.63mm on ASOCA and PDSCA datasets, respectively. Code will be available at this https URL.</li>
</ul>

<h3>Title: Multi-Relation Graph-Kernel Strengthen Network for Graph-Level Clustering</h3>
<ul>
<li><strong>Authors: </strong>Renda Han, Guangzhen Yao, Wenxin Zhang, Yu Li, Wen Xin, Huajie Lei, Mengfei Li, Zeyu Zhang, Chengze Du, Yahe Tian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01605">https://arxiv.org/abs/2504.01605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01605">https://arxiv.org/pdf/2504.01605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01605]] Multi-Relation Graph-Kernel Strengthen Network for Graph-Level Clustering(https://arxiv.org/abs/2504.01605)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph-level clustering is a fundamental task of data mining, aiming at dividing unlabeled graphs into distinct groups. However, existing deep methods that are limited by pooling have difficulty extracting diverse and complex graph structure features, while traditional graph kernel methods rely on exhaustive substructure search, unable to adaptive handle multi-relational data. This limitation hampers producing robust and representative graph-level embeddings. To address this issue, we propose a novel Multi-Relation Graph-Kernel Strengthen Network for Graph-Level Clustering (MGSN), which integrates multi-relation modeling with graph kernel techniques to fully leverage their respective advantages. Specifically, MGSN constructs multi-relation graphs to capture diverse semantic relationships between nodes and graphs, which employ graph kernel methods to extract graph similarity features, enriching the representation space. Moreover, a relation-aware representation refinement strategy is designed, which adaptively aligns multi-relation information across views while enhancing graph-level features through a progressive fusion process. Extensive experiments on multiple benchmark datasets demonstrate the superiority of MGSN over state-of-the-art methods. The results highlight its ability to leverage multi-relation structures and graph kernel features, establishing a new paradigm for robust graph-level clustering.</li>
</ul>

<h3>Title: Vers une modélisation de la confiance dans le renseignement sur les menaces cyber</h3>
<ul>
<li><strong>Authors: </strong>Laurent Bobelin, Sabine Frittella, Mariam Wehbe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01606">https://arxiv.org/abs/2504.01606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01606">https://arxiv.org/pdf/2504.01606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01606]] Vers une modélisation de la confiance dans le renseignement sur les menaces cyber(https://arxiv.org/abs/2504.01606)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Cyber threat intelligence (CTI) is essential for effective system defense. CTI is a collection of information about current or past threats to a computer system. This information is gathered by an agent through observation, or based on a set of sources. Building intelligence only makes sense if you have confidence in it. To achieve this, it is necessary to estimate the confidence in each piece of information gathered, taking into account the different dimensions that can make it up: reliability of the source, competence, plausibility of the information, credibility of the information, for example. The information gathered must then be combined with other information to consolidate an agent's knowledge. Recent advances have been made in the theory underlying the modeling of trust for decision-making based on uncertain information, notably by using multivalued logic. This approach makes it possible to deal with unknown values of trust-building parameters, or to easily integrate dimensions. In this article we present the problem of CTI and CTI information sharing, and the reasons that led us to use a logic-based solution for an initial implementation.</li>
</ul>

<h3>Title: 3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01619">https://arxiv.org/abs/2504.01619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01619">https://arxiv.org/pdf/2504.01619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01619]] 3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian Splatting(https://arxiv.org/abs/2504.01619)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-3D generation have shown remarkable results by leveraging 3D priors in combination with 2D diffusion. However, previous methods utilize 3D priors that lack detailed and complex structural information, limiting them to generating simple objects and presenting challenges for creating intricate structures such as bonsai. In this paper, we propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with complex structures. Technically, we first design a trainable 3D space colonization algorithm to produce bonsai structures, which are then enhanced through random sampling and point cloud augmentation to serve as the 3D Gaussian priors. We introduce two bonsai generation pipelines with distinct structural levels: fine structure conditioned generation, which initializes 3D Gaussians using a 3D structure prior to produce detailed and complex bonsai, and coarse structure conditioned generation, which employs a multi-view structure consistency module to align 2D and 3D structures. Moreover, we have compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental results demonstrate that 3DBonsai significantly outperforms existing methods, providing a new benchmark for structure-aware 3D bonsai generation.</li>
</ul>

<h3>Title: A Conic Transformation Approach for Solving the Perspective-Three-Point Problem</h3>
<ul>
<li><strong>Authors: </strong>Haidong Wu, Snehal Bhayani, Janne Heikkilä</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01620">https://arxiv.org/abs/2504.01620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01620">https://arxiv.org/pdf/2504.01620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01620]] A Conic Transformation Approach for Solving the Perspective-Three-Point Problem(https://arxiv.org/abs/2504.01620)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a conic transformation method to solve the Perspective-Three-Point (P3P) problem. In contrast to the current state-of-the-art solvers, which formulate the P3P problem by intersecting two conics and constructing a degenerate conic to find the intersection, our approach builds upon a new formulation based on a transformation that maps the two conics to a new coordinate system, where one of the conics becomes a standard parabola in a canonical form. This enables expressing one variable in terms of the other variable, and as a consequence, substantially simplifies the problem of finding the conic intersection. Moreover, the polynomial coefficients are fast to compute, and we only need to determine the real-valued intersection points, which avoids the requirement of using computationally expensive complex arithmetic. While the current state-of-the-art methods reduce the conic intersection problem to solving a univariate cubic equation, our approach, despite resulting in a quartic equation, is still faster thanks to this new simplified formulation. Extensive evaluations demonstrate that our method achieves higher speed while maintaining robustness and stability comparable to state-of-the-art methods.</li>
</ul>

<h3>Title: Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions</h3>
<ul>
<li><strong>Authors: </strong>Giulia Marchiori Pietrosanti, Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01632">https://arxiv.org/abs/2504.01632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01632">https://arxiv.org/pdf/2504.01632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01632]] Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions(https://arxiv.org/abs/2504.01632)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The robustness of DNNs is a crucial factor in safety-critical applications, particularly in complex and dynamic environments where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remained underexplored. This paper fills this gap by introducing specialized metrics for benchmarking the spatial robustness of segmentation models, alongside with an evaluation framework to assess the impact of localized corruptions. Furthermore, we uncover the inherent complexity of characterizing worst-case robustness using a single localized adversarial perturbation. To address this, we propose region-aware multi-attack adversarial analysis, a method that enables a deeper understanding of model robustness against adversarial perturbations applied to specific regions. The proposed metrics and analysis were evaluated on 15 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones and vice-versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.</li>
</ul>

<h3>Title: FlowR: Flowing from Sparse to Dense 3D Reconstructions</h3>
<ul>
<li><strong>Authors: </strong>Tobias Fischer, Samuel Rota Bulò, Yung-Hsu Yang, Nikhil Varma Keetha, Lorenzo Porzi, Norman Müller, Katja Schwarz, Jonathon Luiten, Marc Pollefeys, Peter Kontschieder</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01647">https://arxiv.org/abs/2504.01647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01647">https://arxiv.org/pdf/2504.01647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01647]] FlowR: Flowing from Sparse to Dense 3D Reconstructions(https://arxiv.org/abs/2504.01647)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D Gaussian splatting enables high-quality novel view synthesis (NVS) at real-time frame rates. However, its quality drops sharply as we depart from the training views. Thus, dense captures are needed to match the high-quality expectations of some applications, e.g. Virtual Reality (VR). However, such dense captures are very laborious and expensive to obtain. Existing works have explored using 2D generative models to alleviate this requirement by distillation or generating additional training views. These methods are often conditioned only on a handful of reference input views and thus do not fully exploit the available 3D information, leading to inconsistent generation results and reconstruction artifacts. To tackle this problem, we propose a multi-view, flow matching model that learns a flow to connect novel view renderings from possibly sparse reconstructions to renderings that we expect from dense reconstructions. This enables augmenting scene captures with novel, generated views to improve reconstruction quality. Our model is trained on a novel dataset of 3.6M image pairs and can process up to 45 views at 540x960 resolution (91K tokens) on one H100 GPU in a single forward pass. Our pipeline consistently improves NVS in sparse- and dense-view scenarios, leading to higher-quality reconstructions than prior works across multiple, widely-used NVS benchmarks.</li>
</ul>

<h3>Title: ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01648">https://arxiv.org/abs/2504.01648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01648">https://arxiv.org/pdf/2504.01648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01648]] ProtoGuard-guided PROPEL: Class-Aware Prototype Enhancement and Progressive Labeling for Incremental 3D Point Cloud Segmentation(https://arxiv.org/abs/2504.01648)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D point cloud semantic segmentation technology has been widely used. However, in real-world scenarios, the environment is evolving. Thus, offline-trained segmentation models may lead to catastrophic forgetting of previously seen classes. Class-incremental learning (CIL) is designed to address the problem of catastrophic forgetting. While point clouds are common, we observe high similarity and unclear boundaries between different classes. Meanwhile, they are known to be imbalanced in class distribution. These lead to issues including misclassification between similar classes and the long-tail problem, which have not been adequately addressed in previous CIL methods. We thus propose ProtoGuard and PROPEL (Progressive Refinement Of PsEudo-Labels). In the base-class training phase, ProtoGuard maintains geometric and semantic prototypes for each class, which are combined into prototype features using an attention mechanism. In the novel-class training phase, PROPEL inherits the base feature extractor and classifier, guiding pseudo-label propagation and updates based on density distribution and semantic similarity. Extensive experiments show that our approach achieves remarkable results on both the S3DIS and ScanNet datasets, improving the mIoU of 3D point cloud segmentation by a maximum of 20.39% under the 5-step CIL scenario on S3DIS.</li>
</ul>

<h3>Title: Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Haosheng Li, Yuecong Xu, Junjie Chen, Kemi Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01659">https://arxiv.org/abs/2504.01659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01659">https://arxiv.org/pdf/2504.01659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01659]] Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks(https://arxiv.org/abs/2504.01659)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation (UDA) frameworks have shown good generalization capabilities for 3D point cloud semantic segmentation models on clean data. However, existing works overlook adversarial robustness when the source domain itself is compromised. To comprehensively explore the robustness of the UDA frameworks, we first design a stealthy adversarial point cloud generation attack that can significantly contaminate datasets with only minor perturbations to the point cloud surface. Based on that, we propose a novel dataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds. With the generated corrupted data, we further develop the Adversarial Adaptation Framework (AAF) as the countermeasure. Specifically, by extending the key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss) and utilizing a decoder branch, our approach enables the model to focus on long-tail classes during the pre-training phase and leverages high-confidence decoded point cloud information to restore point cloud structures during the adaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where the results demonstrate that our AAF method can mitigate performance degradation under source adversarial perturbations for UDA in the 3D point cloud segmentation application.</li>
</ul>

<h3>Title: Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish</h3>
<ul>
<li><strong>Authors: </strong>Cedric Lothritz, Jordi Cabot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01667">https://arxiv.org/abs/2504.01667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01667">https://arxiv.org/pdf/2504.01667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01667]] Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish(https://arxiv.org/abs/2504.01667)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as ChatGPT, Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks.</li>
</ul>

<h3>Title: Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Junjie Chen, Yuecong Xu, Haosheng Li, Kemi Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01668">https://arxiv.org/abs/2504.01668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01668">https://arxiv.org/pdf/2504.01668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01668]] Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation(https://arxiv.org/abs/2504.01668)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>3D point cloud semantic segmentation (PCSS) is a cornerstone for environmental perception in robotic systems and autonomous driving, enabling precise scene understanding through point-wise classification. While unsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing methods critically overlook the inherent vulnerability to real-world perturbations (e.g., snow, fog, rain) and adversarial distortions. This work first identifies two intrinsic limitations that undermine current PCSS-UDA robustness: (a) unsupervised features overlap from unaligned boundaries in shared-class regions and (b) feature structure erosion caused by domain-invariant learning that suppresses target-specific patterns. To address the proposed problems, we propose a tripartite framework consisting of: 1) a robustness evaluation model quantifying resilience against adversarial attack/corruption types through robustness metrics; 2) an invertible attention alignment module (IAAM) enabling bidirectional domain mapping while preserving discriminative structure via attention-guided overlap suppression; and 3) a contrastive memory bank with quality-aware contrastive learning that progressively refines pseudo-labels with feature quality for more discriminative representations. Extensive experiments on SynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of 14.3\% under adversarial attack.</li>
</ul>

<h3>Title: Satellite Edge Artificial Intelligence with Large Models: Architectures and Technologies</h3>
<ul>
<li><strong>Authors: </strong>Yuanming Shi, Jingyang Zhu, Chunxiao Jiang, Linling Kuang, Khaled B. Letaief</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01676">https://arxiv.org/abs/2504.01676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01676">https://arxiv.org/pdf/2504.01676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01676]] Satellite Edge Artificial Intelligence with Large Models: Architectures and Technologies(https://arxiv.org/abs/2504.01676)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Driven by the growing demand for intelligent remote sensing applications, large artificial intelligence (AI) models pre-trained on large-scale unlabeled datasets and fine-tuned for downstream tasks have significantly improved learning performance for various downstream tasks due to their generalization capabilities. However, many specific downstream tasks, such as extreme weather nowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield surveillance, require real-time data processing. Traditional methods via transferring raw data to ground stations for processing often cause significant issues in terms of latency and trustworthiness. To address these challenges, satellite edge AI provides a paradigm shift from ground-based to on-board data processing by leveraging the integrated communication-and-computation capabilities in space computing power networks (Space-CPN), thereby enhancing the timeliness, effectiveness, and trustworthiness for remote sensing downstream tasks. Moreover, satellite edge large AI model (LAM) involves both the training (i.e., fine-tuning) and inference phases, where a key challenge lies in developing computation task decomposition principles to support scalable LAM deployment in resource-constrained space networks with time-varying topologies. In this article, we first propose a satellite federated fine-tuning architecture to split and deploy the modules of LAM over space and ground networks for efficient LAM fine-tuning. We then introduce a microservice-empowered satellite edge LAM inference architecture that virtualizes LAM components into lightweight microservices tailored for multi-task multimodal inference. Finally, we discuss the future directions for enhancing the efficiency and scalability of satellite edge LAM, including task-oriented communication, brain-inspired computing, and satellite edge AI network optimization.</li>
</ul>

<h3>Title: InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Noam Elata, Hyungjin Chung, Jong Chul Ye, Tomer Michaeli, Michael Elad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01689">https://arxiv.org/abs/2504.01689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01689">https://arxiv.org/pdf/2504.01689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01689]] InvFussion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems(https://arxiv.org/abs/2504.01689)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models have demonstrated remarkable capabilities in handling inverse problems, offering high-quality posterior-sampling-based solutions. Despite significant advances, a fundamental trade-off persists, regarding the way the conditioned synthesis is employed: Training-based methods achieve high quality results, while zero-shot approaches trade this with flexibility. This work introduces a framework that combines the best of both worlds -- the strong performance of supervised approaches and the flexibility of zero-shot methods. This is achieved through a novel architectural design that seamlessly integrates the degradation operator directly into the denoiser. In each block, our proposed architecture applies the degradation operator on the network activations and conditions the output using the attention mechanism, enabling adaptation to diverse degradation scenarios while maintaining high performance. Our work demonstrates the versatility of the proposed architecture, operating as a general MMSE estimator, a posterior sampler, or a Neural Posterior Principal Component estimator. This flexibility enables a wide range of downstream tasks, highlighting the broad applicability of our framework. The proposed modification of the denoiser network offers a versatile, accurate, and computationally efficient solution, demonstrating the advantages of dedicated network architectures for complex inverse problems. Experimental results on the FFHQ and ImageNet datasets demonstrate state-of-the-art posterior-sampling performance, surpassing both training-based and zero-shot alternatives.</li>
</ul>

<h3>Title: ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yi-Long Lu, Chunhui Zhang, Jiajun Song, Lifeng Fan, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01698">https://arxiv.org/abs/2504.01698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01698">https://arxiv.org/pdf/2504.01698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01698]] ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs(https://arxiv.org/abs/2504.01698)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in rule-based reinforcement learning (RL), applied during the post-training phase of large language models (LLMs), have significantly enhanced their capabilities in structured reasoning tasks such as mathematics and logical inference. However, the effectiveness of RL in social reasoning, particularly in Theory of Mind (ToM), the ability to infer others' mental states, remains largely unexplored. In this study, we demonstrate that RL methods effectively unlock ToM reasoning capabilities even in small-scale LLMs (0.5B to 7B parameters). Using a modest dataset comprising 3200 questions across diverse scenarios, our RL-trained 7B model achieves 84.50\% accuracy on the Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite significantly fewer parameters. While smaller models ($\leq$3B parameters) suffer from reasoning collapse, larger models (7B parameters) maintain stable performance through consistent belief tracking. Additionally, our RL-based models demonstrate robust generalization to higher-order, out-of-distribution ToM problems, novel textual presentations, and previously unseen datasets. These findings highlight RL's potential to enhance social cognitive reasoning, bridging the gap between structured problem-solving and nuanced social inference in LLMs.</li>
</ul>

<h3>Title: Sky of Unlearning (SoUL): Rewiring Federated Machine Unlearning via Selective Pruning</h3>
<ul>
<li><strong>Authors: </strong>Md Mahabub Uz Zaman, Xiang Sun, Jingjing Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01705">https://arxiv.org/abs/2504.01705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01705">https://arxiv.org/pdf/2504.01705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01705]] Sky of Unlearning (SoUL): Rewiring Federated Machine Unlearning via Selective Pruning(https://arxiv.org/abs/2504.01705)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>The Internet of Drones (IoD), where drones collaborate in data collection and analysis, has become essential for applications such as surveillance and environmental monitoring. Federated learning (FL) enables drones to train machine learning models in a decentralized manner while preserving data privacy. However, FL in IoD networks is susceptible to attacks like data poisoning and model inversion. Federated unlearning (FU) mitigates these risks by eliminating adversarial data contributions, preventing their influence on the model. This paper proposes sky of unlearning (SoUL), a federated unlearning framework that efficiently removes the influence of unlearned data while maintaining model performance. A selective pruning algorithm is designed to identify and remove neurons influential in unlearning but minimally impact the overall performance of the model. Simulations demonstrate that SoUL outperforms existing unlearning methods, achieves accuracy comparable to full retraining, and reduces computation and communication overhead, making it a scalable and efficient solution for resource-constrained IoD networks.</li>
</ul>

<h3>Title: InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation</h3>
<ul>
<li><strong>Authors: </strong>Bowen Cao, Deng Cai, Wai Lam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01707">https://arxiv.org/abs/2504.01707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01707">https://arxiv.org/pdf/2504.01707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01707]] InfiniteICL: Breaking the Limit of Context Window Size via Long Short-term Memory Transformation(https://arxiv.org/abs/2504.01707)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is critical for large language models (LLMs), but its effectiveness is constrained by finite context windows, particularly in ultra-long contexts. To overcome this, we introduce InfiniteICL, a framework that parallels context and parameters in LLMs with short- and long-term memory in human cognitive systems, focusing on transforming temporary context knowledge into permanent parameter updates. This approach significantly reduces memory usage, maintains robust performance across varying input lengths, and theoretically enables infinite context integration through the principles of context knowledge elicitation, selection, and consolidation. Evaluations demonstrate that our method reduces context length by 90% while achieving 103% average performance of full-context prompting across fact recall, grounded reasoning, and skill acquisition tasks. When conducting sequential multi-turn transformations on complex, real-world contexts (with length up to 2M tokens), our approach surpasses full-context prompting while using only 0.4% of the original contexts. These findings highlight InfiniteICL's potential to enhance the scalability and efficiency of LLMs by breaking the limitations of conventional context window sizes.</li>
</ul>

<h3>Title: DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, Yongming Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01724">https://arxiv.org/abs/2504.01724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01724">https://arxiv.org/pdf/2504.01724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01724]] DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance(https://arxiv.org/abs/2504.01724)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: this https URL.</li>
</ul>

<h3>Title: FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Ulas Gunes, Matias Turkulainen, Xuqian Ren, Arno Solin, Juho Kannala, Esa Rahtu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01732">https://arxiv.org/abs/2504.01732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01732">https://arxiv.org/pdf/2504.01732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01732]] FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking(https://arxiv.org/abs/2504.01732)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The development of large-scale 3D scene reconstruction and novel view synthesis methods mostly rely on datasets comprising perspective images with narrow fields of view (FoV). While effective for small-scale scenes, these datasets require large image sets and extensive structure-from-motion (SfM) processing, limiting scalability. To address this, we introduce a fisheye image dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense point clouds that can be used as geometric ground-truth, enabling robust benchmarking under challenging conditions such as occlusions and reflections. While the baseline experiments focus on vanilla Gaussian Splatting and NeRF based Nerfacto methods, the dataset supports diverse approaches for scene reconstruction, novel view synthesis, and image-based rendering.</li>
</ul>

<h3>Title: AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chaohu Liu, Tianyi Gui, Yu Liu, Linli Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01735">https://arxiv.org/abs/2504.01735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01735">https://arxiv.org/pdf/2504.01735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01735]] AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization(https://arxiv.org/abs/2504.01735)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently witnessed remarkable advancements and are increasingly being deployed in real-world applications. However, inheriting the sensitivity of visual neural networks, LVLMs remain vulnerable to adversarial attacks, which can result in erroneous or malicious outputs. While existing efforts utilize adversarial fine-tuning to enhance robustness, they often suffer from performance degradation on clean inputs. In this paper, we proposes AdPO, a novel adversarial defense strategy for LVLMs based on preference optimization. For the first time, we reframe adversarial training as a preference optimization problem, aiming to enhance the model's preference for generating normal outputs on clean inputs while rejecting the potential misleading outputs for adversarial examples. Notably, AdPO achieves this by solely modifying the image encoder, e.g., CLIP ViT, resulting in superior clean and adversarial performance in a variety of downsream tasks. Considering that training involves large language models (LLMs), the computational cost increases significantly. We validate that training on smaller LVLMs and subsequently transferring to larger models can achieve competitive performance while maintaining efficiency comparable to baseline methods. Our comprehensive experiments confirm the effectiveness of the proposed AdPO, which provides a novel perspective for future adversarial defense research.</li>
</ul>

<h3>Title: Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers</h3>
<ul>
<li><strong>Authors: </strong>Lukas Boehm, Jonas Leo Mueller, Christoffer Loeffler, Leo Schwinn, Bjoern Eskofier, Dario Zanca</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01739">https://arxiv.org/abs/2504.01739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01739">https://arxiv.org/pdf/2504.01739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01739]] Understanding Cross-Model Perceptual Invariances Through Ensemble Metamers(https://arxiv.org/abs/2504.01739)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>Understanding the perceptual invariances of artificial neural networks is essential for improving explainability and aligning models with human vision. Metamers - stimuli that are physically distinct yet produce identical neural activations - serve as a valuable tool for investigating these invariances. We introduce a novel approach to metamer generation by leveraging ensembles of artificial neural networks, capturing shared representational subspaces across diverse architectures, including convolutional neural networks and vision transformers. To characterize the properties of the generated metamers, we employ a suite of image-based metrics that assess factors such as semantic fidelity and naturalness. Our findings show that convolutional neural networks generate more recognizable and human-like metamers, while vision transformers produce realistic but less transferable metamers, highlighting the impact of architectural biases on representational invariances.</li>
</ul>

<h3>Title: Stable Structure Learning with HC-Stable and Tabu-Stable Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Neville K. Kitson, Anthony C. Constantinou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01740">https://arxiv.org/abs/2504.01740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01740">https://arxiv.org/pdf/2504.01740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01740]] Stable Structure Learning with HC-Stable and Tabu-Stable Algorithms(https://arxiv.org/abs/2504.01740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many Bayesian Network structure learning algorithms are unstable, with the learned graph sensitive to arbitrary dataset artifacts, such as the ordering of columns (i.e., variable order). PC-Stable attempts to address this issue for the widely-used PC algorithm, prompting researchers to use the "stable" version instead. However, this problem seems to have been overlooked for score-based algorithms. In this study, we show that some widely-used score-based algorithms, as well as hybrid and constraint-based algorithms, including PC-Stable, suffer from the same issue. We propose a novel solution for score-based greedy hill-climbing that eliminates instability by determining a stable node order, leading to consistent results regardless of variable ordering. Two implementations, HC-Stable and Tabu-Stable, are introduced. Tabu-Stable achieves the highest BIC scores across all networks, and the highest accuracy for categorical networks. These results highlight the importance of addressing instability in structure learning and provide a robust and practical approach for future applications. This extends the scope and impact of our previous work presented at Probabilistic Graphical Models 2024 by incorporating continuous variables. The implementation, along with usage instructions, is freely available on GitHub at this https URL.</li>
</ul>

<h3>Title: A Two-Timescale Approach for Wireless Federated Learning with Parameter Freezing and Power Control</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Ouyang, Yuan Liu, Hang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01752">https://arxiv.org/abs/2504.01752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01752">https://arxiv.org/pdf/2504.01752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01752]] A Two-Timescale Approach for Wireless Federated Learning with Parameter Freezing and Power Control(https://arxiv.org/abs/2504.01752)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables distributed devices to train a shared machine learning (ML) model collaboratively while protecting their data privacy. However, the resource-limited mobile devices suffer from intensive computation-and-communication costs of model parameters. In this paper, we observe the phenomenon that the model parameters tend to be stabilized long before convergence during training process. Based on this observation, we propose a two-timescale FL framework by joint optimization of freezing stabilized parameters and controlling transmit power for the unstable parameters to balance the energy consumption and convergence. First, we analyze the impact of model parameter freezing and unreliable transmission on the convergence rate. Next, we formulate a two-timescale optimization problem of parameter freezing percentage and transmit power to minimize the model convergence error subject to the energy budget. To solve this problem, we decompose it into parallel sub-problems and decompose each sub-problem into two different timescales problems using the Lyapunov optimization method. The optimal parameter freezing and power control strategies are derived in an online fashion. Experimental results demonstrate the superiority of the proposed scheme compared with the benchmark schemes.</li>
</ul>

<h3>Title: Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Ye, Lianping Yang, Hegui Zhu, Zenghao Zheng, Xin Wang, Yantao Lo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01764">https://arxiv.org/abs/2504.01764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01764">https://arxiv.org/pdf/2504.01764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01764]] Dual-stream Transformer-GCN Model with Contextualized Representations Learning for Monocular 3D Human Pose Estimation(https://arxiv.org/abs/2504.01764)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to monocular 3D human pose estimation using contextualized representation learning with the Transformer-GCN dual-stream model. Monocular 3D human pose estimation is challenged by depth ambiguity, limited 3D-labeled training data, imbalanced modeling, and restricted model generalization. To address these limitations, our work introduces a groundbreaking motion pre-training method based on contextualized representation learning. Specifically, our method involves masking 2D pose features and utilizing a Transformer-GCN dual-stream model to learn high-dimensional representations through a self-distillation setup. By focusing on contextualized representation learning and spatial-temporal modeling, our approach enhances the model's ability to understand spatial-temporal relationships between postures, resulting in superior generalization. Furthermore, leveraging the Transformer-GCN dual-stream model, our approach effectively balances global and local interactions in video pose estimation. The model adaptively integrates information from both the Transformer and GCN streams, where the GCN stream effectively learns local relationships between adjacent key points and frames, while the Transformer stream captures comprehensive global spatial and temporal features. Our model achieves state-of-the-art performance on two benchmark datasets, with an MPJPE of 38.0mm and P-MPJPE of 31.9mm on Human3.6M, and an MPJPE of 15.9mm on MPI-INF-3DHP. Furthermore, visual experiments on public datasets and in-the-wild videos demonstrate the robustness and generalization capabilities of our approach.</li>
</ul>

<h3>Title: OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sumeth Yuenyong, Thodsaporn Chay-intr, Kobkrit Viriyayudhakorn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01789">https://arxiv.org/abs/2504.01789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01789">https://arxiv.org/pdf/2504.01789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01789]] OpenThaiGPT 1.6 and R1: Thai-Centric Open Source and Reasoning Large Language Models(https://arxiv.org/abs/2504.01789)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present OpenThaiGPT 1.6 and R1 (OTG-1.6 and OTG-R1), Thai-centric Large Language Models (LLMs) developed through distinct methodologies to enhance generalization and reasoning capabilities. OTG-1.6 employs Task Arithmetic model merging for broad generalization, while OTG-R1 integrates multi-stage training with the Less-Is-More Reasoning Hypothesis (LIMO) for advanced reasoning. Benchmark evaluations demonstrate superior performance across Thai language tasks, achieving competitive results against larger-scale open-source Thai LLMs. This paper details the proposed models, training processes, benchmarks, and results, highlighting improvements over previous models and establishing new performance standards for Thai-centric LLMs.</li>
</ul>

<h3>Title: UniViTAR: Unified Vision Transformer with Native Resolution</h3>
<ul>
<li><strong>Authors: </strong>Limeng Qiao, Yiyang Gan, Bairui Wang, Jie Qin, Shuang Xu, Siqi Yang, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01792">https://arxiv.org/abs/2504.01792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01792">https://arxiv.org/pdf/2504.01792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01792]] UniViTAR: Unified Vision Transformer with Native Resolution(https://arxiv.org/abs/2504.01792)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Conventional Vision Transformer simplifies visual modeling by standardizing input resolutions, often disregarding the variability of natural visual data and compromising spatial-contextual fidelity. While preliminary explorations have superficially investigated native resolution modeling, existing approaches still lack systematic analysis from a visual representation perspective. To bridge this gap, we introduce UniViTAR, a family of homogeneous vision foundation models tailored for unified visual modality and native resolution scenario in the era of multimodal. Our framework first conducts architectural upgrades to the vanilla paradigm by integrating multiple advanced components. Building upon these improvements, a progressive training paradigm is introduced, which strategically combines two core mechanisms: (1) resolution curriculum learning, transitioning from fixed-resolution pretraining to native resolution tuning, thereby leveraging ViT's inherent adaptability to variable-length sequences, and (2) visual modality adaptation via inter-batch image-video switching, which balances computational efficiency with enhanced temporal reasoning. In parallel, a hybrid training framework further synergizes sigmoid-based contrastive loss with feature distillation from a frozen teacher model, thereby accelerating early-stage convergence. Finally, trained exclusively on public datasets, externsive experiments across multiple model scales from 0.3B to 1B demonstrate its effectiveness.</li>
</ul>

<h3>Title: Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Zhijun Wang, Jiahuan Li, Hao Zhou, Rongxiang Weng, Jingang Wang, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01801">https://arxiv.org/abs/2504.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01801">https://arxiv.org/pdf/2504.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01801]] Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training(https://arxiv.org/abs/2504.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable multilingual capabilities despite the extreme language imbalance in the pre-training data. In this paper, we closely examine the reasons behind this phenomenon, focusing on the pre-training corpus. We find that the existence of code-switching, alternating between different languages within a context, is key to multilingual capabilities. We conduct an analysis to investigate code-switching in the pre-training corpus, examining its presence and categorizing it into four types within two quadrants. We then assess its impact on multilingual performance. These types of code-switching data are unbalanced in proportions and demonstrate different effects on facilitating language transfer. To better explore the power of code-switching for language alignment during pre-training, we investigate the strategy of synthetic code-switching. We continuously scale up the synthetic code-switching data and observe remarkable improvements in both benchmarks and representation space. Extensive experiments indicate that incorporating synthetic code-switching data enables better language alignment and generalizes well to high, medium, and low-resource languages with pre-training corpora of varying qualities.</li>
</ul>

<h3>Title: DISINFOX: an open-source threat exchange platform serving intelligence on disinformation and influence operations</h3>
<ul>
<li><strong>Authors: </strong>Felipe Sánchez González, Javier Pastor-Galindo, José A. Ruipérez-Valiente</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01803">https://arxiv.org/abs/2504.01803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01803">https://arxiv.org/pdf/2504.01803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01803]] DISINFOX: an open-source threat exchange platform serving intelligence on disinformation and influence operations(https://arxiv.org/abs/2504.01803)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper introduces DISINFOX, an open-source threat intelligence exchange platform for the structured collection, management, and dissemination of disinformation incidents and influence operations. Analysts can upload and correlate information manipulation and interference incidents, while clients can access and analyze the data through an interactive web interface or programmatically via a public API. This facilitates integration with other vendors, providing a unified view of cybersecurity and disinformation events. The solution is fully containerized using Docker, comprising a web-based frontend for user interaction, a backend REST API for managing core functionalities, and a public API for structured data retrieval, enabling seamless integration with existing Cyber Threat Intelligence (CTI) workflows. In particular, DISINFOX models the incidents through DISARM Tactics, Techniques, and Procedures (TTPs), a MITRE ATT&CK-like framework for disinformation, with a custom data model based on the Structured Threat Information eXpression (STIX2) standard. As an open-source solution, DISINFOX provides a reproducible and extensible hub for researchers, analysts, and policymakers seeking to enhance the detection, investigation, and mitigation of disinformation threats. The intelligence generated from a custom dataset has been tested and utilized by a local instance of OpenCTI, a mature CTI platform, via a custom-built connector, validating the platform with the exchange of more than 100 disinformation incidents.</li>
</ul>

<h3>Title: Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kun Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01805">https://arxiv.org/abs/2504.01805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01805">https://arxiv.org/pdf/2504.01805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01805]] Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning(https://arxiv.org/abs/2504.01805)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enhancing the spatial reasoning capabilities of Multi-modal Large Language Models (MLLMs) for video understanding is crucial yet challenging. We present Spatial-R1, a targeted approach involving two key contributions: the curation of SR, a new video spatial reasoning dataset from ScanNet with automatically generated QA pairs across seven task types, and the application of Task-Specific Group Relative Policy Optimization (GRPO) for fine-tuning. By training the Qwen2.5-VL-7B-Instruct model on SR using GRPO, Spatial-R1 significantly advances performance on the VSI-Bench benchmark, achieving a 7.4\% gain over the baseline and outperforming strong contemporary models. This work validates the effectiveness of specialized data curation and optimization techniques for improving complex spatial reasoning in video MLLMs.</li>
</ul>

<h3>Title: Implicit Bias Injection Attacks against Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Huayang Huang, Xiangye Jin, Jiaxu Miao, Yu Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01819">https://arxiv.org/abs/2504.01819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01819">https://arxiv.org/pdf/2504.01819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01819]] Implicit Bias Injection Attacks against Text-to-Image Diffusion Models(https://arxiv.org/abs/2504.01819)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion</a></li>
<li><strong>Abstract: </strong>The proliferation of text-to-image diffusion models (T2I DMs) has led to an increased presence of AI-generated images in daily life. However, biased T2I models can generate content with specific tendencies, potentially influencing people's perceptions. Intentional exploitation of these biases risks conveying misleading information to the public. Current research on bias primarily addresses explicit biases with recognizable visual patterns, such as skin color and gender. This paper introduces a novel form of implicit bias that lacks explicit visual features but can manifest in diverse ways across various semantic contexts. This subtle and versatile nature makes this bias challenging to detect, easy to propagate, and adaptable to a wide range of scenarios. We further propose an implicit bias injection attack framework (IBI-Attacks) against T2I diffusion models by precomputing a general bias direction in the prompt embedding space and adaptively adjusting it based on different inputs. Our attack module can be seamlessly integrated into pre-trained diffusion models in a plug-and-play manner without direct manipulation of user input or model retraining. Extensive experiments validate the effectiveness of our scheme in introducing bias through subtle and diverse modifications while preserving the original semantics. The strong concealment and transferability of our attack across various scenarios further underscore the significance of our approach. Code is available at this https URL.</li>
</ul>

<h3>Title: YourBench: Easy Custom Evaluation Sets for Everyone</h3>
<ul>
<li><strong>Authors: </strong>Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskia, Thomas Wolf, Gokhan Tur, Dilek Hakkani-Tür</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01833">https://arxiv.org/abs/2504.01833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01833">https://arxiv.org/pdf/2504.01833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01833]] YourBench: Easy Custom Evaluation Sets for Everyone(https://arxiv.org/abs/2504.01833)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. We introduce YourBench, a novel, open-source framework that addresses these limitations by enabling dynamic, automated generation of reliable, up-to-date, and domain-tailored benchmarks cheaply and without manual annotation, directly from user-provided documents. We demonstrate its efficacy by replicating 7 diverse MMLU subsets using minimal source text, achieving this for under 15 USD in total inference costs while perfectly preserving the relative model performance rankings (Spearman Rho = 1) observed on the original benchmark. To ensure that YourBench generates data grounded in provided input instead of relying on posterior parametric knowledge in models, we also introduce Tempora-0325, a novel dataset of over 7K diverse documents, published exclusively after March 2025. Our comprehensive analysis spans 26 SoTA models from 7 major families across varying scales (3-671B parameters) to validate the quality of generated evaluations through rigorous algorithmic checks (e.g., citation grounding) and human assessments. We release the YourBench library, the Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all evaluation and inference traces to facilitate reproducible research and empower the community to generate bespoke benchmarks on demand, fostering more relevant and trustworthy LLM evaluation.</li>
</ul>

<h3>Title: Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images</h3>
<ul>
<li><strong>Authors: </strong>Nusrat Munia, Abdullah-Al-Zubaer Imran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01838">https://arxiv.org/abs/2504.01838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01838">https://arxiv.org/pdf/2504.01838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01838]] Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images(https://arxiv.org/abs/2504.01838)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) in skin disease diagnosis has improved significantly, but a major concern is that these models frequently show biased performance across subgroups, especially regarding sensitive attributes such as skin color. To address these issues, we propose a novel generative AI-based framework, namely, Dermatology Diffusion Transformer (DermDiT), which leverages text prompts generated via Vision Language Models and multimodal text-image learning to generate new dermoscopic images. We utilize large vision language models to generate accurate and proper prompts for each dermoscopic image which helps to generate synthetic images to improve the representation of underrepresented groups (patient, disease, etc.) in highly imbalanced datasets for clinical diagnoses. Our extensive experimentation showcases the large vision language models providing much more insightful representations, that enable DermDiT to generate high-quality images. Our code is available at this https URL</li>
</ul>

<h3>Title: LARGE: Legal Retrieval Augmented Generation Evaluation Tool</h3>
<ul>
<li><strong>Authors: </strong>Minhu Park, Hongseok Oh, Eunkyung Choi, Wonseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01840">https://arxiv.org/abs/2504.01840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01840">https://arxiv.org/pdf/2504.01840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01840]] LARGE: Legal Retrieval Augmented Generation Evaluation Tool(https://arxiv.org/abs/2504.01840)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at this https URL.</li>
</ul>

<h3>Title: shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python</h3>
<ul>
<li><strong>Authors: </strong>Martin Jullum, Lars Henry Berge Olsen, Jon Lachmann, Annabelle Redelmeier</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01842">https://arxiv.org/abs/2504.01842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01842">https://arxiv.org/pdf/2504.01842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01842]] shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python(https://arxiv.org/abs/2504.01842)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper introduces the shapr package, a versatile tool for generating Shapley value explanations for machine learning and statistical regression models in both R and Python. The package emphasizes conditional Shapley value estimates, providing a comprehensive range of approaches for accurately capturing feature dependencies, which is crucial for correct model interpretation and lacking in similar software. In addition to regular tabular data, the shapr R-package includes specialized functionality for explaining time series forecasts. The package offers a minimal set of user functions with sensible defaults for most use cases while providing extensive flexibility for advanced users to fine-tune computations. Additional features include parallelized computations, iterative estimation with convergence detection, and rich visualization tools. shapr also extends its functionality to compute causal and asymmetric Shapley values when causal information is available. In addition, we introduce the shaprpy Python library, which brings core capabilities of shapr to the Python ecosystem. Overall, the package aims to enhance the interpretability of predictive models within a powerful and user-friendly framework.</li>
</ul>

<h3>Title: Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Choi, Junoh Kang, Bohyung Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01855">https://arxiv.org/abs/2504.01855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01855">https://arxiv.org/pdf/2504.01855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01855]] Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions(https://arxiv.org/abs/2504.01855)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion probabilistic models (DPMs), while effective in generating high-quality samples, often suffer from high computational costs due to their iterative sampling process. To address this, we propose an enhanced ODE-based sampling method for DPMs inspired by Richardson extrapolation, which reduces numerical error and improves convergence rates. Our method, RX-DPM, leverages multiple ODE solutions at intermediate time steps to extrapolate the denoised prediction in DPMs. This significantly enhances the accuracy of estimations for the final sample while maintaining the number of function evaluations (NFEs). Unlike standard Richardson extrapolation, which assumes uniform discretization of the time grid, we develop a more general formulation tailored to arbitrary time step scheduling, guided by local truncation error derived from a baseline sampling method. The simplicity of our approach facilitates accurate estimation of numerical solutions without significant computational overhead, and allows for seamless and convenient integration into various DPMs and solvers. Additionally, RX-DPM provides explicit error estimates, effectively demonstrating the faster convergence as the leading error term's order increases. Through a series of experiments, we show that the proposed method improves the quality of generated samples without requiring additional sampling iterations.</li>
</ul>

<h3>Title: Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Yu, Tuo Li, Changhong Wang, Hui Chen, Lang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01857">https://arxiv.org/abs/2504.01857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01857">https://arxiv.org/pdf/2504.01857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01857]] Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models(https://arxiv.org/abs/2504.01857)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) has emerged as a critical mechanism for enhancing reasoning capabilities in large language models (LLMs), with self-consistency demonstrating notable promise in boosting performance. However, inherent linguistic biases in multilingual training corpora frequently cause semantic drift and logical inconsistencies, especially in sub-10B parameter LLMs handling complex inference tasks. To overcome these constraints, we propose the Cross-Lingual Consistency (CLC) framework, an innovative inference paradigm that integrates multilingual reasoning paths through majority voting to elevate LLMs' reasoning capabilities. Empirical evaluations on the CMATH dataset reveal CLC's superiority over the conventional self-consistency method, delivering 9.5%, 6.5%, and 6.0% absolute accuracy gains for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively. Expanding CLC's linguistic scope to 11 diverse languages implies two synergistic benefits: 1) neutralizing linguistic biases in multilingual training corpora through multilingual ensemble voting, 2) escaping monolingual reasoning traps by exploring the broader multilingual solution space. This dual benefits empirically enables more globally optimal reasoning paths compared to monolingual self-consistency baselines, as evidenced by the 4.1%-18.5% accuracy gains using Gemma2-9B-Instruct on the MGSM dataset.</li>
</ul>

<h3>Title: Interpreting Emergent Planning in Model-Free Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01871">https://arxiv.org/abs/2504.01871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01871">https://arxiv.org/pdf/2504.01871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01871]] Interpreting Emergent Planning in Model-Free Reinforcement Learning(https://arxiv.org/abs/2504.01871)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL</li>
</ul>

<h3>Title: A Diffusion-Based Framework for Occluded Object Movement</h3>
<ul>
<li><strong>Authors: </strong>Zheng-Peng Duan, Jiawei Zhang, Siyu Liu, Zheng Lin, Chun-Le Guo, Dongqing Zou, Jimmy Ren, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01873">https://arxiv.org/abs/2504.01873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01873">https://arxiv.org/pdf/2504.01873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01873]] A Diffusion-Based Framework for Occluded Object Movement(https://arxiv.org/abs/2504.01873)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Seamlessly moving objects within a scene is a common requirement for image editing, but it is still a challenge for existing editing methods. Especially for real-world images, the occlusion situation further increases the difficulty. The main difficulty is that the occluded portion needs to be completed before movement can proceed. To leverage the real-world knowledge embedded in the pre-trained diffusion models, we propose a Diffusion-based framework specifically designed for Occluded Object Movement, named DiffOOM. The proposed DiffOOM consists of two parallel branches that perform object de-occlusion and movement simultaneously. The de-occlusion branch utilizes a background color-fill strategy and a continuously updated object mask to focus the diffusion process on completing the obscured portion of the target object. Concurrently, the movement branch employs latent optimization to place the completed object in the target location and adopts local text-conditioned guidance to integrate the object into new surroundings appropriately. Extensive evaluations demonstrate the superior performance of our method, which is further validated by a comprehensive user study.</li>
</ul>

<h3>Title: TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables</h3>
<ul>
<li><strong>Authors: </strong>Abhilash Shankarampeta, Harsh Mahajan, Tushar Kataria, Dan Roth, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01879">https://arxiv.org/abs/2504.01879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01879">https://arxiv.org/pdf/2504.01879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01879]] TransientTables: Evaluating LLMs' Reasoning on Temporally Evolving Semi-structured Tables(https://arxiv.org/abs/2504.01879)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans continuously make new discoveries, and understanding temporal sequence of events leading to these breakthroughs is essential for advancing science and society. This ability to reason over time allows us to identify future steps and understand the effects of financial and political decisions on our lives. However, large language models (LLMs) are typically trained on static datasets, limiting their ability to perform effective temporal reasoning. To assess the temporal reasoning capabilities of LLMs, we present the TRANSIENTTABLES dataset, which comprises 3,971 questions derived from over 14,000 tables, spanning 1,238 entities across multiple time periods. We introduce a template-based question-generation pipeline that harnesses LLMs to refine both templates and questions. Additionally, we establish baseline results using state-of-the-art LLMs to create a benchmark. We also introduce novel modeling strategies centered around task decomposition, enhancing LLM performance.</li>
</ul>

<h3>Title: CO-DEFEND: Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection</h3>
<ul>
<li><strong>Authors: </strong>Diego Cajaraville-Aboy, Marta Moure-Garrido, Carlos Beis-Penedo, Carlos Garcia-Rubio, Rebeca P. Díaz-Redondo, Celeste Campo, Ana Fernández-Vilas, Manuel Fernández-Veiga</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01882">https://arxiv.org/abs/2504.01882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01882">https://arxiv.org/pdf/2504.01882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01882]] CO-DEFEND: Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection(https://arxiv.org/abs/2504.01882)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>The use of DNS over HTTPS (DoH) tunneling by an attacker to hide malicious activity within encrypted DNS traffic poses a serious threat to network security, as it allows malicious actors to bypass traditional monitoring and intrusion detection systems while evading detection by conventional traffic analysis techniques. Machine Learning (ML) techniques can be used to detect DoH tunnels; however, their effectiveness relies on large datasets containing both benign and malicious traffic. Sharing such datasets across entities is challenging due to privacy concerns. In this work, we propose CO-DEFEND (Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection), a Decentralized Federated Learning (DFL) framework that enables multiple entities to collaboratively train a classification machine learning model while preserving data privacy and enhancing resilience against single points of failure. The proposed DFL framework, which is scalable and privacy-preserving, is based on a federation process that allows multiple entities to train online their local models using incoming DoH flows in real time as they are processed by the entity. In addition, we adapt four classical machine learning algorithms, Support Vector Machines (SVM), Logistic Regression (LR), Decision Trees (DT), and Random Forest (RF), for federated scenarios, comparing their results with more computationally complex alternatives such as neural networks. We compare our proposed method by using the dataset CIRA-CIC-DoHBrw-2020 with existing machine learning approaches to demonstrate its effectiveness in detecting malicious DoH tunnels and the benefits it brings.</li>
</ul>

<h3>Title: Multi-fidelity Parameter Estimation Using Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Caroline Tatsuoka, Minglei Yang, Dongbin Xiu, Guannan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01894">https://arxiv.org/abs/2504.01894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01894">https://arxiv.org/pdf/2504.01894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01894]] Multi-fidelity Parameter Estimation Using Conditional Diffusion Models(https://arxiv.org/abs/2504.01894)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a multi-fidelity method for uncertainty quantification of parameter estimates in complex systems, leveraging generative models trained to sample the target conditional distribution. In the Bayesian inference setting, traditional parameter estimation methods rely on repeated simulations of potentially expensive forward models to determine the posterior distribution of the parameter values, which may result in computationally intractable workflows. Furthermore, methods such as Markov Chain Monte Carlo (MCMC) necessitate rerunning the entire algorithm for each new data observation, further increasing the computational burden. Hence, we propose a novel method for efficiently obtaining posterior distributions of parameter estimates for high-fidelity models given data observations of interest. The method first constructs a low-fidelity, conditional generative model capable of amortized Bayesian inference and hence rapid posterior density approximation over a wide-range of data observations. When higher accuracy is needed for a specific data observation, the method employs adaptive refinement of the density approximation. It uses outputs from the low-fidelity generative model to refine the parameter sampling space, ensuring efficient use of the computationally expensive high-fidelity solver. Subsequently, a high-fidelity, unconditional generative model is trained to achieve greater accuracy in the target posterior distribution. Both low- and high- fidelity generative models enable efficient sampling from the target posterior and do not require repeated simulation of the high-fidelity forward model. We demonstrate the effectiveness of the proposed method on several numerical examples, including cases with multi-modal densities, as well as an application in plasma physics for a runaway electron simulation model.</li>
</ul>

<h3>Title: Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights</h3>
<ul>
<li><strong>Authors: </strong>Célia Nouri, Jean-Philippe Cointet, Chloé Clavel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01902">https://arxiv.org/abs/2504.01902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01902">https://arxiv.org/pdf/2504.01902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01902]] Graphically Speaking: Unmasking Abuse in Social Media with Conversation Insights(https://arxiv.org/abs/2504.01902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting abusive language in social media conversations poses significant challenges, as identifying abusiveness often depends on the conversational context, characterized by the content and topology of preceding comments. Traditional Abusive Language Detection (ALD) models often overlook this context, which can lead to unreliable performance metrics. Recent Natural Language Processing (NLP) methods that integrate conversational context often depend on limited and simplified representations, and report inconsistent results. In this paper, we propose a novel approach that utilize graph neural networks (GNNs) to model social media conversations as graphs, where nodes represent comments, and edges capture reply structures. We systematically investigate various graph representations and context windows to identify the optimal configuration for ALD. Our GNN model outperform both context-agnostic baselines and linear context-aware methods, achieving significant improvements in F1 scores. These findings demonstrate the critical role of structured conversational context and establish GNNs as a robust framework for advancing context-aware abusive language detection.</li>
</ul>

<h3>Title: Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries</h3>
<ul>
<li><strong>Authors: </strong>Furkan Çolhak, Hasan Coşkun, Tsafac Nkombong Regine Cyrille, Tedi Hoxa, Mert İlhan Ecevit, Mehmet Nafiz Aydın</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01905">https://arxiv.org/abs/2504.01905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01905">https://arxiv.org/pdf/2504.01905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01905]] Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries(https://arxiv.org/abs/2504.01905)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The Internet of Vehicles (IoV) may face challenging cybersecurity attacks that may require sophisticated intrusion detection systems, necessitating a rapid development and response system. This research investigates the performance advantages of GPU-accelerated libraries (cuML) compared to traditional CPU-based implementations (scikit-learn), focusing on the speed and efficiency required for machine learning models used in IoV threat detection environments. The comprehensive evaluations conducted employ four machine learning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across three distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings demonstrate that GPU-accelerated implementations dramatically improved computational efficiency, with training times reduced by a factor of up to 159 and prediction speeds accelerated by up to 95 times compared to traditional CPU processing, all while preserving detection accuracy. This remarkable performance breakthrough empowers researchers and security specialists to harness GPU acceleration for creating faster, more effective threat detection systems that meet the urgent real-time security demands of today's connected vehicle networks.</li>
</ul>

<h3>Title: Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Andrey Sidorenko, Michael Platzer, Mario Scriminaci, Paul Tiwald</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01908">https://arxiv.org/abs/2504.01908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01908">https://arxiv.org/pdf/2504.01908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01908]] Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation Framework(https://arxiv.org/abs/2504.01908)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Evaluating the quality of synthetic data remains a key challenge for ensuring privacy and utility in data-driven research. In this work, we present an evaluation framework that quantifies how well synthetic data replicates original distributional properties while ensuring privacy. The proposed approach employs a holdout-based benchmarking strategy that facilitates quantitative assessment through low- and high-dimensional distribution comparisons, embedding-based similarity measures, and nearest-neighbor distance metrics. The framework supports various data types and structures, including sequential and contextual information, and enables interpretable quality diagnostics through a set of standardized metrics. These contributions aim to support reproducibility and methodological consistency in benchmarking of synthetic data generation techniques. The code of the framework is available at this https URL.</li>
</ul>

<h3>Title: Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Baban Gain, Dibyanayan Bandyopadhyay, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01919">https://arxiv.org/abs/2504.01919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01919">https://arxiv.org/pdf/2504.01919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01919]] Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation(https://arxiv.org/abs/2504.01919)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges such as hallucinations, evaluation inconsistencies, and inherited biases while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models.</li>
</ul>

<h3>Title: Client Selection in Federated Learning with Data Heterogeneity and Network Latencies</h3>
<ul>
<li><strong>Authors: </strong>Harsh Vardhan, Xiaofan Yu, Tajana Rosing, Arya Mazumdar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01921">https://arxiv.org/abs/2504.01921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01921">https://arxiv.org/pdf/2504.01921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01921]] Client Selection in Federated Learning with Data Heterogeneity and Network Latencies(https://arxiv.org/abs/2504.01921)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a distributed machine learning paradigm where multiple clients conduct local training based on their private data, then the updated models are sent to a central server for global aggregation. The practical convergence of FL is challenged by multiple factors, with the primary hurdle being the heterogeneity among clients. This heterogeneity manifests as data heterogeneity concerning local data distribution and latency heterogeneity during model transmission to the server. While prior research has introduced various efficient client selection methods to alleviate the negative impacts of either of these heterogeneities individually, efficient methods to handle real-world settings where both these heterogeneities exist simultaneously do not exist. In this paper, we propose two novel theoretically optimal client selection schemes that can handle both these heterogeneities. Our methods involve solving simple optimization problems every round obtained by minimizing the theoretical runtime to convergence. Empirical evaluations on 9 datasets with non-iid data distributions, 2 practical delay distributions, and non-convex neural network models demonstrate that our algorithms are at least competitive to and at most 20 times better than best existing baselines.</li>
</ul>

<h3>Title: Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time</h3>
<ul>
<li><strong>Authors: </strong>Haykel Snoussi, Davood Karimi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01925">https://arxiv.org/abs/2504.01925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01925">https://arxiv.org/pdf/2504.01925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01925]] Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time(https://arxiv.org/abs/2504.01925)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Early and accurate assessment of brain microstructure using diffusion Magnetic Resonance Imaging (dMRI) is crucial for identifying neurodevelopmental disorders in neonates, but remains challenging due to low signal-to-noise ratio (SNR), motion artifacts, and ongoing myelination. In this study, we propose a rotationally equivariant Spherical Convolutional Neural Network (sCNN) framework tailored for neonatal dMRI. We predict the Fiber Orientation Distribution (FOD) from multi-shell dMRI signals acquired with a reduced set of gradient directions (30% of the full protocol), enabling faster and more cost-effective acquisitions. We train and evaluate the performance of our sCNN using real data from 43 neonatal dMRI datasets provided by the Developing Human Connectome Project (dHCP). Our results demonstrate that the sCNN achieves significantly lower mean squared error (MSE) and higher angular correlation coefficient (ACC) compared to a Multi-Layer Perceptron (MLP) baseline, indicating improved accuracy in FOD estimation. Furthermore, tractography results based on the sCNN-predicted FODs show improved anatomical plausibility, coverage, and coherence compared to those from the MLP. These findings highlight that sCNNs, with their inherent rotational equivariance, offer a promising approach for accurate and clinically efficient dMRI analysis, paving the way for improved diagnostic capabilities and characterization of early brain development.</li>
</ul>

<h3>Title: Is the Reversal Curse a Binding Problem? Uncovering Limitations of Transformers from a Basic Generalization Failure</h3>
<ul>
<li><strong>Authors: </strong>Boshi Wang, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01928">https://arxiv.org/abs/2504.01928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01928">https://arxiv.org/pdf/2504.01928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01928]] Is the Reversal Curse a Binding Problem? Uncovering Limitations of Transformers from a Basic Generalization Failure(https://arxiv.org/abs/2504.01928)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Despite their impressive capabilities, LLMs exhibit a basic generalization failure known as the Reversal Curse, where they struggle to learn reversible factual associations. Understanding why this occurs could help identify weaknesses in current models and advance their generalization and robustness. In this paper, we conjecture that the Reversal Curse in LLMs is a manifestation of the long-standing binding problem in cognitive science, neuroscience and AI. Specifically, we identify two primary causes of the Reversal Curse stemming from transformers' limitations in conceptual binding: the inconsistency and entanglements of concept representations. We perform a series of experiments that support these conjectures. Our exploration leads to a model design based on JEPA (Joint-Embedding Predictive Architecture) that for the first time breaks the Reversal Curse without side-stepping it with specialized data augmentation or non-causal masking, and moreover, generalization could be further improved by incorporating special memory layers that support disentangled concept representations. We demonstrate that the skill of reversal unlocks a new kind of memory integration that enables models to solve large-scale arithmetic reasoning problems via parametric forward-chaining, outperforming frontier LLMs based on non-parametric memory and prolonged explicit reasoning.</li>
</ul>

<h3>Title: A thorough benchmark of automatic text classification: From traditional approaches to large language models</h3>
<ul>
<li><strong>Authors: </strong>Washington Cunha, Leonardo Rocha, Marcos André Gonçalves</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01930">https://arxiv.org/abs/2504.01930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01930">https://arxiv.org/pdf/2504.01930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01930]] A thorough benchmark of automatic text classification: From traditional approaches to large language models(https://arxiv.org/abs/2504.01930)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Automatic text classification (ATC) has experienced remarkable advancements in the past decade, best exemplified by recent small and large language models (SLMs and LLMs), leveraged by Transformer architectures. Despite recent effectiveness improvements, a comprehensive cost-benefit analysis investigating whether the effectiveness gains of these recent approaches compensate their much higher costs when compared to more traditional text classification approaches such as SVMs and Logistic Regression is still missing in the literature. In this context, this work's main contributions are twofold: (i) we provide a scientifically sound comparative analysis of the cost-benefit of twelve traditional and recent ATC solutions including five open LLMs, and (ii) a large benchmark comprising {22 datasets}, including sentiment analysis and topic classification, with their (train-validation-test) partitions based on folded cross-validation procedures, along with documentation, and code. The release of code, data, and documentation enables the community to replicate experiments and advance the field in a more scientifically sound manner. Our comparative experimental results indicate that LLMs outperform traditional approaches (up to 26%-7.1% on average) and SLMs (up to 4.9%-1.9% on average) in terms of effectiveness. However, LLMs incur significantly higher computational costs due to fine-tuning, being, on average 590x and 8.5x slower than traditional methods and SLMs, respectively. Results suggests the following recommendations: (1) LLMs for applications that require the best possible effectiveness and can afford the costs; (2) traditional methods such as Logistic Regression and SVM for resource-limited applications or those that cannot afford the cost of tuning large LLMs; and (3) SLMs like Roberta for near-optimal effectiveness-efficiency trade-off.</li>
</ul>

<h3>Title: Hessian-aware Training for Enhancing DNNs Resilience to Parameter Corruptions</h3>
<ul>
<li><strong>Authors: </strong>Tahmid Hasan Prato, Seijoon Kim, Lizhong Chen, Sanghyun Hong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01933">https://arxiv.org/abs/2504.01933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01933">https://arxiv.org/pdf/2504.01933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01933]] Hessian-aware Training for Enhancing DNNs Resilience to Parameter Corruptions(https://arxiv.org/abs/2504.01933)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Deep neural networks are not resilient to parameter corruptions: even a single-bitwise error in their parameters in memory can cause an accuracy drop of over 10%, and in the worst cases, up to 99%. This susceptibility poses great challenges in deploying models on computing platforms, where adversaries can induce bit-flips through software or bitwise corruptions may occur naturally. Most prior work addresses this issue with hardware or system-level approaches, such as integrating additional hardware components to verify a model's integrity at inference. However, these methods have not been widely deployed as they require infrastructure or platform-wide modifications. In this paper, we propose a new approach to addressing this issue: training models to be more resilient to bitwise corruptions to their parameters. Our approach, Hessian-aware training, promotes models with $flatter$ loss surfaces. We show that, while there have been training methods, designed to improve generalization through Hessian-based approaches, they do not enhance resilience to parameter corruptions. In contrast, models trained with our method demonstrate increased resilience to parameter corruptions, particularly with a 20$-$50% reduction in the number of bits whose individual flipping leads to a 90$-$100% accuracy drop. Moreover, we show the synergy between ours and existing hardware and system-level defenses.</li>
</ul>

<h3>Title: ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement</h3>
<ul>
<li><strong>Authors: </strong>Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01934">https://arxiv.org/abs/2504.01934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01934">https://arxiv.org/pdf/2504.01934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01934]] ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement(https://arxiv.org/abs/2504.01934)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: this https URL.</li>
</ul>

<h3>Title: A Unified Approach to Analysis and Design of Denoising Markov Models</h3>
<ul>
<li><strong>Authors: </strong>Yinuo Ren, Grant M. Rotskoff, Lexing Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01938">https://arxiv.org/abs/2504.01938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01938">https://arxiv.org/pdf/2504.01938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01938]] A Unified Approach to Analysis and Design of Denoising Markov Models(https://arxiv.org/abs/2504.01938)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Probabilistic generative models based on measure transport, such as diffusion and flow-based models, are often formulated in the language of Markovian stochastic dynamics, where the choice of the underlying process impacts both algorithmic design choices and theoretical analysis. In this paper, we aim to establish a rigorous mathematical foundation for denoising Markov models, a broad class of generative models that postulate a forward process transitioning from the target distribution to a simple, easy-to-sample distribution, alongside a backward process particularly constructed to enable efficient sampling in the reverse direction. Leveraging deep connections with nonequilibrium statistical mechanics and generalized Doob's $h$-transform, we propose a minimal set of assumptions that ensure: (1) explicit construction of the backward generator, (2) a unified variational objective directly minimizing the measure transport discrepancy, and (3) adaptations of the classical score-matching approach across diverse dynamics. Our framework unifies existing formulations of continuous and discrete diffusion models, identifies the most general form of denoising Markov models under certain regularity assumptions on forward generators, and provides a systematic recipe for designing denoising Markov models driven by arbitrary Lévy-type processes. We illustrate the versatility and practical effectiveness of our approach through novel denoising Markov models employing geometric Brownian motion and jump processes as forward dynamics, highlighting the framework's potential flexibility and capability in modeling complex distributions.</li>
</ul>

<h3>Title: OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</h3>
<ul>
<li><strong>Authors: </strong>Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01943">https://arxiv.org/abs/2504.01943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01943">https://arxiv.org/pdf/2504.01943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01943]] OpenCodeReasoning: Advancing Data Distillation for Competitive Coding(https://arxiv.org/abs/2504.01943)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.</li>
</ul>

<h3>Title: Efficient Federated Learning Tiny Language Models for Mobile Network Feature Prediction</h3>
<ul>
<li><strong>Authors: </strong>Daniel Becking, Ingo Friese, Karsten Müller, Thomas Buchholz, Mandy Galkow-Schneider, Wojciech Samek, Detlev Marpe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01947">https://arxiv.org/abs/2504.01947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01947">https://arxiv.org/pdf/2504.01947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01947]] Efficient Federated Learning Tiny Language Models for Mobile Network Feature Prediction(https://arxiv.org/abs/2504.01947)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>In telecommunications, Autonomous Networks (ANs) automatically adjust configurations based on specific requirements (e.g., bandwidth) and available resources. These networks rely on continuous monitoring and intelligent mechanisms for self-optimization, self-repair, and self-protection, nowadays enhanced by Neural Networks (NNs) to enable predictive modeling and pattern recognition. Here, Federated Learning (FL) allows multiple AN cells - each equipped with NNs - to collaboratively train models while preserving data privacy. However, FL requires frequent transmission of large neural data and thus an efficient, standardized compression strategy for reliable communication. To address this, we investigate NNCodec, a Fraunhofer implementation of the ISO/IEC Neural Network Coding (NNC) standard, within a novel FL framework that integrates tiny language models (TLMs) for various mobile network feature prediction (e.g., ping, SNR or band frequency). Our experimental results on the Berlin V2X dataset demonstrate that NNCodec achieves transparent compression (i.e., negligible performance loss) while reducing communication overhead to below 1%, showing the effectiveness of combining NNC with FL in collaboratively learned autonomous mobile networks.</li>
</ul>

<h3>Title: Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging</h3>
<ul>
<li><strong>Authors: </strong>Mohini Anand, Xavier Tricoche</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01953">https://arxiv.org/abs/2504.01953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01953">https://arxiv.org/pdf/2504.01953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01953]] Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging(https://arxiv.org/abs/2504.01953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Understanding the complex myocardial architecture is critical for diagnosing and treating heart disease. However, existing methods often struggle to accurately capture this intricate structure from Diffusion Tensor Imaging (DTI) data, particularly due to the lack of ground truth labels and the ambiguous, intertwined nature of fiber trajectories. We present a novel deep learning framework for unsupervised clustering of myocardial fibers, providing a data-driven approach to identifying distinct fiber bundles. We uniquely combine a Bidirectional Long Short-Term Memory network to capture local sequential information along fibers, with a Transformer autoencoder to learn global shape features, with pointwise incorporation of essential anatomical context. Clustering these representations using a density-based algorithm identifies 33 to 62 robust clusters, successfully capturing the subtle distinctions in fiber trajectories with varying levels of granularity. Our framework offers a new, flexible, and quantitative way to analyze myocardial structure, achieving a level of delineation that, to our knowledge, has not been previously achieved, with potential applications in improving surgical planning, characterizing disease-related remodeling, and ultimately, advancing personalized cardiac care.</li>
</ul>

<h3>Title: Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities</h3>
<ul>
<li><strong>Authors: </strong>Jing Liu, Wenxuan Wang, Yisi Zhang, Yepeng Tang, Xingjian He, Longteng Guo, Tongtian Yue, Xinlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01954">https://arxiv.org/abs/2504.01954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01954">https://arxiv.org/pdf/2504.01954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01954]] Towards Unified Referring Expression Segmentation Across Omni-Level Visual Target Granularities(https://arxiv.org/abs/2504.01954)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Referring expression segmentation (RES) aims at segmenting the entities' masks that match the descriptive language expression. While traditional RES methods primarily address object-level grounding, real-world scenarios demand a more versatile framework that can handle multiple levels of target granularity, such as multi-object, single object or part-level references. This introduces great challenges due to the diverse and nuanced ways users describe targets. However, existing datasets and models mainly focus on designing grounding specialists for object-level target localization, lacking the necessary data resources and unified frameworks for the more practical multi-grained RES. In this paper, we take a step further towards visual granularity unified RES task. To overcome the limitation of data scarcity, we introduce a new multi-granularity referring expression segmentation (MRES) task, alongside the RefCOCOm benchmark, which includes part-level annotations for advancing finer-grained visual understanding. In addition, we create MRES-32M, the largest visual grounding dataset, comprising over 32.2M masks and captions across 1M images, specifically designed for part-level vision-language grounding. To tackle the challenges of multi-granularity RES, we propose UniRES++, a unified multimodal large language model that integrates object-level and part-level RES tasks. UniRES++ incorporates targeted designs for fine-grained visual feature exploration. With the joint model architecture and parameters, UniRES++ achieves state-of-the-art performance across multiple benchmarks, including RefCOCOm for MRES, gRefCOCO for generalized RES, and RefCOCO, RefCOCO+, RefCOCOg for classic RES. To foster future research into multi-grained visual grounding, our RefCOCOm benchmark, MRES-32M dataset and model UniRES++ will be publicly available at this https URL.</li>
</ul>

<h3>Title: Scene-Centric Unsupervised Panoptic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Oliver Hahn, Christoph Reich, Nikita Araslanov, Daniel Cremers, Christian Rupprecht, Stefan Roth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01955">https://arxiv.org/abs/2504.01955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01955">https://arxiv.org/pdf/2504.01955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01955]] Scene-Centric Unsupervised Panoptic Segmentation(https://arxiv.org/abs/2504.01955)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.</li>
</ul>

<h3>Title: VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Wang, Fangfu Liu, Jiawei Chi, Yueqi Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01956">https://arxiv.org/abs/2504.01956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01956">https://arxiv.org/pdf/2504.01956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01956]] VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step(https://arxiv.org/abs/2504.01956)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: this https URL</li>
</ul>

<h3>Title: Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.01960">https://arxiv.org/abs/2504.01960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.01960">https://arxiv.org/pdf/2504.01960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.01960]] Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis(https://arxiv.org/abs/2504.01960)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
