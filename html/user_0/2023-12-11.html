<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models. (arXiv:2312.04724v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04724">http://arxiv.org/abs/2312.04724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04724]] Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models(http://arxiv.org/abs/2312.04724)</code></li>
<li>Summary: <p>This paper presents CyberSecEval, a comprehensive benchmark developed to help
bolster the cybersecurity of Large Language Models (LLMs) employed as coding
assistants. As what we believe to be the most extensive unified cybersecurity
safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs
in two crucial security domains: their propensity to generate insecure code and
their level of compliance when asked to assist in cyberattacks. Through a case
study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large
language model families, CyberSecEval effectively pinpointed key cybersecurity
risks. More importantly, it offered practical insights for refining these
models. A significant observation from the study was the tendency of more
advanced models to suggest insecure code, highlighting the critical need for
integrating security considerations in the development of sophisticated LLMs.
CyberSecEval, with its automated test case generation and evaluation pipeline
covers a broad scope and equips LLM designers and researchers with a tool to
broadly measure and enhance the cybersecurity safety properties of LLMs,
contributing to the development of more secure AI systems.
</p></li>
</ul>

<h3>Title: Hiding Functions within Functions: Steganography by Implicit Neural Representations. (arXiv:2312.04743v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04743">http://arxiv.org/abs/2312.04743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04743]] Hiding Functions within Functions: Steganography by Implicit Neural Representations(http://arxiv.org/abs/2312.04743)</code></li>
<li>Summary: <p>Deep steganography utilizes the powerful capabilities of deep neural networks
to embed and extract messages, but its reliance on an additional message
extractor limits its practical use due to the added suspicion it can raise from
steganalyzers. To address this problem, we propose StegaINR, which utilizes
Implicit Neural Representation (INR) to implement steganography. StegaINR
embeds a secret function into a stego function, which serves as both the
message extractor and the stego media for secure transmission on a public
channel. Recipients need only use a shared key to recover the secret function
from the stego function, allowing them to obtain the secret message. Our
approach makes use of continuous functions, enabling it to handle various types
of messages. To our knowledge, this is the first work to introduce INR into
steganography. We performed evaluations on image and climate data to test our
method in different deployment contexts.
</p></li>
</ul>

<h3>Title: Towards Efficient Secure Aggregation in FL: Partial Vector Freezing for Cost Compression. (arXiv:2312.04920v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04920">http://arxiv.org/abs/2312.04920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04920]] Towards Efficient Secure Aggregation in FL: Partial Vector Freezing for Cost Compression(http://arxiv.org/abs/2312.04920)</code></li>
<li>Summary: <p>Secure aggregation of user vectors has become a critical issue in the field
of federated learning. Many Secure Aggregation Protocols (SAP) face exorbitant
computation costs, which severely limit their applicability. We uncover that
current endeavors to reduce computation costs tend to overlook a crucial fact:
a considerable portion of SAP's computation burden stems from processing each
entry in the private vectors. Given this observation, we propose PVF, a
portable module for compressing computation costs. PVF is able to ``freeze'' a
substantial portion of the private vector through specific linear
transformations, only requiring $\frac{1}{\lambda}$ of the original vector to
participate in SAP. Eventually, users can ``thaw'' the public sum of the
``frozen entries" by the result of SAP. To enhance functionality, we introduce
extensions that can enforce consistency constraints on users' original vectors,
verify aggregated results, and enhance security when a portion of the private
vector is known to the server. We demonstrate that PVF can seamlessly integrate
with various SAP and prove that it poses no threat to user privacy in the
semi-honest and active adversary settings. We select $8$ baselines,
encompassing $6$ distinct types of SAP, and explore the acceleration effects of
PVF on these SAP. Empirical investigations indicate that when $\lambda=100$,
PVF yields up to $99.5\times$ speedup and up to $32.3\times$ communication
reduction, with the potential to approach nearly $1000\times$ acceleration as
$\lambda$ increases.
</p></li>
</ul>

<h3>Title: AHSecAgg and TSKG: Lightweight Secure Aggregation for Federated Learning Without Compromise. (arXiv:2312.04937v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04937">http://arxiv.org/abs/2312.04937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04937]] AHSecAgg and TSKG: Lightweight Secure Aggregation for Federated Learning Without Compromise(http://arxiv.org/abs/2312.04937)</code></li>
<li>Summary: <p>Leveraging federated learning (FL) to enable cross-domain privacy-sensitive
data mining represents a vital breakthrough to accomplish privacy-preserving
learning. However, attackers can infer the original user data by analyzing the
uploaded intermediate parameters during the aggregation process. Therefore,
secure aggregation has become a critical issue in the field of FL. Many secure
aggregation protocols face the problem of high computation costs, which
severely limits their applicability. To this end, we propose AHSecAgg, a
lightweight secure aggregation protocol using additive homomorphic masks.
AHSecAgg significantly reduces computation overhead without compromising the
dropout handling capability or model accuracy. We prove the security of
AHSecAgg in semi-honest and active adversary settings. In addition, in
cross-silo scenarios where the group of participants is relatively fixed during
each round, we propose TSKG, a lightweight Threshold Signature based masking
key generation method. TSKG can generate different temporary secrets and shares
for different aggregation rounds using the initial key and thus effectively
eliminates the cost of secret sharing and key agreement. We prove TSKG does not
sacrifice security. Extensive experiments show that AHSecAgg significantly
outperforms state-of-the-art mask-based secure aggregation protocols in terms
of computational efficiency, and TSKG effectively reduces the computation and
communication costs for existing secure aggregation protocols.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: DeepFidelity: Perceptual Forgery Fidelity Assessment for Deepfake Detection. (arXiv:2312.04961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04961">http://arxiv.org/abs/2312.04961</a></li>
<li>Code URL: https://github.com/shimmer-ghq/deepfidelity</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04961]] DeepFidelity: Perceptual Forgery Fidelity Assessment for Deepfake Detection(http://arxiv.org/abs/2312.04961)</code></li>
<li>Summary: <p>Deepfake detection refers to detecting artificially generated or edited faces
in images or videos, which plays an essential role in visual information
security. Despite promising progress in recent years, Deepfake detection
remains a challenging problem due to the complexity and variability of face
forgery techniques. Existing Deepfake detection methods are often devoted to
extracting features by designing sophisticated networks but ignore the
influence of perceptual quality of faces. Considering the complexity of the
quality distribution of both real and fake faces, we propose a novel Deepfake
detection framework named DeepFidelity to adaptively distinguish real and fake
faces with varying image quality by mining the perceptual forgery fidelity of
face images. Specifically, we improve the model's ability to identify complex
samples by mapping real and fake face data of different qualities to different
scores to distinguish them in a more detailed way. In addition, we propose a
network structure called Symmetric Spatial Attention Augmentation based vision
Transformer (SSAAFormer), which uses the symmetry of face images to promote the
network to model the geographic long-distance relationship at the shallow level
and augment local features. Extensive experiments on multiple benchmark
datasets demonstrate the superiority of the proposed method over
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning. (arXiv:2312.05219v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05219">http://arxiv.org/abs/2312.05219</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05219]] Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning(http://arxiv.org/abs/2312.05219)</code></li>
<li>Summary: <p>Accurate analysis and classification of facial attributes are essential in
various applications, from human-computer interaction to security systems. In
this work, a novel approach to enhance facial classification and recognition
tasks through the integration of 3D facial models with deep learning methods
was proposed. We extract the most useful information for various tasks using
the 3D Facial Model, leading to improved classification accuracy. Combining 3D
facial insights with ResNet architecture, our approach achieves notable
results: 100% individual classification, 95.4% gender classification, and 83.5%
expression classification accuracy. This method holds promise for advancing
facial analysis and recognition research.
</p></li>
</ul>

<h3>Title: The Evolution of DNS Security and Privacy. (arXiv:2312.04577v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04577">http://arxiv.org/abs/2312.04577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04577]] The Evolution of DNS Security and Privacy(http://arxiv.org/abs/2312.04577)</code></li>
<li>Summary: <p>DNS, one of the fundamental protocols of the TCP/IP stack, has evolved over
the years to protect against threats and attacks. This study examines the risks
associated with DNS and explores recent advancements that contribute towards
making the DNS ecosystem resilient against various attacks while safeguarding
user privacy.
</p></li>
</ul>

<h3>Title: Automated SELinux RBAC Policy Verification Using SMT. (arXiv:2312.04586v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04586">http://arxiv.org/abs/2312.04586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04586]] Automated SELinux RBAC Policy Verification Using SMT(http://arxiv.org/abs/2312.04586)</code></li>
<li>Summary: <p>Security-Enhanced Linux (SELinux) is a Linux kernel module that allows for a
role-based access control (RBAC) mechanism. It provides a fine-grained security
framework enabling system administrators to define security policies at the
system and application level. Whilst SELinux offers robust security features
through a customisable, powerful RBAC model, its manual policy management is
prone to error, leaving the system vulnerable to accidental misconfigurations
or loopholes. We present a tool to automate the conversion of SELinux policies
into satisfiability modulo theories (SMT), enabling the verification of the
intended security configurations using automated theorem proving. Our tool is
capable of flagging common policy misconfigurations by asserting consistency
between supplied RBAC policies and the intended specification by the user in
SMT. RBAC policies are inherently complicated to verify entirely. We envision
that the automated tool presented here can be further extended to identify an
even broader range of policy misconfigurations, relieving the burden of
managing convoluted policies on system administrators.
</p></li>
</ul>

<h3>Title: Using Program Knowledge Graph to Uncover Software Vulnerabilities. (arXiv:2312.04818v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04818">http://arxiv.org/abs/2312.04818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04818]] Using Program Knowledge Graph to Uncover Software Vulnerabilities(http://arxiv.org/abs/2312.04818)</code></li>
<li>Summary: <p>In an increasingly interconnected and data-driven world, the importance of
robust security measures cannot be overstated. A knowledge graph constructed
with information extracted from the system along with the desired security
behavior can be utilized to identify complex security vulnerabilities hidden
underneath the systems. Unfortunately, existing security knowledge graphs are
constructed from coarse-grained information extracted from publicly available
vulnerability reports, which are not equipped to check actual security
violations in real-world system implementations. In this poster, we present a
novel approach of using Program Knowledge Graph that is embedded with
fine-grained execution information of the systems (e.g., callgraph, data-flow,
etc.) along with information extracted from the public vulnerability and
weakness datasets (e.g., CVE and CWE). We further demonstrate that our custom
security knowledge graph can be checked against the standard queries generated
by LLM, providing a powerful way to identify security vulnerabilities and
weaknesses in critical systems.
</p></li>
</ul>

<h3>Title: A stacked ensemble learning IDS model for Software-defined VANET. (arXiv:2312.04956v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04956">http://arxiv.org/abs/2312.04956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04956]] A stacked ensemble learning IDS model for Software-defined VANET(http://arxiv.org/abs/2312.04956)</code></li>
<li>Summary: <p>ntrusion Detection Systems (IDS) are widely employed to detect and mitigate
external network security events. VANETs (Vehicle ad-hoc Networks) are
evolving, especially with the development of Connected Autonomous Vehicles
(CAVs). So, it is crucial to assess how traditional IDS approaches can be
utilised for emerging technologies. To address this concern, our work presents
a stacked ensemble learning approach for IDS, which combines multiple machine
learning algorithms to detect threats more effectively than single algorithm
methods. Using the CICIDS2017 and the VeReMi benchmark data sets, we compare
the performance of our approach with existing machine learning methods and find
that it is more accurate at identifying threats. Our method also incorporates
hyperparameter optimization and feature selection to improve its performance
further. Overall, our results suggest that stacked ensemble learning is a
promising technique for enhancing the effectiveness of IDS.
</p></li>
</ul>

<h3>Title: Reverse Engineering Deep ReLU Networks An Optimization-based Algorithm. (arXiv:2312.04675v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04675">http://arxiv.org/abs/2312.04675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04675]] Reverse Engineering Deep ReLU Networks An Optimization-based Algorithm(http://arxiv.org/abs/2312.04675)</code></li>
<li>Summary: <p>Reverse engineering deep ReLU networks is a critical problem in understanding
the complex behavior and interpretability of neural networks. In this research,
we present a novel method for reconstructing deep ReLU networks by leveraging
convex optimization techniques and a sampling-based approach. Our method begins
by sampling points in the input space and querying the black box model to
obtain the corresponding hyperplanes. We then define a convex optimization
problem with carefully chosen constraints and conditions to guarantee its
convexity. The objective function is designed to minimize the discrepancy
between the reconstructed networks output and the target models output, subject
to the constraints. We employ gradient descent to optimize the objective
function, incorporating L1 or L2 regularization as needed to encourage sparse
or smooth solutions. Our research contributes to the growing body of work on
reverse engineering deep ReLU networks and paves the way for new advancements
in neural network interpretability and security.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Reconciling AI Performance and Data Reconstruction Resilience for Medical Imaging. (arXiv:2312.04590v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04590">http://arxiv.org/abs/2312.04590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04590]] Reconciling AI Performance and Data Reconstruction Resilience for Medical Imaging(http://arxiv.org/abs/2312.04590)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) models are vulnerable to information leakage of
their training data, which can be highly sensitive, for example in medical
imaging. Privacy Enhancing Technologies (PETs), such as Differential Privacy
(DP), aim to circumvent these susceptibilities. DP is the strongest possible
protection for training models while bounding the risks of inferring the
inclusion of training samples or reconstructing the original data. DP achieves
this by setting a quantifiable privacy budget. Although a lower budget
decreases the risk of information leakage, it typically also reduces the
performance of such models. This imposes a trade-off between robust performance
and stringent privacy. Additionally, the interpretation of a privacy budget
remains abstract and challenging to contextualize. In this study, we contrast
the performance of AI models at various privacy budgets against both,
theoretical risk bounds and empirical success of reconstruction attacks. We
show that using very large privacy budgets can render reconstruction attacks
impossible, while drops in performance are negligible. We thus conclude that
not using DP -- at all -- is negligent when applying AI models to sensitive
data. We deem those results to lie a foundation for further debates on striking
a balance between privacy risks and model performance.
</p></li>
</ul>

<h3>Title: Diffence: Fencing Membership Privacy With Diffusion Models. (arXiv:2312.04692v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04692">http://arxiv.org/abs/2312.04692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04692]] Diffence: Fencing Membership Privacy With Diffusion Models(http://arxiv.org/abs/2312.04692)</code></li>
<li>Summary: <p>Deep learning models, while achieving remarkable performance across various
tasks, are vulnerable to member inference attacks, wherein adversaries identify
if a specific data point was part of a model's training set. This
susceptibility raises substantial privacy concerns, especially when models are
trained on sensitive datasets. Current defense methods often struggle to
provide robust protection without hurting model utility, and they often require
retraining the model or using extra data. In this work, we introduce a novel
defense framework against membership attacks by leveraging generative models.
The key intuition of our defense is to remove the differences between member
and non-member inputs which can be used to perform membership attacks, by
re-generating input samples before feeding them to the target model. Therefore,
our defense works \emph{pre-inference}, which is unlike prior defenses that are
either training-time (modify the model) or post-inference time (modify the
model's output).
</p>
<p>A unique feature of our defense is that it works on input samples only,
without modifying the training or inference phase of the target model.
Therefore, it can be cascaded with other defense mechanisms as we demonstrate
through experiments. Through extensive experimentation, we show that our
approach can serve as a robust plug-n-play defense mechanism, enhancing
membership privacy without compromising model utility in both baseline and
defended settings. For example, our method enhanced the effectiveness of recent
state-of-the-art defenses, reducing attack accuracy by an average of 5.7\% to
12.4\% across three datasets, without any impact on the model's accuracy. By
integrating our method with prior defenses, we achieve new state-of-the-art
performance in the privacy-utility trade-off.
</p></li>
</ul>

<h3>Title: Seeing ChatGPT Through Universities' Policies, Resources and Guidelines. (arXiv:2312.05235v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05235">http://arxiv.org/abs/2312.05235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05235]] Seeing ChatGPT Through Universities' Policies, Resources and Guidelines(http://arxiv.org/abs/2312.05235)</code></li>
<li>Summary: <p>The advancements in Artificial Intelligence (AI) technologies such as ChatGPT
have gained popularity in recent days. The integration of ChatGPT in
educational contexts has already created attractions due to a wide range of
applications. However, the automatic generation of human-like texts also poses
potential risks to academic integrity, especially when faced with
writing-intensive language courses. Considering the ongoing debates, this study
aims to investigate the academic policies and guidelines established by US
universities regarding the use of ChatGPT in teaching and learning. The data
sources include academic policies, statements, guidelines as well as relevant
resources that were provided by the top 50 universities in the United States,
according to U.S. News. Thematic analysis and qualitative analysis were
employed in the analysis and showed that most top 50 universities were open but
cautious towards the integration of generative AI in teaching and learning and
also expressed their concerns on ethical usage, accuracy, and data privacy.
Most universities also provided a variety of resources and guidelines,
including syllabus templates/samples, workshops and discussions, shared
articles, and one-on-one consultations, with focuses on general technical
introduction, ethical concerns, pedagogical applications, preventive
strategies, data privacy, limitations, and detective tools. The findings will
inform future policy-making regarding the integration of ChatGPT in
college-level education and influence the provision of supportive resources by
universities for the appropriate application of ChatGPT in education.
</p></li>
</ul>

<h3>Title: zkFDL: An efficient and privacy-preserving decentralized federated learning with zero knowledge proof. (arXiv:2312.04579v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04579">http://arxiv.org/abs/2312.04579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04579]] zkFDL: An efficient and privacy-preserving decentralized federated learning with zero knowledge proof(http://arxiv.org/abs/2312.04579)</code></li>
<li>Summary: <p>Federated leaning (FL) has been frequently used in various field of studies
and businesses. Traditional centralized FL systems suffer from serious issues.
To address these concerns, decentralized federated learning (DFL) systems have
been introduced in recent years in which with the help of blockchains, try to
achieve more integrity and efficiency. On the other hand, privacy-preserving is
an uncovered part of these systems. To address this, and also scaling the
blockchain-based computations, we propose a zero knowledge proof (ZKP) based
aggregator (zkDFL) that allows clients to share their large-scale model
parameters with a trusted centralized server without revealing their individual
data to other clients. We utilize blockchain technology to manage the
aggregation algorithm via smart contracts. The server performs a ZKP algorithm
to prove to the clients that the aggregation is done according to the accepted
algorithm. The server can also prove that all inputs of clients have been used.
We evaluate our measure through a public dataset about wearable internet of
things. As demonstrated by numerical evaluations, zkDFL introduces
verifiability of correctness of aggregation process and enhances the privacy
protection and scalability of DFL systems, while the gas cost has declined
significantly.
</p></li>
</ul>

<h3>Title: FedGeo: Privacy-Preserving User Next Location Prediction with Federated Learning. (arXiv:2312.04594v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04594">http://arxiv.org/abs/2312.04594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04594]] FedGeo: Privacy-Preserving User Next Location Prediction with Federated Learning(http://arxiv.org/abs/2312.04594)</code></li>
<li>Summary: <p>A User Next Location Prediction (UNLP) task, which predicts the next location
that a user will move to given his/her trajectory, is an indispensable task for
a wide range of applications. Previous studies using large-scale trajectory
datasets in a single server have achieved remarkable performance in UNLP task.
However, in real-world applications, legal and ethical issues have been raised
regarding privacy concerns leading to restrictions against sharing human
trajectory datasets to any other server. In response, Federated Learning (FL)
has emerged to address the personal privacy issue by collaboratively training
multiple clients (i.e., users) and then aggregating them. While previous
studies employed FL for UNLP, they are still unable to achieve reliable
performance because of the heterogeneity of clients' mobility. To tackle this
problem, we propose the Federated Learning for Geographic Information (FedGeo),
a FL framework specialized for UNLP, which alleviates the heterogeneity of
clients' mobility and guarantees personal privacy protection. Firstly, we
incorporate prior global geographic adjacency information to the local client
model, since the spatial correlation between locations is trained partially in
each client who has only a heterogeneous subset of the overall trajectories in
FL. We also introduce a novel aggregation method that minimizes the gap between
client models to solve the problem of client drift caused by differences
between client models when learning with their heterogeneous data. Lastly, we
probabilistically exclude clients with extremely heterogeneous data from the FL
process by focusing on clients who visit relatively diverse locations. We show
that FedGeo is superior to other FL methods for model performance in UNLP task.
We also validated our model in a real-world application using our own
customers' mobile phones and the FL agent system.
</p></li>
</ul>

<h3>Title: DPI: Ensuring Strict Differential Privacy for Infinite Data Streaming. (arXiv:2312.04738v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04738">http://arxiv.org/abs/2312.04738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04738]] DPI: Ensuring Strict Differential Privacy for Infinite Data Streaming(http://arxiv.org/abs/2312.04738)</code></li>
<li>Summary: <p>Streaming data, crucial for applications like crowdsourcing analytics,
behavior studies, and real-time monitoring, faces significant privacy risks due
to the large and diverse data linked to individuals. In particular, recent
efforts to release data streams, using the rigorous privacy notion of
differential privacy (DP), have encountered issues with unbounded privacy
leakage. This challenge limits their applicability to only a finite number of
time slots (''finite data stream'') or relaxation to protecting the events
(''event or $w$-event DP'') rather than all the records of users. A persistent
challenge is managing the sensitivity of outputs to inputs in situations where
users contribute many activities and data distributions evolve over time. In
this paper, we present a novel technique for Differentially Private data
streaming over Infinite disclosure (DPI) that effectively bounds the total
privacy leakage of each user in infinite data streams while enabling accurate
data collection and analysis. Furthermore, we also maximize the accuracy of DPI
via a novel boosting mechanism. Finally, extensive experiments across various
streaming applications and real datasets (e.g., COVID-19, Network Traffic, and
USDA Production), show that DPI maintains high utility for infinite data
streams in diverse settings. Code for DPI is available at
https://github.com/ShuyaFeng/DPI.
</p></li>
</ul>

<h3>Title: On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against "Truly Anonymous Synthetic Data''. (arXiv:2312.05114v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05114">http://arxiv.org/abs/2312.05114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05114]] On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against "Truly Anonymous Synthetic Data''(http://arxiv.org/abs/2312.05114)</code></li>
<li>Summary: <p>Training generative models to produce synthetic data is meant to provide a
privacy-friendly approach to data release. However, we get robust guarantees
only when models are trained to satisfy Differential Privacy (DP). Alas, this
is not the standard in industry as many companies use ad-hoc strategies to
empirically evaluate privacy based on the statistical similarity between
synthetic and real data. In this paper, we review the privacy metrics offered
by leading companies in this space and shed light on a few critical flaws in
reasoning about privacy entirely via empirical evaluations. We analyze the
undesirable properties of the most popular metrics and filters and demonstrate
their unreliability and inconsistency through counter-examples. We then present
a reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all
attributes of) at least 78% of the low-density train records (or outliers) with
only black-box access to a single fitted generative model and the privacy
metrics. Finally, we show that applying DP only to the model or using
low-utility generators does not mitigate ReconSyn as the privacy leakage
predominantly comes from the metrics. Overall, our work serves as a warning to
practitioners not to deviate from established privacy-preserving mechanisms.
</p></li>
</ul>

<h3>Title: Topology-Based Reconstruction Prevention for Decentralised Learning. (arXiv:2312.05248v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05248">http://arxiv.org/abs/2312.05248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05248]] Topology-Based Reconstruction Prevention for Decentralised Learning(http://arxiv.org/abs/2312.05248)</code></li>
<li>Summary: <p>Decentralised learning has recently gained traction as an alternative to
federated learning in which both data and coordination are distributed over its
users. To preserve the confidentiality of users' data, decentralised learning
relies on differential privacy, multi-party computation, or a combination
thereof. However, running multiple privacy-preserving summations in sequence
may allow adversaries to perform reconstruction attacks. Unfortunately, current
reconstruction countermeasures either cannot trivially be adapted to the
distributed setting, or add excessive amounts of noise.
</p>
<p>In this work, we first show that passive honest-but-curious adversaries can
reconstruct other users' private data after several privacy-preserving
summations. For example, in subgraphs with 18 users, we show that only three
passive honest-but-curious adversaries succeed at reconstructing private data
11.0% of the time, requiring an average of 8.8 summations per adversary. The
success rate is independent of the size of the full network. We consider weak
adversaries, who do not control the graph topology and can exploit neither the
workings of the summation protocol nor the specifics of users' data.
</p>
<p>We develop a mathematical understanding of how reconstruction relates to
topology and propose the first topology-based decentralised defence against
reconstruction attacks. Specifically, we show that reconstruction requires a
number of adversaries linear in the length of the network's shortest cycle.
Consequently, reconstructing private data from privacy-preserving summations is
impossible in acyclic networks.
</p>
<p>Our work is a stepping stone for a formal theory of decentralised
reconstruction defences based on topology. Such a theory would generalise our
countermeasure beyond summation, define confidentiality in terms of entropy,
and describe the effects of (topology-aware) differential privacy.
</p></li>
</ul>

<h3>Title: Distributed Optimization via Kernelized Multi-armed Bandits. (arXiv:2312.04719v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04719">http://arxiv.org/abs/2312.04719</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04719]] Distributed Optimization via Kernelized Multi-armed Bandits(http://arxiv.org/abs/2312.04719)</code></li>
<li>Summary: <p>Multi-armed bandit algorithms provide solutions for sequential
decision-making where learning takes place by interacting with the environment.
In this work, we model a distributed optimization problem as a multi-agent
kernelized multi-armed bandit problem with a heterogeneous reward setting. In
this setup, the agents collaboratively aim to maximize a global objective
function which is an average of local objective functions. The agents can
access only bandit feedback (noisy reward) obtained from the associated unknown
local function with a small norm in reproducing kernel Hilbert space (RKHS). We
present a fully decentralized algorithm, Multi-agent IGP-UCB (MA-IGP-UCB),
which achieves a sub-linear regret bound for popular classes for kernels while
preserving privacy. It does not necessitate the agents to share their actions,
rewards, or estimates of their local function. In the proposed approach, the
agents sample their individual local functions in a way that benefits the whole
network by utilizing a running consensus to estimate the upper confidence bound
on the global function. Furthermore, we propose an extension, Multi-agent
Delayed IGP-UCB (MAD-IGP-UCB) algorithm, which reduces the dependence of the
regret bound on the number of agents in the network. It provides improved
performance by utilizing a delay in the estimation update step at the cost of
more communication.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting. (arXiv:2312.04902v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04902">http://arxiv.org/abs/2312.04902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04902]] BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting(http://arxiv.org/abs/2312.04902)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are susceptible to backdoor attacks, where
malicious functionality is embedded to allow attackers to trigger incorrect
classifications. Old-school backdoor attacks use strong trigger features that
can easily be learned by victim models. Despite robustness against input
variation, the robustness however increases the likelihood of unintentional
trigger activations. This leaves traces to existing defenses, which find
approximate replacements for the original triggers that can activate the
backdoor without being identical to the original trigger via, e.g., reverse
engineering and sample overlay.
</p>
<p>In this paper, we propose and investigate a new characteristic of backdoor
attacks, namely, backdoor exclusivity, which measures the ability of backdoor
triggers to remain effective in the presence of input variation. Building upon
the concept of backdoor exclusivity, we propose Backdoor Exclusivity LifTing
(BELT), a novel technique which suppresses the association between the backdoor
and fuzzy triggers to enhance backdoor exclusivity for defense evasion.
Extensive evaluation on three popular backdoor benchmarks validate, our
approach substantially enhances the stealthiness of four old-school backdoor
attacks, which, after backdoor exclusivity lifting, is able to evade six
state-of-the-art backdoor countermeasures, at almost no cost of the attack
success rate and normal utility. For example, one of the earliest backdoor
attacks BadNet, enhanced by BELT, evades most of the state-of-the-art defenses
including ABS and MOTH which would otherwise recognize the backdoored model.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. (arXiv:2312.04584v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04584">http://arxiv.org/abs/2312.04584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04584]] Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger(http://arxiv.org/abs/2312.04584)</code></li>
<li>Summary: <p>Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and
malicious methods since they can easily circumvent most of the current backdoor
defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due
to their poisoned-label nature, where users can discover anomalies if they
check the image-label relationship. In particular, we demonstrate that it is
ineffective to directly generalize existing SSBAs to their clean-label variants
by poisoning samples solely from the target class. We reveal that it is
primarily due to two reasons, including \textbf{(1)} the `antagonistic effects'
of ground-truth features and \textbf{(2)} the learning difficulty of
sample-specific features. Accordingly, trigger-related features of existing
SSBAs cannot be effectively learned under the clean-label setting due to their
mild trigger intensity required for ensuring stealthiness. We argue that the
intensity constraint of existing SSBAs is mostly because their trigger patterns
are `content-irrelevant' and therefore act as `noises' for both humans and
DNNs. Motivated by this understanding, we propose to exploit content-relevant
features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design
clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with
attribute trigger (BAAT). Extensive experiments are conducted on benchmark
datasets, which verify the effectiveness of our BAAT and its resistance to
existing defenses.
</p></li>
</ul>

<h3>Title: SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation. (arXiv:2312.04913v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04913">http://arxiv.org/abs/2312.04913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04913]] SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation(http://arxiv.org/abs/2312.04913)</code></li>
<li>Summary: <p>Current Visual-Language Pre-training (VLP) models are vulnerable to
adversarial examples. These adversarial examples present substantial security
risks to VLP models, as they can leverage inherent weaknesses in the models,
resulting in incorrect predictions. In contrast to white-box adversarial
attacks, transfer attacks (where the adversary crafts adversarial examples on a
white-box model to fool another black-box model) are more reflective of
real-world scenarios, thus making them more meaningful for research. By
summarizing and analyzing existing research, we identified two factors that can
influence the efficacy of transfer attacks on VLP models: inter-modal
interaction and data diversity. Based on these insights, we propose a
self-augment-based transfer attack method, termed SA-Attack. Specifically,
during the generation of adversarial images and adversarial texts, we apply
different data augmentation methods to the image modality and text modality,
respectively, with the aim of improving the adversarial transferability of the
generated adversarial images and texts. Experiments conducted on the FLickr30K
and COCO datasets have validated the effectiveness of our method. Our code will
be available after this paper is accepted.
</p></li>
</ul>

<h3>Title: Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks. (arXiv:2312.04748v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04748">http://arxiv.org/abs/2312.04748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04748]] Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks(http://arxiv.org/abs/2312.04748)</code></li>
<li>Summary: <p>Growing applications of large language models (LLMs) trained by a third party
raise serious concerns on the security vulnerability of LLMs.It has been
demonstrated that malicious actors can covertly exploit these vulnerabilities
in LLMs through poisoning attacks aimed at generating undesirable outputs.
While poisoning attacks have received significant attention in the image domain
(e.g., object detection), and classification tasks, their implications for
generative models, particularly in the realm of natural language generation
(NLG) tasks, remain poorly understood. To bridge this gap, we perform a
comprehensive exploration of various poisoning techniques to assess their
effectiveness across a range of generative tasks. Furthermore, we introduce a
range of metrics designed to quantify the success and stealthiness of poisoning
attacks specifically tailored to NLG tasks. Through extensive experiments on
multiple NLG tasks, LLMs and datasets, we show that it is possible to
successfully poison an LLM during the fine-tuning stage using as little as 1\%
of the total tuning data samples. Our paper presents the first systematic
approach to comprehend poisoning attacks targeting NLG tasks considering a wide
range of triggers and attack settings. We hope our findings will assist the AI
security community in devising appropriate defenses against such threats.
</p></li>
</ul>

<h3>Title: FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against Adversarial Attacks. (arXiv:2312.04587v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04587">http://arxiv.org/abs/2312.04587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04587]] FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against Adversarial Attacks(http://arxiv.org/abs/2312.04587)</code></li>
<li>Summary: <p>Federated learning has created a decentralized method to train a machine
learning model without needing direct access to client data. The main goal of a
federated learning architecture is to protect the privacy of each client while
still contributing to the training of the global model. However, the main
advantage of privacy in federated learning is also the easiest aspect to
exploit. Without being able to see the clients' data, it is difficult to
determine the quality of the data. By utilizing data poisoning methods, such as
backdoor or label-flipping attacks, or by sending manipulated information about
their data back to the server, malicious clients are able to corrupt the global
model and degrade performance across all clients within a federation. Our novel
aggregation method, FedBayes, mitigates the effect of a malicious client by
calculating the probabilities of a client's model weights given to the prior
model's weights using Bayesian statistics. Our results show that this approach
negates the effects of malicious clients and protects the overall federation.
</p></li>
</ul>

<h3>Title: Feature Analysis of Encrypted Malicious Traffic. (arXiv:2312.04596v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04596">http://arxiv.org/abs/2312.04596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04596]] Feature Analysis of Encrypted Malicious Traffic(http://arxiv.org/abs/2312.04596)</code></li>
<li>Summary: <p>In recent years there has been a dramatic increase in the number of malware
attacks that use encrypted HTTP traffic for self-propagation or communication.
Antivirus software and firewalls typically will not have access to encryption
keys, and therefore direct detection of malicious encrypted data is unlikely to
succeed. However, previous work has shown that traffic analysis can provide
indications of malicious intent, even in cases where the underlying data
remains encrypted. In this paper, we apply three machine learning techniques to
the problem of distinguishing malicious encrypted HTTP traffic from benign
encrypted traffic and obtain results comparable to previous work. We then
consider the problem of feature analysis in some detail. Previous work has
often relied on human expertise to determine the most useful and informative
features in this problem domain. We demonstrate that such feature-related
information can be obtained directly from machine learning models themselves.
We argue that such a machine learning based approach to feature analysis is
preferable, as it is more reliable, and we can, for example, uncover relatively
unintuitive interactions between features.
</p></li>
</ul>

<h3>Title: TrustFed: A Reliable Federated Learning Framework with Malicious-Attack Resistance. (arXiv:2312.04597v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04597">http://arxiv.org/abs/2312.04597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04597]] TrustFed: A Reliable Federated Learning Framework with Malicious-Attack Resistance(http://arxiv.org/abs/2312.04597)</code></li>
<li>Summary: <p>As a key technology in 6G research, federated learning (FL) enables
collaborative learning among multiple clients while ensuring individual data
privacy. However, malicious attackers among the participating clients can
intentionally tamper with the training data or the trained model, compromising
the accuracy and trustworthiness of the system. To address this issue, in this
paper, we propose a hierarchical audit-based FL (HiAudit-FL) framework, with
the aim to enhance the reliability and security of the learning process. The
hierarchical audit process includes two stages, namely model-audit and
parameter-audit. In the model-audit stage, a low-overhead audit method is
employed to identify suspicious clients. Subsequently, in the parameter-audit
stage, a resource-consuming method is used to detect all malicious clients with
higher accuracy among the suspicious ones. Specifically, we execute the model
audit method among partial clients for multiple rounds, which is modeled as a
partial observation Markov decision process (POMDP) with the aim to enhance the
robustness and accountability of the decision-making in complex and uncertain
environments. Meanwhile, we formulate the problem of identifying malicious
attackers through a multi-round audit as an active sequential hypothesis
testing problem and leverage a diffusion model-based AI-Enabled audit selection
strategy (ASS) to decide which clients should be audited in each round. To
accomplish efficient and effective audit selection, we design a DRL-ASS
algorithm by incorporating the ASS in a deep reinforcement learning (DRL)
framework. Our simulation results demonstrate that HiAudit-FL can effectively
identify and handle potential malicious users accurately, with small system
overhead.
</p></li>
</ul>

<h3>Title: DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions. (arXiv:2312.04730v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04730">http://arxiv.org/abs/2312.04730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04730]] DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions(http://arxiv.org/abs/2312.04730)</code></li>
<li>Summary: <p>With the advancement of Large Language Models (LLMs), significant progress
has been made in code generation, enabling LLMs to transform natural language
into programming code. These Code LLMs have been widely accepted by massive
users and organizations. However, a dangerous nature is hidden in the code,
which is the existence of fatal vulnerabilities. While some LLM providers have
attempted to address these issues by aligning with human guidance, these
efforts fall short of making Code LLMs practical and robust. Without a deep
understanding of the performance of the LLMs under the practical worst cases,
it would be concerning to apply them to various real-world applications. In
this paper, we answer the critical issue: Are existing Code LLMs immune to
generating vulnerable code? If not, what is the possible maximum severity of
this issue in practical deployment scenarios? In this paper, we introduce
DeceptPrompt, a novel algorithm that can generate adversarial natural language
instructions that drive the Code LLMs to generate functionality correct code
with vulnerabilities. DeceptPrompt is achieved through a systematic
evolution-based algorithm with a fine grain loss design. The unique advantage
of DeceptPrompt enables us to find natural prefix/suffix with totally benign
and non-directional semantic meaning, meanwhile, having great power in inducing
the Code LLMs to generate vulnerable code. This feature can enable us to
conduct the almost-worstcase red-teaming on these LLMs in a real scenario,
where users are using natural language. Our extensive experiments and analyses
on DeceptPrompt not only validate the effectiveness of our approach but also
shed light on the huge weakness of LLMs in the code generation task. When
applying the optimized prefix/suffix, the attack success rate (ASR) will
improve by average 50% compared with no prefix/suffix applying.
</p></li>
</ul>

<h3>Title: Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks. (arXiv:2312.04864v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04864">http://arxiv.org/abs/2312.04864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04864]] Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks(http://arxiv.org/abs/2312.04864)</code></li>
<li>Summary: <p>Networks, threat models, and malicious actors are advancing quickly. With the
increased deployment of the 5G networks, the security issues of the attached 5G
physical devices have also increased. Therefore, artificial intelligence based
autonomous end-to-end security design is needed that can deal with incoming
threats by detecting network traffic anomalies. To address this requirement, in
this research, we used a recently published 5G traffic dataset, 5G-NIDD, to
detect network traffic anomalies using machine and deep learning approaches.
First, we analyzed the dataset using three visualization techniques:
t-Distributed Stochastic Neighbor Embedding (t-SNE), Uniform Manifold
Approximation and Projection (UMAP), and Principal Component Analysis (PCA).
Second, we reduced the data dimensionality using mutual information and PCA
techniques. Third, we solve the class imbalance issue by inserting synthetic
records of minority classes. Last, we performed classification using six
different classifiers and presented the evaluation metrics. We received the
best results when K-Nearest Neighbors classifier was used: accuracy (97.2%),
detection rate (96.7%), and false positive rate (2.2%).
</p></li>
</ul>

<h3>Title: Membership Inference Attacks on Diffusion Models via Quantile Regression. (arXiv:2312.05140v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05140">http://arxiv.org/abs/2312.05140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05140]] Membership Inference Attacks on Diffusion Models via Quantile Regression(http://arxiv.org/abs/2312.05140)</code></li>
<li>Summary: <p>Recently, diffusion models have become popular tools for image synthesis
because of their high-quality outputs. However, like other large-scale models,
they may leak private information about their training data. Here, we
demonstrate a privacy vulnerability of diffusion models through a
\emph{membership inference (MI) attack}, which aims to identify whether a
target example belongs to the training set when given the trained diffusion
model. Our proposed MI attack learns quantile regression models that predict (a
quantile of) the distribution of reconstruction loss on examples not used in
training. This allows us to define a granular hypothesis test for determining
the membership of a point in the training set, based on thresholding the
reconstruction loss of that point using a custom threshold tailored to the
example. We also provide a simple bootstrap technique that takes a majority
membership prediction over ``a bag of weak attackers'' which improves the
accuracy over individual quantile regression models. We show that our attack
outperforms the prior state-of-the-art attack while being substantially less
computationally expensive -- prior attacks required training multiple ``shadow
models'' with the same architecture as the model under attack, whereas our
attack requires training only much smaller models.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles. (arXiv:2312.04822v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04822">http://arxiv.org/abs/2312.04822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04822]] SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles(http://arxiv.org/abs/2312.04822)</code></li>
<li>Summary: <p>Cooperative perception for connected and automated vehicles is traditionally
achieved through the fusion of feature maps from two or more vehicles. However,
the absence of feature maps shared from other vehicles can lead to a
significant decline in object detection performance for cooperative perception
models compared to standalone 3D detection models. This drawback impedes the
adoption of cooperative perception as vehicle resources are often insufficient
to concurrently employ two perception models. To tackle this issue, we present
Simultaneous Individual and Cooperative Perception (SiCP), a generic framework
that supports a wide range of the state-of-the-art standalone perception
backbones and enhances them with a novel Dual-Perception Network (DP-Net)
designed to facilitate both individual and cooperative perception. In addition
to its lightweight nature with only 0.13M parameters, DP-Net is robust and
retains crucial gradient information during feature map fusion. As demonstrated
in a comprehensive evaluation on the OPV2V dataset, thanks to DP-Net, SiCP
surpasses state-of-the-art cooperative perception solutions while preserving
the performance of standalone perception solutions.
</p></li>
</ul>

<h3>Title: Towards Stable and Faithful Inpainting. (arXiv:2312.04831v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04831">http://arxiv.org/abs/2312.04831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04831]] Towards Stable and Faithful Inpainting(http://arxiv.org/abs/2312.04831)</code></li>
<li>Summary: <p>Recent progress in inpainting increasingly relies on generative models,
leveraging their strong generation capabilities for addressing ill-conditioned
problems. However, this enhanced generation often introduces instability,
leading to arbitrary object generation within masked regions. This paper
proposes a balanced solution, emphasizing the importance of unmasked regions in
guiding inpainting while preserving generative capacity. Our approach, Aligned
Stable Inpainting with UnKnown Areas Prior (ASUKA), employs a
reconstruction-based masked auto-encoder (MAE) as a stable prior. Aligned with
the robust Stable Diffusion inpainting model (SD), ASUKA significantly improves
inpainting stability. ASUKA further aligns masked and unmasked regions through
an inpainting-specialized decoder, ensuring more faithful inpainting. To
validate effectiveness across domains and masking scenarios, we evaluate on
MISATO, a collection of several existing dataset. Results confirm ASUKA's
efficacy in both stability and fidelity compared to SD and other inpainting
algorithms.
</p></li>
</ul>

<h3>Title: Radar Perception in Autonomous Driving: Exploring Different Data Representations. (arXiv:2312.04861v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04861">http://arxiv.org/abs/2312.04861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04861]] Radar Perception in Autonomous Driving: Exploring Different Data Representations(http://arxiv.org/abs/2312.04861)</code></li>
<li>Summary: <p>With the rapid advancements of sensor technology and deep learning,
autonomous driving systems are providing safe and efficient access to
intelligent vehicles as well as intelligent transportation. Among these
equipped sensors, the radar sensor plays a crucial role in providing robust
perception information in diverse environmental conditions. This review focuses
on exploring different radar data representations utilized in autonomous
driving systems. Firstly, we introduce the capabilities and limitations of the
radar sensor by examining the working principles of radar perception and signal
processing of radar measurements. Then, we delve into the generation process of
five radar representations, including the ADC signal, radar tensor, point
cloud, grid map, and micro-Doppler signature. For each radar representation, we
examine the related datasets, methods, advantages and limitations. Furthermore,
we discuss the challenges faced in these data representations and propose
potential research directions. Above all, this comprehensive review offers an
in-depth insight into how these representations enhance autonomous system
capabilities, providing guidance for radar perception researchers. To
facilitate retrieval and comparison of different data representations, datasets
and methods, we provide an interactive website at
https://radar-camera-fusion.github.io/radar.
</p></li>
</ul>

<h3>Title: Interpretable Underwater Diver Gesture Recognition. (arXiv:2312.04874v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04874">http://arxiv.org/abs/2312.04874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04874]] Interpretable Underwater Diver Gesture Recognition(http://arxiv.org/abs/2312.04874)</code></li>
<li>Summary: <p>In recent years, usage and applications of Autonomous Underwater Vehicles has
grown rapidly. Interaction of divers with the AUVs remains an integral part of
the usage of AUVs for various applications and makes building robust and
efficient underwater gesture recognition systems extremely important. In this
paper, we propose an Underwater Gesture Recognition system trained on the
Cognitive Autonomous Diving Buddy Underwater gesture dataset using deep
learning that achieves 98.01\% accuracy on the dataset, which to the best of
our knowledge is the best performance achieved on this dataset at the time of
writing this paper. We also improve the Gesture Recognition System
Interpretability by using XAI techniques to visualize the model's predictions.
</p></li>
</ul>

<h3>Title: Annotation-Free Group Robustness via Loss-Based Resampling. (arXiv:2312.04893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04893">http://arxiv.org/abs/2312.04893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04893]] Annotation-Free Group Robustness via Loss-Based Resampling(http://arxiv.org/abs/2312.04893)</code></li>
<li>Summary: <p>It is well-known that training neural networks for image classification with
empirical risk minimization (ERM) makes them vulnerable to relying on spurious
attributes instead of causal ones for prediction. Previously, deep feature
re-weighting (DFR) has proposed retraining the last layer of a pre-trained
network on balanced data concerning spurious attributes, making it robust to
spurious correlation. However, spurious attribute annotations are not always
available. In order to provide group robustness without such annotations, we
propose a new method, called loss-based feature re-weighting (LFR), in which we
infer a grouping of the data by evaluating an ERM-pre-trained model on a small
left-out split of the training data. Then, a balanced number of samples is
chosen by selecting high-loss samples from misclassified data points and
low-loss samples from correctly-classified ones. Finally, we retrain the last
layer on the selected balanced groups to make the model robust to spurious
correlation. For a complete assessment, we evaluate LFR on various versions of
Waterbirds and CelebA datasets with different spurious correlations, which is a
novel technique for observing the model's performance in a wide range of
spuriosity rates. While LFR is extremely fast and straightforward, it
outperforms the previous methods that do not assume group label availability,
as well as the DFR with group annotations provided, in cases of high spurious
correlation in the training data.
</p></li>
</ul>

<h3>Title: Accelerating Convolutional Neural Network Pruning via Spatial Aura Entropy. (arXiv:2312.04926v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04926">http://arxiv.org/abs/2312.04926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04926]] Accelerating Convolutional Neural Network Pruning via Spatial Aura Entropy(http://arxiv.org/abs/2312.04926)</code></li>
<li>Summary: <p>In recent years, pruning has emerged as a popular technique to reduce the
computational complexity and memory footprint of Convolutional Neural Network
(CNN) models. Mutual Information (MI) has been widely used as a criterion for
identifying unimportant filters to prune. However, existing methods for MI
computation suffer from high computational cost and sensitivity to noise,
leading to suboptimal pruning performance. We propose a novel method to improve
MI computation for CNN pruning, using the spatial aura entropy. The spatial
aura entropy is useful for evaluating the heterogeneity in the distribution of
the neural activations over a neighborhood, providing information about local
features. Our method effectively improves the MI computation for CNN pruning,
leading to more robust and efficient pruning. Experimental results on the
CIFAR-10 benchmark dataset demonstrate the superiority of our approach in terms
of pruning performance and computational efficiency.
</p></li>
</ul>

<h3>Title: Scientific Preparation for CSST: Classification of Galaxy and Nebula/Star Cluster Based on Deep Learning. (arXiv:2312.04948v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04948">http://arxiv.org/abs/2312.04948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04948]] Scientific Preparation for CSST: Classification of Galaxy and Nebula/Star Cluster Based on Deep Learning(http://arxiv.org/abs/2312.04948)</code></li>
<li>Summary: <p>The Chinese Space Station Telescope (abbreviated as CSST) is a future
advanced space telescope. Real-time identification of galaxy and nebula/star
cluster (abbreviated as NSC) images is of great value during CSST survey. While
recent research on celestial object recognition has progressed, the rapid and
efficient identification of high-resolution local celestial images remains
challenging. In this study, we conducted galaxy and NSC image classification
research using deep learning methods based on data from the Hubble Space
Telescope. We built a Local Celestial Image Dataset and designed a deep
learning model named HR-CelestialNet for classifying images of the galaxy and
NSC. HR-CelestialNet achieved an accuracy of 89.09% on the testing set,
outperforming models such as AlexNet, VGGNet and ResNet, while demonstrating
faster recognition speeds. Furthermore, we investigated the factors influencing
CSST image quality and evaluated the generalization ability of HR-CelestialNet
on the blurry image dataset, demonstrating its robustness to low image quality.
The proposed method can enable real-time identification of celestial images
during CSST survey mission.
</p></li>
</ul>

<h3>Title: MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04960">http://arxiv.org/abs/2312.04960</a></li>
<li>Code URL: https://github.com/xiaoyunxxy/mimir</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04960]] MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness(http://arxiv.org/abs/2312.04960)</code></li>
<li>Summary: <p>Vision Transformers (ViTs) achieve superior performance on various tasks
compared to convolutional neural networks (CNNs), but ViTs are also vulnerable
to adversarial attacks. Adversarial training is one of the most successful
methods to build robust CNN models. Thus, recent works explored new
methodologies for adversarial training of ViTs based on the differences between
ViTs and CNNs, such as better training strategies, preventing attention from
focusing on a single block, or discarding low-attention embeddings. However,
these methods still follow the design of traditional supervised adversarial
training, limiting the potential of adversarial training on ViTs. This paper
proposes a novel defense method, MIMIR, which aims to build a different
adversarial training methodology by utilizing Masked Image Modeling at
pre-training. We create an autoencoder that accepts adversarial examples as
input but takes the clean examples as the modeling target. Then, we create a
mutual information (MI) penalty following the idea of the Information
Bottleneck. Among the two information source inputs and corresponding
adversarial perturbation, the perturbation information is eliminated due to the
constraint of the modeling target. Next, we provide a theoretical analysis of
MIMIR using the bounds of the MI penalty. We also design two adaptive attacks
when the adversary is aware of the MIMIR defense and show that MIMIR still
performs well. The experimental results show that MIMIR improves (natural and
adversarial) accuracy on average by 4.19\% on CIFAR-10 and 5.52\% on
ImageNet-1K, compared to baselines. On Tiny-ImageNet, we obtained improved
natural accuracy of 2.99\% on average and comparable adversarial accuracy. Our
code and trained models are publicly
available\footnote{\url{https://anonymous.4open.science/r/MIMIR-5444/README.md}}.
</p></li>
</ul>

<h3>Title: From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis. (arXiv:2312.04720v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04720">http://arxiv.org/abs/2312.04720</a></li>
<li>Code URL: https://github.com/clarin-pl/text-augumentation-with-chatgpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04720]] From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis(http://arxiv.org/abs/2312.04720)</code></li>
<li>Summary: <p>In the era of artificial intelligence, data is gold but costly to annotate.
The paper demonstrates a groundbreaking solution to this dilemma using ChatGPT
for text augmentation in sentiment analysis. We leverage ChatGPT's generative
capabilities to create synthetic training data that significantly improves the
performance of smaller models, making them competitive with, or even
outperforming, their larger counterparts. This innovation enables models to be
both efficient and effective, thereby reducing computational cost, inference
time, and memory usage without compromising on quality. Our work marks a key
advancement in the cost-effective development and deployment of robust
sentiment analysis models.
</p></li>
</ul>

<h3>Title: The ICL Consistency Test. (arXiv:2312.04945v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04945">http://arxiv.org/abs/2312.04945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04945]] The ICL Consistency Test(http://arxiv.org/abs/2312.04945)</code></li>
<li>Summary: <p>Just like the previous generation of task-tuned models, large language models
(LLMs) that are adapted to tasks via prompt-based methods like
in-context-learning (ICL) perform well in some setups but not in others. This
lack of consistency in prompt-based learning hints at a lack of robust
generalisation. We here introduce the ICL consistency test -- a contribution to
the GenBench collaborative benchmark task (CBT) -- which evaluates how
consistent a model makes predictions across many different setups while using
the same data. The test is based on different established natural language
inference tasks. We provide preprocessed data constituting 96 different
'setups' and a metric that estimates model consistency across these setups. The
metric is provided on a fine-grained level to understand what properties of a
setup render predictions unstable and on an aggregated level to compare overall
model consistency. We conduct an empirical analysis of eight state-of-the-art
models, and our consistency metric reveals how all tested LLMs lack robust
generalisation.
</p></li>
</ul>

<h3>Title: The Graph Lottery Ticket Hypothesis: Finding Sparse, Informative Graph Structure. (arXiv:2312.04762v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04762">http://arxiv.org/abs/2312.04762</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04762]] The Graph Lottery Ticket Hypothesis: Finding Sparse, Informative Graph Structure(http://arxiv.org/abs/2312.04762)</code></li>
<li>Summary: <p>Graph learning methods help utilize implicit relationships among data items,
thereby reducing training label requirements and improving task performance.
However, determining the optimal graph structure for a particular learning task
remains a challenging research problem.
</p>
<p>In this work, we introduce the Graph Lottery Ticket (GLT) Hypothesis - that
there is an extremely sparse backbone for every graph, and that graph learning
algorithms attain comparable performance when trained on that subgraph as on
the full graph. We identify and systematically study 8 key metrics of interest
that directly influence the performance of graph learning algorithms.
Subsequently, we define the notion of a "winning ticket" for graph structure -
an extremely sparse subset of edges that can deliver a robust approximation of
the entire graph's performance. We propose a straightforward and efficient
algorithm for finding these GLTs in arbitrary graphs. Empirically, we observe
that performance of different graph learning algorithms can be matched or even
exceeded on graphs with the average degree as low as 5.
</p></li>
</ul>

<h3>Title: StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning. (arXiv:2312.04865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04865">http://arxiv.org/abs/2312.04865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04865]] StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning(http://arxiv.org/abs/2312.04865)</code></li>
<li>Summary: <p>Graph contrastive learning (GCL) has become a powerful tool for learning
graph data, but its scalability remains a significant challenge. In this work,
we propose a simple yet effective training framework called Structural
Compression (StructComp) to address this issue. Inspired by a sparse low-rank
approximation on the diffusion matrix, StructComp trains the encoder with the
compressed nodes. This allows the encoder not to perform any message passing
during the training stage, and significantly reduces the number of sample pairs
in the contrastive loss. We theoretically prove that the original GCL loss can
be approximated with the contrastive loss computed by StructComp. Moreover,
StructComp can be regarded as an additional regularization term for GCL models,
resulting in a more robust encoder. Empirical studies on seven benchmark
datasets show that StructComp greatly reduces the time and memory consumption
while improving model performance compared to the vanilla GCL models and
scalable training methods.
</p></li>
</ul>

<h3>Title: HC-Ref: Hierarchical Constrained Refinement for Robust Adversarial Training of GNNs. (arXiv:2312.04879v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04879">http://arxiv.org/abs/2312.04879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04879]] HC-Ref: Hierarchical Constrained Refinement for Robust Adversarial Training of GNNs(http://arxiv.org/abs/2312.04879)</code></li>
<li>Summary: <p>Recent studies have shown that attackers can catastrophically reduce the
performance of GNNs by maliciously modifying the graph structure or node
features on the graph. Adversarial training, which has been shown to be one of
the most effective defense mechanisms against adversarial attacks in computer
vision, holds great promise for enhancing the robustness of GNNs. There is
limited research on defending against attacks by performing adversarial
training on graphs, and it is crucial to delve deeper into this approach to
optimize its effectiveness. Therefore, based on robust adversarial training on
graphs, we propose a hierarchical constraint refinement framework (HC-Ref) that
enhances the anti-perturbation capabilities of GNNs and downstream classifiers
separately, ultimately leading to improved robustness. We propose corresponding
adversarial regularization terms that are conducive to adaptively narrowing the
domain gap between the normal part and the perturbation part according to the
characteristics of different layers, promoting the smoothness of the predicted
distribution of both parts. Moreover, existing research on graph robust
adversarial training primarily concentrates on training from the standpoint of
node feature perturbations and seldom takes into account alterations in the
graph structure. This limitation makes it challenging to prevent attacks based
on topological changes in the graph. This paper generates adversarial examples
by utilizing graph structure perturbations, offering an effective approach to
defend against attack methods that are based on topological changes. Extensive
experiments on two real-world graph benchmarks show that HC-Ref successfully
resists various attacks and has better node classification performance compared
to several baseline methods.
</p></li>
</ul>

<h3>Title: Optimal Multi-Distribution Learning. (arXiv:2312.05134v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05134">http://arxiv.org/abs/2312.05134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05134]] Optimal Multi-Distribution Learning(http://arxiv.org/abs/2312.05134)</code></li>
<li>Summary: <p>Multi-distribution learning (MDL), which seeks to learn a shared model that
minimizes the worst-case risk across $k$ distinct data distributions, has
emerged as a unified framework in response to the evolving demand for
robustness, fairness, multi-group collaboration, etc. Achieving data-efficient
MDL necessitates adaptive sampling, also called on-demand sampling, throughout
the learning process. However, there exist substantial gaps between the
state-of-the-art upper and lower bounds on the optimal sample complexity.
Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we
propose a novel algorithm that yields an $varepsilon$-optimal randomized
hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$
(modulo some logarithmic factor), matching the best-known lower bound. Our
algorithmic ideas and theory have been further extended to accommodate
Rademacher classes. The proposed algorithms are oracle-efficient, which access
the hypothesis class solely through an empirical risk minimization oracle.
Additionally, we establish the necessity of randomization, unveiling a large
sample size barrier when only deterministic hypotheses are permitted. These
findings successfully resolve three open problems presented in COLT 2023 (i.e.,
Awasthi et al., (2023, Problem 1, 3 and 4)).
</p></li>
</ul>

<h3>Title: Conformal Prediction in Multi-User Settings: An Evaluation. (arXiv:2312.05195v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05195">http://arxiv.org/abs/2312.05195</a></li>
<li>Code URL: https://github.com/enriquegit/conformal-prediction-multiuser</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05195]] Conformal Prediction in Multi-User Settings: An Evaluation(http://arxiv.org/abs/2312.05195)</code></li>
<li>Summary: <p>Typically, machine learning models are trained and evaluated without making
any distinction between users (e.g, using traditional hold-out and
cross-validation). However, this produces inaccurate performance metrics
estimates in multi-user settings. That is, situations where the data were
collected by multiple users with different characteristics (e.g., age, gender,
height, etc.) which is very common in user computer interaction and medical
applications. For these types of scenarios model evaluation strategies that
provide better performance estimates have been proposed such as mixed,
user-independent, user-dependent, and user-adaptive models. Although those
strategies are better suited for multi-user systems, they are typically
assessed with respect to performance metrics that capture the overall behavior
of the models and do not provide any performance guarantees for individual
predictions nor they provide any feedback about the predictions' uncertainty.
In order to overcome those limitations, in this work we evaluated the conformal
prediction framework in several multi-user settings. Conformal prediction is a
model agnostic method that provides confidence guarantees on the predictions,
thus, increasing the trustworthiness and robustness of the models. We conducted
extensive experiments using different evaluation strategies and found
significant differences in terms of conformal performance measures. We also
proposed several visualizations based on matrices, graphs, and charts that
capture different aspects of the resulting prediction sets.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Fine-Grained Extraction of Road Networks via Joint Learning of Connectivity and Segmentation. (arXiv:2312.04744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04744">http://arxiv.org/abs/2312.04744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04744]] Fine-Grained Extraction of Road Networks via Joint Learning of Connectivity and Segmentation(http://arxiv.org/abs/2312.04744)</code></li>
<li>Summary: <p>Road network extraction from satellite images is widely applicated in
intelligent traffic management and autonomous driving fields. The
high-resolution remote sensing images contain complex road areas and distracted
background, which make it a challenge for road extraction. In this study, we
present a stacked multitask network for end-to-end segmenting roads while
preserving connectivity correctness. In the network, a global-aware module is
introduced to enhance pixel-level road feature representation and eliminate
background distraction from overhead images; a road-direction-related
connectivity task is added to ensure that the network preserves the graph-level
relationships of the road segments. We also develop a stacked multihead
structure to jointly learn and effectively utilize the mutual information
between connectivity learning and segmentation learning. We evaluate the
performance of the proposed network on three public remote sensing datasets.
The experimental results demonstrate that the network outperforms the
state-of-the-art methods in terms of road segmentation accuracy and
connectivity maintenance.
</p></li>
</ul>

<h3>Title: Unify Change Point Detection and Segment Classification in a Regression Task for Transportation Mode Identification. (arXiv:2312.04821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04821">http://arxiv.org/abs/2312.04821</a></li>
<li>Code URL: https://github.com/radetzkyli/trajyolo-ssd</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04821]] Unify Change Point Detection and Segment Classification in a Regression Task for Transportation Mode Identification(http://arxiv.org/abs/2312.04821)</code></li>
<li>Summary: <p>Identifying travelers' transportation modes is important in transportation
science and location-based services. It's appealing for researchers to leverage
GPS trajectory data to infer transportation modes with the popularity of
GPS-enabled devices, e.g., smart phones. Existing studies frame this problem as
classification task. The dominant two-stage studies divide the trip into
single-one mode segments first and then categorize these segments. The over
segmentation strategy and inevitable error propagation bring difficulties to
classification stage and make optimizing the whole system hard. The recent
one-stage works throw out trajectory segmentation entirely to avoid these by
directly conducting point-wise classification for the trip, whereas leaving
predictions dis-continuous. To solve above-mentioned problems, inspired by YOLO
and SSD in object detection, we propose to reframe change point detection and
segment classification as a unified regression task instead of the existing
classification task. We directly regress coordinates of change points and
classify associated segments. In this way, our method divides the trip into
segments under a supervised manner and leverage more contextual information,
obtaining predictions with high accuracy and continuity. Two frameworks,
TrajYOLO and TrajSSD, are proposed to solve the regression task and various
feature extraction backbones are exploited. Exhaustive experiments on GeoLife
dataset show that the proposed method has competitive overall identification
accuracy of 0.853 when distinguishing five modes: walk, bike, bus, car, train.
As for change point detection, our method increases precision at the cost of
drop in recall. All codes are available at
https://github.com/RadetzkyLi/TrajYOLO-SSD.
</p></li>
</ul>

<h3>Title: Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs. (arXiv:2312.04782v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04782">http://arxiv.org/abs/2312.04782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04782]] Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs(http://arxiv.org/abs/2312.04782)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are now widely used in various applications,
making it crucial to align their ethical standards with human values. However,
recent jail-breaking methods demonstrate that this alignment can be undermined
using carefully constructed prompts. In our study, we reveal a new threat to
LLM alignment when a bad actor has access to the model's output logits, a
common feature in both open-source LLMs and many commercial LLM APIs (e.g.,
certain GPT models). It does not rely on crafting specific prompts. Instead, it
exploits the fact that even when an LLM rejects a toxic request, a harmful
response often hides deep in the output logits. By forcefully selecting
lower-ranked output tokens during the auto-regressive generation process at a
few critical output positions, we can compel the model to reveal these hidden
responses. We term this process model interrogation. This approach differs from
and outperforms jail-breaking methods, achieving 92% effectiveness compared to
62%, and is 10 to 20 times faster. The harmful content uncovered through our
method is more relevant, complete, and clear. Additionally, it can complement
jail-breaking strategies, with which results in further boosting attack
performance. Our findings indicate that interrogation can extract toxic
knowledge even from models specifically designed for coding tasks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning for 6G: Paradigms, Taxonomy, Recent Advances and Insights. (arXiv:2312.04688v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04688">http://arxiv.org/abs/2312.04688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04688]] Federated Learning for 6G: Paradigms, Taxonomy, Recent Advances and Insights(http://arxiv.org/abs/2312.04688)</code></li>
<li>Summary: <p>Artificial Intelligence (AI) is expected to play an instrumental role in the
next generation of wireless systems, such as sixth-generation (6G) mobile
network. However, massive data, energy consumption, training complexity, and
sensitive data protection in wireless systems are all crucial challenges that
must be addressed for training AI models and gathering intelligence and
knowledge from distributed devices. Federated Learning (FL) is a recent
framework that has emerged as a promising approach for multiple learning agents
to build an accurate and robust machine learning models without sharing raw
data. By allowing mobile handsets and devices to collaboratively learn a global
model without explicit sharing of training data, FL exhibits high privacy and
efficient spectrum utilization. While there are a lot of survey papers
exploring FL paradigms and usability in 6G privacy, none of them has clearly
addressed how FL can be used to improve the protocol stack and wireless
operations. The main goal of this survey is to provide a comprehensive overview
on FL usability to enhance mobile services and enable smart ecosystems to
support novel use-cases. This paper examines the added-value of implementing FL
throughout all levels of the protocol stack. Furthermore, it presents important
FL applications, addresses hot topics, provides valuable insights and explicits
guidance for future research and developments. Our concluding remarks aim to
leverage the synergy between FL and future 6G, while highlighting FL's
potential to revolutionize wireless industry and sustain the development of
cutting-edge mobile services.
</p></li>
</ul>

<h3>Title: PFLlib: Personalized Federated Learning Algorithm Library. (arXiv:2312.04992v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04992">http://arxiv.org/abs/2312.04992</a></li>
<li>Code URL: https://github.com/TsingZ0/PFLlib</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04992]] PFLlib: Personalized Federated Learning Algorithm Library(http://arxiv.org/abs/2312.04992)</code></li>
<li>Summary: <p>Amid the ongoing advancements in Federated Learning (FL), a machine learning
paradigm that allows collaborative learning with data privacy protection,
personalized FL (pFL) has gained significant prominence as a research direction
within the FL domain. Whereas traditional FL (tFL) focuses on jointly learning
a global model, pFL aims to achieve a balance between the global and
personalized objectives of each client in FL settings. To foster the pFL
research community, we propose PFLlib, a comprehensive pFL algorithm library
with an integrated evaluation platform. In PFLlib, We implement 34
state-of-the-art FL algorithms (including 7 classic tFL algorithms and 27 pFL
algorithms) and provide various evaluation environments with three
statistically heterogeneous scenarios and 14 datasets. At present, PFLlib has
already gained 850 stars and 199 forks on GitHub.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: GraphMETRO: Mitigating Complex Distribution Shifts in GNNs via Mixture of Aligned Experts. (arXiv:2312.04693v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04693">http://arxiv.org/abs/2312.04693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04693]] GraphMETRO: Mitigating Complex Distribution Shifts in GNNs via Mixture of Aligned Experts(http://arxiv.org/abs/2312.04693)</code></li>
<li>Summary: <p>Graph Neural Networks' (GNNs) ability to generalize across complex
distributions is crucial for real-world applications. However, prior research
has primarily focused on specific types of distribution shifts, such as larger
graph size, or inferred shifts from constructed data environments, which is
highly limited when confronted with multiple and nuanced distribution shifts.
For instance, in a social graph, a user node might experience increased
interactions and content alterations, while other user nodes encounter distinct
shifts. Neglecting such complexities significantly impedes generalization. To
address it, we present GraphMETRO, a novel framework that enhances GNN
generalization under complex distribution shifts in both node and graph-level
tasks. Our approach employs a mixture-of-experts (MoE) architecture with a
gating model and expert models aligned in a shared representation space. The
gating model identifies key mixture components governing distribution shifts,
while each expert generates invariant representations w.r.t. a mixture
component. Finally, GraphMETRO aggregates representations from multiple experts
to generate the final invariant representation. Our experiments on synthetic
and realworld datasets demonstrate GraphMETRO's superiority and
interpretability. To highlight, GraphMETRO achieves state-of-the-art
performances on four real-world datasets from GOOD benchmark, outperforming the
best baselines on WebKB and Twitch datasets by 67% and 4.2%, respectively.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: Seamless: Multilingual Expressive and Streaming Speech Translation. (arXiv:2312.05187v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05187">http://arxiv.org/abs/2312.05187</a></li>
<li>Code URL: https://github.com/facebookresearch/seamless_communication</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05187]] Seamless: Multilingual Expressive and Streaming Speech Translation(http://arxiv.org/abs/2312.05187)</code></li>
<li>Summary: <p>Large-scale automatic speech translation systems today lack key features that
help machine-mediated communication feel seamless when compared to
human-to-human dialogue. In this work, we introduce a family of models that
enable end-to-end expressive and multilingual translations in a streaming
fashion. First, we contribute an improved version of the massively multilingual
and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model,
incorporating an updated UnitY2 framework, was trained on more low-resource
language data. SeamlessM4T v2 provides the foundation on which our next two
models are initiated. SeamlessExpressive enables translation that preserves
vocal styles and prosody. Compared to previous efforts in expressive speech
research, our work addresses certain underexplored aspects of prosody, such as
speech rate and pauses, while also preserving the style of one's voice. As for
SeamlessStreaming, our model leverages the Efficient Monotonic Multihead
Attention mechanism to generate low-latency target translations without waiting
for complete source utterances. As the first of its kind, SeamlessStreaming
enables simultaneous speech-to-speech/text translation for multiple source and
target languages. To ensure that our models can be used safely and responsibly,
we implemented the first known red-teaming effort for multimodal machine
translation, a system for the detection and mitigation of added toxicity, a
systematic evaluation of gender bias, and an inaudible localized watermarking
mechanism designed to dampen the impact of deepfakes. Consequently, we bring
major components from SeamlessExpressive and SeamlessStreaming together to form
Seamless, the first publicly available system that unlocks expressive
cross-lingual communication in real-time. The contributions to this work are
publicly released and accessible at
https://github.com/facebookresearch/seamless_communication
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion. (arXiv:2312.04654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04654">http://arxiv.org/abs/2312.04654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04654]] NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion(http://arxiv.org/abs/2312.04654)</code></li>
<li>Summary: <p>We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.
</p></li>
</ul>

<h3>Title: ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations. (arXiv:2312.04655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04655">http://arxiv.org/abs/2312.04655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04655]] ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations(http://arxiv.org/abs/2312.04655)</code></li>
<li>Summary: <p>Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g.,
DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional
T2I benchmarks, at the cost of significant computational resources. The unCLIP
stack comprises T2I prior and diffusion image decoder. The T2I prior model
alone adds a billion parameters compared to the Latent Diffusion Models, which
increases the computational and high-quality data requirements. We introduce
ECLIPSE, a novel contrastive learning method that is both parameter and
data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g.,
CLIP) to distill the knowledge into the prior model. We demonstrate that the
ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere
2.8% of the data, surpasses the baseline T2I priors with an average of 71.6%
preference score under resource-limited setting. It also attains performance on
par with SOTA big models, achieving an average of 63.36% preference score in
terms of the ability to follow the text compositions. Extensive experiments on
two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE
priors consistently deliver high performance while significantly reducing
resource dependency.
</p></li>
</ul>

<h3>Title: Fine-Tuning InstructPix2Pix for Advanced Image Colorization. (arXiv:2312.04780v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04780">http://arxiv.org/abs/2312.04780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04780]] Fine-Tuning InstructPix2Pix for Advanced Image Colorization(http://arxiv.org/abs/2312.04780)</code></li>
<li>Summary: <p>This paper presents a novel approach to human image colorization by
fine-tuning the InstructPix2Pix model, which integrates a language model
(GPT-3) with a text-to-image model (Stable Diffusion). Despite the original
InstructPix2Pix model's proficiency in editing images based on textual
instructions, it exhibits limitations in the focused domain of colorization. To
address this, we fine-tuned the model using the IMDB-WIKI dataset, pairing
black-and-white images with a diverse set of colorization prompts generated by
ChatGPT. This paper contributes by (1) applying fine-tuning techniques to
stable diffusion models specifically for colorization tasks, and (2) employing
generative models to create varied conditioning prompts. After finetuning, our
model outperforms the original InstructPix2Pix model on multiple metrics
quantitatively, and we produce more realistically colored images qualitatively.
The code for this project is provided on the GitHub Repository
https://github.com/AllenAnZifeng/DeepLearning282.
</p></li>
</ul>

<h3>Title: Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video. (arXiv:2312.04784v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04784">http://arxiv.org/abs/2312.04784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04784]] Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video(http://arxiv.org/abs/2312.04784)</code></li>
<li>Summary: <p>Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLab to close this gap. ReCaLab is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable NeRF is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate viewpoint-agnostic
textures using 2D-3D correspondences. This enables to separately generate
albedo and shading which jointly compose an RGB prediction. The design allows
to control intermediate results for human pose, body shape, texture, and
lighting with text prompts. An image-conditioned diffusion model thereby helps
to animate appearance and pose of the 3D avatar to create video sequences with
previously unseen human motion. Extensive experiments show that ReCaLab
outperforms previous monocular approaches in terms of image quality for image
synthesis tasks. ReCaLab even outperforms multi-view methods that leverage up
to 19x more synchronized videos for the task of novel pose rendering. Moreover,
natural language offers an intuitive user interface for creative manipulation
of 3D human avatars.
</p></li>
</ul>

<h3>Title: MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model. (arXiv:2312.04802v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04802">http://arxiv.org/abs/2312.04802</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04802]] MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model(http://arxiv.org/abs/2312.04802)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where
an imperceptible perturbation is added to the image that can fool the DNNs.
Diffusion-based adversarial purification focuses on using the diffusion model
to generate a clean image against such adversarial attacks. Unfortunately, the
generative process of the diffusion model is also inevitably affected by
adversarial perturbation since the diffusion model is also a deep network where
its input has adversarial perturbation. In this work, we propose
MimicDiffusion, a new diffusion-based adversarial purification technique, that
directly approximates the generative process of the diffusion model with the
clean image as input. Concretely, we analyze the differences between the guided
terms using the clean image and the adversarial sample. After that, we first
implement MimicDiffusion based on Manhattan distance. Then, we propose two
guidance to purify the adversarial perturbation and approximate the clean
diffusion model. Extensive experiments on three image datasets including
CIFAR-10, CIFAR-100, and ImageNet with three classifier backbones including
WideResNet-70-16, WideResNet-28-10, and ResNet50 demonstrate that
MimicDiffusion significantly performs better than the state-of-the-art
baselines. On CIFAR-10, CIFAR-100, and ImageNet, it achieves 92.67\%, 61.35\%,
and 61.53\% average robust accuracy, which are 18.49\%, 13.23\%, and 17.64\%
higher, respectively. The code is available in the supplementary material.
</p></li>
</ul>

<h3>Title: RL Dreams: Policy Gradient Optimization for Score Distillation based 3D Generation. (arXiv:2312.04806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04806">http://arxiv.org/abs/2312.04806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04806]] RL Dreams: Policy Gradient Optimization for Score Distillation based 3D Generation(http://arxiv.org/abs/2312.04806)</code></li>
<li>Summary: <p>3D generation has rapidly accelerated in the past decade owing to the
progress in the field of generative modeling. Score Distillation Sampling (SDS)
based rendering has improved 3D asset generation to a great extent. Further,
the recent work of Denoising Diffusion Policy Optimization (DDPO) demonstrates
that the diffusion process is compatible with policy gradient methods and has
been demonstrated to improve the 2D diffusion models using an aesthetic scoring
function. We first show that this aesthetic scorer acts as a strong guide for a
variety of SDS-based methods and demonstrates its effectiveness in text-to-3D
synthesis. Further, we leverage the DDPO approach to improve the quality of the
3D rendering obtained from 2D diffusion models. Our approach, DDPO3D, employs
the policy gradient method in tandem with aesthetic scoring. To the best of our
knowledge, this is the first method that extends policy gradient methods to 3D
score-based rendering and shows improvement across SDS-based methods such as
DreamGaussian, which are currently driving research in text-to-3D synthesis.
Our approach is compatible with score distillation-based methods, which would
facilitate the integration of diverse reward functions into the generative
process. Our project page can be accessed via https://ddpo3d.github.io.
</p></li>
</ul>

<h3>Title: RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models. (arXiv:2312.04810v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04810">http://arxiv.org/abs/2312.04810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04810]] RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models(http://arxiv.org/abs/2312.04810)</code></li>
<li>Summary: <p>Recent text-conditioned image generation models have demonstrated an
exceptional capacity to produce diverse and creative imagery with high visual
quality. However, when pre-trained on billion-sized datasets randomly collected
from the Internet, where potential biased human preferences exist, these models
tend to produce images with common and recurring stereotypes, particularly for
certain racial groups. In this paper, we conduct an initial analysis of the
publicly available Stable Diffusion model and its derivatives, highlighting the
presence of racial stereotypes. These models often generate distorted or biased
images for certain racial groups, emphasizing stereotypical characteristics. To
address these issues, we propose a framework called "RS-Corrector", designed to
establish an anti-stereotypical preference in the latent space and update the
latent code for refined generated results. The correction process occurs during
the inference stage without requiring fine-tuning of the original model.
Extensive empirical evaluations demonstrate that the introduced \themodel
effectively corrects the racial stereotypes of the well-trained Stable
Diffusion model while leaving the original model unchanged.
</p></li>
</ul>

<h3>Title: Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting. (arXiv:2312.04820v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04820">http://arxiv.org/abs/2312.04820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04820]] Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting(http://arxiv.org/abs/2312.04820)</code></li>
<li>Summary: <p>We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.
</p></li>
</ul>

<h3>Title: Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy. (arXiv:2312.04823v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04823">http://arxiv.org/abs/2312.04823</a></li>
<li>Code URL: https://github.com/ChenLiu-1996/DiffusionSpectralEntropy</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04823]] Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy(http://arxiv.org/abs/2312.04823)</code></li>
<li>Summary: <p>Entropy and mutual information in neural networks provide rich information on
the learning process, but they have proven difficult to compute reliably in
high dimensions. Indeed, in noisy and high-dimensional data, traditional
estimates in ambient dimensions approach a fixed entropy and are prohibitively
hard to compute. To address these issues, we leverage data geometry to access
the underlying manifold and reliably compute these information-theoretic
measures. Specifically, we define diffusion spectral entropy (DSE) in neural
representations of a dataset as well as diffusion spectral mutual information
(DSMI) between different variables representing data. First, we show that they
form noise-resistant measures of intrinsic dimensionality and relationship
strength in high-dimensional simulated data that outperform classic Shannon
entropy, nonparametric estimation, and mutual information neural estimation
(MINE). We then study the evolution of representations in classification
networks with supervised learning, self-supervision, or overfitting. We observe
that (1) DSE of neural representations increases during training; (2) DSMI with
the class label increases during generalizable learning but stays stagnant
during overfitting; (3) DSMI with the input signal shows differing trends: on
MNIST it increases, while on CIFAR-10 and STL-10 it decreases. Finally, we show
that DSE can be used to guide better network initialization and that DSMI can
be used to predict downstream classification accuracy across 962 models on
ImageNet. The official implementation is available at
https://github.com/ChenLiu-1996/DiffusionSpectralEntropy.
</p></li>
</ul>

<h3>Title: HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models. (arXiv:2312.04867v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04867">http://arxiv.org/abs/2312.04867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04867]] HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models(http://arxiv.org/abs/2312.04867)</code></li>
<li>Summary: <p>Existing hands datasets are largely short-range and the interaction is weak
due to the self-occlusion and self-similarity of hands, which can not yet fit
the need for interacting hands motion generation. To rescue the data scarcity,
we propose HandDiffuse12.5M, a novel dataset that consists of temporal
sequences with strong two-hand interactions. HandDiffuse12.5M has the largest
scale and richest interactions among the existing two-hand datasets. We further
present a strong baseline method HandDiffuse for the controllable motion
generation of interacting hands using various controllers. Specifically, we
apply the diffusion model as the backbone and design two motion representations
for different controllers. To reduce artifacts, we also propose Interaction
Loss which explicitly quantifies the dynamic interaction process. Our
HandDiffuse enables various applications with vivid two-hand interactions,
i.e., motion in-betweening and trajectory control. Experiments show that our
method outperforms the state-of-the-art techniques in motion generation and can
also contribute to data augmentation for other datasets. Our dataset,
corresponding codes, and pre-trained models will be disseminated to the
community for future research towards two-hand interaction modeling.
</p></li>
</ul>

<h3>Title: MVDD: Multi-View Depth Diffusion Models. (arXiv:2312.04875v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04875">http://arxiv.org/abs/2312.04875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04875]] MVDD: Multi-View Depth Diffusion Models(http://arxiv.org/abs/2312.04875)</code></li>
<li>Summary: <p>Denoising diffusion models have demonstrated outstanding results in 2D image
generation, yet it remains a challenge to replicate its success in 3D shape
generation. In this paper, we propose leveraging multi-view depth, which
represents complex 3D shapes in a 2D data format that is easy to denoise. We
pair this representation with a diffusion model, MVDD, that is capable of
generating high-quality dense point clouds with 20K+ points with fine-grained
details. To enforce 3D consistency in multi-view depth, we introduce an
epipolar line segment attention that conditions the denoising step for a view
on its neighboring views. Additionally, a depth fusion module is incorporated
into diffusion steps to further ensure the alignment of depth maps. When
augmented with surface reconstruction, MVDD can also produce high-quality 3D
meshes. Furthermore, MVDD stands out in other tasks such as depth completion,
and can serve as a 3D prior, significantly boosting many downstream tasks, such
as GAN inversion. State-of-the-art results from extensive experiments
demonstrate MVDD's excellent ability in 3D shape generation, depth completion,
and its potential as a 3D prior for downstream tasks.
</p></li>
</ul>

<h3>Title: UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models. (arXiv:2312.04884v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04884">http://arxiv.org/abs/2312.04884</a></li>
<li>Code URL: https://github.com/zym-pku/udifftext</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04884]] UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models(http://arxiv.org/abs/2312.04884)</code></li>
<li>Summary: <p>Text-to-Image (T2I) generation methods based on diffusion model have garnered
significant attention in the last few years. Although these image synthesis
methods produce visually appealing results, they frequently exhibit spelling
errors when rendering text within the generated images. Such errors manifest as
missing, incorrect or extraneous characters, thereby severely constraining the
performance of text image generation based on diffusion models. To address the
aforementioned issue, this paper proposes a novel approach for text image
generation, utilizing a pre-trained diffusion model (i.e., Stable Diffusion
[27]). Our approach involves the design and training of a light-weight
character-level text encoder, which replaces the original CLIP encoder and
provides more robust text embeddings as conditional guidance. Then, we
fine-tune the diffusion model using a large-scale dataset, incorporating local
attention control under the supervision of character-level segmentation maps.
Finally, by employing an inference stage refinement process, we achieve a
notably high sequence accuracy when synthesizing text in arbitrarily given
images. Both qualitative and quantitative results demonstrate the superiority
of our method to the state of the art. Furthermore, we showcase several
potential applications of the proposed UDiffText, including text-centric image
synthesis, scene text editing, etc. Code and model will be available at
https://github.com/ZYM-PKU/UDiffText .
</p></li>
</ul>

<h3>Title: Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors. (arXiv:2312.04963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04963">http://arxiv.org/abs/2312.04963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04963]] Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors(http://arxiv.org/abs/2312.04963)</code></li>
<li>Summary: <p>Most 3D generation research focuses on up-projecting 2D foundation models
into the 3D space, either by minimizing 2D Score Distillation Sampling (SDS)
loss or fine-tuning on multi-view datasets. Without explicit 3D priors, these
methods often lead to geometric anomalies and multi-view inconsistency.
Recently, researchers have attempted to improve the genuineness of 3D objects
by directly training on 3D datasets, albeit at the cost of low-quality texture
generation due to the limited texture diversity in 3D datasets. To harness the
advantages of both approaches, we propose Bidirectional Diffusion(BiDiff), a
unified framework that incorporates both a 3D and a 2D diffusion process, to
preserve both 3D fidelity and 2D texture richness, respectively. Moreover, as a
simple combination may yield inconsistent generation results, we further bridge
them with novel bidirectional guidance. In addition, our method can be used as
an initialization of optimization-based models to further improve the quality
of 3D model and efficiency of optimization, reducing the generation process
from 3.4 hours to 20 minutes. Experimental results have shown that our model
achieves high-quality, diverse, and scalable 3D generation. Project website:
https://bidiff.github.io/.
</p></li>
</ul>

<h3>Title: Inversion-Free Image Editing with Natural Language. (arXiv:2312.04965v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04965">http://arxiv.org/abs/2312.04965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04965]] Inversion-Free Image Editing with Natural Language(http://arxiv.org/abs/2312.04965)</code></li>
<li>Summary: <p>Despite recent advances in inversion-based editing, text-guided image
manipulation remains challenging for diffusion models. The primary bottlenecks
include 1) the time-consuming nature of the inversion process; 2) the struggle
to balance consistency with accuracy; 3) the lack of compatibility with
efficient consistency sampling methods used in consistency models. To address
the above issues, we start by asking ourselves if the inversion process can be
eliminated for editing. We show that when the initial sample is known, a
special variance schedule reduces the denoising step to the same form as the
multi-step consistency sampling. We name this Denoising Diffusion Consistent
Model (DDCM), and note that it implies a virtual inversion strategy without
explicit inversion in sampling. We further unify the attention control
mechanisms in a tuning-free framework for text-guided editing. Combining them,
we present inversion-free editing (InfEdit), which allows for consistent and
faithful editing for both rigid and non-rigid semantic changes, catering to
intricate modifications without compromising on the image's integrity and
explicit inversion. Through extensive experiments, InfEdit shows strong
performance in various editing tasks and also maintains a seamless workflow
(less than 3 seconds on one single A40), demonstrating the potential for
real-time applications. Project Page: https://sled-group.github.io/InfEdit/
</p></li>
</ul>

<h3>Title: Customizing Motion in Text-to-Video Diffusion Models. (arXiv:2312.04966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04966">http://arxiv.org/abs/2312.04966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04966]] Customizing Motion in Text-to-Video Diffusion Models(http://arxiv.org/abs/2312.04966)</code></li>
<li>Summary: <p>We introduce an approach for augmenting text-to-video generation models with
customized motions, extending their capabilities beyond the motions depicted in
the original training data. By leveraging a few video samples demonstrating
specific movements as input, our method learns and generalizes the input motion
patterns for diverse, text-specified scenarios. Our contributions are
threefold. First, to achieve our results, we finetune an existing text-to-video
model to learn a novel mapping between the depicted motion in the input
examples to a new unique token. To avoid overfitting to the new custom motion,
we introduce an approach for regularization over videos. Second, by leveraging
the motion priors in a pretrained model, our method can produce novel videos
featuring multiple people doing the custom motion, and can invoke the motion in
combination with other motions. Furthermore, our approach extends to the
multimodal customization of motion and appearance of individualized subjects,
enabling the generation of videos featuring unique characters and distinct
motions. Third, to validate our method, we introduce an approach for
quantitatively evaluating the learned custom motion and perform a systematic
ablation study. We show that our method significantly outperforms prior
appearance-based customization approaches when extended to the motion
customization task.
</p></li>
</ul>

<h3>Title: SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control. (arXiv:2312.05039v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05039">http://arxiv.org/abs/2312.05039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05039]] SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control(http://arxiv.org/abs/2312.05039)</code></li>
<li>Summary: <p>The field of generative image inpainting and object insertion has made
significant progress with the recent advent of latent diffusion models.
Utilizing a precise object mask can greatly enhance these applications.
However, due to the challenges users encounter in creating high-fidelity masks,
there is a tendency for these methods to rely on more coarse masks (e.g.,
bounding box) for these applications. This results in limited control and
compromised background content preservation. To overcome these limitations, we
introduce SmartMask, which allows any novice user to create detailed masks for
precise object insertion. Combined with a ControlNet-Inpaint model, our
experiments demonstrate that SmartMask achieves superior object insertion
quality, preserving the background content more effectively than previous
methods. Notably, unlike prior works the proposed approach can also be used
even without user-mask guidance, which allows it to perform mask-free object
insertion at diverse positions and scales. Furthermore, we find that when used
iteratively with a novel instruction-tuning based planning model, SmartMask can
be used to design detailed layouts from scratch. As compared with user-scribble
based layout design, we observe that SmartMask allows for better quality
outputs with layout-to-image generation methods. Project page is available at
https://smartmask-gen.github.io
</p></li>
</ul>

<h3>Title: DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models. (arXiv:2312.05107v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05107">http://arxiv.org/abs/2312.05107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05107]] DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models(http://arxiv.org/abs/2312.05107)</code></li>
<li>Summary: <p>In this paper, we present DreaMoving, a diffusion-based controllable video
generation framework to produce high-quality customized human dance videos.
Specifically, given target identity and posture sequences, DreaMoving can
generate a video of the target identity dancing anywhere driven by the posture
sequences. To this end, we propose a Video ControlNet for motion-controlling
and a Content Guider for identity preserving. The proposed model is easy to use
and can be adapted to most stylized diffusion models to generate diverse
results. The project page is available at
https://dreamoving.github.io/dreamoving.
</p></li>
</ul>

<h3>Title: SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation. (arXiv:2312.05239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05239">http://arxiv.org/abs/2312.05239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05239]] SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation(http://arxiv.org/abs/2312.05239)</code></li>
<li>Summary: <p>Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.
</p></li>
</ul>

<h3>Title: KBFormer: A Diffusion Model for Structured Entity Completion. (arXiv:2312.05253v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05253">http://arxiv.org/abs/2312.05253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05253]] KBFormer: A Diffusion Model for Structured Entity Completion(http://arxiv.org/abs/2312.05253)</code></li>
<li>Summary: <p>We develop a generative attention-based approach to modeling structured
entities comprising different property types, such as numerical, categorical,
string, and composite. This approach handles such heterogeneous data through a
mixed continuous-discrete diffusion process over the properties. Our flexible
framework can model entities with arbitrary hierarchical properties, enabling
applications to structured Knowledge Base (KB) entities and tabular data. Our
approach obtains state-of-the-art performance on a majority of cases across 15
datasets. In addition, experiments with a device KB and a nuclear physics
dataset demonstrate the model's ability to learn representations useful for
entity completion in diverse settings. This has many downstream use cases,
including modeling numerical properties with high accuracy - critical for
science applications, which also benefit from the model's inherent
probabilistic nature.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning. (arXiv:2312.04793v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04793">http://arxiv.org/abs/2312.04793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04793]] User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning(http://arxiv.org/abs/2312.04793)</code></li>
<li>Summary: <p>Image captioning bridges the gap between vision and language by automatically
generating natural language descriptions for images. Traditional image
captioning methods often overlook the preferences and characteristics of users.
Personalized image captioning solves this problem by incorporating user prior
knowledge into the model, such as writing styles and preferred vocabularies.
Most existing methods emphasize the user context fusion process by memory
networks or transformers. However, these methods ignore the distinct domains of
each dataset. Therefore, they need to update the entire caption model
parameters when meeting new samples, which is time-consuming and
calculation-intensive. To address this challenge, we propose a novel
personalized image captioning framework that leverages user context to consider
personality factors. Additionally, our framework utilizes the prefix-tuning
paradigm to extract knowledge from a frozen large language model, reducing the
gap between different language domains. Specifically, we employ CLIP to extract
the visual features of an image and align the semantic space using a
query-guided mapping network. By incorporating the transformer layer, we merge
the visual features with the user's contextual prior knowledge to generate
informative prefixes. Moreover, we employ GPT-2 as the frozen large language
model. With a small number of parameters to be trained, our model performs
efficiently and effectively. Our model outperforms existing baseline models on
Instagram and YFCC100M datasets across five evaluation metrics, demonstrating
its superiority, including twofold improvements in metrics such as BLEU-4 and
CIDEr.
</p></li>
</ul>

<h3>Title: Adapting Vision Transformer for Efficient Change Detection. (arXiv:2312.04869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04869">http://arxiv.org/abs/2312.04869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04869]] Adapting Vision Transformer for Efficient Change Detection(http://arxiv.org/abs/2312.04869)</code></li>
<li>Summary: <p>Most change detection models based on vision transformers currently follow a
"pretraining then fine-tuning" strategy. This involves initializing the model
weights using large scale classification datasets, which can be either natural
images or remote sensing images. However, fully tuning such a model requires
significant time and resources. In this paper, we propose an efficient tuning
approach that involves freezing the parameters of the pretrained image encoder
and introducing additional training parameters. Through this approach, we have
achieved competitive or even better results while maintaining extremely low
resource consumption across six change detection benchmarks. For example,
training time on LEVIR-CD, a change detection benchmark, is only half an hour
with 9 GB memory usage, which could be very convenient for most researchers.
Additionally, the decoupled tuning framework can be extended to any pretrained
model for semantic change detection and multi temporal change detection as
well. We hope that our proposed approach will serve as a part of foundational
model to inspire more unified training approaches on change detection in the
future.
</p></li>
</ul>

<h3>Title: Reconstructing Hands in 3D with Transformers. (arXiv:2312.05251v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05251">http://arxiv.org/abs/2312.05251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05251]] Reconstructing Hands in 3D with Transformers(http://arxiv.org/abs/2312.05251)</code></li>
<li>Summary: <p>We present an approach that can reconstruct hands in 3D from monocular input.
Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based
architecture and can analyze hands with significantly increased accuracy and
robustness compared to previous work. The key to HaMeR's success lies in
scaling up both the data used for training and the capacity of the deep network
for hand reconstruction. For training data, we combine multiple datasets that
contain 2D or 3D hand annotations. For the deep model, we use a large scale
Vision Transformer architecture. Our final model consistently outperforms the
previous baselines on popular 3D hand pose benchmarks. To further evaluate the
effect of our design in non-controlled settings, we annotate existing
in-the-wild datasets with 2D hand keypoint annotations. On this newly collected
dataset of annotations, HInt, we demonstrate significant improvements over
existing baselines. We make our code, data and models available on the project
website: https://geopavlakos.github.io/hamer/.
</p></li>
</ul>

<h3>Title: On Sarcasm Detection with OpenAI GPT-based Models. (arXiv:2312.04642v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04642">http://arxiv.org/abs/2312.04642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04642]] On Sarcasm Detection with OpenAI GPT-based Models(http://arxiv.org/abs/2312.04642)</code></li>
<li>Summary: <p>Sarcasm is a form of irony that requires readers or listeners to interpret
its intended meaning by considering context and social cues. Machine learning
classification models have long had difficulty detecting sarcasm due to its
social complexity and contradictory nature.
</p>
<p>This paper explores the applications of the Generative Pretrained Transformer
(GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting
sarcasm in natural language. It tests fine-tuned and zero-shot models of
different sizes and releases.
</p>
<p>The GPT models were tested on the political and balanced (pol-bal) portion of
the popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the
fine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and
$F_1$-score of 0.81, outperforming prior models. In the zero-shot case, one of
GPT-4 models yields an accuracy of 0.70 and $F_1$-score of 0.75. Other models
score lower. Additionally, a model's performance may improve or deteriorate
with each release, highlighting the need to reassess performance after each
release.
</p></li>
</ul>

<h3>Title: Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games. (arXiv:2312.04657v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04657">http://arxiv.org/abs/2312.04657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04657]] Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games(http://arxiv.org/abs/2312.04657)</code></li>
<li>Summary: <p>In this work, we introduce a self-supervised behavior cloning transformer for
text games, which are challenging benchmarks for multi-step reasoning in
virtual environments. Traditionally, Behavior Cloning Transformers excel in
such tasks but rely on supervised training data. Our approach auto-generates
training data by exploring trajectories (defined by common macro-action
sequences) that lead to reward within the games, while determining the
generality and utility of these trajectories by rapidly training small models
then evaluating their performance on unseen development games. Through
empirical analysis, we show our method consistently uncovers generalizable
training data, achieving about 90\% performance of supervised systems across
three benchmark text games.
</p></li>
</ul>

<h3>Title: Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning. (arXiv:2312.04736v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04736">http://arxiv.org/abs/2312.04736</a></li>
<li>Code URL: https://github.com/uoe-agents/feedback-dt</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04736]] Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning(http://arxiv.org/abs/2312.04736)</code></li>
<li>Summary: <p>Despite numerous successes, the field of reinforcement learning (RL) remains
far from matching the impressive generalisation power of human behaviour
learning. One possible way to help bridge this gap be to provide RL agents with
richer, more human-like feedback expressed in natural language. To investigate
this idea, we first extend BabyAI to automatically generate language feedback
from the environment dynamics and goal condition success. Then, we modify the
Decision Transformer architecture to take advantage of this additional signal.
We find that training with language feedback either in place of or in addition
to the return-to-go or goal descriptions improves agents' generalisation
performance, and that agents can benefit from feedback even when this is only
available during training, but not at inference.
</p></li>
</ul>

<h3>Title: Converting Epics/Stories into Pseudocode using Transformers. (arXiv:2312.05047v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05047">http://arxiv.org/abs/2312.05047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05047]] Converting Epics/Stories into Pseudocode using Transformers(http://arxiv.org/abs/2312.05047)</code></li>
<li>Summary: <p>The conversion of user epics or stories into their appropriate representation
in pseudocode or code is a time-consuming task, which can take up a large
portion of the time in an industrial project. With this research paper, we aim
to present a methodology to generate pseudocode from a given agile user story
of small functionalities so as to reduce the overall time spent on the
industrial project. Pseudocode is a programming language agnostic
representation of the steps involved in a computer program, which can be easily
converted into any programming language. Leveraging the potential of Natural
Language Processing, we want to simplify the development process in
organizations that use the Agile Model of Software Development. We present a
methodology to convert a problem described in the English language into
pseudocode. This methodology divides the Text to Pseudocode conversion task
into two stages or subtasks, each of which is treated like an individual
machine translation task. Stage 1 is Text to Code Conversion and Stage 2 is
Code to Pseudocode Conversion. We find that the CodeT5 model gives the best
results in terms of BLEU score when trained separately on the two subtasks
mentioned above. BLEU score is a metric that is used to measure the similarity
between a machine-translated text and a set of reference translations.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Damage GAN: A Generative Model for Imbalanced Data. (arXiv:2312.04862v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04862">http://arxiv.org/abs/2312.04862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04862]] Damage GAN: A Generative Model for Imbalanced Data(http://arxiv.org/abs/2312.04862)</code></li>
<li>Summary: <p>This study delves into the application of Generative Adversarial Networks
(GANs) within the context of imbalanced datasets. Our primary aim is to enhance
the performance and stability of GANs in such datasets. In pursuit of this
objective, we introduce a novel network architecture known as Damage GAN,
building upon the ContraD GAN framework which seamlessly integrates GANs and
contrastive learning. Through the utilization of contrastive learning, the
discriminator is trained to develop an unsupervised representation capable of
distinguishing all provided samples. Our approach draws inspiration from the
straightforward framework for contrastive learning of visual representations
(SimCLR), leading to the formulation of a distinctive loss function. We also
explore the implementation of self-damaging contrastive learning (SDCLR) to
further enhance the optimization of the ContraD GAN model. Comparative
evaluations against baseline models including the deep convolutional GAN
(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed
model, Damage GAN, in terms of generated image distribution, model stability,
and image quality when applied to imbalanced datasets.
</p></li>
</ul>

<h3>Title: Synthesizing Traffic Datasets using Graph Neural Networks. (arXiv:2312.05031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05031">http://arxiv.org/abs/2312.05031</a></li>
<li>Code URL: https://github.com/gvogiatzis/trafficgen</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05031]] Synthesizing Traffic Datasets using Graph Neural Networks(http://arxiv.org/abs/2312.05031)</code></li>
<li>Summary: <p>Traffic congestion in urban areas presents significant challenges, and
Intelligent Transportation Systems (ITS) have sought to address these via
automated and adaptive controls. However, these systems often struggle to
transfer simulated experiences to real-world scenarios. This paper introduces a
novel methodology for bridging this `sim-real' gap by creating photorealistic
images from 2D traffic simulations and recorded junction footage. We propose a
novel image generation approach, integrating a Conditional Generative
Adversarial Network with a Graph Neural Network (GNN) to facilitate the
creation of realistic urban traffic images. We harness GNNs' ability to process
information at different levels of abstraction alongside segmented images for
preserving locality data. The presented architecture leverages the power of
SPADE and Graph ATtention (GAT) network models to create images based on
simulated traffic scenarios. These images are conditioned by factors such as
entity positions, colors, and time of day. The uniqueness of our approach lies
in its ability to effectively translate structured and human-readable
conditions, encoded as graphs, into realistic images. This advancement
contributes to applications requiring rich traffic image datasets, from data
augmentation to urban traffic solutions. We further provide an application to
test the model's capabilities, including generating images with manually
defined positions for various entities.
</p></li>
</ul>

<h3>Title: SparQ Attention: Bandwidth-Efficient LLM Inference. (arXiv:2312.04985v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04985">http://arxiv.org/abs/2312.04985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04985]] SparQ Attention: Bandwidth-Efficient LLM Inference(http://arxiv.org/abs/2312.04985)</code></li>
<li>Summary: <p>Generative large language models (LLMs) have opened up numerous novel
possibilities, but due to their significant computational requirements their
ubiquitous use remains challenging. Some of the most useful applications
require processing large numbers of samples at a time and using long contexts,
both significantly increasing the memory communication load of the models. We
introduce SparQ Attention, a technique for increasing the inference throughput
of LLMs by reducing the memory bandwidth requirements within the attention
blocks through selective fetching of the cached history. Our proposed technique
can be applied directly to off-the-shelf LLMs during inference, without
requiring any modification to the pre-training setup or additional fine-tuning.
We show how SparQ Attention can decrease the attention memory bandwidth
requirements up to eight times without any loss in accuracy by evaluating Llama
2 and Pythia models on a wide range of downstream tasks.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos. (arXiv:2312.04746v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04746">http://arxiv.org/abs/2312.04746</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04746]] Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos(http://arxiv.org/abs/2312.04746)</code></li>
<li>Summary: <p>The gigapixel scale of whole slide images (WSIs) poses a challenge for
histopathology multi-modal chatbots, requiring a global WSI analysis for
diagnosis, compounding evidence from different WSI patches. Current visual
instruction datasets, generated through large language models, focus on
creating question/answer pairs for individual image patches, which may lack
diagnostic capacity on their own in histopathology, further complicated by the
absence of spatial grounding in histopathology image captions. To bridge this
gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131
histopathology-specific instruction question/answer pairs, that is collected by
leveraging educational histopathology videos from YouTube, which provides
spatial localization of captions by automatically extracting narrators' cursor
movements. In addition, we provide contextual reasoning by extracting diagnosis
and supporting facts from the entire video content to guide the extrapolative
reasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can
reason beyond the given single image patch, enabling diagnostic reasoning and
the capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a
comprehensive evaluation dataset created from 985 images and 1283
human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using
public histopathology datasets, where Quilt-LLaVA significantly outperforms
SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set
VQA. Our code, data, and model are publicly available at quilt-llava.github.io.
</p></li>
</ul>

<h3>Title: Retrieval-based Video Language Model for Efficient Long Video Question Answering. (arXiv:2312.04931v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04931">http://arxiv.org/abs/2312.04931</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04931]] Retrieval-based Video Language Model for Efficient Long Video Question Answering(http://arxiv.org/abs/2312.04931)</code></li>
<li>Summary: <p>The remarkable natural language understanding, reasoning, and generation
capabilities of large language models (LLMs) have made them attractive for
application to video question answering (Video QA) tasks, utilizing video
tokens as contextual input. However, employing LLMs for long video
understanding presents significant challenges and remains under-explored. The
extensive number of video tokens leads to considerable computational costs for
LLMs while using aggregated tokens results in loss of vision details. Moreover,
the presence of abundant question-irrelevant tokens introduces noise to the
video QA process. To address these issues, we introduce a simple yet effective
retrieval-based video language model (R-VLM) for efficient and interpretable
long video QA. Specifically, given a question (query) and a long video, our
model identifies and selects the most relevant $K$ video chunks and uses their
associated visual tokens to serve as context for the LLM inference. This
effectively reduces the number of video tokens, eliminates noise interference,
and enhances system performance. Our experimental results validate the
effectiveness of our framework for comprehending long videos. Furthermore,
based on the retrieved chunks, our model is interpretable that provides the
justifications on where we get the answers.
</p></li>
</ul>

<h3>Title: Latent Skill Discovery for Chain-of-Thought Reasoning. (arXiv:2312.04684v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04684">http://arxiv.org/abs/2312.04684</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04684]] Latent Skill Discovery for Chain-of-Thought Reasoning(http://arxiv.org/abs/2312.04684)</code></li>
<li>Summary: <p>Recent advances in Large Language Models (LLMs) have led to an emergent
ability of chain-of-thought (CoT) prompting, a prompt reasoning strategy that
adds intermediate rationale steps between questions and answers to construct
prompts. Conditioned on these prompts, LLMs can effectively learn in context to
generate rationales that lead to more accurate answers than when answering the
same question directly. To design LLM prompts, one important setting, called
demonstration selection, considers selecting demonstrations from an example
bank. Existing methods use various heuristics for this selection, but for CoT
prompting, which involves unique rationales, it is essential to base the
selection upon the intrinsic skills that CoT rationales need, for instance, the
skills of addition or subtraction for math word problems.
</p>
<p>To address this requirement, we introduce a novel approach named Reasoning
Skill Discovery (RSD) that use unsupervised learning to create a latent space
representation of rationales, called a reasoning skill. Simultaneously, RSD
learns a reasoning policy to determine the required reasoning skill for a given
question. This can then guide the selection of examples that demonstrate the
required reasoning skills. Our approach offers several desirable properties: it
is (1) theoretically grounded, (2) sample-efficient, requiring no LLM inference
or manual prompt design, and (3) LLM-agnostic. Empirically, RSD outperforms
existing methods by up to 6% in terms of the answer accuracy across multiple
reasoning tasks.
</p></li>
</ul>

<h3>Title: Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04691">http://arxiv.org/abs/2312.04691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04691]] Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models(http://arxiv.org/abs/2312.04691)</code></li>
<li>Summary: <p>Large language models (LLMs) with billions of parameters and pretrained on
massive amounts of data are now capable of near or better than state-of-the-art
performance in a variety of downstream natural language processing tasks.
Neural machine translation (NMT) is one such task that LLMs have been applied
to with great success. However, little research has focused on applying LLMs to
the more difficult subset of NMT called simultaneous translation (SimulMT),
where translation begins before the entire source context is available to the
model. In this paper, we address key challenges facing LLMs fine-tuned for
SimulMT, validate classical SimulMT concepts and practices in the context of
LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,
and introduce Simul-LLM, the first open-source fine-tuning and evaluation
pipeline development framework for LLMs focused on SimulMT.
</p></li>
</ul>

<h3>Title: Efficient Large Language Models Fine-Tuning On Graphs. (arXiv:2312.04737v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04737">http://arxiv.org/abs/2312.04737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04737]] Efficient Large Language Models Fine-Tuning On Graphs(http://arxiv.org/abs/2312.04737)</code></li>
<li>Summary: <p>Learning from Text-Attributed Graphs (TAGs) has attracted significant
attention due to its wide range of real-world applications. The rapid evolution
of large language models (LLMs) has revolutionized the way we process textual
data, which indicates a strong potential to replace shallow text embedding
generally used in Graph Neural Networks (GNNs). However, we find that existing
LLM approaches that exploit text information in graphs suffer from inferior
computation and data efficiency. In this work, we introduce a novel and
efficient approach for the end-to-end fine-tuning of Large Language Models
(LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost
and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover,
it transfers the rick knowledge in LLMs to downstream graph learning tasks
effectively with limited labeled data in semi-supervised learning. Its superior
computation and data efficiency are demonstrated through comprehensive
experiments, offering a promising solution for a wide range of LLMs and graph
learning tasks on TAGs.
</p></li>
</ul>

<h3>Title: HuRef: HUman-REadable Fingerprint for Large Language Models. (arXiv:2312.04828v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04828">http://arxiv.org/abs/2312.04828</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04828]] HuRef: HUman-REadable Fingerprint for Large Language Models(http://arxiv.org/abs/2312.04828)</code></li>
<li>Summary: <p>Protecting the copyright of large language models (LLMs) has become crucial
due to their resource-intensive training and accompanying carefully designed
licenses. However, identifying the original base model of an LLM is challenging
due to potential parameter alterations through fine-tuning or continued
pretraining. In this study, we introduce HuRef, a human-readable fingerprint
for LLMs that uniquely identifies the base model without exposing model
parameters or interfering with training. We first observe that the vector
direction of LLM parameters remains stable after the model has converged during
pretraining, showing negligible perturbations through subsequent training
steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF,
which makes it a sufficient condition to identify the base model. The necessity
is validated by continuing to train an LLM with an extra term to drive away the
model parameters' direction and the model becomes damaged. However, this
direction is vulnerable to simple attacks like dimension permutation or matrix
rotation, which significantly change it without affecting performance. To
address this, leveraging the Transformer structure, we systematically analyze
potential attacks and define three invariant terms that identify an LLM's base
model. We make these invariant terms human-readable by mapping them to a
Gaussian vector using a convolutional encoder and then converting it into a
natural image with StyleGAN2. Our method generates a dog image as an identity
fingerprint for an LLM, where the dog's appearance strongly indicates the LLM's
base model. Experimental results across various LLMs demonstrate the
effectiveness of our method, the generated dog image remains invariant to
different training steps, including SFT, RLHF, or even continued pretraining
with augmented vocabulary in a new language.
</p></li>
</ul>

<h3>Title: Ophtha-LLaMA2: A Large Language Model for Ophthalmology. (arXiv:2312.04906v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04906">http://arxiv.org/abs/2312.04906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04906]] Ophtha-LLaMA2: A Large Language Model for Ophthalmology(http://arxiv.org/abs/2312.04906)</code></li>
<li>Summary: <p>In recent years, pre-trained large language models (LLMs) have achieved
tremendous success in the field of Natural Language Processing (NLP). Prior
studies have primarily focused on general and generic domains, with relatively
less research on specialized LLMs in the medical field. The specialization and
high accuracy requirements for diagnosis in the medical field, as well as the
challenges in collecting large-scale data, have constrained the application and
development of LLMs in medical scenarios. In the field of ophthalmology,
clinical diagnosis mainly relies on doctors' interpretation of reports and
making diagnostic decisions. In order to take advantage of LLMs to provide
decision support for doctors, we collected three modalities of ophthalmic
report data and fine-tuned the LLaMA2 model, successfully constructing an LLM
termed the "Ophtha-LLaMA2" specifically tailored for ophthalmic disease
diagnosis. Inference test results show that even with a smaller fine-tuning
dataset, Ophtha-LLaMA2 performs significantly better in ophthalmic diagnosis
compared to other LLMs. It demonstrates that the Ophtha-LLaMA2 exhibits
satisfying accuracy and efficiency in ophthalmic disease diagnosis, making it a
valuable tool for ophthalmologists to provide improved diagnostic support for
patients. This research provides a useful reference for the application of LLMs
in the field of ophthalmology, while showcasing the immense potential and
prospects in this domain.
</p></li>
</ul>

<h3>Title: PathFinder: Guided Search over Multi-Step Reasoning Paths. (arXiv:2312.05180v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05180">http://arxiv.org/abs/2312.05180</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05180]] PathFinder: Guided Search over Multi-Step Reasoning Paths(http://arxiv.org/abs/2312.05180)</code></li>
<li>Summary: <p>With recent advancements in large language models, methods like
chain-of-thought prompting to elicit reasoning chains have been shown to
improve results on reasoning tasks. However, tasks that require multiple steps
of reasoning still pose significant challenges to state-of-the-art models.
Drawing inspiration from the beam search algorithm, we propose PathFinder, a
tree-search-based reasoning path generation approach. It enhances diverse
branching and multi-hop reasoning through the integration of dynamic decoding,
enabled by varying sampling methods and parameters. Using constrained
reasoning, PathFinder integrates novel quality constraints, pruning, and
exploration methods to enhance the efficiency and the quality of generation.
Moreover, it includes scoring and ranking features to improve candidate
selection. Our approach outperforms competitive baselines on three complex
arithmetic and commonsense reasoning tasks by 6% on average. Our model
generalizes well to longer, unseen reasoning chains, reflecting similar
complexities to beam search with large branching factors.
</p></li>
</ul>

<h3>Title: EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism. (arXiv:2312.04916v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04916">http://arxiv.org/abs/2312.04916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04916]] EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism(http://arxiv.org/abs/2312.04916)</code></li>
<li>Summary: <p>We present EE-LLM, a framework for large-scale training and inference of
early-exit large language models (LLMs). While recent works have shown
preliminary evidence for the efficacy of early exiting in accelerating LLM
inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs
by supporting their training and inference with massive 3D parallelism. Built
upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and
performance optimizations tailored to early exiting, including a lightweight
method that facilitates backpropagation for the early-exit training objective
with pipeline parallelism, techniques of leveraging idle resources in the
original pipeline schedule for computation related to early-exit layers, and
two approaches of early-exit inference that are compatible with KV caching for
autoregressive generation. Our analytical and empirical study shows that EE-LLM
achieves great training efficiency with negligible computational overhead
compared to standard LLM training, as well as outstanding inference speedup
without compromising output quality. To facilitate further research and
adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: gcDLSeg: Integrating Graph-cut into Deep Learning for Binary Semantic Segmentation. (arXiv:2312.04713v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04713">http://arxiv.org/abs/2312.04713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04713]] gcDLSeg: Integrating Graph-cut into Deep Learning for Binary Semantic Segmentation(http://arxiv.org/abs/2312.04713)</code></li>
<li>Summary: <p>Binary semantic segmentation in computer vision is a fundamental problem. As
a model-based segmentation method, the graph-cut approach was one of the most
successful binary segmentation methods thanks to its global optimality
guarantee of the solutions and its practical polynomial-time complexity.
Recently, many deep learning (DL) based methods have been developed for this
task and yielded remarkable performance, resulting in a paradigm shift in this
field. To combine the strengths of both approaches, we propose in this study to
integrate the graph-cut approach into a deep learning network for end-to-end
learning. Unfortunately, backward propagation through the graph-cut module in
the DL network is challenging due to the combinatorial nature of the graph-cut
algorithm. To tackle this challenge, we propose a novel residual graph-cut loss
and a quasi-residual connection, enabling the backward propagation of the
gradients of the residual graph-cut loss for effective feature learning guided
by the graph-cut segmentation model. In the inference phase, globally optimal
segmentation is achieved with respect to the graph-cut energy defined on the
optimized image features learned from DL networks. Experiments on the public
AZH chronic wound data set and the pancreas cancer data set from the medical
segmentation decathlon (MSD) demonstrated promising segmentation accuracy, and
improved robustness against adversarial attacks.
</p></li>
</ul>

<h3>Title: E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation. (arXiv:2312.04727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04727">http://arxiv.org/abs/2312.04727</a></li>
<li>Code URL: https://github.com/boqian333/e2enet-medical</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04727]] E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation(http://arxiv.org/abs/2312.04727)</code></li>
<li>Summary: <p>Deep neural networks have evolved as the leading approach in 3D medical image
segmentation due to their outstanding performance. However, the ever-increasing
model size and computation cost of deep neural networks have become the primary
barrier to deploying them on real-world resource-limited hardware. In pursuit
of improving performance and efficiency, we propose a 3D medical image
segmentation model, named Efficient to Efficient Network (E2ENet),
incorporating two parametrically and computationally efficient designs. i.
Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse
informative multi-scale features while reducing redundancy. ii. Restricted
depth-shift in 3D convolution: it leverages the 3D spatial information while
keeping the model and computational complexity as 2D-based methods. We conduct
extensive experiments on BTCV, AMOS-CT and Brain Tumor Segmentation Challenge,
demonstrating that E2ENet consistently achieves a superior trade-off between
accuracy and efficiency than prior arts across various resource constraints.
E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while
saving over 68\% parameter count and 29\% FLOPs in the inference phase,
compared with the previous best-performing method. Our code has been made
available at: https://github.com/boqian333/E2ENet-Medical.
</p></li>
</ul>

<h3>Title: Visual Grounding of Whole Radiology Reports for 3D CT Images. (arXiv:2312.04794v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04794">http://arxiv.org/abs/2312.04794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04794]] Visual Grounding of Whole Radiology Reports for 3D CT Images(http://arxiv.org/abs/2312.04794)</code></li>
<li>Summary: <p>Building a large-scale training dataset is an essential problem in the
development of medical image recognition systems. Visual grounding techniques,
which automatically associate objects in images with corresponding
descriptions, can facilitate labeling of large number of images. However,
visual grounding of radiology reports for CT images remains challenging,
because so many kinds of anomalies are detectable via CT imaging, and resulting
report descriptions are long and complex. In this paper, we present the first
visual grounding framework designed for CT image and report pairs covering
various body parts and diverse anomaly types. Our framework combines two
components of 1) anatomical segmentation of images, and 2) report structuring.
The anatomical segmentation provides multiple organ masks of given CT images,
and helps the grounding model recognize detailed anatomies. The report
structuring helps to accurately extract information regarding the presence,
location, and type of each anomaly described in corresponding reports. Given
the two additional image/report features, the grounding model can achieve
better localization. In the verification process, we constructed a large-scale
dataset with region-description correspondence annotations for 10,410 studies
of 7,321 unique patients. We evaluated our framework using grounding accuracy,
the percentage of correctly localized anomalies, as a metric and demonstrated
that the combination of the anatomical segmentation and the report structuring
improves the performance with a large margin over the baseline model (66.0% vs
77.8%). Comparison with the prior techniques also showed higher performance of
our method.
</p></li>
</ul>

<h3>Title: DARNet: Bridging Domain Gaps in Cross-Domain Few-Shot Segmentation with Dynamic Adaptation. (arXiv:2312.04813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04813">http://arxiv.org/abs/2312.04813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04813]] DARNet: Bridging Domain Gaps in Cross-Domain Few-Shot Segmentation with Dynamic Adaptation(http://arxiv.org/abs/2312.04813)</code></li>
<li>Summary: <p>Few-shot segmentation (FSS) aims to segment novel classes in a query image by
using only a small number of supporting images from base classes. However, in
cross-domain few-shot segmentation (CD-FSS), leveraging features from
label-rich domains for resource-constrained domains poses challenges due to
domain discrepancies. This work presents a Dynamically Adaptive Refine (DARNet)
method that aims to balance generalization and specificity for CD-FSS. Our
method includes the Channel Statistics Disruption (CSD) strategy, which
perturbs feature channel statistics in the source domain, bolstering
generalization to unknown target domains. Moreover, recognizing the variability
across target domains, an Adaptive Refine Self-Matching (ARSM) method is also
proposed to adjust the matching threshold and dynamically refine the prediction
result with the self-matching method, enhancing accuracy. We also present a
Test-Time Adaptation (TTA) method to refine the model's adaptability to diverse
feature distributions. Our approach demonstrates superior performance against
state-of-the-art methods in CD-FSS tasks.
</p></li>
</ul>

<h3>Title: VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement. (arXiv:2312.04885v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04885">http://arxiv.org/abs/2312.04885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04885]] VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement(http://arxiv.org/abs/2312.04885)</code></li>
<li>Summary: <p>In recent years, online Video Instance Segmentation (VIS) methods have shown
remarkable advancement with their powerful query-based detectors. Utilizing the
output queries of the detector at the frame level, these methods achieve high
accuracy on challenging benchmarks. However, we observe the heavy reliance of
these methods on the location information that leads to incorrect matching when
positional cues are insufficient for resolving ambiguities. Addressing this
issue, we present VISAGE that enhances instance association by explicitly
leveraging appearance information. Our method involves a generation of queries
that embed appearances from backbone feature maps, which in turn get used in
our suggested simple tracker for robust associations. Finally, enabling
accurate matching in complex scenarios by resolving the issue of over-reliance
on location information, we achieve competitive performance on multiple VIS
benchmarks. For instance, on YTVIS19 and YTVIS21, our method achieves 54.5 AP
and 50.8 AP. Furthermore, to highlight appearance-awareness not fully addressed
by existing benchmarks, we generate a synthetic dataset where our method
outperforms others significantly by leveraging the appearance cue. Code will be
made available at https://github.com/KimHanjung/VISAGE.
</p></li>
</ul>

<h3>Title: Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images. (arXiv:2312.04947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04947">http://arxiv.org/abs/2312.04947</a></li>
<li>Code URL: https://github.com/vlar-group/unsupobjseg</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04947]] Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images(http://arxiv.org/abs/2312.04947)</code></li>
<li>Summary: <p>In this paper, we study the problem of unsupervised object segmentation from
single images. We do not introduce a new algorithm, but systematically
investigate the effectiveness of existing unsupervised models on challenging
real-world images. We first introduce seven complexity factors to
quantitatively measure the distributions of background and foreground object
biases in appearance and geometry for datasets with human annotations. With the
aid of these factors, we empirically find that, not surprisingly, existing
unsupervised models fail to segment generic objects in real-world images,
although they can easily achieve excellent performance on numerous simple
synthetic datasets, due to the vast gap in objectness biases between synthetic
and real images. By conducting extensive experiments on multiple groups of
ablated real-world datasets, we ultimately find that the key factors underlying
the failure of existing unsupervised models on real-world images are the
challenging distributions of background and foreground object biases in
appearance and geometry. Because of this, the inductive biases introduced in
existing unsupervised models can hardly capture the diverse object
distributions. Our research results suggest that future work should exploit
more explicit objectness biases in the network design.
</p></li>
</ul>

<h3>Title: Point2CAD: Reverse Engineering CAD Models from 3D Point Clouds. (arXiv:2312.04962v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04962">http://arxiv.org/abs/2312.04962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04962]] Point2CAD: Reverse Engineering CAD Models from 3D Point Clouds(http://arxiv.org/abs/2312.04962)</code></li>
<li>Summary: <p>Computer-Aided Design (CAD) model reconstruction from point clouds is an
important problem at the intersection of computer vision, graphics, and machine
learning; it saves the designer significant time when iterating on in-the-wild
objects. Recent advancements in this direction achieve relatively reliable
semantic segmentation but still struggle to produce an adequate topology of the
CAD model. In this work, we analyze the current state of the art for that
ill-posed task and identify shortcomings of existing methods. We propose a
hybrid analytic-neural reconstruction scheme that bridges the gap between
segmented point clouds and structured CAD models and can be readily combined
with different segmentation backbones. Moreover, to power the surface fitting
stage, we propose a novel implicit neural representation of freeform surfaces,
driving up the performance of our overall CAD reconstruction scheme. We
extensively evaluate our method on the popular ABC benchmark of CAD models and
set a new state-of-the-art for that dataset. Project page:
https://www.obukhov.ai/point2cad}{https://www.obukhov.ai/point2cad.
</p></li>
</ul>

<h3>Title: ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting. (arXiv:2312.04964v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04964">http://arxiv.org/abs/2312.04964</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04964]] ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting(http://arxiv.org/abs/2312.04964)</code></li>
<li>Summary: <p>The long-tailed distribution problem in medical image analysis reflects a
high prevalence of common conditions and a low prevalence of rare ones, which
poses a significant challenge in developing a unified model capable of
identifying rare or novel tumor categories not encountered during training. In
this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)
based on query-disentangling and self-prompting to segment unseen tumor
categories beyond the training set. ZePT disentangles the object queries into
two subsets and trains them in two stages. Initially, it learns a set of
fundamental queries for organ segmentation through an object-aware feature
grouping strategy, which gathers organ-level visual features. Subsequently, it
refines the other set of advanced queries that focus on the auto-generated
visual prompts for unseen tumor segmentation. Moreover, we introduce
query-knowledge alignment at the feature level to enhance each query's
discriminative representation and generalizability. Extensive experiments on
various tumor segmentation tasks demonstrate the performance superiority of
ZePT, which surpasses the previous counterparts and evidence the promising
ability for zero-shot tumor segmentation in real-world settings. Codes will be
made publicly available.
</p></li>
</ul>

<h3>Title: Continual learning for surface defect segmentation by subnetwork creation and selection. (arXiv:2312.05100v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05100">http://arxiv.org/abs/2312.05100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05100]] Continual learning for surface defect segmentation by subnetwork creation and selection(http://arxiv.org/abs/2312.05100)</code></li>
<li>Summary: <p>We introduce a new continual (or lifelong) learning algorithm called LDA-CP&amp;S
that performs segmentation tasks without undergoing catastrophic forgetting.
The method is applied to two different surface defect segmentation problems
that are learned incrementally, i.e. providing data about one type of defect at
a time, while still being capable of predicting every defect that was seen
previously. Our method creates a defect-related subnetwork for each defect type
via iterative pruning and trains a classifier based on linear discriminant
analysis (LDA). At the inference stage, we first predict the defect type with
LDA and then predict the surface defects using the selected subnetwork. We
compare our method with other continual learning methods showing a significant
improvement -- mean Intersection over Union better by a factor of two when
compared to existing methods on both datasets. Importantly, our approach shows
comparable results with joint training when all the training data (all defects)
are seen simultaneously
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
