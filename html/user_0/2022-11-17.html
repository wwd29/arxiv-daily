<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Securer and Faster Privacy-Preserving Distributed Machine Learning. (arXiv:2211.09353v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09353">http://arxiv.org/abs/2211.09353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09353] Securer and Faster Privacy-Preserving Distributed Machine Learning](http://arxiv.org/abs/2211.09353)</code></li>
<li>Summary: <p>With the development of machine learning, it is difficult for a single server
to process all the data. So machine learning tasks need to be spread across
multiple servers, turning centralized machine learning into a distributed one.
However, privacy remains an unsolved problem in distributed machine learning.
Multi-key homomorphic encryption over torus (MKTFHE) is one of the suitable
candidates to solve the problem. However, there may be security risks in the
decryption of MKTFHE and the most recent result about MKFHE only supports the
Boolean operation and linear operation. So, MKTFHE cannot compute the
non-linear function like Sigmoid directly and it is still hard to perform
common machine learning such as logistic regression and neural networks in high
performance.
</p></li>
</ul>

<p>This paper first introduces secret sharing to propose a new distributed
decryption protocol for MKTFHE, then designs an MKTFHE-friendly activation
function, and finally utilizes them to implement logistic regression and neural
network training in MKTFHE. We prove the correctness and security of our
decryption protocol and compare the efficiency and accuracy between using
Taylor polynomials of Sigmoid and our proposed function as an activation
function. The experiments show that the efficiency of our function is 10 times
higher than using 7-order Taylor polynomials straightly and the accuracy of the
training model is similar to that of using a high-order polynomial as an
activation function scheme.
</p>

<h2>security</h2>
<h3>Title: Social Networks are Divulging Your Identity behind Crypto Addresses. (arXiv:2211.09656v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09656">http://arxiv.org/abs/2211.09656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09656] Social Networks are Divulging Your Identity behind Crypto Addresses](http://arxiv.org/abs/2211.09656)</code></li>
<li>Summary: <p>Cryptocurrencies, such as Bitcoin and Ethereum, are becoming increasingly
prevalent mainly due to their anonymity, decentralization, transparency, and
security. However, the completely public ledger makes the trace and analysis of
each account possible as long as the identity behind the public address is
revealed. Theoretically, social networks could make that happen when addresses
are posted on social network platforms using accounts containing personal
information. To verify such a possibility, we have collected public data from
two major platforms, i.e. Twitter and Reddit, aiming to find potential privacy
leakage behind the ETH public address. In the end, an easy-to-use retrieval
application is also built for a better illustration.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: DeepPrivacy2: Towards Realistic Full-Body Anonymization. (arXiv:2211.09454v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09454">http://arxiv.org/abs/2211.09454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09454] DeepPrivacy2: Towards Realistic Full-Body Anonymization](http://arxiv.org/abs/2211.09454)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) are widely adapted for anonymization
of human figures. However, current state-of-the-art limit anonymization to the
task of face anonymization. In this paper, we propose a novel anonymization
framework (DeepPrivacy2) for realistic anonymization of human figures and
faces. We introduce a new large and diverse dataset for human figure synthesis,
which significantly improves image quality and diversity of generated images.
Furthermore, we propose a style-based GAN that produces high quality, diverse
and editable anonymizations. We demonstrate that our full-body anonymization
framework provides stronger privacy guarantees than previously proposed
methods.
</p></li>
</ul>

<h3>Title: HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors. (arXiv:2211.09648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09648">http://arxiv.org/abs/2211.09648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09648] HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors](http://arxiv.org/abs/2211.09648)</code></li>
<li>Summary: <p>The main streams of human activity recognition (HAR) algorithms are developed
based on RGB cameras which are suffered from illumination, fast motion,
privacy-preserving, and large energy consumption. Meanwhile, the biologically
inspired event cameras attracted great interest due to their unique features,
such as high dynamic range, dense temporal but sparse spatial resolution, low
latency, low power, etc. As it is a newly arising sensor, even there is no
realistic large-scale dataset for HAR. Considering its great practical value,
in this paper, we propose a large-scale benchmark dataset to bridge this gap,
termed HARDVS, which contains 300 categories and more than 100K event
sequences. We evaluate and report the performance of multiple popular HAR
algorithms, which provide extensive baselines for future works to compare. More
importantly, we propose a novel spatial-temporal feature learning and fusion
framework, termed ESTF, for event stream based human activity recognition. It
first projects the event streams into spatial and temporal embeddings using
StemNet, then, encodes and fuses the dual-view representations using
Transformer networks. Finally, the dual features are concatenated and fed into
a classification head for activity prediction. Extensive experiments on
multiple datasets fully validated the effectiveness of our model. Both the
dataset and source code will be released on
\url{https://github.com/Event-AHU/HARDVS}.
</p></li>
</ul>

<h3>Title: ConStruct-VL: Data-Free Continual Structured VL Concepts Learning. (arXiv:2211.09790v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09790">http://arxiv.org/abs/2211.09790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09790] ConStruct-VL: Data-Free Continual Structured VL Concepts Learning](http://arxiv.org/abs/2211.09790)</code></li>
<li>Summary: <p>Recently, large-scale pre-trained Vision-and-Language (VL) foundation models
have demonstrated remarkable capabilities in many zero-shot downstream tasks,
achieving competitive results for recognizing objects defined by as little as
short text prompts. However, it has also been shown that VL models are still
brittle in Structured VL Concept (SVLC) reasoning, such as the ability to
recognize object attributes, states, and inter-object relations. This leads to
reasoning mistakes, which need to be corrected as they occur by teaching VL
models the missing SVLC skills; often this must be done using private data
where the issue was found, which naturally leads to a data-free continual (no
task-id) VL learning setting. In this work, we introduce the first Continual
Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it
is challenging for many existing data-free CL strategies. We, therefore,
propose a data-free method comprised of a new approach of Adversarial
Pseudo-Replay (APR) which generates adversarial reminders of past tasks from
past task models. To use this method efficiently, we also propose a continual
parameter-efficient Layered-LoRA (LaLo) neural architecture allowing
no-memory-cost access to all past models at train time. We show this approach
outperforms all data-free methods by as much as ~7% while even matching some
levels of experience-replay (prohibitive for applications where data-privacy
must be preserved).
</p></li>
</ul>

<h3>Title: Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09273">http://arxiv.org/abs/2211.09273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09273] Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning](http://arxiv.org/abs/2211.09273)</code></li>
<li>Summary: <p>Emotional Surveillance is an emerging area with wide-reaching privacy
concerns. These concerns are exacerbated by ubiquitous IoT devices with
multiple sensors that can support these surveillance use cases. The work
presented here considers one such use case: the use of a speech emotion
recognition (SER) classifier tied to a smart speaker. This work demonstrates
the ability to evade black-box SER classifiers tied to a smart speaker without
compromising the utility of the smart speaker. This privacy concern is
considered through the lens of adversarial evasion of machine learning. Our
solution, Defeating Acoustic Recognition of Emotion via Genetic Programming
(DARE-GP), uses genetic programming to generate non-invasive additive audio
perturbations (AAPs). By constraining the evolution of these AAPs,
transcription accuracy can be protected while simultaneously degrading SER
classifier performance. The additive nature of these AAPs, along with an
approach that generates these AAPs for a fixed set of users in an utterance and
user location-independent manner, supports real-time, real-world evasion of SER
classifiers. DARE-GP's use of spectral features, which underlay the emotional
content of speech, allows the transferability of AAPs to previously unseen
black-box SER classifiers. Further, DARE-GP outperforms state-of-the-art SER
evasion techniques and is robust against defenses employed by a knowledgeable
adversary. The evaluations in this work culminate with acoustic evaluations
against two off-the-shelf commercial smart speakers, where a single AAP could
evade a black box classifier over 70% of the time. The final evaluation
deployed AAP playback on a small-form-factor system (raspberry pi) integrated
with a wake-word system to evaluate the efficacy of a real-world, real-time
deployment where DARE-GP is automatically invoked with the smart speaker's wake
word.
</p></li>
</ul>

<h3>Title: Permutation-Invariant Tabular Data Synthesis. (arXiv:2211.09286v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09286">http://arxiv.org/abs/2211.09286</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09286] Permutation-Invariant Tabular Data Synthesis](http://arxiv.org/abs/2211.09286)</code></li>
<li>Summary: <p>Tabular data synthesis is an emerging approach to circumvent strict
regulations on data privacy while discovering knowledge through big data.
Although state-of-the-art AI-based tabular data synthesizers, e.g., table-GAN,
CTGAN, TVAE, and CTAB-GAN, are effective at generating synthetic tabular data,
their training is sensitive to column permutations of input data. In this
paper, we first conduct an extensive empirical study to disclose such a
property of permutation invariance and an in-depth analysis of the existing
synthesizers. We show that changing the input column order worsens the
statistical difference between real and synthetic data by up to 38.67% due to
the encoding of tabular data and the network architectures. To fully unleash
the potential of big synthetic tabular data, we propose two solutions: (i)
AE-GAN, a synthesizer that uses an autoencoder network to represent the tabular
data and GAN networks to synthesize the latent representation, and (ii) a
feature sorting algorithm to find the suitable column order of input data for
CNN-based synthesizers. We evaluate the proposed solutions on five datasets in
terms of the sensitivity to the column permutation, the quality of synthetic
data, and the utility in downstream analyses. Our results show that we enhance
the property of permutation-invariance when training synthesizers and further
improve the quality and utility of synthetic data, up to 22%, compared to the
existing synthesizers.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Generalizable Deepfake Detection with Phase-Based Motion Analysis. (arXiv:2211.09363v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09363">http://arxiv.org/abs/2211.09363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09363] Generalizable Deepfake Detection with Phase-Based Motion Analysis](http://arxiv.org/abs/2211.09363)</code></li>
<li>Summary: <p>We propose PhaseForensics, a DeepFake (DF) video detection method that
leverages a phase-based motion representation of facial temporal dynamics.
Existing methods relying on temporal inconsistencies for DF detection present
many advantages over the typical frame-based methods. However, they still show
limited cross-dataset generalization and robustness to common distortions.
These shortcomings are partially due to error-prone motion estimation and
landmark tracking, or the susceptibility of the pixel intensity-based features
to spatial distortions and the cross-dataset domain shifts. Our key insight to
overcome these issues is to leverage the temporal phase variations in the
band-pass components of the Complex Steerable Pyramid on face sub-regions. This
not only enables a robust estimate of the temporal dynamics in these regions,
but is also less prone to cross-dataset variations. Furthermore, the band-pass
filters used to compute the local per-frame phase form an effective defense
against the perturbations commonly seen in gradient-based adversarial attacks.
Overall, with PhaseForensics, we show improved distortion and adversarial
robustness, and state-of-the-art cross-dataset generalization, with 91.2%
video-level AUC on the challenging CelebDFv2 (a recent state-of-the-art
compares at 86.9%).
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Targeted Attention for Generalized- and Zero-Shot Learning. (arXiv:2211.09322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09322">http://arxiv.org/abs/2211.09322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09322] Targeted Attention for Generalized- and Zero-Shot Learning](http://arxiv.org/abs/2211.09322)</code></li>
<li>Summary: <p>The Zero-Shot Learning (ZSL) task attempts to learn concepts without any
labeled data. Unlike traditional classification/detection tasks, the evaluation
environment is provided unseen classes never encountered during training. As
such, it remains both challenging, and promising on a variety of fronts,
including unsupervised concept learning, domain adaptation, and dataset drift
detection. Recently, there have been a variety of approaches towards solving
ZSL, including improved metric learning methods, transfer learning,
combinations of semantic and image domains using, e.g. word vectors, and
generative models to model the latent space of known classes to classify unseen
classes. We find many approaches require intensive training augmentation with
attributes or features that may be commonly unavailable (attribute-based
learning) or susceptible to adversarial attacks (generative learning). We
propose combining approaches from the related person re-identification task for
ZSL, with key modifications to ensure sufficiently improved performance in the
ZSL setting without the need for feature or training dataset augmentation. We
are able to achieve state-of-the-art performance on the CUB200 and Cars196
datasets in the ZSL setting compared to recent works, with NMI (normalized
mutual inference) of 63.27 and top-1 of 61.04 for CUB200, and NMI 66.03 with
top-1 82.75% in Cars196. We also show state-of-the-art results in the
Generalized Zero-Shot Learning (GZSL) setting, with Harmonic Mean R-1 of 66.14%
on the CUB200 dataset.
</p></li>
</ul>

<h3>Title: Towards Good Practices in Evaluating Transfer Adversarial Attacks. (arXiv:2211.09565v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09565">http://arxiv.org/abs/2211.09565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09565] Towards Good Practices in Evaluating Transfer Adversarial Attacks](http://arxiv.org/abs/2211.09565)</code></li>
<li>Summary: <p>Transfer adversarial attacks raise critical security concerns in real-world,
black-box scenarios. However, the actual progress of attack methods is
difficult to assess due to two main limitations in existing evaluations. First,
existing evaluations are unsystematic and sometimes unfair since new methods
are often directly added to old ones without complete comparisons to similar
methods. Second, existing evaluations mainly focus on transferability but
overlook another key attack property: stealthiness. In this work, we design
good practices to address these limitations. We first introduce a new attack
categorization, which enables our systematic analyses of similar attacks in
each specific category. Our analyses lead to new findings that complement or
even challenge existing knowledge. Furthermore, we comprehensively evaluate 23
representative attacks against 9 defenses on ImageNet. We pay particular
attention to stealthiness, by adopting diverse imperceptibility metrics and
looking into new, finer-grained characteristics. Our evaluation reveals new
important insights: 1) Transferability is highly contextual, and some white-box
defenses may give a false sense of security since they are actually vulnerable
to (black-box) transfer attacks; 2) All transfer attacks are less stealthy, and
their stealthiness can vary dramatically under the same $L_{\infty}$ bound.
</p></li>
</ul>

<h3>Title: T-SEA: Transfer-based Self-Ensemble Attack on Object Detection. (arXiv:2211.09773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09773">http://arxiv.org/abs/2211.09773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09773] T-SEA: Transfer-based Self-Ensemble Attack on Object Detection](http://arxiv.org/abs/2211.09773)</code></li>
<li>Summary: <p>Compared to query-based black-box attacks, transfer-based black-box attacks
do not require any information of the attacked models, which ensures their
secrecy. However, most existing transfer-based approaches rely on ensembling
multiple models to boost the attack transferability, which is time- and
resource-intensive, not to mention the difficulty of obtaining diverse models
on the same task. To address this limitation, in this work, we focus on the
single-model transfer-based black-box attack on object detection, utilizing
only one model to achieve a high-transferability adversarial attack on multiple
black-box detectors. Specifically, we first make observations on the patch
optimization process of the existing method and propose an enhanced attack
framework by slightly adjusting its training strategies. Then, we analogize
patch optimization with regular model optimization, proposing a series of
self-ensemble approaches on the input data, the attacked model, and the
adversarial patch to efficiently make use of the limited information and
prevent the patch from overfitting. The experimental results show that the
proposed framework can be applied with multiple classical base attack methods
(e.g., PGD and MIM) to greatly improve the black-box transferability of the
well-optimized patch on multiple mainstream detectors, meanwhile boosting
white-box performance. Our code is available at
https://github.com/VDIGPKU/T-SEA.
</p></li>
</ul>

<h3>Title: Ignore Previous Prompt: Attack Techniques For Language Models. (arXiv:2211.09527v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09527">http://arxiv.org/abs/2211.09527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09527] Ignore Previous Prompt: Attack Techniques For Language Models](http://arxiv.org/abs/2211.09527)</code></li>
<li>Summary: <p>Transformer-based large language models (LLMs) provide a powerful foundation
for natural language tasks in large-scale customer-facing applications.
However, studies that explore their vulnerabilities emerging from malicious
user interaction are scarce. By proposing PromptInject, a prosaic alignment
framework for mask-based iterative adversarial prompt composition, we examine
how GPT-3, the most widely deployed language model in production, can be easily
misaligned by simple handcrafted inputs. In particular, we investigate two
types of attacks -- goal hijacking and prompt leaking -- and demonstrate that
even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit
GPT-3's stochastic nature, creating long-tail risks. The code for PromptInject
is available at https://github.com/agencyenterprise/PromptInject.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: A Unified Multimodal De- and Re-coupling Framework for RGB-D Motion Recognition. (arXiv:2211.09146v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09146">http://arxiv.org/abs/2211.09146</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09146] A Unified Multimodal De- and Re-coupling Framework for RGB-D Motion Recognition](http://arxiv.org/abs/2211.09146)</code></li>
<li>Summary: <p>Motion recognition is a promising direction in computer vision, but the
training of video classification models is much harder than images due to
insufficient data and considerable parameters. To get around this, some works
strive to explore multimodal cues from RGB-D data. Although improving motion
recognition to some extent, these methods still face sub-optimal situations in
the following aspects: (i) Data augmentation, i.e., the scale of the RGB-D
datasets is still limited, and few efforts have been made to explore novel data
augmentation strategies for videos; (ii) Optimization mechanism, i.e., the
tightly space-time-entangled network structure brings more challenges to
spatiotemporal information modeling; And (iii) cross-modal knowledge fusion,
i.e., the high similarity between multimodal representations caused to
insufficient late fusion. To alleviate these drawbacks, we propose to improve
RGB-D-based motion recognition both from data and algorithm perspectives in
this paper. In more detail, firstly, we introduce a novel video data
augmentation method dubbed ShuffleMix, which acts as a supplement to MixUp, to
provide additional temporal regularization for motion recognition. Secondly, a
Unified Multimodal De-coupling and multi-stage Re-coupling framework, termed
UMDR, is proposed for video representation learning. Finally, a novel
cross-modal Complement Feature Catcher (CFCer) is explored to mine potential
commonalities features in multimodal information as the auxiliary fusion
stream, to improve the late fusion results. The seamless combination of these
novel designs forms a robust spatiotemporal representation and achieves better
performance than state-of-the-art methods on four public motion datasets.
Specifically, UMDR achieves unprecedented improvements of +4.5% on the Chalearn
IsoGD dataset.Our code is available at
https://github.com/zhoubenjia/MotionRGBD-PAMI.
</p></li>
</ul>

<h3>Title: 3D-QueryIS: A Query-based Framework for 3D Instance Segmentation. (arXiv:2211.09375v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09375">http://arxiv.org/abs/2211.09375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09375] 3D-QueryIS: A Query-based Framework for 3D Instance Segmentation](http://arxiv.org/abs/2211.09375)</code></li>
<li>Summary: <p>Previous top-performing methods for 3D instance segmentation often maintain
inter-task dependencies and the tendency towards a lack of robustness. Besides,
inevitable variations of different datasets make these methods become
particularly sensitive to hyper-parameter values and manifest poor
generalization capability. In this paper, we address the aforementioned
challenges by proposing a novel query-based method, termed as 3D-QueryIS, which
is detector-free, semantic segmentation-free, and cluster-free. Specifically,
we propose to generate representative points in an implicit manner, and use
them together with the initial queries to generate the informative instance
queries. Then, the class and binary instance mask predictions can be produced
by simply applying MLP layers on top of the instance queries and the extracted
point cloud embeddings. Thus, our 3D-QueryIS is free from the accumulated
errors caused by the inter-task dependencies. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness and efficiency of our proposed
3D-QueryIS method.
</p></li>
</ul>

<h3>Title: aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception. (arXiv:2211.09445v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09445">http://arxiv.org/abs/2211.09445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09445] aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception](http://arxiv.org/abs/2211.09445)</code></li>
<li>Summary: <p>Autonomous driving is a popular research area within the computer vision
research community. Since autonomous vehicles are highly safety-critical,
ensuring robustness is essential for real-world deployment. While several
public multimodal datasets are accessible, they mainly comprise two sensor
modalities (camera, LiDAR) which are not well suited for adverse weather. In
addition, they lack far-range annotations, making it harder to train neural
networks that are the base of a highway assistant function of an autonomous
vehicle. Therefore, we introduce a multimodal dataset for robust autonomous
driving with long-range perception. The dataset consists of 176 scenes with
synchronized and calibrated LiDAR, camera, and radar sensors covering a
360-degree field of view. The collected data was captured in highway, urban,
and suburban areas during daytime, night, and rain and is annotated with 3D
bounding boxes with consistent identifiers across frames. Furthermore, we
trained unimodal and multimodal baseline models for 3D object detection. Data
are available at \url{https://github.com/aimotive/aimotive_dataset}.
</p></li>
</ul>

<h3>Title: ImLiDAR: Cross-Sensor Dynamic Message Propagation Network for 3D Object Detection. (arXiv:2211.09518v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09518">http://arxiv.org/abs/2211.09518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09518] ImLiDAR: Cross-Sensor Dynamic Message Propagation Network for 3D Object Detection](http://arxiv.org/abs/2211.09518)</code></li>
<li>Summary: <p>LiDAR and camera, as two different sensors, supply geometric (point clouds)
and semantic (RGB images) information of 3D scenes. However, it is still
challenging for existing methods to fuse data from the two cross sensors,
making them complementary for quality 3D object detection (3OD). We propose
ImLiDAR, a new 3OD paradigm to narrow the cross-sensor discrepancies by
progressively fusing the multi-scale features of camera Images and LiDAR point
clouds. ImLiDAR enables to provide the detection head with cross-sensor yet
robustly fused features. To achieve this, two core designs exist in ImLiDAR.
First, we propose a cross-sensor dynamic message propagation module to combine
the best of the multi-scale image and point features. Second, we raise a direct
set prediction problem that allows designing an effective set-based detector to
tackle the inconsistency of the classification and localization confidences,
and the sensitivity of hand-tuned hyperparameters. Besides, the novel set-based
detector can be detachable and easily integrated into various detection
networks. Comparisons on both the KITTI and SUN-RGBD datasets show clear visual
and numerical improvements of our ImLiDAR over twenty-three state-of-the-art
3OD methods.
</p></li>
</ul>

<h3>Title: CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers. (arXiv:2211.09643v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09643">http://arxiv.org/abs/2211.09643</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09643] CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers](http://arxiv.org/abs/2211.09643)</code></li>
<li>Summary: <p>When considering post-training quantization, prior work has typically focused
on developing a mixed precision scheme or learning the best way to partition a
network for quantization. In our work, CPT-V, we look at a general way to
improve the accuracy of networks that have already been quantized, simply by
perturbing the quantization scales. Borrowing the idea of contrastive loss from
self-supervised learning, we find a robust way to jointly minimize a loss
function using just 1,000 calibration images. In order to determine the best
performing quantization scale, CPT-V contrasts the features of quantized and
full precision models in a self-supervised fashion.
</p></li>
</ul>

<p>Unlike traditional reconstruction-based loss functions, the use of a
contrastive loss function not only rewards similarity between the quantized and
full precision outputs but also helps in distinguishing the quantized output
from other outputs within a given batch. In addition, in contrast to prior
works, CPT-V proposes a block-wise evolutionary search to minimize a global
contrastive loss objective, allowing for accuracy improvement of existing
vision transformer (ViT) quantization schemes. For example, CPT-V improves the
top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for
3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a
variety of other ViT architectures further demonstrate its robustness in
extreme quantization scenarios. Our code is available at <link>.
</p>

<h3>Title: Assessing Neural Network Robustness via Adversarial Pivotal Tuning. (arXiv:2211.09782v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09782">http://arxiv.org/abs/2211.09782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09782] Assessing Neural Network Robustness via Adversarial Pivotal Tuning](http://arxiv.org/abs/2211.09782)</code></li>
<li>Summary: <p>The ability to assess the robustness of image classifiers to a diverse set of
manipulations is essential to their deployment in the real world. Recently,
semantic manipulations of real images have been considered for this purpose, as
they may not arise using standard adversarial settings. However, such semantic
manipulations are often limited to style, color or attribute changes. While
expressive, these manipulations do not consider the full capacity of a
pretrained generator to affect adversarial image manipulations. In this work,
we aim at leveraging the full capacity of a pretrained image generator to
generate highly detailed, diverse and photorealistic image manipulations.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). APT first finds a pivot latent space
input to a pretrained generator that best reconstructs an input image. It then
adjusts the weights of the generator to create small, but semantic,
manipulations which fool a pretrained classifier. Crucially, APT changes both
the input and the weights of the pretrained generator, while preserving its
expressive latent editing capability, thus allowing the use of its full
capacity in creating semantic adversarial manipulations. We demonstrate that
APT generates a variety of semantic image manipulations, which preserve the
input image class, but which fool a variety of pretrained classifiers. We
further demonstrate that classifiers trained to be robust to other robustness
benchmarks, are not robust to our generated manipulations and propose an
approach to improve the robustness towards our generated manipulations. Code
available at: https://captaine.github.io/apt/
</p></li>
</ul>

<h3>Title: ConNER: Consistency Training for Cross-lingual Named Entity Recognition. (arXiv:2211.09394v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09394">http://arxiv.org/abs/2211.09394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09394] ConNER: Consistency Training for Cross-lingual Named Entity Recognition](http://arxiv.org/abs/2211.09394)</code></li>
<li>Summary: <p>Cross-lingual named entity recognition (NER) suffers from data scarcity in
the target languages, especially under zero-shot settings. Existing
translate-train or knowledge distillation methods attempt to bridge the
language gap, but often introduce a high level of noise. To solve this problem,
consistency training methods regularize the model to be robust towards
perturbations on data or hidden states. However, such methods are likely to
violate the consistency hypothesis, or mainly focus on coarse-grain
consistency. We propose ConNER as a novel consistency training framework for
cross-lingual NER, which comprises of: (1) translation-based consistency
training on unlabeled target-language data, and (2) dropoutbased consistency
training on labeled source-language data. ConNER effectively leverages
unlabeled target-language data and alleviates overfitting on the source
language to enhance the cross-lingual adaptability. Experimental results show
our ConNER achieves consistent improvement over various baseline methods.
</p></li>
</ul>

<h3>Title: UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning. (arXiv:2211.09783v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09783">http://arxiv.org/abs/2211.09783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09783] UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning](http://arxiv.org/abs/2211.09783)</code></li>
<li>Summary: <p>The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{https://github.com/microsoft/UniSumm}.
</p></li>
</ul>

<h3>Title: Introduction to Online Nonstochastic Control. (arXiv:2211.09619v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09619">http://arxiv.org/abs/2211.09619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09619] Introduction to Online Nonstochastic Control](http://arxiv.org/abs/2211.09619)</code></li>
<li>Summary: <p>This text presents an introduction to an emerging paradigm in control of
dynamical systems and differentiable reinforcement learning called online
nonstochastic control. The new approach applies techniques from online convex
optimization and convex relaxations to obtain new methods with provable
guarantees for classical settings in optimal and robust control.
</p></li>
</ul>

<p>The primary distinction between online nonstochastic control and other
frameworks is the objective. In optimal control, robust control, and other
control methodologies that assume stochastic noise, the goal is to perform
comparably to an offline optimal strategy. In online nonstochastic control,
both the cost functions as well as the perturbations from the assumed dynamical
model are chosen by an adversary. Thus the optimal policy is not defined a
priori. Rather, the target is to attain low regret against the best policy in
hindsight from a benchmark class of policies.
</p>
<p>This objective suggests the use of the decision making framework of online
convex optimization as an algorithmic methodology. The resulting methods are
based on iterative mathematical optimization algorithms, and are accompanied by
finite-time regret and computational complexity guarantees.
</p>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Longitudinal thermal imaging for scalable non-residential HVAC and occupant behaviour characterization. (arXiv:2211.09288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09288">http://arxiv.org/abs/2211.09288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09288] Longitudinal thermal imaging for scalable non-residential HVAC and occupant behaviour characterization](http://arxiv.org/abs/2211.09288)</code></li>
<li>Summary: <p>This work presents a study on the characterization of the air-conditioning
(AC) usage pattern of non-residential buildings from thermal images collected
from an urban-scale infrared (IR) observatory. To achieve this first, an image
processing scheme, for cleaning and extraction of the temperature time series
from the thermal images is implemented. To test the accuracy of the thermal
measurements using IR camera, the extracted temperature is compared against the
ground truth surface temperature measurements. It is observed that the
detrended thermal measurements match well with the ground truth surface
temperature measurements. Subsequently, the operational pattern of the
water-cooled systems and window AC units are extracted from the analysis of the
thermal signature. It is observed that for the water-cooled system, the
difference between the rate of change of the window and wall can be used to
extract the operational pattern. While, in the case of the window AC units,
wavelet transform of the AC unit temperature is used to extract the frequency
and time domain information of the AC unit operation. The results of the
analysis are compared against the indoor temperature sensors installed in the
office spaces of the building. It is realized that the accuracy in the
prediction of the operational pattern is highest between 8 pm to 10 am, and it
reduces during the day because of solar radiation and high daytime temperature.
Subsequently, a characterization study is conducted for eight window/split AC
units from the thermal image collected during the nighttime. This forms one of
the first studies on the operational behavior of HVAC systems for
non-residential buildings using the longitudinal thermal imaging technique. The
output from this study can be used to better understand the operational and
occupant behavior, without requiring to deploy a large array of sensors in the
building space.
</p></li>
</ul>

<h3>Title: EPCS: Endpoint-based Part-aware Curve Skeleton Extraction for Low-quality Point Clouds. (arXiv:2211.09488v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09488">http://arxiv.org/abs/2211.09488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09488] EPCS: Endpoint-based Part-aware Curve Skeleton Extraction for Low-quality Point Clouds](http://arxiv.org/abs/2211.09488)</code></li>
<li>Summary: <p>The curve skeleton is an important shape descriptor that has been utilized in
various applications in computer graphics, machine vision, and artificial
intelligence. In this study, the endpoint-based part-aware curve skeleton
(EPCS) extraction method for low-quality point clouds is proposed. The novel
random center shift (RCS) method is first proposed for detecting the endpoints
on point clouds. The endpoints are used as the initial seed points for dividing
each part into layers, and then the skeletal points are obtained by computing
the center points of the oriented bounding box (OBB) of the layers.
Subsequently, the skeletal points are connected, thus forming the branches.
Furthermore, the multi-vector momentum-driven (MVMD) method is also proposed
for locating the junction points that connect the branches. Due to the shape
differences between different parts on point clouds, the global topology of the
skeleton is finally optimized by removing the redundant junction points,
re-connecting some branches using the proposed MVMD method, and applying an
interpolation method based on the splitting operator. Consequently, a complete
and smooth curve skeleton is achieved. The proposed EPCS method is compared
with several state-of-the-art methods, and the experimental results verify its
robustness, effectiveness, and efficiency. Furthermore, the skeleton extraction
and model segmentation results on the point clouds of broken Terracotta also
highlight the utility of the proposed method.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Explainable, Domain-Adaptive, and Federated Artificial Intelligence in Medicine. (arXiv:2211.09317v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09317">http://arxiv.org/abs/2211.09317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09317] Explainable, Domain-Adaptive, and Federated Artificial Intelligence in Medicine](http://arxiv.org/abs/2211.09317)</code></li>
<li>Summary: <p>Artificial intelligence (AI) continues to transform data analysis in many
domains. Progress in each domain is driven by a growing body of annotated data,
increased computational resources, and technological innovations. In medicine,
the sensitivity of the data, the complexity of the tasks, the potentially high
stakes, and a requirement of accountability give rise to a particular set of
challenges. In this review, we focus on three key methodological approaches
that address some of the particular challenges in AI-driven medical decision
making. (1) Explainable AI aims to produce a human-interpretable justification
for each output. Such models increase confidence if the results appear
plausible and match the clinicians expectations. However, the absence of a
plausible explanation does not imply an inaccurate model. Especially in highly
non-linear, complex models that are tuned to maximize accuracy, such
interpretable representations only reflect a small portion of the
justification. (2) Domain adaptation and transfer learning enable AI models to
be trained and applied across multiple domains. For example, a classification
task based on images acquired on different acquisition hardware. (3) Federated
learning enables learning large-scale models without exposing sensitive
personal health information. Unlike centralized AI learning, where the
centralized learning machine has access to the entire training data, the
federated learning process iteratively updates models across multiple sites by
exchanging only parameter updates, not personal health data. This narrative
review covers the basic concepts, highlights relevant corner-stone and
state-of-the-art research in the field, and discusses perspectives.
</p></li>
</ul>

<h3>Title: Federated Multilingual Models for Medical Transcript Analysis. (arXiv:2211.09722v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09722">http://arxiv.org/abs/2211.09722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09722] Federated Multilingual Models for Medical Transcript Analysis](http://arxiv.org/abs/2211.09722)</code></li>
<li>Summary: <p>Federated Learning (FL) is a novel machine learning approach that allows the
model trainer to access more data samples, by training the model across
multiple decentralized data sources, while data access constraints are in
place. Such trained models can achieve significantly higher performance beyond
what can be done when trained on a single data source. As part of FL's
promises, none of the training data is ever transmitted to any central
location, ensuring that sensitive data remains local and private. These
characteristics make FL perfectly suited for large-scale applications in
healthcare, where a variety of compliance constraints restrict how data may be
handled, processed, and stored. Despite the apparent benefits of federated
learning, the heterogeneity in the local data distributions pose significant
challenges, and such challenges are even more pronounced in the case of
multilingual data providers. In this paper we present a federated learning
system for training a large-scale multi-lingual model suitable for fine-tuning
on downstream tasks such as medical entity tagging. Our work represents one of
the first such production-scale systems, capable of training across multiple
highly heterogeneous data providers, and achieving levels of accuracy that
could not be otherwise achieved by using central training with public data.
Finally, we show that the global model performance can be further improved by a
training step performed locally.
</p></li>
</ul>

<h3>Title: Personalized Federated Learning for Multi-task Fault Diagnosis of Rotating Machinery. (arXiv:2211.09406v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09406">http://arxiv.org/abs/2211.09406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09406] Personalized Federated Learning for Multi-task Fault Diagnosis of Rotating Machinery](http://arxiv.org/abs/2211.09406)</code></li>
<li>Summary: <p>Intelligent fault diagnosis is essential to safe operation of machinery.
However, due to scarce fault samples and data heterogeneity in field machinery,
deep learning based diagnosis methods are prone to over-fitting with poor
generalization ability. To solve the problem, this paper proposes a
personalized federated learning framework, enabling multi-task fault diagnosis
method across multiple factories in a privacypreserving manner. Firstly,
rotating machines from different factories with similar vibration feature data
are categorized into machine groups using a federated clustering method. Then,
a multi-task deep learning model based on convolutional neural network is
constructed to diagnose the multiple faults of machinery with heterogeneous
information fusion. Finally, a personalized federated learning framework is
proposed to solve data heterogeneity across different machines using adaptive
hierarchical aggregation strategy. The case study on collected data from real
machines verifies the effectiveness of the proposed framework. The result shows
that the diagnosis accuracy could be improved significantly using the proposed
personalized federated learning, especially for those machines with scarce
fault samples.
</p></li>
</ul>

<h3>Title: FedFA: Federated Learning with Feature Anchors to Align Feature and Classifier for Heterogeneous Data. (arXiv:2211.09299v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09299">http://arxiv.org/abs/2211.09299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09299] FedFA: Federated Learning with Feature Anchors to Align Feature and Classifier for Heterogeneous Data](http://arxiv.org/abs/2211.09299)</code></li>
<li>Summary: <p>Federated learning allows multiple clients to collaboratively train a model
without exchanging their data, thus preserving data privacy. Unfortunately, it
suffers significant performance degradation under heterogeneous data at
clients. Common solutions in local training involve designing a specific
auxiliary loss to regularize weight divergence or feature inconsistency.
However, we discover that these approaches fall short of the expected
performance because they ignore the existence of a vicious cycle between
classifier divergence and feature mapping inconsistency across clients, such
that client models are updated in inconsistent feature space with diverged
classifiers. We then propose a simple yet effective framework named Federated
learning with Feature Anchors (FedFA) to align the feature mappings and
calibrate classifier across clients during local training, which allows client
models updating in a shared feature space with consistent classifiers. We
demonstrate that this modification brings similar classifiers and a virtuous
cycle between feature consistency and classifier similarity across clients.
Extensive experiments show that FedFA significantly outperforms the
state-of-the-art federated learning algorithms on various image classification
datasets under label and feature distribution skews.
</p></li>
</ul>

<h3>Title: FedSiam-DA: Dual-aggregated Federated Learning via Siamese Networks under Non-IID Data. (arXiv:2211.09421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09421">http://arxiv.org/abs/2211.09421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09421] FedSiam-DA: Dual-aggregated Federated Learning via Siamese Networks under Non-IID Data](http://arxiv.org/abs/2211.09421)</code></li>
<li>Summary: <p>Federated learning is a distributed learning that allows each client to keep
the original data locally and only upload the parameters of the local model to
the server. Despite federated learning can address data island, it remains
challenging to train with data heterogeneous in a real application. In this
paper, we propose FedSiam-DA, a novel dual-aggregated contrastive federated
learning approach, to personalize both local and global models, under various
settings of data heterogeneity. Firstly, based on the idea of contrastive
learning in the Siamese Network, FedSiam-DA regards the local and global model
as different branches of the Siamese Network during the local training and
controls the update direction of the model by constantly changing model
similarity to personalize the local model. Secondly, FedSiam-DA introduces
dynamic weights based on model similarity for each local model and exercises
the dual-aggregated mechanism to further improve the generalization of the
global model. Moreover, we provide extensive experiments on benchmark datasets,
the results demonstrate that FedSiam-DA achieves outperforming several previous
FL approaches on heterogeneous datasets.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: Interpretable Dimensionality Reduction by Feature Preserving Manifold Approximation and Projection. (arXiv:2211.09321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09321">http://arxiv.org/abs/2211.09321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09321] Interpretable Dimensionality Reduction by Feature Preserving Manifold Approximation and Projection](http://arxiv.org/abs/2211.09321)</code></li>
<li>Summary: <p>Nonlinear dimensionality reduction lacks interpretability due to the absence
of source features in low-dimensional embedding space. We propose an
interpretable method featMAP to preserve source features by tangent space
embedding. The core of our proposal is to utilize local singular value
decomposition (SVD) to approximate the tangent space which is embedded to
low-dimensional space by maintaining the alignment. Based on the embedding
tangent space, featMAP enables the interpretability by locally demonstrating
the source features and feature importance. Furthermore, featMAP embeds the
data points by anisotropic projection to preserve the local similarity and
original density. We apply featMAP to interpreting digit classification, object
detection and MNIST adversarial examples. FeatMAP uses source features to
explicitly distinguish the digits and objects and to explain the
misclassification of adversarial examples. We also compare featMAP with other
state-of-the-art methods on local and global metrics.
</p></li>
</ul>

<h3>Title: Engineering Monosemanticity in Toy Models. (arXiv:2211.09169v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2211.09169">http://arxiv.org/abs/2211.09169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2211.09169] Engineering Monosemanticity in Toy Models](http://arxiv.org/abs/2211.09169)</code></li>
<li>Summary: <p>In some neural networks, individual neurons correspond to natural
``features'' in the input. Such \emph{monosemantic} neurons are of great help
in interpretability studies, as they can be cleanly understood. In this work we
report preliminary attempts to engineer monosemanticity in toy models. We find
that models can be made more monosemantic without increasing the loss by just
changing which local minimum the training process finds. More monosemantic loss
minima have moderate negative biases, and we are able to use this fact to
engineer highly monosemantic models. We are able to mechanistically interpret
these models, including the residual polysemantic neurons, and uncover a simple
yet surprising algorithm. Finally, we find that providing models with more
neurons per layer makes the models more monosemantic, albeit at increased
computational cost. These findings point to a number of new questions and
avenues for engineering monosemanticity, which we intend to study these in
future work.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
