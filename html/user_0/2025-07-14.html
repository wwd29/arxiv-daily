<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-14</h1>
<h3>Title: MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model</h3>
<ul>
<li><strong>Authors: </strong>K. Sahit Reddy, N. Ragavenderan, Vasanth K., Ganesh N. Naik, Vishalakshi Prabhu, Nagaraja G. S</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08013">https://arxiv.org/abs/2507.08013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08013">https://arxiv.org/pdf/2507.08013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08013]] MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model(https://arxiv.org/abs/2507.08013)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent advances in natural language processing (NLP) have been driven bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel at understanding complex texts, but biomedical literature, withits domain-specific terminology, poses challenges that models likeWord2Vec and bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5, despite capturing context, fall short in tasks needingbidirectional understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a pretrained BERT model trained on a large biomedicaldataset and equipped with domain-specific vocabulary that enhances thecomprehension of biomedical terminology. MedicalBERT model is furtheroptimized and fine-tuned to address diverse tasks, including named entityrecognition, relation extraction, question answering, sentence similarity, anddocument classification. Performance metrics such as the F1-score,accuracy, and Pearson correlation are employed to showcase the efficiencyof our model in comparison to other BERT-based models such as BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost of the benchmarks, and surpasses the general-purpose BERT model by5.67% on average across all the tasks evaluated respectively. This work alsounderscores the potential of leveraging pretrained BERT models for medicalNLP tasks, demonstrating the effectiveness of transfer learning techniques incapturing domain-specific information. (PDF) MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model. Available from: this https URL [accessed Jul 06 2025].</li>
</ul>

<h3>Title: Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking</h3>
<ul>
<li><strong>Authors: </strong>Aldan Creo, Raul Castro Fernandez, Manuel Cebrian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08014">https://arxiv.org/abs/2507.08014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08014">https://arxiv.org/pdf/2507.08014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08014]] Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking(https://arxiv.org/abs/2507.08014)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety. We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development. Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.</li>
</ul>

<h3>Title: Mechanistic Indicators of Understanding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pierre Beckmann, Matthieu Queloz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08017">https://arxiv.org/abs/2507.08017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08017">https://arxiv.org/pdf/2507.08017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08017]] Mechanistic Indicators of Understanding in Large Language Models(https://arxiv.org/abs/2507.08017)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent findings in mechanistic interpretability (MI), the field probing the inner workings of Large Language Models (LLMs), challenge the view that these models rely solely on superficial statistics. Here, we offer an accessible synthesis of these findings that doubles as an introduction to MI, all while integrating these findings within a novel theoretical framework for thinking about machine understanding. We argue that LLMs develop internal structures that are functionally analogous to the kind of understanding that consists in seeing connections. To sharpen this idea, we propose a three-tiered conception of machine understanding. First, conceptual understanding emerges when a model forms "features" as directions in latent space, thereby learning the connections between diverse manifestations of something. Second, state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world. Third, principled understanding emerges when a model ceases to rely on a collection of memorized facts and discovers a "circuit" that connects these facts. However, we conclude by exploring the "parallel mechanisms" phenomenon, arguing that while LLMs exhibit forms of understanding, their cognitive architecture remains different from ours, and the debate should shift from whether LLMs understand to how their strange minds work.</li>
</ul>

<h3>Title: Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Nikita Mounier, Parsa Idehpour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08018">https://arxiv.org/abs/2507.08018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08018">https://arxiv.org/pdf/2507.08018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08018]] Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation(https://arxiv.org/abs/2507.08018)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A key challenge for iterative text generation is enabling models to efficiently identify and correct their own errors. We propose Review, Remask, Refine (R3), a relatively simple yet elegant framework that requires no additional model training and can be applied to any pre-trained masked text diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is utilized for the Review of intermediate generated blocks. The framework then translates these PRM scores into a Remask strategy: the lower a block's PRM score, indicating potential mistakes, the greater the proportion of tokens within that block are remasked. Finally, the model is compelled to Refine these targeted segments, focusing its efforts more intensively on specific sub-optimal parts of past generations, leading to improved final output.</li>
</ul>

<h3>Title: Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Aryan Varshney, Venkat Ram Reddy Ganuthula</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08019">https://arxiv.org/abs/2507.08019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08019">https://arxiv.org/pdf/2507.08019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08019]] Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks(https://arxiv.org/abs/2507.08019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates whether large language models (LLMs) exhibit consistent behavior (signal) or random variation (noise) when screening resumes against job descriptions, and how their performance compares to human experts. Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini) across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context) with identical and randomized resumes, benchmarked against three human recruitment experts. Analysis of variance revealed significant mean differences in four of eight LLM-only conditions and consistently significant differences between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts strongly to company context (p < 0.001), Gemini partially (p = 0.038 for Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly from human experts across contexts. Meta-cognition analysis highlighted adaptive weighting patterns that differ markedly from human evaluation approaches. Findings suggest LLMs offer interpretable patterns with detailed prompts but diverge substantially from human judgment, informing their deployment in automated hiring systems.</li>
</ul>

<h3>Title: Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Zhang, Yuxi Li, Kailong Wang, Shuai Yuan, Ling Shi, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08020">https://arxiv.org/abs/2507.08020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08020">https://arxiv.org/pdf/2507.08020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08020]] Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation(https://arxiv.org/abs/2507.08020)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across domains such as healthcare, education, and cybersecurity. However, this openness also introduces significant security risks, particularly through embedding space poisoning, which is a subtle attack vector where adversaries manipulate the internal semantic representations of input data to bypass safety alignment mechanisms. While previous research has investigated universal perturbation methods, the dynamics of LLM safety alignment at the embedding level remain insufficiently understood. Consequently, more targeted and accurate adversarial perturbation techniques, which pose significant threats, have not been adequately studied. In this work, we propose ETTA (Embedding Transformation Toxicity Attenuation), a novel framework that identifies and attenuates toxicity-sensitive dimensions in embedding space via linear transformations. ETTA bypasses model refusal behaviors while preserving linguistic coherence, without requiring model fine-tuning or access to training data. Evaluated on five representative open-source LLMs using the AdvBench benchmark, ETTA achieves a high average attack success rate of 88.61%, outperforming the best baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR on instruction-tuned defenses). These results highlight a critical vulnerability in current alignment strategies and underscore the need for embedding-aware defenses.</li>
</ul>

<h3>Title: Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis</h3>
<ul>
<li><strong>Authors: </strong>Li Li, Yongliang Wu, Jingze Zhu, Jiawei Peng, Jianfei Cai, Xu Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08021">https://arxiv.org/abs/2507.08021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08021">https://arxiv.org/pdf/2507.08021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08021]] Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis(https://arxiv.org/abs/2507.08021)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The evolution of large models has witnessed the emergence of In-Context Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous studies have demonstrated the effectiveness of ICL. Inspired by the success of Large Language Models (LLMs), researchers have developed Large Multimodal Models (LMMs) with ICL capabilities. However, explorations of demonstration configuration for multimodal ICL remain preliminary. Additionally, the controllability of In-Context Examples (ICEs) provides an efficient and cost-effective means to observe and analyze the inference characteristics of LMMs under varying inputs. This paper conducts a comprehensive external and internal investigation of multimodal in-context learning on the image captioning task. Externally, we explore demonstration configuration strategies through three dimensions: shot number, image retrieval, and caption assignment. We employ multiple metrics to systematically and thoroughly evaluate and summarize key findings. Internally, we analyze typical LMM attention characteristics and develop attention-based metrics to quantify model behaviors. We also conduct auxiliary experiments to explore the feasibility of attention-driven model acceleration and compression. We further compare performance variations between LMMs with identical model design and pretraining strategies and explain the differences from the angles of pre-training data features. Our study reveals both how ICEs configuration strategies impact model performance through external experiments and characteristic typical patterns through internal inspection, providing dual perspectives for understanding multimodal ICL in LMMs. Our method of combining external and internal analysis to investigate large models, along with our newly proposed metrics, can be applied to broader research areas.</li>
</ul>

<h3>Title: "Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs</h3>
<ul>
<li><strong>Authors: </strong>W. Russell Neuman, Chad Coleman, Ali Dasdan, Safinah Ali, Manan Shah, Kund Meghani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08027">https://arxiv.org/abs/2507.08027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08027">https://arxiv.org/pdf/2507.08027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08027]] "Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs(https://arxiv.org/abs/2507.08027)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have revealed a consistent liberal orientation in the ethical and political responses generated by most commercial large language models (LLMs), yet the underlying causes and resulting implications remain unclear. This paper systematically investigates the political temperament of seven prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity (Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes Moral Foundations Theory, a dozen established political ideology scales and a new index of current political controversies. We find strong and consistent prioritization of liberal-leaning values, particularly care and fairness, across most models. Further analysis attributes this trend to four overlapping factors: Liberal-leaning training corpora, reinforcement learning from human feedback (RLHF), the dominance of liberal frameworks in academic ethical discourse and safety-driven fine-tuning practices. We also distinguish between political "bias" and legitimate epistemic differences, cautioning against conflating the two. A comparison of base and fine-tuned model pairs reveals that fine-tuning generally increases liberal lean, an effect confirmed through both self-report and empirical testing. We argue that this "liberal tilt" is not a programming error or the personal preference of programmers but an emergent property of training on democratic rights-focused discourse. Finally, we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance philosophical aspiration, reflecting a moral stance unanchored to personal identity or interest. Rather than undermining democratic discourse, this pattern may offer a new lens through which to examine collective reasoning.</li>
</ul>

<h3>Title: A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models</h3>
<ul>
<li><strong>Authors: </strong>Sonali Sharma, Ahmed M. Alaa, Roxana Daneshjou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08030">https://arxiv.org/abs/2507.08030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08030">https://arxiv.org/pdf/2507.08030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08030]] A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models(https://arxiv.org/abs/2507.08030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.</li>
</ul>

<h3>Title: Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding</h3>
<ul>
<li><strong>Authors: </strong>Hong Jia, Shiya Fu, Vassilis Kostakos, Feng Xia, Ting Dang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08031">https://arxiv.org/abs/2507.08031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08031">https://arxiv.org/pdf/2507.08031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08031]] Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding(https://arxiv.org/abs/2507.08031)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of Small Language Models (SLMs) as privacy-preserving alternatives for sensitive applications raises a fundamental question about their inherent understanding capabilities compared to Large Language Models (LLMs). This paper investigates the mental health understanding capabilities of current SLMs through systematic evaluation across diverse classification tasks. Employing zero-shot and few-shot learning paradigms, we benchmark their performance against established LLM baselines to elucidate their relative strengths and limitations in this critical domain. We assess five state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding tasks. Our findings reveal that SLMs achieve mean performance within 2\% of LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot settings), demonstrating notable competence despite orders of magnitude fewer parameters. Both model categories experience similar degradation on multi-class severity tasks (a drop of over 30\%), suggesting that nuanced clinical understanding challenges transcend model scale. Few-shot prompting provides substantial improvements for SLMs (up to 14.6\%), while LLM gains are more variable. Our work highlights the potential of SLMs in mental health understanding, showing they can be effective privacy-preserving tools for analyzing sensitive online text data. In particular, their ability to quickly adapt and specialize with minimal data through few-shot learning positions them as promising candidates for scalable mental health screening tools.</li>
</ul>

<h3>Title: Integrating External Tools with Large Language Models to Improve Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Nripesh Niketan, Hadj Batatia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08034">https://arxiv.org/abs/2507.08034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08034">https://arxiv.org/pdf/2507.08034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08034]] Integrating External Tools with Large Language Models to Improve Accuracy(https://arxiv.org/abs/2507.08034)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper deals with improving querying large language models (LLMs). It is well-known that without relevant contextual information, LLMs can provide poor quality responses or tend to hallucinate. Several initiatives have proposed integrating LLMs with external tools to provide them with up-to-date data to improve accuracy. In this paper, we propose a framework to integrate external tools to enhance the capabilities of LLMs in answering queries in educational settings. Precisely, we develop a framework that allows accessing external APIs to request additional relevant information. Integrated tools can also provide computational capabilities such as calculators or calendars. The proposed framework has been evaluated using datasets from the Multi-Modal Language Understanding (MMLU) collection. The data consists of questions on mathematical and scientific reasoning. Results compared to state-of-the-art language models show that the proposed approach significantly improves performance. Our Athena framework achieves 83% accuracy in mathematical reasoning and 88% in scientific reasoning, substantially outperforming all tested models including GPT-4o, LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline model (LLaMA-Large) achieving only 67% and 79% respectively. These promising results open the way to creating complex computing ecosystems around LLMs to make their use more natural to support various tasks and activities.</li>
</ul>

<h3>Title: CRISP: Complex Reasoning with Interpretable Step-based Plans</h3>
<ul>
<li><strong>Authors: </strong>Matan Vetzler, Koren Lazar, Guy Uziel, Eran Hirsch, Ateret Anaby-Tavor, Leshem Choshen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08037">https://arxiv.org/abs/2507.08037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08037">https://arxiv.org/pdf/2507.08037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08037]] CRISP: Complex Reasoning with Interpretable Step-based Plans(https://arxiv.org/abs/2507.08037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) underscore the need for stronger reasoning capabilities to solve complex problems effectively. While Chain-of-Thought (CoT) reasoning has been a step forward, it remains insufficient for many domains. A promising alternative is explicit high-level plan generation, but existing approaches largely assume that LLMs can produce effective plans through few-shot prompting alone, without additional training. In this work, we challenge this assumption and introduce CRISP (Complex Reasoning with Interpretable Step-based Plans), a multi-domain dataset of high-level plans for mathematical reasoning and code generation. The plans in CRISP are automatically generated and rigorously validated--both intrinsically, using an LLM as a judge, and extrinsically, by evaluating their impact on downstream task performance. We demonstrate that fine-tuning a small model on CRISP enables it to generate higher-quality plans than much larger models using few-shot prompting, while significantly outperforming Chain-of-Thought reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning on one domain improves plan generation in the other, highlighting the generalizability of learned planning capabilities.</li>
</ul>

<h3>Title: Towards Evaluating Robustness of Prompt Adherence in Text to Image Models</h3>
<ul>
<li><strong>Authors: </strong>Sujith Vemishetty, Advitiya Arora, Anupama Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08039">https://arxiv.org/abs/2507.08039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08039">https://arxiv.org/pdf/2507.08039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08039]] Towards Evaluating Robustness of Prompt Adherence in Text to Image Models(https://arxiv.org/abs/2507.08039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The advancements in the domain of LLMs in recent years have surprised many, showcasing their remarkable capabilities and diverse applications. Their potential applications in various real-world scenarios have led to significant research on their reliability and effectiveness. On the other hand, multimodal LLMs and Text-to-Image models have only recently gained prominence, especially when compared to text-only LLMs. Their reliability remains constrained due to insufficient research on assessing their performance and robustness. This paper aims to establish a comprehensive evaluation framework for Text-to-Image models, concentrating particularly on their adherence to prompts. We created a novel dataset that aimed to assess the robustness of these models in generating images that conform to the specified factors of variation in the input text prompts. Our evaluation studies present findings on three variants of Stable Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro 1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions generated by the gpt-4o model for our ground-truth images, which are then used to generate artificial images by passing these descriptions to the Text-to-Image models. We then pass these generated images again through gpt-4o using the same system prompt and compare the variation between the two descriptions. Our results reveal that these models struggle to create simple binary images with only two factors of variation: a simple geometric shape and its location. We also show, using pre-trained VAEs on our dataset, that they fail to generate images that follow our input dataset distribution.</li>
</ul>

<h3>Title: Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing</h3>
<ul>
<li><strong>Authors: </strong>Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08045">https://arxiv.org/abs/2507.08045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08045">https://arxiv.org/pdf/2507.08045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08045]] Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing(https://arxiv.org/abs/2507.08045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation. We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.</li>
</ul>

<h3>Title: A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters</h3>
<ul>
<li><strong>Authors: </strong>Rolando A.Hernandez-Hernandez, Adrian Rubio-Solis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08047">https://arxiv.org/abs/2507.08047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08047">https://arxiv.org/pdf/2507.08047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08047]] A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters(https://arxiv.org/abs/2507.08047)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to be an effective technique for the classification of different natural signals such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder (ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active image classification and applied to Unmanned Aerial Vehicles (UAVs). The proposed methodology is a hierarchical ELM learning framework that consists of two main phases: 1) self-taught feature extraction and 2) supervised feature classification. First, unsupervised multilayer feature encoding is achieved by stacking a number of ELM-AEs, in which input data is projected into a number of high-level representations. At the second phase, the final features are classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with a fast output reduction layer based on the SC algorithm; an improved version of the algorithm Center of Sets Type Reducer without Sorting Requirement (COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments for the classification of images are suggested. First, the HML-ELM is applied to solve a number of benchmark problems for image classification. Secondly, a number of real experiments to the active classification and transport of four different objects between two predefined locations using a UAV is implemented. Experiments demonstrate that the proposed HML-ELM delivers a superior efficiency compared to other similar methodologies such as ML-ELM, Multilayer Fuzzy Extreme Learning Machine (ML-FELM) and ELM.</li>
</ul>

<h3>Title: An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Ming Wang, Zhaoyang Duan, Dong Xue, Fangzhou Liu, Zhongheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08050">https://arxiv.org/abs/2507.08050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08050">https://arxiv.org/pdf/2507.08050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08050]] An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis(https://arxiv.org/abs/2507.08050)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>The labor-intensive nature of medical data annotation presents a significant challenge for respiratory disease diagnosis, resulting in a scarcity of high-quality labeled datasets in resource-constrained settings. Moreover, patient privacy concerns complicate the direct sharing of local medical data across institutions, and existing centralized data-driven approaches, which rely on amounts of available data, often compromise data privacy. This study proposes a federated few-shot learning framework with privacy-preserving mechanisms to address the issues of limited labeled data and privacy protection in diagnosing respiratory diseases. In particular, a meta-stochastic gradient descent algorithm is proposed to mitigate the overfitting problem that arises from insufficient data when employing traditional gradient descent methods for neural network training. Furthermore, to ensure data privacy against gradient leakage, differential privacy noise from a standard Gaussian distribution is integrated into the gradients during the training of private models with local data, thereby preventing the reconstruction of medical images. Given the impracticality of centralizing respiratory disease data dispersed across various medical institutions, a weighted average algorithm is employed to aggregate local diagnostic models from different clients, enhancing the adaptability of a model across diverse scenarios. Experimental results show that the proposed method yields compelling results with the implementation of differential privacy, while effectively diagnosing respiratory diseases using data from different structures, categories, and distributions.</li>
</ul>

<h3>Title: Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging</h3>
<ul>
<li><strong>Authors: </strong>Mazen Ali, António Pereira, Fabio Gentile, Aser Cortines, Sam Mugel, Román Orús, Stelios P. Neophytides, Michalis Mavrovouniotis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08052">https://arxiv.org/abs/2507.08052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08052">https://arxiv.org/pdf/2507.08052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08052]] Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging(https://arxiv.org/abs/2507.08052)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Cloud and cloud shadow masking is a crucial preprocessing step in hyperspectral satellite imaging, enabling the extraction of high-quality, analysis-ready data. This study evaluates various machine learning approaches, including gradient boosting methods such as XGBoost and LightGBM as well as convolutional neural networks (CNNs). All boosting and CNN models achieved accuracies exceeding 93%. Among the investigated models, the CNN with feature reduction emerged as the most efficient, offering a balance of high accuracy, low storage requirements, and rapid inference times on both CPUs and GPUs. Variations of this version, with only up to 597 trainable parameters, demonstrated the best trade-off in terms of deployment feasibility, accuracy, and computational efficiency. These results demonstrate the potential of lightweight artificial intelligence (AI) models for real-time hyperspectral image processing, supporting the development of on-board satellite AI systems for space-based applications.</li>
</ul>

<h3>Title: The relative importance of being Gaussian</h3>
<ul>
<li><strong>Authors: </strong>F. Alberto Grünbaum, Tondgi Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08059">https://arxiv.org/abs/2507.08059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08059">https://arxiv.org/pdf/2507.08059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08059]] The relative importance of being Gaussian(https://arxiv.org/abs/2507.08059)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The remarkable results for denoising in computer vision using diffusion models given in \cite{SDWMG,HJA,HHG} yield a robust mathematical justification for algorithms based on crucial properties of a sequence of Gaussian independent $N(0,1)$ random variables. In particular the derivations use the fact that a Gaussian distribution is determined by its mean and variance and that the sum of two Gaussians is another Gaussian. \bigskip The issue raised in this short note is the following: suppose we use the algorithm without any changes but replace the nature of the noise and use, for instance, uniformly distributed noise or noise with a Beta distribution, or noise which is a random superposition of two Gaussians with very different variances. One could, of course, try to modify the algorithm keeping in mind the nature of the noise, but this is not what we do. Instead we study the performance of the algorithm when used with noise that is very far in nature from the Gaussian case, where it is designed to work well. Usually these algorithms are implemented on very powerful computers. Our experiments are all carried out on a small laptop and for the smallest possible image size. Exploring how our observations are confirmed or changed when dealing in different situations remains an interesting challenge.</li>
</ul>

<h3>Title: Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions</h3>
<ul>
<li><strong>Authors: </strong>Simon Matrenok, Skander Moalla, Caglar Gulcehre</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08068">https://arxiv.org/abs/2507.08068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08068">https://arxiv.org/pdf/2507.08068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08068]] Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions(https://arxiv.org/abs/2507.08068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models with pointwise absolute rewards has so far required online, on-policy algorithms such as PPO and GRPO. In contrast, simpler methods that can leverage offline or off-policy data, such as DPO and REBEL, are limited to learning from preference pairs or relative signals. To bridge this gap, we introduce \emph{Quantile Reward Policy Optimization} (QRPO), which learns from pointwise absolute rewards while preserving the simplicity and offline applicability of DPO-like methods. QRPO uses quantile rewards to enable regression to the closed-form solution of the KL-regularized RL objective. This reward yields an analytically tractable partition function, removing the need for relative signals to cancel this term. Moreover, QRPO scales with increased compute to estimate quantile rewards, opening a new dimension for pre-computation scaling. Empirically, QRPO consistently achieves top performance on chat and coding evaluations -- reward model scores, AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse datasets and 8B-scale models. Finally, we find that training with robust rewards instead of converting them to preferences induces less length bias.</li>
</ul>

<h3>Title: Low-rank Momentum Factorization for Memory Efficient Training</h3>
<ul>
<li><strong>Authors: </strong>Pouria Mahdavinia, Mehrdad Mahdavi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08091">https://arxiv.org/abs/2507.08091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08091">https://arxiv.org/pdf/2507.08091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08091]] Low-rank Momentum Factorization for Memory Efficient Training(https://arxiv.org/abs/2507.08091)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large foundation models presents significant memory challenges due to stateful optimizers like AdamW, often requiring several times more GPU memory than inference. While memory-efficient methods like parameter-efficient fine-tuning (e.g., LoRA) and optimizer state compression exist, recent approaches like GaLore bridge these by using low-rank gradient projections and subspace moment accumulation. However, such methods may struggle with fixed subspaces or computationally costly offline resampling (e.g., requiring full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which maintains a dynamically updated low-rank SVD representation of the first-order momentum, closely approximating its full-rank counterpart throughout training. This factorization enables a memory-efficient fine-tuning method that adaptively updates the optimization subspace at each iteration. Crucially, MoFaSGD leverages the computed low-rank momentum factors to perform efficient spectrally normalized updates, offering an alternative to subspace moment accumulation. We establish theoretical convergence guarantees for MoFaSGD, proving it achieves an optimal rate for non-convex stochastic optimization under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness on large language model alignment benchmarks, achieving a competitive trade-off between memory reduction (comparable to LoRA) and performance compared to state-of-the-art low-rank optimization methods. Our implementation is available at this https URL.</li>
</ul>

<h3>Title: An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images</h3>
<ul>
<li><strong>Authors: </strong>Babak Memar, Luigi Russo, Silvia Liberata Ullo, Paolo Gamba</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08096">https://arxiv.org/abs/2507.08096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08096">https://arxiv.org/pdf/2507.08096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08096]] An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images(https://arxiv.org/abs/2507.08096)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate estimation of building heights using very high resolution (VHR) synthetic aperture radar (SAR) imagery is crucial for various urban applications. This paper introduces a Deep Learning (DL)-based methodology for automated building height estimation from single VHR COSMO-SkyMed images: an object-based regression approach based on bounding box detection followed by height estimation. This model was trained and evaluated on a unique multi-continental dataset comprising eight geographically diverse cities across Europe, North and South America, and Asia, employing a cross-validation strategy to explicitly assess out-of-distribution (OOD) generalization. The results demonstrate highly promising performance, particularly on European cities where the model achieves a Mean Absolute Error (MAE) of approximately one building story (2.20 m in Munich), significantly outperforming recent state-of-the-art methods in similar OOD scenarios. Despite the increased variability observed when generalizing to cities in other continents, particularly in Asia with its distinct urban typologies and prevalence of high-rise structures, this study underscores the significant potential of DL for robust cross-city and cross-continental transfer learning in building height estimation from single VHR SAR data.</li>
</ul>

<h3>Title: GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Walter, Hannah Bast</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08107">https://arxiv.org/abs/2507.08107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08107">https://arxiv.org/pdf/2507.08107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08107]] GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs(https://arxiv.org/abs/2507.08107)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose a new approach for generating SPARQL queries on RDF knowledge graphs from natural language questions or keyword queries, using a large language model. Our approach does not require fine-tuning. Instead, it uses the language model to explore the knowledge graph by strategically executing SPARQL queries and searching for relevant IRIs and literals. We evaluate our approach on a variety of benchmarks (for knowledge graphs of different kinds and sizes) and language models (of different scales and types, commercial as well as open-source) and compare it with existing approaches. On Wikidata we reach state-of-the-art results on multiple benchmarks, despite the zero-shot setting. On Freebase we come close to the best few-shot methods. On other, less commonly evaluated knowledge graphs and benchmarks our approach also performs well overall. We conduct several additional studies, like comparing different ways of searching the graphs, incorporating a feedback mechanism, or making use of few-shot examples.</li>
</ul>

<h3>Title: Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing</h3>
<ul>
<li><strong>Authors: </strong>Reilly Raab, Mike Parker, Dan Nally, Sadie Montgomery, Anastasia Bernat, Sai Munikoti, Sameera Horawalavithana</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08109">https://arxiv.org/abs/2507.08109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08109">https://arxiv.org/pdf/2507.08109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08109]] Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing(https://arxiv.org/abs/2507.08109)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, explainability</a></li>
<li><strong>Abstract: </strong>The advent of language models (LMs) has the potential to dramatically accelerate tasks that may be cast to text-processing; however, real-world adoption is hindered by concerns regarding safety, explainability, and bias. How can we responsibly leverage LMs in a transparent, auditable manner -- minimizing risk and allowing human experts to focus on informed decision-making rather than data-processing or prompt engineering? In this work, we propose a framework for declaring statically typed, LM-powered subroutines (i.e., callable, function-like procedures) for use within conventional asynchronous code -- such that sparse feedback from human experts is used to improve the performance of each subroutine online (i.e., during use). In our implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and data-dependencies) are recorded and exposed to audit on demand. We package this framework as a library to support its adoption and continued development. While this framework may be applicable across several real-world decision workflows (e.g., in healthcare and legal fields), we evaluate it in the context of public comment processing as mandated by the 1969 National Environmental Protection Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an application that compiles, organizes, and summarizes a corpus of public commentary submitted in response to a project requiring environmental review. We quantitatively evaluate the application by comparing its outputs (when operating without human feedback) to historical ``ground-truth'' data as labelled by human annotators during the preparation of official environmental impact statements.</li>
</ul>

<h3>Title: Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores</h3>
<ul>
<li><strong>Authors: </strong>Vivek Chari, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08143">https://arxiv.org/abs/2507.08143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08143">https://arxiv.org/pdf/2507.08143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08143]] Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores(https://arxiv.org/abs/2507.08143)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. Unfortunately the ability to use long contexts in generation is complicated by the large memory requirement of the KV cache, which scales linearly with the context length. This memory footprint is often the dominant resource bottleneck in real-world deployments, limiting throughput and increasing serving cost. One way to address this is by compressing the KV cache, which can be done either with knowledge of the question being asked (query-aware) or without knowledge of the query (query-agnostic). We present Compactor, a parameter-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 1/2 the tokens in both synthetic and real-world context tasks, with minimal computational overhead. We further introduce a procedure for context-calibrated compression, which allows one to infer the maximum compression ratio a given context can support. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 63%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families.</li>
</ul>

<h3>Title: Distilling Empathy from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Henry J. Xie, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08151">https://arxiv.org/abs/2507.08151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08151">https://arxiv.org/pdf/2507.08151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08151]] Distilling Empathy from Large Language Models(https://arxiv.org/abs/2507.08151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The distillation of knowledge from Large Language Models (LLMs) into Smaller Language Models (SLMs), preserving the capabilities and performance of LLMs while reducing model size, has played a key role in the proliferation of LLMs. Because SLMs are considerably smaller than LLMs, they are often utilized in domains where human interaction is frequent but resources are highly constrained, e.g., smart phones. Therefore, it is crucial to ensure that empathy, a fundamental aspect of positive human interactions, already instilled into LLMs, is retained by SLMs after distillation. In this paper, we develop a comprehensive approach for effective empathy distillation from LLMs into SLMs. Our approach features a two-step fine-tuning process that fully leverages datasets of empathetic dialogue responses distilled from LLMs. We explore several distillation methods beyond basic direct prompting and propose four unique sets of prompts for targeted empathy improvement to significantly enhance the empathy distillation process. Our evaluations demonstrate that SLMs fine-tuned through the two-step fine-tuning process with distillation datasets enhanced by the targeted empathy improvement prompts significantly outperform the base SLM at generating empathetic responses with a win rate of 90%. Our targeted empathy improvement prompts substantially outperform the basic direct prompting with a 10% improvement in win rate.</li>
</ul>

<h3>Title: ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction</h3>
<ul>
<li><strong>Authors: </strong>Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Rajiv Ramnath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08153">https://arxiv.org/abs/2507.08153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08153">https://arxiv.org/pdf/2507.08153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08153]] ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction(https://arxiv.org/abs/2507.08153)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Traffic accidents are rare, yet high-impact events that require long-context multimodal reasoning for accurate risk forecasting. In this paper, we introduce ALCo-FM, a unified adaptive long-context foundation model that computes a volatility pre-score to dynamically select context windows for input data and encodes and fuses these multimodal data via shallow cross attention. Following a local GAT layer and a BigBird-style sparse global transformer over H3 hexagonal grids, coupled with Monte Carlo dropout for confidence, the model yields superior, well-calibrated predictions. Trained on data from 15 US cities with a class-weighted loss to counter label imbalance, and fine-tuned with minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in large-scale urban risk prediction. Code and dataset are available at: this https URL</li>
</ul>

<h3>Title: Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries</h3>
<ul>
<li><strong>Authors: </strong>Marika Swanberg, Meenatchi Sundaram Muthu Selva Annamalai, Jamie Hayes, Borja Balle, Adam Smith</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08158">https://arxiv.org/abs/2507.08158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08158">https://arxiv.org/pdf/2507.08158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08158]] Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries(https://arxiv.org/abs/2507.08158)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Differential Privacy (DP) is a family of definitions that bound the worst-case privacy leakage of a mechanism. One important feature of the worst-case DP guarantee is it naturally implies protections against adversaries with less prior information, more sophisticated attack goals, and complex measures of a successful attack. However, the analytical tradeoffs between the adversarial model and the privacy protections conferred by DP are not well understood thus far. To that end, this work sheds light on what the worst-case guarantee of DP implies about the success of attackers that are more representative of real-world privacy risks. In this paper, we present a single flexible framework that generalizes and extends the patchwork of bounds on DP mechanisms found in prior work. Our framework allows us to compute high-probability guarantees for DP mechanisms on a large family of natural attack settings that previous bounds do not capture. One class of such settings is the approximate reconstruction of multiple individuals' data, such as inferring nearly entire columns of a tabular data set from noisy marginals and extracting sensitive information from DP-trained language models. We conduct two empirical case studies to illustrate the versatility of our bounds and compare them to the success of state-of-the-art attacks. Specifically, we study attacks that extract non-uniform PII from a DP-trained language model, as well as multi-column reconstruction attacks where the adversary has access to some columns in the clear and attempts to reconstruct the remaining columns for each person's record. We find that the absolute privacy risk of attacking non-uniform data is highly dependent on the adversary's prior probability of success. Our high probability bounds give us a nuanced understanding of the privacy leakage of DP mechanisms in a variety of previously understudied attack settings.</li>
</ul>

<h3>Title: Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Frederick Shpilevskiy, Saiyue Lyu, Krishnamurthy Dj Dvijotham, Mathias Lécuyer, Pierre-André Noël</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08163">https://arxiv.org/abs/2507.08163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08163">https://arxiv.org/pdf/2507.08163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08163]] Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion(https://arxiv.org/abs/2507.08163)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, diffusion</a></li>
<li><strong>Abstract: </strong>We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the predictions of a vision model against adversarial examples, while adapting to the input. Our key insight is to reinterpret a guided denoising diffusion model as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms refining a pure noise sample into an image. We show that these adaptive mechanisms can be composed through a GDP privacy filter to analyze the end-to-end robustness of the guided denoising process, yielding a provable certification that extends the adaptive randomized smoothing analysis. We demonstrate that our design, under a specific guiding strategy, can improve both certified accuracy and standard accuracy on ImageNet for an $\ell_2$ threat model.</li>
</ul>

<h3>Title: GPUHammer: Rowhammer Attacks on GPU Memories are Practical</h3>
<ul>
<li><strong>Authors: </strong>Chris S. Lin, Joyce Qu, Gururaj Saileshwar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08166">https://arxiv.org/abs/2507.08166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08166">https://arxiv.org/pdf/2507.08166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08166]] GPUHammer: Rowhammer Attacks on GPU Memories are Practical(https://arxiv.org/abs/2507.08166)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Rowhammer is a read disturbance vulnerability in modern DRAM that causes bit-flips, compromising security and reliability. While extensively studied on Intel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR memories, critical for emerging machine learning applications, remains unexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary mapping of physical memory to GDDR banks and rows, (2) high memory latency and faster refresh rates that hinder effective hammering, and (3) proprietary mitigations in GDDR memories, difficult to reverse-engineer without FPGA-based test platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA GPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer GDDR DRAM row mappings, and employs GPU-specific memory access optimizations to amplify hammering intensity and bypass mitigations. Thus, we demonstrate the first successful Rowhammer attack on a discrete GPU, injecting up to 8 bit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also show how an attacker can use these to tamper with ML models, causing significant accuracy drops (up to 80%).</li>
</ul>

<h3>Title: Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors</h3>
<ul>
<li><strong>Authors: </strong>Md. Saif Hassan Onim, Travis S. Humble, Himanshu Thapliyal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08175">https://arxiv.org/abs/2507.08175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08175">https://arxiv.org/pdf/2507.08175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08175]] Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors(https://arxiv.org/abs/2507.08175)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>We investigate the feasibility of inferring emotional states exclusively from physiological signals, thereby presenting a privacy-preserving alternative to conventional facial recognition techniques. We conduct a performance comparison of classical machine learning algorithms and hybrid quantum machine learning (QML) methods with a quantum kernel-based model. Our results indicate that the quantum-enhanced SVM surpasses classical counterparts in classification performance across all emotion categories, even when trained on limited datasets. The F1 scores over all classes are over 80% with around a maximum of 36% improvement in the recall values. The integration of wearable sensor data with quantum machine learning not only enhances accuracy and robustness but also facilitates unobtrusive emotion recognition. This methodology holds promise for populations with impaired communication abilities, such as individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans with Post-Traumatic Stress Disorder (PTSD). The findings establish an early foundation for passive emotional monitoring in clinical and assisted living conditions.</li>
</ul>

<h3>Title: Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Arun Vignesh Malarkkan, Haoyue Bai, Xinyuan Wang, Anjali Kaushik, Dongjie Wang, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08177">https://arxiv.org/abs/2507.08177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08177">https://arxiv.org/pdf/2507.08177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08177]] Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity(https://arxiv.org/abs/2507.08177)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>As cyber-physical systems grow increasingly interconnected and spatially distributed, ensuring their resilience against evolving cyberattacks has become a critical priority. Spatio-Temporal Anomaly detection plays an important role in ensuring system security and operational integrity. However, current data-driven approaches, largely driven by black-box deep learning, face challenges in interpretability, adaptability to distribution shifts, and robustness under evolving system dynamics. In this paper, we advocate for a causal learning perspective to advance anomaly detection in spatially distributed infrastructures that grounds detection in structural cause-effect relationships. We identify and formalize three key directions: causal graph profiling, multi-view fusion, and continual causal graph learning, each offering distinct advantages in uncovering dynamic cause-effect structures across time and space. Drawing on real-world insights from systems such as water treatment infrastructures, we illustrate how causal models provide early warning signals and root cause attribution, addressing the limitations of black-box detectors. Looking ahead, we outline the future research agenda centered on multi-modality, generative AI-driven, and scalable adaptive causal frameworks. Our objective is to lay a new research trajectory toward scalable, adaptive, explainable, and spatially grounded anomaly detection systems. We hope to inspire a paradigm shift in cybersecurity research, promoting causality-driven approaches to address evolving threats in interconnected infrastructures.</li>
</ul>

<h3>Title: CTRLS: Chain-of-Thought Reasoning via Latent State-Transition</h3>
<ul>
<li><strong>Authors: </strong>Junda Wu, Yuxin Xiong, Xintong Li, Zhengmian Hu, Tong Yu, Rui Wang, Xiang Chen, Jingbo Shang, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08182">https://arxiv.org/abs/2507.08182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08182">https://arxiv.org/pdf/2507.08182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08182]] CTRLS: Chain-of-Thought Reasoning via Latent State-Transition(https://arxiv.org/abs/2507.08182)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) reasoning enables large language models (LLMs) to break down complex problems into interpretable intermediate steps, significantly enhancing model transparency and performance in reasoning tasks. However, conventional CoT methods rely on heuristic sampling without structured modeling of reasoning transitions, constraining their ability to systematically explore and discover diverse and effective reasoning trajectories. In this work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov decision process (MDP) with latent state transitions, enabling principled and state-aware exploration via distributional reinforcement learning. By modelling reasoning actions as explicit probability distributions in latent space, our approach explicitly models epistemic uncertainty, facilitating robust exploration of the reasoning space. As part of our framework, we introduce an on-policy reinforcement learning strategy incorporating epsilon-greedy exploration and entropy-based regularization to iteratively refine latent state transitions without requiring additional fine-tuning of the underlying LLM. Theoretical analyses provide evidence lower bounds (ELBO), theoretically grounding our transition-aware modeling of latent reasoning dynamics. Further experiments demonstrate improvements in reasoning accuracy, diversity, and exploration efficiency across benchmark reasoning tasks.</li>
</ul>

<h3>Title: TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs</h3>
<ul>
<li><strong>Authors: </strong>Duygu Nur Yaldiz, Yavuz Faruk Bakman, Sungmin Kang, Alperen Öziş, Hayrettin Eren Yildiz, Mitash Ashish Shah, Zhiqi Huang, Anoop Kumar, Alfy Samuel, Daben Liu, Sai Praneeth Karimireddy, Salman Avestimehr</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08203">https://arxiv.org/abs/2507.08203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08203">https://arxiv.org/pdf/2507.08203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08203]] TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs(https://arxiv.org/abs/2507.08203)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative Large Language Models (LLMs)inevitably produce untruthful responses. Accurately predicting the truthfulness of these outputs is critical, especially in high-stakes settings. To accelerate research in this domain and make truthfulness prediction methods more accessible, we introduce TruthTorchLM an open-source, comprehensive Python library featuring over 30 truthfulness prediction methods, which we refer to as Truth Methods. Unlike existing toolkits such as Guardrails, which focus solely on document-grounded verification, or LM-Polygraph, which is limited to uncertainty-based methods, TruthTorchLM offers a broad and extensible collection of techniques. These methods span diverse tradeoffs in computational cost, access level (e.g., black-box vs white-box), grounding document requirements, and supervision type (self-supervised or supervised). TruthTorchLM is seamlessly compatible with both HuggingFace and LiteLLM, enabling support for locally hosted and API-based models. It also provides a unified interface for generation, evaluation, calibration, and long-form truthfulness prediction, along with a flexible framework for extending the library with new methods. We conduct an evaluation of representative truth methods on three datasets, TriviaQA, GSM8K, and FactScore-Bio. The code is available at this https URL</li>
</ul>

<h3>Title: HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08205">https://arxiv.org/abs/2507.08205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08205">https://arxiv.org/pdf/2507.08205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08205]] HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation(https://arxiv.org/abs/2507.08205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In medical image segmentation, convolutional neural networks (CNNs) and transformers are dominant. For CNNs, given the local receptive fields of convolutional layers, long-range spatial correlations are captured through consecutive convolutions and pooling. However, as the computational cost and memory footprint can be prohibitively large, 3D models can only afford fewer layers than 2D models with reduced receptive fields and abstract levels. For transformers, although long-range correlations can be captured by multi-head attention, its quadratic complexity with respect to input size is computationally demanding. Therefore, either model may require input size reduction to allow more filters and layers for better segmentation. Nevertheless, given their discrete nature, models trained with patch-wise training or image downsampling may produce suboptimal results when applied on higher resolutions. To address this issue, here we propose the resolution-robust HNOSeg-XS architecture. We model image segmentation by learnable partial differential equations through the Fourier neural operator which has the zero-shot super-resolution property. By replacing the Fourier transform by the Hartley transform and reformulating the problem in the frequency domain, we created the HNOSeg-XS model, which is resolution robust, fast, memory efficient, and extremely parameter efficient. When tested on the BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS showed its superior resolution robustness with fewer than 34.7k model parameters. It also achieved the overall best inference time (< 0.24 s) and memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer models.</li>
</ul>

<h3>Title: EvA: Evolutionary Attacks on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Sadegh Akhondzadeh, Soroush H. Zargarbashi, Jimin Cao, Aleksandar Bojchevski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08212">https://arxiv.org/abs/2507.08212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08212">https://arxiv.org/pdf/2507.08212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08212]] EvA: Evolutionary Attacks on Graphs(https://arxiv.org/abs/2507.08212)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Even a slight perturbation in the graph structure can cause a significant drop in the accuracy of graph neural networks (GNNs). Most existing attacks leverage gradient information to perturb edges. This relaxes the attack's optimization problem from a discrete to a continuous space, resulting in solutions far from optimal. It also restricts the adaptability of the attack to non-differentiable objectives. Instead, we introduce a few simple yet effective enhancements of an evolutionary-based algorithm to solve the discrete optimization problem directly. Our Evolutionary Attack (EvA) works with any black-box model and objective, eliminating the need for a differentiable proxy loss. This allows us to design two novel attacks that reduce the effectiveness of robustness certificates and break conformal sets. The memory complexity of our attack is linear in the attack budget. Among our experiments, EvA shows $\sim$11\% additional drop in accuracy on average compared to the best previous attack, revealing significant untapped potential in designing attacks.</li>
</ul>

<h3>Title: SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches</h3>
<ul>
<li><strong>Authors: </strong>Jackson Borchardt, Saul Kato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08223">https://arxiv.org/abs/2507.08223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08223">https://arxiv.org/pdf/2507.08223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08223]] SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches(https://arxiv.org/abs/2507.08223)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present SurfDist, a convolutional neural network architecture for three-dimensional volumetric instance segmentation. SurfDist enables prediction of instances represented as closed surfaces composed of smooth parametric surface patches, specifically bicubic Bézier triangles. SurfDist is a modification of the popular model architecture StarDist-3D which breaks StarDist-3D's coupling of instance parameterization dimension and instance voxel resolution, and it produces predictions which may be upsampled to arbitrarily high resolutions without introduction of voxelization artifacts. For datasets with blob-shaped instances, common in biomedical imaging, SurfDist can outperform StarDist-3D with more compact instance parameterizations. We detail SurfDist's technical implementation and show one synthetic and one real-world dataset for which it outperforms StarDist-3D. These results demonstrate that interpretable instance surface models can be learned effectively alongside instance membership.</li>
</ul>

<h3>Title: Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?</h3>
<ul>
<li><strong>Authors: </strong>KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08232">https://arxiv.org/abs/2507.08232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08232">https://arxiv.org/pdf/2507.08232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08232]] Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?(https://arxiv.org/abs/2507.08232)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used as proxy students in the development of Intelligent Tutoring Systems (ITSs) and in piloting test questions. However, to what extent these proxy students accurately emulate the behavior and characteristics of real students remains an open question. To investigate this, we collected a dataset of 489 items from the National Assessment of Educational Progress (NAEP), covering mathematics and reading comprehension in grades 4, 8, and 12. We then apply an Item Response Theory (IRT) model to position 11 diverse and state-of-the-art LLMs on the same ability scale as real student populations. Our findings reveal that, without guidance, strong general-purpose models consistently outperform the average student at every grade, while weaker or domain-mismatched models may align incidentally. Using grade-enforcement prompts changes models' performance, but whether they align with the average grade-level student remains highly model- and prompt-specific: no evaluated model-prompt pair fits the bill across subjects and grades, underscoring the need for new training and evaluation strategies. We conclude by providing guidelines for the selection of viable proxies based on our findings.</li>
</ul>

<h3>Title: InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems</h3>
<ul>
<li><strong>Authors: </strong>Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Rajiv Ramnath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08235">https://arxiv.org/abs/2507.08235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08235">https://arxiv.org/pdf/2507.08235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08235]] InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems(https://arxiv.org/abs/2507.08235)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Smart buildings generate vast streams of sensor and control data, but facility managers often lack clear explanations for anomalous energy usage. We propose InsightBuild, a two-stage framework that integrates causality analysis with a fine-tuned large language model (LLM) to provide human-readable, causal explanations of energy consumption patterns. First, a lightweight causal inference module applies Granger causality tests and structural causal discovery on building telemetry (e.g., temperature, HVAC settings, occupancy) drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM, fine-tuned on aligned pairs of sensor-level causes and textual explanations, receives as input the detected causal relations and generates concise, actionable explanations. We evaluate InsightBuild on two real-world datasets (Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth causes for a held-out set of anomalies. Our results demonstrate that combining explicit causal discovery with LLM-based natural language generation yields clear, precise explanations that assist facility managers in diagnosing and mitigating energy inefficiencies.</li>
</ul>

<h3>Title: Data Generation without Function Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hadi Daneshmand, Ashkan Soleymani</a></li>
<li><strong>Subjects: </strong>cs.LG, math-ph, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08239">https://arxiv.org/abs/2507.08239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08239">https://arxiv.org/pdf/2507.08239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08239]] Data Generation without Function Estimation(https://arxiv.org/abs/2507.08239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Estimating the score function (or other population-density-dependent functions) is a fundamental component of most generative models. However, such function estimation is computationally and statistically challenging. Can we avoid function estimation for data generation? We propose an estimation-free generative method: A set of points whose locations are deterministically updated with (inverse) gradient descent can transport a uniform distribution to arbitrary data distribution, in the mean field regime, without function estimation, training neural networks, and even noise injection. The proposed method is built upon recent advances in the physics of interacting particles. We show, both theoretically and experimentally, that these advances can be leveraged to develop novel generative methods.</li>
</ul>

<h3>Title: CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry</h3>
<ul>
<li><strong>Authors: </strong>Chandra Sekhar Mukherjee, Joonyoung Bae, Jiapeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08243">https://arxiv.org/abs/2507.08243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08243">https://arxiv.org/pdf/2507.08243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08243]] CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry(https://arxiv.org/abs/2507.08243)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Density and geometry have long served as two of the fundamental guiding principles in clustering algorithm design, with algorithm usually focusing either on the density structure of the data (e.g., HDBSCAN and Density Peak Clustering) or the complexity of underlying geometry (e.g., manifold clustering algorithms). In this paper, we identify and formalize a recurring but often overlooked interaction between distribution and geometry and leverage this insight to design our clustering enhancement framework CoreSPECT (Core Space Projection-based Enhancement of Clustering Techniques). Our framework boosts the performance of simple algorithms like K-Means and GMM by applying them to strategically selected regions, then extending the partial partition to a complete partition for the dataset using a novel neighborhood graph based multi-layer propagation procedure. We apply our framework on 15 datasets from three different domains and obtain consistent and substantial gain in clustering accuracy for both K-Means and GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by 14%, often surpassing the performance of both manifold-based and recent density-based clustering algorithms. We further support our framework with initial theoretical guarantees, ablation to demonstrate the usefulness of the individual steps and with evidence of robustness to noise.</li>
</ul>

<h3>Title: Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification</h3>
<ul>
<li><strong>Authors: </strong>Jason Kahei Tam, Murilo Gustineli, Anthony Miyaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08248">https://arxiv.org/abs/2507.08248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08248">https://arxiv.org/pdf/2507.08248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08248]] Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification(https://arxiv.org/abs/2507.08248)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Accurate identification of fungi species presents a unique challenge in computer vision due to fine-grained inter-species variation and high intra-species variation. This paper presents our approach for the FungiCLEF 2025 competition, which focuses on few-shot fine-grained visual categorization (FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented with multiple vision transformer models, data augmentation, weighted sampling, and incorporating textual information. We also explored generative AI models for zero-shot classification using structured prompting but found them to significantly underperform relative to vision-based models. Our final model outperformed both competition baselines and highlighted the effectiveness of domain specific pretraining and balanced sampling strategies. Our approach ranked 35/74 on the private test set in post-completion evaluation, this suggests additional work can be done on metadata selection and domain-adapted multi-modal learning. Our code is available at this https URL.</li>
</ul>

<h3>Title: Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)</h3>
<ul>
<li><strong>Authors: </strong>Hossein Jamali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08255">https://arxiv.org/abs/2507.08255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08255">https://arxiv.org/pdf/2507.08255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08255]] Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)(https://arxiv.org/abs/2507.08255)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Missing data presents a critical challenge in real-world datasets, significantly degrading the performance of machine learning models. While Large Language Models (LLMs) have recently demonstrated remarkable capabilities in tabular data imputation, exemplified by frameworks like UnIMP, their reliance on classical embedding methods often limits their ability to capture complex, non-linear correlations, particularly in mixed-type data scenarios encompassing numerical, categorical, and textual features. This paper introduces Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into an LLM-based imputation architecture. Our core innovation lies in replacing conventional classical input embeddings with quantum feature maps generated by an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the model to leverage quantum phenomena such as superposition and entanglement, thereby learning richer, more expressive representations of data and enhancing the recovery of intricate missingness patterns. Our experiments on benchmark mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by up to 15.2% for numerical features (RMSE) and improves classification accuracy by 8.7% for categorical features (F1-Score) compared to state-of-the-art classical and LLM-based methods. These compelling results underscore the profound potential of quantum-enhanced representations for complex data imputation tasks, even with near-term quantum hardware.</li>
</ul>

<h3>Title: A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hiroshi Yoshihara, Taiki Yamaguchi, Yuichi Inoue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08267">https://arxiv.org/abs/2507.08267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08267">https://arxiv.org/pdf/2507.08267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08267]] A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning(https://arxiv.org/abs/2507.08267)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a systematic methodology for combining them to maximize both accuracy and efficiency remains largely unexplored. This paper introduces a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO). We posit that these methods play complementary, not competing, roles: a prolonged SFT phase first pushes the model's accuracy to its limits, after which a GRPO phase dramatically improves token efficiency while preserving this peak performance. Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length. The efficacy of our recipe is rigorously validated through top-tier performance on challenging benchmarks, including a high rank among over 2,200 teams in the strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the community with a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient. To ensure full reproducibility and empower future research, we will open-source our entire framework, including all code, model checkpoints, and training configurations at this https URL.</li>
</ul>

<h3>Title: Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone</h3>
<ul>
<li><strong>Authors: </strong>J.D. Peiffer, Kunal Shah, Irina Djuraskovic, Shawana Anarwala, Kayan Abdou, Rujvee Patel, Prakash Jayabalan, Brenton Pennicooke, R. James Cotton</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08268">https://arxiv.org/abs/2507.08268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08268">https://arxiv.org/pdf/2507.08268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08268]] Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone(https://arxiv.org/abs/2507.08268)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>The way a person moves is a direct reflection of their neurological and musculoskeletal health, yet it remains one of the most underutilized vital signs in clinical practice. Although clinicians visually observe movement impairments, they lack accessible and validated methods to objectively measure movement in routine care. This gap prevents wider use of biomechanical measurements in practice, which could enable more sensitive outcome measures or earlier identification of impairment. We present our Portable Biomechanics Laboratory (PBL), which includes a secure, cloud-enabled smartphone app for data collection and a novel algorithm for fitting biomechanical models to this data. We extensively validated PBL's biomechanical measures using a large, clinically representative dataset. Next, we tested the usability and utility of our system in neurosurgery and sports medicine clinics. We found joint angle errors within 3 degrees across participants with neurological injury, lower-limb prosthesis users, pediatric inpatients, and controls. In addition to being easy to use, gait metrics computed from the PBL showed high reliability and were sensitive to clinical differences. For example, in individuals undergoing decompression surgery for cervical myelopathy, the mJOA score is a common patient-reported outcome measure; we found that PBL gait metrics correlated with mJOA scores and demonstrated greater responsiveness to surgical intervention than the patient-reported outcomes. These findings support the use of handheld smartphone video as a scalable, low-burden tool for capturing clinically meaningful biomechanical data, offering a promising path toward accessible monitoring of mobility impairments. We release the first clinically validated method for measuring whole-body kinematics from handheld smartphone video at this https URL .</li>
</ul>

<h3>Title: Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Aleksei Ilin, Gor Matevosyan, Xueying Ma, Vladimir Eremin, Suhaa Dada, Muqun Li, Riyaaz Shaik, Haluk Noyan Tokgozoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08284">https://arxiv.org/abs/2507.08284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08284">https://arxiv.org/pdf/2507.08284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08284]] Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training(https://arxiv.org/abs/2507.08284)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>We introduce a lightweight yet highly effective safety guardrail framework for language models, demonstrating that small-scale language models can achieve, and even surpass, the performance of larger counterparts in content moderation tasks. This is accomplished through high-fidelity synthetic data generation and adversarial training. The synthetic data generation process begins with human-curated seed data, which undergoes query augmentation and paraphrasing to create diverse and contextually rich examples. This augmented data is then subjected to multiple rounds of curation, ensuring high fidelity and relevance. Inspired by recent advances in the Generative Adversarial Network (GAN) architecture, our adversarial training employs reinforcement learning to guide a generator that produces challenging synthetic examples. These examples are used to fine-tune the safety classifier, enhancing its ability to detect and mitigate harmful content. Additionally, we incorporate strategies from recent research on efficient LLM training, leveraging the capabilities of smaller models to improve the performance of larger generative models. With iterative adversarial training and the generation of diverse, high-quality synthetic data, our framework enables small language models (SLMs) to serve as robust safety guardrails. This approach not only reduces computational overhead but also enhances resilience against adversarial attacks, offering a scalable and efficient solution for content moderation in AI systems.</li>
</ul>

<h3>Title: TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data</h3>
<ul>
<li><strong>Authors: </strong>Aufa Nasywa Rahman, Bimo Sunarfri Hantono, Guntur Dharma Putra</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08286">https://arxiv.org/abs/2507.08286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08286">https://arxiv.org/pdf/2507.08286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08286]] TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data(https://arxiv.org/abs/2507.08286)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Open banking framework enables third party providers to access financial data across banking institutions, leading to unprecedented innovations in the financial sector. However, some open banking standards remain susceptible to severe technological risks, including unverified data sources, inconsistent data integrity, and lack of immutability. In this paper, we propose a layered architecture that provides assurance in data trustworthiness with three distinct levels of trust, covering source validation, data-level authentication, and tamper-proof storage. The first layer guarantees the source legitimacy using decentralized identity and verifiable presentation, while the second layer verifies data authenticity and consistency using cryptographic signing. Lastly, the third layer guarantees data immutability through the Tangle, a directed acyclic graph distributed ledger. We implemented a proof-of-concept implementation of our solution to evaluate its performance, where the results demonstrate that the system scales linearly with a stable throughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and 350 MiB memory. Compared to a real-world open banking implementation, our solution offers significantly reduced latency and stronger data integrity assurance. Overall, our solution offers a practical and efficient system for secure data sharing in financial ecosystems while maintaining regulatory compliance.</li>
</ul>

<h3>Title: Invariant-based Robust Weights Watermark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, Xiaobing Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08288">https://arxiv.org/abs/2507.08288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08288">https://arxiv.org/pdf/2507.08288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08288]] Invariant-based Robust Weights Watermark for Large Language Models(https://arxiv.org/abs/2507.08288)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking technology has gained significant attention due to the increasing importance of intellectual property (IP) rights, particularly with the growing deployment of large language models (LLMs) on billions resource-constrained edge devices. To counter the potential threats of IP theft by malicious users, this paper introduces a robust watermarking scheme without retraining or fine-tuning for transformer models. The scheme generates a unique key for each user and derives a stable watermark value by solving linear constraints constructed from model invariants. Moreover, this technology utilizes noise mechanism to hide watermark locations in multi-user scenarios against collusion attack. This paper evaluates the approach on three popular models (Llama3, Phi3, Gemma), and the experimental results confirm the strong robustness across a range of attack methods (fine-tuning, pruning, quantization, permutation, scaling, reversible matrix and collusion attacks).</li>
</ul>

<h3>Title: Cross-Resolution SAR Target Detection Using Structural Hierarchy Adaptation and Reliable Adjacency Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiang Qin, Bin Zou, Haolin Li, Lamei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08290">https://arxiv.org/abs/2507.08290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08290">https://arxiv.org/pdf/2507.08290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08290]] Cross-Resolution SAR Target Detection Using Structural Hierarchy Adaptation and Reliable Adjacency Alignment(https://arxiv.org/abs/2507.08290)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, interpretability</a></li>
<li><strong>Abstract: </strong>In recent years, continuous improvements in SAR resolution have significantly benefited applications such as urban monitoring and target detection. However, the improvement in resolution leads to increased discrepancies in scattering characteristics, posing challenges to the generalization ability of target detection models. While domain adaptation technology is a potential solution, the inevitable discrepancies caused by resolution differences often lead to blind feature adaptation and unreliable semantic propagation, ultimately degrading the domain adaptation performance. To address these challenges, this paper proposes a novel SAR target detection method (termed CR-Net), that incorporates structure priors and evidential learning theory into the detection model, enabling reliable domain adaptation for cross-resolution detection. To be specific, CR-Net integrates Structure-induced Hierarchical Feature Adaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA module is introduced to establish structural correlations between targets and achieve structure-aware feature adaptation, thereby enhancing the interpretability of the feature adaptation process. Afterwards, the RSAA module is proposed to enhance reliable semantic alignment, by leveraging the secure adjacency set to transfer valuable discriminative knowledge from the source domain to the target domain. This further improves the discriminability of the detection model in the target domain. Based on experimental results from different-resolution datasets,the proposed CR-Net significantly enhances cross-resolution adaptation by preserving intra-domain structures and improving discriminability. It achieves state-of-the-art (SOTA) performance in cross-resolution SAR target detection.</li>
</ul>

<h3>Title: KAT-V1: Kwai-AutoThink Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Zizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, Bing Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08297">https://arxiv.org/abs/2507.08297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08297">https://arxiv.org/pdf/2507.08297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08297]] KAT-V1: Kwai-AutoThink Technical Report(https://arxiv.org/abs/2507.08297)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model developed to address the overthinking problem in reasoning-intensive tasks, where an automatic thinking training paradigm is proposed to dynamically switch between reasoning and non-reasoning modes based on task complexity. Specifically, first, we construct the dual-regime dataset based on a novel tagging pipeline and a multi-agent synthesis strategy, and then we apply Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling efficient and fine-grained reasoning transfer with minimal pretraining cost. Besides, we implement a cold-start initialization strategy that introduces mode-selection priors using majority-vote signals and intent-aware prompting. Finally, we propose Step-SRPO, a reinforcement learning algorithm that incorporates intermediate supervision into the GRPO framework, offering structured guidance over both reasoning-mode selection and response accuracy. Extensive experiments across multiple benchmarks demonstrate that KAT consistently matches or even outperforms current state-of-the-art models, including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of reasoning-intensive tasks while reducing token usage by up to approximately 30\%. Beyond academic evaluation, KAT has been successfully deployed in Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world development workflows with high accuracy, efficiency, and controllable reasoning behaviors. Moreover, we are actively training a 200B Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage results already demonstrate promising improvements in performance and efficiency, further showing the scalability of the AutoThink paradigm.</li>
</ul>

<h3>Title: M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation</h3>
<ul>
<li><strong>Authors: </strong>Kui Jiang, Shiyu Liu, Junjun Jiang, Xin Yang, Hongxun Yang, Xiaopeng Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08307">https://arxiv.org/abs/2507.08307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08307">https://arxiv.org/pdf/2507.08307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08307]] M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation(https://arxiv.org/abs/2507.08307)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating this http URL, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction this http URL, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video this http URL across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is this https URL</li>
</ul>

<h3>Title: Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency</h3>
<ul>
<li><strong>Authors: </strong>Yupu Liang, Yaping Zhang, Zhiyang Zhang, Zhiyuan Chen, Yang Zhao, Lu Xiang, Chengqing Zong, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08309">https://arxiv.org/abs/2507.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08309">https://arxiv.org/pdf/2507.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08309]] Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency(https://arxiv.org/abs/2507.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown strong performance in document image tasks, especially Optical Character Recognition (OCR). However, they struggle with Document Image Machine Translation (DIMT), which requires handling both cross-modal and cross-lingual challenges. Previous efforts to enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT dataset often result in the forgetting of the model's existing monolingual abilities, such as OCR. To address these challenges, we introduce a novel fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR proficiency, inspired by the concept "Bilingual Cognitive Advantage". Specifically, SSR prompts the model to generate OCR text before producing translation text, which allows the model to leverage its strong monolingual OCR ability while learning to translate text across languages. Comprehensive experiments demonstrate the proposed SSR learning helps mitigate catastrophic forgetting, improving the generalization ability of MLLMs on both OCR and DIMT tasks.</li>
</ul>

<h3>Title: Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices</h3>
<ul>
<li><strong>Authors: </strong>Jesus Lopez, Viviana Cadena, Mohammad Saidur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08312">https://arxiv.org/abs/2507.08312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08312">https://arxiv.org/pdf/2507.08312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08312]] Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices(https://arxiv.org/abs/2507.08312)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>The rapid advancement of quantum computing poses a critical threat to classical cryptographic algorithms such as RSA and ECC, particularly in Internet of Things (IoT) devices, where secure communication is essential but often constrained by limited computational resources. This paper investigates the feasibility of deploying post-quantum cryptography (PQC) algorithms on resource-constrained devices. In particular, we implement three PQC algorithms -- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with Raspberry Pi devices. Leveraging the Open Quantum Safe (\texttt{liboqs}) library in conjunction with \texttt{mbedTLS}, we develop quantum-secure key exchange protocols, and evaluate their performance in terms of computational overhead, memory usage, and energy consumption for quantum secure communication. Experimental results demonstrate that the integration of PQC algorithms on constrained hardware is practical, reinforcing the urgent need for quantum-resilient cryptographic frameworks in next-generation IoT devices. The implementation of this paper is available at this https URL.</li>
</ul>

<h3>Title: CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation</h3>
<ul>
<li><strong>Authors: </strong>Yinzhu Quan, Xinrui Li, Ying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08325">https://arxiv.org/abs/2507.08325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08325">https://arxiv.org/pdf/2507.08325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08325]] CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation(https://arxiv.org/abs/2507.08325)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In e-commerce private-domain channels such as instant messaging and e-mail, merchants engage customers directly as part of their Customer Relationship Management (CRM) programmes to drive retention and conversion. While a few top performers excel at crafting outbound messages, most merchants struggle to write persuasive copy because they lack both expertise and scalable tools. We introduce CRMAgent, a multi-agent system built on large language models (LLMs) that generates high-quality message templates and actionable writing guidance through three complementary modes. First, group-based learning enables the agent to learn from a merchant's own top-performing messages within the same audience segment and rewrite low-performing ones. Second, retrieval-and-adaptation fetches templates that share the same audience segment and exhibit high similarity in voucher type and product category, learns their successful patterns, and adapts them to the current campaign. Third, a rule-based fallback provides a lightweight zero-shot rewrite when no suitable references are available. Extensive experiments show that CRMAgent consistently outperforms merchants' original templates, delivering significant gains in both audience-match and marketing-effectiveness metrics.</li>
</ul>

<h3>Title: Interpretability-Aware Pruning for Efficient Medical Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Nikita Malik, Pratinav Seth, Neeraj Kumar Singh, Chintan Chitroda, Vinay Kumar Sankarapu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08330">https://arxiv.org/abs/2507.08330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08330">https://arxiv.org/pdf/2507.08330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08330]] Interpretability-Aware Pruning for Efficient Medical Image Analysis(https://arxiv.org/abs/2507.08330)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning has driven significant advances in medical image analysis, yet its adoption in clinical practice remains constrained by the large size and lack of transparency in modern models. Advances in interpretability techniques such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated Gradients make it possible to assess the contribution of individual components within neural networks trained on medical imaging tasks. In this work, we introduce an interpretability-guided pruning framework that reduces model complexity while preserving both predictive performance and transparency. By selectively retaining only the most relevant parts of each layer, our method enables targeted compression that maintains clinically meaningful representations. Experiments across multiple medical image classification benchmarks demonstrate that this approach achieves high compression rates with minimal loss in accuracy, paving the way for lightweight, interpretable models suited for real-world deployment in healthcare settings.</li>
</ul>

<h3>Title: Qualcomm Trusted Application Emulation for Fuzzing Testing</h3>
<ul>
<li><strong>Authors: </strong>Chun-I Fan, Li-En Chang, Cheng-Han Shie</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08331">https://arxiv.org/abs/2507.08331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08331">https://arxiv.org/pdf/2507.08331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08331]] Qualcomm Trusted Application Emulation for Fuzzing Testing(https://arxiv.org/abs/2507.08331)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>In recent years, the increasing awareness of cybersecurity has led to a heightened focus on information security within hardware devices and products. Incorporating Trusted Execution Environments (TEEs) into product designs has become a standard practice for safeguarding sensitive user information. However, vulnerabilities within these components present significant risks, if exploited by attackers, these vulnerabilities could lead to the leakage of sensitive data, thereby compromising user privacy and security. This research centers on trusted applications (TAs) within the Qualcomm TEE and introduces a novel emulator specifically designed for these applications. Through reverse engineering techniques, we thoroughly analyze Qualcomm TAs and develop a partial emulation environment that accurately emulates their behavior. Additionally, we integrate fuzzing testing techniques into the emulator to systematically uncover potential vulnerabilities within Qualcomm TAs, demonstrating its practical effectiveness in identifying real-world security flaws. This research makes a significant contribution by being the first to provide both the implementation methods and source codes for a Qualcomm TAs emulator, offering a valuable reference for future research efforts. Unlike previous approaches that relied on complex and resource-intensive full-system simulations, our approach is lightweight and effective, making security testing of TA more convenient.</li>
</ul>

<h3>Title: CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Kim, In-su Jang, Pyongkun Kim, Kwang-Ju Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08334">https://arxiv.org/abs/2507.08334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08334">https://arxiv.org/pdf/2507.08334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08334]] CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models(https://arxiv.org/abs/2507.08334)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) provide interpretable and controllable generative modeling by routing generation through explicit, human-understandable concepts. However, previous generative CBMs often rely on auxiliary visual cues at the bottleneck to compensate for information not captured by the concepts, which undermines interpretability and compositionality. We propose CoCo-Bot, a post-hoc, composable concept bottleneck generative model that eliminates the need for auxiliary cues by transmitting all information solely through explicit concepts. Guided by diffusion-based energy functions, CoCo-Bot supports robust post-hoc interventions-such as concept composition and negation-across arbitrary concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that CoCo-Bot improves concept-level controllability and interpretability, while maintaining competitive visual quality.</li>
</ul>

<h3>Title: What Factors Affect LLMs and RLLMs in Financial Question Answering?</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Xuesi Hu, Jiageng Wu, Yuntao Zou, Qiancheng Zhang, Dagang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08339">https://arxiv.org/abs/2507.08339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08339">https://arxiv.org/pdf/2507.08339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08339]] What Factors Affect LLMs and RLLMs in Financial Question Answering?(https://arxiv.org/abs/2507.08339)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, the development of large language models (LLMs) and reasoning large language models (RLLMs) have gained considerable attention from many researchers. RLLMs enhance the reasoning capabilities of LLMs through Long Chain-of-Thought (Long CoT) processes, significantly improving the performance of LLMs in addressing complex problems. However, there are few works that systematically explore what methods can fully unlock the performance of LLMs and RLLMs within the financial domain. To investigate the impact of various methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the effects of prompting methods, agentic frameworks, and multilingual alignment methods on financial question-answering tasks. Our research findings indicate: (1) Current prompting methods and agent frameworks enhance the performance of LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess inherent Long CoT capabilities, which limits the effectiveness of conventional methods in further enhancing their performance; (3) Current advanced multilingual alignment methods primarily improve the multilingual performance of LLMs by extending the reasoning length, which yields minimal benefits for RLLMs. We hope that this study can serve as an important reference for LLMs and RLLMs in the field of financial question answering.</li>
</ul>

<h3>Title: Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</h3>
<ul>
<li><strong>Authors: </strong>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08340">https://arxiv.org/abs/2507.08340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08340">https://arxiv.org/pdf/2507.08340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08340]] Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement(https://arxiv.org/abs/2507.08340)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at this https URL</li>
</ul>

<h3>Title: Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization</h3>
<ul>
<li><strong>Authors: </strong>Itai Mondshine, Tzuf Paz-Argaman, Reut Tsarfaty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08342">https://arxiv.org/abs/2507.08342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08342">https://arxiv.org/pdf/2507.08342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08342]] Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization(https://arxiv.org/abs/2507.08342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automatic n-gram based metrics such as ROUGE are widely used for evaluating generative tasks such as summarization. While these metrics are considered indicative (even if imperfect) of human evaluation for English, their suitability for other languages remains unclear. To address this, we systematically assess evaluation metrics for generation both n-gram-based and neural based to evaluate their effectiveness across languages and tasks. Specifically, we design a large-scale evaluation suite across eight languages from four typological families: agglutinative, isolating, low-fusional, and high-fusional, spanning both low- and high-resource settings, to analyze their correlation with human judgments. Our findings highlight the sensitivity of evaluation metrics to the language type. For example, in fusional languages, n-gram-based metrics show lower correlation with human assessments compared to isolating and agglutinative languages. We also demonstrate that proper tokenization can significantly mitigate this issue for morphologically rich fusional languages, sometimes even reversing negative trends. Additionally, we show that neural-based metrics specifically trained for evaluation, such as COMET, consistently outperform other neural metrics and better correlate with human judgments in low-resource languages. Overall, our analysis highlights the limitations of n-gram metrics for fusional languages and advocates for greater investment in neural-based metrics trained for evaluation tasks.</li>
</ul>

<h3>Title: Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation</h3>
<ul>
<li><strong>Authors: </strong>Junxue Yang, Xin Liao, Weixuan Tang, Jianhua Yang, Zheng Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08343">https://arxiv.org/abs/2507.08343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08343">https://arxiv.org/pdf/2507.08343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08343]] Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation(https://arxiv.org/abs/2507.08343)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Deep hiding has been exploring the hiding capability of deep learning-based models, aiming to conceal image-level messages into cover images and reveal them from generated stego images. Existing schemes are easily detected by steganalyzers due to their large payloads and their limitation to feature extraction based solely on either pure convolution or pure transformer operators within a single range, as well as pixel-level loss constraints. To address the issue, in this paper, we introduce generation-based adversarial attacks into color JPEG image deep hiding and propose a multi-range representations-driven adversarial stego generation framework called MRAG from a steganalysis perspective. Specifically, we integrate the local-range neighbor reception characteristic of the convolution and the global-range dependency modeling of the transformer to construct MRAG. Meanwhile, we use the transformed images obtained through coarse-grained and fine-grained frequency decomposition as inputs, introducing multi-grained information. Furthermore, a features angle-norm disentanglement loss is designed to constrain the generated stegos closer to covers in the angle and norm space of the steganalyzer's classified features. Consequently, small yet effective adversarial perturbations can be injected into the process of generating stegos, ensuring that stegos maintain favorable secret restorability and imperceptibility. Extensive experiments demonstrate that MRAG can achieve state-of-the-art performance.</li>
</ul>

<h3>Title: MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jihao Gu, Fei Wang, Kun Li, Yanyan Wei, Zhiliang Wu, Dan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08344">https://arxiv.org/abs/2507.08344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08344">https://arxiv.org/pdf/2507.08344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08344]] MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion(https://arxiv.org/abs/2507.08344)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we present MM-Gesture, the solution developed by our team HFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd MiGA Challenge at IJCAI 2025, achieving superior performance compared to previous state-of-the-art methods. MM-Gesture is a multimodal fusion framework designed specifically for recognizing subtle and short-duration micro-gestures (MGs), integrating complementary cues from joint, limb, RGB video, Taylor-series video, optical-flow video, and depth video modalities. Utilizing PoseConv3D and Video Swin Transformer architectures with a novel modality-weighted ensemble strategy, our method further enhances RGB modality performance through transfer learning pre-trained on the larger MA-52 dataset. Extensive experiments on the iMiGUE benchmark, including ablation studies across different modalities, validate the effectiveness of our proposed approach, achieving a top-1 accuracy of 73.213%.</li>
</ul>

<h3>Title: Exploring Design of Multi-Agent LLM Dialogues for Research Ideation</h3>
<ul>
<li><strong>Authors: </strong>Keisuke Ueda, Wataru Hirota, Takuto Asakura, Takahiro Omi, Kosuke Takahashi, Kosuke Arima, Tatsuya Ishigaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08350">https://arxiv.org/abs/2507.08350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08350">https://arxiv.org/pdf/2507.08350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08350]] Exploring Design of Multi-Agent LLM Dialogues for Research Ideation(https://arxiv.org/abs/2507.08350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used to support creative tasks such as research idea generation. While recent work has shown that structured dialogues between LLMs can improve the novelty and feasibility of generated ideas, the optimal design of such interactions remains unclear. In this study, we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific ideation. We compare different configurations of agent roles, number of agents, and dialogue depth to understand how these factors influence the novelty and feasibility of generated ideas. Our experimental setup includes settings where one agent generates ideas and another critiques them, enabling iterative improvement. Our results show that enlarging the agent cohort, deepening the interaction depth, and broadening agent persona heterogeneity each enrich the diversity of generated ideas. Moreover, specifically increasing critic-side diversity within the ideation-critique-revision loop further boosts the feasibility of the final proposals. Our findings offer practical guidelines for building effective multi-agent LLM systems for scientific ideation. Our code is available at this https URL.</li>
</ul>

<h3>Title: scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hegang Chen, Yuyin Lu, Zhiming Dai, Fu Lee Wang, Qing Li, Yanghui Rao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08355">https://arxiv.org/abs/2507.08355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08355">https://arxiv.org/pdf/2507.08355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08355]] scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling(https://arxiv.org/abs/2507.08355)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in sequencing technologies have enabled researchers to explore cellular heterogeneity at single-cell resolution. Meanwhile, interpretability has gained prominence parallel to the rapid increase in the complexity and performance of deep learning models. In recent years, topic models have been widely used for interpretable single-cell embedding learning and clustering analysis, which we refer to as single-cell embedded topic models. However, previous studies evaluated the interpretability of the models mainly through qualitative analysis, and these single-cell embedded topic models suffer from the potential problem of interpretation collapse. Furthermore, their neglect of external biological knowledge constrains analytical performance. Here, we present scE2TM, an external knowledge-guided single-cell embedded topic model that provides a high-quality cell embedding and strong interpretation, contributing to comprehensive scRNA-seq data analysis. Our comprehensive evaluation across 20 scRNA-seq datasets demonstrates that scE2TM achieves significant clustering performance gains compared to 7 state-of-the-art methods. In addition, we propose a new interpretability evaluation benchmark that introduces 10 metrics to quantitatively assess the interpretability of single-cell embedded topic models. The results show that the interpretation provided by scE2TM performs encouragingly in terms of diversity and consistency with the underlying biological signals, contributing to a better revealing of the underlying biological mechanisms.</li>
</ul>

<h3>Title: Cycle Context Verification for In-Context Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shishuai Hu, Zehui Liao, Liangli Zhen, Huazhu Fu, Yong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08357">https://arxiv.org/abs/2507.08357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08357">https://arxiv.org/pdf/2507.08357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08357]] Cycle Context Verification for In-Context Medical Image Segmentation(https://arxiv.org/abs/2507.08357)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is emerging as a promising technique for achieving universal medical image segmentation, where a variety of objects of interest across imaging modalities can be segmented using a single model. Nevertheless, its performance is highly sensitive to the alignment between the query image and in-context image-mask pairs. In a clinical scenario, the scarcity of annotated medical images makes it challenging to select optimal in-context pairs, and fine-tuning foundation ICL models on contextual data is infeasible due to computational costs and the risk of catastrophic forgetting. To address this challenge, we propose Cycle Context Verification (CCV), a novel framework that enhances ICL-based medical image segmentation by enabling self-verification of predictions and accordingly enhancing contextual alignment. Specifically, CCV employs a cyclic pipeline in which the model initially generates a segmentation mask for the query image. Subsequently, the roles of the query and an in-context pair are swapped, allowing the model to validate its prediction by predicting the mask of the original in-context image. The accuracy of this secondary prediction serves as an implicit measure of the initial query segmentation. A query-specific prompt is introduced to alter the query image and updated to improve the measure, thereby enhancing the alignment between the query and in-context pairs. We evaluated CCV on seven medical image segmentation datasets using two ICL foundation models, demonstrating its superiority over existing methods. Our results highlight CCV's ability to enhance ICL-based segmentation, making it a robust solution for universal medical image segmentation. The code will be available at this https URL.</li>
</ul>

<h3>Title: Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text</h3>
<ul>
<li><strong>Authors: </strong>Phuong Nam Lê, Charlotte Schneider-Depré, Alexandre Goossens, Alexander Stevens, Aurélie Leribaux, Johannes De Smedt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08362">https://arxiv.org/abs/2507.08362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08362">https://arxiv.org/pdf/2507.08362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08362]] Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text(https://arxiv.org/abs/2507.08362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient planning, resource management, and consistent operations often rely on converting textual process documents into formal Business Process Model and Notation (BPMN) models. However, this conversion process remains time-intensive and costly. Existing approaches, whether rule-based or machine-learning-based, still struggle with writing styles and often fail to identify parallel structures in process descriptions. This paper introduces an automated pipeline for extracting BPMN models from text, leveraging the use of machine learning and large language models. A key contribution of this work is the introduction of a newly annotated dataset, which significantly enhances the training process. Specifically, we augment the PET dataset with 15 newly annotated documents containing 32 parallel gateways for model training, a critical feature often overlooked in existing datasets. This addition enables models to better capture parallel structures, a common but complex aspect of process descriptions. The proposed approach demonstrates adequate performance in terms of reconstruction accuracy, offering a promising foundation for organizations to accelerate BPMN model creation.</li>
</ul>

<h3>Title: Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer</h3>
<ul>
<li><strong>Authors: </strong>Francesco De Cristofaro, Felix Hofbaur, Aixi Yang, Arno Eichberger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08365">https://arxiv.org/abs/2507.08365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08365">https://arxiv.org/pdf/2507.08365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08365]] Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer(https://arxiv.org/abs/2507.08365)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Lane changes of preceding vehicles have a great impact on the motion planning of automated vehicles especially in complex traffic situations. Predicting them would benefit the public in terms of safety and efficiency. While many research efforts have been made in this direction, few concentrated on predicting maneuvers within a set time interval compared to predicting at a set prediction time. In addition, there exist a lack of comparisons between different architectures to try to determine the best performing one and to assess how to correctly choose the input for such models. In this paper the structure of an LSTM, a CNN and a Transformer network are described and implemented to predict the intention of human drivers to perform a lane change. We show how the data was prepared starting from a publicly available dataset (highD), which features were used, how the networks were designed and finally we compare the results of the three networks with different configurations of input data. We found that transformer networks performed better than the other networks and was less affected by overfitting. The accuracy of the method spanned from $82.79\%$ to $96.73\%$ for different input configurations and showed overall good performances considering also precision and recall.</li>
</ul>

<h3>Title: Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuki Yoshihara, Linjing Jiang, Nihan Karatas, Hitoshi Kanamori, Asuka Harada, Takahiro Tanaka</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08367">https://arxiv.org/abs/2507.08367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08367">https://arxiv.org/pdf/2507.08367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08367]] Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment(https://arxiv.org/abs/2507.08367)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the potential of a multimodal large language model (LLM), specifically ChatGPT-4o, to perform human-like interpretations of traffic scenes using static dashcam images. Herein, we focus on three judgment tasks relevant to elderly driver assessments: evaluating traffic density, assessing intersection visibility, and recognizing stop signs recognition. These tasks require contextual reasoning rather than simple object detection. Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated the performance of the model with human annotations serving as the reference standard. Evaluation metrics included precision, recall, and F1-score. Results indicate that prompt design considerably affects performance, with recall for intersection visibility increasing from 21.7% (zero-shot) to 57.0% (multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In stop-sign detection, the model demonstrated high precision (up to 86.3%) but a lower recall (approximately 76.7%), indicating a conservative response tendency. Output stability analysis revealed that humans and the model faced difficulties interpreting structurally ambiguous scenes. However, the model's explanatory texts corresponded with its predictions, enhancing interpretability. These findings suggest that, with well-designed prompts, LLMs hold promise as supportive tools for scene-level driving risk assessments. Future studies should explore scalability using larger datasets, diverse annotators, and next-generation model architectures for elderly driver assessments.</li>
</ul>

<h3>Title: From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Sen Wang, Shao Zeng, Tianjun Gu, Zhizhong Zhang, Ruixin Zhang, Shouhong Ding, Jingyun Zhang, Jun Wang, Xin Tan, Yuan Xie, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08380">https://arxiv.org/abs/2507.08380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08380">https://arxiv.org/pdf/2507.08380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08380]] From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning(https://arxiv.org/abs/2507.08380)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation.</li>
</ul>

<h3>Title: Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset</h3>
<ul>
<li><strong>Authors: </strong>Mathias Zinnen, Prathmesh Madhu, Inger Leemans, Peter Bell, Azhar Hussian, Hang Tran, Ali Hürriyetoğlu, Andreas Maier, Vincent Christlein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08384">https://arxiv.org/abs/2507.08384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08384">https://arxiv.org/pdf/2507.08384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08384]] Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset(https://arxiv.org/abs/2507.08384)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world applications of computer vision in the humanities require algorithms to be robust against artistic abstraction, peripheral objects, and subtle differences between fine-grained target classes. Existing datasets provide instance-level annotations on artworks but are generally biased towards the image centre and limited with regard to detailed object classes. The proposed ODOR dataset fills this gap, offering 38,116 object-level annotations across 4712 images, spanning an extensive set of 139 fine-grained categories. Conducting a statistical analysis, we showcase challenging dataset properties, such as a detailed set of categories, dense and overlapping objects, and spatial distribution over the whole image canvas. Furthermore, we provide an extensive baseline analysis for object detection models and highlight the challenging properties of the dataset through a set of secondary studies. Inspiring further research on artwork object detection and broader visual cultural heritage studies, the dataset challenges researchers to explore the intersection of object recognition and smell perception.</li>
</ul>

<h3>Title: Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling</h3>
<ul>
<li><strong>Authors: </strong>Meihua Dang, Jiaqi Han, Minkai Xu, Kai Xu, Akash Srivastava, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08390">https://arxiv.org/abs/2507.08390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08390">https://arxiv.org/pdf/2507.08390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08390]] Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling(https://arxiv.org/abs/2507.08390)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have emerged as a powerful paradigm for language modeling, rivaling auto-regressive models by training-time scaling. However, inference-time scaling in discrete diffusion models remains relatively under-explored. In this work, we study sampling-based approaches for achieving high-quality text generation from discrete diffusion models in reward-guided settings. We introduce a novel inference-time scaling approach based on particle Gibbs sampling for discrete diffusion models. The particle Gibbs sampling algorithm iteratively refines full diffusion trajectories using conditional Sequential Monte Carlo as its transition mechanism. This process ensures that the updated samples progressively improve and move closer to the reward-weighted target distribution. Unlike existing inference-time scaling methods, which are often limited to single diffusion trajectories, our approach leverages iterative refinement across multiple trajectories. Within this framework, we further analyze the trade-offs between four key axes for inference-time scaling under fixed compute budgets: particle Gibbs iterations, particle count, denoising steps, and reward estimation cost. Empirically, our method consistently outperforms prior inference-time strategies on reward-guided text generation tasks, achieving significant improvement in accuracy under varying compute budgets.</li>
</ul>

<h3>Title: Subject-Consistent and Pose-Diverse Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhanxin Gao, Beier Zhu, Liang Yao, Jian Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08396">https://arxiv.org/abs/2507.08396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08396">https://arxiv.org/pdf/2507.08396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08396]] Subject-Consistent and Pose-Diverse Text-to-Image Generation(https://arxiv.org/abs/2507.08396)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in this https URL.</li>
</ul>

<h3>Title: PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Yongjian Zhang, Longguang Wang, Kunhong Li, Ye Zhang, Yun Wang, Liang Lin, Yulan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08400">https://arxiv.org/abs/2507.08400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08400">https://arxiv.org/pdf/2507.08400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08400]] PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models(https://arxiv.org/abs/2507.08400)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work presents PanMatch, a versatile foundation model for robust correspondence matching. Unlike previous methods that rely on task-specific architectures and domain-specific fine-tuning to support tasks like stereo matching, optical flow or feature matching, our key insight is that any two-frame correspondence matching task can be addressed within a 2D displacement estimation framework using the same model weights. Such a formulation eliminates the need for designing specialized unified architectures or task-specific ensemble models. Instead, it achieves multi-task integration by endowing displacement estimation algorithms with unprecedented generalization capabilities. To this end, we highlight the importance of a robust feature extractor applicable across multiple domains and tasks, and propose the feature transformation pipeline that leverage all-purpose features from Large Vision Models to endow matching baselines with zero-shot cross-view matching capabilities. Furthermore, we assemble a cross-domain dataset with near 1.8 million samples from stereo matching, optical flow, and feature matching domains to pretrain PanMatch. We demonstrate the versatility of PanMatch across a wide range of domains and downstream tasks using the same model weights. Our model outperforms UniMatch and Flow-Anything on cross-task evaluations, and achieves comparable performance to most state-of-the-art task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch presents unprecedented zero-shot performance in abnormal scenarios, such as rainy day and satellite imagery, where most existing robust algorithms fail to yield meaningful results.</li>
</ul>

<h3>Title: Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shijun Yang, Xiang Zhang, Wanqing Zhao, Hangzai Luo, Sheng Zhong, Jinye Peng, Jianping Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08410">https://arxiv.org/abs/2507.08410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08410">https://arxiv.org/pdf/2507.08410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08410]] Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models(https://arxiv.org/abs/2507.08410)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt learning facilitates the efficient adaptation of Vision-Language Models (VLMs) to various downstream tasks. However, it faces two significant challenges: (1) inadequate modeling of class embedding distributions for unseen instances, leading to suboptimal generalization on novel classes; (2) prevailing methodologies predominantly confine cross-modal alignment to the final output layer of vision and text encoders, which fundamentally limits their capacity to preserve topological consistency with pre-trained multi-modal embedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance Conditional Prompt Learning), a novel paradigm designed for conditional prompt generation. MuGCP leverages Multi-modal Large Language Models (MLLMs) as conditional prompt learners to adaptively generate Semantic Conditional Prompts (SCP) that incorporate rich, fine-grained high-level semantic knowledge for image instances. To ensure effective alignment and interaction across the multi-modal space of Vision-Language Models (VLMs), we introduce the Attention Mutual-Guidance (AMG) module, which facilitates interactions between visual and semantic information. Through mutual guidance, the AMG module generates Visual Conditional Prompts (VCP), enhancing the model's performance in multi-modal tasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that integrates SCP and VCP with contextual prompts, ensuring seamless coordination among the different prompts and enhancing the modeling of class embeddings and instance-specific knowledge. Our MuGCP outperforms existing state-of-the-art methods on 14 different datasets. The code will be made available after publication.</li>
</ul>

<h3>Title: InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes</h3>
<ul>
<li><strong>Authors: </strong>Zesong Yang, Bangbang Yang, Wenqi Dong, Chenxuan Cao, Liyuan Cui, Yuewen Ma, Zhaopeng Cui, Hujun Bao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08416">https://arxiv.org/abs/2507.08416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08416">https://arxiv.org/pdf/2507.08416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08416]] InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes(https://arxiv.org/abs/2507.08416)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Humans can naturally identify and mentally complete occluded objects in cluttered environments. However, imparting similar cognitive ability to robotics remains challenging even with advanced reconstruction techniques, which models scenes as undifferentiated wholes and fails to recognize complete object from partial observations. In this paper, we propose InstaScene, a new paradigm towards holistic 3D perception of complex scenes with a primary goal: decomposing arbitrary instances while ensuring complete reconstruction. To achieve precise decomposition, we develop a novel spatial contrastive learning by tracing rasterization of each instance across views, significantly enhancing semantic supervision in cluttered scenes. To overcome incompleteness from limited observations, we introduce in-situ generation that harnesses valuable observations and geometric cues, effectively guiding 3D generative models to reconstruct complete instances that seamlessly align with the real world. Experiments on scene decomposition and object completion across complex real-world and synthetic scenes demonstrate that our method achieves superior decomposition accuracy while producing geometrically faithful and visually intact objects.</li>
</ul>

<h3>Title: Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Wongi Jeong, Kyungryeol Lee, Hoigi Seo, Se Young Chun</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08422">https://arxiv.org/abs/2507.08422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08422">https://arxiv.org/pdf/2507.08422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08422]] Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers(https://arxiv.org/abs/2507.08422)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.</li>
</ul>

<h3>Title: RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices</h3>
<ul>
<li><strong>Authors: </strong>Anirudh Varanasi, Robin Degraeve, Philippe Roussel, Clement Merckling</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08424">https://arxiv.org/abs/2507.08424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08424">https://arxiv.org/pdf/2507.08424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08424]] RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices(https://arxiv.org/abs/2507.08424)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Random telegraph noise is a prevalent variability phenomenon in nanoelectronic devices, arising from stochastic carrier exchange at defect sites and critically impacting device reliability and performance. Conventional analysis techniques often rely on restrictive assumptions or manual interventions, limiting their applicability to complex, noisy datasets. Here, we introduce RTNinja, a generalized, fully automated machine learning framework for the unsupervised analysis of random telegraph noise signals. RTNinja deconvolves complex signals to identify the number and characteristics of hidden individual sources, without requiring prior knowledge of the system. The framework comprises two modular components: LevelsExtractor, which uses Bayesian inference and model selection to denoise and discretize the signal; and SourcesMapper, which infers source configurations through probabilistic clustering and optimization. To evaluate performance, we developed a Monte Carlo simulator that generates labeled datasets spanning broad signal-to-noise ratios and source complexities; across 7000 such datasets, RTNinja consistently demonstrated high-fidelity signal reconstruction and accurate extraction of source amplitudes and activity patterns. Our results demonstrate that RTNinja offers a robust, scalable, and device-agnostic tool for random telegraph noise characterization, enabling large-scale statistical benchmarking, reliability-centric technology qualification, predictive failure modeling, and device physics exploration in next-generation nanoelectronics.</li>
</ul>

<h3>Title: A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Lu Xiang, Yang Zhao, Yaping Zhang, Chengqing Zong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08425">https://arxiv.org/abs/2507.08425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08425">https://arxiv.org/pdf/2507.08425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08425]] A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities(https://arxiv.org/abs/2507.08425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated their transformative potential across numerous disciplinary studies, reshaping the existing research methodologies and fostering interdisciplinary collaboration. However, a systematic understanding of their integration into diverse disciplines remains underexplored. This survey paper provides a comprehensive overview of the application of LLMs in interdisciplinary studies, categorising research efforts from both a technical perspective and with regard to their applicability. From a technical standpoint, key methodologies such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration are examined, which enhance the adaptability and effectiveness of LLMs in discipline-specific contexts. From the perspective of their applicability, this paper explores how LLMs are contributing to various disciplines including mathematics, physics, chemistry, biology, and the humanities and social sciences, demonstrating their role in discipline-specific tasks. The prevailing challenges are critically examined and the promising research directions are highlighted alongside the recent advances in LLMs. By providing a comprehensive overview of the technical developments and applications in this field, this survey aims to serve as an invaluable resource for the researchers who are navigating the complex landscape of LLMs in the context of interdisciplinary studies.</li>
</ul>

<h3>Title: ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains</h3>
<ul>
<li><strong>Authors: </strong>Zilu Dong, Xiangqing Shen, Zinong Yang, Rui Xia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08427">https://arxiv.org/abs/2507.08427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08427">https://arxiv.org/pdf/2507.08427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08427]] ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains(https://arxiv.org/abs/2507.08427)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current knowledge editing methods for large language models (LLMs) struggle to maintain logical consistency when propagating ripple effects to associated facts. We propose ChainEdit, a framework that synergizes knowledge graph-derived logical rules with LLM logical reasoning capabilities to enable systematic chain updates. By automatically extracting logical patterns from structured knowledge bases and aligning them with LLMs' internal logics, ChainEdit dynamically generates and edits logically connected knowledge clusters. Experiments demonstrate an improvement of more than 30% in logical generalization over baselines while preserving editing reliability and specificity. We further address evaluation biases in existing benchmarks through knowledge-aware protocols that disentangle external dependencies. This work establishes new state-of-the-art performance on ripple effect while ensuring internal logical consistency after knowledge editing.</li>
</ul>

<h3>Title: Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences</h3>
<ul>
<li><strong>Authors: </strong>Selina Heller, Mohamed Ibrahim, David Antony Selby, Sebastian Vollmer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08440">https://arxiv.org/abs/2507.08440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08440">https://arxiv.org/pdf/2507.08440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08440]] Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences(https://arxiv.org/abs/2507.08440)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Decision conferences are structured, collaborative meetings that bring together experts from various fields to address complex issues and reach a consensus on recommendations for future actions or policies. These conferences often rely on facilitated discussions to ensure productive dialogue and collective agreement. Recently, Large Language Models (LLMs) have shown significant promise in simulating real-world scenarios, particularly through collaborative multi-agent systems that mimic group interactions. In this work, we present a novel LLM-based multi-agent system designed to simulate decision conferences, specifically focusing on detecting agreement among the participant agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance detection, which identifies the position an agent takes on a given issue, and stance polarity detection, which identifies the sentiment as positive, negative, or neutral. These models are further assessed within the multi-agent system to determine their effectiveness in complex simulations. Our results indicate that LLMs can reliably detect agreement even in dynamic and nuanced debates. Incorporating an agreement-detection agent within the system can also improve the efficiency of group debates and enhance the overall quality and coherence of deliberations, making them comparable to real-world decision conferences regarding outcome and decision-making. These findings demonstrate the potential for LLM-based multi-agent systems to simulate group decision-making processes. They also highlight that such systems could be instrumental in supporting decision-making with expert elicitation workshops across various domains.</li>
</ul>

<h3>Title: KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Georgios Balanos, Evangelos Chasanis, Konstantinos Skianis, Evaggelia Pitoura</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08443">https://arxiv.org/abs/2507.08443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08443">https://arxiv.org/pdf/2507.08443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08443]] KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations(https://arxiv.org/abs/2507.08443)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances language models by grounding responses in external information, yet explainability remains a critical challenge, particularly when retrieval relies on unstructured text. Knowledge graphs (KGs) offer a solution by introducing structured, semantically rich representations of entities and their relationships, enabling transparent retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex, a RAG system that improves both factual grounding and explainability by leveraging a domain-specific KG constructed via prompt-based information extraction. Given a user query, KGRAG-Ex identifies relevant entities and semantic paths in the graph, which are then transformed into pseudo-paragraphs: natural language representations of graph substructures that guide corpus retrieval. To improve interpretability and support reasoning transparency, we incorporate perturbation-based explanation methods that assess the influence of specific KG-derived components on the generated answers. We conduct a series of experiments to analyze the sensitivity of the system to different perturbation methods, the relationship between graph component importance and their structural positions, the influence of semantic node types, and how graph metrics correspond to the influence of components within the explanations process.</li>
</ul>

<h3>Title: Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Yihang Wu, Songhua Li, Wenjie Ma, Xin Ma, Qiang Li, Qi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08448">https://arxiv.org/abs/2507.08448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08448">https://arxiv.org/pdf/2507.08448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08448]] Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT(https://arxiv.org/abs/2507.08448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>3D reconstruction, which aims to recover the dense three-dimensional structure of a scene, is a cornerstone technology for numerous applications, including augmented/virtual reality, autonomous driving, and robotics. While traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo (MVS) achieve high precision through iterative optimization, they are limited by complex workflows, high computational cost, and poor robustness in challenging scenarios like texture-less regions. Recently, deep learning has catalyzed a paradigm shift in 3D reconstruction. A new family of models, exemplified by DUSt3R, has pioneered a feed-forward approach. These models employ a unified deep network to jointly infer camera poses and dense geometry directly from an Unconstrained set of images in a single forward pass. This survey provides a systematic review of this emerging domain. We begin by dissecting the technical framework of these feed-forward models, including their Transformer-based correspondence modeling, joint pose and geometry regression mechanisms, and strategies for scaling from two-view to multi-view scenarios. To highlight the disruptive nature of this new paradigm, we contrast it with both traditional pipelines and earlier learning-based methods like MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation metrics. Finally, we discuss the technology's broad application prospects and identify key future challenges and opportunities, such as model accuracy and scalability, and handling dynamic scenes.</li>
</ul>

<h3>Title: Space filling positionality and the Spiroformer</h3>
<ul>
<li><strong>Authors: </strong>M. Maurin, M.Á. Evangelista-Alvarado, P. Suárez-Serrato</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG, math.DS, math.SG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08456">https://arxiv.org/abs/2507.08456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08456">https://arxiv.org/pdf/2507.08456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08456]] Space filling positionality and the Spiroformer(https://arxiv.org/abs/2507.08456)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers excel when dealing with sequential data. Generalizing transformer models to geometric domains, such as manifolds, we encounter the problem of not having a well-defined global order. We propose a solution with attention heads following a space-filling curve. As a first experimental example, we present the Spiroformer, a transformer that follows a polar spiral on the $2$-sphere.</li>
</ul>

<h3>Title: A document is worth a structured record: Principled inductive bias design for document recognition</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Meyer, Lukas Tuggener, Sascha Hänzi, Daniel Schmid, Erdal Ayfer, Benjamin F. Grewe, Ahmed Abdulkadir, Thilo Stadelmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08458">https://arxiv.org/abs/2507.08458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08458">https://arxiv.org/pdf/2507.08458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08458]] A document is worth a structured record: Principled inductive bias design for document recognition(https://arxiv.org/abs/2507.08458)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Many document types use intrinsic, convention-driven structures that serve to encode precise and structured information, such as the conventions governing engineering drawings. However, state-of-the-art approaches treat document recognition as a mere computer vision problem, neglecting these underlying document-type-specific structural properties, making them dependent on sub-optimal heuristic post-processing and rendering many less frequent or more complicated document types inaccessible to modern document recognition. We suggest a novel perspective that frames document recognition as a transcription task from a document to a record. This implies a natural grouping of documents based on the intrinsic structure inherent in their transcription, where related document types can be treated (and learned) similarly. We propose a method to design structure-specific inductive biases for the underlying machine-learned end-to-end document recognition systems, and a respective base transformer architecture that we successfully adapt to different structures. We demonstrate the effectiveness of the so-found inductive biases in extensive experiments with progressively complex record structures from monophonic sheet music, shape drawings, and simplified engineering drawings. By integrating an inductive bias for unrestricted graph structures, we train the first-ever successful end-to-end model to transcribe engineering drawings to their inherently interlinked information. Our approach is relevant to inform the design of document recognition systems for document types that are less well understood than standard OCR, OMR, etc., and serves as a guide to unify the design of future document foundation models.</li>
</ul>

<h3>Title: Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Zishan Xu, Shuyi Xie, Qingsong Lv, Shupei Xiao, Linlin Song, Sui Wenjuan, Fan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08459">https://arxiv.org/abs/2507.08459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08459">https://arxiv.org/pdf/2507.08459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08459]] Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework(https://arxiv.org/abs/2507.08459)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the widespread application of Large Language Models (LLMs) in various tasks, the mainstream LLM platforms generate massive user-model interactions daily. In order to efficiently analyze the performance of models and diagnose failures in their answers, it is essential to develop an automated framework to systematically categorize and attribute errors. However, existing evaluation models lack error attribution capability. In this work, we establish a comprehensive Misattribution Framework with 6 primary and 15 secondary categories to facilitate in-depth analysis. Based on this framework, we present AttriData, a dataset specifically designed for error attribution, encompassing misattribution, along with the corresponding scores and feedback. We also propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first general-purpose judge model capable of simultaneously generating score, misattribution, and feedback. Extensive experiments and analyses are conducted to confirm the effectiveness and robustness of our proposed method.</li>
</ul>

<h3>Title: F3-Net: Foundation Model for Full Abnormality Segmentation of Medical Images with Flexible Input Modality Requirement</h3>
<ul>
<li><strong>Authors: </strong>Seyedeh Sahar Taheri Otaghsara, Reza Rahmanzadeh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08460">https://arxiv.org/abs/2507.08460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08460">https://arxiv.org/pdf/2507.08460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08460]] F3-Net: Foundation Model for Full Abnormality Segmentation of Medical Images with Flexible Input Modality Requirement(https://arxiv.org/abs/2507.08460)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>F3-Net is a foundation model designed to overcome persistent challenges in clinical medical image segmentation, including reliance on complete multimodal inputs, limited generalizability, and narrow task specificity. Through flexible synthetic modality training, F3-Net maintains robust performance even in the presence of missing MRI sequences, leveraging a zero-image strategy to substitute absent modalities without relying on explicit synthesis networks, thereby enhancing real-world applicability. Its unified architecture supports multi-pathology segmentation across glioma, metastasis, stroke, and white matter lesions without retraining, outperforming CNN-based and transformer-based models that typically require disease-specific fine-tuning. Evaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022, F3-Net demonstrates strong resilience to domain shifts and clinical heterogeneity. On the whole pathology dataset, F3-Net achieves average Dice Similarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET 2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a versatile, scalable solution bridging the gap between deep learning research and practical clinical deployment.</li>
</ul>

<h3>Title: Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study</h3>
<ul>
<li><strong>Authors: </strong>Marina Luketina, Andrea Benkel, Christoph G. Schuetz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08468">https://arxiv.org/abs/2507.08468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08468">https://arxiv.org/pdf/2507.08468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08468]] Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study(https://arxiv.org/abs/2507.08468)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper provides an experimental evaluation of the capability of large language models (LLMs) to assist in legal decision-making within the framework of Austrian and European Union value-added tax (VAT) law. In tax consulting practice, clients often describe cases in natural language, making LLMs a prime candidate for supporting automated decision-making and reducing the workload of tax professionals. Given the requirement for legally grounded and well-justified analyses, the propensity of LLMs to hallucinate presents a considerable challenge. The experiments focus on two common methods for enhancing LLM performance: fine-tuning and retrieval-augmented generation (RAG). In this study, these methods are applied on both textbook cases and real-world cases from a tax consulting firm to systematically determine the best configurations of LLM-based systems and assess the legal-reasoning capabilities of LLMs. The findings highlight the potential of using LLMs to support tax consultants by automating routine tasks and providing initial analyses, although current prototypes are not ready for full automation due to the sensitivity of the legal domain. The findings indicate that LLMs, when properly configured, can effectively support tax professionals in VAT tasks and provide legally grounded justifications for decisions. However, limitations remain regarding the handling of implicit client knowledge and context-specific documentation, underscoring the need for future integration of structured background information.</li>
</ul>

<h3>Title: Evaluating SAE interpretability without explanations</h3>
<ul>
<li><strong>Authors: </strong>Gonçalo Paulo, Nora Belrose</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08473">https://arxiv.org/abs/2507.08473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08473">https://arxiv.org/pdf/2507.08473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08473]] Evaluating SAE interpretability without explanations(https://arxiv.org/abs/2507.08473)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) and transcoders have become important tools for machine learning interpretability. However, measuring how interpretable they are remains challenging, with weak consensus about which benchmarks to use. Most evaluation procedures start by producing a single-sentence explanation for each latent. These explanations are then evaluated based on how well they enable an LLM to predict the activation of a latent in new contexts. This method makes it difficult to disentangle the explanation generation and evaluation process from the actual interpretability of the latents discovered. In this work, we adapt existing methods to assess the interpretability of sparse coders, with the advantage that they do not require generating natural language explanations as an intermediate step. This enables a more direct and potentially standardized assessment of interpretability. Furthermore, we compare the scores produced by our interpretability metrics with human evaluations across similar tasks and varying setups, offering suggestions for the community on improving the evaluation of these techniques.</li>
</ul>

<h3>Title: SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction</h3>
<ul>
<li><strong>Authors: </strong>Haitao Lin, Junjie Wang, Zhifeng Gao, Xiaohong Ji, Rong Zhu, Linfeng Zhang, Guolin Ke, Weinan E</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08475">https://arxiv.org/abs/2507.08475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08475">https://arxiv.org/pdf/2507.08475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08475]] SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction(https://arxiv.org/abs/2507.08475)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The essence of a chemical reaction lies in the redistribution and reorganization of electrons, which is often manifested through electron transfer or the migration of electron pairs. These changes are inherently discrete and abrupt in the physical world, such as alterations in the charge states of atoms or the formation and breaking of chemical bonds. To model the transition of states, we propose SynBridge, a bidirectional flow-based generative model to achieve multi-task reaction prediction. By leveraging a graph-to-graph transformer network architecture and discrete flow bridges between any two discrete distributions, SynBridge captures bidirectional chemical transformations between graphs of reactants and products through the bonds' and atoms' discrete states. We further demonstrate the effectiveness of our method through extensive experiments on three benchmark datasets (USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in both forward and retrosynthesis tasks. Our ablation studies and noise scheduling analysis reveal the benefits of structured diffusion over discrete spaces for reaction prediction.</li>
</ul>

<h3>Title: ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qingliang Meng, Hao Wu, Wei Liang, Wei Xu, Qing Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08477">https://arxiv.org/abs/2507.08477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08477">https://arxiv.org/pdf/2507.08477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08477]] ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition(https://arxiv.org/abs/2507.08477)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The deep integration of large language models and automatic speech recognition systems has become a promising research direction with high practical value. To address the overfitting issue commonly observed in Low-Rank Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work proposes an innovative training paradigm Iterative LoRA Training (ILT) in combination with an Iterative Pseudo Labeling strategy, effectively enhancing the theoretical upper bound of model performance. Based on Whisper-large-v3 and Qwen2-Audio, we conduct systematic experiments using a three-stage training process: Focus Training, Feed Back Training, and Fix Training. Experimental results demonstrate the effectiveness of the proposed method. Furthermore, the MegaAIS research team applied this technique in the Interspeech 2025 Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM), achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2 (Speech Separation and Recognition Task), showcasing the practical feasibility and strong application potential of our approach.</li>
</ul>

<h3>Title: A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench</h3>
<ul>
<li><strong>Authors: </strong>David Schlangen, Sherzod Hakimov, Jonathan Jordan, Philipp Sadler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08491">https://arxiv.org/abs/2507.08491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08491">https://arxiv.org/pdf/2507.08491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08491]] A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench(https://arxiv.org/abs/2507.08491)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There are currently two main paradigms for evaluating large language models (LLMs), reference-based evaluation and preference-based evaluation. The first, carried over from the evaluation of machine learning models in general, relies on pre-defined task instances, for which reference task executions are available. The second, best exemplified by the LM-arena, relies on (often self-selected) users bringing their own intents to a site that routes these to several models in parallel, among whose responses the user then selects their most preferred one. The former paradigm hence excels at control over what is tested, while the latter comes with higher ecological validity, testing actual use cases interactively. Recently, a third complementary paradigm has emerged that combines some of the strengths of these approaches, offering control over multi-turn, reference-free, repeatable interactions, while stressing goal-directedness: dialogue game based evaluation. While the utility of this approach has been shown by several projects, its adoption has been held back by the lack of a mature, easily re-usable implementation. In this paper, we present clembench, which has been in continuous development since 2023 and has in its latest release been optimized for ease of general use. We describe how it can be used to benchmark one's own models (using a provided set of benchmark game instances in English), as well as how easily the benchmark itself can be extended with new, tailor-made targeted tests.</li>
</ul>

<h3>Title: LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning</h3>
<ul>
<li><strong>Authors: </strong>Shibo Sun, Xue Li, Donglin Di, Mingjie Wei, Lanshun Nie, Wei-Nan Zhang, Dechen Zhan, Yang Song, Lei Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08496">https://arxiv.org/abs/2507.08496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08496">https://arxiv.org/pdf/2507.08496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08496]] LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning(https://arxiv.org/abs/2507.08496)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have advanced procedural planning for embodied AI systems through strong reasoning abilities, the integration of multimodal inputs and counterfactual reasoning remains underexplored. To tackle these challenges, we introduce LLaPa, a vision-language model framework designed for multimodal procedural planning. LLaPa generates executable action sequences from textual task descriptions and visual environmental images using vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary modules to improve procedural planning. The first module, the Task-Environment Reranker (TER), leverages task-oriented segmentation to create a task-sensitive feature space, aligning textual descriptions with visual environments and emphasizing critical regions for procedural execution. The second module, the Counterfactual Activities Retriever (CAR), identifies and emphasizes potential counterfactual conditions, enhancing the model's reasoning capability in counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED benchmarks demonstrate that LLaPa generates higher-quality plans with superior LCS and correctness, outperforming advanced models. The code and models are available this https URL.</li>
</ul>

<h3>Title: Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop</h3>
<ul>
<li><strong>Authors: </strong>Mengze Hong, Chen Jason Zhang, Di Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08498">https://arxiv.org/abs/2507.08498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08498">https://arxiv.org/pdf/2507.08498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08498]] Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop(https://arxiv.org/abs/2507.08498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic model used for uncovering abstract topics within document collections. In this paper, we explore the effectiveness of augmenting topic models with Large Language Models (LLMs) through integration into two key phases: Initialization and Post-Correction. Since the LDA is highly dependent on the quality of its initialization, we conduct extensive experiments on the LLM-guided topic clustering for initializing the Gibbs sampling algorithm. Interestingly, the experimental results reveal that while the proposed initialization strategy improves the early iterations of LDA, it has no effect on the convergence and yields the worst performance compared to the baselines. The LLM-enabled post-correction, on the other hand, achieved a promising improvement of 5.86% in the coherence evaluation. These results highlight the practical benefits of the LLM-in-the-loop approach and challenge the belief that LLMs are always the superior text mining alternative.</li>
</ul>

<h3>Title: PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Huang, Xia Cui</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08499">https://arxiv.org/abs/2507.08499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08499">https://arxiv.org/pdf/2507.08499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08499]] PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts(https://arxiv.org/abs/2507.08499)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in Text-Based Emotion Detection (Track A), which focuses on multi-label emotion detection in short texts. We propose a feature-centric framework that dynamically adapts document representations and learning algorithms to optimize language-specific performance. Our study evaluates three key components: document representation, dimensionality reduction, and model training in 28 languages, highlighting five for detailed analysis. The results show that TF-IDF remains highly effective for low-resource languages, while contextual embeddings like FastText and transformer-based document representations, such as those produced by Sentence-BERT, exhibit language-specific strengths. Principal Component Analysis (PCA) reduces training time without compromising performance, particularly benefiting FastText and neural models such as Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores the trade-off between model complexity and processing cost. Our framework provides a scalable solution for multilingual emotion detection, addressing the challenges of linguistic diversity and resource constraints.</li>
</ul>

<h3>Title: Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R</h3>
<ul>
<li><strong>Authors: </strong>Pablo Robin Guerrero, Yueyang Pan, Sanidhya Kashyap</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08505">https://arxiv.org/abs/2507.08505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08505">https://arxiv.org/pdf/2507.08505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08505]] Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R(https://arxiv.org/abs/2507.08505)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) offer promising capabilities for mobile devices, but their deployment faces significant challenges due to computational limitations and energy inefficiency, especially for real-time applications. This study provides a comprehensive survey of deployment frameworks for VLMs on mobile devices, evaluating this http URL, MLC-Imp, and mllm in the context of running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R while running VLMs, with measurements covering CPU, GPU, and NPU utilization, temperature, inference time, power consumption, and user experience. Benchmarking revealed critical performance bottlenecks across frameworks: CPU resources were consistently over-utilized during token generation, while GPU and NPU accelerators were largely unused. When the GPU was used, primarily for image feature extraction, it was saturated, leading to degraded device responsiveness. The study contributes framework-level benchmarks, practical profiling tools, and an in-depth analysis of hardware utilization bottlenecks, highlighting the consistent overuse of CPUs and the ineffective or unstable use of GPUs and NPUs in current deployment frameworks.</li>
</ul>

<h3>Title: SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Haotian Xu, Jinrui Zhou, Xichong Zhang, Mingjun Xiao, He Sun, Yin Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08508">https://arxiv.org/abs/2507.08508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08508">https://arxiv.org/pdf/2507.08508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08508]] SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation(https://arxiv.org/abs/2507.08508)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed machine learning paradigm which coordinates multiple clients to collaboratively train a global model via a central server. Sequential Federated Learning (SFL) is a newly-emerging FL training framework where the global model is trained in a sequential manner across clients. Since SFL can provide strong convergence guarantees under data heterogeneity, it has attracted significant research attention in recent years. However, experiments show that SFL suffers from severe catastrophic forgetting in heterogeneous environments, meaning that the model tends to forget knowledge learned from previous clients. To address this issue, we propose an SFL framework with discrepancy-aware multi-teacher knowledge distillation, called SFedKD, which selects multiple models from the previous round to guide the current round of training. In SFedKD, we extend the single-teacher Decoupled Knowledge Distillation approach to our multi-teacher setting and assign distinct weights to teachers' target-class and non-target-class knowledge based on the class distributional discrepancy between teacher and student data. Through this fine-grained weighting strategy, SFedKD can enhance model training efficacy while mitigating catastrophic forgetting. Additionally, to prevent knowledge dilution, we eliminate redundant teachers for the knowledge distillation and formalize it as a variant of the maximum coverage problem. Based on the greedy strategy, we design a complementary-based teacher selection mechanism to ensure that the selected teachers achieve comprehensive knowledge space coverage while reducing communication and computational costs. Extensive experiments show that SFedKD effectively overcomes catastrophic forgetting in SFL and outperforms state-of-the-art FL methods.</li>
</ul>

<h3>Title: Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Yufei Zheng, Wenjun Wang, Wenjun Gan, Jiawei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08520">https://arxiv.org/abs/2507.08520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08520">https://arxiv.org/pdf/2507.08520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08520]] Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification(https://arxiv.org/abs/2507.08520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Occluded person re-identification aims to retrieve holistic images based on occluded ones. Existing methods often rely on aligning visible body parts, applying occlusion augmentation, or complementing missing semantics using holistic images. However, they face challenges in handling diverse occlusion scenarios not seen during training and the issue of feature contamination from holistic images. To address these limitations, we propose Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation (OGFR), which simultaneously mitigates these challenges. OGFR adopts a teacher-student distillation architecture that effectively incorporates diverse occlusion patterns into feature representation while transferring the purified discriminative holistic knowledge from the holistic to the occluded branch through reinforced knowledge distillation. Specifically, an Occlusion-Aware Vision Transformer is designed to leverage learnable occlusion pattern embeddings to explicitly model such diverse occlusion types, thereby guiding occlusion-aware robust feature representation. Moreover, we devise a Feature Erasing and Purification Module within the holistic branch, in which an agent is employed to identify low-quality patch tokens of holistic images that contain noisy negative information via deep reinforcement learning, and substitute these patch tokens with learnable embedding tokens to avoid feature contamination and further excavate identity-related discriminative clues. Afterward, with the assistance of knowledge distillation, the student branch effectively absorbs the purified holistic knowledge to precisely learn robust representation regardless of the interference of occlusions.</li>
</ul>

<h3>Title: The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>David Pomerenke, Jonas Nothnagel, Simon Ostermann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08538">https://arxiv.org/abs/2507.08538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08538">https://arxiv.org/pdf/2507.08538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08538]] The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks(https://arxiv.org/abs/2507.08538)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To ensure equitable access to the benefits of large language models (LLMs), it is essential to evaluate their capabilities across the world's languages. We introduce the AI Language Proficiency Monitor, a comprehensive multilingual benchmark that systematically assesses LLM performance across up to 200 languages, with a particular focus on low-resource languages. Our benchmark aggregates diverse tasks including translation, question answering, math, and reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We provide an open-source, auto-updating leaderboard and dashboard that supports researchers, developers, and policymakers in identifying strengths and gaps in model performance. In addition to ranking models, the platform offers descriptive insights such as a global proficiency map and trends over time. By complementing and extending prior multilingual benchmarks, our work aims to foster transparency, inclusivity, and progress in multilingual AI. The system is available at this https URL.</li>
</ul>

<h3>Title: White-Basilisk: A Hybrid Model for Code Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08540">https://arxiv.org/abs/2507.08540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08540">https://arxiv.org/pdf/2507.08540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08540]] White-Basilisk: A Hybrid Model for Code Vulnerability Detection(https://arxiv.org/abs/2507.08540)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The model's capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications.</li>
</ul>

<h3>Title: CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes</h3>
<ul>
<li><strong>Authors: </strong>Tianyou Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08542">https://arxiv.org/abs/2507.08542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08542">https://arxiv.org/pdf/2507.08542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08542]] CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes(https://arxiv.org/abs/2507.08542)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Circular RNAs (circRNAs) are important components of the non-coding RNA regulatory network. Previous circRNA identification primarily relies on high-throughput RNA sequencing (RNA-seq) data combined with alignment-based algorithms that detect back-splicing signals. However, these methods face several limitations: they can't predict circRNAs directly from genomic DNA sequences and relies heavily on RNA experimental data; they involve high computational costs due to complex alignment and filtering steps; and they are inefficient for large-scale or genome-wide circRNA prediction. The challenge is even greater in plants, where plant circRNA splice sites often lack the canonical GT-AG motif seen in human mRNA splicing, and no efficient deep learning model with strong generalization capability currently exists. Furthermore, the number of currently identified plant circRNAs is likely far lower than their true abundance. In this paper, we propose a deep learning framework named CircFormerMoE based on transformers and mixture-of experts for predicting circRNAs directly from plant genomic DNA. Our framework consists of two subtasks known as splicing site detection (SSD) and splicing site pairing (SSP). The model's effectiveness has been validated on gene data of 10 plant species. Trained on known circRNA instances, it is also capable of discovering previously unannotated circRNAs. In addition, we performed interpretability analyses on the trained model to investigate the sequence patterns contributing to its predictions. Our framework provides a fast and accurate computational method and tool for large-scale circRNA discovery in plants, laying a foundation for future research in plant functional genomics and non-coding RNA annotation.</li>
</ul>

<h3>Title: RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features</h3>
<ul>
<li><strong>Authors: </strong>Inye Na, Nejung Rue, Jiwon Chung, Hyunjin Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08546">https://arxiv.org/abs/2507.08546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08546">https://arxiv.org/pdf/2507.08546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08546]] RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features(https://arxiv.org/abs/2507.08546)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image retrieval is a valuable field for supporting clinical decision-making, yet current methods primarily support 2D images and require fully annotated queries, limiting clinical flexibility. To address this, we propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging handcrafted radiomics descriptors with deep learning-based embeddings at the tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits volumetric data to leverage richer spatial context in medical images. We employ a promptable segmentation model (e.g., SAM) to derive tumor-specific image embeddings, which are aligned with radiomics features extracted from the same tumor via contrastive learning. These representations are further enriched by anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables flexible querying based on shape, location, or partial feature sets. Extensive experiments on both lung CT and brain MRI public datasets demonstrate that radiomics features significantly enhance retrieval specificity, while APE provides global anatomical context essential for location-based searches. Notably, our framework requires only minimal user prompts (e.g., a single point), minimizing segmentation overhead and supporting diverse clinical scenarios. The capability to query using either image embeddings or selected radiomics attributes highlights its adaptability, potentially benefiting diagnosis, treatment planning, and research on large-scale medical imaging repositories. Our code is available at this https URL.</li>
</ul>

<h3>Title: SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2</h3>
<ul>
<li><strong>Authors: </strong>Alen Adamyan, Tomáš Čížek, Matej Straka, Klara Janouskova, Martin Schmid</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08548">https://arxiv.org/abs/2507.08548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08548">https://arxiv.org/pdf/2507.08548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08548]] SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2(https://arxiv.org/abs/2507.08548)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.</li>
</ul>

<h3>Title: Image Translation with Kernel Prediction Networks for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Cristina Mata, Michael S. Ryoo, Henrik Turbell</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08554">https://arxiv.org/abs/2507.08554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08554">https://arxiv.org/pdf/2507.08554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08554]] Image Translation with Kernel Prediction Networks for Semantic Segmentation(https://arxiv.org/abs/2507.08554)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation relies on many dense pixel-wise annotations to achieve the best performance, but owing to the difficulty of obtaining accurate annotations for real world data, practitioners train on large-scale synthetic datasets. Unpaired image translation is one method used to address the ensuing domain gap by generating more realistic training data in low-data regimes. Current methods for unpaired image translation train generative adversarial networks (GANs) to perform the translation and enforce pixel-level semantic matching through cycle consistency. These methods do not guarantee that the semantic matching holds, posing a problem for semantic segmentation where performance is sensitive to noisy pixel labels. We propose a novel image translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that guarantees semantic matching between the synthetic label and translation. DA-KPN estimates pixel-wise input transformation parameters of a lightweight and simple translation function. To ensure the pixel-wise transformation is realistic, DA-KPN uses multi-scale discriminators to distinguish between translated and target samples. We show DA-KPN outperforms previous GAN-based methods on syn2real benchmarks for semantic segmentation with limited access to real image labels and achieves comparable performance on face parsing.</li>
</ul>

<h3>Title: STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Ning, Zilin Bian, Kaan Ozbay, Semiha Ergan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08563">https://arxiv.org/abs/2507.08563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08563">https://arxiv.org/pdf/2507.08563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08563]] STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving(https://arxiv.org/abs/2507.08563)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate vehicle trajectory prediction is essential for ensuring safety and efficiency in fully autonomous driving systems. While existing methods primarily focus on modeling observed motion patterns and interactions with other vehicles, they often neglect the potential risks posed by the uncertain or aggressive behaviors of surrounding vehicles. In this paper, we propose a novel spatial-temporal risk-attentive trajectory prediction framework that incorporates a risk potential field to assess perceived risks arising from behaviors of nearby vehicles. The framework leverages a spatial-temporal encoder and a risk-attentive feature fusion decoder to embed the risk potential field into the extracted spatial-temporal feature representations for trajectory prediction. A risk-scaled loss function is further designed to improve the prediction accuracy of high-risk scenarios, such as short relative spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate that our method reduces average prediction errors by 4.8% and 31.2% respectively compared to state-of-the-art approaches, especially in high-risk scenarios. The proposed framework provides interpretable, risk-aware predictions, contributing to more robust decision-making for autonomous driving systems.</li>
</ul>

<h3>Title: AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Preslav Aleksandrov, Meghdad Kurmanji, Fernando Garcia Redondo, David O'Shea, William Shen, Alex Iacob, Lorenzo Sani, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08567">https://arxiv.org/abs/2507.08567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08567">https://arxiv.org/pdf/2507.08567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08567]] AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling(https://arxiv.org/abs/2507.08567)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a novel recursive generalization of the encoder-only Transformer architecture, which achieves better perplexity than a standard Transformer and allows for the dynamic scaling of compute resources at test time. This simple, recursive approach is a complement to scaling large language model (LLM) performance through parameter and token counts. AbbIE performs its iterations in latent space, but unlike latent reasoning models, does not require a specialized dataset or training protocol. We show that AbbIE upward generalizes (ability to generalize to arbitrary iteration lengths) at test time by only using 2 iterations during train time, far outperforming alternative iterative methods. AbbIE's ability to scale its computational expenditure based on the complexity of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context learning tasks versus other iterative and standard methods and up to 5\% improvement in language perplexity. The results from this study open a new avenue to Transformer performance scaling. We perform all of our evaluations on model sizes up to 350M parameters.</li>
</ul>

<h3>Title: A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Mingda Zhang, Kaiwen Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08574">https://arxiv.org/abs/2507.08574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08574">https://arxiv.org/pdf/2507.08574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08574]] A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism(https://arxiv.org/abs/2507.08574)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study aims to develop a novel multi-modal fusion framework for brain tumor segmentation that integrates spatial-language-vision information through bidirectional interactive attention mechanisms to improve segmentation accuracy and boundary delineation. Methods: We propose two core components: Multi-modal Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text descriptions through hierarchical semantic decoupling, and Bidirectional Interactive Visual-semantic Attention (BIVA) enabling iterative information exchange between modalities. The framework was evaluated on BraTS 2020 dataset comprising 369 multi-institutional MRI scans. Results: The proposed method achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of 2.8256mm across enhancing tumor, tumor core, and whole tumor regions, outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D U-Net. Ablation studies confirmed critical contributions of semantic and spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion combined with bidirectional interactive attention significantly enhances brain tumor segmentation performance, establishing new paradigms for integrating clinical knowledge into medical image analysis.</li>
</ul>

<h3>Title: Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India</h3>
<ul>
<li><strong>Authors: </strong>Ando Shah, Rajveer Singh, Akram Zaytar, Girmaw Abebe Tadesse, Caleb Robinson, Negar Tafti, Stephen A. Wood, Rahul Dodhia, Juan M. Lavista Ferres</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08605">https://arxiv.org/abs/2507.08605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08605">https://arxiv.org/pdf/2507.08605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08605]] Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India(https://arxiv.org/abs/2507.08605)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, generative</a></li>
<li><strong>Abstract: </strong>Rice cultivation consumes 24-30% of global freshwater, creating critical water management challenges in major rice-producing regions. Sustainable irrigation practices like direct seeded rice (DSR) and alternate wetting and drying (AWD) can reduce water use by 20-40% while maintaining yields, helping secure long-term agricultural productivity as water scarcity intensifies - a key component of the Zero Hunger Sustainable Development Goal. However, limited data on adoption rates of these practices prevents evidence-based policymaking and targeted resource allocation. We developed a novel remote sensing framework to monitor sustainable water management practices at scale in Punjab, India - a region facing severe groundwater depletion of 41.6 cm/year. To collect essential ground truth data, we partnered with the Nature Conservancy's Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained approximately 1,400 farmers on water-saving techniques while documenting their field-level practices. Using this data, we created a classification system with Sentinel-1 satellite imagery that separates water management along sowing and irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing DSR from traditional puddled transplanted rice without requiring prior knowledge of planting dates. We demonstrated scalability by mapping DSR adoption across approximately 3 million agricultural plots in Punjab, with district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77) with government records. This study provides policymakers with a powerful tool to track sustainable water management adoption, target interventions, and measure program impacts at scale.</li>
</ul>

<h3>Title: Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data</h3>
<ul>
<li><strong>Authors: </strong>Parag Dutta, Ambedkar Dukkipati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08610">https://arxiv.org/abs/2507.08610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08610">https://arxiv.org/pdf/2507.08610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08610]] Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data(https://arxiv.org/abs/2507.08610)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Image captioning is an important problem in developing various AI systems, and these tasks require large volumes of annotated images to train the models. Since all existing labelled datasets are already used for training the large Vision Language Models (VLMs), it becomes challenging to improve the performance of the same. Considering this, it is essential to consider the unsupervised image captioning performance, which remains relatively under-explored. To that end, we propose LoGIC (Lewis Communication Game for Image Captioning), a Multi-agent Reinforcement Learning game. The proposed method consists of two agents, a 'speaker' and a 'listener', with the objective of learning a strategy for communicating in natural language. We train agents in the cooperative common-reward setting using the GRPO algorithm and show that improvement in image captioning performance emerges as a consequence of the agents learning to play the game. We show that using pre-trained VLMs as the 'speaker' and Large Language Model (LLM) for language understanding in the 'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without additional labels, a $2$ units advantage in absolute metrics compared to the $44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the 'speaker' with lightweight components: (i) a ViT for image perception and (ii) a GPT2 language generation, and train them from scratch using LoGIC, obtaining a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over existing unsupervised image-captioning methods.</li>
</ul>

<h3>Title: Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift</h3>
<ul>
<li><strong>Authors: </strong>Tianrun Yu, Jiaqi Wang, Haoyu Wang, Mingquan Lin, Han Liu, Nelson S. Yee, Fenglong Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08617">https://arxiv.org/abs/2507.08617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08617">https://arxiv.org/pdf/2507.08617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08617]] Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift(https://arxiv.org/abs/2507.08617)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Collaborative fairness is a crucial challenge in federated learning. However, existing approaches often overlook a practical yet complex form of heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of this setting, which motivates the design of FedAKD (Federated Asynchronous Knowledge Distillation)- simple yet effective approach that balances accurate prediction with collaborative fairness. FedAKD consists of client and server updates. In the client update, we introduce a novel asynchronous knowledge distillation strategy based on our preliminary analysis, which reveals that while correctly predicted samples exhibit similar feature distributions across clients, incorrectly predicted samples show significant variability. This suggests that imbalanced covariate shift primarily arises from misclassified samples. Leveraging this insight, our approach first applies traditional knowledge distillation to update client models while keeping the global model fixed. Next, we select correctly predicted high-confidence samples and update the global model using these samples while keeping client models fixed. The server update simply aggregates all client models. We further provide a theoretical proof of FedAKD's convergence. Experimental results on public datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records (EHR) dataset demonstrate that FedAKD significantly improves collaborative fairness, enhances predictive accuracy, and fosters client participation even under highly heterogeneous data distributions.</li>
</ul>

<h3>Title: A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1</h3>
<ul>
<li><strong>Authors: </strong>Marcin Pietroń, Rafał Olszowski, Jakub Gomułka, Filip Gampel, Andrzej Tomski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08621">https://arxiv.org/abs/2507.08621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08621">https://arxiv.org/pdf/2507.08621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08621]] A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1(https://arxiv.org/abs/2507.08621)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLM's, using diverse datasets such as this http URL and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings.</li>
</ul>

<h3>Title: Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates</h3>
<ul>
<li><strong>Authors: </strong>Natalia Bottaioli (Université Paris-Saclay, ENS Paris-Saclay, CNRS, Centre Borelli, France, Facultad de Ingeniería, Universidad de la República, Montevideo, Uruguay, Digital Sense, Montevideo, Uruguay)Solène Tarride (TEKLIA, Paris, France)Jérémy Anger (Université Paris-Saclay, ENS Paris-Saclay, CNRS, Centre Borelli, France)Seginus Mowlavi (Université Paris-Saclay, ENS Paris-Saclay, CNRS, Centre Borelli, France)Marina Gardella (IMPA, Rio de Janeiro, Brazil)Antoine Tadros (Université Paris-Saclay, ENS Paris-Saclay, CNRS, Centre Borelli, France)Gabriele Facciolo (Université Paris-Saclay, ENS Paris-Saclay, CNRS, Centre Borelli, France)Rafael Grompone von Gioi (Université Paris-Saclay, ENS Paris-Saclay, CNRS, Centre Borelli, France)Christopher Kermorvant (TEKLIA, Paris, France)Jean-Michel Morel (City University of Hong Kong, Hong Kong)Javier Preciozzi (Facultad de Ingeniería, Universidad de la República, Montevideo, Uruguay, Digital Sense, Montevideo, Uruguay)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08636">https://arxiv.org/abs/2507.08636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08636">https://arxiv.org/pdf/2507.08636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08636]] Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates(https://arxiv.org/abs/2507.08636)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This study evaluates the recently proposed Document Attention Network (DAN) for extracting key-value information from Uruguayan birth certificates, handwritten in Spanish. We investigate two annotation strategies for automatically transcribing handwritten documents, fine-tuning DAN with minimal training data and annotation effort. Experiments were conducted on two datasets containing the same images (201 scans of birth certificates written by more than 15 different writers) but with different annotation methods. Our findings indicate that normalized annotation is more effective for fields that can be standardized, such as dates and places of birth, whereas diplomatic annotation performs much better for fields containing names and surnames, which can not be standardized.</li>
</ul>

<h3>Title: Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)</h3>
<ul>
<li><strong>Authors: </strong>Vincenzo Dentamaro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08637">https://arxiv.org/abs/2507.08637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08637">https://arxiv.org/pdf/2507.08637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08637]] Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)(https://arxiv.org/abs/2507.08637)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models are computationally costly on long sequences since regular attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time complexity that is pivotal to enable successful long-sequence processing without the performance trade-off. WERSA merges content-adaptive random spectral features together with multi-resolution Haar wavelets and learnable parameters to selectively attend to informative scales of data while preserving linear efficiency. Large-scale comparisons \textbf{on single GPU} and across various benchmarks (vision, NLP, hierarchical reasoning) and various attention mechanisms (like Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer, Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in all tests. On ArXiv classification, WERSA improves accuracy over vanilla attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable methods, operating on data that gives Out-Of-Memory errors to quadratic methods while being \textbf{twice as fast} as Waveformer, its next-best competitor. By significantly reducing computational loads without compromising accuracy, WERSA makes possible more practical, more affordable, long-context models, in particular on low-resource hardware, for more sustainable and more scalable AI development.</li>
</ul>

<h3>Title: DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images</h3>
<ul>
<li><strong>Authors: </strong>Haoran Sun, Haoyu Bian, Shaoning Zeng, Yunbo Rao, Xu Xu, Lin Mei, Jianping Gou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08648">https://arxiv.org/abs/2507.08648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08648">https://arxiv.org/pdf/2507.08648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08648]] DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images(https://arxiv.org/abs/2507.08648)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Common knowledge indicates that the process of constructing image datasets usually depends on the time-intensive and inefficient method of manual collection and annotation. Large models offer a solution via data generation. Nonetheless, real-world data are obviously more valuable comparing to artificially intelligence generated data, particularly in constructing image datasets. For this reason, we propose a novel method for auto-constructing datasets from real-world images by a multiagent collaborative system, named as DatasetAgent. By coordinating four different agents equipped with Multi-modal Large Language Models (MLLMs), as well as a tool package for image optimization, DatasetAgent is able to construct high-quality image datasets according to user-specified requirements. In particular, two types of experiments are conducted, including expanding existing datasets and creating new ones from scratch, on a variety of open-source datasets. In both cases, multiple image datasets constructed by DatasetAgent are used to train various vision models for image classification, object detection, and image segmentation.</li>
</ul>

<h3>Title: Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Zach Eidex, Mojtaba Safari, Tonghe Wang, Vanessa Wildman, David S. Yu, Hui Mao, Erik Middlebrooks, Aparna Kesewala, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08655">https://arxiv.org/abs/2507.08655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08655">https://arxiv.org/pdf/2507.08655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08655]] Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model(https://arxiv.org/abs/2507.08655)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over standard clinical field strengths (1.5T, 3T). However, 7T scanners are costly, scarce, and introduce additional challenges such as susceptibility artifacts. We propose an efficient transformer-based model (7T-Restormer) to synthesize 7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods: Our model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding 7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128 slices) were randomly divided into 105 (25; 80) training cases (19,204 slices), 19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145 slices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans, respectively. The synthetic 7T T1 maps were compared against the ResViT and ResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/- 4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs, and 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using 10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter ResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter ResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T (0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus was superior to single-field strategies. Restricting the model to 1.5T increased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely on 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We propose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T and 3T T1W scans with higher quality than existing state-of-the-art methods. Our approach makes the benefits of 7T MRI more accessible to standard clinical workflows.</li>
</ul>

<h3>Title: KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiyao Zhang, Chengli Zhong, Hui Xu, Qige Li, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08665">https://arxiv.org/abs/2507.08665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08665">https://arxiv.org/pdf/2507.08665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08665]] KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment(https://arxiv.org/abs/2507.08665)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) show promising progress in formalizing informal mathematics into machine-verifiable theorems. However, these methods still face bottlenecks due to the limited quantity and quality of multilingual parallel corpora. In this paper, we propose a novel neuro-symbolic framework KELPS (Knowledge-Equation based Logical Processing System) to address these problems. KELPS is an iterative framework for translating, synthesizing, and filtering informal data into multiple formal languages (Lean, Coq, and Isabelle). First, we translate natural language into Knowledge Equations (KEs), a novel language that we designed, theoretically grounded in assertional logic. Next, we convert them to target languages through rigorously defined rules that preserve both syntactic structure and semantic meaning. This process yielded a parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3 (81%) and Herald (81.3%) across multiple datasets. All datasets and codes are available in the supplementary materials.</li>
</ul>

<h3>Title: ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way</h3>
<ul>
<li><strong>Authors: </strong>Rajarshi Roy, Devleena Das, Ankesh Banerjee, Arjya Bhattacharjee, Kousik Dasgupta, Subarna Tripathi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08679">https://arxiv.org/abs/2507.08679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08679">https://arxiv.org/pdf/2507.08679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08679]] ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way(https://arxiv.org/abs/2507.08679)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce ByDeWay, a training-free framework designed to enhance the performance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel prompting strategy called Layered-Depth-Based Prompting (LDP), which improves spatial reasoning and grounding without modifying any model parameters. It segments the scene into closest, mid-range, and farthest layers using monocular depth estimation, then generates region-specific captions with a grounded vision-language model. These structured, depth-aware captions are appended to the image-question prompt, enriching it with spatial context. This guides MLLMs to produce more grounded and less hallucinated responses. Our method is lightweight, modular, and compatible with black-box MLLMs. Experiments on hallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show consistent improvements across multiple MLLMs, validating the effectiveness of depth-aware prompting in a zero-training setting.</li>
</ul>

<h3>Title: MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Debashis Gupta, Aditi Golder, Rongkhun Zhu, Kangning Cui, Wei Tang, Fan Yang, Ovidiu Csillik, Sarra Alaqahtani, V. Paul Pauca</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08683">https://arxiv.org/abs/2507.08683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08683">https://arxiv.org/pdf/2507.08683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08683]] MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing(https://arxiv.org/abs/2507.08683)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive learning (CL) has emerged as a powerful paradigm for learning transferable representations without the reliance on large labeled datasets. Its ability to capture intrinsic similarities and differences among data samples has led to state-of-the-art results in computer vision tasks. These strengths make CL particularly well-suited for Earth System Observation (ESO), where diverse satellite modalities such as optical and SAR imagery offer naturally aligned views of the same geospatial regions. However, ESO presents unique challenges, including high inter-class similarity, scene clutter, and ambiguous boundaries, which complicate representation learning -- especially in low-label, multi-label settings. Existing CL frameworks often focus on intra-modality self-supervision or lack mechanisms for multi-label alignment and semantic precision across modalities. In this work, we introduce MoSAiC, a unified framework that jointly optimizes intra- and inter-modality contrastive learning with a multi-label supervised contrastive loss. Designed specifically for multi-modal satellite imagery, MoSAiC enables finer semantic disentanglement and more robust representation learning across spectrally similar and spatially complex classes. Experiments on two benchmark datasets, BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both fully supervised and self-supervised baselines in terms of accuracy, cluster coherence, and generalization in low-label and high-class-overlap scenarios.</li>
</ul>

<h3>Title: An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan</h3>
<ul>
<li><strong>Authors: </strong>Mengyuan Liu, Jeongkyu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08690">https://arxiv.org/abs/2507.08690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08690">https://arxiv.org/pdf/2507.08690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08690]] An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan(https://arxiv.org/abs/2507.08690)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI) enables non-invasive, high-resolution analysis of muscle structures. However, automated segmentation remains limited by high computational costs, reliance on large training datasets, and reduced accuracy in segmenting smaller muscles. Convolutional neural network (CNN)-based methods, while powerful, often suffer from substantial computational overhead, limited generalizability, and poor interpretability across diverse populations. This study proposes a training-free segmentation approach based on keypoint tracking, which integrates keypoint selection with Lucas-Kanade optical flow. The proposed method achieves a mean Dice similarity coefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection strategy, performing comparably to state-of-the-art CNN-based models while substantially reducing computational demands and enhancing interpretability. This scalable framework presents a robust and explainable alternative for muscle segmentation in clinical and research applications.</li>
</ul>

<h3>Title: Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Waqar Muhammad Ashraf, Amir H. Keshavarzzadeh, Abdulelah S. Alshehri, Abdulrahman bin Jumah, Ramit Debnath, Vivek Dua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08697">https://arxiv.org/abs/2507.08697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08697">https://arxiv.org/pdf/2507.08697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08697]] Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning(https://arxiv.org/abs/2507.08697)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The domain-consistent adoption of artificial intelligence (AI) remains low in thermal power plants due to the black-box nature of AI algorithms and low representation of domain knowledge in conventional data-centric analytics. In this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT) framework that incorporates the Mahalanobis distance-based constraint to introduce domain knowledge into data-centric analytics. The developed MAD-OPT framework is applied to maximize thermal efficiency and minimize turbine heat rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT framework can estimate domain-informed optimal process conditions under different ambient conditions, and the optimal solutions are found to be robust as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to estimate optimal process conditions beyond the design power generation limit of the gas turbine system, and have found comparable results with the actual data of the power plant. We demonstrate that implementing data-centric optimization analytics without incorporating domain-informed constraints may provide ineffective solutions that may not be implementable in the real operation of the gas turbine system. This research advances the integration of the data-driven domain knowledge into machine learning-powered analytics that enhances the domain-informed operation excellence and paves the way for safe AI adoption in thermal power systems.</li>
</ul>

<h3>Title: KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Songlin Zhai, Guilin Qi, Yuan Meng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08704">https://arxiv.org/abs/2507.08704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08704">https://arxiv.org/pdf/2507.08704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08704]] KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation(https://arxiv.org/abs/2507.08704)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) play a critical role in enhancing large language models (LLMs) by introducing structured and grounded knowledge into the learning process. However, most existing KG-enhanced approaches rely on parameter-intensive fine-tuning, which risks catastrophic forgetting and degrades the pretrained model's generalization. Moreover, they exhibit limited adaptability to real-time knowledge updates due to their static integration frameworks. To address these issues, we introduce the first test-time KG-augmented framework for LLMs, built around a dedicated knowledge graph-guided attention (KGA) module that enables dynamic knowledge fusion without any parameter updates. The proposed KGA module augments the standard self-attention mechanism with two synergistic pathways: outward and inward aggregation. Specifically, the outward pathway dynamically integrates external knowledge into input representations via input-driven KG fusion. This inward aggregation complements the outward pathway by refining input representations through KG-guided filtering, suppressing task-irrelevant signals and amplifying knowledge-relevant patterns. Importantly, while the outward pathway handles knowledge fusion, the inward path selects the most relevant triples and feeds them back into the fusion process, forming a closed-loop enhancement mechanism. By synergistically combining these two pathways, the proposed method supports real-time knowledge fusion exclusively at test-time, without any parameter modification. Extensive experiments on five benchmarks verify the comparable knowledge fusion performance of KGA.</li>
</ul>

<h3>Title: SGPMIL: Sparse Gaussian Process Multiple Instance Learning</h3>
<ul>
<li><strong>Authors: </strong>Andreas Lolos, Stergios Christodoulidis, Maria Vakalopoulou, Jose Dolz, Aris Moustakas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08711">https://arxiv.org/abs/2507.08711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08711">https://arxiv.org/pdf/2507.08711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08711]] SGPMIL: Sparse Gaussian Process Multiple Instance Learning(https://arxiv.org/abs/2507.08711)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Multiple Instance Learning (MIL) offers a natural solution for settings where only coarse, bag-level labels are available, without having access to instance-level annotations. This is usually the case in digital pathology, which consists of gigapixel sized images. While deterministic attention-based MIL approaches achieve strong bag-level performance, they often overlook the uncertainty inherent in instance relevance. In this paper, we address the lack of uncertainty quantification in instance-level attention scores by introducing \textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in Sparse Gaussian Processes (SGP). By learning a posterior distribution over attention scores, SGPMIL enables principled uncertainty estimation, resulting in more reliable and calibrated instance relevance maps. Our approach not only preserves competitive bag-level performance but also significantly improves the quality and interpretability of instance-level predictions under uncertainty. SGPMIL extends prior work by introducing feature scaling in the SGP predictive mean function, leading to faster training, improved efficiency, and enhanced instance-level performance. Extensive experiments on multiple well-established digital pathology datasets highlight the effectiveness of our approach across both bag- and instance-level evaluations. Our code will be made publicly available.</li>
</ul>

<h3>Title: On the Effect of Regularization in Policy Mirror Descent</h3>
<ul>
<li><strong>Authors: </strong>Jan Felix Kleuker, Aske Plaat, Thomas Moerland</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08718">https://arxiv.org/abs/2507.08718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08718">https://arxiv.org/pdf/2507.08718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08718]] On the Effect of Regularization in Policy Mirror Descent(https://arxiv.org/abs/2507.08718)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Policy Mirror Descent (PMD) has emerged as a unifying framework in reinforcement learning (RL) by linking policy gradient methods with a first-order optimization method known as mirror descent. At its core, PMD incorporates two key regularization components: (i) a distance term that enforces a trust region for stable policy updates and (ii) an MDP regularizer that augments the reward function to promote structure and robustness. While PMD has been extensively studied in theory, empirical investigations remain scarce. This work provides a large-scale empirical analysis of the interplay between these two regularization techniques, running over 500k training seeds on small RL environments. Our results demonstrate that, although the two regularizers can partially substitute each other, their precise combination is critical for achieving robust performance. These findings highlight the potential for advancing research on more robust algorithms in RL, particularly with respect to hyperparameter sensitivity.</li>
</ul>

<h3>Title: Multilingual Multimodal Software Developer for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Linzheng Chai, Jian Yang, Shukai Liu, Wei Zhang, Liran Wang, Ke Jin, Tao Sun, Congnan Liu, Chenchen Zhang, Hualei Zhu, Jiaheng Liu, Xianjie Wu, Ge Zhang, Tianyu Liu, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08719">https://arxiv.org/abs/2507.08719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08719">https://arxiv.org/pdf/2507.08719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08719]] Multilingual Multimodal Software Developer for Code Generation(https://arxiv.org/abs/2507.08719)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has significantly improved code generation, yet most models remain text-only, neglecting crucial visual aids like diagrams and flowcharts used in real-world software development. To bridge this gap, we introduce MM-Coder, a Multilingual Multimodal software developer. MM-Coder integrates visual design inputs-Unified Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with textual instructions to enhance code generation accuracy and architectural alignment. To enable this, we developed MMc-Instruct, a diverse multimodal instruction-tuning dataset including visual-workflow-based code generation, allowing MM-Coder to synthesize textual and graphical information like human developers, distinct from prior work on narrow tasks. Furthermore, we introduce MMEval, a new benchmark for evaluating multimodal code generation, addressing existing text-only limitations. Our evaluations using MMEval highlight significant remaining challenges for models in precise visual information capture, instruction following, and advanced programming knowledge. Our work aims to revolutionize industrial programming by enabling LLMs to interpret and implement complex specifications conveyed through both text and visual designs.</li>
</ul>

<h3>Title: Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series</h3>
<ul>
<li><strong>Authors: </strong>Azimov Sherkhon, Susana Lopez-Moreno, Eric Dolores-Cuenca, Sieun Lee, Sangil Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08738">https://arxiv.org/abs/2507.08738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08738">https://arxiv.org/pdf/2507.08738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08738]] Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series(https://arxiv.org/abs/2507.08738)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63 model and El Nino-Southern Oscillation. However, their reliance on fixed nonlinearities - polynomial expansions in NVAR or random feature maps in RC - limits their adaptability to high noise or real-world data. These methods also scale poorly in high-dimensional settings due to costly matrix inversion during readout computation. We propose an adaptive NVAR model that combines delay-embedded linear inputs with features generated by a shallow, learnable multi-layer perceptron (MLP). The MLP and linear readout are jointly trained using gradient-based optimization, enabling the model to learn data-driven nonlinearities while preserving a simple readout structure. Unlike standard NVAR, our approach avoids the need for an exhaustive and sensitive grid search over ridge and delay parameters. Instead, tuning is restricted to neural network hyperparameters, improving scalability. Initial experiments on chaotic systems tested under noise-free and synthetically noisy conditions showed that the adaptive model outperformed the standard NVAR in predictive accuracy and showed robust forecasting under noisy conditions with a lower observation frequency.</li>
</ul>

<h3>Title: HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer</h3>
<ul>
<li><strong>Authors: </strong>Tianlong Ai, Tianzhu Liu, Haochen Jiang, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08741">https://arxiv.org/abs/2507.08741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08741">https://arxiv.org/pdf/2507.08741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08741]] HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer(https://arxiv.org/abs/2507.08741)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Hierarchical land cover and land use (LCLU) classification aims to assign pixel-wise labels with multiple levels of semantic granularity to remote sensing (RS) imagery. However, existing deep learning-based methods face two major challenges: 1) They predominantly adopt a flat classification paradigm, which limits their ability to generate end-to-end multi-granularity hierarchical predictions aligned with tree-structured hierarchies used in practice. 2) Most cross-domain studies focus on performance degradation caused by sensor or scene variations, with limited attention to transferring LCLU models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop classification). These limitations hinder the flexibility and generalization of LCLU models in practical applications. To address these challenges, we propose HieraRS, a novel hierarchical interpretation paradigm that enables multi-granularity predictions and supports the efficient transfer of LCLU models to cross-domain tasks with heterogeneous tree-structured hierarchies. We introduce the Bidirectional Hierarchical Consistency Constraint Mechanism (BHCCM), which can be seamlessly integrated into mainstream flat classification models to generate hierarchical predictions, while improving both semantic consistency and classification accuracy. Furthermore, we present TransLU, a dual-branch cross-domain transfer framework comprising two key components: Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment (CDSA). TransLU supports dynamic category expansion and facilitates the effective adaptation of LCLU models to heterogeneous hierarchies. In addition, we construct MM-5B, a large-scale multi-modal hierarchical land use dataset featuring pixel-wise annotations. The code and MM-5B dataset will be released at: this https URL.</li>
</ul>

<h3>Title: Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection</h3>
<ul>
<li><strong>Authors: </strong>Rei Tamaru, Pei Li, Bin Ran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08743">https://arxiv.org/abs/2507.08743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08743">https://arxiv.org/pdf/2507.08743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08743]] Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection(https://arxiv.org/abs/2507.08743)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Digital Twins (DT) have the potential to transform traffic management and operations by creating dynamic, virtual representations of transportation systems that sense conditions, analyze operations, and support decision-making. A key component for DT of the transportation system is dynamic roadway geometry sensing. However, existing approaches often rely on static maps or costly sensors, limiting scalability and adaptability. Additionally, large-scale DTs that collect and analyze data from multiple sources face challenges in privacy, communication, and computational efficiency. To address these challenges, we introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated Twin), a unified framework that combines real-time lane detection, DT synchronization, and federated meta-learning. At the core of Geo-ORBIT is GeoLane, a lightweight lane detection model that learns lane geometries from vehicle trajectory data using roadside cameras. We extend this model through Meta-GeoLane, which learns to personalize detection parameters for local entities, and FedMeta-GeoLane, a federated learning strategy that ensures scalable and privacy-preserving adaptation across roadside deployments. Our system is integrated with CARLA and SUMO to create a high-fidelity DT that renders highway scenarios and captures traffic flows in real-time. Extensive experiments across diverse urban scenes show that FedMeta-GeoLane consistently outperforms baseline and meta-learning approaches, achieving lower geometric error and stronger generalization to unseen locations while drastically reducing communication overhead. This work lays the foundation for flexible, context-aware infrastructure modeling in DTs. The framework is publicly available at this https URL.</li>
</ul>

<h3>Title: ML-Based Automata Simplification for Symbolic Accelerators</h3>
<ul>
<li><strong>Authors: </strong>Tiffany Yu, Rye Stahle-Smith, Darssan Eswaramoorthi, Rasha Karakchi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08751">https://arxiv.org/abs/2507.08751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08751">https://arxiv.org/pdf/2507.08751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08751]] ML-Based Automata Simplification for Symbolic Accelerators(https://arxiv.org/abs/2507.08751)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Symbolic accelerators are increasingly used for symbolic data processing in domains such as genomics, NLP, and cybersecurity. However, these accelerators face scalability issues due to excessive memory use and routing complexity, especially when targeting a large set. We present AutoSlim, a machine learning-based graph simplification framework designed to reduce the complexity of symbolic accelerators built on Non-deterministic Finite Automata (NFA) deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest classification to prune low-impact transitions based on edge scores and structural features, significantly reducing automata graph density while preserving semantic correctness. Unlike prior tools, AutoSlim targets automated score-aware simplification with weighted transitions, enabling efficient ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in NAPOLY+ and conducted performance measurements including latency, throughput, and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs and over 30 percent pruning in transitions, while scaling to graphs an order of magnitude larger than existing benchmarks. Our results also demonstrate how hardware interconnection (fanout) heavily influences hardware cost and that AutoSlim's pruning mitigates resource blowup.</li>
</ul>

<h3>Title: Compress Any Segment Anything Model (SAM)</h3>
<ul>
<li><strong>Authors: </strong>Juntong Fan, Zhiwei Hao, Jianqiang Shen, Shang-Ling Jui, Yi Zhang, Jing-Xiao Liao, Feng-Lei Fan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08765">https://arxiv.org/abs/2507.08765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08765">https://arxiv.org/pdf/2507.08765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08765]] Compress Any Segment Anything Model (SAM)(https://arxiv.org/abs/2507.08765)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, segmentation</a></li>
<li><strong>Abstract: </strong>Due to the excellent performance in yielding high-quality, zero-shot segmentation, Segment Anything Model (SAM) and its variants have been widely applied in diverse scenarios such as healthcare and intelligent manufacturing. Therefore, effectively compressing SAMs has become an increasingly pressing practical need. In this study, we propose Birkhoff, a novel data-free compression algorithm for SAM and its variants. Unlike quantization, pruning, distillation, and other compression methods, Birkhoff embodies versatility across model types, agility in deployment, faithfulness to the original model, and compactness in model size. Specifically, Birkhoff introduces a novel compression algorithm: Hyper-Compression, whose core principle is to find a dense trajectory to turn a high-dimensional parameter vector into a low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer operator, HyperLinear, to fuse decompression and matrix multiplication to significantly accelerate inference of the compressed SAMs. Extensive experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff performs consistently and competitively in compression time, compression ratio, post-compression performance, and inference speed. For example, Birkhoff can achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance drop without using any fine-tuning data. Moreover, the compression is finished within 60 seconds for all models.</li>
</ul>

<h3>Title: A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Farooq</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08766">https://arxiv.org/abs/2507.08766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08766">https://arxiv.org/pdf/2507.08766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08766]] A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification(https://arxiv.org/abs/2507.08766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This study presents a hybrid model for classifying handwritten digits in the MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well Hopfield network. The approach employs a CNN to extract high-dimensional features from input images, which are then clustered into class-specific prototypes using k-means clustering. These prototypes serve as attractors in a multi-well energy landscape, where a Hopfield network performs classification by minimizing an energy function that balances feature similarity and class this http URL model's design enables robust handling of intraclass variability, such as diverse handwriting styles, while providing an interpretable framework through its energy-based decision process. Through systematic optimization of the CNN architecture and the number of wells, the model achieves a high test accuracy of 99.2% on 10,000 MNIST images, demonstrating its effectiveness for image classification tasks. The findings highlight the critical role of deep feature extraction and sufficient prototype coverage in achieving high performance, with potential for broader applications in pattern recognition.</li>
</ul>

<h3>Title: BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08771">https://arxiv.org/abs/2507.08771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08771">https://arxiv.org/pdf/2507.08771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08771]] BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity(https://arxiv.org/abs/2507.08771)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (this https URL).</li>
</ul>

<h3>Title: From One to More: Contextual Part Latents for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08772">https://arxiv.org/abs/2507.08772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08772">https://arxiv.org/pdf/2507.08772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08772]] From One to More: Contextual Part Latents for 3D Generation(https://arxiv.org/abs/2507.08772)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.</li>
</ul>

<h3>Title: One Token to Fool LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Yulai Zhao, Haolin Liu, Dian Yu, S.Y. Kung, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08794">https://arxiv.org/abs/2507.08794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08794">https://arxiv.org/pdf/2507.08794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08794]] One Token to Fool LLM-as-a-Judge(https://arxiv.org/abs/2507.08794)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning openers like "Thought process:" and "Let's solve this problem step by step." can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at this https URL and this https URL.</li>
</ul>

<h3>Title: KV Cache Steering for Inducing Reasoning in Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, Cees G. M. Snoek, Yuki M. Asano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08799">https://arxiv.org/abs/2507.08799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08799">https://arxiv.org/pdf/2507.08799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08799]] KV Cache Steering for Inducing Reasoning in Small Language Models(https://arxiv.org/abs/2507.08799)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.</li>
</ul>

<h3>Title: NeuralOS: Towards Simulating Operating Systems via Neural Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Luke Rivard, Sun Sun, Hongyu Guo, Wenhu Chen, Yuntian Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08800">https://arxiv.org/abs/2507.08800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08800">https://arxiv.org/pdf/2507.08800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08800]] NeuralOS: Towards Simulating Operating Systems via Neural Generative Models(https://arxiv.org/abs/2507.08800)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems.</li>
</ul>

<h3>Title: Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective</h3>
<ul>
<li><strong>Authors: </strong>Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08801">https://arxiv.org/abs/2507.08801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08801">https://arxiv.org/pdf/2507.08801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08801]] Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective(https://arxiv.org/abs/2507.08801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at this https URL.</li>
</ul>

<h3>Title: The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?</h3>
<ul>
<li><strong>Authors: </strong>Denis Sutter, Julian Minder, Thomas Hofmann, Tiago Pimentel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.08802">https://arxiv.org/abs/2507.08802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.08802">https://arxiv.org/pdf/2507.08802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.08802]] The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?(https://arxiv.org/abs/2507.08802)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
