<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-10</h1>
<h3>Title: Different Paths, Same Destination: Designing New Physics-Inspired Dynamical Systems with Engineered Stability to Minimize the Ising Hamiltonian</h3>
<ul>
<li><strong>Authors: </strong>E.M.H.E.B. Ekanayake, N. Shukla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06280">https://arxiv.org/abs/2504.06280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06280">https://arxiv.org/pdf/2504.06280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06280]] Different Paths, Same Destination: Designing New Physics-Inspired Dynamical Systems with Engineered Stability to Minimize the Ising Hamiltonian(https://arxiv.org/abs/2504.06280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Oscillator Ising machines (OIMs) represent an exemplar case of using physics-inspired non-linear dynamical systems to solve computationally challenging combinatorial optimization problems (COPs). The computational performance of such systems is highly sensitive to the underlying dynamical properties, the topology of the input graph, and their relative compatibility. In this work, we explore the concept of designing different dynamical systems that minimize the same objective function but exhibit drastically different dynamical properties. Our goal is to leverage this diversification in dynamics to reduce the sensitivity of the computational performance to the underlying graph, and subsequently, enhance the overall effectiveness of such physics-based computational methods. To this end, we introduce a novel dynamical system, the Dynamical Ising Machine (DIM), which, like the OIM, minimizes the Ising Hamiltonian but offers significantly different dynamical properties. We analyze the characteristic properties of the DIM and compare them with those of the OIM. We also show that the relative performance of each model is dependent on the input graph. Our work illustrates that using multiple dynamical systems with varying properties to solve the same COP enables an effective method that is less sensitive to the input graph, while producing robust solutions.</li>
</ul>

<h3>Title: Reducing Formal Context Extraction: A Newly Proposed Framework from Big Corpora</h3>
<ul>
<li><strong>Authors: </strong>Bryar A. Hassan, Shko M. Qader, Alla A. Hassan, Joan Lu, Aram M. Ahmed, Jafar Majidpour, Tarik A. Rashid</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06285">https://arxiv.org/abs/2504.06285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06285">https://arxiv.org/pdf/2504.06285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06285]] Reducing Formal Context Extraction: A Newly Proposed Framework from Big Corpora(https://arxiv.org/abs/2504.06285)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Automating the extraction of concept hierarchies from free text is advantageous because manual generation is frequently labor- and resource-intensive. Free result, the whole procedure for concept hierarchy learning from free text entails several phases, including sentence-level text processing, sentence splitting, and tokenization. Lemmatization is after formal context analysis (FCA) to derive the pairings. Nevertheless, there could be a few uninteresting and incorrect pairings in the formal context. It may take a while to generate formal context; thus, size reduction formal context is necessary to weed out irrelevant and incorrect pairings to extract the concept lattice and hierarchies more quickly. This study aims to propose a framework for reducing formal context in extracting concept hierarchies from free text to reduce the ambiguity of the formal context. We achieve this by reducing the size of the formal context using a hybrid of a WordNet-based method and a frequency-based technique. Using 385 samples from the Wikipedia corpus and the suggested framework, tests are carried out to examine the reduced size of formal context, leading to concept lattice and concept hierarchy. With the help of concept lattice-invariants, the generated formal context lattice is compared to the normal one. In contrast to basic ones, the homomorphic between the resultant lattices retains up to 98% of the quality of the generating concept hierarchies, and the reduced concept lattice receives the structural connection of the standard one. Additionally, the new framework is compared to five baseline techniques to calculate the running time on random datasets with various densities. The findings demonstrate that, in various fill ratios, hybrid approaches of the proposed method outperform other indicated competing strategies in concept lattice performance.</li>
</ul>

<h3>Title: Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Liang, Hezhe Qiao, Wei Huang, Qizhou Wang, Mingsheng Shang, Lin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06292">https://arxiv.org/abs/2504.06292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06292">https://arxiv.org/pdf/2504.06292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06292]] Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction(https://arxiv.org/abs/2504.06292)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of vulnerable road users through accurate prediction of pedestrian crossing intention (PCI) plays a crucial role in the context of autonomous and assisted driving. Analyzing the set of observation video frames in ego-view has been widely used in most PCI prediction methods to forecast the cross intent. However, they struggle to capture the critical events related to pedestrian behaviour along the temporal dimension due to the high redundancy of the video frames, which results in the sub-optimal performance of PCI prediction. Our research addresses the challenge by introducing a novel approach called \underline{T}emporal-\underline{c}ontextual Event \underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module (TMM), which aims to manage the redundancy by clustering the observed video frames into multiple key temporal events. Then, the Contextual Attention Block (CAB) is employed to adaptively aggregate multiple event features along with visual and non-visual data. By synthesizing the temporal feature extraction and contextual attention on the key information across the critical events, TCL can learn expressive representation for the PCI prediction. Extensive experiments are carried out on three widely adopted datasets, including PIE, JAAD-beh, and JAAD-all. The results show that TCL substantially surpasses the state-of-the-art methods. Our code can be accessed at this https URL.</li>
</ul>

<h3>Title: Well2Flow: Reconstruction of reservoir states from sparse wells using score-based generative models</h3>
<ul>
<li><strong>Authors: </strong>Shiqin Zeng, Haoyun Li, Abhinav Prakash Gahlot, Felix J. Herrmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06305">https://arxiv.org/abs/2504.06305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06305">https://arxiv.org/pdf/2504.06305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06305]] Well2Flow: Reconstruction of reservoir states from sparse wells using score-based generative models(https://arxiv.org/abs/2504.06305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study investigates the use of score-based generative models for reservoir simulation, with a focus on reconstructing spatially varying permeability and saturation fields in saline aquifers, inferred from sparse observations at two well locations. By modeling the joint distribution of permeability and saturation derived from high-fidelity reservoir simulations, the proposed neural network is trained to learn the complex spatiotemporal dynamics governing multiphase fluid flow in porous media. During inference, the framework effectively reconstructs both permeability and saturation fields by conditioning on sparse vertical profiles extracted from well log data. This approach introduces a novel methodology for incorporating physical constraints and well log guidance into generative models, significantly enhancing the accuracy and physical plausibility of the reconstructed subsurface states. Furthermore, the framework demonstrates strong generalization capabilities across varying geological scenarios, highlighting its potential for practical deployment in data-scarce reservoir management tasks.</li>
</ul>

<h3>Title: Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights</h3>
<ul>
<li><strong>Authors: </strong>Tahniat Khan, Soroor Motie, Sedef Akinli Kocak, Shaina Raza</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06307">https://arxiv.org/abs/2504.06307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06307">https://arxiv.org/pdf/2504.06307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06307]] Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights(https://arxiv.org/abs/2504.06307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45\% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.</li>
</ul>

<h3>Title: Rethinking RoPE: A Mathematical Blueprint for N-dimensional Positional Encoding</h3>
<ul>
<li><strong>Authors: </strong>Haiping Liu, Hongpeng Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06308">https://arxiv.org/abs/2504.06308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06308">https://arxiv.org/pdf/2504.06308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06308]] Rethinking RoPE: A Mathematical Blueprint for N-dimensional Positional Encoding(https://arxiv.org/abs/2504.06308)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Rotary Position Embedding (RoPE) is widely adopted in Transformers due to its ability to encode relative positions with high efficiency and extrapolation capability. However, existing RoPE variants lack a unified theoretical foundation, especially in higher dimensions. In this paper, we propose a systematic mathematical framework for RoPE grounded in Lie group and Lie algebra theory. We identify two core properties of RoPE, named relativity and reversibility, and derive general constraints and constructions for valid RoPE in 1D, 2D, and N-dimensional (ND). We prove that RoPE must lie in the basis of a maximal abelian subalgebra (MASA) of the special orthogonal Lie algebra, and show that standard RoPE corresponds to the maximal toral subalgebra. Furthermore, we propose to model inter-dimensional interactions by learning an orthogonal basis transformation. Our framework unifies and explains existing RoPE designs, while enabling principled extensions to new modalities and tasks.</li>
</ul>

<h3>Title: DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Peizhi Niu, Yu-Hsiang Wang, Vishal Rana, Chetan Rupakheti, Abhishek Pandey, Olgica Milenkovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06312">https://arxiv.org/abs/2504.06312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06312">https://arxiv.org/pdf/2504.06312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06312]] DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation(https://arxiv.org/abs/2504.06312)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a new graph diffusion model for small molecule generation, \emph{DMol}, which outperforms the state-of-the-art DiGress model in terms of validity by roughly $1.5\%$ across all benchmarking datasets while reducing the number of diffusion steps by at least $10$-fold, and the running time to roughly one half. The performance improvements are a result of a careful change in the objective function and a ``graph noise" scheduling approach which, at each diffusion step, allows one to only change a subset of nodes of varying size in the molecule graph. Another relevant property of the method is that it can be easily combined with junction-tree-like graph representations that arise by compressing a collection of relevant ring structures into supernodes. Unlike classical junction-tree techniques that involve VAEs and require complicated reconstruction steps, compressed DMol directly performs graph diffusion on a graph that compresses only a carefully selected set of frequent carbon rings into supernodes, which results in straightforward sample generation. This compressed DMol method offers additional validity improvements over generic DMol of roughly $2\%$, increases the novelty of the method, and further improves the running time due to reductions in the graph size.</li>
</ul>

<h3>Title: Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</h3>
<ul>
<li><strong>Authors: </strong>Yanhao Dong, Yubo Miao, Weinan Li, Xiao Zheng, Chao Wang, Feng Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06319">https://arxiv.org/abs/2504.06319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06319">https://arxiv.org/pdf/2504.06319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06319]] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching(https://arxiv.org/abs/2504.06319)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.</li>
</ul>

<h3>Title: Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Michael Somma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06320">https://arxiv.org/abs/2504.06320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06320">https://arxiv.org/pdf/2504.06320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06320]] Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems(https://arxiv.org/abs/2504.06320)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Cyberattacks on critical infrastructure, particularly water distribution systems, have increased due to rapid digitalization and the integration of IoT devices and industrial control systems (ICS). These cyber-physical systems (CPS) introduce new vulnerabilities, requiring robust and automated intrusion detection systems (IDS) to mitigate potential threats. This study addresses key challenges in anomaly detection by leveraging time correlations in sensor data, integrating physical principles into machine learning models, and optimizing computational efficiency for edge applications. We build upon the concept of temporal differential consistency (TDC) loss to capture the dynamics of the system, ensuring meaningful relationships between dynamic states. Expanding on this foundation, we propose a hybrid autoencoder-based approach, referred to as hybrid TDC-AE, which extends TDC by incorporating both deterministic nodes and conventional statistical nodes. This hybrid structure enables the model to account for non-deterministic processes. Our approach achieves state-of-the-art classification performance while improving time to detect anomalies by 3%, outperforming the BATADAL challenge leader without requiring domain-specific knowledge, making it broadly applicable. Additionally, it maintains the computational efficiency of conventional autoencoders while reducing the number of fully connected layers, resulting in a more sustainable and efficient solution. The method demonstrates how leveraging physics-inspired consistency principles enhances anomaly detection and strengthens the resilience of cyber-physical systems.</li>
</ul>

<h3>Title: Mosaic: Composite Projection Pruning for Resource-efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bailey J. Eccles, Leon Wong, Blesson Varghese</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06323">https://arxiv.org/abs/2504.06323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06323">https://arxiv.org/pdf/2504.06323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06323]] Mosaic: Composite Projection Pruning for Resource-efficient LLMs(https://arxiv.org/abs/2504.06323)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models.</li>
</ul>

<h3>Title: Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images</h3>
<ul>
<li><strong>Authors: </strong>Hicham Talaoubrid, Anissa Mokraoui, Ismail Ben Ayed, Axel Prouvost, Sonimith Hang, Monit Korn, Rémi Harvey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06330">https://arxiv.org/abs/2504.06330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06330">https://arxiv.org/pdf/2504.06330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06330]] Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images(https://arxiv.org/abs/2504.06330)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper investigates the application of Low-Rank Adaptation (LoRA) to small models for cross-domain few-shot object detection in aerial images. Originally designed for large-scale models, LoRA helps mitigate overfitting, making it a promising approach for resource-constrained settings. We integrate LoRA into DiffusionDet, and evaluate its performance on the DOTA and DIOR datasets. Our results show that LoRA applied after an initial fine-tuning slightly improves performance in low-shot settings (e.g., 1-shot and 5-shot), while full fine-tuning remains more effective in higher-shot configurations. These findings highlight LoRA's potential for efficient adaptation in aerial object detection, encouraging further research into parameter-efficient fine-tuning strategies for few-shot learning. Our code is available here: this https URL.</li>
</ul>

<h3>Title: Query Understanding in LLM-based Conversational Information Seeking</h3>
<ul>
<li><strong>Authors: </strong>Yifei Yuan, Zahra Abbasiantaeb, Yang Deng, Mohammad Aliannejadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06356">https://arxiv.org/abs/2504.06356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06356">https://arxiv.org/pdf/2504.06356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06356]] Query Understanding in LLM-based Conversational Information Seeking(https://arxiv.org/abs/2504.06356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Query understanding in Conversational Information Seeking (CIS) involves accurately interpreting user intent through context-aware interactions. This includes resolving ambiguities, refining queries, and adapting to evolving information needs. Large Language Models (LLMs) enhance this process by interpreting nuanced language and adapting dynamically, improving the relevance and precision of search results in real-time. In this tutorial, we explore advanced techniques to enhance query understanding in LLM-based CIS systems. We delve into LLM-driven methods for developing robust evaluation metrics to assess query understanding quality in multi-turn interactions, strategies for building more interactive systems, and applications like proactive query management and query reformulation. We also discuss key challenges in integrating LLMs for query understanding in conversational search systems and outline future research directions. Our goal is to deepen the audience's understanding of LLM-based conversational query understanding and inspire discussions to drive ongoing advancements in this field.</li>
</ul>

<h3>Title: From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Golovkin, Nikolay Nemtsev, Vasyl Shandyba, Oleg Udin, Nikita Kasatkin, Pavel Kononov, Anton Afanasiev, Sergey Ulasen, Andrei Boiarov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06357">https://arxiv.org/abs/2504.06357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06357">https://arxiv.org/pdf/2504.06357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06357]] From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction(https://arxiv.org/abs/2504.06357)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Game State Reconstruction (GSR), a critical task in Sports Video Understanding, involves precise tracking and localization of all individuals on the football field-players, goalkeepers, referees, and others - in real-world coordinates. This capability enables coaches and analysts to derive actionable insights into player movements, team formations, and game dynamics, ultimately optimizing training strategies and enhancing competitive advantage. Achieving accurate GSR using a single-camera setup is highly challenging due to frequent camera movements, occlusions, and dynamic scene content. In this work, we present a robust end-to-end pipeline for tracking players across an entire match using a single-camera setup. Our solution integrates a fine-tuned YOLOv5m for object detection, a SegFormer-based camera parameter estimator, and a DeepSORT-based tracking framework enhanced with re-identification, orientation prediction, and jersey number recognition. By ensuring both spatial accuracy and temporal consistency, our method delivers state-of-the-art game state reconstruction, securing first place in the SoccerNet Game State Reconstruction Challenge 2024 and significantly outperforming competing methods.</li>
</ul>

<h3>Title: Towards Calibration Enhanced Network by Inverse Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Cheng, Zi Pong Lim, Sarthak Ketanbhai Modi, Yon Shin Teo, Yushi Cao, Shang-Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06358">https://arxiv.org/abs/2504.06358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06358">https://arxiv.org/pdf/2504.06358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06358]] Towards Calibration Enhanced Network by Inverse Adversarial Attack(https://arxiv.org/abs/2504.06358)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Test automation has become increasingly important as the complexity of both design and content in Human Machine Interface (HMI) software continues to grow. Current standard practice uses Optical Character Recognition (OCR) techniques to automatically extract textual information from HMI screens for validation. At present, one of the key challenges faced during the automation of HMI screen validation is the noise handling for the OCR models. In this paper, we propose to utilize adversarial training techniques to enhance OCR models in HMI testing scenarios. More specifically, we design a new adversarial attack objective for OCR models to discover the decision boundaries in the context of HMI testing. We then adopt adversarial training to optimize the decision boundaries towards a more robust and accurate OCR model. In addition, we also built an HMI screen dataset based on real-world requirements and applied multiple types of perturbation onto the clean HMI dataset to provide a more complete coverage for the potential scenarios. We conduct experiments to demonstrate how using adversarial training techniques yields more robust OCR models against various kinds of noises, while still maintaining high OCR model accuracy. Further experiments even demonstrate that the adversarial training models exhibit a certain degree of robustness against perturbations from other patterns.</li>
</ul>

<h3>Title: SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL</h3>
<ul>
<li><strong>Authors: </strong>Jacques Cloete, Nikolaus Vertovec, Alessandro Abate</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06386">https://arxiv.org/abs/2504.06386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06386">https://arxiv.org/pdf/2504.06386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06386]] SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL(https://arxiv.org/abs/2504.06386)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates.</li>
</ul>

<h3>Title: SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hritam Basak, Zhaozheng Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06389">https://arxiv.org/abs/2504.06389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06389">https://arxiv.org/pdf/2504.06389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06389]] SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation(https://arxiv.org/abs/2504.06389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Domain Adaptation (DA) and Semi-supervised Learning (SSL) converge in Semi-supervised Domain Adaptation (SSDA), where the objective is to transfer knowledge from a source domain to a target domain using a combination of limited labeled target samples and abundant unlabeled target data. Although intuitive, a simple amalgamation of DA and SSL is suboptimal in semantic segmentation due to two major reasons: (1) previous methods, while able to learn good segmentation boundaries, are prone to confuse classes with similar visual appearance due to limited supervision; and (2) skewed and imbalanced training data distribution preferring source representation learning whereas impeding from exploring limited information about tailed classes. Language guidance can serve as a pivotal semantic bridge, facilitating robust class discrimination and mitigating visual ambiguities by leveraging the rich semantic relationships encoded in pre-trained language models to enhance feature representations across domains. Therefore, we propose the first language-guided SSDA setting for semantic segmentation in this work. Specifically, we harness the semantic generalization capabilities inherent in vision-language models (VLMs) to establish a synergistic framework within the SSDA paradigm. To address the inherent class-imbalance challenges in long-tailed distributions, we introduce class-balanced segmentation loss formulations that effectively regularize the learning process. Through extensive experimentation across diverse domain adaptation scenarios, our approach demonstrates substantial performance improvements over contemporary state-of-the-art (SoTA) methodologies. Code is available: \href{this https URL}{GitHub}.</li>
</ul>

<h3>Title: PromptHMR: Promptable Human Mesh Recovery</h3>
<ul>
<li><strong>Authors: </strong>Yufu Wang, Yu Sun, Priyanka Patel, Kostas Daniilidis, Michael J. Black, Muhammed Kocabas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06397">https://arxiv.org/abs/2504.06397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06397">https://arxiv.org/pdf/2504.06397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06397]] PromptHMR: Promptable Human Mesh Recovery(https://arxiv.org/abs/2504.06397)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Human pose and shape (HPS) estimation presents challenges in diverse scenarios such as crowded scenes, person-person interactions, and single-view reconstruction. Existing approaches lack mechanisms to incorporate auxiliary "side information" that could enhance reconstruction accuracy in such challenging scenarios. Furthermore, the most accurate methods rely on cropped person detections and cannot exploit scene context while methods that process the whole image often fail to detect people and are less accurate than methods that use crops. While recent language-based methods explore HPS reasoning through large language or vision-language models, their metric accuracy is well below the state of the art. In contrast, we present PromptHMR, a transformer-based promptable method that reformulates HPS estimation through spatial and semantic prompts. Our method processes full images to maintain scene context and accepts multiple input modalities: spatial prompts like bounding boxes and masks, and semantic prompts like language descriptions or interaction labels. PromptHMR demonstrates robust performance across challenging scenarios: estimating people from bounding boxes as small as faces in crowded scenes, improving body shape estimation through language descriptions, modeling person-person interactions, and producing temporally coherent motions in videos. Experiments on benchmarks show that PromptHMR achieves state-of-the-art performance while offering flexible prompt-based control over the HPS estimation process.</li>
</ul>

<h3>Title: Sharpness-Aware Parameter Selection for Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Saber Malekmohammadi, Hong kyu Lee, Li Xiong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06398">https://arxiv.org/abs/2504.06398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06398">https://arxiv.org/pdf/2504.06398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06398]] Sharpness-Aware Parameter Selection for Machine Unlearning(https://arxiv.org/abs/2504.06398)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>It often happens that some sensitive personal information, such as credit card numbers or passwords, are mistakenly incorporated in the training of machine learning models and need to be removed afterwards. The removal of such information from a trained model is a complex task that needs to partially reverse the training process. There have been various machine unlearning techniques proposed in the literature to address this problem. Most of the proposed methods revolve around removing individual data samples from a trained model. Another less explored direction is when features/labels of a group of data samples need to be reverted. While the existing methods for these tasks do the unlearning task by updating the whole set of model parameters or only the last layer of the model, we show that there are a subset of model parameters that have the largest contribution in the unlearning target features. More precisely, the model parameters with the largest corresponding diagonal value in the Hessian matrix (computed at the learned model parameter) have the most contribution in the unlearning task. By selecting these parameters and updating them during the unlearning stage, we can have the most progress in unlearning. We provide theoretical justifications for the proposed strategy by connecting it to sharpness-aware minimization and robust unlearning. We empirically show the effectiveness of the proposed strategy in improving the efficacy of unlearning with a low computational cost.</li>
</ul>

<h3>Title: Unifying Autoregressive and Diffusion-Based Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Nima Fathi, Torsten Scholak, Pierre-André Noël</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06416">https://arxiv.org/abs/2504.06416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06416">https://arxiv.org/pdf/2504.06416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06416]] Unifying Autoregressive and Diffusion-Based Sequence Generation(https://arxiv.org/abs/2504.06416)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models. We introduce hyperschedules, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (e.g., GPT) and conventional diffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two hybrid token-wise noising processes that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a novel inference algorithm that leverages this new feature in a simplified context inspired from MDLM. To support efficient training and inference, we design attention masks compatible with KV-caching. Our methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.</li>
</ul>

<h3>Title: TRIDENT: Tri-modal Real-time Intrusion Detection Engine for New Targets</h3>
<ul>
<li><strong>Authors: </strong>Ildi Alla, Selma Yahia, Valeria Loscri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06417">https://arxiv.org/abs/2504.06417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06417">https://arxiv.org/pdf/2504.06417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06417]] TRIDENT: Tri-modal Real-time Intrusion Detection Engine for New Targets(https://arxiv.org/abs/2504.06417)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, extraction</a></li>
<li><strong>Abstract: </strong>The increasing availability of drones and their potential for malicious activities pose significant privacy and security risks, necessitating fast and reliable detection in real-world environments. However, existing drone detection systems often struggle in real-world settings due to environmental noise and sensor limitations. This paper introduces TRIDENT, a tri-modal drone detection framework that integrates synchronized audio, visual, and RF data to enhance robustness and reduce dependence on individual sensors. TRIDENT introduces two fusion strategies - Late Fusion and GMU Fusion - to improve multi-modal integration while maintaining efficiency. The framework incorporates domain-specific feature extraction techniques alongside a specialized data augmentation pipeline that simulates real-world sensor degradation to improve generalization capabilities. A diverse multi-sensor dataset is collected in urban and non-urban environments under varying lighting conditions, ensuring comprehensive evaluation. Experimental results show that TRIDENT achieves 98.8 percent accuracy in real-world recordings and 83.26 percent in a more complex setting (augmented data), outperforming unimodal and dual-modal baselines. Moreover, TRIDENT operates in real-time, detecting drones in just 6.09 ms while consuming only 75.27 mJ per detection, making it highly efficient for resource-constrained devices. The dataset and code have been released to ensure reproducibility (this https URL).</li>
</ul>

<h3>Title: Releasing Differentially Private Event Logs Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Frederik Wangelik, Majid Rafiei, Mahsa Pourbafrani, Wil M.P. van der Aalst</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06418">https://arxiv.org/abs/2504.06418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06418">https://arxiv.org/pdf/2504.06418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06418]] Releasing Differentially Private Event Logs Using Generative Models(https://arxiv.org/abs/2504.06418)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, the industry has been witnessing an extended usage of process mining and automated event data analysis. Consequently, there is a rising significance in addressing privacy apprehensions related to the inclusion of sensitive and private information within event data utilized by process mining algorithms. State-of-the-art research mainly focuses on providing quantifiable privacy guarantees, e.g., via differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques designed for the release of trace variants are still insufficient to meet all the demands of industry-scale utilization. Moreover, ensuring privacy guarantees in situations characterized by a high occurrence of infrequent trace variants remains a challenging endeavor. In this paper, we introduce two novel approaches for releasing differentially private trace variants based on trained generative models. With TraVaG, we leverage \textit{Generative Adversarial Networks} (GANs) to sample from a privatized implicit variant distribution. Our second method employs \textit{Denoising Diffusion Probabilistic Models} that reconstruct artificial trace variants from noise via trained Markov chains. Both methods offer industry-scale benefits and elevate the degree of privacy assurances, particularly in scenarios featuring a substantial prevalence of infrequent variants. Also, they overcome the shortcomings of conventional privacy preservation techniques, such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data demonstrate that our approaches surpass state-of-the-art techniques in terms of privacy guarantees and utility preservation.</li>
</ul>

<h3>Title: S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Zeng, Yinglong Xia, Zhuokai Zhao, Gilbert Jiang, Qiang Zhang, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Benyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06426">https://arxiv.org/abs/2504.06426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06426">https://arxiv.org/pdf/2504.06426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06426]] S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning(https://arxiv.org/abs/2504.06426)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) architectures enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Specifically, S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S'MoRE emulates the capacity of many experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S'MoRE improves "structural flexibility" of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S'MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation.</li>
</ul>

<h3>Title: D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition</h3>
<ul>
<li><strong>Authors: </strong>Rupayan Mallick, Sibo Dong, Nataniel Ruiz, Sarah Adel Bargal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06432">https://arxiv.org/abs/2504.06432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06432">https://arxiv.org/pdf/2504.06432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06432]] D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition(https://arxiv.org/abs/2504.06432)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Applications of diffusion models for visual tasks have been quite noteworthy. This paper targets making classification models more robust to occlusions for the task of object recognition by proposing a pipeline that utilizes a frozen diffusion model. Diffusion features have demonstrated success in image generation and image completion while understanding image context. Occlusion can be posed as an image completion problem by deeming the pixels of the occluder to be `missing.' We hypothesize that such features can help hallucinate object visual features behind occluding objects, and hence we propose using them to enable models to become more occlusion robust. We design experiments to include input-based augmentations as well as feature-based augmentations. Input-based augmentations involve finetuning on images where the occluder pixels are inpainted, and feature-based augmentations involve augmenting classification features with intermediate diffusion features. We demonstrate that our proposed use of diffusion-based features results in models that are more robust to partial object occlusions for both Transformers and ConvNets on ImageNet with simulated occlusions. We also propose a dataset that encompasses real-world occlusions and demonstrate that our method is more robust to partial object occlusions.</li>
</ul>

<h3>Title: Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini</h3>
<ul>
<li><strong>Authors: </strong>Dogus Yuksel, Mehmet Cem Catalbas, Bora Oc</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06436">https://arxiv.org/abs/2504.06436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06436">https://arxiv.org/pdf/2504.06436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06436]] Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini(https://arxiv.org/abs/2504.06436)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As leading examples of large language models, ChatGPT and Gemini claim to provide accurate and unbiased information, emphasizing their commitment to political neutrality and avoidance of personal bias. This research investigates the political tendency of large language models and the existence of differentiation according to the query language. For this purpose, ChatGPT and Gemini were subjected to a political axis test using 14 different languages. The findings of the study suggest that these large language models do exhibit political tendencies, with both models demonstrating liberal and leftist biases. A comparative analysis revealed that Gemini exhibited a more pronounced liberal and left-wing tendency compared to ChatGPT. The study also found that these political biases varied depending on the language used for inquiry. The study delves into the factors that constitute political tendencies and linguistic differentiation, exploring differences in the sources and scope of educational data, structural and grammatical features of languages, cultural and political contexts, and the model's response to linguistic features. From this standpoint, and an ethical perspective, it is proposed that artificial intelligence tools should refrain from asserting a lack of political tendencies and neutrality, instead striving for political neutrality and executing user queries by incorporating these tendencies.</li>
</ul>

<h3>Title: Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuehan Qin, Shawn Li, Yi Nian, Xinyan Velocity Yu, Yue Zhao, Xuezhe Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06438">https://arxiv.org/abs/2504.06438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06438">https://arxiv.org/pdf/2504.06438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06438]] Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning(https://arxiv.org/abs/2504.06438)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown substantial capacity for generating fluent, contextually appropriate responses. However, they can produce hallucinated outputs, especially when a user query includes one or more false premises-claims that contradict established facts. Such premises can mislead LLMs into offering fabricated or misleading details. Existing approaches include pretraining, fine-tuning, and inference-time techniques that often rely on access to logits or address hallucinations after they occur. These methods tend to be computationally expensive, require extensive training data, or lack proactive mechanisms to prevent hallucination before generation, limiting their efficiency in real-time applications. We propose a retrieval-based framework that identifies and addresses false premises before generation. Our method first transforms a user's query into a logical representation, then applies retrieval-augmented generation (RAG) to assess the validity of each premise using factual sources. Finally, we incorporate the verification results into the LLM's prompt to maintain factual consistency in the final output. Experiments show that this approach effectively reduces hallucinations, improves factual accuracy, and does not require access to model logits or large-scale fine-tuning.</li>
</ul>

<h3>Title: Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fay Elhassan, Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06446">https://arxiv.org/abs/2504.06446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06446">https://arxiv.org/pdf/2504.06446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06446]] Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights of Large Language Models(https://arxiv.org/abs/2504.06446)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>The indistinguishability of AI-generated content from human text raises challenges in transparency and accountability. While several methods exist to watermark models behind APIs, embedding watermark strategies directly into model weights that are later reflected in the outputs of the model is challenging. In this study we propose a strategy to finetune a pair of low-rank adapters of a model, one serving as the text-generating model, and the other as the detector, so that a subtle watermark is embedded into the text generated by the first model and simultaneously optimized for detectability by the second. In this way, the watermarking strategy is fully learned end-to-end. This process imposes an optimization challenge, as balancing watermark robustness, naturalness, and task performance requires trade-offs. We discuss strategies on how to optimize this min-max objective and present results showing the effect of this modification to instruction finetuning.</li>
</ul>

<h3>Title: Federated Neural Architecture Search with Model-Agnostic Meta Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Huang, Jiechao Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06457">https://arxiv.org/abs/2504.06457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06457">https://arxiv.org/pdf/2504.06457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06457]] Federated Neural Architecture Search with Model-Agnostic Meta Learning(https://arxiv.org/abs/2504.06457)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) often struggles with data heterogeneity due to the naturally uneven distribution of user data across devices. Federated Neural Architecture Search (NAS) enables collaborative search for optimal model architectures tailored to heterogeneous data to achieve higher accuracy. However, this process is time-consuming due to extensive search space and retraining. To overcome this, we introduce FedMetaNAS, a framework that integrates meta-learning with NAS within the FL context to expedite the architecture search by pruning the search space and eliminating the retraining stage. Our approach first utilizes the Gumbel-Softmax reparameterization to facilitate relaxation of the mixed operations in the search space. We then refine the local search process by incorporating Model-Agnostic Meta-Learning, where a task-specific learner adapts both weights and architecture parameters (alphas) for individual tasks, while a meta learner adjusts the overall model weights and alphas based on the gradient information from task learners. Following the meta-update, we propose soft pruning using the same trick on search space to gradually sparsify the architecture, ensuring that the performance of the chosen architecture remains robust after pruning which allows for immediate use of the model without retraining. Experimental evaluations demonstrate that FedMetaNAS significantly accelerates the search process by more than 50\% with higher accuracy compared to FedNAS.</li>
</ul>

<h3>Title: Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Sai Adith Senthil Kumar, Hao Yan, Saipavan Perepa, Murong Yue, Ziyu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06460">https://arxiv.org/abs/2504.06460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06460">https://arxiv.org/pdf/2504.06460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06460]] Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following(https://arxiv.org/abs/2504.06460)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub "counterfactual instruction following". We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research.</li>
</ul>

<h3>Title: Mind the Gap: Evaluating Vision Systems in Small Data Applications</h3>
<ul>
<li><strong>Authors: </strong>Samuel Stevens, S M Rayeed, Jenna Kline</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06486">https://arxiv.org/abs/2504.06486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06486">https://arxiv.org/pdf/2504.06486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06486]] Mind the Gap: Evaluating Vision Systems in Small Data Applications(https://arxiv.org/abs/2504.06486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The practical application of AI tools for specific computer vision tasks relies on the "small-data regime" of hundreds to thousands of labeled samples. This small-data regime is vital for applications requiring expensive expert annotations, such as ecological monitoring, medical diagnostics or industrial quality control. We find, however, that computer vision research has ignored the small data regime as evaluations increasingly focus on zero- and few-shot learning. We use the Natural World Tasks (NeWT) benchmark to compare multi-modal large language models (MLLMs) and vision-only methods across varying training set sizes. MLLMs exhibit early performance plateaus, while vision-only methods improve throughout the small-data regime, with performance gaps widening beyond 10 training examples. We provide the first comprehensive comparison between these approaches in small-data contexts and advocate for explicit small-data evaluations in AI research to better bridge theoretical advances with practical deployments.</li>
</ul>

<h3>Title: Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mingchen Li, Di Zhuang, Keyu Chen, Dumindu Samaraweera, Morris Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06492">https://arxiv.org/abs/2504.06492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06492">https://arxiv.org/pdf/2504.06492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06492]] Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction(https://arxiv.org/abs/2504.06492)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Link prediction in graph data utilizes various algorithms and machine learning/deep learning models to predict potential relationships between graph nodes. This technique has found widespread use in numerous real-world applications, including recommendation systems, community networks, and biological structures. However, recent research has highlighted the vulnerability of link prediction models to adversarial attacks, such as poisoning and evasion attacks. Addressing the vulnerability of these models is crucial to ensure stable and robust performance in link prediction applications. While many works have focused on enhancing the robustness of the Graph Convolution Network (GCN) model, the Variational Graph Auto-Encoder (VGAE), a sophisticated model for link prediction, has not been thoroughly investigated in the context of graph adversarial attacks. To bridge this gap, this article proposes an unweighted graph poisoning attack approach using meta-learning techniques to undermine VGAE's link prediction performance. We conducted comprehensive experiments on diverse datasets to evaluate the proposed method and its parameters, comparing it with existing approaches in similar settings. Our results demonstrate that our approach significantly diminishes link prediction performance and outperforms other state-of-the-art methods.</li>
</ul>

<h3>Title: STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints</h3>
<ul>
<li><strong>Authors: </strong>Xiaohang Yang, Qing Wang, Jiahao Yang, Gregory Slabaugh, Shanxin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06504">https://arxiv.org/abs/2504.06504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06504">https://arxiv.org/pdf/2504.06504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06504]] STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints(https://arxiv.org/abs/2504.06504)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Motion retargeting seeks to faithfully replicate the spatio-temporal motion characteristics of a source character onto a target character with a different body shape. Apart from motion semantics preservation, ensuring geometric plausibility and maintaining temporal consistency are also crucial for effective motion retargeting. However, many existing methods prioritize either geometric plausibility or temporal consistency. Neglecting geometric plausibility results in interpenetration while neglecting temporal consistency leads to motion jitter. In this paper, we propose a novel sequence-to-sequence model for seamless Spatial-Temporal aware motion Retargeting (STaR), with penetration and consistency constraints. STaR consists of two modules: (1) a spatial module that incorporates dense shape representation and a novel limb penetration constraint to ensure geometric plausibility while preserving motion semantics, and (2) a temporal module that utilizes a temporal transformer and a novel temporal consistency constraint to predict the entire motion sequence at once while enforcing multi-level trajectory smoothness. The seamless combination of the two modules helps us achieve a good balance between the semantic, geometric, and temporal targets. Extensive experiments on the Mixamo and ScanRet datasets demonstrate that our method produces plausible and coherent motions while significantly reducing interpenetration rates compared with other approaches.</li>
</ul>

<h3>Title: CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Khai Phan Tran, Xue Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06529">https://arxiv.org/abs/2504.06529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06529">https://arxiv.org/pdf/2504.06529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06529]] CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction(https://arxiv.org/abs/2504.06529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Document-level Relation Extraction (DocRE) involves identifying relations between entities across multiple sentences in a document. Evidence sentences, crucial for precise entity pair relationships identification, enhance focus on essential text segments, improving DocRE performance. However, existing evidence retrieval systems often overlook the collaborative nature among semantically similar entity pairs in the same document, hindering the effectiveness of the evidence retrieval task. To address this, we propose a novel evidence retrieval framework, namely CDER. CDER employs an attentional graph-based architecture to capture collaborative patterns and incorporates a dynamic sub-structure for additional robustness in evidence retrieval. Experimental results on the benchmark DocRE dataset show that CDER not only excels in the evidence retrieval task but also enhances overall performance of existing DocRE system.</li>
</ul>

<h3>Title: WaveHiTS: Wavelet-Enhanced Hierarchical Time Series Modeling for Wind Direction Nowcasting in Eastern Inner Mongolia</h3>
<ul>
<li><strong>Authors: </strong>Hailong Shu, Weiwei Song, Yue Wang, Jiping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06532">https://arxiv.org/abs/2504.06532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06532">https://arxiv.org/pdf/2504.06532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06532]] WaveHiTS: Wavelet-Enhanced Hierarchical Time Series Modeling for Wind Direction Nowcasting in Eastern Inner Mongolia(https://arxiv.org/abs/2504.06532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Wind direction forecasting plays a crucial role in optimizing wind energy production, but faces significant challenges due to the circular nature of directional data, error accumulation in multi-step forecasting, and complex meteorological interactions. This paper presents a novel model, WaveHiTS, which integrates wavelet transform with Neural Hierarchical Interpolation for Time Series to address these challenges. Our approach decomposes wind direction into U-V components, applies wavelet transform to capture multi-scale frequency patterns, and utilizes a hierarchical structure to model temporal dependencies at multiple scales, effectively mitigating error propagation. Experiments conducted on real-world meteorological data from Inner Mongolia, China demonstrate that WaveHiTS significantly outperforms deep learning models (RNN, LSTM, GRU), transformer-based approaches (TFT, Informer, iTransformer), and hybrid models (EMD-LSTM). The proposed model achieves RMSE values of approximately 19.2°-19.4° compared to 56°-64° for deep learning recurrent models, maintaining consistent accuracy across all forecasting steps up to 60 minutes ahead. Moreover, WaveHiTS demonstrates superior robustness with vector correlation coefficients (VCC) of 0.985-0.987 and hit rates of 88.5%-90.1%, substantially outperforming baseline models. Ablation studies confirm that each component-wavelet transform, hierarchical structure, and U-V decomposition-contributes meaningfully to overall performance. These improvements in wind direction nowcasting have significant implications for enhancing wind turbine yaw control efficiency and grid integration of wind energy.</li>
</ul>

<h3>Title: Flexible Graph Similarity Computation With A Proactive Optimization Strategy</h3>
<ul>
<li><strong>Authors: </strong>Zhouyang Liu, Ning Liu, Yixin Chen, Jiezhong He, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06533">https://arxiv.org/abs/2504.06533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06533">https://arxiv.org/pdf/2504.06533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06533]] Flexible Graph Similarity Computation With A Proactive Optimization Strategy(https://arxiv.org/abs/2504.06533)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph Edit Distance (GED) is an important similarity measure in graph retrieval, which quantifies the minimum cost of transforming one graph into another through edit operations, and offers flexibility by allowing customizable operation costs. Recent learning-based approaches approximate GEDs with the distances between representations in vector spaces. However, these methods often struggle with varying operation costs due to neglecting the impact of these costs on determining optimal graph mappings. Furthermore, they rely on isolated node distances as guidance, necessitating inefficient reactive refinements of mappings. To address these issues, we propose Graph Edit Network (GEN), a novel learning-based approach for flexible GED computation. By identifying the limitations of existing methods in capturing flexibility of GED, we introduce a principled yet simple solution that incorporates the operation costs before establishing mappings. To improve matching efficiency, we propose a strategy that proactively optimizes guidance from a graph perspective. This strategy initializes guidance as each node's alignment difficulty and captures the interdependencies between matches within and across graphs through a difficulty propagation mechanism, enabling more informed decisions. As a result, GEN selects optimal matches in a single step, minimizing the need for costly refinements. Results on real-world and synthetic datasets demonstrate the effectiveness, time efficiency, and adaptability of GEN, achieving up to 37.8\% error reduction and 72.7\% inference time reduction compared with state-of-the-art models, while performing robustly under varying cost settings and graph sizes.</li>
</ul>

<h3>Title: Lugha-Llama: Adapting Large Language Models for African Languages</h3>
<ul>
<li><strong>Authors: </strong>Happy Buzaaba, Alexander Wettig, David Ifeoluwa Adelani, Christiane Fellbaum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06536">https://arxiv.org/abs/2504.06536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06536">https://arxiv.org/pdf/2504.06536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06536]] Lugha-Llama: Adapting Large Language Models for African Languages(https://arxiv.org/abs/2504.06536)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive results in a wide range of natural language applications. However, they often struggle to recognize low-resource languages, in particular African languages, which are not well represented in large training corpora. In this paper, we consider how to adapt LLMs to low-resource African languages. We find that combining curated data from African languages with high-quality English educational texts results in a training mix that substantially improves the model's performance on these languages. On the challenging IrokoBench dataset, our models consistently achieve the best performance amongst similarly sized baselines, particularly on knowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the cross-lingual question answering benchmark AfriQA, our models outperform the base model by over 10%. To better understand the role of English data during training, we translate a subset of 200M tokens into Swahili language and perform an analysis which reveals that the content of these data is primarily responsible for the strong performance. We release our models and data to encourage future research on African languages.</li>
</ul>

<h3>Title: Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms</h3>
<ul>
<li><strong>Authors: </strong>Mutahar Ali, Arjun Arunasalam, Habiba Farrukh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06552">https://arxiv.org/abs/2504.06552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06552">https://arxiv.org/pdf/2504.06552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06552]] Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms(https://arxiv.org/abs/2504.06552)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The widespread adoption of conversational AI platforms has introduced new security and privacy risks. While these risks and their mitigation strategies have been extensively researched from a technical perspective, users' perceptions of these platforms' security and privacy remain largely unexplored. In this paper, we conduct a large-scale analysis of over 2.5M user posts from the r/ChatGPT Reddit community to understand users' security and privacy concerns and attitudes toward conversational AI platforms. Our qualitative analysis reveals that users are concerned about each stage of the data lifecycle (i.e., collection, usage, and retention). They seek mitigations for security vulnerabilities, compliance with privacy regulations, and greater transparency and control in data handling. We also find that users exhibit varied behaviors and preferences when interacting with these platforms. Some users proactively safeguard their data and adjust privacy settings, while others prioritize convenience over privacy risks, dismissing privacy concerns in favor of benefits, or feel resigned to inevitable data sharing. Through qualitative content and regression analysis, we discover that users' concerns evolve over time with the evolving AI landscape and are influenced by technological developments and major events. Based on our findings, we provide recommendations for users, platforms, enterprises, and policymakers to enhance transparency, improve data controls, and increase user trust and adoption.</li>
</ul>

<h3>Title: TabKAN: Advancing Tabular Data Analysis using Kolmograv-Arnold Network</h3>
<ul>
<li><strong>Authors: </strong>Ali Eslamian, Alireza Afzal Aghaei, Qiang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06559">https://arxiv.org/abs/2504.06559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06559">https://arxiv.org/pdf/2504.06559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06559]] TabKAN: Advancing Tabular Data Analysis using Kolmograv-Arnold Network(https://arxiv.org/abs/2504.06559)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Tabular data analysis presents unique challenges due to its heterogeneous feature types, missing values, and complex interactions. While traditional machine learning methods, such as gradient boosting, often outperform deep learning approaches, recent advancements in neural architectures offer promising alternatives. This paper introduces TabKAN, a novel framework that advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs leverage learnable activation functions on edges, enhancing both interpretability and training efficiency. Our contributions include: (1) the introduction of modular KAN-based architectures tailored for tabular data analysis, (2) the development of a transfer learning framework for KAN models, enabling effective knowledge transfer between domains, (3) the development of model-specific interpretability for tabular data learning, reducing reliance on post hoc and model-agnostic analysis, and (4) comprehensive evaluation of vanilla supervised learning across binary and multi-class classification tasks. Through extensive benchmarking on diverse public datasets, TabKAN demonstrates superior performance in supervised learning while significantly outperforming classical and Transformer-based models in transfer learning scenarios. Our findings highlight the advantage of KAN-based architectures in efficiently transferring knowledge across domains, bridging the gap between traditional machine learning and deep learning for structured data.</li>
</ul>

<h3>Title: NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables</h3>
<ul>
<li><strong>Authors: </strong>Lanrui Wang, Mingyu Zheng, Hongyin Tang, Zheng Lin, Yanan Cao, Jingang Wang, Xunliang Cai, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06560">https://arxiv.org/abs/2504.06560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06560">https://arxiv.org/pdf/2504.06560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06560]] NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables(https://arxiv.org/abs/2504.06560)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Processing structured tabular data, particularly lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks primarily focus on unstructured text, neglecting the challenges of long and complex structured tables. To address this gap, we introduce NeedleInATable (NIAT), a novel task that treats each table cell as a "needle" and requires the model to extract the target cell under different queries. Evaluation results of mainstream LLMs on this benchmark show they lack robust long-table comprehension, often relying on superficial correlations or shortcuts for complex table understanding tasks, revealing significant limitations in processing intricate tabular data. To this end, we propose a data synthesis method to enhance models' long-table comprehension capabilities. Experimental results show that our synthesized training data significantly enhances LLMs' performance on the NIAT task, outperforming both long-context LLMs and long-table agent methods. This work advances the evaluation of LLMs' genuine long-structured table comprehension capabilities and paves the way for progress in long-context and table understanding applications.</li>
</ul>

<h3>Title: FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion</h3>
<ul>
<li><strong>Authors: </strong>Longguang Zhong, Fanqi Wan, Ziyi Yang, Guosheng Liang, Tianyuan Shi, Xiaojun Quan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06562">https://arxiv.org/abs/2504.06562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06562">https://arxiv.org/pdf/2504.06562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06562]] FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion(https://arxiv.org/abs/2504.06562)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Heterogeneous model fusion enhances the performance of LLMs by integrating the knowledge and capabilities of multiple structurally diverse models. However, existing approaches often rely solely on selecting the best output for each prompt from source models, which underutilizes their full potential due to limited source knowledge and results in sparse optimization signals. To address this limitation, we propose FuseRL, a novel two-stage framework comprising FuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT establishes a robust initialization by integrating the strengths of heterogeneous source models through weighted supervised fine-tuning (SFT) on diverse outputs for each prompt. FusePO optimizes weighted preferences based on the outputs of multiple source models to enable superior alignment performance. Extensive experiments demonstrate the effectiveness of our framework across various preference alignment methods, including RLOO, DPO, and SimPO. Using Llama-3.1-8B-Instruct as the target model, our approach achieves state-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard benchmarks. Further analysis suggests that FuseSFT regularizes the training process to reduce overfitting, while FusePO introduces dense and diverse signals for preference optimization.</li>
</ul>

<h3>Title: Domain Generalization via Discrete Codebook Learning</h3>
<ul>
<li><strong>Authors: </strong>Shaocong Long, Qianyu Zhou, Xikun Jiang, Chenhao Ying, Lizhuang Ma, Yuan Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06572">https://arxiv.org/abs/2504.06572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06572">https://arxiv.org/pdf/2504.06572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06572]] Domain Generalization via Discrete Codebook Learning(https://arxiv.org/abs/2504.06572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Domain generalization (DG) strives to address distribution shifts across diverse environments to enhance model's generalizability. Current DG approaches are confined to acquiring robust representations with continuous features, specifically training at the pixel level. However, this DG paradigm may struggle to mitigate distribution gaps in dealing with a large space of continuous features, rendering it susceptible to pixel details that exhibit spurious correlations or noise. In this paper, we first theoretically demonstrate that the domain gaps in continuous representation learning can be reduced by the discretization process. Based on this inspiring finding, we introduce a novel learning paradigm for DG, termed Discrete Domain Generalization (DDG). DDG proposes to use a codebook to quantize the feature map into discrete codewords, aligning semantic-equivalent information in a shared discrete representation space that prioritizes semantic-level information over pixel-level intricacies. By learning at the semantic level, DDG diminishes the number of latent features, optimizing the utilization of the representation space and alleviating the risks associated with the wide-ranging space of continuous features. Extensive experiments across widely employed benchmarks in DG demonstrate DDG's superior performance compared to state-of-the-art approaches, underscoring its potential to reduce the distribution gaps and enhance the model's generalizability.</li>
</ul>

<h3>Title: Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Li An, Yujian Liu, Yepeng Liu, Yang Zhang, Yuheng Bu, Shiyu Chang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06575">https://arxiv.org/abs/2504.06575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06575">https://arxiv.org/pdf/2504.06575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06575]] Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning(https://arxiv.org/abs/2504.06575)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria: high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text-transforming it into hate speech-while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs. Our code is available at this https URL.</li>
</ul>

<h3>Title: Bypassing Safety Guardrails in LLMs Using Humor</h3>
<ul>
<li><strong>Authors: </strong>Pedro Cisneros-Velarde</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06577">https://arxiv.org/abs/2504.06577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06577">https://arxiv.org/pdf/2504.06577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06577]] Bypassing Safety Guardrails in LLMs Using Humor(https://arxiv.org/abs/2504.06577)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor.</li>
</ul>

<h3>Title: NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Rian Adam Rajagede, Muhammad Husni Santriaji, Muhammad Arya Fikriansyah, Hilal Hudan Nuha, Yanjie Fu, Yan Solihin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06591">https://arxiv.org/abs/2504.06591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06591">https://arxiv.org/pdf/2504.06591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06591]] NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural Networks(https://arxiv.org/abs/2504.06591)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Fault tolerance in Deep Neural Networks (DNNs) deployed on resource-constrained systems presents unique challenges for high-accuracy applications with strict timing requirements. Memory bit-flips can severely degrade DNN accuracy, while traditional protection approaches like Triple Modular Redundancy (TMR) often sacrifice accuracy to maintain reliability, creating a three-way dilemma between reliability, accuracy, and timeliness. We introduce NAPER, a novel protection approach that addresses this challenge through ensemble learning. Unlike conventional redundancy methods, NAPER employs heterogeneous model redundancy, where diverse models collectively achieve higher accuracy than any individual model. This is complemented by an efficient fault detection mechanism and a real-time scheduler that prioritizes meeting deadlines by intelligently scheduling recovery operations without interrupting inference. Our evaluations demonstrate NAPER's superiority: 40% faster inference in both normal and fault conditions, maintained accuracy 4.2% higher than TMR-based strategies, and guaranteed uninterrupted operation even during fault recovery. NAPER effectively balances the competing demands of accuracy, reliability, and timeliness in real-time DNN applications</li>
</ul>

<h3>Title: Automated Business Process Analysis: An LLM-Based Approach to Value Assessment</h3>
<ul>
<li><strong>Authors: </strong>William De Michele, Abel Armas Cervantes, Lea Frermann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06600">https://arxiv.org/abs/2504.06600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06600">https://arxiv.org/pdf/2504.06600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06600]] Automated Business Process Analysis: An LLM-Based Approach to Value Assessment(https://arxiv.org/abs/2504.06600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Business processes are fundamental to organizational operations, yet their optimization remains challenging due to the timeconsuming nature of manual process analysis. Our paper harnesses Large Language Models (LLMs) to automate value-added analysis, a qualitative process analysis technique that aims to identify steps in the process that do not deliver value. To date, this technique is predominantly manual, time-consuming, and subjective. Our method offers a more principled approach which operates in two phases: first, decomposing high-level activities into detailed steps to enable granular analysis, and second, performing a value-added analysis to classify each step according to Lean principles. This approach enables systematic identification of waste while maintaining the semantic understanding necessary for qualitative analysis. We develop our approach using 50 business process models, for which we collect and publish manual ground-truth labels. Our evaluation, comparing zero-shot baselines with more structured prompts reveals (a) a consistent benefit of structured prompting and (b) promising performance for both tasks. We discuss the potential for LLMs to augment human expertise in qualitative process analysis while reducing the time and subjectivity inherent in manual approaches.</li>
</ul>

<h3>Title: Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program</h3>
<ul>
<li><strong>Authors: </strong>Minghe Gao, Xuqi Liu, Zhongqi Yue, Yang Wu, Shuang Chen, Juncheng Li, Siliang Tang, Fei Wu, Tat-Seng Chua, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06606">https://arxiv.org/abs/2504.06606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06606">https://arxiv.org/pdf/2504.06606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06606]] Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program(https://arxiv.org/abs/2504.06606)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reward signal usage for Large Language Models (LLMs) are remarkable. However, significant challenges exist when transitioning reward signal to the multimodal domain, including labor-intensive annotations, over-reliance on one-step rewards, and inadequate evaluation. To address these issues, we propose SVIP, a novel approach to train a step-level multi-dimensional Chain-of-Thought~(CoT) reward model automatically. It generates code for solving visual tasks and transforms the analysis of code blocks into the evaluation of CoT step as training samples. Then, we train SVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The advantages of SVIP-Reward are evident throughout the entire process of MLLM. We also introduce a benchmark for CoT reward model training and testing. Experimental results demonstrate that SVIP-Reward improves MLLM performance across training and inference-time scaling, yielding better results on benchmarks while reducing hallucinations and enhancing reasoning ability.</li>
</ul>

<h3>Title: Visually Similar Pair Alignment for Robust Cross-Domain Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Onkar Krishna, Hiroki Ohashi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06607">https://arxiv.org/abs/2504.06607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06607">https://arxiv.org/pdf/2504.06607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06607]] Visually Similar Pair Alignment for Robust Cross-Domain Object Detection(https://arxiv.org/abs/2504.06607)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Domain gaps between training data (source) and real-world environments (target) often degrade the performance of object detection models. Most existing methods aim to bridge this gap by aligning features across source and target domains but often fail to account for visual differences, such as color or orientation, in alignment pairs. This limitation leads to less effective domain adaptation, as the model struggles to manage both domain-specific shifts (e.g., fog) and visual variations simultaneously. In this work, we demonstrate for the first time, using a custom-built dataset, that aligning visually similar pairs significantly improves domain adaptation. Based on this insight, we propose a novel memory-based system to enhance domain alignment. This system stores precomputed features of foreground objects and background areas from the source domain, which are periodically updated during training. By retrieving visually similar source features for alignment with target foreground and background features, the model effectively addresses domain-specific differences while reducing the impact of visual variations. Extensive experiments across diverse domain shift scenarios validate our method's effectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k, surpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively.</li>
</ul>

<h3>Title: Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization</h3>
<ul>
<li><strong>Authors: </strong>Sumeyye Meryem Tasyurek, Tugce Kiziltepe, Hacer Yalim Keles</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06610">https://arxiv.org/abs/2504.06610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06610">https://arxiv.org/pdf/2504.06610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06610]] Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization(https://arxiv.org/abs/2504.06610)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this work, we propose a simple gloss-free, transformer-based sign language production (SLP) framework that directly maps spoken-language text to sign pose sequences. We first train a pose autoencoder that encodes sign poses into a compact latent space using an articulator-based disentanglement strategy, where features corresponding to the face, right hand, left hand, and body are modeled separately to promote structured and interpretable representation learning. Next, a non-autoregressive transformer decoder is trained to predict these latent representations from sentence-level text embeddings. To guide this process, we apply channel-aware regularization by aligning predicted latent distributions with priors extracted from the ground-truth encodings using a KL-divergence loss. The contribution of each channel to the loss is weighted according to its associated articulator region, enabling the model to account for the relative importance of different articulators during training. Our approach does not rely on gloss supervision or pretrained models, and achieves state-of-the-art results on the PHOENIX14T dataset using only a modest training set.</li>
</ul>

<h3>Title: FACT: Multinomial Misalignment Classification for Point Cloud Registration</h3>
<ul>
<li><strong>Authors: </strong>Ludvig Dillén, Per-Erik Forssén, Johan Edstedt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06627">https://arxiv.org/abs/2504.06627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06627">https://arxiv.org/pdf/2504.06627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06627]] FACT: Multinomial Misalignment Classification for Point Cloud Registration(https://arxiv.org/abs/2504.06627)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present FACT, a method for predicting alignment quality (i.e., registration error) of registered lidar point cloud pairs. This is useful e.g. for quality assurance of large, automatically registered 3D models. FACT extracts local features from a registered pair and processes them with a point transformer-based network to predict a misalignment class. We generalize prior work that study binary alignment classification of registration errors, by recasting it as multinomial misalignment classification. To achieve this, we introduce a custom regression-by-classification loss function that combines the cross-entropy and Wasserstein losses, and demonstrate that it outperforms both direct regression and prior binary classification. FACT successfully classifies point-cloud pairs registered with both the classical ICP and GeoTransformer, while other choices, such as standard point-cloud-quality metrics and registration residuals are shown to be poor choices for predicting misalignment. On a synthetically perturbed point-cloud task introduced by the CorAl method, we show that FACT achieves substantially better performance than CorAl. Finally, we demonstrate how FACT can assist experts in correcting misaligned point-cloud maps. Our code is available at this https URL.</li>
</ul>

<h3>Title: Rethinking LayerNorm in Image Restoration Transformers</h3>
<ul>
<li><strong>Authors: </strong>MinKyu Lee, Sangeek Hyun, Woojin Jun, Hyunjun Kim, Jiwoo Chung, Jae-Pil Heo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06629">https://arxiv.org/abs/2504.06629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06629">https://arxiv.org/pdf/2504.06629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06629]] Rethinking LayerNorm in Image Restoration Transformers(https://arxiv.org/abs/2504.06629)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work investigates abnormal feature behaviors observed in image restoration (IR) Transformers. Specifically, we identify two critical issues: feature entropy becoming excessively small and feature magnitudes diverging up to a million-fold scale. We pinpoint the root cause to the per-token normalization aspect of conventional LayerNorm, which disrupts essential spatial correlations and internal feature statistics. To address this, we propose a simple normalization strategy tailored for IR Transformers. Our approach applies normalization across the entire spatio-channel dimension, effectively preserving spatial correlations. Additionally, we introduce an input-adaptive rescaling method that aligns feature statistics to the unique statistical requirements of each input. Experimental results verify that this combined strategy effectively resolves feature divergence, significantly enhancing both the stability and performance of IR Transformers across various IR tasks.</li>
</ul>

<h3>Title: PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering</h3>
<ul>
<li><strong>Authors: </strong>Yifan Gao, Zihang Lin, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, Hongtao Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06632">https://arxiv.org/abs/2504.06632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06632">https://arxiv.org/pdf/2504.06632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06632]] PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering(https://arxiv.org/abs/2504.06632)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Product posters, which integrate subject, scene, and text, are crucial promotional tools for attracting customers. Creating such posters using modern image generation methods is valuable, while the main challenge lies in accurately rendering text, especially for complex writing systems like Chinese, which contains over 10,000 individual characters. In this work, we identify the key to precise text rendering as constructing a character-discriminative visual feature as a control signal. Based on this insight, we propose a robust character-wise representation as control and we develop TextRenderNet, which achieves a high text rendering accuracy of over 90%. Another challenge in poster generation is maintaining the fidelity of user-specific products. We address this by introducing SceneGenNet, an inpainting-based model, and propose subject fidelity feedback learning to further enhance fidelity. Based on TextRenderNet and SceneGenNet, we present PosterMaker, an end-to-end generation framework. To optimize PosterMaker efficiently, we implement a two-stage training strategy that decouples text rendering and background generation learning. Experimental results show that PosterMaker outperforms existing baselines by a remarkable margin, which demonstrates its effectiveness.</li>
</ul>

<h3>Title: Crafting Query-Aware Selective Attention for Single Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Junyoung Kim, Youngrok Kim, Siyeol Jung, Donghyun Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06634">https://arxiv.org/abs/2504.06634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06634">https://arxiv.org/pdf/2504.06634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06634]] Crafting Query-Aware Selective Attention for Single Image Super-Resolution(https://arxiv.org/abs/2504.06634)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Single Image Super-Resolution (SISR) reconstructs high-resolution images from low-resolution inputs, enhancing image details. While Vision Transformer (ViT)-based models improve SISR by capturing long-range dependencies, they suffer from quadratic computational costs or employ selective attention mechanisms that do not explicitly focus on query-relevant regions. Despite these advancements, prior work has overlooked how selective attention mechanisms should be effectively designed for SISR. We propose SSCAN, which dynamically selects the most relevant key-value windows based on query similarity, ensuring focused feature extraction while maintaining efficiency. In contrast to prior approaches that apply attention globally or heuristically, our method introduces a query-aware window selection strategy that better aligns attention computation with important image regions. By incorporating fixed-sized windows, SSCAN reduces memory usage and enforces linear token-to-token complexity, making it scalable for large images. Our experiments demonstrate that SSCAN outperforms existing attention-based SISR methods, achieving up to 0.14 dB PSNR improvement on urban datasets, guaranteeing both computational efficiency and reconstruction quality in SISR.</li>
</ul>

<h3>Title: HGMamba: Enhancing 3D Human Pose Estimation with a HyperGCN-Mamba Network</h3>
<ul>
<li><strong>Authors: </strong>Hu Cui, Tessai Hayama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06638">https://arxiv.org/abs/2504.06638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06638">https://arxiv.org/pdf/2504.06638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06638]] HGMamba: Enhancing 3D Human Pose Estimation with a HyperGCN-Mamba Network(https://arxiv.org/abs/2504.06638)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D human pose lifting is a promising research area that leverages estimated and ground-truth 2D human pose data for training. While existing approaches primarily aim to enhance the performance of estimated 2D poses, they often struggle when applied to ground-truth 2D pose data. We observe that achieving accurate 3D pose reconstruction from ground-truth 2D poses requires precise modeling of local pose structures, alongside the ability to extract robust global spatio-temporal features. To address these challenges, we propose a novel Hyper-GCN and Shuffle Mamba (HGMamba) block, which processes input data through two parallel streams: Hyper-GCN and Shuffle-Mamba. The Hyper-GCN stream models the human body structure as hypergraphs with varying levels of granularity to effectively capture local joint dependencies. Meanwhile, the Shuffle Mamba stream leverages a state space model to perform spatio-temporal scanning across all joints, enabling the establishment of global dependencies. By adaptively fusing these two representations, HGMamba achieves strong global feature modeling while excelling at local structure modeling. We stack multiple HGMamba blocks to create three variants of our model, allowing users to select the most suitable configuration based on the desired speed-accuracy trade-off. Extensive evaluations on the Human3.6M and MPI-INF-3DHP benchmark datasets demonstrate the effectiveness of our approach. HGMamba-B achieves state-of-the-art results, with P1 errors of 38.65 mm and 14.33 mm on the respective datasets. Code and models are available: this https URL</li>
</ul>

<h3>Title: AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Tiange Huang, Yongjun Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06643">https://arxiv.org/abs/2504.06643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06643">https://arxiv.org/pdf/2504.06643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06643]] AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection(https://arxiv.org/abs/2504.06643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \textbf{A}uto\textbf{M}asked Attention for UMTS\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets.</li>
</ul>

<h3>Title: Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction</h3>
<ul>
<li><strong>Authors: </strong>Nan Peng, Xun Zhou, Mingming Wang, Guisong Chen, Songming Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06647">https://arxiv.org/abs/2504.06647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06647">https://arxiv.org/pdf/2504.06647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06647]] Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction(https://arxiv.org/abs/2504.06647)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across prior-free, map-absent, and map-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-free scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at this https URL.</li>
</ul>

<h3>Title: ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM Intrinsic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Wang, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06650">https://arxiv.org/abs/2504.06650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06650">https://arxiv.org/pdf/2504.06650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06650]] ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM Intrinsic Reasoning(https://arxiv.org/abs/2504.06650)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pre-trained large language models (LLMs) have been demonstrated to possess intrinsic reasoning capabilities that can emerge naturally when expanding the response space. However, the neural representation mechanisms underlying these intrinsic capabilities and approaches for their optimal utilization remain inadequately understood. In this work, we make the key discovery that a simple linear classifier can effectively detect intrinsic reasoning capabilities in LLMs' activation space, particularly within specific representation types and network layers. Based on this finding, we propose a classifier-guided search framework that strategically explore a tree-structured response space. In each node expansion, the classifier serves as a scoring and ranking mechanism that efficiently allocates computational resources by identifying and prioritizing more thoughtful reasoning directions for continuation. After completing the tree expansion, we collect answers from all branches to form a candidate answer pool. We propose a branch-aggregation selection method that marginalizes over all supporting branches by aggregating their thoughtfulness scores, thereby identifying the optimal answer from the pool. Experimental results show that our framework's comprehensive exploration not only covers valid reasoning chains but also effectively identifies them, achieving significant improvements across multiple arithmetic reasoning benchmarks.</li>
</ul>

<h3>Title: A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty</h3>
<ul>
<li><strong>Authors: </strong>Xiaohua Feng, Yuyuan Li, Chengye Wang, Junlin Liu, Li Zhang, Chaochao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06658">https://arxiv.org/abs/2504.06658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06658">https://arxiv.org/pdf/2504.06658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06658]] A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty(https://arxiv.org/abs/2504.06658)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Driven by privacy protection laws and regulations, unlearning in Large Language Models (LLMs) is gaining increasing attention. However, current research often neglects the interpretability of the unlearning process, particularly concerning sample-level unlearning difficulty. Existing studies typically assume a uniform unlearning difficulty across samples. This simplification risks attributing the performance of unlearning algorithms to sample selection rather than the algorithm's design, potentially steering the development of LLM unlearning in the wrong direction. Thus, we investigate the relationship between LLM unlearning and sample characteristics, with a focus on unlearning difficulty. Drawing inspiration from neuroscience, we propose a Memory Removal Difficulty ($\mathrm{MRD}$) metric to quantify sample-level unlearning difficulty. Using $\mathrm{MRD}$, we analyze the characteristics of hard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an $\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning algorithms, which prioritizes easily forgettable samples, thereby improving unlearning efficiency and effectiveness. We validate the proposed metric and method using public benchmarks and datasets, with results confirming its effectiveness.</li>
</ul>

<h3>Title: Bridging the Gap Between Preference Alignment and Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Xiaohua Feng, Yuyuan Li, Huwei Ji, Jiaming Zhang, Li Zhang, Tianyu Du, Chaochao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06659">https://arxiv.org/abs/2504.06659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06659">https://arxiv.org/pdf/2504.06659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06659]] Bridging the Gap Between Preference Alignment and Machine Unlearning(https://arxiv.org/abs/2504.06659)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite advances in Preference Alignment (PA) for Large Language Models (LLMs), mainstream methods like Reinforcement Learning with Human Feedback (RLHF) face notable challenges. These approaches require high-quality datasets of positive preference examples, which are costly to obtain and computationally intensive due to training instability, limiting their use in low-resource scenarios. LLM unlearning technique presents a promising alternative, by directly removing the influence of negative examples. However, current research has primarily focused on empirical validation, lacking systematic quantitative analysis. To bridge this gap, we propose a framework to explore the relationship between PA and LLM unlearning. Specifically, we introduce a bi-level optimization-based method to quantify the impact of unlearning specific negative examples on PA performance. Our analysis reveals that not all negative examples contribute equally to alignment improvement when unlearned, and the effect varies significantly across examples. Building on this insight, we pose a crucial question: how can we optimally select and weight negative examples for unlearning to maximize PA performance? To answer this, we propose a framework called Unlearning to Align (U2A), which leverages bi-level optimization to efficiently select and unlearn examples for optimal PA performance. We validate the proposed method through extensive experiments, with results confirming its effectiveness.</li>
</ul>

<h3>Title: Robust and Noise-resilient Long-Term Prediction of Spatiotemporal Data Using Variational Mode Graph Neural Networks with 3D Attention</h3>
<ul>
<li><strong>Authors: </strong>Osama Ahmad, Zubair Khalid</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06660">https://arxiv.org/abs/2504.06660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06660">https://arxiv.org/pdf/2504.06660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06660]] Robust and Noise-resilient Long-Term Prediction of Spatiotemporal Data Using Variational Mode Graph Neural Networks with 3D Attention(https://arxiv.org/abs/2504.06660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper focuses on improving the robustness of spatiotemporal long-term prediction using a variational mode graph convolutional network (VMGCN) by introducing 3D channel attention. The deep learning network for this task relies on historical data inputs, yet real-time data can be corrupted by sensor noise, altering its distribution. We model this noise as independent and identically distributed (i.i.d.) Gaussian noise and incorporate it into the LargeST traffic volume dataset, resulting in data with both inherent and additive noise components. Our approach involves decomposing the corrupted signal into modes using variational mode decomposition, followed by feeding the data into a learning pipeline for prediction. We integrate a 3D attention mechanism encompassing spatial, temporal, and channel attention. The spatial and temporal attention modules learn their respective correlations, while the channel attention mechanism is used to suppress noise and highlight the significant modes in the spatiotemporal signals. Additionally, a learnable soft thresholding method is implemented to exclude unimportant modes from the feature vector, and a feature reduction method based on the signal-to-noise ratio (SNR) is applied. We compare the performance of our approach against baseline models, demonstrating that our method achieves superior long-term prediction accuracy, robustness to noise, and improved performance with mode truncation compared to the baseline models. The code of the paper is available at this https URL.</li>
</ul>

<h3>Title: SEE: Continual Fine-tuning with Sequential Ensemble of Experts</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Wang, Yafu Li, Xiaoye Qu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06664">https://arxiv.org/abs/2504.06664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06664">https://arxiv.org/pdf/2504.06664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06664]] SEE: Continual Fine-tuning with Sequential Ensemble of Experts(https://arxiv.org/abs/2504.06664)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continual fine-tuning of large language models (LLMs) suffers from catastrophic forgetting. Rehearsal-based methods mitigate this problem by retaining a small set of old data. Nevertheless, they still suffer inevitable performance loss. Although training separate experts for each task can help prevent forgetting, effectively assembling them remains a challenge. Some approaches use routers to assign tasks to experts, but in continual learning, they often require retraining for optimal performance. To address these challenges, we introduce the Sequential Ensemble of Experts (SEE) framework. SEE removes the need for an additional router, allowing each expert to independently decide whether a query should be handled. The framework employs distributed routing, and during continual fine-tuning, SEE only requires the training of new experts for incoming tasks rather than retraining the entire system. Experiments reveal that the SEE outperforms prior approaches, including multi-task learning, in continual fine-tuning. It also demonstrates remarkable generalization ability, as the expert can effectively identify out-of-distribution queries, which can then be directed to a more generalized model for resolution. This work highlights the promising potential of integrating routing and response mechanisms within each expert, paving the way for the future of distributed model ensembling.</li>
</ul>

<h3>Title: Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception</h3>
<ul>
<li><strong>Authors: </strong>Ruotian Peng, Haiying He, Yake Wei, Yandong Wen, Di Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06666">https://arxiv.org/abs/2504.06666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06666">https://arxiv.org/pdf/2504.06666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06666]] Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception(https://arxiv.org/abs/2504.06666)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality image captions play a crucial role in improving the performance of cross-modal applications such as text-to-image generation, text-to-video generation, and text-image retrieval. To generate long-form, high-quality captions, many recent studies have employed multimodal large language models (MLLMs). However, current MLLMs often produce captions that lack fine-grained details or suffer from hallucinations, a challenge that persists in both open-source and closed-source models. Inspired by Feature-Integration theory, which suggests that attention must focus on specific regions to integrate visual information effectively, we propose a \textbf{divide-then-aggregate} strategy. Our method first divides the image into semantic and spatial patches to extract fine-grained details, enhancing the model's local perception of the image. These local details are then hierarchically aggregated to generate a comprehensive global description. To address hallucinations and inconsistencies in the generated captions, we apply a semantic-level filtering process during hierarchical aggregation. This training-free pipeline can be applied to both open-source models (LLaVA-1.5, LLaVA-1.6, Mini-Gemini) and closed-source models (Claude-3.5-Sonnet, GPT-4o, GLM-4V-Plus). Extensive experiments demonstrate that our method generates more detailed, reliable captions, advancing multimodal description generation without requiring model retraining. The source code are available at this https URL</li>
</ul>

<h3>Title: NLP Security and Ethics, in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Heather Lent, Erick Galinkin, Yiyi Chen, Jens Myrup Pedersen, Leon Derczynski, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06669">https://arxiv.org/abs/2504.06669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06669">https://arxiv.org/pdf/2504.06669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06669]] NLP Security and Ethics, in the Wild(https://arxiv.org/abs/2504.06669)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>As NLP models are used by a growing number of end-users, an area of increasing importance is NLP Security (NLPSec): assessing the vulnerability of models to malicious attacks and developing comprehensive countermeasures against them. While work at the intersection of NLP and cybersecurity has the potential to create safer NLP for all, accidental oversights can result in tangible harm (e.g., breaches of privacy or proliferation of malicious models). In this emerging field, however, the research ethics of NLP have not yet faced many of the long-standing conundrums pertinent to cybersecurity, until now. We thus examine contemporary works across NLPSec, and explore their engagement with cybersecurity's ethical norms. We identify trends across the literature, ultimately finding alarming gaps on topics like harm minimization and responsible disclosure. To alleviate these concerns, we provide concrete recommendations to help NLP researchers navigate this space more ethically, bridging the gap between traditional cybersecurity and NLP ethics, which we frame as ``white hat NLP''. The goal of this work is to help cultivate an intentional culture of ethical research for those working in NLP Security.</li>
</ul>

<h3>Title: RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism</h3>
<ul>
<li><strong>Authors: </strong>Elia Peruzzo, Dejia Xu, Xingqian Xu, Humphrey Shi, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06672">https://arxiv.org/abs/2504.06672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06672">https://arxiv.org/pdf/2504.06672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06672]] RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism(https://arxiv.org/abs/2504.06672)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation is experiencing rapid growth, driven by advances in diffusion models and the development of better and larger datasets. However, producing high-quality videos remains challenging due to the high-dimensional data and the complexity of the task. Recent efforts have primarily focused on enhancing visual quality and addressing temporal inconsistencies, such as flickering. Despite progress in these areas, the generated videos often fall short in terms of motion complexity and physical plausibility, with many outputs either appearing static or exhibiting unrealistic motion. In this work, we propose a framework to improve the realism of motion in generated videos, exploring a complementary direction to much of the existing literature. Specifically, we advocate for the incorporation of a retrieval mechanism during the generation phase. The retrieved videos act as grounding signals, providing the model with demonstrations of how the objects move. Our pipeline is designed to apply to any text-to-video diffusion model, conditioning a pretrained model on the retrieved samples with minimal fine-tuning. We demonstrate the superiority of our approach through established metrics, recently proposed benchmarks, and qualitative results, and we highlight additional applications of the framework.</li>
</ul>

<h3>Title: Probability Density Geodesics in Image Diffusion Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Qingtao Yu, Jaskirat Singh, Zhaoyuan Yang, Peter Henry Tu, Jing Zhang, Hongdong Li, Richard Hartley, Dylan Campbell</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06675">https://arxiv.org/abs/2504.06675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06675">https://arxiv.org/pdf/2504.06675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06675]] Probability Density Geodesics in Image Diffusion Latent Space(https://arxiv.org/abs/2504.06675)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.</li>
</ul>

<h3>Title: Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Llewyn Salt, Marcus Gallagher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06683">https://arxiv.org/abs/2504.06683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06683">https://arxiv.org/pdf/2504.06683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06683]] Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning(https://arxiv.org/abs/2504.06683)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Hyperparameter optimisation (HPO) is crucial for achieving strong performance in reinforcement learning (RL), as RL algorithms are inherently sensitive to hyperparameter settings. Probabilistic Curriculum Learning (PCL) is a curriculum learning strategy designed to improve RL performance by structuring the agent's learning process, yet effective hyperparameter tuning remains challenging and computationally demanding. In this paper, we provide an empirical analysis of hyperparameter interactions and their effects on the performance of a PCL algorithm within standard RL tasks, including point-maze navigation and DC motor control. Using the AlgOS framework integrated with Optuna's Tree-Structured Parzen Estimator (TPE), we present strategies to refine hyperparameter search spaces, enhancing optimisation efficiency. Additionally, we introduce a novel SHAP-based interpretability approach tailored specifically for analysing hyperparameter impacts, offering clear insights into how individual hyperparameters and their interactions influence RL performance. Our work contributes practical guidelines and interpretability tools that significantly improve the effectiveness and computational feasibility of hyperparameter optimisation in reinforcement learning.</li>
</ul>

<h3>Title: CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yoshihiro Yamada</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06704">https://arxiv.org/abs/2504.06704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06704">https://arxiv.org/pdf/2504.06704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06704]] CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers(https://arxiv.org/abs/2504.06704)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have driven remarkable breakthroughs in natural language processing and computer vision, yet their standard attention mechanism still imposes O(N^2) complexity, hindering scalability to longer sequences. We introduce Circular-convolutional ATtention (CAT), a Fourier-based approach that efficiently applies circular convolutions to reduce complexity without sacrificing representational power. CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and introduces no heavier operations, resulting in consistent accuracy improvements and about a 10% speedup in naive PyTorch implementations on large-scale benchmarks such as ImageNet-1k and WikiText-103. Grounded in an engineering-isomorphism framework, CAT's design not only offers practical efficiency and ease of implementation but also provides insights to guide the development of next-generation, high-performance Transformer architectures. Finally, our ablation studies highlight the key conditions underlying CAT's success, shedding light on broader principles for scalable attention mechanisms.</li>
</ul>

<h3>Title: Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap</h3>
<ul>
<li><strong>Authors: </strong>Pascal Schöttle, Matthias Janetschek, Florian Merkle, Martin Nocker, Christoph Egger</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06712">https://arxiv.org/abs/2504.06712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06712">https://arxiv.org/pdf/2504.06712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06712]] Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap(https://arxiv.org/abs/2504.06712)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) has rapidly expanded across various sectors, with consumer IoT devices - such as smart thermostats and security cameras - experiencing growth. Although these devices improve efficiency and promise additional comfort, they also introduce new security challenges. Common and easy-to-explore vulnerabilities make IoT devices prime targets for malicious actors. Upcoming mandatory security certifications offer a promising way to mitigate these risks by enforcing best practices and providing transparency. Regulatory bodies are developing IoT security frameworks, but a universal standard for large-scale systematic security assessment is lacking. Existing manual testing approaches are expensive, limiting their efficacy in the diverse and rapidly evolving IoT domain. This paper reviews current IoT security challenges and assessment efforts, identifies gaps, and proposes a roadmap for scalable, automated security assessment, leveraging a model-based testing approach and machine learning techniques to strengthen consumer IoT security.</li>
</ul>

<h3>Title: Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Pedro Hermosilla, Christian Stippel, Leon Sick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06719">https://arxiv.org/abs/2504.06719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06719">https://arxiv.org/pdf/2504.06719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06719]] Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding(https://arxiv.org/abs/2504.06719)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (this https URL).</li>
</ul>

<h3>Title: Plastic tensor networks for interpretable generative modeling</h3>
<ul>
<li><strong>Authors: </strong>Katsuya O. Akamatsu, Kenji Harada, Tsuyoshi Okubo, Naoki Kawashima</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06722">https://arxiv.org/abs/2504.06722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06722">https://arxiv.org/pdf/2504.06722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06722]] Plastic tensor networks for interpretable generative modeling(https://arxiv.org/abs/2504.06722)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>A structural optimization scheme for a single-layer nonnegative adaptive tensor tree (NATT) that models a target probability distribution is proposed. The NATT scheme, by construction, has the advantage that it is interpretable as a probabilistic graphical model. We consider the NATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT) optimization scheme and demonstrate their effectiveness on a variety of generative modeling tasks where the objective is to infer the hidden structure of a provided dataset. Our results show that in terms of minimizing the negative log-likelihood, the single-layer scheme has model performance comparable to the Born machine scheme, though not better. The tasks include deducing the structure of binary bitwise operations, learning the internal structure of random Bayesian networks given only visible sites, and a real-world example related to hierarchical clustering where a cladogram is constructed from mitochondrial DNA sequences. In doing so, we also show the importance of the choice of network topology and the versatility of a least-mutual information criterion in selecting a candidate structure for a tensor tree, as well as discuss aspects of these tensor tree generative models including their information content and interpretability.</li>
</ul>

<h3>Title: EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture</h3>
<ul>
<li><strong>Authors: </strong>Wenfeng Feng, Guoying Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06738">https://arxiv.org/abs/2504.06738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06738">https://arxiv.org/pdf/2504.06738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06738]] EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture(https://arxiv.org/abs/2504.06738)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models. Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. To address this, we introduce a layer-aligned encoder-decoder architecture, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. EDIT is naturally interpretable demonstrated through sequential attention maps, illustrating the refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves consistent performance improvements over DeiT3 models. These results highlight the effectiveness of EDIT's design in addressing attention sink and improving visual feature extraction.</li>
</ul>

<h3>Title: MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Ylli Sadikaj, Hongkuan Zhou, Lavdim Halilaj, Stefan Schmid, Steffen Staab, Claudia Plant</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06740">https://arxiv.org/abs/2504.06740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06740">https://arxiv.org/pdf/2504.06740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06740]] MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning(https://arxiv.org/abs/2504.06740)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Precise optical inspection in industrial applications is crucial for minimizing scrap rates and reducing the associated costs. Besides merely detecting if a product is anomalous or not, it is crucial to know the distinct type of defect, such as a bent, cut, or scratch. The ability to recognize the "exact" defect type enables automated treatments of the anomalies in modern production lines. Current methods are limited to solely detecting whether a product is defective or not without providing any insights on the defect type, nevertheless detecting and identifying multiple defects. We propose MultiADS, a zero-shot learning approach, able to perform Multi-type Anomaly Detection and Segmentation. The architecture of MultiADS comprises CLIP and extra linear layers to align the visual- and textual representation in a joint feature space. To the best of our knowledge, our proposal, is the first approach to perform a multi-type anomaly segmentation task in zero-shot learning. Contrary to the other baselines, our approach i) generates specific anomaly masks for each distinct defect type, ii) learns to distinguish defect types, and iii) simultaneously identifies multiple defect types present in an anomalous product. Additionally, our approach outperforms zero/few-shot learning SoTA methods on image-level and pixel-level anomaly detection and segmentation tasks on five commonly used datasets: MVTec-AD, Visa, MPDD, MAD and Real-IAD.</li>
</ul>

<h3>Title: Large Scale Supervised Pretraining For Traumatic Brain Injury Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Constantin Ulrich, Tassilo Wald, Fabian Isensee, Klaus H. Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06741">https://arxiv.org/abs/2504.06741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06741">https://arxiv.org/pdf/2504.06741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06741]] Large Scale Supervised Pretraining For Traumatic Brain Injury Segmentation(https://arxiv.org/abs/2504.06741)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The segmentation of lesions in Moderate to Severe Traumatic Brain Injury (msTBI) presents a significant challenge in neuroimaging due to the diverse characteristics of these lesions, which vary in size, shape, and distribution across brain regions and tissue types. This heterogeneity complicates traditional image processing techniques, resulting in critical errors in tasks such as image registration and brain parcellation. To address these challenges, the AIMS-TBI Segmentation Challenge 2024 aims to advance innovative segmentation algorithms specifically designed for T1-weighted MRI data, the most widely utilized imaging modality in clinical practice. Our proposed solution leverages a large-scale multi-dataset supervised pretraining approach inspired by the MultiTalent method. We train a Resenc L network on a comprehensive collection of datasets covering various anatomical and pathological structures, which equips the model with a robust understanding of brain anatomy and pathology. Following this, the model is fine-tuned on msTBI-specific data to optimize its performance for the unique characteristics of T1-weighted MRI scans and outperforms the baseline without pretraining up to 2 Dice points.</li>
</ul>

<h3>Title: nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Ertl, Shuhan Xiao, Stefan Denner, Robin Peretzke, David Zimmerer, Peter Neher, Fabian Isensee, Klaus Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06742">https://arxiv.org/abs/2504.06742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06742">https://arxiv.org/pdf/2504.06742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06742]] nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection(https://arxiv.org/abs/2504.06742)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Landmark detection plays a crucial role in medical imaging tasks that rely on precise spatial localization, including specific applications in diagnosis, treatment planning, image registration, and surgical navigation. However, manual annotation is labor-intensive and requires expert knowledge. While deep learning shows promise in automating this task, progress is hindered by limited public datasets, inconsistent benchmarks, and non-standardized baselines, restricting reproducibility, fair comparisons, and model this http URL work introduces nnLandmark, a self-configuring deep learning framework for 3D medical landmark detection, adapting nnU-Net to perform heatmap-based regression. By leveraging nnU-Net's automated configuration, nnLandmark eliminates the need for manual parameter tuning, offering out-of-the-box usability. It achieves state-of-the-art accuracy across two public datasets, with a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML) dental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset (AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm. With its strong generalization, reproducibility, and ease of deployment, nnLandmark establishes a reliable baseline for 3D landmark detection, supporting research in anatomical localization and clinical workflows that depend on precise landmark identification. The code will be available soon.</li>
</ul>

<h3>Title: More Efficient Stealth Address Protocol</h3>
<ul>
<li><strong>Authors: </strong>Marija Mikic, Mihajlo Srbakoski, Strahinja Praska</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06744">https://arxiv.org/abs/2504.06744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06744">https://arxiv.org/pdf/2504.06744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06744]] More Efficient Stealth Address Protocol(https://arxiv.org/abs/2504.06744)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, steal</a></li>
<li><strong>Abstract: </strong>The integration of privacy-preserving transactions into public blockchains such as Ethereum remains a major challenge. The Stealth Address Protocol (SAP) provides recipient anonymity by generating unlinkable stealth addresses. Existing SAPs, such as the Dual-Key Stealth Address Protocol and the Curvy Protocol, have shown significant improvements in efficiency, but remain vulnerable to quantum attacks. Post-quantum SAPs based on lattice-based cryptography, such as the Module-LWE SAP, on the other hand, offer quantum resistance while achieving better performance. In this paper, we present a novel hybrid SAP that combines the Curvy protocol with the computational advantages of the Module-LWE technique while remaining Ethereum-friendly. In contrast to full post-quantum solutions, our approach does not provide quantum security, but achieves a significant speedup in scanning the ephemeral public key registry, about three times faster than the Curvy protocol. We present a detailed cryptographic construction of our protocol and compare its performance with existing solutions. Our results prove that this hybrid approach is the most efficient Ethereum-compatible SAP to date.</li>
</ul>

<h3>Title: Compass Control: Multi Object Orientation Control for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Rishbuh Parihar, Vaibhav Agrawal, Sachidanand VS, R. Venkatesh Babu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06752">https://arxiv.org/abs/2504.06752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06752">https://arxiv.org/pdf/2504.06752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06752]] Compass Control: Multi Object Orientation Control for Text-to-Image Generation(https://arxiv.org/abs/2504.06752)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.</li>
</ul>

<h3>Title: FedMerge: Federated Personalization via Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Shutong Chen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06768">https://arxiv.org/abs/2504.06768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06768">https://arxiv.org/pdf/2504.06768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06768]] FedMerge: Federated Personalization via Model Merging(https://arxiv.org/abs/2504.06768)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>One global model in federated learning (FL) might not be sufficient to serve many clients with non-IID tasks and distributions. While there has been advances in FL to train multiple global models for better personalization, they only provide limited choices to clients so local finetuning is still indispensable. In this paper, we propose a novel ``FedMerge'' approach that can create a personalized model per client by simply merging multiple global models with automatically optimized and customized weights. In FedMerge, a few global models can serve many non-IID clients, even without further local finetuning. We formulate this problem as a joint optimization of global models and the merging weights for each client. Unlike existing FL approaches where the server broadcasts one or multiple global models to all clients, the server only needs to send a customized, merged model to each client. Moreover, instead of periodically interrupting the local training and re-initializing it to a global model, the merged model aligns better with each client's task and data distribution, smoothening the local-global gap between consecutive rounds caused by client drift. We evaluate FedMerge on three different non-IID settings applied to different domains with diverse tasks and data types, in which FedMerge consistently outperforms existing FL approaches, including clustering-based and mixture-of-experts (MoE) based methods.</li>
</ul>

<h3>Title: Domain Generalization through Attenuation of Domain-Specific Information</h3>
<ul>
<li><strong>Authors: </strong>Reiji Saito, Kazuhiro Hotta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06781">https://arxiv.org/abs/2504.06781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06781">https://arxiv.org/pdf/2504.06781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06781]] Domain Generalization through Attenuation of Domain-Specific Information(https://arxiv.org/abs/2504.06781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new evaluation metric called Domain Independence (DI) and Attenuation of Domain-Specific Information (ADSI) which is specifically designed for domain-generalized semantic segmentation in automotive images. DI measures the presence of domain-specific information: a lower DI value indicates strong domain dependence, while a higher DI value suggests greater domain independence. This makes it roughly where domain-specific information exists and up to which frequency range it is present. As a result, it becomes possible to effectively suppress only the regions in the image that contain domain-specific information, enabling feature extraction independent of the domain. ADSI uses a Butterworth filter to remove the low-frequency components of images that contain inherent domain-specific information such as sensor characteristics and lighting conditions. However, since low-frequency components also contain important information such as color, we should not remove them completely. Thus, a scalar value (ranging from 0 to 1) is multiplied by the low-frequency components to retain essential information. This helps the model learn more domain-independent features. In experiments, GTA5 (synthetic dataset) was used as training images, and a real-world dataset was used for evaluation, and the proposed method outperformed conventional approaches. Similarly, in experiments that the Cityscapes (real-world dataset) was used for training and various environment datasets such as rain and nighttime were used for evaluation, the proposed method demonstrated its robustness under nighttime conditions.</li>
</ul>

<h3>Title: Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Shuoshuo Xu, Kai Zhao, James Loney, Zili Li, Andrea Visentin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06785">https://arxiv.org/abs/2504.06785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06785">https://arxiv.org/pdf/2504.06785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06785]] Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring(https://arxiv.org/abs/2504.06785)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments.</li>
</ul>

<h3>Title: Beware of "Explanations" of AI</h3>
<ul>
<li><strong>Authors: </strong>David Martens, Galit Shmueli, Theodoros Evgeniou, Kevin Bauer, Christian Janiesch, Stefan Feuerriegel, Sebastian Gabel, Sofie Goethals, Travis Greene, Nadja Klein, Mathias Kraus, Niklas Kühl, Claudia Perlich, Wouter Verbeke, Alona Zharova, Patrick Zschech, Foster Provost</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06791">https://arxiv.org/abs/2504.06791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06791">https://arxiv.org/pdf/2504.06791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06791]] Beware of "Explanations" of AI(https://arxiv.org/abs/2504.06791)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a "good" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations.</li>
</ul>

<h3>Title: A Meaningful Perturbation Metric for Evaluating Explainability Methods</h3>
<ul>
<li><strong>Authors: </strong>Danielle Cohen, Hila Chefer, Lior Wolf</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06800">https://arxiv.org/abs/2504.06800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06800">https://arxiv.org/pdf/2504.06800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06800]] A Meaningful Perturbation Metric for Evaluating Explainability Methods(https://arxiv.org/abs/2504.06800)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have demonstrated remarkable success, yet their wide adoption is often hindered by their opaque decision-making. To address this, attribution methods have been proposed to assign relevance values to each part of the input. However, different methods often produce entirely different relevance maps, necessitating the development of standardized metrics to evaluate them. Typically, such evaluation is performed through perturbation, wherein high- or low-relevance regions of the input image are manipulated to examine the change in prediction. In this work, we introduce a novel approach, which harnesses image generation models to perform targeted perturbation. Specifically, we focus on inpainting only the high-relevance pixels of an input image to modify the model's predictions while preserving image fidelity. This is in contrast to existing approaches, which often produce out-of-distribution modifications, leading to unreliable results. Through extensive experiments, we demonstrate the effectiveness of our approach in generating meaningful rankings across a wide range of models and attribution methods. Crucially, we establish that the ranking produced by our metric exhibits significantly higher correlation with human preferences compared to existing approaches, underscoring its potential for enhancing interpretability in DNNs.</li>
</ul>

<h3>Title: DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06803">https://arxiv.org/abs/2504.06803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06803">https://arxiv.org/pdf/2504.06803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06803]] DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation(https://arxiv.org/abs/2504.06803)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \emph{static} inference paradigm, which inevitably introduces redundant computation in certain \emph{diffusion timesteps} and \emph{spatial regions}. To overcome this inefficiency, we propose \textbf{Dy}namic \textbf{Di}ffusion \textbf{T}ransformer (DyDiT), an architecture that \emph{dynamically} adjusts its computation along both \emph{timestep} and \emph{spatial} dimensions. Specifically, we introduce a \emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.</li>
</ul>

<h3>Title: Robust Classification with Noisy Labels Based on Posterior Maximization</h3>
<ul>
<li><strong>Authors: </strong>Nicola Novello, Andrea M. Tonello</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06805">https://arxiv.org/abs/2504.06805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06805">https://arxiv.org/pdf/2504.06805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06805]] Robust Classification with Noisy Labels Based on Posterior Maximization(https://arxiv.org/abs/2504.06805)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Designing objective functions robust to label noise is crucial for real-world classification algorithms. In this paper, we investigate the robustness to label noise of an $f$-divergence-based class of objective functions recently proposed for supervised classification, herein referred to as $f$-PML. We show that, in the presence of label noise, any of the $f$-PML objective functions can be corrected to obtain a neural network that is equal to the one learned with the clean dataset. Additionally, we propose an alternative and novel correction approach that, during the test phase, refines the posterior estimated by the neural network trained in the presence of label noise. Then, we demonstrate that, even if the considered $f$-PML objective functions are not symmetric, they are robust to symmetric label noise for any choice of $f$-divergence, without the need for any correction approach. This allows us to prove that the cross-entropy, which belongs to the $f$-PML class, is robust to symmetric label noise. Finally, we show that such a class of objective functions can be used together with refined training strategies, achieving competitive performance against state-of-the-art techniques of classification with label noise.</li>
</ul>

<h3>Title: Hybrid CNN with Chebyshev Polynomial Expansion for Medical Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Roy, Bhavesh Gyanchandani, Aditya Oza</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06811">https://arxiv.org/abs/2504.06811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06811">https://arxiv.org/pdf/2504.06811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06811]] Hybrid CNN with Chebyshev Polynomial Expansion for Medical Image Analysis(https://arxiv.org/abs/2504.06811)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lung cancer remains one of the leading causes of cancer-related mortality worldwide, with early and accurate diagnosis playing a pivotal role in improving patient outcomes. Automated detection of pulmonary nodules in computed tomography (CT) scans is a challenging task due to variability in nodule size, shape, texture, and location. Traditional Convolutional Neural Networks (CNNs) have shown considerable promise in medical image analysis; however, their limited ability to capture fine-grained spatial-spectral variations restricts their performance in complex diagnostic scenarios. In this study, we propose a novel hybrid deep learning architecture that incorporates Chebyshev polynomial expansions into CNN layers to enhance expressive power and improve the representation of underlying anatomical structures. The proposed Chebyshev-CNN leverages the orthogonality and recursive properties of Chebyshev polynomials to extract high-frequency features and approximate complex nonlinear functions with greater fidelity. The model is trained and evaluated on benchmark lung cancer imaging datasets, including LUNA16 and LIDC-IDRI, achieving superior performance in classifying pulmonary nodules as benign or malignant. Quantitative results demonstrate significant improvements in accuracy, sensitivity, and specificity compared to traditional CNN-based approaches. This integration of polynomial-based spectral approximation within deep learning provides a robust framework for enhancing automated medical diagnostics and holds potential for broader applications in clinical decision support systems.</li>
</ul>

<h3>Title: A Graph Diffusion Algorithm for Lexical Similarity Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Karol Mikula, Mariana Sarkociová Remešíková</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06816">https://arxiv.org/abs/2504.06816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06816">https://arxiv.org/pdf/2504.06816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06816]] A Graph Diffusion Algorithm for Lexical Similarity Evaluation(https://arxiv.org/abs/2504.06816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present an algorithm for evaluating lexical similarity between a given language and several reference language clusters. As an input, we have a list of concepts and the corresponding translations in all considered languages. Moreover, each reference language is assigned to one of $c$ language clusters. For each of the concepts, the algorithm computes the distance between each pair of translations. Based on these distances, it constructs a weighted directed graph, where every vertex represents a language. After, it solves a graph diffusion equation with a Dirichlet boundary condition, where the unknown is a map from the vertex set to $\mathbb{R}^c$. The resulting coordinates are values from the interval $[0,1]$ and they can be interpreted as probabilities of belonging to each of the clusters or as a lexical similarity distribution with respect to the reference clusters. The distances between translations are calculated using phonetic transcriptions and a modification of the Damerau-Levenshtein distance. The algorithm can be useful in analyzing relationships between languages spoken in multilingual territories with a lot of mutual influences. We demonstrate this by presenting a case study regarding various European languages.</li>
</ul>

<h3>Title: Regret Bounds for Robust Online Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Alexander Appel, Vanessa Kosoy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06820">https://arxiv.org/abs/2504.06820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06820">https://arxiv.org/pdf/2504.06820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06820]] Regret Bounds for Robust Online Decision Making(https://arxiv.org/abs/2504.06820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a framework which generalizes "decision making with structured observations" by allowing robust (i.e. multivalued) models. In this framework, each model associates each decision with a convex set of probability distributions over outcomes. Nature can choose distributions out of this set in an arbitrary (adversarial) manner, that can be nonoblivious and depend on past history. The resulting framework offers much greater generality than classical bandits and reinforcement learning, since the realizability assumption becomes much weaker and more realistic. We then derive a theory of regret bounds for this framework. Although our lower and upper bounds are not tight, they are sufficient to fully characterize power-law learnability. We demonstrate this theory in two special cases: robust linear bandits and tabular robust online reinforcement learning. In both cases, we derive regret bounds that improve state-of-the-art (except that we do not address computational efficiency).</li>
</ul>

<h3>Title: Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms</h3>
<ul>
<li><strong>Authors: </strong>Xiaotian Ye, Mengqi Zhang, Shu Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06823">https://arxiv.org/abs/2504.06823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06823">https://arxiv.org/pdf/2504.06823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06823]] Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms(https://arxiv.org/abs/2504.06823)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge is fundamental to the overall capabilities of Large Language Models (LLMs). The knowledge paradigm of a model, which dictates how it encodes and utilizes knowledge, significantly affects its performance. Despite the continuous development of LLMs under existing knowledge paradigms, issues within these frameworks continue to constrain model potential. This blog post highlight three critical open problems limiting model capabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of reverse knowledge generalization (the reversal curse), and (3) conflicts in internal knowledge. We review recent progress made in addressing these issues and discuss potential general solutions. Based on observations in these areas, we propose a hypothetical paradigm based on Contextual Knowledge Scaling, and further outline implementation pathways that remain feasible within contemporary techniques. Evidence suggests this approach holds potential to address current shortcomings, serving as our vision for future model paradigms. This blog post aims to provide researchers with a brief overview of progress in LLM knowledge systems, while provide inspiration for the development of next-generation model architectures.</li>
</ul>

<h3>Title: IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments</h3>
<ul>
<li><strong>Authors: </strong>Can Zhang, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06827">https://arxiv.org/abs/2504.06827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06827">https://arxiv.org/pdf/2504.06827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06827]] IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments(https://arxiv.org/abs/2504.06827)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Adaptive Locally Linear Embedding</h3>
<ul>
<li><strong>Authors: </strong>Ali Goli, Mahdieh Alizadeh, Hadi Sadoghi Yazdi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06829">https://arxiv.org/abs/2504.06829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06829">https://arxiv.org/pdf/2504.06829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06829]] Adaptive Locally Linear Embedding(https://arxiv.org/abs/2504.06829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Manifold learning techniques, such as Locally linear embedding (LLE), are designed to preserve the local neighborhood structures of high-dimensional data during dimensionality reduction. Traditional LLE employs Euclidean distance to define neighborhoods, which can struggle to capture the intrinsic geometric relationships within complex data. A novel approach, Adaptive locally linear embedding(ALLE), is introduced to address this limitation by incorporating a dynamic, data-driven metric that enhances topological preservation. This method redefines the concept of proximity by focusing on topological neighborhood inclusion rather than fixed distances. By adapting the metric based on the local structure of the data, it achieves superior neighborhood preservation, particularly for datasets with complex geometries and high-dimensional structures. Experimental results demonstrate that ALLE significantly improves the alignment between neighborhoods in the input and feature spaces, resulting in more accurate and topologically faithful embeddings. This approach advances manifold learning by tailoring distance metrics to the underlying data, providing a robust solution for capturing intricate relationships in high-dimensional datasets.</li>
</ul>

<h3>Title: Symbolic Parallel Composition for Multi-language Protocol Verification</h3>
<ul>
<li><strong>Authors: </strong>Faezeh Nasrabadi, Robert Künnemann, Hamed Nemati</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06833">https://arxiv.org/abs/2504.06833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06833">https://arxiv.org/pdf/2504.06833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06833]] Symbolic Parallel Composition for Multi-language Protocol Verification(https://arxiv.org/abs/2504.06833)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The implementation of security protocols often combines different languages. This practice, however, poses a challenge to traditional verification techniques, which typically assume a single-language environment and, therefore, are insufficient to handle challenges presented by the interplay of different languages. To address this issue, we establish principles for combining multiple programming languages operating on different atomic types using a symbolic execution semantics. This facilitates the (parallel) composition of labeled transition systems, improving the analysis of complex systems by streamlining communication between diverse programming languages. By treating the Dolev-Yao (DY) model as a symbolic abstraction, our approach eliminates the need for translation between different base types, such as bitstrings and DY terms. Our technique provides a foundation for securing interactions in multi-language environments, enhancing program verification and system analysis in complex, interconnected systems.</li>
</ul>

<h3>Title: LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Wang, Haoran Wu, Yiming Rong, Deyang Jiang, Yixin Zhang, Yunlong Zhao, Shuang Xu, Bo XU</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06835">https://arxiv.org/abs/2504.06835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06835">https://arxiv.org/pdf/2504.06835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06835]] LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding(https://arxiv.org/abs/2504.06835)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long video understanding is a complex task that requires both spatial detail and temporal awareness. While Vision-Language Models (VLMs) obtain frame-level understanding capabilities through multi-frame input, they suffer from information loss due to the sparse sampling strategy. In contrast, Video Large Language Models (Video-LLMs) capture temporal relationships within visual features but are limited by the scarcity of high-quality video-text datasets. To transfer long video understanding capabilities to VLMs with minimal data and computational cost, we propose Lightweight Video Compression (LVC), a novel method featuring the Query-Attention Video Compression mechanism, which effectively tackles the sparse sampling problem in VLMs. By training only the alignment layer with 10k short video-text pairs, LVC significantly enhances the temporal reasoning abilities of VLMs. Extensive experiments show that LVC provides consistent performance improvements across various models, including the InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC achieves scores of 68.2 and 65.9 on the long video understanding benchmarks MLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%. The enhanced models and code will be publicly available soon.</li>
</ul>

<h3>Title: Determining Fetal Orientations From Blind Sweep Ultrasound Video</h3>
<ul>
<li><strong>Authors: </strong>Jakub Maciej Wiśniewski, Anders Nymark Christensen, Mary Le Ngo, Martin Grønnebæk Tolsgaard, Chun Kit Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06836">https://arxiv.org/abs/2504.06836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06836">https://arxiv.org/pdf/2504.06836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06836]] Determining Fetal Orientations From Blind Sweep Ultrasound Video(https://arxiv.org/abs/2504.06836)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Cognitive demands of fetal ultrasound examinations pose unique challenges among clinicians. With the goal of providing an assistive tool, we developed an automated pipeline for predicting fetal orientation from ultrasound videos acquired following a simple blind sweep protocol. Leveraging on a pre-trained head detection and segmentation model, this is achieved by first determining the fetal presentation (cephalic or breech) with a template matching approach, followed by the fetal lie (facing left or right) by analyzing the spatial distribution of segmented brain anatomies. Evaluation on a dataset of third-trimester ultrasound scans demonstrated the promising accuracy of our pipeline. This work distinguishes itself by introducing automated fetal lie prediction and by proposing an assistive paradigm that augments sonographer expertise rather than replacing it. Future research will focus on enhancing acquisition efficiency, and exploring real-time clinical integration to improve workflow and support for obstetric clinicians.</li>
</ul>

<h3>Title: ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seonghwan Park, Jaehyeon Jeong, Yongjun Kim, Jaeho Lee, Namhoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06838">https://arxiv.org/abs/2504.06838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06838">https://arxiv.org/pdf/2504.06838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06838]] ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models(https://arxiv.org/abs/2504.06838)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent studies have introduced various approaches for prompt-tuning black-box vision-language models, referred to as black-box prompt-tuning (BBPT). While BBPT has demonstrated considerable potential, it is often found that many existing methods require an excessive number of queries (i.e., function evaluations), which poses a significant challenge in real-world scenarios where the number of allowed queries is limited. To tackle this issue, we propose Zeroth-order Intrinsic-dimensional Prompt-tuning (ZIP), a novel approach that enables efficient and robust prompt optimization in a purely black-box setting. The key idea of ZIP is to reduce the problem dimensionality and the variance of zeroth-order gradient estimates, such that the training is done fast with far less queries. We achieve this by re-parameterizing prompts in low-rank representations and designing intrinsic-dimensional clipping of estimated gradients. We evaluate ZIP on 13+ vision-language tasks in standard benchmarks and show that it achieves an average improvement of approximately 6% in few-shot accuracy and 48% in query efficiency compared to the best-performing alternative BBPT methods, establishing a new state of the art. Our ablation analysis further shows that the proposed clipping mechanism is robust and nearly optimal, without the need to manually select the clipping threshold, matching the result of expensive hyperparameter search.</li>
</ul>

<h3>Title: Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Angela Lopez-Cardona, Sebastian Idesis, Ioannis Arapakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06843">https://arxiv.org/abs/2504.06843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06843">https://arxiv.org/pdf/2504.06843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06843]] Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions(https://arxiv.org/abs/2504.06843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, the integration of cognitive neuroscience in Natural Language Processing (NLP) has gained significant attention. This article provides a critical and timely overview of recent advancements in leveraging cognitive signals, particularly Eye-tracking (ET) signals, to enhance Language Models (LMs) and Multimodal Large Language Models (MLLMs). By incorporating user-centric cognitive signals, these approaches address key challenges, including data scarcity and the environmental costs of training large-scale models. Cognitive signals enable efficient data augmentation, faster convergence, and improved human alignment. The review emphasises the potential of ET data in tasks like Visual Question Answering (VQA) and mitigating hallucinations in MLLMs, and concludes by discussing emerging challenges and research trends.</li>
</ul>

<h3>Title: CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading</h3>
<ul>
<li><strong>Authors: </strong>Mishan Aliev, Dmitry Baranchuk, Kirill Struminsky</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06856">https://arxiv.org/abs/2504.06856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06856">https://arxiv.org/pdf/2504.06856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06856]] CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading(https://arxiv.org/abs/2504.06856)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work investigates text-to-texture synthesis using diffusion models to generate physically-based texture maps. We aim to achieve realistic model appearances under varying lighting conditions. A prominent solution for the task is score distillation sampling. It allows recovering a complex texture using gradient guidance given a differentiable rasterization and shading pipeline. However, in practice, the aforementioned solution in conjunction with the widespread latent diffusion models produces severe visual artifacts and requires additional regularization such as implicit texture parameterization. As a more direct alternative, we propose an approach using cascaded diffusion models for texture synthesis (CasTex). In our setup, score distillation sampling yields high-quality textures out-of-the box. In particular, we were able to omit implicit texture parameterization in favor of an explicit parameterization to improve the procedure. In the experiments, we show that our approach significantly outperforms state-of-the-art optimization-based solutions on public texture synthesis benchmarks.</li>
</ul>

<h3>Title: EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Diljeet Jagpal, Xi Chen, Vinay P. Namboodiri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06861">https://arxiv.org/abs/2504.06861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06861">https://arxiv.org/pdf/2504.06861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06861]] EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation(https://arxiv.org/abs/2504.06861)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation models, which limit their adaptability and scalability. In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, our approach can ensure appropriate control between coherence and variance for the frames. Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation.</li>
</ul>

<h3>Title: MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking</h3>
<ul>
<li><strong>Authors: </strong>Chang Nie, Yiqing Xu, Guangming Wang, Zhe Liu, Yanzi Miao, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06863">https://arxiv.org/abs/2504.06863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06863">https://arxiv.org/pdf/2504.06863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06863]] MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking(https://arxiv.org/abs/2504.06863)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Moving object segmentation plays a vital role in understanding dynamic visual environments. While existing methods rely on multi-frame image sequences to identify moving objects, single-image MOS is critical for applications like motion intention prediction and handling camera frame drops. However, segmenting moving objects from a single image remains challenging for existing methods due to the absence of temporal cues. To address this gap, we propose MovSAM, the first framework for single-image moving object segmentation. MovSAM leverages a Multimodal Large Language Model (MLLM) enhanced with Chain-of-Thought (CoT) prompting to search the moving object and generate text prompts based on deep thinking for segmentation. These prompts are cross-fused with visual features from the Segment Anything Model (SAM) and a Vision-Language Model (VLM), enabling logic-driven moving object segmentation. The segmentation results then undergo a deep thinking refinement loop, allowing MovSAM to iteratively improve its understanding of the scene context and inter-object relationships with logical reasoning. This innovative approach enables MovSAM to segment moving objects in single images by considering scene understanding. We implement MovSAM in the real world to validate its practical application and effectiveness for autonomous driving scenarios where the multi-frame methods fail. Furthermore, despite the inherent advantage of multi-frame methods in utilizing temporal information, MovSAM achieves state-of-the-art performance across public MOS benchmarks, reaching 92.5\% on J\&F. Our implementation will be available at this https URL.</li>
</ul>

<h3>Title: ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities</h3>
<ul>
<li><strong>Authors: </strong>Dingkun Yan, Xinrui Wang, Yusuke Iwasawa, Yutaka Matsuo, Suguru Saito, Jiaxian Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06895">https://arxiv.org/abs/2504.06895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06895">https://arxiv.org/pdf/2504.06895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06895]] ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities(https://arxiv.org/abs/2504.06895)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reference-based sketch colorization methods have garnered significant attention due to their potential applications in the animation production industry. However, most existing methods are trained with image triplets of sketch, reference, and ground truth that are semantically and spatially well-aligned, while real-world references and sketches often exhibit substantial misalignment. This mismatch in data distribution between training and inference leads to overfitting, consequently resulting in spatial artifacts and significant degradation in overall colorization quality, limiting potential applications of current methods for general purposes. To address this limitation, we conduct an in-depth analysis of the \textbf{carrier}, defined as the latent representation facilitating information transfer from reference to sketch. Based on this analysis, we propose a novel workflow that dynamically adapts the carrier to optimize distinct aspects of colorization. Specifically, for spatially misaligned artifacts, we introduce a split cross-attention mechanism with spatial masks, enabling region-specific reference injection within the diffusion process. To mitigate semantic neglect of sketches, we employ dedicated background and style encoders to transfer detailed reference information in the latent feature space, achieving enhanced spatial control and richer detail synthesis. Furthermore, we propose character-mask merging and background bleaching as preprocessing steps to improve foreground-background integration and background generation. Extensive qualitative and quantitative evaluations, including a user study, demonstrate the superior performance of our proposed method compared to existing approaches. An ablation study further validates the efficacy of each proposed component.</li>
</ul>

<h3>Title: MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Mao, Yuhan Wang, Yucheng Tang, Daguang Xu, Kang Wang, Yang Yang, Zongwei Zhou, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06897">https://arxiv.org/abs/2504.06897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06897">https://arxiv.org/pdf/2504.06897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06897]] MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs(https://arxiv.org/abs/2504.06897)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other's generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints.</li>
</ul>

<h3>Title: UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Emmanuelle Bourigault, Amir Jamaludin, Abdullah Hamdi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06908">https://arxiv.org/abs/2504.06908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06908">https://arxiv.org/pdf/2504.06908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06908]] UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation(https://arxiv.org/abs/2504.06908)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, segmentation</a></li>
<li><strong>Abstract: </strong>In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at this https URL , and the filtered labels will be made available with the UK Biobank.</li>
</ul>

<h3>Title: An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks</h3>
<ul>
<li><strong>Authors: </strong>Miro Miranda, Francisco Mena, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06915">https://arxiv.org/abs/2504.06915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06915">https://arxiv.org/pdf/2504.06915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06915]] An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks(https://arxiv.org/abs/2504.06915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Missing instances in time series data impose a significant challenge to deep learning models, particularly in regression tasks. In the Earth Observation field, satellite failure or cloud occlusion frequently results in missing time-steps, introducing uncertainties in the predicted output and causing a decline in predictive performance. While many studies address missing time-steps through data augmentation to improve model robustness, the uncertainty arising at the input level is commonly overlooked. To address this gap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that explicitly accounts for input-level uncertainty by randomly dropping time-steps during inference using a predefined dropout ratio, thereby simulating the effect of missing data. To bypass the need for costly searches for the optimal dropout ratio, we extend this approach with Monte Carlo Concrete Temporal Dropout (MC-ConcTD), a method that learns the optimal dropout distribution directly. Both MC-TD and MC-ConcTD are applied during inference, leveraging Monte Carlo sampling for uncertainty quantification. Experiments on three EO time-series datasets demonstrate that MC-ConcTD improves predictive performance and uncertainty calibration compared to existing approaches. Additionally, we highlight the advantages of adaptive dropout tuning over manual selection, making uncertainty quantification more robust and accessible for EO applications.</li>
</ul>

<h3>Title: Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple Domains</h3>
<ul>
<li><strong>Authors: </strong>Ming Liu, Massimo Poesio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06917">https://arxiv.org/abs/2504.06917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06917">https://arxiv.org/pdf/2504.06917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06917]] Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple Domains(https://arxiv.org/abs/2504.06917)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the growth of the Internet, buying habits have changed, and customers have become more dependent on the online opinions of other customers to guide their purchases. Identifying fake reviews thus became an important area for Natural Language Processing (NLP) research. However, developing high-performance NLP models depends on the availability of large amounts of training data, which are often not available for low-resource languages or domains. In this research, we used large language models to generate datasets to train fake review detectors. Our approach was used to generate fake reviews in different domains (book reviews, restaurant reviews, and hotel reviews) and different languages (English and Chinese). Our results demonstrate that our data augmentation techniques result in improved performance at fake review detection for all domains and languages. The accuracy of our fake review detection model can be improved by 0.3 percentage points on DeRev TEST, 10.9 percentage points on Amazon TEST, 8.3 percentage points on Yelp TEST and 7.2 percentage points on DianPing TEST using the augmented datasets.</li>
</ul>

<h3>Title: The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06923">https://arxiv.org/abs/2504.06923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06923">https://arxiv.org/pdf/2504.06923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06923]] The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data(https://arxiv.org/abs/2504.06923)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, generative</a></li>
<li><strong>Abstract: </strong>Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility. In this paper, we present an extensive measurement study of four discretization strategies in the context of DP marginal generative models. More precisely, we design DP versions of three discretizers (uniform, quantile, and k-means) and reimplement the PrivTree algorithm. We find that optimizing both the choice of discretizer and bin count can improve utility, on average, by almost 30% across six DP marginal models, compared to the default strategy and number of bins, with PrivTree being the best-performing discretizer in the majority of cases. We demonstrate that, while DP generative models with non-private discretization remain vulnerable to membership inference attacks, applying DP during discretization effectively mitigates this risk. Finally, we propose an optimized approach for automatically selecting the optimal number of bins, achieving high utility while reducing both privacy budget consumption and computational overhead.</li>
</ul>

<h3>Title: Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sergio Romero-Tapiador, Ruben Tolosana, Blanca Lacruz-Pleguezuelos, Laura Judith Marcos Zambrano, Guadalupe X.Bazán, Isabel Espinosa-Salinas, Julian Fierrez, Javier Ortega-Garcia, Enrique Carrillo de Santa Pau, Aythami Morales</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06925">https://arxiv.org/abs/2504.06925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06925">https://arxiv.org/pdf/2504.06925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06925]] Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition(https://arxiv.org/abs/2504.06925)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Automatic dietary assessment based on food images remains a challenge, requiring precise food detection, segmentation, and classification. Vision-Language Models (VLMs) offer new possibilities by integrating visual and textual reasoning. In this study, we evaluate six state-of-the-art VLMs (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their capabilities in food recognition at different levels. For the experimental framework, we introduce the FoodNExTDB, a unique food image database that contains 9,263 expert-labeled images across 10 categories (e.g., "protein source"), 62 subcategories (e.g., "poultry"), and 9 cooking styles (e.g., "grilled"). In total, FoodNExTDB includes 50k nutritional labels generated by seven experts who manually annotated all images in the database. Also, we propose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts for the inter-annotator variability. Results show that closed-source models outperform open-source ones, achieving over 90% EWR in recognizing food products in images containing a single product. Despite their potential, current VLMs face challenges in fine-grained food recognition, particularly in distinguishing subtle differences in cooking styles and visually similar food items, which limits their reliability for automatic dietary assessment. The FoodNExTDB database is publicly available at this https URL.</li>
</ul>

<h3>Title: RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Urška Matjašec, Nikola Simidjievski, Mateja Jamnik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06927">https://arxiv.org/abs/2504.06927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06927">https://arxiv.org/pdf/2504.06927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06927]] RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data(https://arxiv.org/abs/2504.06927)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Tree-based models are often robust to uninformative features and can accurately capture non-smooth, complex decision boundaries. Consequently, they often outperform neural network-based models on tabular datasets at a significantly lower computational cost. Nevertheless, the capability of traditional tree-based ensembles to express complex relationships efficiently is limited by using a single feature to make splits. To improve the efficiency and expressiveness of tree-based methods, we propose Random Oblique Fast Interpretable Greedy-Tree Sums (RO-FIGS). RO-FIGS builds on Fast Interpretable Greedy-Tree Sums, and extends it by learning trees with oblique or multivariate splits, where each split consists of a linear combination learnt from random subsets of features. This helps uncover interactions between features and improves performance. The proposed method is suitable for tabular datasets with both numerical and categorical features. We evaluate RO-FIGS on 22 real-world tabular datasets, demonstrating superior performance and much smaller models over other tree- and neural network-based methods. Additionally, we analyse their splits to reveal valuable insights into feature interactions, enriching the information learnt from SHAP summary plots, and thereby demonstrating the enhanced interpretability of RO-FIGS models. The proposed method is well-suited for applications, where balance between accuracy and interpretability is essential.</li>
</ul>

<h3>Title: ASRL:A robust loss function with potential for development</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Hui, Anran Zhang, Xintong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06935">https://arxiv.org/abs/2504.06935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06935">https://arxiv.org/pdf/2504.06935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06935]] ASRL:A robust loss function with potential for development(https://arxiv.org/abs/2504.06935)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this article, we proposed a partition:wise robust loss function based on the previous robust loss function. The characteristics of this loss function are that it achieves high robustness and a wide range of applicability through partition-wise design and adaptive parameter adjustment. Finally, the advantages and development potential of this loss function were verified by applying this loss function to the regression question and using five different datasets (with different dimensions, different sample numbers, and different fields) to compare with the other loss functions. The results of multiple experiments have proven the advantages of our loss function .</li>
</ul>

<h3>Title: RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts</h3>
<ul>
<li><strong>Authors: </strong>Natalia Loukachevitch, Natalia Tkachenko, Anna Lapanitsyna, Mikhail Tikhomirov, Nicolay Rusnachenko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06947">https://arxiv.org/abs/2504.06947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06947">https://arxiv.org/pdf/2504.06947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06947]] RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts(https://arxiv.org/abs/2504.06947)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.</li>
</ul>

<h3>Title: Adaptive Computation Pruning for the Forgetting Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zhixuan Lin, Johan Obando-Ceron, Xu Owen He, Aaron Courville</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06949">https://arxiv.org/abs/2504.06949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06949">https://arxiv.org/pdf/2504.06949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06949]] Adaptive Computation Pruning for the Forgetting Transformer(https://arxiv.org/abs/2504.06949)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at this https URL.</li>
</ul>

<h3>Title: PathSegDiff: Pathology Segmentation using Diffusion model representations</h3>
<ul>
<li><strong>Authors: </strong>Sachin Kumar Danisetty, Alexandros Graikos, Srikar Yellapragada, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06950">https://arxiv.org/abs/2504.06950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06950">https://arxiv.org/pdf/2504.06950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06950]] PathSegDiff: Pathology Segmentation using Diffusion model representations(https://arxiv.org/abs/2504.06950)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation is crucial in many computational pathology pipelines, including accurate disease diagnosis, subtyping, outcome, and survivability prediction. The common approach for training a segmentation model relies on a pre-trained feature extractor and a dataset of paired image and mask annotations. These are used to train a lightweight prediction model that translates features into per-pixel classes. The choice of the feature extractor is central to the performance of the final segmentation model, and recent literature has focused on finding tasks to pre-train the feature extractor. In this paper, we propose PathSegDiff, a novel approach for histopathology image segmentation that leverages Latent Diffusion Models (LDMs) as pre-trained featured extractors. Our method utilizes a pathology-specific LDM, guided by a self-supervised encoder, to extract rich semantic information from H\&E stained histopathology images. We employ a simple, fully convolutional network to process the features extracted from the LDM and generate segmentation masks. Our experiments demonstrate significant improvements over traditional methods on the BCSS and GlaS datasets, highlighting the effectiveness of domain-specific diffusion pre-training in capturing intricate tissue structures and enhancing segmentation accuracy in histopathology images.</li>
</ul>

<h3>Title: A Comparison of Deep Learning Methods for Cell Detection in Digital Cytology</h3>
<ul>
<li><strong>Authors: </strong>Marco Acerbis, Nataša Sladoje, Joakim Lindblad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06957">https://arxiv.org/abs/2504.06957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06957">https://arxiv.org/pdf/2504.06957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06957]] A Comparison of Deep Learning Methods for Cell Detection in Digital Cytology(https://arxiv.org/abs/2504.06957)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate and efficient cell detection is crucial in many biomedical image analysis tasks. We evaluate the performance of several Deep Learning (DL) methods for cell detection in Papanicolaou-stained cytological Whole Slide Images (WSIs), focusing on accuracy of predictions and computational efficiency. We examine recentoff-the-shelf algorithms as well as custom-designed detectors, applying them to two datasets: the CNSeg Dataset and the Oral Cancer (OC) Dataset. Our comparison includes well-established segmentation methods such as StarDist, Cellpose, and the Segment Anything Model 2 (SAM2), alongside centroid-based Fully Convolutional Regression Network (FCRN) approaches. We introduce a suitable evaluation metric to assess the accuracy of predictions based on the distance from ground truth positions. We also explore the impact of dataset size and data augmentation techniques on model performance. Results show that centroid-based methods, particularly the Improved Fully Convolutional Regression Network (IFCRN) method, outperform segmentation-based methods in terms of both detection accuracy and computational efficiency. This study highlights the potential of centroid-based detectors as a preferred option for cell detection in resource-limited environments, offering faster processing times and lower GPU memory usage without compromising accuracy.</li>
</ul>

<h3>Title: VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06958">https://arxiv.org/abs/2504.06958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06958">https://arxiv.org/pdf/2504.06958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06958]] VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning(https://arxiv.org/abs/2504.06958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.</li>
</ul>

<h3>Title: Towards LLMs Robustness to Changes in Prompt Format Styles</h3>
<ul>
<li><strong>Authors: </strong>Lilian Ngweta, Kiran Kate, Jason Tsay, Yara Rizk</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06969">https://arxiv.org/abs/2504.06969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06969">https://arxiv.org/pdf/2504.06969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06969]] Towards LLMs Robustness to Changes in Prompt Format Styles(https://arxiv.org/abs/2504.06969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have gained popularity in recent years for their utility in various applications. However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations. In the literature, this problem is commonly referred to as prompt brittleness. Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks. Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge. We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt few-shot examples. MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable. Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.</li>
</ul>

<h3>Title: Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Daiwei Zhang, Joaquin Gajardo, Tomislav Medic, Isinsu Katircioglu, Mike Boss, Norbert Kirchgessner, Achim Walter, Lukas Roth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06978">https://arxiv.org/abs/2504.06978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06978">https://arxiv.org/pdf/2504.06978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06978]] Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting(https://arxiv.org/abs/2504.06978)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Automated extraction of plant morphological traits is crucial for supporting crop breeding and agricultural management through high-throughput field phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due to their scalability and affordability, enabling volumetric measurements that 2D approaches cannot directly capture. While advanced methods like Neural Radiance Fields (NeRFs) have shown promise, their application has been limited to counting or extracting traits from only a few plants or organs. Furthermore, accurately measuring complex structures like individual wheat heads-essential for studying crop yields-remains particularly challenging due to occlusions and the dense arrangement of crop canopies in field conditions. The recent development of 3D Gaussian Splatting (3DGS) offers a promising alternative for HTFP due to its high-quality reconstructions and explicit point-based representation. In this paper, we present Wheat3DGS, a novel approach that leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance segmentation and morphological measurement of hundreds of wheat heads automatically, representing the first application of 3DGS to HTFP. We validate the accuracy of wheat head extraction against high-resolution laser scan data, obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and 40.2% for length, width, and volume. We provide additional comparisons to NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating superior results. Our approach enables rapid, non-destructive measurements of key yield-related traits at scale, with significant implications for accelerating crop breeding and improving our understanding of wheat development.</li>
</ul>

<h3>Title: DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction</h3>
<ul>
<li><strong>Authors: </strong>Yuan Xiao, Yuchen Chen, Shiqing Ma, Haocheng Huang, Chunrong Fang, Yanwei Chen, Weisong Sun, Yunfeng Zhu, Xiaofang Zhang, Zhenyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07002">https://arxiv.org/abs/2504.07002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07002">https://arxiv.org/pdf/2504.07002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07002]] DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction(https://arxiv.org/abs/2504.07002)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, steal, watermark</a></li>
<li><strong>Abstract: </strong>Watermarking is a technique to help identify the source of data points, which can be used to help prevent the misuse of protected datasets. Existing methods on code watermarking, leveraging the idea from the backdoor research, embed stealthy triggers as this http URL their high resilience against dilution attacks and backdoor detections, the robustness has not been fully evaluated. To fill this gap, we propose DeCoMa, a dual-channel approach to Detect and purify Code dataset this http URL overcome the high barrier created by the stealthy and hidden nature of code watermarks, DeCoMa leverages dual-channel constraints on code to generalize and map code samples into standardized templates. Subsequently, DeCoMa extracts hidden watermarks by identifying outlier associations between paired elements within the standardized templates. Finally, DeCoMa purifies the watermarked dataset by removing all samples containing the detected watermark, enabling the silent appropriation of protected code. We conduct extensive experiments to evaluate the effectiveness and efficiency of DeCoMa, covering 14 types of code watermarks and 3 representative intelligent code tasks (a total of 14 scenarios). Experimental results demonstrate that DeCoMa achieves a stable recall of 100% in 14 code watermark detection scenarios, significantly outperforming the baselines. Additionally, DeCoMa effectively attacks code watermarks with embedding rates as low as 0.1%, while maintaining comparable model performance after training on the purified dataset. Furthermore, as DeCoMa requires no model training for detection, it achieves substantially higher efficiency than all baselines, with a speedup ranging from 31.5 to 130.9X. The results call for more advanced watermarking techniques for code models, while DeCoMa can serve as a baseline for future evaluation.</li>
</ul>

<h3>Title: Latent Diffusion U-Net Representations Contain Positional Embeddings and Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Jonas Loos, Lorenz Linhardt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07008">https://arxiv.org/abs/2504.07008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07008">https://arxiv.org/pdf/2504.07008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07008]] Latent Diffusion U-Net Representations Contain Positional Embeddings and Anomalies(https://arxiv.org/abs/2504.07008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable capabilities in synthesizing realistic images, spurring interest in using their representations for various downstream tasks. To better understand the robustness of these representations, we analyze popular Stable Diffusion models using representational similarity and norms. Our findings reveal three phenomena: (1) the presence of a learned positional embedding in intermediate representations, (2) high-similarity corner artifacts, and (3) anomalous high-norm artifacts. These findings underscore the need to further investigate the properties of diffusion model representations before considering them for downstream tasks that require robust features. Project page: this https URL</li>
</ul>

<h3>Title: FAME: Introducing Fuzzy Additive Models for Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Omer Bahadir Gokmen, Yusuf Guven, Tufan Kumbasar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07011">https://arxiv.org/abs/2504.07011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07011">https://arxiv.org/pdf/2504.07011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07011]] FAME: Introducing Fuzzy Additive Models for Explainable AI(https://arxiv.org/abs/2504.07011)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>In this study, we introduce the Fuzzy Additive Model (FAM) and FAM with Explainability (FAME) as a solution for Explainable Artificial Intelligence (XAI). The family consists of three layers: (1) a Projection Layer that compresses the input space, (2) a Fuzzy Layer built upon Single Input-Single Output Fuzzy Logic Systems (SFLS), where SFLS functions as subnetworks within an additive index model, and (3) an Aggregation Layer. This architecture integrates the interpretability of SFLS, which uses human-understandable if-then rules, with the explainability of input-output relationships, leveraging the additive model structure. Furthermore, using SFLS inherently addresses issues such as the curse of dimensionality and rule explosion. To further improve interpretability, we propose a method for sculpting antecedent space within FAM, transforming it into FAME. We show that FAME captures the input-output relationships with fewer active rules, thus improving clarity. To learn the FAM family, we present a deep learning framework. Through the presented comparative results, we demonstrate the promising potential of FAME in reducing model complexity while retaining interpretability, positioning it as a valuable tool for XAI.</li>
</ul>

<h3>Title: LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware</h3>
<ul>
<li><strong>Authors: </strong>Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, Kimia Azar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07015">https://arxiv.org/abs/2504.07015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07015">https://arxiv.org/pdf/2504.07015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07015]] LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware(https://arxiv.org/abs/2504.07015)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, transformer, large language model</a></li>
<li><strong>Abstract: </strong>As modern hardware designs grow in complexity and size, ensuring security across the confidentiality, integrity, and availability (CIA) triad becomes increasingly challenging. Information flow tracking (IFT) is a widely-used approach to tracing data propagation, identifying unauthorized activities that may compromise confidentiality or/and integrity in hardware. However, traditional IFT methods struggle with scalability and adaptability, particularly in high-density and interconnected architectures, leading to tracing bottlenecks that limit applicability in large-scale hardware. To address these limitations and show the potential of transformer-based models in integrated circuit (IC) design, this paper introduces LLM-IFT that integrates large language models (LLM) for the realization of the IFT process in hardware. LLM-IFT exploits LLM-driven structured reasoning to perform hierarchical dependency analysis, systematically breaking down even the most complex designs. Through a multi-step LLM invocation, the framework analyzes both intra-module and inter-module dependencies, enabling comprehensive IFT assessment. By focusing on a set of Trust-Hub vulnerability test cases at both the IP level and the SoC level, our experiments demonstrate a 100\% success rate in accurate IFT analysis for confidentiality and integrity checks in hardware.</li>
</ul>

<h3>Title: ShadowBinding: Realizing Effective Microarchitectures for In-Core Secure Speculation Schemes</h3>
<ul>
<li><strong>Authors: </strong>Amund Bergland Kvalsvik, Magnus Själander</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07018">https://arxiv.org/abs/2504.07018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07018">https://arxiv.org/pdf/2504.07018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07018]] ShadowBinding: Realizing Effective Microarchitectures for In-Core Secure Speculation Schemes(https://arxiv.org/abs/2504.07018)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>Secure speculation schemes have shown great promise in the war against speculative side-channel attacks, and will be a key building block for developing secure, high-performance architectures moving forward. As the field matures, the need for rigorous microarchitectures, and corresponding performance and cost analysis, become critical for evaluating secure schemes and for enabling their future adoption. In ShadowBinding, we present effective microarchitectures for two state-of-the-art secure schemes, uncovering and mitigating fundamental microarchitectural limitations within the analyzed schemes, and provide important design characteristics. We uncover that Speculative Taint Tracking's (STT's) rename-based taint computation must be completed in a single cycle, creating an expensive dependency chain which greatly limits performance for wider processor cores. We also introduce a novel michroarchitectural approach for STT, named STT-Issue, which, by delaying the taint computation to the issue stage, eliminates the dependency chain, achieving better instructions per cycle (IPC), timing, area, and performance results. Through a comprehensive evaluation of our STT and Non-Speculative Data Access (NDA) microarchitectural designs on the RISC-V Berkeley Out-of-Order Machine, we find that the IPC impact of in-core secure schemes is higher than previously estimated, close to 20% for the highest performance core. With insights into timing from our RTL evaluation, the performance loss, created by the combined impact of IPC and timing, becomes even greater, at 35%, 27%, and 22% for STT-Rename, STT-Issue, and NDA, respectively. If these trends were to hold for leading processor core designs, the performance impact would be well over 30%, even for the best-performing scheme.</li>
</ul>

<h3>Title: Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety</h3>
<ul>
<li><strong>Authors: </strong>Chad Melton, Alex Sorokine, Steve Peterson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07022">https://arxiv.org/abs/2504.07022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07022">https://arxiv.org/pdf/2504.07022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07022]] Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety(https://arxiv.org/abs/2504.07022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments.</li>
</ul>

<h3>Title: Identifying Key Challenges of Hardness-Based Resampling</h3>
<ul>
<li><strong>Authors: </strong>Pawel Pukowski, Venet Osmani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07031">https://arxiv.org/abs/2504.07031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07031">https://arxiv.org/pdf/2504.07031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07031]] Identifying Key Challenges of Hardness-Based Resampling(https://arxiv.org/abs/2504.07031)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Performance gap across classes remains a persistent challenge in machine learning, often attributed to variations in class hardness. One way to quantify class hardness is through sample complexity - the minimum number of samples required to effectively learn a given class. Sample complexity theory suggests that class hardness is driven by differences in the amount of data required for generalization. That is, harder classes need substantially more samples to achieve generalization. Therefore, hardness-based resampling is a promising approach to mitigate these performance disparities. While resampling has been studied extensively in data-imbalanced settings, its impact on balanced datasets remains unexplored. This raises the fundamental question whether resampling is effective because it addresses data imbalance or hardness imbalance. We begin addressing this question by introducing class imbalance into balanced datasets and evaluate its effect on performance disparities. We oversample hard classes and undersample easy classes to bring hard classes closer to their sample complexity requirements while maintaining a constant dataset size for fairness. We estimate class-level hardness using the Area Under the Margin (AUM) hardness estimator and leverage it to compute resampling ratios. Using these ratios, we perform hardness-based resampling on the well-known CIFAR-10 and CIFAR-100 datasets. Contrary to theoretical expectations, our results show that hardness-based resampling does not meaningfully affect class-wise performance disparities. To explain this discrepancy, we conduct detailed analyses to identify key challenges unique to hardness-based imbalance, distinguishing it from traditional data-based imbalance. Our insights help explain why theoretical sample complexity expectations fail to translate into practical performance gains and we provide guidelines for future research.</li>
</ul>

<h3>Title: Context Switching for Secure Multi-programming of Near-Term Quantum Computers</h3>
<ul>
<li><strong>Authors: </strong>Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, Poulami Das</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07048">https://arxiv.org/abs/2504.07048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07048">https://arxiv.org/pdf/2504.07048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07048]] Context Switching for Secure Multi-programming of Near-Term Quantum Computers(https://arxiv.org/abs/2504.07048)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems. We identify that ZKTAs succeed because the attacker's program consistently runs with the same victim program in a fixed context. To mitigate this, we propose QONTEXTS: a context-switching technique that defends against ZKTAs by running programs across multiple contexts, each handling only a subset of trials. QONTEXTS uses multi-programming with frequent context switching while identifying a unique set of programs for each context. This helps limit only a fraction of execution to ZKTAs. We enhance QONTEXTS with attack detection capabilities that compare the distributions from different contexts against each other to identify noisy contexts executed with ZKTAs. Our evaluations on real IBMQ systems show that QONTEXTS increases program resilience by three orders of magnitude and fidelity by 1.33$\times$ on average. Moreover, QONTEXTS improves throughput by 2$\times$, advancing security in multi-programmed environments.</li>
</ul>

<h3>Title: To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Tian Qin, David Alvarez-Melis, Samy Jelassi, Eran Malach</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07052">https://arxiv.org/abs/2504.07052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07052">https://arxiv.org/pdf/2504.07052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07052]] To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning(https://arxiv.org/abs/2504.07052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test-time compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling--especially under a fixed compute budget remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models into suboptimal strategies, and (2) explicit CoT supervision can discourage "implicit" (non-verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm.</li>
</ul>

<h3>Title: TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Liang-Hsuan Tseng, Yi-Chang Chen, Kuan-Yi Lee, Da-Shan Shiu, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07053">https://arxiv.org/abs/2504.07053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07053">https://arxiv.org/pdf/2504.07053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07053]] TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling(https://arxiv.org/abs/2504.07053)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in text-based natural language processing tasks but remain constrained by their reliance on textual inputs and outputs. To enable more natural human-LLM interaction, recent progress have focused on deriving a spoken language model (SLM) that can not only listen but also generate speech. To achieve this, a promising direction is to conduct speech-text joint modeling. However, recent SLM still lag behind text LLM due to the modality mismatch. One significant mismatch can be the sequence lengths between speech and text tokens. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through the special aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. Furthermore, by leveraging TASTE, we can adapt text-based LLMs into effective SLMs with parameter-efficient fine-tuning techniques such as Low-Rank Adaptation (LoRA). Experimental results on benchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based SLMs perform similarly to previous full-finetuning methods. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and models are publicly available at this https URL.</li>
</ul>

<h3>Title: HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification</h3>
<ul>
<li><strong>Authors: </strong>Bibek Paudel, Alexander Lyzhov, Preetam Joshi, Puneet Anand</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07069">https://arxiv.org/abs/2504.07069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07069">https://arxiv.org/pdf/2504.07069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07069]] HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification(https://arxiv.org/abs/2504.07069)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a comprehensive system for detecting hallucinations in large language model (LLM) outputs in enterprise settings. We present a novel taxonomy of LLM responses specific to hallucination in enterprise applications, categorizing them into context-based, common knowledge, enterprise-specific, and innocuous statements. Our hallucination detection model HDM-2 validates LLM responses with respect to both context and generally known facts (common knowledge). It provides both hallucination scores and word-level annotations, enabling precise identification of problematic content. To evaluate it on context-based and common-knowledge hallucinations, we introduce a new dataset HDMBench. Experimental results demonstrate that HDM-2 out-performs existing approaches across RagTruth, TruthfulQA, and HDMBench datasets. This work addresses the specific challenges of enterprise deployment, including computational efficiency, domain specialization, and fine-grained error identification. Our evaluation dataset, model weights, and inference code are publicly available.</li>
</ul>

<h3>Title: A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhouhang Xie, Junda Wu, Yiran Shen, Yu Xia, Xintong Li, Aaron Chang, Ryan Rossi, Sachin Kumar, Bodhisattwa Prasad Majumder, Jingbo Shang, Prithviraj Ammanabrolu, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07070">https://arxiv.org/abs/2504.07070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07070">https://arxiv.org/pdf/2504.07070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07070]] A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models(https://arxiv.org/abs/2504.07070)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalized preference alignment for large language models (LLMs), the process of tailoring LLMs to individual users' preferences, is an emerging research direction spanning the area of NLP and personalization. In this survey, we present an analysis of works on personalized alignment and modeling for LLMs. We introduce a taxonomy of preference alignment techniques, including training time, inference time, and additionally, user-modeling based methods. We provide analysis and discussion on the strengths and limitations of each group of techniques and then cover evaluation, benchmarks, as well as open problems in the field.</li>
</ul>

<h3>Title: Detecting AI-generated Artwork</h3>
<ul>
<li><strong>Authors: </strong>Meien Li, Mark Stamp</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07078">https://arxiv.org/abs/2504.07078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07078">https://arxiv.org/pdf/2504.07078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07078]] Detecting AI-generated Artwork(https://arxiv.org/abs/2504.07078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The high efficiency and quality of artwork generated by Artificial Intelligence (AI) has created new concerns and challenges for human artists. In particular, recent improvements in generative AI have made it difficult for people to distinguish between human-generated and AI-generated art. In this research, we consider the potential utility of various types of Machine Learning (ML) and Deep Learning (DL) models in distinguishing AI-generated artwork from human-generated artwork. We focus on three challenging artistic styles, namely, baroque, cubism, and expressionism. The learning models we test are Logistic Regression (LR), Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental results yield a multiclass accuracy of 0.8208 over six classes, and an impressive accuracy of 0.9758 for the binary classification problem of distinguishing AI-generated from human-generated art.</li>
</ul>

<h3>Title: DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Atharva Pandey, Kshitij Dubey, Rahul Sharma, Amit Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07080">https://arxiv.org/abs/2504.07080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07080">https://arxiv.org/pdf/2504.07080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07080]] DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning(https://arxiv.org/abs/2504.07080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math when presented with novel problems outside standard benchmarks. Going beyond final accuracy, we propose a deductive consistency metric to analyze chain-of-thought output from language models (LMs).Formally, deductive reasoning involves two subtasks: understanding a set of input premises and inferring the conclusions that follow from them. The proposed metric studies LMs' performance on these subtasks, with the goal of explaining LMs' reasoning errors on novel problems: how well do LMs understand input premises with increasing context lengths, and how well can they infer conclusions over multiple reasoning hops? Since existing benchmarks may be memorized, we develop a pipeline to evaluate LMs' deductive consistency on novel, perturbed versions of benchmark problems. On novel grade school math problems (GSM-8k), we find that LMs are fairly robust to increasing number of input premises, but suffer significant accuracy decay as the number of reasoning hops is increased. Interestingly, these errors are masked in the original benchmark as all models achieve near 100% accuracy. As we increase the number of solution steps using a synthetic dataset, prediction over multiple hops still remains the major source of error compared to understanding input premises. Other factors, such as shifts in language style or natural propagation of early errors do not explain the trends. Our analysis provides a new view to characterize LM reasoning -- as computations over a window of input premises and reasoning hops -- that can provide unified evaluation across problem domains.</li>
</ul>

<h3>Title: GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</h3>
<ul>
<li><strong>Authors: </strong>Mengchen Zhang, Tong Wu, Jing Tan, Ziwei Liu, Gordon Wetzstein, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07083">https://arxiv.org/abs/2504.07083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07083">https://arxiv.org/pdf/2504.07083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07083]] GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography(https://arxiv.org/abs/2504.07083)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: this https URL.</li>
</ul>

<h3>Title: Identifying Unknown Stochastic Dynamics via Finite expression methods</h3>
<ul>
<li><strong>Authors: </strong>Senwei Liang, Chunmei Wang, Xingjian Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07085">https://arxiv.org/abs/2504.07085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07085">https://arxiv.org/pdf/2504.07085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07085]] Identifying Unknown Stochastic Dynamics via Finite expression methods(https://arxiv.org/abs/2504.07085)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Modeling stochastic differential equations (SDEs) is crucial for understanding complex dynamical systems in various scientific fields. Recent methods often employ neural network-based models, which typically represent SDEs through a combination of deterministic and stochastic terms. However, these models usually lack interpretability and have difficulty generalizing beyond their training domain. This paper introduces the Finite Expression Method (FEX), a symbolic learning approach designed to derive interpretable mathematical representations of the deterministic component of SDEs. For the stochastic component, we integrate FEX with advanced generative modeling techniques to provide a comprehensive representation of SDEs. The numerical experiments on linear, nonlinear, and multidimensional SDEs demonstrate that FEX generalizes well beyond the training domain and delivers more accurate long-term predictions compared to neural network-based methods. The symbolic expressions identified by FEX not only improve prediction accuracy but also offer valuable scientific insights into the underlying dynamics of the systems, paving the way for new scientific discoveries.</li>
</ul>

<h3>Title: A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility</h3>
<ul>
<li><strong>Authors: </strong>Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, Matthias Bethge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07086">https://arxiv.org/abs/2504.07086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07086">https://arxiv.org/pdf/2504.07086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07086]] A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility(https://arxiv.org/abs/2504.07086)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.</li>
</ul>

<h3>Title: KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Elan Markowitz, Krupa Galiya, Greg Ver Steeg, Aram Galstyan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07087">https://arxiv.org/abs/2504.07087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07087">https://arxiv.org/pdf/2504.07087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07087]] KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs(https://arxiv.org/abs/2504.07087)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graphs have emerged as a popular method for injecting up-to-date, factual knowledge into large language models (LLMs). This is typically achieved by converting the knowledge graph into text that the LLM can process in context. While multiple methods of encoding knowledge graphs have been proposed, the impact of this textualization process on LLM performance remains under-explored. We introduce KG-LLM-Bench, a comprehensive and extensible benchmark spanning five knowledge graph understanding tasks, and evaluate how different encoding strategies affect performance across various base models. Our extensive experiments with seven language models and five textualization strategies provide insights for optimizing LLM performance on KG reasoning tasks.</li>
</ul>

<h3>Title: Are We Done with Object-Centric Learning?</h3>
<ul>
<li><strong>Authors: </strong>Alexander Rubinstein, Ameya Prabhu, Matthias Bethge, Seong Joon Oh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07092">https://arxiv.org/abs/2504.07092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07092">https://arxiv.org/pdf/2504.07092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07092]] Are We Done with Object-Centric Learning?(https://arxiv.org/abs/2504.07092)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called $\textbf{Object-Centric Classification with Applied Masks (OCCAM)}$, demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available $\href{this https URL}{here}$.</li>
</ul>

<h3>Title: Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Shivakumar Nayak, Krishnateja Killamsetty, Ligong Han, Abhishek Bhandwaldar, Prateek Chanda, Kai Xu, Hao Wang, Aldo Pareja, Oleg Silkin, Mustafa Eyceoz, Akash Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07097">https://arxiv.org/abs/2504.07097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07097">https://arxiv.org/pdf/2504.07097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07097]] Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning(https://arxiv.org/abs/2504.07097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
