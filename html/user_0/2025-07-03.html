<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-03</h1>
<h3>Title: A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques</h3>
<ul>
<li><strong>Authors: </strong>Mohammed K. Alzaylaee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01018">https://arxiv.org/abs/2507.01018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01018">https://arxiv.org/pdf/2507.01018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01018]] A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques(https://arxiv.org/abs/2507.01018)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Smart homes that integrate Internet of Things (IoT) devices face increasing cybersecurity risks, posing significant challenges to these environments. The study explores security threats in smart homes ecosystems, categorizing them into vulnerabilities at the network layer, device level, and those from cloud-based and AI-driven systems. Research findings indicate that post-quantum encryption, coupled with AI-driven anomaly detection, is highly effective in enhancing security; however, computational resource demands present significant challenges. Blockchain authentication together with zero-trust structures builds security resilience, although they need changes to existing infrastructure. The specific security strategies show their effectiveness through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack sufficient scalability according to the results. The research demonstrates the requirement for improvement in cryptographic techniques, alongside AI-enhanced threat detection and adaptive security models which must achieve a balance between performance and efficiency and real-time applicability within smart home ecosystems.</li>
</ul>

<h3>Title: MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered</h3>
<ul>
<li><strong>Authors: </strong>Imran Mirza, Cole Huang, Ishwara Vasista, Rohan Patil, Asli Akalin, Sean O'Brien, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01019">https://arxiv.org/abs/2507.01019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01019">https://arxiv.org/pdf/2507.01019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01019]] MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered(https://arxiv.org/abs/2507.01019)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems, which consist of multiple AI models interacting within a shared environment, are increasingly used for persona-based interactions. However, if not carefully designed, these systems can reinforce implicit biases in large language models (LLMs), raising concerns about fairness and equitable representation. We present MALIBU, a novel benchmark developed to assess the degree to which LLM-based multi-agent systems implicitly reinforce social biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems through scenario-based assessments. AI models complete tasks within predefined contexts, and their responses undergo evaluation by an LLM-based multi-agent judging system in two phases. In the first phase, judges score responses labeled with specific demographic personas (e.g., gender, race, religion) across four metrics. In the second phase, judges compare paired responses assigned to different personas, scoring them and selecting the superior response. Our study quantifies biases in LLM-generated outputs, revealing that bias mitigation may favor marginalized personas over true neutrality, emphasizing the need for nuanced detection, balanced fairness strategies, and transparent evaluation benchmarks in multi-agent systems.</li>
</ul>

<h3>Title: AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aashray Reddy, Andrew Zagula, Nicholas Saban</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01020">https://arxiv.org/abs/2507.01020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01020">https://arxiv.org/pdf/2507.01020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01020]] AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models(https://arxiv.org/abs/2507.01020)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) continue to exhibit vulnerabilities to jailbreaking attacks: carefully crafted malicious inputs intended to circumvent safety guardrails and elicit harmful responses. As such, we present AutoAdv, a novel framework that automates adversarial prompt generation to systematically evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach leverages a parametric attacker LLM to produce semantically disguised malicious prompts through strategic rewriting techniques, specialized system prompts, and optimized hyperparameter configurations. The primary contribution of our work is a dynamic, multi-turn attack methodology that analyzes failed jailbreak attempts and iteratively generates refined follow-up prompts, leveraging techniques such as roleplaying, misdirection, and contextual manipulation. We quantitatively evaluate attack success rate (ASR) using the StrongREJECT (arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns. Through extensive empirical evaluation of state-of-the-art models--including ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our automated attacks achieving jailbreak success rates of up to 86% for harmful content generation. Our findings reveal that current safety mechanisms remain susceptible to sophisticated multi-turn attacks, emphasizing the urgent need for more robust defense strategies.</li>
</ul>

<h3>Title: Few-Shot Inspired Generative Zero-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Md Shakil Ahamed Shohag, Q. M. Jonathan Wu, Farhad Pourpanah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01026">https://arxiv.org/abs/2507.01026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01026">https://arxiv.org/pdf/2507.01026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01026]] Few-Shot Inspired Generative Zero-Shot Learning(https://arxiv.org/abs/2507.01026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative zero-shot learning (ZSL) methods typically synthesize visual features for unseen classes using predefined semantic attributes, followed by training a fully supervised classification model. While effective, these methods require substantial computational resources and extensive synthetic data, thereby relaxing the original ZSL assumptions. In this paper, we propose FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis. Our key insight is that class-level attributes exhibit instance-level variability, i.e., some attributes may be absent or partially visible, yet conventional ZSL methods treat them as uniformly present. To address this, we introduce Model-Specific Attribute Scoring (MSAS), which dynamically re-scores class attributes based on model-specific optimization to approximate instance-level variability without access to unseen data. We further estimate group-level prototypes as clusters of instances based on MSAS-adjusted attribute scores, which serve as representative synthetic features for each unseen class. To mitigate the resulting data imbalance, we introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training a semantic-aware contrastive classifier (SCC) using these prototypes. Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves competitive performance using far fewer synthetic features.</li>
</ul>

<h3>Title: DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization</h3>
<ul>
<li><strong>Authors: </strong>Zijian Ye, Wei Huang, Yifei Yu, Tianhe Ren, Zhongrui Wang, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01027">https://arxiv.org/abs/2507.01027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01027">https://arxiv.org/pdf/2507.01027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01027]] DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization(https://arxiv.org/abs/2507.01027)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable performance but face substantial computational and memory challenges that limit their practical deployment. Quantization has emerged as a promising solution; however, its effectiveness is often limited by quantization errors arising from weight distributions that are not quantization-friendly and the presence of activation outliers. To address these challenges, we introduce DBellQuant, an innovative post-training quantization (PTQ) framework that achieves nearly 1-bit weight compression and 6-bit activation quantization with minimal performance degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB) algorithm, which transforms single-bell weight distributions into dual-bell forms to reduce binarization errors and applies inverse transformations to smooth activations. DBellQuant sets a new state-of-the-art by preserving superior model performance under aggressive weight and activation quantization. For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of 14.39 on LLaMA2-13B with 6-bit activation quantization, significantly outperforming BiLLM's 21.35 without activation quantization, underscoring its potential in compressing LLMs for real-world applications.</li>
</ul>

<h3>Title: PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhou, Yingli Zuo, Shichang Feng, Peng Wan, Qi Zhu, Daoqiang Zhang, Wei Shao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01029">https://arxiv.org/abs/2507.01029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01029">https://arxiv.org/pdf/2507.01029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01029]] PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning(https://arxiv.org/abs/2507.01029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>With the development of generative artificial intelligence and instruction tuning techniques, multimodal large language models (MLLMs) have made impressive progress on general reasoning tasks. Benefiting from the chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning problem step-by-step. However, existing MLLMs still face significant challenges when applied to pathology visual reasoning tasks: (1) LLMs often underperforms because they lack domain-specific information, which can lead to model hallucinations. (2) The additional reasoning steps in CoT may introduce errors, leading to the divergence of answers. To address these limitations, we propose PathCoT, a novel zero-shot CoT prompting method which integrates the pathology expert-knowledge into the reasoning process of MLLMs and incorporates self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides the MLLM with prior knowledge to perform as pathology experts, and provides comprehensive analysis of the image with their domain-specific knowledge. By incorporating the experts' knowledge, PathCoT can obtain the answers with CoT reasoning. Furthermore, PathCoT incorporates a self-evaluation step that assesses both the results generated directly by MLLMs and those derived through CoT, finally determining the reliable answer. The experimental results on the PathMMU dataset demonstrate the effectiveness of our method on pathology visual understanding and reasoning.</li>
</ul>

<h3>Title: An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks</h3>
<ul>
<li><strong>Authors: </strong>Nan Mu, Hongbo Yang, Chen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01032">https://arxiv.org/abs/2507.01032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01032">https://arxiv.org/pdf/2507.01032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01032]] An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks(https://arxiv.org/abs/2507.01032)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Background and Objective: High-throughput multi-omics technologies have proven invaluable for elucidating disease mechanisms and enabling early diagnosis. However, the high cost of multi-omics profiling imposes a significant economic burden, with over reliance on full omics data potentially leading to unnecessary resource consumption. To address these issues, we propose an uncertainty-aware, multi-view dynamic decision framework for omics data classification that aims to achieve high diagnostic accuracy while minimizing testing costs. Methodology: At the single-omics level, we refine the activation functions of neural networks to generate Dirichlet distribution parameters, utilizing subjective logic to quantify both the belief masses and uncertainty mass of classification results. Belief mass reflects the support of a specific omics modality for a disease class, while the uncertainty parameter captures limitations in data quality and model discriminability, providing a more trustworthy basis for decision-making. At the multi omics level, we employ a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous modalities, leveraging their complementarity to boost diagnostic accuracy and robustness. A dynamic decision mechanism is then applied that omics data are incrementally introduced for each patient until either all data sources are utilized or the model confidence exceeds a predefined threshold, potentially before all data sources are utilized. Results and Conclusion: We evaluate our approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN. In three datasets, over 50% of cases achieved accurate classification using a single omics modality, effectively reducing redundant testing. Meanwhile, our method maintains diagnostic performance comparable to full-omics models and preserves essential biological insights.</li>
</ul>

<h3>Title: Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya</h3>
<ul>
<li><strong>Authors: </strong>Asma Agaal, Mansour Essgaer, Hend M. Farkash, Zulaiha Ali Othman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01034">https://arxiv.org/abs/2507.01034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01034">https://arxiv.org/pdf/2507.01034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01034]] Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya(https://arxiv.org/abs/2507.01034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate electricity forecasting is crucial for grid stability and energy planning, especially in Benghazi, Libya, where frequent load shedding, generation deficits, and infrastructure limitations persist. This study proposes a data-driven approach to forecast electricity load, generation, and deficits for 2025 using historical data from 2019 (a year marked by instability) and 2023 (a more stable year). Multiple time series models were applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural networks. The dataset was enhanced through missing value imputation, outlier smoothing, and log transformation. Performance was assessed using mean squared error, root mean squared error, mean absolute error, and mean absolute percentage error. LSTM outperformed all other models, showing strong capabilities in modeling non-stationary and seasonal patterns. A key contribution of this work is an optimized LSTM framework that integrates exogenous factors such as temperature and humidity, offering robust performance in forecasting multiple electricity indicators. These results provide practical insights for policymakers and grid operators to enable proactive load management and resource planning in data-scarce, volatile regions.</li>
</ul>

<h3>Title: Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems</h3>
<ul>
<li><strong>Authors: </strong>Yushang Zhao, Haotian Lyu, Yike Peng, Aijia Sun, Feng Jiang, Xinyue Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01035">https://arxiv.org/abs/2507.01035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01035">https://arxiv.org/pdf/2507.01035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01035]] Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems(https://arxiv.org/abs/2507.01035)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>The incessant advent of online services demands high speed and efficient recommender systems (ReS) that can maintain real-time performance along with processing very complex user-item interactions. The present study, therefore, considers computational bottlenecks involved in hybrid Graph Neural Network (GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their inference latency and training efficiency. An extensive methodology was used: hybrid GNN-LLM integrated architecture-optimization strategies(quantization, LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2. Experimental improvements were significant, with the optimal Hybrid + FPGA + DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms of latency, while LoRA brought down training time by 66% (3.8 hours) in comparison to the non-optimized baseline. Irrespective of domain, such as accuracy or efficiency, it can be established that hardware-software co-design and parameter-efficient tuning permit hybrid models to outperform GNN or LLM approaches implemented independently. It recommends the use of FPGA as well as LoRA for real-time deployment. Future work should involve federated learning along with advanced fusion architectures for better scalability and privacy preservation. Thus, this research marks the fundamental groundwork concerning next-generation ReS balancing low-latency response with cutting-edge personalization.</li>
</ul>

<h3>Title: Data Classification with Dynamically Growing and Shrinking Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Szymon Świderski, Agnieszka Jastrzębska</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01043">https://arxiv.org/abs/2507.01043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01043">https://arxiv.org/pdf/2507.01043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01043]] Data Classification with Dynamically Growing and Shrinking Neural Networks(https://arxiv.org/abs/2507.01043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The issue of data-driven neural network model construction is one of the core problems in the domain of Artificial Intelligence. A standard approach assumes a fixed architecture with trainable weights. A conceptually more advanced assumption is that we not only train the weights, but also find out the optimal model architecture. We present a new method that realizes just that. This article is an extended version of our conference paper titled "Dynamic Growing and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the paper, we show in detail how to create a neural network with a procedure that allows dynamic shrinking and growing of the model while it is being trained. The decision-making mechanism for the architectural design is governed by a Monte Carlo tree search procedure which simulates network behavior and allows to compare several candidate architecture changes to choose the best one. The proposed method was validated using both visual and time series datasets, demonstrating its particular effectiveness in multivariate time series classification. This is attributed to the architecture's ability to adapt dynamically, allowing independent modifications for each time series. The approach is supplemented by Python source code for reproducibility. Experimental evaluations in visual pattern and multivariate time series classification tasks revealed highly promising performance, underscoring the method's robustness and adaptability.</li>
</ul>

<h3>Title: Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals</h3>
<ul>
<li><strong>Authors: </strong>Xiao Gu, Wei Tang, Jinpei Han, Veer Sangha, Fenglin Liu, Shreyank N Gowda, Antonio H. Ribeiro, Patrick Schwab, Kim Branson, Lei Clifton, Antonio Luiz P. Ribeiro, Zhangdaihong Liu, David A. Clifton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01045">https://arxiv.org/abs/2507.01045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01045">https://arxiv.org/pdf/2507.01045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01045]] Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals(https://arxiv.org/abs/2507.01045)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms (PPG), are of paramount importance for the diagnosis, prevention, and management of cardiovascular diseases, and have been extensively used in a variety of clinical tasks. Conventional deep learning approaches for analyzing these signals typically rely on homogeneous datasets and static bespoke models, limiting their robustness and generalizability across diverse clinical settings and acquisition protocols. In this study, we present a cardiac sensing foundation model (CSFM) that leverages advanced transformer architectures and a generative, masked pretraining strategy to learn unified representations from vast, heterogeneous health records. Our model is pretrained on an innovative multi-modal integration of data from multiple large-scale datasets (including MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the corresponding clinical or machine-generated text reports from approximately 1.7 million individuals. We demonstrate that the embeddings derived from our CSFM not only serve as effective feature extractors across diverse cardiac sensing scenarios, but also enable seamless transfer learning across varying input configurations and sensor modalities. Extensive evaluations across diagnostic tasks, demographic information recognition, vital sign measurement, clinical outcome prediction, and ECG question answering reveal that CSFM consistently outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits robust performance across multiple ECG lead configurations from standard 12-lead systems to single-lead setups, and in scenarios where only ECG, only PPG, or a combination thereof is available. These findings highlight the potential of CSFM as a versatile and scalable solution, for comprehensive cardiac monitoring.</li>
</ul>

<h3>Title: Variational Digital Twins</h3>
<ul>
<li><strong>Authors: </strong>Logan A. Burnett, Umme Mahbuba Nabila, Majdi I. Radaideh</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01047">https://arxiv.org/abs/2507.01047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01047">https://arxiv.org/pdf/2507.01047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01047]] Variational Digital Twins(https://arxiv.org/abs/2507.01047)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While digital twins (DT) hold promise for providing real-time insights into complex energy assets, much of the current literature either does not offer a clear framework for information exchange between the model and the asset, lacks key features needed for real-time implementation, or gives limited attention to model uncertainty. Here, we aim to solve these gaps by proposing a variational digital twin (VDT) framework that augments standard neural architectures with a single Bayesian output layer. This lightweight addition, along with a novel VDT updating algorithm, lets a twin update in seconds on commodity GPUs while producing calibrated uncertainty bounds that can inform experiment design, control algorithms, and model reliability. The VDT is evaluated on four energy-sector problems. For critical-heat-flux prediction, uncertainty-driven active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third the training time of random sampling. A three-year renewable-generation twin maintains R2 > 0.95 for solar output and curbs error growth for volatile wind forecasts via monthly updates that process only one month of data at a time. A nuclear reactor transient cooldown twin reconstructs thermocouple signals with R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating robustness to degraded instrumentation. Finally, a physics-informed Li-ion battery twin, retrained after every ten discharges, lowers voltage mean-squared error by an order of magnitude relative to the best static model while adapting its credible intervals as the cell approaches end-of-life. These results demonstrate that combining modest Bayesian augmentation with efficient update schemes turns conventional surrogates into uncertainty-aware, data-efficient, and computationally tractable DTs, paving the way for dependable models across industrial and scientific energy systems.</li>
</ul>

<h3>Title: 3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Emanuel Vaz Vargas, Afrânio José de Melo Junior, Celso José Munaro, Cláudio Benevenuto de Campos Lima, Eduardo Toledo de Lima Junior, Felipe Muntzberg Barrocas, Flávio Miguel Varejão, Guilherme Fidelis Peixer, Igor de Melo Nery Oliveira, Jader Riso Barbosa Jr., Jaime Andrés Lozano Cadena, Jean Carlos Dias de Araújo, João Neuenschwander Escosteguy Carneiro, Lucas Gouveia Omena Lopes, Lucas Pereira de Gouveia, Mateus de Araujo Fernandes, Matheus Lima Scramignon, Patrick Marques Ciarelli, Rodrigo Castello Branco, Rogério Leite Alves Pinto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01048">https://arxiv.org/abs/2507.01048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01048">https://arxiv.org/pdf/2507.01048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01048]] 3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells(https://arxiv.org/abs/2507.01048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the oil industry, undesirable events in oil wells can cause economic losses, environmental accidents, and human casualties. Solutions based on Artificial Intelligence and Machine Learning for Early Detection of such events have proven valuable for diverse applications across industries. In 2019, recognizing the importance and the lack of public datasets related to undesirable events in oil wells, Petrobras developed and publicly released the first version of the 3W Dataset, which is essentially a set of Multivariate Time Series labeled by experts. Since then, the 3W Dataset has been developed collaboratively and has become a foundational reference for numerous works in the field. This data article describes the current publicly available version of the 3W Dataset, which contains structural modifications and additional labeled data. The detailed description provided encourages and supports the 3W community and new 3W users to improve previous published results and to develop new robust methodologies, digital products and services capable of detecting undesirable events in oil wells with enough anticipation to enable corrective or mitigating actions.</li>
</ul>

<h3>Title: Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization</h3>
<ul>
<li><strong>Authors: </strong>Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01050">https://arxiv.org/abs/2507.01050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01050">https://arxiv.org/pdf/2507.01050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01050]] Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization(https://arxiv.org/abs/2507.01050)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: this https URL</li>
</ul>

<h3>Title: Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Farooq</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01052">https://arxiv.org/abs/2507.01052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01052">https://arxiv.org/pdf/2507.01052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01052]] Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals(https://arxiv.org/abs/2507.01052)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this study we introduce a novel energy functional for long-sequence memory, building upon the framework of dense Hopfield networks which achieves exponential storage capacity through higher-order interactions. Building upon earlier work on long-sequence Hopfield memory models, we propose a temporal kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient sequential retrieval of patterns over extended sequences. We demonstrate the successful application of this technique for the storage and sequential retrieval of movies frames which are well suited for this because of the high dimensional vectors that make up each frame creating enough variation between even sequential frames in the high dimensional space. The technique has applications in modern transformer architectures, including efficient long-sequence modeling, memory augmentation, improved attention with temporal bias, and enhanced handling of long-term dependencies in time-series data. Our model offers a promising approach to address the limitations of transformers in long-context tasks, with potential implications for natural language processing, forecasting, and beyond.</li>
</ul>

<h3>Title: Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors</h3>
<ul>
<li><strong>Authors: </strong>Biplov Paneru</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01068">https://arxiv.org/abs/2507.01068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01068">https://arxiv.org/pdf/2507.01068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01068]] Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors(https://arxiv.org/abs/2507.01068)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, interpretability</a></li>
<li><strong>Abstract: </strong>This study leverages an Inertial Measurement Unit (IMU) dataset to develop explainable AI methods for the early detection and prediction of Freezing of Gait (FOG), a common symptom in Parkinson's disease. Machine learning models, including CatBoost, XGBoost, and Extra Trees classifiers, are employed to accurately categorize FOG episodes based on relevant clinical features. A Stacking Ensemble model achieves superior performance, surpassing a hybrid bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP interpretability analysis reveals that time (seconds) is the most influential factor in distinguishing gait patterns. Additionally, the proposed FOG prediction framework incorporates federated learning, where models are trained locally on individual devices and aggregated on a central server using a federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for enhanced predictive capability.</li>
</ul>

<h3>Title: Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs</h3>
<ul>
<li><strong>Authors: </strong>Dian Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01073">https://arxiv.org/abs/2507.01073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01073">https://arxiv.org/pdf/2507.01073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01073]] Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs(https://arxiv.org/abs/2507.01073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have achieved remarkable success in molecular property prediction. However, traditional graph representations struggle to effectively encode the inherent 3D spatial structures of molecules, as molecular orientations in 3D space introduce significant variability, severely limiting model generalization and robustness. Existing approaches primarily focus on rotation-invariant and rotation-equivariant methods. Invariant methods often rely heavily on prior knowledge and lack sufficient generalizability, while equivariant methods suffer from high computational costs. To address these limitations, this paper proposes a novel plug-and-play 3D encoding module leveraging rotational sampling. By computing the expectation over the SO(3) rotational group, the method naturally achieves approximate rotational invariance. Furthermore, by introducing a carefully designed post-alignment strategy, strict invariance can be achieved without compromising performance. Experimental evaluations on the QM9 and C10 Datasets demonstrate superior predictive accuracy, robustness, and generalization performance compared to existing methods. Moreover, the proposed approach maintains low computational complexity and enhanced interpretability, providing a promising direction for efficient and effective handling of 3D molecular information in drug discovery and material design.</li>
</ul>

<h3>Title: Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Bogdan, Arina Cazacu, Laura Vasilie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01077">https://arxiv.org/abs/2507.01077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01077">https://arxiv.org/pdf/2507.01077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01077]] Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels(https://arxiv.org/abs/2507.01077)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Anomaly detection often relies on supervised or clustering approaches, with limited success in specialized domains like automotive communication systems where scalable solutions are essential. We propose a novel decoder-only Large Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU) communication logs. Our approach addresses two key challenges: the lack of LLMs tailored for ECU communication and the complexity of inconsistent ground truth data. By learning from UDP communication logs, we formulate anomaly detection simply as identifying deviations in time from normal behavior. We introduce an entropy regularization technique that increases model's uncertainty in known anomalies while maintaining consistency in similar scenarios. Our solution offers three novelties: a decoder-only anomaly detection architecture, a way to handle inconsistent labeling, and an adaptable LLM for different ECU communication use cases. By leveraging the generative capabilities of decoder-only models, we present a new technique that addresses the high cost and error-prone nature of manual labeling through a more scalable system that is able to learn from a minimal set of examples, while improving detection accuracy in complex communication environments.</li>
</ul>

<h3>Title: yProv4ML: Effortless Provenance Tracking for Machine Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Gabriele Padovani, Valentine Anantharaj, Sandro Fiore</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01078">https://arxiv.org/abs/2507.01078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01078">https://arxiv.org/pdf/2507.01078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01078]] yProv4ML: Effortless Provenance Tracking for Machine Learning Systems(https://arxiv.org/abs/2507.01078)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of interest in large language models (LLMs) reflects their potential for flexibility and generalization, and attracted the attention of a diverse range of researchers. However, the advent of these techniques has also brought to light the lack of transparency and rigor with which development is pursued. In particular, the inability to determine the number of epochs and other hyperparameters in advance presents challenges in identifying the best model. To address this challenge, machine learning frameworks such as MLFlow can automate the collection of this type of information. However, these tools capture data using proprietary formats and pose little attention to lineage. This paper proposes yProv4ML, a framework to capture provenance information generated during machine learning processes in PROV-JSON format, with minimal code modifications.</li>
</ul>

<h3>Title: Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept</h3>
<ul>
<li><strong>Authors: </strong>Edouard Lansiaux, Ramy Azzouz, Emmanuel Chazard, Amélie Vromant, Eric Wiel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01080">https://arxiv.org/abs/2507.01080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01080">https://arxiv.org/pdf/2507.01080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01080]] Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept(https://arxiv.org/abs/2507.01080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Triage errors, including undertriage and overtriage, are persistent challenges in emergency departments (EDs). With increasing patient influx and staff shortages, the integration of artificial intelligence (AI) into triage protocols has gained attention. This study compares the performance of three AI models [Natural Language Processing (NLP), Large Language Models (LLM), and Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes against the FRENCH scale and clinical this http URL conducted a retrospective analysis of a prospectively recruited cohort gathering adult patient triage data over a 7-month period at the Roger Salengro Hospital ED (Lille, France). Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2) URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic details, verbatim chief complaints, vital signs, and triage outcomes based on the FRENCH scale and GEMSA coding. The primary outcome was the concordance of AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse triage (-4.343). Secondary analyses highlighted the effectiveness of URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness with structured data versus raw transcripts (either for GEMSA prediction or for FRENCH prediction). LLM architecture, through abstraction of patient representations, offers the most accurate triage predictions among tested models. Integrating AI into ED workflows could enhance patient safety and operational efficiency, though integration into clinical workflows requires addressing model limitations and ensuring ethical transparency.</li>
</ul>

<h3>Title: Geometry-aware 4D Video Generation for Robot Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, Shuran Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01099">https://arxiv.org/abs/2507.01099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01099">https://arxiv.org/pdf/2507.01099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01099]] Geometry-aware 4D Video Generation for Robot Manipulation(https://arxiv.org/abs/2507.01099)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding and predicting the dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. This geometric supervision enables the model to learn a shared 3D representation of the scene, allowing it to predict future video sequences from novel viewpoints based solely on the given RGB-D observations, without requiring camera poses as inputs. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints.</li>
</ul>

<h3>Title: Quasi-twisted codes: decoding and applications in code-based cryptography</h3>
<ul>
<li><strong>Authors: </strong>Bhagyalekshmy S, Rutuja Kshirsagar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01118">https://arxiv.org/abs/2507.01118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01118">https://arxiv.org/pdf/2507.01118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01118]] Quasi-twisted codes: decoding and applications in code-based cryptography(https://arxiv.org/abs/2507.01118)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Quasi-twisted (QT) codes generalize several important families of linear codes, including cyclic, constacyclic, and quasi-cyclic codes. Despite their potential, to the best of our knowledge, there exists no efficient decoding algorithm for QT codes. In this work, we propose a syndrome-based decoding method capable of efficiently correcting up to (d* - 1)/2 errors, where d* denotes an HT-like lower bound on the minimum distance of QT codes, which we formalize here. Additionally, we introduce a Niederreiter-like cryptosystem constructed from QT codes. This cryptosystem is resistant to some classical attacks as well as some quantum attacks based on Quantum Fourier Sampling.</li>
</ul>

<h3>Title: Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions</h3>
<ul>
<li><strong>Authors: </strong>Rahul A. Burange, Harsh K. Shinde, Omkar Mutyalwar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01123">https://arxiv.org/abs/2507.01123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01123">https://arxiv.org/pdf/2507.01123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01123]] Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions(https://arxiv.org/abs/2507.01123)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Landslides pose severe threats to infrastructure, economies, and human lives, necessitating accurate detection and predictive mapping across diverse geographic regions. With advancements in deep learning and remote sensing, automated landslide detection has become increasingly effective. This study presents a comprehensive approach integrating multi-source satellite imagery and deep learning models to enhance landslide identification and prediction. We leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and Digital Elevation Model (DEM) layers to capture critical environmental features influencing landslide occurrences. Various geospatial analysis techniques are employed to assess the impact of terra in characteristics, vegetation cover, and rainfall on detection accuracy. Additionally, we evaluate the performance of multiple stateof-the-art deep learning segmentation models, including U-Net, DeepLabV3+, and Res-Net, to determine their effectiveness in landslide detection. The proposed framework contributes to the development of reliable early warning systems, improved disaster risk management, and sustainable land-use planning. Our findings provide valuable insights into the potential of deep learning and multi-source remote sensing in creating robust, scalable, and transferable landslide prediction models.</li>
</ul>

<h3>Title: FlashDP: Private Training Large Language Models with Efficient DP-SGD</h3>
<ul>
<li><strong>Authors: </strong>Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01154">https://arxiv.org/abs/2507.01154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01154">https://arxiv.org/pdf/2507.01154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01154]] FlashDP: Private Training Large Language Models with Efficient DP-SGD(https://arxiv.org/abs/2507.01154)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to \textbf{50\%} but also cuts down redundant computations by \textbf{20\%}, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a \textbf{90\%} throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in this https URL.</li>
</ul>

<h3>Title: Event-based evaluation of abstractive news summarization</h3>
<ul>
<li><strong>Authors: </strong>Huiling You, Samia Touileb, Erik Velldal, Lilja Øvrelid</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01160">https://arxiv.org/abs/2507.01160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01160">https://arxiv.org/pdf/2507.01160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01160]] Event-based evaluation of abstractive news summarization(https://arxiv.org/abs/2507.01160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An abstractive summary of a news article contains its most important information in a condensed version. The evaluation of automatically generated summaries by generative language models relies heavily on human-authored summaries as gold references, by calculating overlapping units or similarity scores. News articles report events, and ideally so should the summaries. In this work, we propose to evaluate the quality of abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles. We experiment on a richly annotated Norwegian dataset comprising both events annotations and summaries authored by expert human annotators. Our approach provides more insight into the event information contained in the summaries.</li>
</ul>

<h3>Title: cp_measure: API-first feature extraction for image-based profiling workflows</h3>
<ul>
<li><strong>Authors: </strong>Alán F. Muñoz (1), Tim Treis (2),  (1), Alexandr A. Kalinin (1), Shatavisha Dasgupta (1), Fabian Theis (2), Anne E. Carpenter (1), Shantanu Singh (1) ((1) Broad Institute of MIT and Harvard, United States,(2) Institute of Computational Biology, Helmholtz Zentrum München, Germany)</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.CB, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01163">https://arxiv.org/abs/2507.01163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01163">https://arxiv.org/pdf/2507.01163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01163]] cp_measure: API-first feature extraction for image-based profiling workflows(https://arxiv.org/abs/2507.01163)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Biological image analysis has traditionally focused on measuring specific visual properties of interest for cells or other entities. A complementary paradigm gaining increasing traction is image-based profiling - quantifying many distinct visual features to form comprehensive profiles which may reveal hidden patterns in cellular states, drug responses, and disease mechanisms. While current tools like CellProfiler can generate these feature sets, they pose significant barriers to automated and reproducible analyses, hindering machine learning workflows. Here we introduce cp_measure, a Python library that extracts CellProfiler's core measurement capabilities into a modular, API-first tool designed for programmatic feature extraction. We demonstrate that cp_measure features retain high fidelity with CellProfiler features while enabling seamless integration with the scientific Python ecosystem. Through applications to 3D astrocyte imaging and spatial transcriptomics, we showcase how cp_measure enables reproducible, automated image-based profiling pipelines that scale effectively for machine learning applications in computational biology.</li>
</ul>

<h3>Title: Matching and Linking Entries in Historical Swedish Encyclopedias</h3>
<ul>
<li><strong>Authors: </strong>Simon Börjesson, Erik Ersmark, Pierre Nugues</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01170">https://arxiv.org/abs/2507.01170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01170">https://arxiv.org/pdf/2507.01170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01170]] Matching and Linking Entries in Historical Swedish Encyclopedias(https://arxiv.org/abs/2507.01170)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and 20th centuries. It was written by a team of experts and aimed to be an intellectual reference, stressing precision and accuracy. This encyclopedia had four main editions remarkable by their size, ranging from 20 to 38 volumes. As a consequence, the \textit{Nordisk familjebok} had a considerable influence in universities, schools, the media, and society overall. As new editions were released, the selection of entries and their content evolved, reflecting intellectual changes in Sweden. In this paper, we used digitized versions from \textit{Project Runeberg}. We first resegmented the raw text into entries and matched pairs of entries between the first and second editions using semantic sentence embeddings. We then extracted the geographical entries from both editions using a transformer-based classifier and linked them to Wikidata. This enabled us to identify geographic trends and possible shifts between the first and second editions, written between 1876-1899 and 1904-1926, respectively. Interpreting the results, we observe a small but significant shift in geographic focus away from Europe and towards North America, Africa, Asia, Australia, and northern Scandinavia from the first to the second edition, confirming the influence of the First World War and the rise of new powers. The code and data are available on GitHub at this https URL.</li>
</ul>

<h3>Title: Diffusion Explorer: Interactive Exploration of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Alec Helbling, Duen Horng Chau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01178">https://arxiv.org/abs/2507.01178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01178">https://arxiv.org/pdf/2507.01178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01178]] Diffusion Explorer: Interactive Exploration of Diffusion Models(https://arxiv.org/abs/2507.01178)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have been central to the development of recent image, video, and even text generation systems. They posses striking geometric properties that can be faithfully portrayed in low-dimensional settings. However, existing resources for explaining diffusion either require an advanced theoretical foundation or focus on their neural network architectures rather than their rich geometric properties. We introduce Diffusion Explorer, an interactive tool to explain the geometric properties of diffusion models. Users can train 2D diffusion models in the browser and observe the temporal dynamics of their sampling process. Diffusion Explorer leverages interactive animation, which has been shown to be a powerful tool for making engaging visualizations of dynamic systems, making it well suited to explaining diffusion models which represent stochastic processes that evolve over time. Diffusion Explorer is open source and a live demo is available at this http URL.</li>
</ul>

<h3>Title: Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform</h3>
<ul>
<li><strong>Authors: </strong>Pedro R. X. Carmo, Igor de Moura, Assis T. de Oliveira Filho, Djamel Sadok, Cleber Zanchettin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01208">https://arxiv.org/abs/2507.01208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01208">https://arxiv.org/pdf/2507.01208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01208]] Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform(https://arxiv.org/abs/2507.01208)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Modern vehicles are increasingly connected, and in this context, automotive Ethernet is one of the technologies that promise to provide the necessary infrastructure for intra-vehicle communication. However, these systems are subject to attacks that can compromise safety, including flow injection attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often designed to combat this problem, but they require expensive hardware to run in real time. In this work, we propose to evaluate and apply fast neural network inference techniques like Distilling and Prunning for deploying IDS models on low-cost platforms in real time. The results show that these techniques can achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4, with AUCROC values of 0.9890.</li>
</ul>

<h3>Title: MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Adamu Lawan, Juhua Pu, Haruna Yunusa, Jawad Muhammad, Muhammad Lawan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01213">https://arxiv.org/abs/2507.01213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01213">https://arxiv.org/pdf/2507.01213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01213]] MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis(https://arxiv.org/abs/2507.01213)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language Processing (NLP) task that extracts aspects from text and determines their associated sentiments, enabling fine-grained analysis of user opinions. Existing ABSA methods struggle to balance computational efficiency with high performance: deep learning models often lack global context, transformers demand significant computational resources, and Mamba-based approaches face CUDA dependency and diminished local correlations. Recent advancements in Extended Long Short-Term Memory (xLSTM) models, particularly their efficient modeling of long-range dependencies, have significantly advanced the NLP community. However, their potential in ABSA remains untapped. To this end, we propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework integrating a bi-directional mLSTM architecture with forward and partially flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context modeling by processing the initial sequence segment in reverse with dedicated parameters, preserving critical short-range patterns. We further introduce an mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that dynamically combines forward mLSTM outputs as query and key with PF-mLSTM outputs as value, optimizing short-range dependency capture while maintaining global context and efficiency. Experimental results on three benchmark datasets demonstrate that MEGA outperforms state-of-the-art baselines, achieving superior accuracy and efficiency in ABSA tasks.</li>
</ul>

<h3>Title: PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01216">https://arxiv.org/abs/2507.01216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01216">https://arxiv.org/pdf/2507.01216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01216]] PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning(https://arxiv.org/abs/2507.01216)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data, labels or fine-tuned models to the server. To address those issues, we develop PAE MobiLLM, a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops a one-token (i.e., ``pivot'' token) activation shortcut that transmits only a single activation dimension instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data, labels or fine-tuned models.</li>
</ul>

<h3>Title: Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW</h3>
<ul>
<li><strong>Authors: </strong>Di Zhang, Yihang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01241">https://arxiv.org/abs/2507.01241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01241">https://arxiv.org/pdf/2507.01241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01241]] Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW(https://arxiv.org/abs/2507.01241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Stochastic gradient-based descent (SGD), have long been central to training large language models (LLMs). However, their effectiveness is increasingly being questioned, particularly in large-scale applications where empirical evidence suggests potential performance limitations. In response, this paper proposes a stochastic conjugate subgradient method together with adaptive sampling tailored specifically for training LLMs. The method not only achieves faster convergence per iteration but also demonstrates improved scalability compared to traditional SGD techniques. It leverages sample complexity analysis to adaptively choose the sample size, employs a stochastic conjugate subgradient approach to determine search directions and utilizing an AdamW-like algorithm to adaptively adjust step sizes. This approach preserves the key advantages of first-order methods while effectively addressing the nonconvexity and non-smoothness inherent in LLMs training. Additionally, we provide a detailed analysis of the advantage of the algorithm. Experimental results show that the proposed method not only maintains, but in many cases surpasses, the scalability of traditional SGD techniques, significantly enhancing both the speed and accuracy of the optimization process.</li>
</ul>

<h3>Title: Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer</h3>
<ul>
<li><strong>Authors: </strong>Runze Cheng, Xihang Qiu, Ming Li, Ye Zhang, Chun Li, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01254">https://arxiv.org/abs/2507.01254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01254">https://arxiv.org/pdf/2507.01254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01254]] Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer(https://arxiv.org/abs/2507.01254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal MRI provides critical complementary information for accurate brain tumor segmentation. However, conventional methods struggle when certain modalities are missing due to issues such as image quality, protocol inconsistencies, patient allergies, or financial constraints. To address this, we propose a robust single-modality parallel processing framework that achieves high segmentation accuracy even with incomplete modalities. Leveraging Holder divergence and mutual information, our model maintains modality-specific features while dynamically adjusting network parameters based on the available inputs. By using these divergence- and information-based loss functions, the framework effectively quantifies discrepancies between predictions and ground-truth labels, resulting in consistently accurate segmentation. Extensive evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior performance over existing methods in handling missing modalities.</li>
</ul>

<h3>Title: AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01255">https://arxiv.org/abs/2507.01255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01255">https://arxiv.org/pdf/2507.01255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01255]] AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation(https://arxiv.org/abs/2507.01255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The rapid advancement of AI-generated video models has created a pressing need for robust and interpretable evaluation frameworks. Existing metrics are limited to producing numerical scores without explanatory comments, resulting in low interpretability and human evaluation alignment. To address those challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video Evaluation(AIGVE), which can provide not only numerical scores but also multi-aspect language comment feedback in evaluating these generated videos. Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising 2,500 AI-generated videos and 22,500 human-annotated detailed comments and numerical scores across nine critical evaluation aspects. Leveraging AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a novel token-wise weighted loss and a dynamic frame sampling strategy to better align with human evaluators. Comprehensive experiments across supervised and zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art performance in both scoring correlation and comment quality, significantly outperforming prior baselines including GPT-4o and VideoScore. In addition, we further showcase a multi-agent refinement framework where feedback from AIGVE-MACS drives iterative improvements in video generation, leading to 53.5% quality enhancement. This work establishes a new paradigm for comprehensive, human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2 and AIGVE-MACS at this https URL.</li>
</ul>

<h3>Title: GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant</h3>
<ul>
<li><strong>Authors: </strong>Michał Matak, Jarosław A. Chudziak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01259">https://arxiv.org/abs/2507.01259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01259">https://arxiv.org/pdf/2507.01259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01259]] GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant(https://arxiv.org/abs/2507.01259)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper we discuss the capability of large language models to base their answer and provide proper references when dealing with legal matters of non-english and non-chinese speaking country. We discuss the history of legal information retrieval, the difference between case law and statute law, its impact on the legal tasks and analyze the latest research in this field. Basing on that background we introduce gAIus, the architecture of the cognitive LLM-based agent, whose responses are based on the knowledge retrieved from certain legal act, which is Polish Civil Code. We propose a retrieval mechanism which is more explainable, human-friendly and achieves better results than embedding-based approaches. To evaluate our method we create special dataset based on single-choice questions from entrance exams for law apprenticeships conducted in Poland. The proposed architecture critically leveraged the abilities of used large language models, improving the gpt-3.5-turbo-0125 by 419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%. At the end of our paper we show the possible future path of research and potential applications of our findings.</li>
</ul>

<h3>Title: PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01271">https://arxiv.org/abs/2507.01271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01271">https://arxiv.org/pdf/2507.01271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01271]] PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning(https://arxiv.org/abs/2507.01271)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, unlearning techniques, which are methods for inducing a model to "forget" previously learned information, have attracted attention as a way to address privacy and copyright concerns in large language models (LLMs) and large multimodal models (LMMs). While several unlearning benchmarks have been established for LLMs, a practical evaluation framework for unlearning in LMMs has been less explored. Specifically, existing unlearning benchmark for LMMs considers only scenarios in which the model is required to unlearn fine-tuned knowledge through a single unlearning operation. In this study, we introduce PULSE protocol for realistic unlearning scenarios for LMMs by introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for analyzing the effect across different knowledge acquisition phases and (ii) Long-term Sustainability Evaluation to address sequential requests. We then evaluate existing unlearning methods along these dimensions. Our results reveal that, although some techniques can successfully unlearn knowledge acquired through fine-tuning, they struggle to eliminate information learned during pre-training. Moreover, methods that effectively unlearn a batch of target data in a single operation exhibit substantial performance degradation when the same data are split and unlearned sequentially.</li>
</ul>

<h3>Title: Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Chengxu Liu, Lu Qi, Jinshan Pan, Xueming Qian, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01275">https://arxiv.org/abs/2507.01275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01275">https://arxiv.org/pdf/2507.01275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01275]] Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing(https://arxiv.org/abs/2507.01275)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unpaired image dehazing has attracted increasing attention due to its flexible data requirements during model training. Dominant methods based on contrastive learning not only introduce haze-unrelated content information, but also ignore haze-specific properties in the frequency domain (\ie,~haze-related degradation is mainly manifested in the amplitude spectrum). To address these issues, we propose a novel frequency domain-based diffusion model, named \ours, for fully exploiting the beneficial knowledge in unpaired clear data. In particular, inspired by the strong generative ability shown by Diffusion Models (DMs), we tackle the dehazing task from the perspective of frequency domain reconstruction and perform the DMs to yield the amplitude spectrum consistent with the distribution of clear images. To implement it, we propose an Amplitude Residual Encoder (ARE) to extract the amplitude residuals, which effectively compensates for the amplitude gap from the hazy to clear domains, as well as provide supervision for the DMs training. In addition, we propose a Phase Correction Module (PCM) to eliminate artifacts by further refining the phase spectrum during dehazing with a simple attention mechanism. Experimental results demonstrate that our \ours outperforms other state-of-the-art methods on both synthetic and real-world datasets.</li>
</ul>

<h3>Title: Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening</h3>
<ul>
<li><strong>Authors: </strong>Cindy Lie Tabuse, David Restepo, Carolina Gracitelli, Fernando Korn Malerbi, Caio Regatieri, Luis Filipe Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01278">https://arxiv.org/abs/2507.01278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01278">https://arxiv.org/pdf/2507.01278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01278]] Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening(https://arxiv.org/abs/2507.01278)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can simulate clinical reasoning based on natural language prompts, but their utility in ophthalmology is largely unexplored. This study evaluated GPT-4's ability to interpret structured textual descriptions of retinal fundus photographs and simulate clinical decisions for diabetic retinopathy (DR) and glaucoma screening, including the impact of adding real or synthetic clinical metadata. We conducted a retrospective diagnostic validation study using 300 annotated fundus images. GPT-4 received structured prompts describing each image, with or without patient metadata. The model was tasked with assigning an ICDR severity score, recommending DR referral, and estimating the cup-to-disc ratio for glaucoma referral. Performance was evaluated using accuracy, macro and weighted F1 scores, and Cohen's kappa. McNemar's test and change rate analysis were used to assess the influence of metadata. GPT-4 showed moderate performance for ICDR classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25), driven mainly by correct identification of normal cases. Performance improved in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For glaucoma referral, performance was poor across all settings (accuracy ~78%, F1 <0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes (McNemar p > 0.05), and predictions remained consistent across conditions. GPT-4 can simulate basic ophthalmic decision-making from structured prompts but lacks precision for complex tasks. While not suitable for clinical use, LLMs may assist in education, documentation, or image annotation workflows in ophthalmology.</li>
</ul>

<h3>Title: Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization</h3>
<ul>
<li><strong>Authors: </strong>Juan Chen, Baolong Bi, Wei Zhang, Jingyan Sui, Xiaofei Zhu, Yuanzhuo Wang, Lingrui Mei, Shenghua Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01281">https://arxiv.org/abs/2507.01281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01281">https://arxiv.org/pdf/2507.01281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01281]] Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization(https://arxiv.org/abs/2507.01281)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating their parametric knowledge with external retrieved content. However, knowledge conflicts caused by internal inconsistencies or noisy retrieved content can severely undermine the generation reliability of RAG this http URL this work, we argue that LLMs should rethink all evidence, including both retrieved content and internal knowledge, before generating this http URL propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel framework that improves trustworthiness through Conflict-Driven Summarization of all available this http URL-RAG first derives parameter-aware evidence by comparing parameter records to identify diverse internal perspectives. It then refines retrieved evidences to produce context-aware evidence, removing irrelevant or misleading content. To detect and summarize conflicts, we distill a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable synthesis across multiple this http URL further ensure evaluation integrity, we introduce a QA Repair step to correct outdated or ambiguous benchmark this http URL on revised QA datasets with retrieval data show that CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios with noisy or conflicting evidence.</li>
</ul>

<h3>Title: Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01285">https://arxiv.org/abs/2507.01285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01285">https://arxiv.org/pdf/2507.01285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01285]] Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation(https://arxiv.org/abs/2507.01285)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>Graph federated recommendation systems offer a privacy-preserving alternative to traditional centralized recommendation architectures, which often raise concerns about data security. While federated learning enables personalized recommendations without exposing raw user data, existing aggregation methods overlook the unique properties of user embeddings in this setting. Indeed, traditional aggregation methods fail to account for their complexity and the critical role of user similarity in recommendation effectiveness. Moreover, evolving user interactions require adaptive aggregation while preserving the influence of high-relevance anchor users (the primary users before expansion in graph-based frameworks). To address these limitations, we introduce Dist-FedAvg, a novel distance-based aggregation method designed to enhance personalization and aggregation efficiency in graph federated learning. Our method assigns higher aggregation weights to users with similar embeddings, while ensuring that anchor users retain significant influence in local updates. Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg consistently outperforms baseline aggregation techniques, improving recommendation accuracy while maintaining seamless integration into existing federated learning frameworks.</li>
</ul>

<h3>Title: Learning an Ensemble Token from Task-driven Priors in Facial Analysis</h3>
<ul>
<li><strong>Authors: </strong>Sunyong Seo, Semin Kim, Jongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01290">https://arxiv.org/abs/2507.01290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01290">https://arxiv.org/pdf/2507.01290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01290]] Learning an Ensemble Token from Task-driven Priors in Facial Analysis(https://arxiv.org/abs/2507.01290)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Facial analysis exhibits task-specific feature variations. While Convolutional Neural Networks (CNNs) have enabled the fine-grained representation of spatial information, Vision Transformers (ViTs) have facilitated the representation of semantic information at the patch level. Although the generalization of conventional methodologies has advanced visual interpretability, there remains paucity of research that preserves the unified feature representation on single task learning during the training process. In this work, we introduce ET-Fuser, a novel methodology for learning ensemble token by leveraging attention mechanisms based on task priors derived from pre-trained models for facial analysis. Specifically, we propose a robust prior unification learning method that generates a ensemble token within a self-attention mechanism, which shares the mutual information along the pre-trained encoders. This ensemble token approach offers high efficiency with negligible computational cost. Our results show improvements across a variety of facial analysis, with statistically significant enhancements observed in the feature representations.</li>
</ul>

<h3>Title: La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation</h3>
<ul>
<li><strong>Authors: </strong>Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01299">https://arxiv.org/abs/2507.01299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01299">https://arxiv.org/pdf/2507.01299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01299]] La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation(https://arxiv.org/abs/2507.01299)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.</li>
</ul>

<h3>Title: DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Worameth Chinchuthakun, Pakkapon Phongthawee, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01305">https://arxiv.org/abs/2507.01305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01305">https://arxiv.org/pdf/2507.01305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01305]] DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting(https://arxiv.org/abs/2507.01305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a simple yet effective technique for estimating lighting from a single low-dynamic-range (LDR) image by reframing the task as a chrome ball inpainting problem. This approach leverages a pre-trained diffusion model, Stable Diffusion XL, to overcome the generalization failures of existing methods that rely on limited HDR panorama datasets. While conceptually simple, the task remains challenging because diffusion models often insert incorrect or inconsistent content and cannot readily generate chrome balls in HDR format. Our analysis reveals that the inpainting process is highly sensitive to the initial noise in the diffusion process, occasionally resulting in unrealistic outputs. To address this, we first introduce DiffusionLight, which uses iterative inpainting to compute a median chrome ball from multiple outputs to serve as a stable, low-frequency lighting prior that guides the generation of a high-quality final result. To generate high-dynamic-range (HDR) light probes, an Exposure LoRA is fine-tuned to create LDR images at multiple exposure values, which are then merged. While effective, DiffusionLight is time-intensive, requiring approximately 30 minutes per estimation. To reduce this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to about 30 seconds with minimal quality loss. This 60x speedup is achieved by training a Turbo LoRA to directly predict the averaged chrome balls from the iterative process. Inference is further streamlined into a single denoising pass using a LoRA swapping technique. Experimental results that show our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios. Our code is available at this https URL</li>
</ul>

<h3>Title: ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zhiyao Ren, Siyuan Liang, Aishan Liu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01321">https://arxiv.org/abs/2507.01321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01321">https://arxiv.org/pdf/2507.01321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01321]] ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks(https://arxiv.org/abs/2507.01321)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has demonstrated remarkable success in large language models (LLMs) due to its adaptability and parameter-free nature. However, it also introduces a critical vulnerability to backdoor attacks, where adversaries can manipulate LLM behaviors by simply poisoning a few ICL demonstrations. In this paper, we propose, for the first time, the dual-learning hypothesis, which posits that LLMs simultaneously learn both the task-relevant latent concepts and backdoor latent concepts within poisoned demonstrations, jointly influencing the probability of model outputs. Through theoretical analysis, we derive an upper bound for ICL backdoor effects, revealing that the vulnerability is dominated by the concept preference ratio between the task and the backdoor. Motivated by these findings, we propose ICLShield, a defense mechanism that dynamically adjusts the concept preference ratio. Our method encourages LLMs to select clean demonstrations during the ICL phase by leveraging confidence and similarity scores, effectively mitigating susceptibility to backdoor attacks. Extensive experiments across multiple LLMs and tasks demonstrate that our method achieves state-of-the-art defense effectiveness, significantly outperforming existing approaches (+26.02% on average). Furthermore, our method exhibits exceptional adaptability and defensive performance even for closed-source models (e.g., GPT-4).</li>
</ul>

<h3>Title: Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Jiansong Chen, Ke Zeng, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01327">https://arxiv.org/abs/2507.01327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01327">https://arxiv.org/pdf/2507.01327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01327]] Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy(https://arxiv.org/abs/2507.01327)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Detecting abnormal events in real-world customer service dialogues is highly challenging due to the complexity of business data and the dynamic nature of customer interactions. Moreover, models must demonstrate strong out-of-domain (OOD) generalization to enable rapid adaptation across different business scenarios and maximize commercial value. In this work, we propose a novel Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that leverages the advanced reasoning capabilities of large language models for abnormal event detection. APARL introduces a dual-loop dynamic curriculum learning architecture, enabling the model to progressively focus on more challenging samples as its proficiency increases. This design effectively addresses performance bottlenecks and significantly enhances OOD transferability. Extensive evaluations on food delivery dialogue tasks show that our model achieves significantly enhanced adaptability and robustness, attaining the highest F1 score with an average improvement of 17.19\%, and an average improvement of 9.59\% in OOD transfer tests. This method provides a superior solution for industrial deployment of anomaly detection models, contributing to improved operational efficiency and commercial benefits.</li>
</ul>

<h3>Title: Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nifu Dan, Yujun Cai, Yiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01334">https://arxiv.org/abs/2507.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01334">https://arxiv.org/pdf/2507.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01334]] Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs(https://arxiv.org/abs/2507.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains.</li>
</ul>

<h3>Title: Physics-informed Ground Reaction Dynamics from Human Motion Capture</h3>
<ul>
<li><strong>Authors: </strong>Cuong Le, Huy-Phuong Le, Duc Le, Minh-Thien Duong, Van-Binh Nguyen, My-Ha Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01340">https://arxiv.org/abs/2507.01340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01340">https://arxiv.org/pdf/2507.01340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01340]] Physics-informed Ground Reaction Dynamics from Human Motion Capture(https://arxiv.org/abs/2507.01340)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Body dynamics are crucial information for the analysis of human motions in important research fields, ranging from biomechanics, sports science to computer vision and graphics. Modern approaches collect the body dynamics, external reactive force specifically, via force plates, synchronizing with human motion capture data, and learn to estimate the dynamics from a black-box deep learning model. Being specialized devices, force plates can only be installed in laboratory setups, imposing a significant limitation on the learning of human dynamics. To this end, we propose a novel method for estimating human ground reaction dynamics directly from the more reliable motion capture data with physics laws and computational simulation as constrains. We introduce a highly accurate and robust method for computing ground reaction forces from motion capture data using Euler's integration scheme and PD algorithm. The physics-based reactive forces are used to inform the learning model about the physics-informed motion dynamics thus improving the estimation accuracy. The proposed approach was tested on the GroundLink dataset, outperforming the baseline model on: 1) the ground reaction force estimation accuracy compared to the force plates measurement; and 2) our simulated root trajectory precision. The implementation code is available at this https URL</li>
</ul>

<h3>Title: Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation</h3>
<ul>
<li><strong>Authors: </strong>Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01347">https://arxiv.org/abs/2507.01347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01347">https://arxiv.org/pdf/2507.01347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01347]] Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation(https://arxiv.org/abs/2507.01347)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce Generalized Test-Time Augmentation (GTTA), a highly effective method for improving the performance of a trained model, which unlike other existing Test-Time Augmentation approaches from the literature is general enough to be used off-the-shelf for many vision and non-vision tasks, such as classification, regression, image segmentation and object detection. By applying a new general data transformation, that randomly perturbs multiple times the PCA subspace projection of a test input, GTTA forms robust ensembles at test time in which, due to sound statistical properties, the structural and systematic noises in the initial input data is filtered out and final estimator errors are reduced. Different from other existing methods, we also propose a final self-supervised learning stage in which the ensemble output, acting as an unsupervised teacher, is used to train the initial single student model, thus reducing significantly the test time computational cost, at no loss in accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on various vision and non-vision well-known datasets and tasks, such as image classification and segmentation, speech recognition and house price prediction, validate the generality of the proposed GTTA. Furthermore, we also prove its effectiveness on the more specific real-world task of salmon segmentation and detection in low-visibility underwater videos, for which we introduce DeepSalmon, the largest dataset of its kind in the literature.</li>
</ul>

<h3>Title: Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy</h3>
<ul>
<li><strong>Authors: </strong>Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01352">https://arxiv.org/abs/2507.01352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01352">https://arxiv.org/pdf/2507.01352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01352]] Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy(https://arxiv.org/abs/2507.01352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.</li>
</ul>

<h3>Title: Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chugang Yi, Minghan Yu, Weikang Qian, Yixin Wen, Haizhao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01354">https://arxiv.org/abs/2507.01354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01354">https://arxiv.org/pdf/2507.01354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01354]] Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion(https://arxiv.org/abs/2507.01354)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Effective hydrological modeling and extreme weather analysis demand precipitation data at a kilometer-scale resolution, which is significantly finer than the 10 km scale offered by standard global products like IMERG. To address this, we propose the Wavelet Diffusion Model (WDM), a generative framework that achieves 10x spatial super-resolution (downscaling to 1 km) and delivers a 9x inference speedup over pixel-based diffusion models. WDM is a conditional diffusion model that learns the learns the complex structure of precipitation from MRMS radar data directly in the wavelet domain. By focusing on high-frequency wavelet coefficients, it generates exceptionally realistic and detailed 1-km precipitation fields. This wavelet-based approach produces visually superior results with fewer artifacts than pixel-space models, and delivers a significant gains in sampling efficiency. Our results demonstrate that WDM provides a robust solution to the dual challenges of accuracy and speed in geoscience super-resolution, paving the way for more reliable hydrological forecasts.</li>
</ul>

<h3>Title: 3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01367">https://arxiv.org/abs/2507.01367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01367">https://arxiv.org/pdf/2507.01367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01367]] 3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation(https://arxiv.org/abs/2507.01367)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:this https URL.</li>
</ul>

<h3>Title: Activation Reward Models for Few-Shot Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Tianning Chai, Chancharik Mitra, Brandon Huang, Gautam Rajendrakumar Gare, Zhiqiu Lin, Assaf Arbelle, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Deva Ramanan, Roei Herzig</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01368">https://arxiv.org/abs/2507.01368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01368">https://arxiv.org/pdf/2507.01368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01368]] Activation Reward Models for Few-Shot Model Alignment(https://arxiv.org/abs/2507.01368)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to human preferences is a central challenge in improving the quality of the models' generative outputs for real-world applications. A common approach is to use reward modeling to encode preferences, enabling alignment via post-training using reinforcement learning. However, traditional reward modeling is not easily adaptable to new preferences because it requires a separate reward model, commonly trained on large preference datasets. To address this, we introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward modeling method that leverages activation steering to construct well-aligned reward signals using minimal supervision and no additional model finetuning. Activation RMs outperform existing few-shot reward modeling approaches such as LLM-as-a-judge with in-context learning, voting-based scoring, and token probability scoring on standard reward modeling benchmarks. Furthermore, we demonstrate the effectiveness of Activation RMs in mitigating reward hacking behaviors, highlighting their utility for safety-critical applications. Toward this end, we propose PreferenceHack, a novel few-shot setting benchmark, the first to test reward models on reward hacking in a paired preference format. Finally, we show that Activation RM achieves state-of-the-art performance on this benchmark, surpassing even GPT-4o.</li>
</ul>

<h3>Title: Distributional Soft Actor-Critic with Diffusion Policy</h3>
<ul>
<li><strong>Authors: </strong>Tong Liu, Yinuo Wang, Xujie Song, Wenjun Zou, Liangfa Chen, Likun Wang, Bin Shuai, Jingliang Duan, Shengbo Eben Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01381">https://arxiv.org/abs/2507.01381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01381">https://arxiv.org/pdf/2507.01381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01381]] Distributional Soft Actor-Critic with Diffusion Policy(https://arxiv.org/abs/2507.01381)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning has been proven to be highly effective in handling complex control tasks. Traditional methods typically use unimodal distributions, such as Gaussian distributions, to model the output of value distributions. However, unimodal distribution often and easily causes bias in value function estimation, leading to poor algorithm performance. This paper proposes a distributional reinforcement learning algorithm called DSAC-D (Distributed Soft Actor Critic with Diffusion Policy) to address the challenges of estimating bias in value functions and obtaining multimodal policy representations. A multimodal distributional policy iteration framework that can converge to the optimal policy was established by introducing policy entropy and value distribution function. A diffusion value network that can accurately characterize the distribution of multi peaks was constructed by generating a set of reward samples through reverse sampling using a diffusion model. Based on this, a distributional reinforcement learning algorithm with dual diffusion of the value network and the policy network was derived. MuJoCo testing tasks demonstrate that the proposed algorithm not only learns multimodal policy, but also achieves state-of-the-art (SOTA) performance in all 9 control tasks, with significant suppression of estimation bias and total average return improvement of over 10\% compared to existing mainstream algorithms. The results of real vehicle testing show that DSAC-D can accurately characterize the multimodal distribution of different driving styles, and the diffusion policy network can characterize multimodal trajectories.</li>
</ul>

<h3>Title: CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning</h3>
<ul>
<li><strong>Authors: </strong>Kuniaki Saito, Donghyun Kim, Kwanyong Park, Atsushi Hashimoto, Yoshitaka Ushiku</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01409">https://arxiv.org/abs/2507.01409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01409">https://arxiv.org/pdf/2507.01409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01409]] CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning(https://arxiv.org/abs/2507.01409)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An image captioning model flexibly switching its language pattern, e.g., descriptiveness and length, should be useful since it can be applied to diverse applications. However, despite the dramatic improvement in generative vision-language models, fine-grained control over the properties of generated captions is not easy due to two reasons: (i) existing models are not given the properties as a condition during training and (ii) existing models cannot smoothly transition its language pattern from one state to the other. Given this challenge, we propose a new approach, CaptionSmiths, to acquire a single captioning model that can handle diverse language patterns. First, our approach quantifies three properties of each caption, length, descriptiveness, and uniqueness of a word, as continuous scalar values, without human annotation. Given the values, we represent the conditioning via interpolation between two endpoint vectors corresponding to the extreme states, e.g., one for a very short caption and one for a very long caption. Empirical results demonstrate that the resulting model can smoothly change the properties of the output captions and show higher lexical alignment than baselines. For instance, CaptionSmiths reduces the error in controlling caption length by 506\% despite better lexical alignment. Code will be available on this https URL.</li>
</ul>

<h3>Title: Decomposing Prediction Mechanisms for In-Context Recall</h3>
<ul>
<li><strong>Authors: </strong>Sultan Daniels, Dylan Davis, Dhruv Gautam, Wentinn Liao, Gireeja Ranade, Anant Sahai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01414">https://arxiv.org/abs/2507.01414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01414">https://arxiv.org/pdf/2507.01414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01414]] Decomposing Prediction Mechanisms for In-Context Recall(https://arxiv.org/abs/2507.01414)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce a new family of toy problems that combine features of linear-regression-style continuous in-context learning (ICL) with discrete associative recall. We pretrain transformer models on sample traces from this toy, specifically symbolically-labeled interleaved state observations from randomly drawn linear deterministic dynamical systems. We study if the transformer models can recall the state of a sequence previously seen in its context when prompted to do so with the corresponding in-context label. Taking a closer look at this task, it becomes clear that the model must perform two functions: (1) identify which system's state should be recalled and apply that system to its last seen state, and (2) continuing to apply the correct system to predict the subsequent states. Training dynamics reveal that the first capability emerges well into a model's training. Surprisingly, the second capability, of continuing the prediction of a resumed sequence, develops much earlier. Via out-of-distribution experiments, and a mechanistic analysis on model weights via edge pruning, we find that next-token prediction for this toy problem involves at least two separate mechanisms. One mechanism uses the discrete symbolic labels to do the associative recall required to predict the start of a resumption of a previously seen sequence. The second mechanism, which is largely agnostic to the discrete symbolic labels, performs a "Bayesian-style" prediction based on the previous token and the context. These two mechanisms have different learning dynamics. To confirm that this multi-mechanism (manifesting as separate phase transitions) phenomenon is not just an artifact of our toy setting, we used OLMo training checkpoints on an ICL translation task to see a similar phenomenon: a decisive gap in the emergence of first-task-token performance vs second-task-token performance.</li>
</ul>

<h3>Title: Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Gu, Ziyue Qiao, Zechao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01417">https://arxiv.org/abs/2507.01417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01417">https://arxiv.org/pdf/2507.01417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01417]] Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention(https://arxiv.org/abs/2507.01417)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-Distribution (OOD) detection is critical for safely deploying deep models in open-world environments, where inputs may lie outside the training distribution. During inference on a model trained exclusively with In-Distribution (ID) data, we observe a salient gradient phenomenon: around an ID sample, the local gradient directions for "enhancing" that sample's predicted class remain relatively consistent, whereas OOD samples--unseen in training--exhibit disorganized or conflicting gradient directions in the same neighborhood. Motivated by this observation, we propose an inference-stage technique to short-circuit those feature coordinates that spurious gradients exploit to inflate OOD confidence, while leaving ID classification largely intact. To circumvent the expense of recomputing the logits after this gradient short-circuit, we further introduce a local first-order approximation that accurately captures the post-modification outputs without a second forward pass. Experiments on standard OOD benchmarks show our approach yields substantial improvements. Moreover, the method is lightweight and requires minimal changes to the standard inference pipeline, offering a practical path toward robust OOD detection in real-world applications.</li>
</ul>

<h3>Title: DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Liu, Bingshu Wang, Ze Wang, C.L. Philip Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01422">https://arxiv.org/abs/2507.01422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01422">https://arxiv.org/pdf/2507.01422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01422]] DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal(https://arxiv.org/abs/2507.01422)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed method's superiority over state-of-the-art. Our code and dataset will be publicly available.</li>
</ul>

<h3>Title: A Compact 16-bit S-box over Tower Field $\F_{(((2^2)^2)^2)^2}$ with High Security</h3>
<ul>
<li><strong>Authors: </strong>Bahram Rashidi, Behrooz Khadem</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01423">https://arxiv.org/abs/2507.01423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01423">https://arxiv.org/pdf/2507.01423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01423]] A Compact 16-bit S-box over Tower Field $\F_{(((2^2)^2)^2)^2}$ with High Security(https://arxiv.org/abs/2507.01423)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a compact and secure 16-bit substitution box (S-box) designed over the composite field $\F_{(((2^2)^2)^2)^2}$, optimized for both hardware efficiency and cryptographic robustness. The proposed S-box decomposes operations into subfields, leveraging a tower field architecture. This enables significant hardware reduction through optimized field inversion and a low-cost affine transformation. Security evaluations confirm resilience against linear, differential, algebraic and DPA attacks, validated via metrics including Nonlinearity (32512), Differential Uniformity (4), Algebraic Degree (15), Transparency order (15.9875) and SNR (0.34e-08). The hardware results, in 65 nm CMOS technology, show the proposed 16-bit S-box has lower hardware resources consumption and lower critical path delay (CPD) than those of other 16-bit S-boxes. By integrating high algebraic complexity with resource-efficient structures, this work addresses the growing demand for scalable cryptographic primitives in data-sensitive applications, demonstrating that larger S-boxes can enhance security without proportional hardware costs. The results underscore the viability of composite field-based architectures in balancing security and efficiency for modern block ciphers.</li>
</ul>

<h3>Title: DiffMark: Diffusion-based Robust Watermark Against Deepfakes</h3>
<ul>
<li><strong>Authors: </strong>Chen Sun, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Liejun Wang, Dan Ma, Gaobo Yang, Keqin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01428">https://arxiv.org/abs/2507.01428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01428">https://arxiv.org/pdf/2507.01428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01428]] DiffMark: Diffusion-based Robust Watermark Against Deepfakes(https://arxiv.org/abs/2507.01428)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Deepfakes pose significant security and privacy threats through malicious facial manipulations. While robust watermarking can aid in authenticity verification and source tracking, existing methods often lack the sufficient robustness against Deepfake manipulations. Diffusion models have demonstrated remarkable performance in image generation, enabling the seamless fusion of watermark with image during generation. In this study, we propose a novel robust watermarking framework based on diffusion model, called DiffMark. By modifying the training and sampling scheme, we take the facial image and watermark as conditions to guide the diffusion model to progressively denoise and generate corresponding watermarked image. In the construction of facial condition, we weight the facial image by a timestep-dependent factor that gradually reduces the guidance intensity with the decrease of noise, thus better adapting to the sampling process of diffusion model. To achieve the fusion of watermark condition, we introduce a cross information fusion (CIF) module that leverages a learnable embedding table to adaptively extract watermark features and integrates them with image features via cross-attention. To enhance the robustness of the watermark against Deepfake manipulations, we integrate a frozen autoencoder during training phase to simulate Deepfake manipulations. Additionally, we introduce Deepfake-resistant guidance that employs specific Deepfake model to adversarially guide the diffusion sampling process to generate more robust watermarked images. Experimental results demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ting Xu, Xiaoxiao Deng, Xiandong Meng, Haifeng Yang, Yan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01437">https://arxiv.org/abs/2507.01437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01437">https://arxiv.org/pdf/2507.01437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01437]] Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction(https://arxiv.org/abs/2507.01437)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenges posed by the unstructured nature and high-dimensional semantic complexity of electronic health record texts. A deep learning method based on attention mechanisms is proposed to achieve unified modeling for information extraction and multi-label disease prediction. The study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is used to perform representation learning over clinical text. Multi-layer self-attention mechanisms are employed to capture key medical entities and their contextual relationships. A Sigmoid-based multi-label classifier is then applied to predict multiple disease labels. The model incorporates a context-aware semantic alignment mechanism, enhancing its representational capacity in typical medical scenarios such as label co-occurrence and sparse information. To comprehensively evaluate model performance, a series of experiments were conducted, including baseline comparisons, hyperparameter sensitivity analysis, data perturbation studies, and noise injection tests. Results demonstrate that the proposed method consistently outperforms representative existing approaches across multiple performance metrics. The model maintains strong generalization under varying data scales, interference levels, and model depth configurations. The framework developed in this study offers an efficient algorithmic foundation for processing real-world clinical texts and presents practical significance for multi-label medical text modeling tasks.</li>
</ul>

<h3>Title: TurboReg: TurboClique for Robust and Efficient Point Cloud Registration</h3>
<ul>
<li><strong>Authors: </strong>Shaocheng Yan, Pengcheng Shi, Zhenjun Zhao, Kaixin Wang, Kuang Cao, Ji Wu, Jiayuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01439">https://arxiv.org/abs/2507.01439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01439">https://arxiv.org/pdf/2507.01439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01439]] TurboReg: TurboClique for Robust and Efficient Point Cloud Registration(https://arxiv.org/abs/2507.01439)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust estimation is essential in correspondence-based Point Cloud Registration (PCR). Existing methods using maximal clique search in compatibility graphs achieve high recall but suffer from exponential time complexity, limiting their use in time-sensitive applications. To address this challenge, we propose a fast and robust estimator, TurboReg, built upon a novel lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a highly-constrained compatibility graph. The lightweight nature of the 3-clique allows for efficient parallel searching, and the highly-constrained compatibility graph ensures robust spatial consistency for stable transformation estimation. Next, PGS selects matching pairs with high SC$^2$ scores as pivots, effectively guiding the search toward TurboCliques with higher inlier ratios. Moreover, the PGS algorithm has linear time complexity and is significantly more efficient than the maximal clique search with exponential time complexity. Extensive experiments show that TurboReg achieves state-of-the-art performance across multiple real-world datasets, with substantial speed improvements. For example, on the 3DMatch+FCGF dataset, TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving higher recall. Our code is accessible at \href{this https URL}{\texttt{TurboReg}}.</li>
</ul>

<h3>Title: OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yuxing Liu, Ji Zhang, Zhou Xuchuan, Jingzhong Xiao, Huimin Yang, Jiaxin Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01455">https://arxiv.org/abs/2507.01455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01455">https://arxiv.org/pdf/2507.01455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01455]] OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes(https://arxiv.org/abs/2507.01455)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous objects within images. Existing pixel-wise methods typically assign anomaly scores individually and employ a global thresholding strategy to segment anomalies. Despite their effectiveness, these approaches encounter significant challenges in real-world applications: (1) neglecting spatial correlations among pixels within the same object, resulting in fragmented segmentation; (2) variabil ity in anomaly score distributions across image regions, causing global thresholds to either generate false positives in background areas or miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel multi-level anomaly segmentation framework designed to address these limitations through a coarse-to-fine anomaly detection strategy. OoDDINO combines an uncertainty-guided anomaly detection model with a pixel-level segmentation model within a two-stage cascade architecture. Initially, we propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that sequentially integrates multiple uncertainty metrics with visual representations, employing orthogonal constraints to strengthen the detection model's capacity for localizing anomalous regions accurately. Subsequently, we develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically generates region-specific thresholds based on object-level detection outputs and pixel-wise anomaly scores. This approach allows for distinct thresholding strategies within foreground and background areas, achieving fine-grained anomaly segmentation. The proposed framework is compatible with other pixel-wise anomaly detection models, which acts as a plug-in to boost the performance. Extensive experiments on two benchmark datasets validate our framework's superiority and compatibility over state-of-the-art methods.</li>
</ul>

<h3>Title: NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Max Gandyra, Alessandro Santonicola, Michael Beetz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01463">https://arxiv.org/abs/2507.01463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01463">https://arxiv.org/pdf/2507.01463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01463]] NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation(https://arxiv.org/abs/2507.01463)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed, for all kinds of novel objects, without (re-) training, has proven to be a difficult task. To handle this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). This work stems from and improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also leverages on recent vision foundation models, namely: Grounded-SAM 2 and DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise bounding boxes and their corresponding segmentation masks; while DINOv2's zero-shot capabilities are employed to generate the image embeddings. The quality of those masks, together with their embeddings, is of vital importance to our approach; as the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings. Differently to SAM-6D, calculating the latter involves a prior patch filtering based on the distance between each patch and its corresponding cyclic/roundtrip patch in the image grid. Furthermore, the average confidence of the proposals' bounding box and mask is used as an additional weighting factor for the object matching score. We empirically show that NOCTIS, without further training/fine tuning, outperforms the best RGB and RGB-D methods on the seven core datasets of the BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects" task.</li>
</ul>

<h3>Title: A new efficient RPKI Design</h3>
<ul>
<li><strong>Authors: </strong>Haya Schulmann, Niklas Vogel</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01465">https://arxiv.org/abs/2507.01465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01465">https://arxiv.org/pdf/2507.01465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01465]] A new efficient RPKI Design(https://arxiv.org/abs/2507.01465)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Resource Public Key Infrastructure (RPKI) is a critical security mechanism for BGP, but the complexity of its architecture is a growing concern as its adoption scales. Current RPKI design heavily reuses legacy PKI components, such as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols, all these introduce excessive cryptographic validation, redundant metadata, and inefficiencies in both storage and processing. We show that these design choices, although based on established standards, create significant performance bottlenecks, increase the vulnerability surface, and hinder scalability for wide-scale Internet deployment. In this paper, we perform the first systematic analysis of the root causes of complexity in RPKI's design and experimentally quantify their real-world impact. We show that over 70% of validation time in RPKI relying parties is spent on certificate parsing and signature verification, much of it unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI), a backwards-compatible redesign that preserves all security guarantees while substantially reducing protocol overhead. iRPKI eliminates EE-certificates and ROA signatures, merges revocation and integrity objects, replaces verbose encodings with Protobuf, and restructures repository metadata for more efficient access. We experimentally demonstrate that our implementation of iRPKI in the Routinator validator achieves a 20x speed-up of processing time, 18x improvement of bandwidth requirements and 8x reduction in cache memory footprint, while also eliminating classes of vulnerabilities that have led to at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the feasibility of deploying RPKI at scale in the Internet, and especially in constrained environments. Our design may be deployed incrementally without impacting existing operations.</li>
</ul>

<h3>Title: Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think</h3>
<ul>
<li><strong>Authors: </strong>Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, Ming-Ming Cheng, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01467">https://arxiv.org/abs/2507.01467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01467">https://arxiv.org/pdf/2507.01467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01467]] Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think(https://arxiv.org/abs/2507.01467)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called Representation Entanglement for Generation (REG), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at: this https URL.</li>
</ul>

<h3>Title: Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects</h3>
<ul>
<li><strong>Authors: </strong>Chentao Shen, Ding Pan, Mingyu Mei, Zaixing He, Xinyue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01478">https://arxiv.org/abs/2507.01478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01478">https://arxiv.org/pdf/2507.01478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01478]] Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects(https://arxiv.org/abs/2507.01478)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual pose tracking is playing an increasingly vital role in industrial contexts in recent years. However, the pose tracking for industrial metal objects remains a challenging task especially in the real world-environments, due to the reflection characteristic of metal objects. To address this issue, we propose a novel 6DoF pose tracking method based on active control points. The method uses image control points to generate edge feature for optimization actively instead of 6DoF pose-based rendering, and serve them as optimization variables. We also introduce an optimal control point regression method to improve robustness. The proposed tracking method performs effectively in both dataset evaluation and real world tasks, providing a viable solution for real-time tracking of industrial metal objects. Our source code is made publicly available at: this https URL.</li>
</ul>

<h3>Title: Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities</h3>
<ul>
<li><strong>Authors: </strong>Yingqiang Gao, Kaede Johnson, David Froehlich, Luisa Carrer, Sarah Ebling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01479">https://arxiv.org/abs/2507.01479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01479">https://arxiv.org/pdf/2507.01479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01479]] Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities(https://arxiv.org/abs/2507.01479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Automatic text simplification (ATS) aims to enhance language accessibility for various target groups, particularly persons with intellectual disabilities. Recent advancements in generative AI, especially large language models (LLMs), have substantially improved the quality of machine-generated text simplifications, thereby mitigating information barriers for the target group. However, existing LLM-based ATS systems do not incorporate preference feedback on text simplifications during training, resulting in a lack of personalization tailored to the specific needs of target group representatives. In this work, we extend the standard supervised fine-tuning (SFT) approach for adapting LLM-based ATS models by leveraging a computationally efficient LLM alignment technique -- direct preference optimization (DPO). Specifically, we post-train LLM-based ATS models using human feedback collected from persons with intellectual disabilities, reflecting their preferences on paired text simplifications generated by mainstream LLMs. Furthermore, we propose a pipeline for developing personalized LLM-based ATS systems, encompassing data collection, model selection, SFT and DPO post-training, and evaluation. Our findings underscore the necessity of active participation of target group persons in designing personalized AI accessibility solutions aligned with human expectations. This work represents a step towards personalizing inclusive AI systems at the target-group level, incorporating insights not only from text simplification experts but also from target group persons themselves.</li>
</ul>

<h3>Title: What Really Matters for Robust Multi-Sensor HD Map Construction?</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshuai Hao, Yuting Zhao, Yuheng Ji, Luanyuan Dai, Peng Hao, Dingzhe Li, Shuai Cheng, Rong Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01484">https://arxiv.org/abs/2507.01484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01484">https://arxiv.org/pdf/2507.01484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01484]] What Really Matters for Robust Multi-Sensor HD Map Construction?(https://arxiv.org/abs/2507.01484)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>High-definition (HD) map construction methods are crucial for providing precise and comprehensive static environmental information, which is essential for autonomous driving systems. While Camera-LiDAR fusion techniques have shown promising results by integrating data from both modalities, existing approaches primarily focus on improving model accuracy and often neglect the robustness of perception models, which is a critical aspect for real-world applications. In this paper, we explore strategies to enhance the robustness of multi-modal fusion methods for HD map construction while maintaining high accuracy. We propose three key components: data augmentation, a novel multi-modal fusion module, and a modality dropout training strategy. These components are evaluated on a challenging dataset containing 10 days of NuScenes data. Our experimental results demonstrate that our proposed methods significantly enhance the robustness of baseline methods. Furthermore, our approach achieves state-of-the-art performance on the clean validation set of the NuScenes dataset. Our findings provide valuable insights for developing more robust and reliable HD map construction models, advancing their applicability in real-world autonomous driving scenarios. Project website: this https URL.</li>
</ul>

<h3>Title: How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations</h3>
<ul>
<li><strong>Authors: </strong>Marc Damie, Florian Hahn, Andreas Peter, Jan Ramon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01487">https://arxiv.org/abs/2507.01487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01487">https://arxiv.org/pdf/2507.01487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01487]] How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations(https://arxiv.org/abs/2507.01487)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building block for private data aggregation. Recently, the field of differential privacy has revived interest in secure shufflers by highlighting the privacy amplification they can provide in various computations. Although several works argue for the utility of secure shufflers, they often treat them as black boxes; overlooking the practical vulnerabilities and performance trade-offs of existing implementations. This leaves a central question open: what makes a good secure shuffler? This survey addresses that question by identifying, categorizing, and comparing 26 secure protocols that realize the necessary shuffling functionality. To enable a meaningful comparison, we adapt and unify existing security definitions into a consistent set of properties. We also present an overview of privacy-preserving technologies that rely on secure shufflers, offer practical guidelines for selecting appropriate protocols, and outline promising directions for future work.</li>
</ul>

<h3>Title: AVC-DPO: Aligned Video Captioning via Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiyang Tang, Hengyi Li, Yifan Du, Wayne Xin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01492">https://arxiv.org/abs/2507.01492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01492">https://arxiv.org/pdf/2507.01492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01492]] AVC-DPO: Aligned Video Captioning via Direct Preference Optimization(https://arxiv.org/abs/2507.01492)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although video multimodal large language models (video MLLMs) have achieved substantial progress in video captioning tasks, it remains challenging to adjust the focal emphasis of video captions according to human preferences. To address this limitation, we propose Aligned Video Captioning via Direct Preference Optimization (AVC-DPO), a post-training framework designed to enhance captioning capabilities in video MLLMs through preference alignment. Our approach designs enhanced prompts that specifically target temporal dynamics and spatial information-two key factors that humans care about when watching a video-thereby incorporating human-centric preferences. AVC-DPO leverages the same foundation model's caption generation responses under varied prompt conditions to conduct preference-aware training and caption alignment. Using this framework, we have achieved exceptional performance in the LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving first place on the Video Detailed Captioning (VDC) benchmark according to the VDCSCORE evaluation metric.</li>
</ul>

<h3>Title: Crop Pest Classification Using Deep Learning Techniques: A Review</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Hassam Ejaz, Muhammad Bilal, Usman Habib</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01494">https://arxiv.org/abs/2507.01494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01494">https://arxiv.org/pdf/2507.01494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01494]] Crop Pest Classification Using Deep Learning Techniques: A Review(https://arxiv.org/abs/2507.01494)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Insect pests continue to bring a serious threat to crop yields around the world, and traditional methods for monitoring them are often slow, manual, and difficult to scale. In recent years, deep learning has emerged as a powerful solution, with techniques like convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid models gaining popularity for automating pest detection. This review looks at 37 carefully selected studies published between 2018 and 2025, all focused on AI-based pest classification. The selected research is organized by crop type, pest species, model architecture, dataset usage, and key technical challenges. The early studies relied heavily on CNNs but latest work is shifting toward hybrid and transformer-based models that deliver higher accuracy and better contextual understanding. Still, challenges like imbalanced datasets, difficulty in detecting small pests, limited generalizability, and deployment on edge devices remain significant hurdles. Overall, this review offers a structured overview of the field, highlights useful datasets, and outlines the key challenges and future directions for AI-based pest monitoring systems.</li>
</ul>

<h3>Title: ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jimyeong Kim, Jungwon Park, Yeji Song, Nojun Kwak, Wonjong Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01496">https://arxiv.org/abs/2507.01496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01496">https://arxiv.org/pdf/2507.01496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01496]] ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation(https://arxiv.org/abs/2507.01496)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Rectified Flow text-to-image models surpass diffusion models in image quality and text alignment, but adapting ReFlow for real-image editing remains challenging. We propose a new real-image editing method for ReFlow by analyzing the intermediate representations of multimodal transformer blocks and identifying three key features. To extract these features from real images with sufficient structural preservation, we leverage mid-step latent, which is inverted only up to the mid-step. We then adapt attention during injection to improve editability and enhance alignment to the target text. Our method is training-free, requires no user-provided mask, and can be applied even without a source prompt. Extensive experiments on two benchmarks with nine baselines demonstrate its superior performance over prior methods, further validated by human evaluations confirming a strong user preference for our approach.</li>
</ul>

<h3>Title: Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images</h3>
<ul>
<li><strong>Authors: </strong>Ozan Durgut, Beril Kallfelz-Sirmacek, Cem Unsalan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01502">https://arxiv.org/abs/2507.01502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01502">https://arxiv.org/pdf/2507.01502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01502]] Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images(https://arxiv.org/abs/2507.01502)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Global warming, loss of biodiversity, and air pollution are among the most significant problems facing Earth. One of the primary challenges in addressing these issues is the lack of monitoring forests to protect them. To tackle this problem, it is important to leverage remote sensing and computer vision methods to automate monitoring applications. Hence, automatic tree crown detection algorithms emerged based on traditional and deep learning methods. In this study, we first introduce two different tree crown detection methods based on these approaches. Then, we form a novel rule-based approach that integrates these two methods to enhance robustness and accuracy of tree crown detection results. While traditional methods are employed for feature extraction and segmentation of forested areas, deep learning methods are used to detect tree crowns in our method. With the proposed rule-based approach, we post-process these results, aiming to increase the number of detected tree crowns through neighboring trees and localized operations. We compare the obtained results with the proposed method in terms of the number of detected tree crowns and report the advantages, disadvantages, and areas for improvement of the obtained outcomes.</li>
</ul>

<h3>Title: Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Robert Aufschläger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01504">https://arxiv.org/abs/2507.01504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01504">https://arxiv.org/pdf/2507.01504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01504]] Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence(https://arxiv.org/abs/2507.01504)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, biometric</a></li>
<li><strong>Abstract: </strong>The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at this https URL.</li>
</ul>

<h3>Title: Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tapas K. Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01509">https://arxiv.org/abs/2507.01509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01509">https://arxiv.org/pdf/2507.01509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01509]] Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation(https://arxiv.org/abs/2507.01509)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Polyp segmentation in colonoscopy images is crucial for early detection and diagnosis of colorectal cancer. However, this task remains a significant challenge due to the substantial variations in polyp shape, size, and color, as well as the high similarity between polyps and surrounding tissues, often compounded by indistinct boundaries. While existing encoder-decoder CNN and transformer-based approaches have shown promising results, they struggle with stable segmentation performance on polyps with weak or blurry boundaries. These methods exhibit limited abilities to distinguish between polyps and non-polyps and capture essential boundary cues. Moreover, their generalizability still falls short of meeting the demands of real-time clinical applications. To address these limitations, we propose SAM-MaGuP, a groundbreaking approach for robust polyp segmentation. By incorporating a boundary distillation module and a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels at resolving weak boundary challenges and amplifies feature learning through enriched global contextual interactions. Extensive evaluations across five diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods, achieving unmatched segmentation accuracy and robustness. Our key innovations, a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in the field, pushing the boundaries of polyp segmentation to new heights.</li>
</ul>

<h3>Title: SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01513">https://arxiv.org/abs/2507.01513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01513">https://arxiv.org/pdf/2507.01513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01513]] SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism(https://arxiv.org/abs/2507.01513)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe this http URL defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMs' built-in this http URL, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing heavy training this http URL bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks. Motivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent this http URL incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating jailbreak risks without compromising utility.</li>
</ul>

<h3>Title: Loss Functions in Diffusion Models: A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Dibyanshu Kumar, Philipp Vaeth, Magda Gregorová</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01516">https://arxiv.org/abs/2507.01516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01516">https://arxiv.org/pdf/2507.01516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01516]] Loss Functions in Diffusion Models: A Comparative Study(https://arxiv.org/abs/2507.01516)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative models, inspiring extensive research into their underlying mechanisms. One of the key questions in this area is the loss functions these models shall train with. Multiple formulations have been introduced in the literature over the past several years with some links and some critical differences stemming from various initial considerations. In this paper, we explore the different target objectives and corresponding loss functions in detail. We present a systematic overview of their relationships, unifying them under the framework of the variational lower bound objective. We complement this theoretical analysis with an empirical study providing insights into the conditions under which these objectives diverge in performance and the underlying factors contributing to such deviations. Additionally, we evaluate how the choice of objective impacts the model ability to achieve specific goals, such as generating high-quality samples or accurately estimating likelihoods. This study offers a unified understanding of loss functions in diffusion models, contributing to more efficient and goal-oriented model designs in future research.</li>
</ul>

<h3>Title: Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights</h3>
<ul>
<li><strong>Authors: </strong>Tomas Zelezny, Jakub Straka, Vaclav Javorek, Ondrej Valach, Marek Hruz, Ivan Gruber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01532">https://arxiv.org/abs/2507.01532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01532">https://arxiv.org/pdf/2507.01532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01532]] Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights(https://arxiv.org/abs/2507.01532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Sign Language Translation (SLT) has evolved significantly, moving from isolated recognition approaches to complex, continuous gloss-free translation systems. This paper explores the impact of pose-based data preprocessing techniques - normalization, interpolation, and augmentation - on SLT performance. We employ a transformer-based architecture, adapting a modified T5 encoder-decoder model to process pose representations. Through extensive ablation studies on YouTubeASL and How2Sign datasets, we analyze how different preprocessing strategies affect translation accuracy. Our results demonstrate that appropriate normalization, interpolation, and augmentation techniques can significantly improve model robustness and generalization abilities. Additionally, we provide a deep analysis of the model's attentions and reveal interesting behavior suggesting that adding a dedicated register token can improve overall model performance. We publish our code on our GitHub repository, including the preprocessed YouTubeASL data.</li>
</ul>

<h3>Title: TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01535">https://arxiv.org/abs/2507.01535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01535">https://arxiv.org/pdf/2507.01535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01535]] TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking(https://arxiv.org/abs/2507.01535)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The Vision Transformer (ViT) model has long struggled with the challenge of quadratic complexity, a limitation that becomes especially critical in unmanned aerial vehicle (UAV) tracking systems, where data must be processed in real time. In this study, we explore the recently proposed State-Space Model, Mamba, leveraging its computational efficiency and capability for long-sequence modeling to effectively process dense image sequences in tracking tasks. First, we highlight the issue of temporal inconsistency in existing Mamba-based methods, specifically the failure to account for temporal continuity in the Mamba scanning mechanism. Secondly, building upon this insight,we propose TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model for handling image sequence of tracking problem. In our framework, the mamba scan is performed in a nested way while independently process temporal and spatial coherent patch tokens. While the template frame is encoded as query token and utilized for tracking in every scan. Extensive experiments conducted on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves state-of-the-art precision while offering noticeable higher speed in UAV tracking.</li>
</ul>

<h3>Title: Cybersecurity Issues in Local Energy Markets</h3>
<ul>
<li><strong>Authors: </strong>Al Hussein Dabashi, Sajjad Maleki, Biswarup Mukherjee, Gregory Epiphaniou, Carsten Maple, Charalambos Konstantinou, Subhash Lakshminarayana</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01536">https://arxiv.org/abs/2507.01536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01536">https://arxiv.org/pdf/2507.01536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01536]] Cybersecurity Issues in Local Energy Markets(https://arxiv.org/abs/2507.01536)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Local Energy Markets (LEMs), though pivotal to the energy transition, face growing cybersecurity threats due to their reliance on smart grid communication standards and vulnerable Internet-of-Things (IoT)-enabled devices. This is a critical issue because such vulnerabilities can be exploited to manipulate market operations, compromise participants' privacy, and destabilize power distribution networks. This work maps LEM communication flows to existing standards, highlights potential impacts of key identified vulnerabilities, and simulates cyberattack scenarios on a privacy-preserving LEM model to assess their impacts. Findings reveal how attackers could distort pricing and demand patterns. We finally present recommendations for researchers, industry developers, policymakers, and LEM stakeholders to secure future LEM deployments.</li>
</ul>

<h3>Title: Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing</h3>
<ul>
<li><strong>Authors: </strong>Álvaro Zaera, Diana Nicoleta Popa, Ivan Sekulic, Paolo Rosso</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01541">https://arxiv.org/abs/2507.01541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01541">https://arxiv.org/pdf/2507.01541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01541]] Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing(https://arxiv.org/abs/2507.01541)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Out-of-scope (OOS) intent detection is a critical challenge in task-oriented dialogue systems (TODS), as it ensures robustness to unseen and ambiguous queries. In this work, we propose a novel but simple modular framework that combines uncertainty modeling with fine-tuned large language models (LLMs) for efficient and accurate OOS detection. The first step applies uncertainty estimation to the output of an in-scope intent detection classifier, which is currently deployed in a real-world TODS handling tens of thousands of user interactions daily. The second step then leverages an emerging LLM-based approach, where a fine-tuned LLM is triggered to make a final decision on instances with high uncertainty. Unlike prior approaches, our method effectively balances computational efficiency and performance, combining traditional approaches with LLMs and yielding state-of-the-art results on key OOS detection benchmarks, including real-world OOS data acquired from a deployed TODS.</li>
</ul>

<h3>Title: Is External Information Useful for Stance Detection with LLMs?</h3>
<ul>
<li><strong>Authors: </strong>Quang Minh Nguyen, Taegyoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01543">https://arxiv.org/abs/2507.01543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01543">https://arxiv.org/pdf/2507.01543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01543]] Is External Information Useful for Stance Detection with LLMs?(https://arxiv.org/abs/2507.01543)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the stance detection task, a text is classified as either favorable, opposing, or neutral towards a target. Prior work suggests that the use of external information, e.g., excerpts from Wikipedia, improves stance detection performance. However, whether or not such information can benefit large language models (LLMs) remains an unanswered question, despite their wide adoption in many reasoning tasks. In this study, we conduct a systematic evaluation on how Wikipedia and web search external information can affect stance detection across eight LLMs and in three datasets with 12 targets. Surprisingly, we find that such information degrades performance in most cases, with macro F1 scores dropping by up to 27.9\%. We explain this through experiments showing LLMs' tendency to align their predictions with the stance and sentiment of the provided information rather than the ground truth stance of the given text. We also find that performance degradation persists with chain-of-thought prompting, while fine-tuning mitigates but does not fully eliminate it. Our findings, in contrast to previous literature on BERT-based systems which suggests that external information enhances performance, highlight the risks of information biases in LLM-based stance classifiers. Code is available at this https URL.</li>
</ul>

<h3>Title: Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01551">https://arxiv.org/abs/2507.01551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01551">https://arxiv.org/pdf/2507.01551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01551]] Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning(https://arxiv.org/abs/2507.01551)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Process Reinforcement Learning~(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models~(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1/3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation.</li>
</ul>

<h3>Title: On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE</h3>
<ul>
<li><strong>Authors: </strong>Koen T. W. Teuwen, Sam Baggen, Emmanuele Zambon, Luca Allodi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01571">https://arxiv.org/abs/2507.01571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01571">https://arxiv.org/pdf/2507.01571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01571]] On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE(https://arxiv.org/abs/2507.01571)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, explainability</a></li>
<li><strong>Abstract: </strong>Automation in Security Operations Centers (SOCs) plays a prominent role in alert classification and incident escalation. However, automated methods must be robust in the presence of imbalanced input data, which can negatively affect performance. Additionally, automated methods should make explainable decisions. In this work, we evaluate the effect of label imbalance on the classification of network intrusion alerts. As our use-case we employ DeepCASE, the state-of-the-art method for automated alert classification. We show that label imbalance impacts both classification performance and correctness of the classification explanations offered by DeepCASE. We conclude tuning the detection rules used in SOCs can significantly reduce imbalance and may benefit the performance and explainability offered by alert post-processing methods such as DeepCASE. Therefore, our findings suggest that traditional methods to improve the quality of input data can benefit automation.</li>
</ul>

<h3>Title: A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Keyan Hu, Xin Guo, Haifeng Li, Chao Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01573">https://arxiv.org/abs/2507.01573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01573">https://arxiv.org/pdf/2507.01573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01573]] A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2507.01573)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Remote sensing semantic segmentation must address both what the ground objects are within an image and where they are located. Consequently, segmentation models must ensure not only the semantic correctness of large-scale patches (low-frequency information) but also the precise localization of boundaries between patches (high-frequency information). However, most existing approaches rely heavily on discriminative learning, which excels at capturing low-frequency features, while overlooking its inherent limitations in learning high-frequency features for semantic segmentation. Recent studies have revealed that diffusion generative models excel at generating high-frequency details. Our theoretical analysis confirms that the diffusion denoising process significantly enhances the model's ability to learn high-frequency features; however, we also observe that these models exhibit insufficient semantic inference for low-frequency features when guided solely by the original image. Therefore, we integrate the strengths of both discriminative and generative learning, proposing the Integration of Discriminative and diffusion-based Generative learning for Boundary Refinement (IDGBR) framework. The framework first generates a coarse segmentation map using a discriminative backbone model. This map and the original image are fed into a conditioning guidance network to jointly learn a guidance representation subsequently leveraged by an iterative denoising diffusion process refining the coarse segmentation. Extensive experiments across five remote sensing semantic segmentation datasets (binary and multi-class segmentation) confirm our framework's capability of consistent boundary refinement for coarse results from diverse discriminative architectures. The source code will be available at this https URL.</li>
</ul>

<h3>Title: A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Masood Jan, Wafa Njima, Xun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01581">https://arxiv.org/abs/2507.01581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01581">https://arxiv.org/pdf/2507.01581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01581]] A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning(https://arxiv.org/abs/2507.01581)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Location information serves as the fundamental element for numerous Internet of Things (IoT) applications. Traditional indoor localization techniques often produce significant errors and raise privacy concerns due to centralized data collection. In response, Machine Learning (ML) techniques offer promising solutions by capturing indoor environment variations. However, they typically require central data aggregation, leading to privacy, bandwidth, and server reliability issues. To overcome these challenges, in this paper, we propose a Federated Learning (FL)-based approach for dynamic indoor localization using a Deep Neural Network (DNN) model. Experimental results show that FL has the nearby performance to Centralized Model (CL) while keeping the data privacy, bandwidth efficiency and server reliability. This research demonstrates that our proposed FL approach provides a viable solution for privacy-enhanced indoor localization, paving the way for advancements in secure and efficient indoor localization systems.</li>
</ul>

<h3>Title: SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation</h3>
<ul>
<li><strong>Authors: </strong>Bryan Constantine Sadihin, Michael Hua Wang, Shei Pern Chua, Hang Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01586">https://arxiv.org/abs/2507.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01586">https://arxiv.org/pdf/2507.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01586]] SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation(https://arxiv.org/abs/2507.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The production of high-quality 2D animation is highly labor-intensive process, as animators are currently required to draw and color a large number of frames by hand. We present SketchColour, the first sketch-to-colour pipeline for 2D animation built on a diffusion transformer (DiT) backbone. By replacing the conventional U-Net denoiser with a DiT-style architecture and injecting sketch information via lightweight channel-concatenation adapters accompanied with LoRA finetuning, our method natively integrates conditioning without the parameter and memory bloat of a duplicated ControlNet, greatly reducing parameter count and GPU memory usage. Evaluated on the SAKUGA dataset, SketchColour outperforms previous state-of-the-art video colourization methods across all metrics, despite using only half the training data of competing models. Our approach produces temporally coherent animations with minimal artifacts such as colour bleeding or object deformation. Our code is available at: this https URL .</li>
</ul>

<h3>Title: Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation</h3>
<ul>
<li><strong>Authors: </strong>Shutong Feng, Hsien-chin Lin, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Renato Vukovic, Milica Gašić</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01594">https://arxiv.org/abs/2507.01594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01594">https://arxiv.org/pdf/2507.01594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01594]] Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation(https://arxiv.org/abs/2507.01594)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Task-oriented dialogue (ToD) systems are designed to help users achieve specific goals through natural language interaction. While recent advances in large language models (LLMs) have significantly improved linguistic fluency and contextual understanding, building effective and emotionally intelligent ToD systems remains a complex challenge. Effective ToD systems must optimise for task success, emotional understanding and responsiveness, and precise information conveyance, all within inherently noisy and ambiguous conversational environments. In this work, we investigate architectural, representational, optimisational as well as emotional considerations of ToD systems. We set up systems covering these design considerations with a challenging evaluation environment composed of a natural-language user simulator coupled with an imperfect natural language understanding module. We propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end \textbf{R}einforcement learning with both short-term (user sentiment) and long-term (task success) rewards. Our findings demonstrate that combining LLM capability with structured reward modelling leads to more resilient and emotionally responsive ToD systems, offering a practical path forward for next-generation conversational agents.</li>
</ul>

<h3>Title: DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yue-Jiang Dong, Wang Zhao, Jiale Xu, Ying Shan, Song-Hai Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01603">https://arxiv.org/abs/2507.01603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01603">https://arxiv.org/pdf/2507.01603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01603]] DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation(https://arxiv.org/abs/2507.01603)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based video depth estimation methods have achieved remarkable success with strong generalization ability. However, predicting depth for long videos remains challenging. Existing methods typically split videos into overlapping sliding windows, leading to accumulated scale discrepancies across different windows, particularly as the number of windows increases. Additionally, these methods rely solely on 2D diffusion priors, overlooking the inherent 3D geometric structure of video depths, which results in geometrically inconsistent predictions. In this paper, we propose DepthSync, a novel, training-free framework using diffusion guidance to achieve scale- and geometry-consistent depth predictions for long videos. Specifically, we introduce scale guidance to synchronize the depth scale across windows and geometry guidance to enforce geometric alignment within windows based on the inherent 3D constraints in video depths. These two terms work synergistically, steering the denoising process toward consistent depth predictions. Experiments on various datasets validate the effectiveness of our method in producing depth estimates with improved scale and geometry consistency, particularly for long videos.</li>
</ul>

<h3>Title: Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</h3>
<ul>
<li><strong>Authors: </strong>Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01607">https://arxiv.org/abs/2507.01607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01607">https://arxiv.org/pdf/2507.01607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01607]] Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems(https://arxiv.org/abs/2507.01607)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The widespread use of deep learning face recognition raises several security concerns. Although prior works point at existing vulnerabilities, DNN backdoor attacks against real-life, unconstrained systems dealing with images captured in the wild remain a blind spot of the literature. This paper conducts the first system-level study of backdoors in deep learning-based face recognition systems. This paper yields four contributions by exploring the feasibility of DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the first time two backdoor attacks on the face detection task: face generation and face landmark shift attacks. We then show that face feature extractors trained with large margin losses also fall victim to backdoor attacks. Combining our models, we then show using 20 possible pipeline configurations and 15 attack cases that a single backdoor enables an attacker to bypass the entire function of a system. Finally, we provide stakeholders with several best practices and countermeasures.</li>
</ul>

<h3>Title: Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Ming Lu, Yan Chen, Zhan Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01608">https://arxiv.org/abs/2507.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01608">https://arxiv.org/pdf/2507.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01608]] Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference(https://arxiv.org/abs/2507.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, compressed domain semantic inference has primarily relied on learned image coding models optimized for mean squared error (MSE). However, MSE-oriented optimization tends to yield latent spaces with limited semantic richness, which hinders effective semantic inference in downstream tasks. Moreover, achieving high performance with these models often requires fine-tuning the entire vision model, which is computationally intensive, especially for large models. To address these problems, we introduce Perception-Oriented Latent Coding (POLC), an approach that enriches the semantic content of latent features for high-performance compressed domain semantic inference. With the semantically rich latent space, POLC requires only a plug-and-play adapter for fine-tuning, significantly reducing the parameter count compared to previous MSE-oriented methods. Experimental results demonstrate that POLC achieves rate-perception performance comparable to state-of-the-art generative image coding methods while markedly enhancing performance in vision tasks, with minimal fine-tuning overhead. Code is available at this https URL.</li>
</ul>

<h3>Title: Chart Question Answering from Real-World Analytical Narratives</h3>
<ul>
<li><strong>Authors: </strong>Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01627">https://arxiv.org/abs/2507.01627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01627">https://arxiv.org/pdf/2507.01627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01627]] Chart Question Answering from Real-World Analytical Narratives(https://arxiv.org/abs/2507.01627)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a new dataset for chart question answering (CQA) constructed from visualization notebooks. The dataset features real-world, multi-view charts paired with natural language questions grounded in analytical narratives. Unlike prior benchmarks, our data reflects ecologically valid reasoning workflows. Benchmarking state-of-the-art multimodal large language models reveals a significant performance gap, with GPT-4.1 achieving an accuracy of 69.3%, underscoring the challenges posed by this more authentic CQA setting.</li>
</ul>

<h3>Title: Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss</h3>
<ul>
<li><strong>Authors: </strong>Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01630">https://arxiv.org/abs/2507.01630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01630">https://arxiv.org/pdf/2507.01630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01630]] Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss(https://arxiv.org/abs/2507.01630)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The task of Human-Object conTact (HOT) detection involves identifying the specific areas of the human body that are touching objects. Nevertheless, current models are restricted to just one type of image, often leading to too much segmentation in areas with little interaction, and struggling to maintain category consistency within specific regions. To tackle this issue, a HOT framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we utilize a semantic-driven prompt mechanism to direct the network's attention towards the relevant regions based on the correlation between image and text. Then a human proximal perception mechanism is employed to dynamically perceive key depth range around the human, using learnable parameters to effectively eliminate regions where interactions are not expected. Calculating depth resolves the uncertainty of the overlap between humans and objects in a 2D perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss (RJLoss) has been created as a new loss to inhibit abnormal categories in the same area. A new evaluation metric called ``AD-Acc.'' is introduced to address the shortcomings of existing methods in addressing negative samples. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in four metrics across two benchmark datasets. Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$, \textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated dataset. Code is available at this https URL.</li>
</ul>

<h3>Title: EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity Leakage in Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Li, Xueping Liang, Xiaorui Gong, Xiu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01635">https://arxiv.org/abs/2507.01635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01635">https://arxiv.org/pdf/2507.01635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01635]] EGNInfoLeaker: Unveiling the Risks of Public Key Reuse and User Identity Leakage in Blockchain(https://arxiv.org/abs/2507.01635)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>While Ethereum's discovery protocols (Discv4/ Discv5) incorporate robust cryptographic designs to protect user privacy, real-world deployment reveals critical vulnerabilities when users deviate from security guidelines. In this paper, we design a system called EGNInfoLeaker. Our study is the first work that uncovers widespread public key reuse across Ethereum's peer-to-peer networks - a practice that fundamentally undermines the protocol's privacy guarantees. Through systematic analysis of 300 real-world network snapshots, we identify 83 users controlling 483 service nodes via public key reuse, enabling precise de-anonymization through IP correlation. Using evidence collected by EGNInfoLeaker, our Graph-Based Identity Association Algorithm links users to network entities and generates comprehensive user profiles. For User27, it exposes the public key, IP, network ID, location (country/region/city), and ISP/ORG details. The EGNInfoLeaker system demonstrates how such cryptographic misuse transforms theoretical anonymity into practical identity leakage, exposing users to surveillance and targeted attacks. These findings establish that protocol security depends not only on sound design but also on strict user compliance. Going forward, our detection framework provides a foundation for enhancing real-world privacy preservation in decentralized networks.</li>
</ul>

<h3>Title: SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement</h3>
<ul>
<li><strong>Authors: </strong>Weijie Yin, Dingkang Yang, Hongyuan Dong, Zijian Kang, Jiacong Wang, Xiao Liang, Chao Feng, Jiao Ran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01643">https://arxiv.org/abs/2507.01643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01643">https://arxiv.org/pdf/2507.01643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01643]] SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement(https://arxiv.org/abs/2507.01643)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) are essential as foundation backbones in establishing the visual comprehension capabilities of Multimodal Large Language Models (MLLMs). Although most ViTs achieve impressive performance through image-text pair-based contrastive learning or self-supervised mechanisms, they struggle to engage in connector-based co-training directly with LLMs due to potential parameter initialization conflicts and modality semantic gaps. To address the above challenges, this paper proposes SAILViT, a gradual feature learning-enhanced ViT for facilitating MLLMs to break through performance bottlenecks in complex multimodal interactions. SAILViT achieves coarse-to-fine-grained feature alignment and world knowledge infusion with gradual feature refinement, which better serves target training demands. We perform thorough empirical analyses to confirm the powerful robustness and generalizability of SAILViT across different dimensions, including parameter sizes, model architectures, training strategies, and data scales. Equipped with SAILViT, existing MLLMs show significant and consistent performance improvements on the OpenCompass benchmark across extensive downstream tasks. SAILViT series models are released at this https URL.</li>
</ul>

<h3>Title: GradMetaNet: An Equivariant Architecture for Learning on Gradients</h3>
<ul>
<li><strong>Authors: </strong>Yoav Gelberg, Yam Eitan, Aviv Navon, Aviv Shamsian, Theo (Moe)Putterman, Michael Bronstein, Haggai Maron</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01649">https://arxiv.org/abs/2507.01649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01649">https://arxiv.org/pdf/2507.01649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01649]] GradMetaNet: An Equivariant Architecture for Learning on Gradients(https://arxiv.org/abs/2507.01649)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Gradients of neural networks encode valuable information for optimization, editing, and analysis of models. Therefore, practitioners often treat gradients as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent works explore learning algorithms that operate directly on gradients but use architectures that are not specifically designed for gradient processing, limiting their applicability. In this paper, we present a principled approach for designing architectures that process gradients. Our approach is guided by three principles: (1) equivariant design that preserves neuron permutation symmetries, (2) processing sets of gradients across multiple data points to capture curvature information, and (3) efficient gradient representation through rank-1 decomposition. Based on these principles, we introduce GradMetaNet, a novel architecture for learning on gradients, constructed from simple equivariant blocks. We prove universality results for GradMetaNet, and show that previous approaches cannot approximate natural gradient-based functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness on a diverse set of gradient-based tasks on MLPs and transformers, such as learned optimization, INR editing, and estimating loss landscape curvature.</li>
</ul>

<h3>Title: Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01652">https://arxiv.org/abs/2507.01652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01652">https://arxiv.org/pdf/2507.01652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01652]] Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective(https://arxiv.org/abs/2507.01652)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.</li>
</ul>

<h3>Title: RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather</h3>
<ul>
<li><strong>Authors: </strong>Yuran Wang, Yingping Liang, Yutao Hu, Ying Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01653">https://arxiv.org/abs/2507.01653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01653">https://arxiv.org/pdf/2507.01653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01653]] RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather(https://arxiv.org/abs/2507.01653)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Learning-based stereo matching models struggle in adverse weather conditions due to the scarcity of corresponding training data and the challenges in extracting discriminative features from degraded images. These limitations significantly hinder zero-shot generalization to out-of-distribution weather conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework that enhances the zero-shot generalization of stereo matching models under adverse weather by addressing both data scarcity and feature extraction challenges. First, we introduce a diffusion-based simulation pipeline with a stereo consistency module, which generates high-quality stereo data tailored for adverse conditions. By training stereo matching models on our synthetic datasets, we reduce the domain gap between clean and degraded images, significantly improving the models' robustness to unseen weather conditions. The stereo consistency module ensures structural alignment across synthesized image pairs, preserving geometric integrity and enhancing depth estimation accuracy. Second, we design a robust feature encoder that combines a specialized ConvNet with a denoising transformer to extract stable and reliable features from degraded images. The ConvNet captures fine-grained local structures, while the denoising transformer refines global representations, effectively mitigating the impact of noise, low visibility, and weather-induced distortions. This enables more accurate disparity estimation even under challenging visual conditions. Extensive experiments demonstrate that \textbf{RobuSTereo} significantly improves the robustness and generalization of stereo matching models across diverse adverse weather scenarios.</li>
</ul>

<h3>Title: SPoT: Subpixel Placement of Tokens in Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Martine Hjelkrem-Tan, Marius Aasan, Gabriel Y. Arteaga, Adín Ramírez Rivera</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01654">https://arxiv.org/abs/2507.01654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01654">https://arxiv.org/pdf/2507.01654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01654]] SPoT: Subpixel Placement of Tokens in Vision Transformers(https://arxiv.org/abs/2507.01654)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers naturally accommodate sparsity, yet standard tokenization methods confine features to discrete patch grids. This constraint prevents models from fully exploiting sparse regimes, forcing awkward compromises. We propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that positions tokens continuously within images, effectively sidestepping grid-based limitations. With our proposed oracle-guided search, we uncover substantial performance gains achievable with ideal subpixel token positioning, drastically reducing the number of tokens necessary for accurate predictions during inference. SPoT provides a new direction for flexible, efficient, and interpretable ViT architectures, redefining sparsity as a strategic advantage rather than an imposed limitation.</li>
</ul>

<h3>Title: AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01663">https://arxiv.org/abs/2507.01663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01663">https://arxiv.org/pdf/2507.01663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01663]] AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training(https://arxiv.org/abs/2507.01663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs.</li>
</ul>

<h3>Title: Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Muzammil Behzad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01673">https://arxiv.org/abs/2507.01673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01673">https://arxiv.org/pdf/2507.01673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01673]] Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition(https://arxiv.org/abs/2507.01673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial expression recognition (FER) in 3D and 4D domains presents a significant challenge in affective computing due to the complexity of spatial and temporal facial dynamics. Its success is crucial for advancing applications in human behavior understanding, healthcare monitoring, and human-computer interaction. In this work, we propose FACET-VLM, a vision-language framework for 3D/4D FER that integrates multiview facial representation learning with semantic guidance from natural language prompts. FACET-VLM introduces three key components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion, Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions, and a multiview consistency loss to enforce structural coherence across views. Our model achieves state-of-the-art accuracy across multiple benchmarks, including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset, demonstrating strong performance in capturing subtle, short-lived emotional cues. The extensive experimental results confirm the effectiveness and substantial contributions of each individual component within the framework. Overall, FACET-VLM offers a robust, extensible, and high-performing solution for multimodal FER in both posed and spontaneous settings.</li>
</ul>

<h3>Title: Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01679">https://arxiv.org/abs/2507.01679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01679">https://arxiv.org/pdf/2507.01679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01679]] Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling(https://arxiv.org/abs/2507.01679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Existing post-training techniques for large language models are broadly categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking demonstration data but can lead to problematic generalization as a form of behavior cloning. Conversely, RFT can significantly enhance a model's performance but is prone to learn unexpected behaviors, and its performance is highly sensitive to the initial policy. In this paper, we propose a unified view of these methods and introduce Prefix-RFT, a hybrid approach that synergizes learning from both demonstration and exploration. Using mathematical reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is both simple and effective. It not only surpasses the performance of standalone SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key advantage is its seamless integration into existing open-source frameworks, requiring only minimal modifications to the standard RFT pipeline. Our analysis highlights the complementary nature of SFT and RFT, and validates that Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore, ablation studies confirm the method's robustness to variations in the quality and quantity of demonstration data. We hope this work offers a new perspective on LLM post-training, suggesting that a unified paradigm that judiciously integrates demonstration and exploration could be a promising direction for future research.</li>
</ul>

<h3>Title: GPT, But Backwards: Exactly Inverting Language Model Outputs</h3>
<ul>
<li><strong>Authors: </strong>Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01693">https://arxiv.org/abs/2507.01693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01693">https://arxiv.org/pdf/2507.01693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01693]] GPT, But Backwards: Exactly Inverting Language Model Outputs(https://arxiv.org/abs/2507.01693)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>While existing auditing techniques attempt to identify potential unwanted behaviours in large language models (LLMs), we address the complementary forensic problem of reconstructing the exact input that led to an existing LLM output - enabling post-incident analysis and potentially the detection of fake output reports. We formalize exact input reconstruction as a discrete optimisation problem with a unique global minimum and introduce SODA, an efficient gradient-based algorithm that operates on a continuous relaxation of the input search space with periodic restarts and parameter decay. Through comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we demonstrate that SODA significantly outperforms existing approaches. We succeed in fully recovering 79.5% of shorter out-of-distribution inputs from next-token logits, without a single false positive, but struggle to extract private information from the outputs of longer (15+ token) input sequences. This suggests that standard deployment practices may currently provide adequate protection against malicious use of our method. Our code is available at this https URL.</li>
</ul>

<h3>Title: Graph Representation-based Model Poisoning on Federated LLMs in CyberEdge Networks</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Cai, Haofan Dong, Houtianfu Wang, Kai Li, Ozgur B. Akan</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01694">https://arxiv.org/abs/2507.01694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01694">https://arxiv.org/pdf/2507.01694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01694]] Graph Representation-based Model Poisoning on Federated LLMs in CyberEdge Networks(https://arxiv.org/abs/2507.01694)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, defense, attack, robust, federate, generative, large language model</a></li>
<li><strong>Abstract: </strong>Federated large language models (FedLLMs) provide powerful generative capabilities in CyberEdge networks while protecting data privacy. However, FedLLMs remains highly vulnerable to model poisoning attacks. This article first reviews recent model poisoning techniques and existing defense mechanisms for FedLLMs, highlighting critical limitations, particularly under non-IID text distributions. In particular, current defenses primarily utilize distance-based outlier detection or norm constraints, operating under the assumption that adversarial updates significantly diverge from benign statistics. This assumption can fail when facing adaptive attackers targeting billionparameter LLMs. Next, this article investigates emerging Graph Representation-Based Model Poisoning (GRMP), a novel attack paradigm that leverages higher-order correlations among honest client gradients to synthesize malicious updates indistinguishable from legitimate model updates. GRMP can effectively evade advanced defenses, resulting in substantial accuracy loss and performance degradation. Moreover, this article outlines a research roadmap emphasizing the importance of graph-aware secure aggregation methods, FedLLMs-specific vulnerability metrics, and evaluation frameworks to strengthen the robustness of future federated language model deployments.</li>
</ul>

<h3>Title: PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution</h3>
<ul>
<li><strong>Authors: </strong>Omkar Shende, Gayathri Ananthanarayanan, Marcello Traiola</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01695">https://arxiv.org/abs/2507.01695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01695">https://arxiv.org/pdf/2507.01695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01695]] PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution(https://arxiv.org/abs/2507.01695)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable ability to model complex patterns across various domains such as computer vision, speech recognition, robotics, etc. While large DNN models are often more accurate than simpler, lightweight models, they are also resource- and energy-hungry. Hence, it is imperative to design methods to reduce reliance on such large models without significant degradation in output accuracy. The high computational cost of these models is often necessary only for a reduced set of challenging inputs, while lighter models can handle most simple ones. Thus, carefully combining properties of existing DNN models in a dynamic, input-based way opens opportunities to improve efficiency without impacting accuracy. In this work, we introduce PERTINENCE, a novel online method designed to analyze the complexity of input features and dynamically select the most suitable model from a pre-trained set to process a given input effectively. To achieve this, we employ a genetic algorithm to explore the training space of an ML-based input dispatcher, enabling convergence towards the Pareto front in the solution space that balances overall accuracy and computational efficiency. We showcase our approach on state-of-the-art Convolutional Neural Networks (CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers (ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's ability to provide alternative solutions to existing state-of-the-art models in terms of trade-offs between accuracy and number of operations. By opportunistically selecting among models trained for the same task, PERTINENCE achieves better or comparable accuracy with up to 36% fewer operations.</li>
</ul>

<h3>Title: Variational Graph Convolutional Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Illia Oleksiienko, Juho Kanniainen, Alexandros Iosifidis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01699">https://arxiv.org/abs/2507.01699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01699">https://arxiv.org/pdf/2507.01699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01699]] Variational Graph Convolutional Neural Networks(https://arxiv.org/abs/2507.01699)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Estimation of model uncertainty can help improve the explainability of Graph Convolutional Networks and the accuracy of the models at the same time. Uncertainty can also be used in critical applications to verify the results of the model by an expert or additional models. In this paper, we propose Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks. We estimate uncertainty in both outputs and layer-wise attentions of the models, which has the potential for improving model explainability. We showcase the benefits of these models in the social trading analysis and the skeleton-based human action recognition tasks on the Finnish board membership, NTU-60, NTU-120 and Kinetics datasets, where we show improvement in model accuracy in addition to estimated model uncertainties.</li>
</ul>

<h3>Title: AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness</h3>
<ul>
<li><strong>Authors: </strong>Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01702">https://arxiv.org/abs/2507.01702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01702">https://arxiv.org/pdf/2507.01702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01702]] AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness(https://arxiv.org/abs/2507.01702)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at this https URL.</li>
</ul>

<h3>Title: Towards Better Attribute Inference Vulnerability Measures</h3>
<ul>
<li><strong>Authors: </strong>Paul Francis, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01710">https://arxiv.org/abs/2507.01710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01710">https://arxiv.org/pdf/2507.01710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01710]] Towards Better Attribute Inference Vulnerability Measures(https://arxiv.org/abs/2507.01710)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>The purpose of anonymizing structured data is to protect the privacy of individuals in the data while retaining the statistical properties of the data. An important class of attack on anonymized data is attribute inference, where an attacker infers the value of an unknown attribute of a target individual given knowledge of one or more known attributes. A major limitation of recent attribute inference measures is that they do not take recall into account, only precision. It is often the case that attacks target only a fraction of individuals, for instance data outliers. Incorporating recall, however, substantially complicates the measure, because one must determine how to combine recall and precision in a composite measure for both the attack and baseline. This paper presents the design and implementation of an attribute inference measure that incorporates both precision and recall. Our design also improves on how the baseline attribute inference is computed. In experiments using a generic best row match attack on moderately-anonymized microdata, we show that in over 25\% of the attacks, our approach correctly labeled the attack to be at risk while the prior approach incorrectly labeled the attack to be safe.</li>
</ul>

<h3>Title: Using Wavelet Domain Fingerprints to Improve Source Camera Identification</h3>
<ul>
<li><strong>Authors: </strong>Xinle Tian, Matthew Nunes, Emiko Dupont, Shaunagh Downing, Freddie Lichtenstein, Matt Burns</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01712">https://arxiv.org/abs/2507.01712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01712">https://arxiv.org/pdf/2507.01712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01712]] Using Wavelet Domain Fingerprints to Improve Source Camera Identification(https://arxiv.org/abs/2507.01712)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Camera fingerprint detection plays a crucial role in source identification and image forensics, with wavelet denoising approaches proving to be particularly effective in extracting sensor pattern noise (SPN). In this article, we propose a modification to wavelet-based SPN extraction. Rather than constructing the fingerprint as an image, we introduce the notion of a wavelet domain fingerprint. This avoids the final inversion step of the denoising algorithm and allows fingerprint comparisons to be made directly in the wavelet domain. As such, our modification streamlines the extraction and comparison process. Experimental results on real-world datasets demonstrate that our method not only achieves higher detection accuracy but can also significantly improve processing speed.</li>
</ul>

<h3>Title: Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Aditya Tomar, Rudra Murthy, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01715">https://arxiv.org/abs/2507.01715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01715">https://arxiv.org/pdf/2507.01715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01715]] Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach(https://arxiv.org/abs/2507.01715)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Bias and stereotypes in language models can cause harm, especially in sensitive areas like content moderation and decision-making. This paper addresses bias and stereotype detection by exploring how jointly learning these tasks enhances model performance. We introduce StereoBias, a unique dataset labeled for bias and stereotype detection across five categories: religion, gender, socio-economic status, race, profession, and others, enabling a deeper study of their relationship. Our experiments compare encoder-only models and fine-tuned decoder-only models using QLoRA. While encoder-only models perform well, decoder-only models also show competitive results. Crucially, joint training on bias and stereotype detection significantly improves bias detection compared to training them separately. Additional experiments with sentiment analysis confirm that the improvements stem from the connection between bias and stereotypes, not multi-task learning alone. These findings highlight the value of leveraging stereotype information to build fairer and more effective AI systems.</li>
</ul>

<h3>Title: Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhongwen Zhang, Yuri Boykov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01721">https://arxiv.org/abs/2507.01721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01721">https://arxiv.org/pdf/2507.01721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01721]] Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation(https://arxiv.org/abs/2507.01721)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We consider weakly supervised segmentation where only a fraction of pixels have ground truth labels (scribbles) and focus on a self-labeling approach optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled pixels. While WSSS methods can directly optimize such losses via gradient descent, prior work suggests that higher-order optimization can improve network training by introducing hidden pseudo-labels and powerful CRF sub-problem solvers, e.g. graph cut. However, previously used hard pseudo-labels can not represent class uncertainty or errors, which motivates soft self-labeling. We derive a principled auxiliary loss and systematically evaluate standard and new CRF relaxations (convex and non-convex), neighborhood systems, and terms connecting network predictions with soft pseudo-labels. We also propose a general continuous sub-problem solver. Using only standard architectures, soft self-labeling consistently improves scribble-based training and outperforms significantly more complex specialized WSSS systems. It can outperform full pixel-precise supervision. Our general ideas apply to other weakly-supervised problems/systems.</li>
</ul>

<h3>Title: When Does Pruning Benefit Vision Representations?</h3>
<ul>
<li><strong>Authors: </strong>Enrico Cassano, Riccardo Renzulli, Andrea Bragagnolo, Marco Grangetto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01722">https://arxiv.org/abs/2507.01722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01722">https://arxiv.org/pdf/2507.01722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01722]] When Does Pruning Benefit Vision Representations?(https://arxiv.org/abs/2507.01722)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Pruning is widely used to reduce the complexity of deep learning models, but its effects on interpretability and representation learning remain poorly understood. This paper investigates how pruning influences vision models across three key dimensions: (i) interpretability, (ii) unsupervised object discovery, and (iii) alignment with human perception. We first analyze different vision network architectures to examine how varying sparsity levels affect feature attribution interpretability methods. Additionally, we explore whether pruning promotes more succinct and structured representations, potentially improving unsupervised object discovery by discarding redundant information while preserving essential features. Finally, we assess whether pruning enhances the alignment between model representations and human perception, investigating whether sparser models focus on more discriminative features similarly to humans. Our findings also reveal the presence of sweet spots, where sparse models exhibit higher interpretability, downstream generalization and human alignment. However, these spots highly depend on the network architectures and their size in terms of trainable parameters. Our results suggest a complex interplay between these three dimensions, highlighting the importance of investigating when and how pruning benefits vision representations.</li>
</ul>

<h3>Title: LLMs for Legal Subsumption in German Employment Contracts</h3>
<ul>
<li><strong>Authors: </strong>Oliver Wardas, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01734">https://arxiv.org/abs/2507.01734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01734">https://arxiv.org/pdf/2507.01734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01734]] LLMs for Legal Subsumption in German Employment Contracts(https://arxiv.org/abs/2507.01734)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Legal work, characterized by its text-heavy and resource-intensive nature, presents unique challenges and opportunities for NLP research. While data-driven approaches have advanced the field, their lack of interpretability and trustworthiness limits their applicability in dynamic legal environments. To address these issues, we collaborated with legal experts to extend an existing dataset and explored the use of Large Language Models (LLMs) and in-context learning to evaluate the legality of clauses in German employment contracts. Our work evaluates the ability of different LLMs to classify clauses as "valid," "unfair," or "void" under three legal context variants: no legal context, full-text sources of laws and court rulings, and distilled versions of these (referred to as examination guidelines). Results show that full-text sources moderately improve performance, while examination guidelines significantly enhance recall for void clauses and weighted F1-Score, reaching 80\%. Despite these advancements, LLMs' performance when using full-text sources remains substantially below that of human lawyers. We contribute an extended dataset, including examination guidelines, referenced legal sources, and corresponding annotations, alongside our code and all log files. Our findings highlight the potential of LLMs to assist lawyers in contract legality review while also underscoring the limitations of the methods presented.</li>
</ul>

<h3>Title: ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01735">https://arxiv.org/abs/2507.01735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01735">https://arxiv.org/pdf/2507.01735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01735]] ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving(https://arxiv.org/abs/2507.01735)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we present details of the 1st W-CODA workshop, held in conjunction with the ECCV 2024. W-CODA aims to explore next-generation solutions for autonomous driving corner cases, empowered by state-of-the-art multimodal perception and comprehension techniques. 5 Speakers from both academia and industry are invited to share their latest progress and opinions. We collect research papers and hold a dual-track challenge, including both corner case scene understanding and generation. As the pioneering effort, we will continuously bridge the gap between frontier autonomous driving techniques and fully intelligent, reliable self-driving agents robust towards corner cases.</li>
</ul>

<h3>Title: HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lin Wu, Zhixiang Chen, Jianglin Lan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01737">https://arxiv.org/abs/2507.01737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01737">https://arxiv.org/pdf/2507.01737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01737]] HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion(https://arxiv.org/abs/2507.01737)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating realistic 3D human-object interactions (HOIs) remains a challenging task due to the difficulty of modeling detailed interaction dynamics. Existing methods treat human and object motions independently, resulting in physically implausible and causally inconsistent behaviors. In this work, we present HOI-Dyn, a novel framework that formulates HOI generation as a driver-responder system, where human actions drive object responses. At the core of our method is a lightweight transformer-based interaction dynamics model that explicitly predicts how objects should react to human motion. To further enforce consistency, we introduce a residual-based dynamics loss that mitigates the impact of dynamics prediction errors and prevents misleading optimization signals. The dynamics model is used only during training, preserving inference efficiency. Through extensive qualitative and quantitative experiments, we demonstrate that our approach not only enhances the quality of HOI generation but also establishes a feasible metric for evaluating the quality of generated interactions.</li>
</ul>

<h3>Title: DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy</h3>
<ul>
<li><strong>Authors: </strong>Ming Dai, Wenxuan Cheng, Jiang-jiang Liu, Sen Yang, Wenxiao Cai, Yanpeng Sun, Wankou Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01738">https://arxiv.org/abs/2507.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01738">https://arxiv.org/pdf/2507.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01738]] DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy(https://arxiv.org/abs/2507.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Image Segmentation (RIS) is a challenging task that aims to segment objects in an image based on natural language expressions. While prior studies have predominantly concentrated on improving vision-language interactions and achieving fine-grained localization, a systematic analysis of the fundamental bottlenecks in existing RIS frameworks remains underexplored. To bridge this gap, we propose DeRIS, a novel framework that decomposes RIS into two key components: perception and cognition. This modular decomposition facilitates a systematic analysis of the primary bottlenecks impeding RIS performance. Our findings reveal that the predominant limitation lies not in perceptual deficiencies, but in the insufficient multi-modal cognitive capacity of current models. To mitigate this, we propose a Loopback Synergy mechanism, which enhances the synergy between the perception and cognition modules, thereby enabling precise segmentation while simultaneously improving robust image-text comprehension. Additionally, we analyze and introduce a simple non-referent sample conversion data augmentation to address the long-tail distribution issue related to target existence judgement in general scenarios. Notably, DeRIS demonstrates inherent adaptability to both non- and multi-referents scenarios without requiring specialized architectural modifications, enhancing its general applicability. The codes and models are available at this https URL.</li>
</ul>

<h3>Title: Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Jin, Grant Mair, Joanna M. Wardlaw, Maria del C. Valdés Hernández</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01744">https://arxiv.org/abs/2507.01744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01744">https://arxiv.org/pdf/2507.01744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01744]] Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans(https://arxiv.org/abs/2507.01744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online.</li>
</ul>

<h3>Title: SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery</h3>
<ul>
<li><strong>Authors: </strong>Nora Gourmelon, Marcel Dreier, Martin Mayr, Thorsten Seehaus, Dakota Pyles, Matthias Braun, Andreas Maier, Vincent Christlein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01747">https://arxiv.org/abs/2507.01747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01747">https://arxiv.org/pdf/2507.01747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01747]] SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery(https://arxiv.org/abs/2507.01747)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Glaciers are losing ice mass at unprecedented rates, increasing the need for accurate, year-round monitoring to understand frontal ablation, particularly the factors driving the calving process. Deep learning models can extract calving front positions from Synthetic Aperture Radar imagery to track seasonal ice losses at the calving fronts of marine- and lake-terminating glaciers. The current state-of-the-art model relies on ImageNet-pretrained weights. However, they are suboptimal due to the domain shift between the natural images in ImageNet and the specialized characteristics of remote sensing imagery, in particular for Synthetic Aperture Radar imagery. To address this challenge, we propose two novel self-supervised multimodal pretraining techniques that leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14 Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the dataset. Additionally, we introduce a novel hybrid model architecture that combines a Swin Transformer encoder with a residual Convolutional Neural Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe) benchmark dataset, outperforming the prior best model by 67 m. Evaluating an ensemble of the proposed model on a multi-annotator study of the benchmark dataset reveals a mean distance error of 75 m, approaching the human performance of 38 m. This advancement enables precise monitoring of seasonal changes in glacier calving fronts.</li>
</ul>

<h3>Title: Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01752">https://arxiv.org/abs/2507.01752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01752">https://arxiv.org/pdf/2507.01752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01752]] Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training(https://arxiv.org/abs/2507.01752)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, its reliance on large volumes of labeled data raises privacy and security concerns such as susceptibility to data poisoning attacks and the risk of overfitting. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. However, black box methods also pose significant challenges, including poor scalability to high-dimensional parameter spaces, as prevalent in large language models (LLMs), and high computational costs due to reliance on numerous model evaluations. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide strong theoretical bounds on generalization, differential privacy, susceptibility to data poisoning attacks, and robustness to extraction attacks. BBoxER operates on top of pre-trained LLMs, offering a lightweight and modular enhancement suitable for deployment in restricted or privacy-sensitive environments, in addition to non-vacuous generalization guarantees. In experiments with LLMs, we demonstrate empirically that Retrofitting methods are able to learn, showing how a few iterations of BBoxER improve performance and generalize well on a benchmark of reasoning datasets. This positions BBoxER as an attractive add-on on top of gradient-based optimization.</li>
</ul>

<h3>Title: Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Peng Zheng, Junke Wang, Yi Chang, Yizhou Yu, Rui Ma, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01756">https://arxiv.org/abs/2507.01756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01756">https://arxiv.org/pdf/2507.01756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01756]] Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis(https://arxiv.org/abs/2507.01756)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have spurred interests in encoding images as discrete tokens and leveraging autoregressive (AR) frameworks for visual generation. However, the quantization process in AR-based visual generation models inherently introduces information loss that degrades image fidelity. To mitigate this limitation, recent studies have explored to autoregressively predict continuous tokens. Unlike discrete tokens that reside in a structured and bounded space, continuous representations exist in an unbounded, high-dimensional space, making density estimation more challenging and increasing the risk of generating out-of-distribution artifacts. Based on the above findings, this work introduces DisCon (Discrete-Conditioned Continuous Autoregressive Model), a novel framework that reinterprets discrete tokens as conditional signals rather than generation targets. By modeling the conditional probability of continuous representations conditioned on discrete tokens, DisCon circumvents the optimization challenges of continuous token modeling while avoiding the information loss caused by quantization. DisCon achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation, outperforming state-of-the-art autoregressive approaches by a clear margin.</li>
</ul>

<h3>Title: Enhanced Generative Model Evaluation with Clipped Density and Coverage</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Salvy, Hugues Talbot, Bertrand Thirion</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01761">https://arxiv.org/abs/2507.01761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01761">https://arxiv.org/pdf/2507.01761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01761]] Enhanced Generative Model Evaluation with Clipped Density and Coverage(https://arxiv.org/abs/2507.01761)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Although generative models have made remarkable progress in recent years, their use in critical applications has been hindered by their incapacity to reliably evaluate sample quality. Quality refers to at least two complementary concepts: fidelity and coverage. Current quality metrics often lack reliable, interpretable values due to an absence of calibration or insufficient robustness to outliers. To address these shortcomings, we introduce two novel metrics, Clipped Density and Clipped Coverage. By clipping individual sample contributions and, for fidelity, the radii of nearest neighbor balls, our metrics prevent out-of-distribution samples from biasing the aggregated values. Through analytical and empirical calibration, these metrics exhibit linear score degradation as the proportion of poor samples increases. Thus, they can be straightforwardly interpreted as equivalent proportions of good samples. Extensive experiments on synthetic and real-world datasets demonstrate that Clipped Density and Clipped Coverage outperform existing methods in terms of robustness, sensitivity, and interpretability for evaluating generative models.</li>
</ul>

<h3>Title: Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range</h3>
<ul>
<li><strong>Authors: </strong>Anis Yusof, Yuancheng Liu, Niklaus Kang, Choon Meng Seah, Zhenkai Liang, Ee-Chien Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01768">https://arxiv.org/abs/2507.01768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01768">https://arxiv.org/pdf/2507.01768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01768]] Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range(https://arxiv.org/abs/2507.01768)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>The prevalence of cyberattacks on Industrial Control Systems (ICS) has highlighted the necessity for robust security measures and incident response to protect critical infrastructure. This is prominent when Operational Technology (OT) systems undergo digital transformation by integrating with Information Technology (IT) systems to enhance operational efficiency, adaptability, and safety. To support analysts in staying abreast of emerging attack patterns, there is a need for ICS datasets that reflect indicators representative of contemporary cyber threats. To address this, we conduct two ICS cyberattack simulations to showcase the impact of trending ICS cyberattacks on a railway cyber range that resembles the railway infrastructure. The attack scenario is designed to blend trending attack trends with attack patterns observed from historical ICS incidents. The resulting evidence is collected as datasets, serving as an essential resource for cyberattack analysis. This captures key indicators that are relevant to the current threat landscape, augmenting the effectiveness of security systems and analysts to protect against ICS cyber threats.</li>
</ul>

<h3>Title: BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification</h3>
<ul>
<li><strong>Authors: </strong>Dalia Rodríguez-Salas, Christian Riess</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01781">https://arxiv.org/abs/2507.01781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01781">https://arxiv.org/pdf/2507.01781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01781]] BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification(https://arxiv.org/abs/2507.01781)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce BranchNet, a neuro-symbolic learning framework that transforms decision tree ensembles into sparse, partially connected neural networks. Each branch, defined as a decision path from root to a parent of leaves, is mapped to a hidden neuron, preserving symbolic structure while enabling gradient-based optimization. The resulting models are compact, interpretable, and require no manual architecture tuning. Evaluated on a suite of structured multi-class classification benchmarks, BranchNet consistently outperforms XGBoost in accuracy, with statistically significant gains. We detail the architecture, training procedure, and sparsity dynamics, and discuss the model's strengths in symbolic interpretability as well as its current limitations, particularly on binary tasks where further adaptive calibration may be beneficial.</li>
</ul>

<h3>Title: MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01785">https://arxiv.org/abs/2507.01785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01785">https://arxiv.org/pdf/2507.01785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01785]] MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining(https://arxiv.org/abs/2507.01785)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a single rater for 17 target languages. MuRating aggregates multiple English "raters" via pairwise comparisons to learn unified document-quality scores,then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to strong baselines, including QuRater, AskLLM, DCLM and so on, our approach boosts average accuracy on both English benchmarks and multilingual evaluations, with especially large gains on knowledge-intensive tasks. We further analyze translation fidelity, selection biases, and underrepresentation of narrative material, outlining directions for future work.</li>
</ul>

<h3>Title: Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01788">https://arxiv.org/abs/2507.01788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01788">https://arxiv.org/pdf/2507.01788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01788]] Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging(https://arxiv.org/abs/2507.01788)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60\%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems.</li>
</ul>

<h3>Title: Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation</h3>
<ul>
<li><strong>Authors: </strong>Zihong Guo, Chen Wan, Yayin Zheng, Hailing Kuang, Xiaohai Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01791">https://arxiv.org/abs/2507.01791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01791">https://arxiv.org/pdf/2507.01791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01791]] Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation(https://arxiv.org/abs/2507.01791)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>The transferability of adversarial examples poses a significant security challenge for deep neural networks, which can be attacked without knowing anything about them. In this paper, we propose a new Segmented Gaussian Pyramid (SGP) attack method to enhance the transferability, particularly against defense models. Unlike existing methods that generally focus on single-scale images, our approach employs Gaussian filtering and three types of downsampling to construct a series of multi-scale examples. Then, the gradients of the loss function with respect to each scale are computed, and their average is used to determine the adversarial perturbations. The proposed SGP can be considered an input transformation with high extensibility that is easily integrated into most existing adversarial attacks. Extensive experiments demonstrate that in contrast to the state-of-the-art methods, SGP significantly enhances attack success rates against black-box defense models, with average attack success rates increasing by 2.3% to 32.6%, based only on transferability.</li>
</ul>

<h3>Title: FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization</h3>
<ul>
<li><strong>Authors: </strong>Peng Zheng, Ye Wang, Rui Ma, Zuxuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01792">https://arxiv.org/abs/2507.01792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01792">https://arxiv.org/pdf/2507.01792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01792]] FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization(https://arxiv.org/abs/2507.01792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Subject-driven image generation plays a crucial role in applications such as virtual try-on and poster design. Existing approaches typically fine-tune pretrained generative models or apply LoRA-based adaptations for individual subjects. However, these methods struggle with multi-subject personalization, as combining independently adapted modules often requires complex re-tuning or joint optimization. We present FreeLoRA, a simple and generalizable framework that enables training-free fusion of subject-specific LoRA modules for multi-subject personalization. Each LoRA module is adapted on a few images of a specific subject using a Full Token Tuning strategy, where it is applied across all tokens in the prompt to encourage weakly supervised token-content alignment. At inference, we adopt Subject-Aware Inference, activating each module only on its corresponding subject tokens. This enables training-free fusion of multiple personalized subjects within a single image, while mitigating overfitting and mutual interference between subjects. Extensive experiments show that FreeLoRA achieves strong performance in both subject fidelity and prompt consistency.</li>
</ul>

<h3>Title: HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision</h3>
<ul>
<li><strong>Authors: </strong>Shengli Zhou, Jianuo Zhu, Qilin Huang, Fangjing Wang, Yanfu Zhang, Feng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01800">https://arxiv.org/abs/2507.01800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01800">https://arxiv.org/pdf/2507.01800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01800]] HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision(https://arxiv.org/abs/2507.01800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the physical world and perform spatial reasoning. Answer-centric supervision is a commonly used training method for 3D VQA models. Many models that utilize this strategy have achieved promising results in 3D VQA tasks. However, the answer-centric approach only supervises the final output of models and allows models to develop reasoning pathways freely. The absence of supervision on the reasoning pathway enables the potential for developing superficial shortcuts through common patterns in question-answer pairs. Moreover, although slow-thinking methods advance large language models, they suffer from underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA model leveraging a hierarchical concentration narrowing supervision method. By mimicking the human process of gradually focusing from a broad area to specific objects while searching for answers, our method guides the model to perform three phases of concentration narrowing through hierarchical supervision. By supervising key checkpoints on a general reasoning pathway, our method can ensure the development of a rational and effective reasoning pathway. Extensive experimental results demonstrate that our method can effectively ensure that the model develops a rational reasoning pathway and performs better. The code is available at this https URL.</li>
</ul>

<h3>Title: AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Bin Rao, Haicheng Liao, Yanchen Guan, Chengyue Wang, Bonan Wang, Jiaxun Zhang, Zhenning Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01801">https://arxiv.org/abs/2507.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01801">https://arxiv.org/pdf/2507.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01801]] AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction(https://arxiv.org/abs/2507.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately predicting the future trajectories of traffic agents is essential in autonomous driving. However, due to the inherent imbalance in trajectory distributions, tail data in natural datasets often represents more complex and hazardous scenarios. Existing studies typically rely solely on a base model's prediction error, without considering the diversity and uncertainty of long-tail trajectory patterns. We propose an adaptive momentum and decoupled contrastive learning framework (AMD), which integrates unsupervised and supervised contrastive learning strategies. By leveraging an improved momentum contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module, our framework enhances the model's ability to recognize rare and complex trajectories. Additionally, we design four types of trajectory random augmentation methods and introduce an online iterative clustering strategy, allowing the model to dynamically update pseudo-labels and better adapt to the distributional shifts in long-tail data. We propose three different criteria to define long-tail trajectories and conduct extensive comparative experiments on the nuScenes and ETH$/$UCY datasets. The results show that AMD not only achieves optimal performance in long-tail trajectory prediction but also demonstrates outstanding overall prediction accuracy.</li>
</ul>

<h3>Title: The Anatomy of Evidence: An Investigation Into Explainable ICD Coding</h3>
<ul>
<li><strong>Authors: </strong>Katharina Beckh, Elisa Studeny, Sujan Sai Gannamaneni, Dario Antweiler, Stefan Rüping</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01802">https://arxiv.org/abs/2507.01802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01802">https://arxiv.org/pdf/2507.01802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01802]] The Anatomy of Evidence: An Investigation Into Explainable ICD Coding(https://arxiv.org/abs/2507.01802)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability</a></li>
<li><strong>Abstract: </strong>Automatic medical coding has the potential to ease documentation and billing processes. For this task, transparency plays an important role for medical coders and regulatory bodies, which can be achieved using explainability methods. However, the evaluation of these approaches has been mostly limited to short text and binary settings due to a scarcity of annotated data. Recent efforts by Cheng et al. (2023) have introduced the MDACE dataset, which provides a valuable resource containing code evidence in clinical records. In this work, we conduct an in-depth analysis of the MDACE dataset and perform plausibility evaluation of current explainable medical coding systems from an applied perspective. With this, we contribute to a deeper understanding of automatic medical coding and evidence extraction. Our findings reveal that ground truth evidence aligns with code descriptions to a certain degree. An investigation into state-of-the-art approaches shows a high overlap with ground truth evidence. We propose match measures and highlight success and failure cases. Based on our findings, we provide recommendations for developing and evaluating explainable medical coding systems.</li>
</ul>

<h3>Title: LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01806">https://arxiv.org/abs/2507.01806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01806">https://arxiv.org/pdf/2507.01806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01806]] LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs(https://arxiv.org/abs/2507.01806)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language Models (LLMs) by enabling parameter-efficient updates. However, their widespread adoption remains limited by the reliance on GPU-based training. In this work, we propose a theoretically grounded approach to LoRA fine-tuning designed specifically for users with limited computational resources, particularly those restricted to standard laptop CPUs. Our method learns a meta-operator that maps any input dataset, represented as a probability distribution, to a set of LoRA weights by leveraging a large bank of pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of performing new gradient-based updates, our pipeline constructs adapters via lightweight combinations of existing LoRAs directly on CPU. While the resulting adapters do not match the performance of GPU-trained counterparts, they consistently outperform the base Mistral model on downstream tasks, offering a practical and accessible alternative to traditional GPU-based fine-tuning.</li>
</ul>

<h3>Title: Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Ji, Jessica Shorland, Joshua Shank, Pascal Delpe-Brice, Latanya Sweeney, Jan Allebach, Ali Shakouri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01808">https://arxiv.org/abs/2507.01808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01808">https://arxiv.org/pdf/2507.01808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01808]] Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems(https://arxiv.org/abs/2507.01808)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Small- and medium-sized manufacturers need innovative data tools but, because of competition and privacy concerns, often do not want to share their proprietary data with researchers who might be interested in helping. This paper introduces a privacy-preserving platform by which manufacturers may safely share their data with researchers through secure methods, so that those researchers then create innovative tools to solve the manufacturers' real-world problems, and then provide tools that execute solutions back onto the platform for others to use with privacy and confidentiality guarantees. We illustrate this problem through a particular use case which addresses an important problem in the large-scale manufacturing of food crystals, which is that quality control relies on image analysis tools. Previous to our research, food crystals in the images were manually counted, which required substantial and time-consuming human efforts, but we have developed and deployed a crystal analysis tool which makes this process both more rapid and accurate. The tool enables automatic characterization of the crystal size distribution and numbers from microscope images while the natural imperfections from the sample preparation are automatically removed; a machine learning model to count high resolution translucent crystals and agglomeration of crystals was also developed to aid in these efforts. The resulting algorithm was then packaged for real-world use on the factory floor via a web-based app secured through the originating privacy-preserving platform, allowing manufacturers to use it while keeping their proprietary data secure. After demonstrating this full process, future directions are also explored.</li>
</ul>

<h3>Title: Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes</h3>
<ul>
<li><strong>Authors: </strong>Nikita Neveditsin, Pawan Lingras, Vijay Mago</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01810">https://arxiv.org/abs/2507.01810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01810">https://arxiv.org/pdf/2507.01810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01810]] Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes(https://arxiv.org/abs/2507.01810)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, extraction</a></li>
<li><strong>Abstract: </strong>We present a comparative analysis of the parseability of structured outputs generated by small language models for open attribute-value extraction from clinical notes. We evaluate three widely used serialization formats: JSON, YAML, and XML, and find that JSON consistently yields the highest parseability. Structural robustness improves with targeted prompting and larger models, but declines for longer documents and certain note types. Our error analysis identifies recurring format-specific failure patterns. These findings offer practical guidance for selecting serialization formats and designing prompts when deploying language models in privacy-sensitive clinical settings.</li>
</ul>

<h3>Title: mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Tristan Torchet, Christian Metzner, Laura Kriener, Melika Payvand</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01829">https://arxiv.org/abs/2507.01829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01829">https://arxiv.org/pdf/2507.01829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01829]] mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling(https://arxiv.org/abs/2507.01829)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Edge devices for temporal processing demand models that capture both short- and long- range dynamics under tight memory constraints. While Transformers excel at sequence modeling, their quadratic memory scaling with sequence length makes them impractical for such settings. Recurrent Neural Networks (RNNs) offer constant memory but train sequentially, and Temporal Convolutional Networks (TCNs), though efficient, scale memory with kernel size. To address this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay Embedding), a hybrid-memory system that integrates a temporal 1D-convolution with learnable spacings followed by a minimal gated recurrent unit (minGRU). This design allows the convolutional layer to realize a flexible delay embedding that captures rapid temporal variations, while the recurrent module efficiently maintains global context with minimal memory overhead. We validate our approach on two synthetic tasks, demonstrating that mGRADE effectively separates and preserves multi-scale temporal features. Furthermore, on challenging pixel-by-pixel image classification benchmarks, mGRADE consistently outperforms both pure convolutional and pure recurrent counterparts using approximately 20% less memory footprint, highlighting its suitability for memory-constrained temporal processing at the edge. This highlights mGRADE's promise as an efficient solution for memory-constrained multi-scale temporal processing at the edge.</li>
</ul>

<h3>Title: Out-of-Distribution Detection Methods Answer the Wrong Questions</h3>
<ul>
<li><strong>Authors: </strong>Yucen Lily Li, Daohan Lu, Polina Kirichenko, Shikai Qiu, Tim G. J. Rudner, C. Bayan Bruss, Andrew Gordon Wilson</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01831">https://arxiv.org/abs/2507.01831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01831">https://arxiv.org/pdf/2507.01831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01831]] Out-of-Distribution Detection Methods Answer the Wrong Questions(https://arxiv.org/abs/2507.01831)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To detect distribution shifts and improve model safety, many out-of-distribution (OOD) detection methods rely on the predictive uncertainty or features of supervised models trained on in-distribution data. In this paper, we critically re-examine this popular family of OOD detection procedures, and we argue that these methods are fundamentally answering the wrong questions for OOD detection. There is no simple fix to this misalignment, since a classifier trained only on in-distribution classes cannot be expected to identify OOD points; for instance, a cat-dog classifier may confidently misclassify an airplane if it contains features that distinguish cats from dogs, despite generally appearing nothing alike. We find that uncertainty-based methods incorrectly conflate high uncertainty with being OOD, while feature-based methods incorrectly conflate far feature-space distance with being OOD. We show how these pathologies manifest as irreducible errors in OOD detection and identify common settings where these methods are ineffective. Additionally, interventions to improve OOD detection such as feature-logit hybrid methods, scaling of model and data size, epistemic uncertainty representation, and outlier exposure also fail to address this fundamental misalignment in objectives. We additionally consider unsupervised density estimation and generative models for OOD detection, which we show have their own fundamental limitations.</li>
</ul>

<h3>Title: Low-Perplexity LLM-Generated Sequences and Where To Find Them</h3>
<ul>
<li><strong>Authors: </strong>Arthur Wuhrmann, Anastasiia Kucherenko, Andrei Kucharavy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01844">https://arxiv.org/abs/2507.01844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01844">https://arxiv.org/pdf/2507.01844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01844]] Low-Perplexity LLM-Generated Sequences and Where To Find Them(https://arxiv.org/abs/2507.01844)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly widespread, understanding how specific training data shapes their outputs is crucial for transparency, accountability, privacy, and fairness. To explore how LLMs leverage and replicate their training data, we introduce a systematic approach centered on analyzing low-perplexity sequences - high-probability text spans generated by the model. Our pipeline reliably extracts such long sequences across diverse topics while avoiding degeneration, then traces them back to their sources in the training data. Surprisingly, we find that a substantial portion of these low-perplexity spans cannot be mapped to the corpus. For those that do match, we quantify the distribution of occurrences across source documents, highlighting the scope and nature of verbatim recall and paving a way toward better understanding of how LLMs training data impacts their behavior.</li>
</ul>

<h3>Title: Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages</h3>
<ul>
<li><strong>Authors: </strong>Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01853">https://arxiv.org/abs/2507.01853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01853">https://arxiv.org/pdf/2507.01853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01853]] Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages(https://arxiv.org/abs/2507.01853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at this https URL eka-eval and a part of ongoing EKA initiative (this https URL), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.</li>
</ul>

<h3>Title: DIY-MKG: An LLM-Based Polyglot Language Learning System</h3>
<ul>
<li><strong>Authors: </strong>Kenan Tang, Yanhong Li, Yao Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01872">https://arxiv.org/abs/2507.01872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01872">https://arxiv.org/pdf/2507.01872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01872]] DIY-MKG: An LLM-Based Polyglot Language Learning System(https://arxiv.org/abs/2507.01872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Existing language learning tools, even those powered by Large Language Models (LLMs), often lack support for polyglot learners to build linguistic connections across vocabularies in multiple languages, provide limited customization for individual learning paces or needs, and suffer from detrimental cognitive offloading. To address these limitations, we design Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system that supports polyglot language learning. DIY-MKG allows the user to build personalized vocabulary knowledge graphs, which are constructed by selective expansion with related words suggested by an LLM. The system further enhances learning through rich annotation capabilities and an adaptive review module that leverages LLMs for dynamic, personalized quiz generation. In addition, DIY-MKG allows users to flag incorrect quiz questions, simultaneously increasing user engagement and providing a feedback loop for prompt refinement. Our evaluation of LLM-based components in DIY-MKG shows that vocabulary expansion is reliable and fair across multiple languages, and that the generated quizzes are highly accurate, validating the robustness of DIY-MKG.</li>
</ul>

<h3>Title: Towards Foundation Auto-Encoders for Time-Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01875">https://arxiv.org/abs/2507.01875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01875">https://arxiv.org/pdf/2507.01875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01875]] Towards Foundation Auto-Encoders for Time-Series Anomaly Detection(https://arxiv.org/abs/2507.01875)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate a novel approach to time-series modeling, inspired by the successes of large pretrained foundation models. We introduce FAE (Foundation Auto-Encoders), a foundation generative-AI model for anomaly detection in time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we mean a model pretrained on massive amounts of time-series data which can learn complex temporal patterns useful for accurate modeling, forecasting, and detection of anomalies on previously unseen datasets. FAE leverages VAEs and Dilated Convolutional Neural Networks (DCNNs) to build a generic model for univariate time-series modeling, which could eventually perform properly in out-of-the-box, zero-shot anomaly detection applications. We introduce the main concepts of FAE, and present preliminary results in different multi-dimensional time-series datasets from various domains, including a real dataset from an operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.</li>
</ul>

<h3>Title: Future Slot Prediction for Unsupervised Object Discovery in Surgical Video</h3>
<ul>
<li><strong>Authors: </strong>Guiqiu Liao, Matjaz Jogan, Marcel Hussing, Edward Zhang, Eric Eaton, Daniel A. Hashimoto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01882">https://arxiv.org/abs/2507.01882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01882">https://arxiv.org/pdf/2507.01882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01882]] Future Slot Prediction for Unsupervised Object Discovery in Surgical Video(https://arxiv.org/abs/2507.01882)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Object-centric slot attention is an emerging paradigm for unsupervised learning of structured, interpretable object-centric representations (slots). This enables effective reasoning about objects and events at a low computational cost and is thus applicable to critical healthcare applications, such as real-time interpretation of surgical video. The heterogeneous scenes in real-world applications like surgery are, however, difficult to parse into a meaningful set of slots. Current approaches with an adaptive slot count perform well on images, but their performance on surgical videos is low. To address this challenge, we propose a dynamic temporal slot transformer (DTST) module that is trained both for temporal reasoning and for predicting the optimal future slot initialization. The model achieves state-of-the-art performance on multiple surgical databases, demonstrating that unsupervised object-centric methods can be applied to real-world data and become part of the common arsenal in healthcare applications.</li>
</ul>

<h3>Title: MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants</h3>
<ul>
<li><strong>Authors: </strong>Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01887">https://arxiv.org/abs/2507.01887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01887">https://arxiv.org/pdf/2507.01887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01887]] MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants(https://arxiv.org/abs/2507.01887)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at reasoning tasks requiring long thought sequences for planning, reflection, and refinement. However, their substantial model size and high computational demands are impractical for widespread deployment. Yet, small language models (SLMs) often struggle to learn long-form CoT reasoning due to their limited capacity, a phenomenon we refer to as the "SLMs Learnability Gap". To address this, we introduce \textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation (MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA employs intermediate-sized models as teacher assistants and utilizes intermediate-length CoT sequences to bridge both the capacity and reasoning length gaps. Our experiments on downstream tasks demonstrate that although SLMs distilled from large teachers can perform poorly, by applying MiCoTA, they achieve significant improvements in reasoning performance. Specifically, Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and 3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform a quantitative experiment demonstrating that our method produces data more closely aligned with base SLM distributions. Our insights pave the way for future research into long-CoT data distillation for SLMs.</li>
</ul>

<h3>Title: High-Layer Attention Pruning with Rescaling</h3>
<ul>
<li><strong>Authors: </strong>Songtao Liu, Peng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01900">https://arxiv.org/abs/2507.01900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01900">https://arxiv.org/pdf/2507.01900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01900]] High-Layer Attention Pruning with Rescaling(https://arxiv.org/abs/2507.01900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pruning is a highly effective approach for compressing large language models (LLMs), significantly reducing inference latency. However, conventional training-free structured pruning methods often employ a heuristic metric that indiscriminately removes some attention heads across all pruning layers, without considering their positions within the network architecture. In this work, we propose a novel pruning algorithm that strategically prunes attention heads in the model's higher layers. Since the removal of attention heads can alter the magnitude of token representations, we introduce an adaptive rescaling parameter that calibrates the representation scale post-pruning to counteract this effect. We conduct comprehensive experiments on a wide range of LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our evaluation includes both generation and discriminative tasks across 27 datasets. The results consistently demonstrate that our method outperforms existing structured pruning methods. This improvement is particularly notable in generation tasks, where our approach significantly outperforms existing baselines.</li>
</ul>

<h3>Title: AI4Research: A Survey of Artificial Intelligence for Scientific Research</h3>
<ul>
<li><strong>Authors: </strong>Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01903">https://arxiv.org/abs/2507.01903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01903">https://arxiv.org/pdf/2507.01903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01903]] AI4Research: A Survey of Artificial Intelligence for Scientific Research(https://arxiv.org/abs/2507.01903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.</li>
</ul>

<h3>Title: Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01908">https://arxiv.org/abs/2507.01908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01908">https://arxiv.org/pdf/2507.01908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01908]] Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning(https://arxiv.org/abs/2507.01908)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly.</li>
</ul>

<h3>Title: Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion</h3>
<ul>
<li><strong>Authors: </strong>Jorge Tapias Gomez, Nishant Nadkarni, Lando S. Bosma, Jue Jiang, Ergys D. Subashi, William P. Segars, James M. Balter, Mert R Sabuncu, Neelam Tyagi, Harini Veeraraghavan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01909">https://arxiv.org/abs/2507.01909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01909">https://arxiv.org/pdf/2507.01909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01909]] Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion(https://arxiv.org/abs/2507.01909)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Objective: Clinical implementation of deformable image registration (DIR) requires voxel-based spatial accuracy metrics such as manually identified landmarks, which are challenging to implement for highly mobile gastrointestinal (GI) organs. To address this, patient-specific digital twins (DT) modeling temporally varying motion were created to assess the accuracy of DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D sequences were generated from static 3D patient scans using published analytical GI motion models through a semi-automated pipeline. Eleven datasets, including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars, and three contrast-enhanced CT scans. The motion amplitudes of the DTs were assessed against real patient stomach motion amplitudes extracted from independent 4D MRI datasets. The generated DTs were then used to assess six different DIR methods using target registration error, Dice similarity coefficient, and the 95th percentile Hausdorff distance using summary metrics and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans from patients treated with MR-guided radiation therapy, dose distributions were warped and accumulated to assess dose warping errors, including evaluations of DIR performance in both low- and high-dose regions for patient-specific error estimation. Main results: Our proposed pipeline synthesized DTs modeling realistic GI motion, achieving mean and maximum motion amplitudes and a mean log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to published real-patient gastric motion data. It also enables the extraction of detailed quantitative DIR performance metrics and rigorous validation of dose mapping accuracy. Significance: The pipeline enables rigorously testing DIR tools for dynamic, anatomically complex regions enabling granular spatial and dosimetric accuracies.</li>
</ul>

<h3>Title: 3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP</h3>
<ul>
<li><strong>Authors: </strong>Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01912">https://arxiv.org/abs/2507.01912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01912">https://arxiv.org/pdf/2507.01912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01912]] 3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP(https://arxiv.org/abs/2507.01912)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In orchard automation, dense foliage during the canopy season severely occludes tree structures, minimizing visibility to various canopy parts such as trunks and branches, which limits the ability of a machine vision system. However, canopy structure is more open and visible during the dormant season when trees are defoliated. In this work, we present an information fusion framework that integrates multi-seasonal structural data to support robotic and automated crop load management during the entire growing season. The framework combines high-resolution RGB-D imagery from both dormant and canopy periods using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for model alignment. Segmentation outputs from YOLOv9-Seg were used to extract depth-informed masks, which enabled accurate 3D point cloud reconstruction via Kinect Fusion; these reconstructed models from each season were subsequently aligned using Fast GICP to achieve spatially coherent multi-season fusion. The YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree geometry, validated with field measurements resulting in root mean square errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and 13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal registration with a minimum fitness score of 0.00197, allowing integrated, comprehensive tree structure modeling despite heavy occlusions during the growing season. This fused structural representation enables robotic systems to access otherwise obscured architectural information, improving the precision of pruning, thinning, and other automated orchard operations.</li>
</ul>

<h3>Title: Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01915">https://arxiv.org/abs/2507.01915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01915">https://arxiv.org/pdf/2507.01915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01915]] Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models(https://arxiv.org/abs/2507.01915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.</li>
</ul>

<h3>Title: Decision-oriented Text Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yu-Shiang Huang, Chuan-Ju Wang, Chung-Chi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01923">https://arxiv.org/abs/2507.01923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01923">https://arxiv.org/pdf/2507.01923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01923]] Decision-oriented Text Evaluation(https://arxiv.org/abs/2507.01923)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural language generation (NLG) is increasingly deployed in high-stakes domains, yet common intrinsic evaluation methods, such as n-gram overlap or sentence plausibility, weakly correlate with actual decision-making efficacy. We propose a decision-oriented framework for evaluating generated text by directly measuring its influence on human and large language model (LLM) decision outcomes. Using market digest texts--including objective morning summaries and subjective closing-bell analyses--as test cases, we assess decision quality based on the financial performance of trades executed by human investors and autonomous LLM agents informed exclusively by these texts. Our findings reveal that neither humans nor LLM agents consistently surpass random performance when relying solely on summaries. However, richer analytical commentaries enable collaborative human-LLM teams to outperform individual human or agent baselines significantly. Our approach underscores the importance of evaluating generated text by its ability to facilitate synergistic decision-making between humans and LLMs, highlighting critical limitations of traditional intrinsic metrics.</li>
</ul>

<h3>Title: Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01924">https://arxiv.org/abs/2507.01924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01924">https://arxiv.org/pdf/2507.01924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01924]] Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection(https://arxiv.org/abs/2507.01924)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The complexity of mental healthcare billing enables anomalies, including fraud. While machine learning methods have been applied to anomaly detection, they often struggle with class imbalance, label scarcity, and complex sequential patterns. This study explores a hybrid deep learning approach combining Long Short-Term Memory (LSTM) networks and Transformers, with pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior work has not evaluated such hybrid models trained on pseudo-labeled data in the context of healthcare billing. The approach is evaluated on two real-world billing datasets related to mental healthcare. The iForest LSTM baseline achieves the highest recall (0.963) on declaration-level data. On the operation-level data, the hybrid iForest-based model achieves the highest recall (0.744), though at the cost of lower precision. These findings highlight the potential of combining pseudo-labeling with hybrid deep learning in complex, imbalanced anomaly detection settings.</li>
</ul>

<h3>Title: evMLP: An Efficient Event-Driven MLP Architecture for Vision</h3>
<ul>
<li><strong>Authors: </strong>Zhentan Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01927">https://arxiv.org/abs/2507.01927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01927">https://arxiv.org/pdf/2507.01927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01927]] evMLP: An Efficient Event-Driven MLP Architecture for Vision(https://arxiv.org/abs/2507.01927)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep neural networks have achieved remarkable results in computer vision tasks. In the early days, Convolutional Neural Networks (CNNs) were the mainstream architecture. In recent years, Vision Transformers (ViTs) have become increasingly popular. In addition, exploring applications of multi-layer perceptrons (MLPs) has provided new perspectives for research into vision model architectures. In this paper, we present evMLP accompanied by a simple event-driven local update mechanism. The proposed evMLP can independently process patches on images or feature maps via MLPs. We define changes between consecutive frames as "events". Under the event-driven local update mechanism, evMLP selectively processes patches where events occur. For sequential image data (e.g., video processing), this approach improves computational performance by avoiding redundant computations. Through ImageNet image classification experiments, evMLP attains accuracy competitive with state-of-the-art models. More significantly, experimental results on multiple video datasets demonstrate that evMLP reduces computational cost via its event-driven local update mechanism while maintaining output consistency with its non-event-driven baseline. The code and trained models are available at this https URL.</li>
</ul>

<h3>Title: Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla</h3>
<ul>
<li><strong>Authors: </strong>Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01931">https://arxiv.org/abs/2507.01931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01931">https://arxiv.org/pdf/2507.01931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01931]] Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla(https://arxiv.org/abs/2507.01931)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, neural models trained on large multilingual text and speech datasets have shown great potential for supporting low-resource languages. This study investigates the performances of two state-of-the-art Automatic Speech Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to evaluate model performances. Through systematic fine-tuning and hyperparameter optimization, including learning rate, epochs, and model checkpoint selection, we have compared the models based on Word Error Rate (WER), Character Error Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources, and offered valuable insights to develop robust speech recognition systems in low-resource linguistic settings.</li>
</ul>

<h3>Title: The Thin Line Between Comprehension and Persuasion in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Adrian de Wynter, Tangming Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01936">https://arxiv.org/abs/2507.01936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01936">https://arxiv.org/pdf/2507.01936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01936]] The Thin Line Between Comprehension and Persuasion in LLMs(https://arxiv.org/abs/2507.01936)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are excellent at maintaining high-level, convincing dialogues. They are being fast deployed as chatbots and evaluators in sensitive areas, such as peer review and mental health applications. This, along with the disparate accounts on their reasoning capabilities, calls for a closer examination of LLMs and their comprehension of dialogue. In this work we begin by evaluating LLMs' ability to maintain a debate--one of the purest yet most complex forms of human communication. Then we measure how this capability relates to their understanding of what is being talked about, namely, their comprehension of dialogical structures and the pragmatic context. We find that LLMs are capable of maintaining coherent, persuasive debates, often swaying the beliefs of participants and audiences alike. We also note that awareness or suspicion of AI involvement encourage people to be more critical of the arguments made. When polling LLMs on their comprehension of deeper structures of dialogue, however, they cannot demonstrate said understanding. Our findings tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand the context. More broadly, for the field of argumentation theory we posit that, if an agent can convincingly maintain a dialogue, it is not necessary for it to know what it is talking about. Hence, the modelling of pragmatic context and coherence are secondary to effectiveness.</li>
</ul>

<h3>Title: Kwai Keye-VL Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01949">https://arxiv.org/abs/2507.01949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01949">https://arxiv.org/pdf/2507.01949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01949]] Kwai Keye-VL Technical Report(https://arxiv.org/abs/2507.01949)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today's digital landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode ``cold-start'' data mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think with image'', and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage.</li>
</ul>

<h3>Title: Test-Time Scaling with Reflective Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01951">https://arxiv.org/abs/2507.01951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01951">https://arxiv.org/pdf/2507.01951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01951]] Test-Time Scaling with Reflective Generative Model(https://arxiv.org/abs/2507.01951)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce our first reflective generative model MetaStone-S1, which obtains OpenAI o3's performance via the self-supervised process reward model (SPRM). Through sharing the backbone network and using task-specific heads for next token prediction and process scoring respectively, SPRM successfully integrates the policy model and process reward model(PRM) into a unified interface without extra process annotation, reducing over 99% PRM parameters for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable for test time scaling (TTS), and we provide three reasoning effort modes (low, medium, and high), based on the controllable thinking length. Moreover, we empirically establish a scaling law that reveals the relationship between total thinking computation and TTS performance. Experiments demonstrate that our MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with only 32B parameter size. To support the research community, we have open-sourced MetaStone-S1 at this https URL.</li>
</ul>

<h3>Title: FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yukang Cao, Chenyang Si, Jinghao Wang, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01953">https://arxiv.org/abs/2507.01953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01953">https://arxiv.org/pdf/2507.01953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01953]] FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model(https://arxiv.org/abs/2507.01953)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present FreeMorph, the first tuning-free method for image morphing that accommodates inputs with different semantics or layouts. Unlike existing methods that rely on finetuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without requiring per-instance training. Despite their efficiency and potential, tuning-free methods face challenges in maintaining high-quality results due to the non-linear nature of the multi-step denoising process and biases inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address these challenges by integrating two key innovations. 1) We first propose a guidance-aware spherical interpolation design that incorporates explicit guidance from the input images by modifying the self-attention modules, thereby addressing identity loss and ensuring directional transitions throughout the generated sequence. 2) We further introduce a step-oriented variation trend that blends self-attention modules derived from each input image to achieve controlled and consistent transitions that respect both inputs. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods, being 10x ~ 50x faster and establishing a new state-of-the-art for image morphing.</li>
</ul>

<h3>Title: How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.01955">https://arxiv.org/abs/2507.01955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.01955">https://arxiv.org/pdf/2507.01955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.01955]] How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks(https://arxiv.org/abs/2507.01955)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc). The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework. We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
