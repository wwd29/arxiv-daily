<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: EnSolver: Uncertainty-Aware CAPTCHA Solver Using Deep Ensembles. (arXiv:2307.15180v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15180">http://arxiv.org/abs/2307.15180</a></li>
<li>Code URL: <a href="https://github.com/hoangcongduc/ensolver">https://github.com/hoangcongduc/ensolver</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15180] EnSolver: Uncertainty-Aware CAPTCHA Solver Using Deep Ensembles](http://arxiv.org/abs/2307.15180) #secure</code></li>
<li>Summary: <p>The popularity of text-based CAPTCHA as a security mechanism to protect
websites from automated bots has prompted researches in CAPTCHA solvers, with
the aim of understanding its failure cases and subsequently making CAPTCHAs
more secure. Recently proposed solvers, built on advances in deep learning, are
able to crack even the very challenging CAPTCHAs with high accuracy. However,
these solvers often perform poorly on out-of-distribution samples that contain
visual features different from those in the training set. Furthermore, they
lack the ability to detect and avoid such samples, making them susceptible to
being locked out by defense systems after a certain number of failed attempts.
In this paper, we propose EnSolver, a novel CAPTCHA solver that utilizes deep
ensemble uncertainty estimation to detect and skip out-of-distribution
CAPTCHAs, making it harder to be detected. We demonstrate the use of our solver
with object detection models and show empirically that it performs well on both
in-distribution and out-of-distribution data, achieving up to 98.1% accuracy
when detecting out-of-distribution data and up to 93% success rate when solving
in-distribution CAPTCHAs.
</p></li>
</ul>

<h3>Title: PUF Probe: A PUF-based Hardware Authentication Equipment for IEDs. (arXiv:2307.15338v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15338">http://arxiv.org/abs/2307.15338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15338] PUF Probe: A PUF-based Hardware Authentication Equipment for IEDs](http://arxiv.org/abs/2307.15338) #secure</code></li>
<li>Summary: <p>Intelligent Electronic Devices (IEDs) are vital components in modern
electrical substations, collectively responsible for monitoring electrical
parameters and performing protective functions. As a result, ensuring the
integrity of IEDs is an essential criteria. While standards like IEC 61850 and
IEC 60870-5-104 establish cyber-security protocols for secure information
exchange in IED-based power systems, the physical integrity of IEDs is often
overlooked, leading to a rise in counterfeit and tainted electronic products.
This paper proposes a physical unclonable function (PUF)-based device (IEDPUF
probe) capable of extracting unique hardware signatures from commercial IEDs.
These signatures can serve as identifiers, facilitating the authentication and
protection of IEDs against counterfeiting. The paper presents the complete
hardware architecture of the IEDPUF probe, along with algorithms for signature
extraction and authentication. The process involves the central computer system
(CCS) initiating IED authentication requests by sending random challenges to
the IEDPUF probe. Based on the challenges, the IEDPUF probe generates
responses, which are then verified by the CCS to authenticate the IED.
Additionally, a two-way authentication technique is employed to ensure that
only verified requests are granted access for signature extraction.
Experimental results confirm the efficacy of the proposed IEDPUF probe. The
results demonstrate its ability to provide real-time responses possessing
randomness while uniquely identifying the IED under investigation. The proposed
IEDPUF probe offers a simple, cost-effective, accurate solution with minimal
storage requirements, enhancing the authenticity and integrity of IEDs within
electrical substations
</p></li>
</ul>

<h3>Title: Provably secure KEM-based protocols over unauthenticated channels. (arXiv:2307.15465v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15465">http://arxiv.org/abs/2307.15465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15465] Provably secure KEM-based protocols over unauthenticated channels](http://arxiv.org/abs/2307.15465) #secure</code></li>
<li>Summary: <p>In this paper we propose a number of KEM-based protocols to establish a
shared secret between two parties, and study their resistance over
unauthenticated channels. This means analyzing the security of the protocol
itself, and its robustness against Man-inthe- Middle attacks. We compare them
with their KEX-based counterparts to highlight the differences that arise
naturally, due to the nature of KEM constructions, in terms of the protocol
itself and the types of attacks that they are subject to. We provide practical
go-to KEM-based protocols instances to migrate to, based on the conditions of
currently-in-use KEX-based protocols.
</p></li>
</ul>

<h3>Title: S3C2 Summit 2202-09: Industry Secure Suppy Chain Summit. (arXiv:2307.15642v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15642">http://arxiv.org/abs/2307.15642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15642] S3C2 Summit 2202-09: Industry Secure Suppy Chain Summit](http://arxiv.org/abs/2307.15642) #secure</code></li>
<li>Summary: <p>Recent years have shown increased cyber attacks targeting less secure
elements in the software supply chain and causing fatal damage to businesses
and organizations. Past well-known examples of software supply chain attacks
are the SolarWinds or log4j incidents that have affected thousands of customers
and businesses. The US government and industry are equally interested in
enhancing software supply chain security. We conducted six panel discussions
with a diverse set of 19 practitioners from industry. We asked them open-ended
questions regarding SBOMs, vulnerable dependencies, malicious commits, build
and deploy, the Executive Order, and standards compliance. The goal of this
summit was to enable open discussions, mutual sharing, and shedding light on
common challenges that industry practitioners with practical experience face
when securing their software supply chain. This paper summarizes the summit
held on September 30, 2022.
</p></li>
</ul>

<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis. (arXiv:2307.15424v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15424">http://arxiv.org/abs/2307.15424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15424] Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis](http://arxiv.org/abs/2307.15424) #privacy</code></li>
<li>Summary: <p>This article provides a comprehensive synthesis of the recent developments in
synthetic data generation via deep generative models, focusing on tabular
datasets. We specifically outline the importance of synthetic data generation
in the context of privacy-sensitive data. Additionally, we highlight the
advantages of using deep generative models over other methods and provide a
detailed explanation of the underlying concepts, including unsupervised
learning, neural networks, and generative models. The paper covers the
challenges and considerations involved in using deep generative models for
tabular datasets, such as data normalization, privacy concerns, and model
evaluation. This review provides a valuable resource for researchers and
practitioners interested in synthetic data generation and its applications.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach. (arXiv:2307.15088v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15088">http://arxiv.org/abs/2307.15088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15088] Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach](http://arxiv.org/abs/2307.15088) #protect</code></li>
<li>Summary: <p>Time-varying pricing tariffs incentivize consumers to shift their electricity
demand and reduce costs, but may increase the energy burden for consumers with
limited response capability. The utility must thus balance affordability and
response incentives when designing these tariffs by considering consumers'
response expectations. This paper proposes a joint learning-based
identification and optimization method to design equitable time-varying
tariffs. Our proposed method encodes historical prices and demand response data
into a recurrent neural network (RNN) to capture high-dimensional and
non-linear consumer price response behaviors. We then embed the RNN into the
tariff design optimization, formulating a non-linear optimization problem with
a quadratic objective. We propose a gradient-based solution method that
achieves fast and scalable computation. Simulation using real-world consumer
data shows that our equitable tariffs protect low-income consumers from price
surges while effectively motivating consumers to reduce peak demand. The method
also ensures revenue recovery for the utility company and achieves robust
performance against demand response uncertainties and prediction errors.
</p></li>
</ul>

<h3>Title: The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15398">http://arxiv.org/abs/2307.15398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15398] The Initial Screening Order Problem](http://arxiv.org/abs/2307.15398) #protect</code></li>
<li>Summary: <p>In this paper we present the initial screening order problem, a crucial step
within candidate screening. It involves a human-like screener with an objective
to find the first k suitable candidates rather than the best k suitable
candidates in a candidate pool given an initial screening order. The initial
screening order represents the way in which the human-like screener arranges
the candidate pool prior to screening. The choice of initial screening order
has considerable effects on the selected set of k candidates. We prove that
under an unbalanced candidate pool (e.g., having more male than female
candidates), the human-like screener can suffer from uneven efforts that hinder
its decision-making over the protected, under-represented group relative to the
non-protected, over-represented group. Other fairness results are proven under
the human-like screener. This research is based on a collaboration with a large
company to better understand its hiring process for potential automation. Our
main contribution is the formalization of the initial screening order problem
which, we argue, opens the path for future extensions of the current works on
ranking algorithms, fairness, and automation for screening procedures.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15539">http://arxiv.org/abs/2307.15539</a></li>
<li>Code URL: <a href="https://github.com/damianliumin/non-adversarial_backdoor">https://github.com/damianliumin/non-adversarial_backdoor</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15539] Backdoor Defense with Non-Adversarial Backdoor](http://arxiv.org/abs/2307.15539) #defense</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not
affect the network's performance on clean data but would manipulate the network
behavior once a trigger pattern is added. Existing defense methods have greatly
reduced attack success rate, but their prediction accuracy on clean data still
lags behind a clean model by a large margin. Inspired by the stealthiness and
effectiveness of backdoor attack, we propose a simple but highly effective
defense framework which injects non-adversarial backdoors targeting poisoned
samples. Following the general steps in backdoor attack, we detect a small set
of suspected samples and then apply a poisoning strategy to them. The
non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on
poisoned data, but has limited influence on clean data. The defense can be
carried out during data preprocessing, without any modification to the standard
end-to-end training pipeline. We conduct extensive experiments on multiple
benchmarks with different architectures and representative attacks. Results
demonstrate that our method achieves state-of-the-art defense effectiveness
with by far the lowest performance drop on clean data. Considering the
surprising defense ability displayed by our framework, we call for more
attention to utilizing backdoor for backdoor defense. Code is available at
https://github.com/damianliumin/non-adversarial_backdoor.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Set-Membership Inference Attacks using Data Watermarking. (arXiv:2307.15067v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15067">http://arxiv.org/abs/2307.15067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15067] Set-Membership Inference Attacks using Data Watermarking](http://arxiv.org/abs/2307.15067) #attack</code></li>
<li>Summary: <p>In this work, we propose a set-membership inference attack for generative
models using deep image watermarking techniques. In particular, we demonstrate
how conditional sampling from a generative model can reveal the watermark that
was injected into parts of the training data. Our empirical results demonstrate
that the proposed watermarking technique is a principled approach for detecting
the non-consensual use of image data in training generative models.
</p></li>
</ul>

<h3>Title: Detecting Morphing Attacks via Continual Incremental Training. (arXiv:2307.15105v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15105">http://arxiv.org/abs/2307.15105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15105] Detecting Morphing Attacks via Continual Incremental Training](http://arxiv.org/abs/2307.15105) #attack</code></li>
<li>Summary: <p>Scenarios in which restrictions in data transfer and storage limit the
possibility to compose a single dataset -- also exploiting different data
sources -- to perform a batch-based training procedure, make the development of
robust models particularly challenging. We hypothesize that the recent
Continual Learning (CL) paradigm may represent an effective solution to enable
incremental training, even through multiple sites. Indeed, a basic assumption
of CL is that once a model has been trained, old data can no longer be used in
successive training iterations and in principle can be deleted. Therefore, in
this paper, we investigate the performance of different Continual Learning
methods in this scenario, simulating a learning model that is updated every
time a new chunk of data, even of variable size, is available. Experimental
results reveal that a particular CL method, namely Learning without Forgetting
(LwF), is one of the best-performing algorithms. Then, we investigate its usage
and parametrization in Morphing Attack Detection and Object Classification
tasks, specifically with respect to the amount of new training data that became
available.
</p></li>
</ul>

<h3>Title: Adversarial training for tabular data with attack propagation. (arXiv:2307.15677v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15677">http://arxiv.org/abs/2307.15677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15677] Adversarial training for tabular data with attack propagation](http://arxiv.org/abs/2307.15677) #attack</code></li>
<li>Summary: <p>Adversarial attacks are a major concern in security-centered applications,
where malicious actors continuously try to mislead Machine Learning (ML) models
into wrongly classifying fraudulent activity as legitimate, whereas system
maintainers try to stop them. Adversarially training ML models that are robust
against such attacks can prevent business losses and reduce the work load of
system maintainers. In such applications data is often tabular and the space
available for attackers to manipulate undergoes complex feature engineering
transformations, to provide useful signals for model training, to a space
attackers cannot access. Thus, we propose a new form of adversarial training
where attacks are propagated between the two spaces in the training loop. We
then test this method empirically on a real world dataset in the domain of
credit card fraud detection. We show that our method can prevent about 30%
performance drops under moderate attacks and is essential under very aggressive
attacks, with a trade-off loss in performance under no attacks smaller than 7%.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: R-LPIPS: An Adversarially Robust Perceptual Similarity Metric. (arXiv:2307.15157v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15157">http://arxiv.org/abs/2307.15157</a></li>
<li>Code URL: <a href="https://github.com/saraghazanfari/r-lpips">https://github.com/saraghazanfari/r-lpips</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15157] R-LPIPS: An Adversarially Robust Perceptual Similarity Metric](http://arxiv.org/abs/2307.15157) #robust</code></li>
<li>Summary: <p>Similarity metrics have played a significant role in computer vision to
capture the underlying semantics of images. In recent years, advanced
similarity metrics, such as the Learned Perceptual Image Patch Similarity
(LPIPS), have emerged. These metrics leverage deep features extracted from
trained neural networks and have demonstrated a remarkable ability to closely
align with human perception when evaluating relative image similarity. However,
it is now well-known that neural networks are susceptible to adversarial
examples, i.e., small perturbations invisible to humans crafted to deliberately
mislead the model. Consequently, the LPIPS metric is also sensitive to such
adversarial examples. This susceptibility introduces significant security
concerns, especially considering the widespread adoption of LPIPS in
large-scale applications. In this paper, we propose the Robust Learned
Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages
adversarially trained deep features. Through a comprehensive set of
experiments, we demonstrate the superiority of R-LPIPS compared to the
classical LPIPS metric. The code is available at
\url{https://github.com/SaraGhazanfari/R-LPIPS}.
</p></li>
</ul>

<h3>Title: D2S: Representing local descriptors and global scene coordinates for camera relocalization. (arXiv:2307.15250v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15250">http://arxiv.org/abs/2307.15250</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15250] D2S: Representing local descriptors and global scene coordinates for camera relocalization](http://arxiv.org/abs/2307.15250) #robust</code></li>
<li>Summary: <p>State-of-the-art visual localization methods mostly rely on complex
procedures to match local descriptors and 3D point clouds. However, these
procedures can incur significant cost in terms of inference, storage, and
updates over time. In this study, we propose a direct learning-based approach
that utilizes a simple network named D2S to represent local descriptors and
their scene coordinates. Our method is characterized by its simplicity and
cost-effectiveness. It solely leverages a single RGB image for localization
during the testing phase and only requires a lightweight model to encode a
complex sparse scene. The proposed D2S employs a combination of a simple loss
function and graph attention to selectively focus on robust descriptors while
disregarding areas such as clouds, trees, and several dynamic objects. This
selective attention enables D2S to effectively perform a binary-semantic
classification for sparse descriptors. Additionally, we propose a new outdoor
dataset to evaluate the capabilities of visual localization methods in terms of
scene generalization and self-updating from unlabeled observations. Our
approach outperforms the state-of-the-art CNN-based methods in scene coordinate
regression in indoor and outdoor environments. It demonstrates the ability to
generalize beyond training data, including scenarios involving transitions from
day to night and adapting to domain shifts, even in the absence of the labeled
data sources. The source code, trained models, dataset, and demo videos are
available at the following link: https://thpjp.github.io/d2s
</p></li>
</ul>

<h3>Title: A Solution to Co-occurrence Bias: Attributes Disentanglement via Mutual Information Minimization for Pedestrian Attribute Recognition. (arXiv:2307.15252v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15252">http://arxiv.org/abs/2307.15252</a></li>
<li>Code URL: <a href="https://github.com/sdret/a-solution-to-co-occurence-bias-in-pedestrian-attribute-recognition">https://github.com/sdret/a-solution-to-co-occurence-bias-in-pedestrian-attribute-recognition</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15252] A Solution to Co-occurrence Bias: Attributes Disentanglement via Mutual Information Minimization for Pedestrian Attribute Recognition](http://arxiv.org/abs/2307.15252) #robust</code></li>
<li>Summary: <p>Recent studies on pedestrian attribute recognition progress with either
explicit or implicit modeling of the co-occurrence among attributes.
Considering that this known a prior is highly variable and unforeseeable
regarding the specific scenarios, we show that current methods can actually
suffer in generalizing such fitted attributes interdependencies onto scenes or
identities off the dataset distribution, resulting in the underlined bias of
attributes co-occurrence. To render models robust in realistic scenes, we
propose the attributes-disentangled feature learning to ensure the recognition
of an attribute not inferring on the existence of others, and which is
sequentially formulated as a problem of mutual information minimization.
Rooting from it, practical strategies are devised to efficiently decouple
attributes, which substantially improve the baseline and establish
state-of-the-art performance on realistic datasets like PETAzs and RAPzs. Code
is released on
https://github.com/SDret/A-Solution-to-Co-occurence-Bias-in-Pedestrian-Attribute-Recognition.
</p></li>
</ul>

<h3>Title: Attentive Multimodal Fusion for Optical and Scene Flow. (arXiv:2307.15301v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15301">http://arxiv.org/abs/2307.15301</a></li>
<li>Code URL: <a href="https://github.com/jiesico/fusionraft">https://github.com/jiesico/fusionraft</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15301] Attentive Multimodal Fusion for Optical and Scene Flow](http://arxiv.org/abs/2307.15301) #robust</code></li>
<li>Summary: <p>This paper presents an investigation into the estimation of optical and scene
flow using RGBD information in scenarios where the RGB modality is affected by
noise or captured in dark environments. Existing methods typically rely solely
on RGB images or fuse the modalities at later stages, which can result in lower
accuracy when the RGB information is unreliable. To address this issue, we
propose a novel deep neural network approach named FusionRAFT, which enables
early-stage information fusion between sensor modalities (RGB and depth). Our
approach incorporates self- and cross-attention layers at different network
levels to construct informative features that leverage the strengths of both
modalities. Through comparative experiments, we demonstrate that our approach
outperforms recent methods in terms of performance on the synthetic dataset
Flyingthings3D, as well as the generalization on the real-world dataset KITTI.
We illustrate that our approach exhibits improved robustness in the presence of
noise and low-lighting conditions that affect the RGB images. We release the
code, models and dataset at https://github.com/jiesico/FusionRAFT.
</p></li>
</ul>

<h3>Title: AffineGlue: Joint Matching and Robust Estimation. (arXiv:2307.15381v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15381">http://arxiv.org/abs/2307.15381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15381] AffineGlue: Joint Matching and Robust Estimation](http://arxiv.org/abs/2307.15381) #robust</code></li>
<li>Summary: <p>We propose AffineGlue, a method for joint two-view feature matching and
robust estimation that reduces the combinatorial complexity of the problem by
employing single-point minimal solvers. AffineGlue selects potential matches
from one-to-many correspondences to estimate minimal models. Guided matching is
then used to find matches consistent with the model, suffering less from the
ambiguities of one-to-one matches. Moreover, we derive a new minimal solver for
homography estimation, requiring only a single affine correspondence (AC) and a
gravity prior. Furthermore, we train a neural network to reject ACs that are
unlikely to lead to a good model. AffineGlue is superior to the SOTA on
real-world datasets, even when assuming that the gravity direction points
downwards. On PhotoTourism, the AUC@10{\deg} score is improved by 6.6 points
compared to the SOTA. On ScanNet, AffineGlue makes SuperPoint and SuperGlue
achieve similar accuracy as the detector-free LoFTR.
</p></li>
</ul>

<h3>Title: Few-shot Image Classification based on Gradual Machine Learning. (arXiv:2307.15524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15524">http://arxiv.org/abs/2307.15524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15524] Few-shot Image Classification based on Gradual Machine Learning](http://arxiv.org/abs/2307.15524) #robust</code></li>
<li>Summary: <p>Few-shot image classification aims to accurately classify unlabeled images
using only a few labeled samples. The state-of-the-art solutions are built by
deep learning, which focuses on designing increasingly complex deep backbones.
Unfortunately, the task remains very challenging due to the difficulty of
transferring the knowledge learned in training classes to new ones. In this
paper, we propose a novel approach based on the non-i.i.d paradigm of gradual
machine learning (GML). It begins with only a few labeled observations, and
then gradually labels target images in the increasing order of hardness by
iterative factor inference in a factor graph. Specifically, our proposed
solution extracts indicative feature representations by deep backbones, and
then constructs both unary and binary factors based on the extracted features
to facilitate gradual learning. The unary factors are constructed based on
class center distance in an embedding space, while the binary factors are
constructed based on k-nearest neighborhood. We have empirically validated the
performance of the proposed approach on benchmark datasets by a comparative
study. Our extensive experiments demonstrate that the proposed approach can
improve the SOTA performance by 1-5% in terms of accuracy. More notably, it is
more robust than the existing deep models in that its performance can
consistently improve as the size of query set increases while the performance
of deep models remains essentially flat or even becomes worse.
</p></li>
</ul>

<h3>Title: Point Clouds Are Specialized Images: A Knowledge Transfer Approach for 3D Understanding. (arXiv:2307.15569v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15569">http://arxiv.org/abs/2307.15569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15569] Point Clouds Are Specialized Images: A Knowledge Transfer Approach for 3D Understanding](http://arxiv.org/abs/2307.15569) #robust</code></li>
<li>Summary: <p>Self-supervised representation learning (SSRL) has gained increasing
attention in point cloud understanding, in addressing the challenges posed by
3D data scarcity and high annotation costs. This paper presents PCExpert, a
novel SSRL approach that reinterprets point clouds as "specialized images".
This conceptual shift allows PCExpert to leverage knowledge derived from
large-scale image modality in a more direct and deeper manner, via extensively
sharing the parameters with a pre-trained image encoder in a multi-way
Transformer architecture. The parameter sharing strategy, combined with a novel
pretext task for pre-training, i.e., transformation estimation, empowers
PCExpert to outperform the state of the arts in a variety of tasks, with a
remarkable reduction in the number of trainable parameters. Notably, PCExpert's
performance under LINEAR fine-tuning (e.g., yielding a 90.02% overall accuracy
on ScanObjectNN) has already approached the results obtained with FULL model
fine-tuning (92.66%), demonstrating its effective and robust representation
capability.
</p></li>
</ul>

<h3>Title: PatchMixer: Rethinking network design to boost generalization for 3D point cloud understanding. (arXiv:2307.15692v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15692">http://arxiv.org/abs/2307.15692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15692] PatchMixer: Rethinking network design to boost generalization for 3D point cloud understanding](http://arxiv.org/abs/2307.15692) #robust</code></li>
<li>Summary: <p>The recent trend in deep learning methods for 3D point cloud understanding is
to propose increasingly sophisticated architectures either to better capture 3D
geometries or by introducing possibly undesired inductive biases. Moreover,
prior works introducing novel architectures compared their performance on the
same domain, devoting less attention to their generalization to other domains.
We argue that the ability of a model to transfer the learnt knowledge to
different domains is an important feature that should be evaluated to
exhaustively assess the quality of a deep network architecture. In this work we
propose PatchMixer, a simple yet effective architecture that extends the ideas
behind the recent MLP-Mixer paper to 3D point clouds. The novelties of our
approach are the processing of local patches instead of the whole shape to
promote robustness to partial point clouds, and the aggregation of patch-wise
features using an MLP as a simpler alternative to the graph convolutions or the
attention mechanisms that are used in prior works. We evaluated our method on
the shape classification and part segmentation tasks, achieving superior
generalization performance compared to a selection of the most relevant deep
architectures.
</p></li>
</ul>

<h3>Title: The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems. (arXiv:2307.15493v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15493">http://arxiv.org/abs/2307.15493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15493] The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems](http://arxiv.org/abs/2307.15493) #robust</code></li>
<li>Summary: <p>Speech recognition systems are a key intermediary in voice-driven
human-computer interaction. Although speech recognition works well for pristine
monologic audio, real-life use cases in open-ended interactive settings still
present many challenges. We argue that timing is mission-critical for dialogue
systems, and evaluate 5 major commercial ASR systems for their conversational
and multilingual support. We find that word error rates for natural
conversational data in 6 languages remain abysmal, and that overlap remains a
key challenge (study 1). This impacts especially the recognition of
conversational words (study 2), and in turn has dire consequences for
downstream intent recognition (study 3). Our findings help to evaluate the
current state of conversational ASR, contribute towards multidimensional error
analysis and evaluation, and identify phenomena that need most attention on the
way to build robust interactive speech technologies.
</p></li>
</ul>

<h3>Title: Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15593">http://arxiv.org/abs/2307.15593</a></li>
<li>Code URL: <a href="https://github.com/jthickstun/watermark">https://github.com/jthickstun/watermark</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15593] Robust Distortion-free Watermarks for Language Models](http://arxiv.org/abs/2307.15593) #robust</code></li>
<li>Summary: <p>We propose a methodology for planting watermarks in text from an
autoregressive language model that are robust to perturbations without changing
the distribution over text up to a certain maximum generation budget. We
generate watermarked text by mapping a sequence of random numbers -- which we
compute using a randomized watermark key -- to a sample from the language
model. To detect watermarked text, any party who knows the key can align the
text to the random number sequence. We instantiate our watermark methodology
with two sampling schemes: inverse transform sampling and exponential minimum
sampling. We apply these watermarks to three language models -- OPT-1.3B,
LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power
and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B
and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq
0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens
via random edits (i.e., substitutions, insertions or deletions). For the
Alpaca-7B model, we conduct a case study on the feasibility of watermarking
responses to typical user instructions. Due to the lower entropy of the
responses, detection is more difficult: around $25\%$ of the responses -- whose
median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and
the watermark is also less robust to certain automated paraphrasing attacks we
implement.
</p></li>
</ul>

<h3>Title: A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity. (arXiv:2307.15154v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15154">http://arxiv.org/abs/2307.15154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15154] A/B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity](http://arxiv.org/abs/2307.15154) #robust</code></li>
<li>Summary: <p>We investigate the fixed-budget best-arm identification (BAI) problem for
linear bandits in a potentially non-stationary environment. Given a finite arm
set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable
sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an
algorithm will aim to correctly identify the best arm $x^<em> :=
\arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as
high as possible. Prior work has addressed the stationary setting where
$\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability
decreases as $\exp(-T /\rho^</em>)$ for a problem-dependent constant $\rho^<em>$. But
in many real-world $A/B/n$ multivariate testing scenarios that motivate our
work, the environment is non-stationary and an algorithm expecting a stationary
setting can easily fail. For robust identification, it is well-known that if
arms are chosen randomly and non-adaptively from a G-optimal design over
$\mathcal{X}$ at each time then the error probability decreases as
$\exp(-T\Delta^2_{(1)}/d)$, where $\Delta_{(1)} = \min_{x \neq x^</em>} (x^<em> -
x)^\top \frac{1}{T}\sum_{t=1}^T \theta_t$. As there exist environments where
$\Delta_{(1)}^2/ d \ll 1/ \rho^</em>$, we are motivated to propose a novel
algorithm $\mathsf{P1}$-$\mathsf{RAGE}$ that aims to obtain the best of both
worlds: robustness to non-stationarity and fast rates of identification in
benign settings. We characterize the error probability of
$\mathsf{P1}$-$\mathsf{RAGE}$ and demonstrate empirically that the algorithm
indeed never performs worse than G-optimal design but compares favorably to the
best algorithms in the stationary setting.
</p></li>
</ul>

<h3>Title: Worrisome Properties of Neural Network Controllers and Their Symbolic Representations. (arXiv:2307.15456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15456">http://arxiv.org/abs/2307.15456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15456] Worrisome Properties of Neural Network Controllers and Their Symbolic Representations](http://arxiv.org/abs/2307.15456) #robust</code></li>
<li>Summary: <p>We raise concerns about controllers' robustness in simple reinforcement
learning benchmark problems. We focus on neural network controllers and their
low neuron and symbolic abstractions. A typical controller reaching high mean
return values still generates an abundance of persistent low-return solutions,
which is a highly undesirable property, easily exploitable by an adversary. We
find that the simpler controllers admit more persistent bad solutions. We
provide an algorithm for a systematic robustness study and prove existence of
persistent solutions and, in some cases, periodic orbits, using a
computer-assisted proof methodology.
</p></li>
</ul>

<h3>Title: From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs. (arXiv:2307.15496v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15496">http://arxiv.org/abs/2307.15496</a></li>
<li>Code URL: <a href="https://github.com/lorenzrichter/PDE-backward-solver">https://github.com/lorenzrichter/PDE-backward-solver</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15496] From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs](http://arxiv.org/abs/2307.15496) #robust</code></li>
<li>Summary: <p>The numerical approximation of partial differential equations (PDEs) poses
formidable challenges in high dimensions since classical grid-based methods
suffer from the so-called curse of dimensionality. Recent attempts rely on a
combination of Monte Carlo methods and variational formulations, using neural
networks for function approximation. Extending previous work (Richter et al.,
2021), we argue that tensor trains provide an appealing framework for parabolic
PDEs: The combination of reformulations in terms of backward stochastic
differential equations and regression-type methods holds the promise of
leveraging latent low-rank structures, enabling both compression and efficient
computation. Emphasizing a continuous-time viewpoint, we develop iterative
schemes, which differ in terms of computational efficiency and robustness. We
demonstrate both theoretically and numerically that our methods can achieve a
favorable trade-off between accuracy and computational efficiency. While
previous methods have been either accurate or fast, we have identified a novel
numerical strategy that can often combine both of these aspects.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data. (arXiv:2307.15198v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15198">http://arxiv.org/abs/2307.15198</a></li>
<li>Code URL: <a href="https://github.com/anonymous4545/jers">https://github.com/anonymous4545/jers</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15198] One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data](http://arxiv.org/abs/2307.15198) #extraction</code></li>
<li>Summary: <p>Brain extraction, registration and segmentation are indispensable
preprocessing steps in neuroimaging studies. The aim is to extract the brain
from raw imaging scans (i.e., extraction step), align it with a target brain
image (i.e., registration step) and label the anatomical brain regions (i.e.,
segmentation step). Conventional studies typically focus on developing separate
methods for the extraction, registration and segmentation tasks in a supervised
setting. The performance of these methods is largely contingent on the quantity
of training samples and the extent of visual inspections carried out by experts
for error correction. Nevertheless, collecting voxel-level labels and
performing manual quality control on high-dimensional neuroimages (e.g., 3D
MRI) are expensive and time-consuming in many medical studies. In this paper,
we study the problem of one-shot joint extraction, registration and
segmentation in neuroimaging data, which exploits only one labeled template
image (a.k.a. atlas) and a few unlabeled raw images for training. We propose a
unified end-to-end framework, called JERS, to jointly optimize the extraction,
registration and segmentation tasks, allowing feedback among them.
Specifically, we use a group of extraction, registration and segmentation
modules to learn the extraction mask, transformation and segmentation mask,
where modules are interconnected and mutually reinforced by self-supervision.
Empirical results on real-world datasets demonstrate that our proposed method
performs exceptionally in the extraction, registration and segmentation tasks.
Our code and data can be found at https://github.com/Anonymous4545/JERS
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design. (arXiv:2307.15245v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15245">http://arxiv.org/abs/2307.15245</a></li>
<li>Code URL: <a href="https://github.com/mmorafah/fedzoo-bench">https://github.com/mmorafah/fedzoo-bench</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15245] A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design](http://arxiv.org/abs/2307.15245) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) has been an area of active research in recent years.
There have been numerous studies in FL to make it more successful in the
presence of data heterogeneity. However, despite the existence of many
publications, the state of progress in the field is unknown. Many of the works
use inconsistent experimental settings and there are no comprehensive studies
on the effect of FL-specific experimental variables on the results and
practical insights for a more comparable and consistent FL experimental setup.
Furthermore, the existence of several benchmarks and confounding variables has
further complicated the issue of inconsistency and ambiguity. In this work, we
present the first comprehensive study on the effect of FL-specific experimental
variables in relation to each other and performance results, bringing several
insights and recommendations for designing a meaningful and well-incentivized
FL experimental setup. We further aid the community by releasing FedZoo-Bench,
an open-source library based on PyTorch with pre-implementation of 22
state-of-the-art methods, and a broad set of standardized and customizable
features available at https://github.com/MMorafah/FedZoo-Bench. We also provide
a comprehensive comparison of several state-of-the-art (SOTA) methods to better
understand the current state of the field and existing limitations.
</p></li>
</ul>

<h3>Title: The Applicability of Federated Learning to Official Statistics. (arXiv:2307.15503v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15503">http://arxiv.org/abs/2307.15503</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15503] The Applicability of Federated Learning to Official Statistics](http://arxiv.org/abs/2307.15503) #federate</code></li>
<li>Summary: <p>This work investigates the potential of Federated Learning (FL) for official
statistics and shows how well the performance of FL models can keep up with
centralized learning methods. At the same time, its utilization can safeguard
the privacy of data holders, thus facilitating access to a broader range of
data and ultimately enhancing official statistics. By simulating three
different use cases, important insights on the applicability of the technology
are gained. The use cases are based on a medical insurance data set, a fine
dust pollution data set and a mobile radio coverage data set - all of which are
from domains close to official statistics. We provide a detailed analysis of
the results, including a comparison of centralized and FL algorithm
performances for each simulation. In all three use cases, we were able to train
models via FL which reach a performance very close to the centralized model
benchmarks. Our key observations and their implications for transferring the
simulations into practice are summarized. We arrive at the conclusion that FL
has the potential to emerge as a pivotal technology in future use cases of
official statistics.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Is this model reliable for everyone? Testing for strong calibration. (arXiv:2307.15247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15247">http://arxiv.org/abs/2307.15247</a></li>
<li>Code URL: <a href="https://github.com/jjfeng/testing_strong_calibration">https://github.com/jjfeng/testing_strong_calibration</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15247] Is this model reliable for everyone? Testing for strong calibration](http://arxiv.org/abs/2307.15247) #fair</code></li>
<li>Summary: <p>In a well-calibrated risk prediction model, the average predicted probability
is close to the true event rate for any given subgroup. Such models are
reliable across heterogeneous populations and satisfy strong notions of
algorithmic fairness. However, the task of auditing a model for strong
calibration is well-known to be difficult -- particularly for machine learning
(ML) algorithms -- due to the sheer number of potential subgroups. As such,
common practice is to only assess calibration with respect to a few predefined
subgroups. Recent developments in goodness-of-fit testing offer potential
solutions but are not designed for settings with weak signal or where the
poorly calibrated subgroup is small, as they either overly subdivide the data
or fail to divide the data at all. We introduce a new testing procedure based
on the following insight: if we can reorder observations by their expected
residuals, there should be a change in the association between the predicted
and observed residuals along this sequence if a poorly calibrated subgroup
exists. This lets us reframe the problem of calibration testing into one of
changepoint detection, for which powerful methods already exist. We begin with
introducing a sample-splitting procedure where a portion of the data is used to
train a suite of candidate models for predicting the residual, and the
remaining data are used to perform a score-based cumulative sum (CUSUM) test.
To further improve power, we then extend this adaptive CUSUM test to
incorporate cross-validation, while maintaining Type I error control under
minimal assumptions. Compared to existing methods, the proposed procedure
consistently achieved higher power in simulation studies and more than doubled
the power when auditing a mortality risk prediction model.
</p></li>
</ul>

<h3>Title: LUCID-GAN: Conditional Generative Models to Locate Unfairness. (arXiv:2307.15466v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15466">http://arxiv.org/abs/2307.15466</a></li>
<li>Code URL: <a href="https://github.com/integrated-intelligence-lab/canonical_sets">https://github.com/integrated-intelligence-lab/canonical_sets</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15466] LUCID-GAN: Conditional Generative Models to Locate Unfairness](http://arxiv.org/abs/2307.15466) #fair</code></li>
<li>Summary: <p>Most group fairness notions detect unethical biases by computing statistical
parity metrics on a model's output. However, this approach suffers from several
shortcomings, such as philosophical disagreement, mutual incompatibility, and
lack of interpretability. These shortcomings have spurred the research on
complementary bias detection methods that offer additional transparency into
the sources of discrimination and are agnostic towards an a priori decision on
the definition of fairness and choice of protected features. A recent proposal
in this direction is LUCID (Locating Unfairness through Canonical Inverse
Design), where canonical sets are generated by performing gradient descent on
the input space, revealing a model's desired input given a preferred output.
This information about the model's mechanisms, i.e., which feature values are
essential to obtain specific outputs, allows exposing potential unethical
biases in its internal logic. Here, we present LUCID-GAN, which generates
canonical inputs via a conditional generative model instead of gradient-based
inverse design. LUCID-GAN has several benefits, including that it applies to
non-differentiable models, ensures that canonical sets consist of realistic
inputs, and allows to assess proxy and intersectional discrimination. We
empirically evaluate LUCID-GAN on the UCI Adult and COMPAS data sets and show
that it allows for detecting unethical biases in black-box models without
requiring access to the training data.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Toward Transparent Sequence Models with Model-Based Tree Markov Model. (arXiv:2307.15367v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15367">http://arxiv.org/abs/2307.15367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15367] Toward Transparent Sequence Models with Model-Based Tree Markov Model](http://arxiv.org/abs/2307.15367) #interpretability</code></li>
<li>Summary: <p>In this study, we address the interpretability issue in complex, black-box
Machine Learning models applied to sequence data. We introduce the Model-Based
tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model
aimed at detecting high mortality risk events and discovering hidden patterns
associated with the mortality risk in Intensive Care Units (ICU). This model
leverages knowledge distilled from Deep Neural Networks (DNN) to enhance
predictive performance while offering clear explanations. Our experimental
results indicate the improved performance of Model-Based trees (MOB trees) via
employing LSTM for learning sequential patterns, which are then transferred to
MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in
the MOB-HSMM enables uncovering potential and explainable sequences using
available information.
</p></li>
</ul>

<h3>Title: Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity. (arXiv:2307.15672v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15672">http://arxiv.org/abs/2307.15672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15672] Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity](http://arxiv.org/abs/2307.15672) #interpretability</code></li>
<li>Summary: <p>Understanding how external stimuli are encoded in distributed neural activity
is of significant interest in clinical and basic neuroscience. To address this
need, it is essential to develop analytical tools capable of handling limited
data and the intrinsic stochasticity present in neural data. In this study, we
propose a straightforward Bayesian time series classifier (BTsC) model that
tackles these challenges whilst maintaining a high level of interpretability.
We demonstrate the classification capabilities of this approach by utilizing
neural data to decode colors in a visual task. The model exhibits consistent
and reliable average performance of 75.55% on 4 patients' dataset, improving
upon state-of-the-art machine learning techniques by about 3.0 percent. In
addition to its high classification accuracy, the proposed BTsC model provides
interpretable results, making the technique a valuable tool to study neural
activity in various tasks and categories. The proposed solution can be applied
to neural data recorded in various tasks, where there is a need for
interpretable results and accurate classification accuracy.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture. (arXiv:2307.15273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15273">http://arxiv.org/abs/2307.15273</a></li>
<li>Code URL: <a href="https://github.com/jbartlett6/sdnet">https://github.com/jbartlett6/sdnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15273] Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture](http://arxiv.org/abs/2307.15273) #diffusion</code></li>
<li>Summary: <p>Fibre orientation distribution (FOD) reconstruction using deep learning has
the potential to produce accurate FODs from a reduced number of
diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion
acquisition invariant representations of the DWI signals are typically used as
input to these methods to ensure that they can be applied flexibly to data with
different b-vectors and b-values; however, this means the network cannot
condition its output directly on the DWI signal. In this work, we propose a
spherical deconvolution network, a model-driven deep learning FOD
reconstruction architecture, that ensures intermediate and output FODs produced
by the network are consistent with the input DWI signals. Furthermore, we
implement a fixel classification penalty within our loss function, encouraging
the network to produce FODs that can subsequently be segmented into the correct
number of fixels and improve downstream fixel-based analysis. Our results show
that the model-based deep learning architecture achieves competitive
performance compared to a state-of-the-art FOD super-resolution network,
FOD-Net. Moreover, we show that the fixel classification penalty can be tuned
to offer improved performance with respect to metrics that rely on accurately
segmented of FODs. Our code is publicly available at
https://github.com/Jbartlett6/SDNet .
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Writer adaptation for offline text recognition: An exploration of neural network-based methods. (arXiv:2307.15071v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15071">http://arxiv.org/abs/2307.15071</a></li>
<li>Code URL: <a href="https://github.com/tobiasvanderwerff/master-thesis">https://github.com/tobiasvanderwerff/master-thesis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15071] Writer adaptation for offline text recognition: An exploration of neural network-based methods](http://arxiv.org/abs/2307.15071) #transformer</code></li>
<li>Summary: <p>Handwriting recognition has seen significant success with the use of deep
learning. However, a persistent shortcoming of neural networks is that they are
not well-equipped to deal with shifting data distributions. In the field of
handwritten text recognition (HTR), this shows itself in poor recognition
accuracy for writers that are not similar to those seen during training. An
ideal HTR model should be adaptive to new writing styles in order to handle the
vast amount of possible writing styles. In this paper, we explore how HTR
models can be made writer adaptive by using only a handful of examples from a
new writer (e.g., 16 examples) for adaptation. Two HTR architectures are used
as base models, using a ResNet backbone along with either an LSTM or
Transformer sequence decoder. Using these base models, two methods are
considered to make them writer adaptive: 1) model-agnostic meta-learning
(MAML), an algorithm commonly used for tasks such as few-shot classification,
and 2) writer codes, an idea originating from automatic speech recognition.
Results show that an HTR-specific version of MAML known as MetaHTR improves
performance compared to the baseline with a 1.4 to 2.0 improvement in word
error rate (WER). The improvement due to writer adaptation is between 0.2 and
0.7 WER, where a deeper model seems to lend itself better to adaptation using
MetaHTR than a shallower model. However, applying MetaHTR to larger HTR models
or sentence-level HTR may become prohibitive due to its high computational and
memory requirements. Lastly, writer codes based on learned features or Hinge
statistical features did not lead to improved recognition performance.
</p></li>
</ul>

<h3>Title: DocDeshadower: Frequency-aware Transformer for Document Shadow Removal. (arXiv:2307.15318v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15318">http://arxiv.org/abs/2307.15318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15318] DocDeshadower: Frequency-aware Transformer for Document Shadow Removal](http://arxiv.org/abs/2307.15318) #transformer</code></li>
<li>Summary: <p>The presence of shadows significantly impacts the visual quality of scanned
documents. However, the existing traditional techniques and deep learning
methods used for shadow removal have several limitations. These methods either
rely heavily on heuristics, resulting in suboptimal performance, or require
large datasets to learn shadow-related features. In this study, we propose the
DocDeshadower, a multi-frequency Transformer-based model built on Laplacian
Pyramid. DocDeshadower is designed to remove shadows at different frequencies
in a coarse-to-fine manner. To achieve this, we decompose the shadow image into
different frequency bands using Laplacian Pyramid. In addition, we introduce
two novel components to this model: the Attention-Aggregation Network and the
Gated Multi-scale Fusion Transformer. The Attention-Aggregation Network is
designed to remove shadows in the low-frequency part of the image, whereas the
Gated Multi-scale Fusion Transformer refines the entire image at a global scale
with its large perceptive field. Our extensive experiments demonstrate that
DocDeshadower outperforms the current state-of-the-art methods in both
qualitative and quantitative terms.
</p></li>
</ul>

<h3>Title: TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts. (arXiv:2307.15324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15324">http://arxiv.org/abs/2307.15324</a></li>
<li>Code URL: <a href="https://github.com/prismformore/multi-task-transformer">https://github.com/prismformore/multi-task-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15324] TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts](http://arxiv.org/abs/2307.15324) #transformer</code></li>
<li>Summary: <p>Learning discriminative task-specific features simultaneously for multiple
distinct tasks is a fundamental problem in multi-task learning. Recent
state-of-the-art models consider directly decoding task-specific features from
one shared task-generic feature (e.g., feature from a backbone layer), and
utilize carefully designed decoders to produce multi-task features. However, as
the input feature is fully shared and each task decoder also shares decoding
parameters for different input samples, it leads to a static feature decoding
process, producing less discriminative task-specific representations. To tackle
this limitation, we propose TaskExpert, a novel multi-task mixture-of-experts
model that enables learning multiple representative task-generic feature spaces
and decoding task-specific features in a dynamic manner. Specifically,
TaskExpert introduces a set of expert networks to decompose the backbone
feature into several representative task-generic features. Then, the
task-specific features are decoded by using dynamic task-specific gating
networks operating on the decomposed task-generic features. Furthermore, to
establish long-range modeling of the task-specific representations from
different layers of TaskExpert, we design a multi-task feature memory that
updates at each layer and acts as an additional feature expert for dynamic
task-specific feature decoding. Extensive experiments demonstrate that our
TaskExpert clearly outperforms previous best-performing methods on all 9
metrics of two competitive multi-task learning benchmarks for visual scene
understanding (i.e., PASCAL-Context and NYUD-v2). Codes and models will be made
publicly available at https://github.com/prismformore/Multi-Task-Transformer
</p></li>
</ul>

<h3>Title: BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering. (arXiv:2307.15335v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15335">http://arxiv.org/abs/2307.15335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15335] BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering](http://arxiv.org/abs/2307.15335) #transformer</code></li>
<li>Summary: <p>Visual Question Answering (VQA) is an intricate and demanding task that
integrates natural language processing (NLP) and computer vision (CV),
capturing the interest of researchers. The English language, renowned for its
wealth of resources, has witnessed notable advancements in both datasets and
models designed for VQA. However, there is a lack of models that target
specific countries such as Vietnam. To address this limitation, we introduce a
transformer-based Vietnamese model named BARTPhoBEiT. This model includes
pre-trained Sequence-to-Sequence and bidirectional encoder representation from
Image Transformers in Vietnamese and evaluates Vietnamese VQA datasets.
Experimental results demonstrate that our proposed model outperforms the strong
baseline and improves the state-of-the-art in six metrics: Accuracy, Precision,
Recall, F1-score, WUPS 0.0, and WUPS 0.9.
</p></li>
</ul>

<h3>Title: Prompt Guided Transformer for Multi-Task Dense Prediction. (arXiv:2307.15362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15362">http://arxiv.org/abs/2307.15362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15362] Prompt Guided Transformer for Multi-Task Dense Prediction](http://arxiv.org/abs/2307.15362) #transformer</code></li>
<li>Summary: <p>Task-conditional architecture offers advantage in parameter efficiency but
falls short in performance compared to state-of-the-art multi-decoder methods.
How to trade off performance and model parameters is an important and difficult
problem. In this paper, we introduce a simple and lightweight task-conditional
model called Prompt Guided Transformer (PGT) to optimize this challenge. Our
approach designs a Prompt-conditioned Transformer block, which incorporates
task-specific prompts in the self-attention mechanism to achieve global
dependency modeling and parameter-efficient feature adaptation across multiple
tasks. This block is integrated into both the shared encoder and decoder,
enhancing the capture of intra- and inter-task features. Moreover, we design a
lightweight decoder to further reduce parameter usage, which accounts for only
2.7% of the total model parameters. Extensive experiments on two multi-task
dense prediction benchmarks, PASCAL-Context and NYUD-v2, demonstrate that our
approach achieves state-of-the-art results among task-conditional methods while
using fewer parameters, and maintains a significant balance between performance
and parameter size.
</p></li>
</ul>

<h3>Title: MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking. (arXiv:2307.15700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15700">http://arxiv.org/abs/2307.15700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15700] MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking](http://arxiv.org/abs/2307.15700) #transformer</code></li>
<li>Summary: <p>As a video task, Multi-Object Tracking (MOT) is expected to capture temporal
information of targets effectively. Unfortunately, most existing methods only
explicitly exploit the object features between adjacent frames, while lacking
the capacity to model long-term temporal information. In this paper, we propose
MeMOTR, a long-term memory-augmented Transformer for multi-object tracking. Our
method is able to make the same object's track embedding more stable and
distinguishable by leveraging long-term memory injection with a customized
memory-attention layer. This significantly improves the target association
ability of our model. Experimental results on DanceTrack show that MeMOTR
impressively surpasses the state-of-the-art method by 7.9\% and 13.0\% on HOTA
and AssA metrics, respectively. Furthermore, our model also outperforms other
Transformer-based methods on association performance on MOT17 and generalizes
well on BDD100K. Code is available at
\href{https://github.com/MCG-NJU/MeMOTR}{https://github.com/MCG-NJU/MeMOTR}.
</p></li>
</ul>

<h3>Title: Cascaded Cross-Modal Transformer for Request and Complaint Detection. (arXiv:2307.15097v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15097">http://arxiv.org/abs/2307.15097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15097] Cascaded Cross-Modal Transformer for Request and Complaint Detection](http://arxiv.org/abs/2307.15097) #transformer</code></li>
<li>Summary: <p>We propose a novel cascaded cross-modal transformer (CCMT) that combines
speech and text transcripts to detect customer requests and complaints in phone
conversations. Our approach leverages a multimodal paradigm by transcribing the
speech using automatic speech recognition (ASR) models and translating the
transcripts into different languages. Subsequently, we combine
language-specific BERT-based models with Wav2Vec2.0 audio features in a novel
cascaded cross-attention transformer model. We apply our system to the Requests
Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics
Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for
the complaint and request classes, respectively.
</p></li>
</ul>

<h3>Title: VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15164">http://arxiv.org/abs/2307.15164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15164] VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings](http://arxiv.org/abs/2307.15164) #transformer</code></li>
<li>Summary: <p>Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion
Classification from essays written in reaction to news articles. Emotion
detection from complex dialogues is challenging and often requires
context/domain understanding. Therefore in this research, we have focused on
developing deep learning (DL) models using the combination of word embedding
representations with tailored prepossessing strategies to capture the nuances
of emotions expressed. Our experiments used static and contextual embeddings
(individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and
Transformer based models. We occupied rank tenth in the emotion detection task
by scoring a Macro F1-Score of 0.2717, validating the efficacy of our
implemented approaches for small and imbalanced datasets with mixed categories
of target emotions.
</p></li>
</ul>

<h3>Title: WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories. (arXiv:2307.15293v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15293">http://arxiv.org/abs/2307.15293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15293] WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories](http://arxiv.org/abs/2307.15293) #transformer</code></li>
<li>Summary: <p>Our research focuses on solving the zero-shot text classification problem in
NLP, with a particular emphasis on innovative self-training strategies. To
achieve this objective, we propose a novel self-training strategy that uses
labels rather than text for training, significantly reducing the model's
training time. Specifically, we use categories from Wikipedia as our training
set and leverage the SBERT pre-trained model to establish positive correlations
between pairs of categories within the same text, facilitating associative
training. For new test datasets, we have improved the original self-training
approach, eliminating the need for prior training and testing data from each
target dataset. Instead, we adopt Wikipedia as a unified training dataset to
better approximate the zero-shot scenario. This modification allows for rapid
fine-tuning and inference across different datasets, greatly reducing the time
required for self-training. Our experimental results demonstrate that this
method can adapt the model to the target dataset within minutes. Compared to
other BERT-based transformer models, our approach significantly reduces the
amount of training data by training only on labels, not the actual text, and
greatly improves training efficiency by utilizing a unified training set.
Additionally, our method achieves state-of-the-art results on both the Yahoo
Topic and AG News datasets.
</p></li>
</ul>

<h3>Title: The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling. (arXiv:2307.15508v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15508">http://arxiv.org/abs/2307.15508</a></li>
<li>Code URL: <a href="https://github.com/briemadu/inc-eval-revisions">https://github.com/briemadu/inc-eval-revisions</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15508] The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling](http://arxiv.org/abs/2307.15508) #transformer</code></li>
<li>Summary: <p>Incremental dialogue model components produce a sequence of output prefixes
based on incoming input. Mistakes can occur due to local ambiguities or to
wrong hypotheses, making the ability to revise past outputs a desirable
property that can be governed by a policy. In this work, we formalise and
characterise edits and revisions in incremental sequence labelling and propose
metrics to evaluate revision policies. We then apply our methodology to profile
the incremental behaviour of three Transformer-based encoders in various tasks,
paving the road for better revision policies.
</p></li>
</ul>

<h3>Title: Universal Recurrent Event Memories for Streaming Data. (arXiv:2307.15694v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15694">http://arxiv.org/abs/2307.15694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15694] Universal Recurrent Event Memories for Streaming Data](http://arxiv.org/abs/2307.15694) #transformer</code></li>
<li>Summary: <p>In this paper, we propose a new event memory architecture (MemNet) for
recurrent neural networks, which is universal for different types of time
series data such as scalar, multivariate or symbolic. Unlike other external
neural memory architectures, it stores key-value pairs, which separate the
information for addressing and for content to improve the representation, as in
the digital archetype. Moreover, the key-value pairs also avoid the compromise
between memory depth and resolution that applies to memories constructed by the
model state. One of the MemNet key characteristics is that it requires only
linear adaptive mapping functions while implementing a nonlinear operation on
the input data. MemNet architecture can be applied without modifications to
scalar time series, logic operators on strings, and also to natural language
processing, providing state-of-the-art results in all application domains such
as the chaotic time series, the symbolic operation tasks, and the
question-answering tasks (bAbI). Finally, controlled by five linear layers,
MemNet requires a much smaller number of training parameters than other
external memory networks as well as the transformer network. The space
complexity of MemNet equals a single self-attention layer. It greatly improves
the efficiency of the attention mechanism and opens the door for IoT
applications.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Med-Flamingo: a Multimodal Medical Few-shot Learner. (arXiv:2307.15189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15189">http://arxiv.org/abs/2307.15189</a></li>
<li>Code URL: <a href="https://github.com/snap-stanford/med-flamingo">https://github.com/snap-stanford/med-flamingo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15189] Med-Flamingo: a Multimodal Medical Few-shot Learner](http://arxiv.org/abs/2307.15189) #generative</code></li>
<li>Summary: <p>Medicine, by its nature, is a multifaceted domain that requires the synthesis
of information across various modalities. Medical generative vision-language
models (VLMs) make a first step in this direction and promise many exciting
clinical applications. However, existing models typically have to be fine-tuned
on sizeable down-stream datasets, which poses a significant limitation as in
many medical applications data is scarce, necessitating models that are capable
of learning from few examples in real-time. Here we propose Med-Flamingo, a
multimodal few-shot learner adapted to the medical domain. Based on
OpenFlamingo-9B, we continue pre-training on paired and interleaved medical
image-text data from publications and textbooks. Med-Flamingo unlocks few-shot
generative medical visual question answering (VQA) abilities, which we evaluate
on several datasets including a novel challenging open-ended VQA dataset of
visual USMLE-style problems. Furthermore, we conduct the first human evaluation
for generative medical VQA where physicians review the problems and blinded
generations in an interactive app. Med-Flamingo improves performance in
generative medical VQA by up to 20\% in clinician's rating and firstly enables
multimodal medical few-shot adaptations, such as rationale generation. We
release our model, code, and evaluation app under
https://github.com/snap-stanford/med-flamingo.
</p></li>
</ul>

<h3>Title: Learning with Constraint Learning: New Perspective, Solution Strategy and Various Applications. (arXiv:2307.15257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15257">http://arxiv.org/abs/2307.15257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15257] Learning with Constraint Learning: New Perspective, Solution Strategy and Various Applications](http://arxiv.org/abs/2307.15257) #generative</code></li>
<li>Summary: <p>The complexity of learning problems, such as Generative Adversarial Network
(GAN) and its variants, multi-task and meta-learning, hyper-parameter learning,
and a variety of real-world vision applications, demands a deeper understanding
of their underlying coupling mechanisms. Existing approaches often address
these problems in isolation, lacking a unified perspective that can reveal
commonalities and enable effective solutions. Therefore, in this work, we
proposed a new framework, named Learning with Constraint Learning (LwCL), that
can holistically examine challenges and provide a unified methodology to tackle
all the above-mentioned complex learning and vision problems. Specifically,
LwCL is designed as a general hierarchical optimization model that captures the
essence of these diverse learning and vision problems. Furthermore, we develop
a gradient-response based fast solution strategy to overcome optimization
challenges of the LwCL framework. Our proposed framework efficiently addresses
a wide range of applications in learning and vision, encompassing three
categories and nine different problem types. Extensive experiments on synthetic
tasks and real-world applications verify the effectiveness of our approach. The
LwCL framework offers a comprehensive solution for tackling complex machine
learning and computer vision problems, bridging the gap between theory and
practice.
</p></li>
</ul>

<h3>Title: Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation. (arXiv:2307.15326v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15326">http://arxiv.org/abs/2307.15326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15326] Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation](http://arxiv.org/abs/2307.15326) #generative</code></li>
<li>Summary: <p>Online ads showing e-commerce products typically rely on the product images
in a catalog sent to the advertising platform by an e-commerce platform. In the
broader ads industry such ads are called dynamic product ads (DPA). It is
common for DPA catalogs to be in the scale of millions (corresponding to the
scale of products which can be bought from the e-commerce platform). However,
not all product images in the catalog may be appealing when directly
re-purposed as an ad image, and this may lead to lower click-through rates
(CTRs). In particular, products just placed against a solid background may not
be as enticing and realistic as a product staged in a natural environment. To
address such shortcomings of DPA images at scale, we propose a generative
adversarial network (GAN) based approach to generate staged backgrounds for
un-staged product images. Generating the entire staged background is a
challenging task susceptible to hallucinations. To get around this, we
introduce a simpler approach called copy-paste staging using retrieval assisted
GANs. In copy paste staging, we first retrieve (from the catalog) staged
products similar to the un-staged input product, and then copy-paste the
background of the retrieved product in the input image. A GAN based in-painting
model is used to fill the holes left after this copy-paste operation. We show
the efficacy of our copy-paste staging method via offline metrics, and human
evaluation. In addition, we show how our staging approach can enable animations
of moving products leading to a video ad from a product image.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: RSGPT: A Remote Sensing Vision Language Model and Benchmark. (arXiv:2307.15266v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15266">http://arxiv.org/abs/2307.15266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15266] RSGPT: A Remote Sensing Vision Language Model and Benchmark](http://arxiv.org/abs/2307.15266) #large language model</code></li>
<li>Summary: <p>The emergence of large-scale large language models, with GPT-4 as a prominent
example, has significantly propelled the rapid advancement of artificial
general intelligence and sparked the revolution of Artificial Intelligence 2.0.
In the realm of remote sensing (RS), there is a growing interest in developing
large vision language models (VLMs) specifically tailored for data analysis in
this domain. However, current research predominantly revolves around visual
recognition tasks, lacking comprehensive, large-scale image-text datasets that
are aligned and suitable for training large VLMs, which poses significant
challenges to effectively training such models for RS applications. In computer
vision, recent research has demonstrated that fine-tuning large vision language
models on small-scale, high-quality datasets can yield impressive performance
in visual and language understanding. These results are comparable to
state-of-the-art VLMs trained from scratch on massive amounts of data, such as
GPT-4. Inspired by this captivating idea, in this work, we build a high-quality
Remote Sensing Image Captioning dataset (RSICap) that facilitates the
development of large VLMs in the RS field. Unlike previous RS datasets that
either employ model-generated captions or short descriptions, RSICap comprises
2,585 human-annotated captions with rich and high-quality information. This
dataset offers detailed descriptions for each image, encompassing scene
descriptions (e.g., residential area, airport, or farmland) as well as object
information (e.g., color, shape, quantity, absolute position, etc). To
facilitate the evaluation of VLMs in the field of RS, we also provide a
benchmark evaluation dataset called RSIEval. This dataset consists of
human-annotated captions and visual question-answer pairs, allowing for a
comprehensive assessment of VLMs in the context of RS.
</p></li>
</ul>

<h3>Title: ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation. (arXiv:2307.15290v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15290">http://arxiv.org/abs/2307.15290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15290] ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation](http://arxiv.org/abs/2307.15290) #large language model</code></li>
<li>Summary: <p>This paper presents the development and evaluation of ChatHome, a
domain-specific language model (DSLM) designed for the intricate field of home
renovation. Considering the proven competencies of large language models (LLMs)
like GPT-4 and the escalating fascination with home renovation, this study
endeavors to reconcile these aspects by generating a dedicated model that can
yield high-fidelity, precise outputs relevant to the home renovation arena.
ChatHome's novelty rests on its methodology, fusing domain-adaptive pretraining
and instruction-tuning over an extensive dataset. This dataset includes
professional articles, standard documents, and web content pertinent to home
renovation. This dual-pronged strategy is designed to ensure that our model can
assimilate comprehensive domain knowledge and effectively address user
inquiries. Via thorough experimentation on diverse datasets, both universal and
domain-specific, including the freshly introduced "EvalHome" domain dataset, we
substantiate that ChatHome not only amplifies domain-specific functionalities
but also preserves its versatility.
</p></li>
</ul>

<h3>Title: TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety. (arXiv:2307.15311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15311">http://arxiv.org/abs/2307.15311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15311] TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety](http://arxiv.org/abs/2307.15311) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown remarkable effectiveness in various
general-domain natural language processing (NLP) tasks. However, their
performance in transportation safety domain tasks has been suboptimal,
primarily attributed to the requirement for specialized transportation safety
expertise in generating accurate responses [1]. To address this challenge, we
introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone
supervised fine-tuning using TrafficSafety-2K dataset which has human labels
from government produced guiding books and ChatGPT-generated instruction-output
pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset
are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.
</p></li>
</ul>

<h3>Title: Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15331">http://arxiv.org/abs/2307.15331</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15331] Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models](http://arxiv.org/abs/2307.15331) #large language model</code></li>
<li>Summary: <p>This paper presents two self-contained tutorials on stance detection in
Twitter data using BERT fine-tuning and prompting large language models (LLMs).
The first tutorial explains BERT architecture and tokenization, guiding users
through training, tuning, and evaluating standard and domain-specific BERT
models with HuggingFace transformers. The second focuses on constructing
prompts and few-shot examples to elicit stances from ChatGPT and open-source
FLAN-T5 without fine-tuning. Various prompting strategies are implemented and
evaluated using confusion matrices and macro F1 scores. The tutorials provide
code, visualizations, and insights revealing the strengths of few-shot ChatGPT
and FLAN-T5 which outperform fine-tuned BERTs. By covering both model
fine-tuning and prompting-based techniques in an accessible, hands-on manner,
these tutorials enable learners to gain applied experience with cutting-edge
methods for stance detection.
</p></li>
</ul>

<h3>Title: Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15337">http://arxiv.org/abs/2307.15337</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15337] Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](http://arxiv.org/abs/2307.15337) #large language model</code></li>
<li>Summary: <p>This work aims at decreasing the end-to-end generation latency of large
language models (LLMs). One of the major causes of the high generation latency
is the sequential decoding approach adopted by almost all state-of-the-art
LLMs. In this work, motivated by the thinking and writing process of humans, we
propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the
skeleton of the answer, and then conducts parallel API calls or batched
decoding to complete the contents of each skeleton point in parallel. Not only
does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs),
but it can also potentially improve the answer quality on several question
categories in terms of diversity and relevance. SoT is an initial attempt at
data-centric optimization for efficiency, and reveal the potential of pushing
LLMs to think more like a human for answer quality.
</p></li>
</ul>

<h3>Title: Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15343">http://arxiv.org/abs/2307.15343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15343] Med-HALT: Medical Domain Hallucination Test for Large Language Models](http://arxiv.org/abs/2307.15343) #large language model</code></li>
<li>Summary: <p>This research paper focuses on the challenges posed by hallucinations in
large language models (LLMs), particularly in the context of the medical
domain. Hallucination, wherein these models generate plausible yet unverified
or incorrect information, can have serious consequences in healthcare
applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain
Hallucination Test), designed specifically to evaluate and reduce
hallucinations. Med-HALT provides a diverse multinational dataset derived from
medical examinations across various countries and includes multiple innovative
testing modalities. Med-HALT includes two categories of tests reasoning and
memory-based hallucination tests, designed to assess LLMs's problem-solving and
information retrieval abilities.
</p></li>
</ul>

<p>Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,
MPT, and Falcon, revealing significant differences in their performance. The
paper provides detailed insights into the dataset, promoting transparency and
reproducibility. Through this work, we aim to contribute to the development of
safer and more reliable language models in healthcare. Our benchmark can be
found at medhalt.github.io
</p>

<h3>Title: Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15411">http://arxiv.org/abs/2307.15411</a></li>
<li>Code URL: <a href="https://github.com/xdwang0726/icl_ll">https://github.com/xdwang0726/icl_ll</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15411] Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning](http://arxiv.org/abs/2307.15411) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have shown remarkable capacity for in-context
learning (ICL), where learning a new task from just a few training examples is
done without being explicitly pre-trained. However, despite the success of
LLMs, there has been little understanding of how ICL learns the knowledge from
the given prompts. In this paper, to make progress toward understanding the
learning behaviour of ICL, we train the same LLMs with the same demonstration
examples via ICL and supervised learning (SL), respectively, and investigate
their performance under label perturbations (i.e., noisy labels and label
imbalance) on a range of classification tasks. First, via extensive
experiments, we find that gold labels have significant impacts on the
downstream in-context performance, especially for large language models;
however, imbalanced labels matter little to ICL across all model sizes. Second,
when comparing with SL, we show empirically that ICL is less sensitive to label
perturbations than SL, and ICL gradually attains comparable performance to SL
as the model size increases.
</p></li>
</ul>

<h3>Title: A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI. (arXiv:2307.15425v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15425">http://arxiv.org/abs/2307.15425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15425] A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI](http://arxiv.org/abs/2307.15425) #large language model</code></li>
<li>Summary: <p>This paper examines the comparative effectiveness of a specialized compiled
language model and a general-purpose model like OpenAI's GPT-3.5 in detecting
SDGs within text data. It presents a critical review of Large Language Models
(LLMs), addressing challenges related to bias and sensitivity. The necessity of
specialized training for precise, unbiased analysis is underlined. A case study
using a company descriptions dataset offers insight into the differences
between the GPT-3.5 and the specialized SDG detection model. While GPT-3.5
boasts broader coverage, it may identify SDGs with limited relevance to the
companies' activities. In contrast, the specialized model zeroes in on highly
pertinent SDGs. The importance of thoughtful model selection is emphasized,
taking into account task requirements, cost, complexity, and transparency.
Despite the versatility of LLMs, the use of specialized models is suggested for
tasks demanding precision and accuracy. The study concludes by encouraging
further research to find a balance between the capabilities of LLMs and the
need for domain-specific expertise and interpretability.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: AC-Norm: Effective Tuning for Medical Image Analysis via Affine Collaborative Normalization. (arXiv:2307.15282v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15282">http://arxiv.org/abs/2307.15282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15282] AC-Norm: Effective Tuning for Medical Image Analysis via Affine Collaborative Normalization](http://arxiv.org/abs/2307.15282) #segmentation</code></li>
<li>Summary: <p>Driven by the latest trend towards self-supervised learning (SSL), the
paradigm of "pretraining-then-finetuning" has been extensively explored to
enhance the performance of clinical applications with limited annotations.
Previous literature on model finetuning has mainly focused on regularization
terms and specific policy models, while the misalignment of channels between
source and target models has not received sufficient attention. In this work,
we revisited the dynamics of batch normalization (BN) layers and observed that
the trainable affine parameters of BN serve as sensitive indicators of domain
information. Therefore, Affine Collaborative Normalization (AC-Norm) is
proposed for finetuning, which dynamically recalibrates the channels in the
target model according to the cross-domain channel-wise correlations without
adding extra parameters. Based on a single-step backpropagation, AC-Norm can
also be utilized to measure the transferability of pretrained models. We
evaluated AC-Norm against the vanilla finetuning and state-of-the-art
fine-tuning methods on transferring diverse pretrained models to the diabetic
retinopathy grade classification, retinal vessel segmentation, CT lung nodule
segmentation/classification, CT liver-tumor segmentation and MRI cardiac
segmentation tasks. Extensive experiments demonstrate that AC-Norm unanimously
outperforms the vanilla finetuning by up to 4% improvement, even under
significant domain shifts where the state-of-the-art methods bring no gains. We
also prove the capability of AC-Norm in fast transferability estimation. Our
code is available at https://github.com/EndoluminalSurgicalVision-IMR/ACNorm.
</p></li>
</ul>

<h3>Title: Local and Global Information in Obstacle Detection on Railway Tracks. (arXiv:2307.15478v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15478">http://arxiv.org/abs/2307.15478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15478] Local and Global Information in Obstacle Detection on Railway Tracks](http://arxiv.org/abs/2307.15478) #segmentation</code></li>
<li>Summary: <p>Reliable obstacle detection on railways could help prevent collisions that
result in injuries and potentially damage or derail the train. Unfortunately,
generic object detectors do not have enough classes to account for all possible
scenarios, and datasets featuring objects on railways are challenging to
obtain. We propose utilizing a shallow network to learn railway segmentation
from normal railway images. The limited receptive field of the network prevents
overconfident predictions and allows the network to focus on the locally very
distinct and repetitive patterns of the railway environment. Additionally, we
explore the controlled inclusion of global information by learning to
hallucinate obstacle-free images. We evaluate our method on a custom dataset
featuring railway images with artificially augmented obstacles. Our proposed
method outperforms other learning-based baseline methods.
</p></li>
</ul>

<h3>Title: Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network. (arXiv:2307.15506v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15506">http://arxiv.org/abs/2307.15506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15506] Improving Image Quality of Sparse-view Lung Cancer CT Images with a Convolutional Neural Network](http://arxiv.org/abs/2307.15506) #segmentation</code></li>
<li>Summary: <p>Purpose: To improve the image quality of sparse-view computed tomography (CT)
images with a U-Net for lung cancer detection and to determine the best
trade-off between number of views, image quality, and diagnostic confidence.
</p></li>
</ul>

<p>Methods: CT images from 41 subjects (34 with lung cancer, seven healthy) were
retrospectively selected (01.2016-12.2018) and forward projected onto 2048-view
sinograms. Six corresponding sparse-view CT data subsets at varying levels of
undersampling were reconstructed from sinograms using filtered backprojection
with 16, 32, 64, 128, 256, and 512 views, respectively. A dual-frame U-Net was
trained and evaluated for each subsampling level on 8,658 images from 22
diseased subjects. A representative image per scan was selected from 19
subjects (12 diseased, seven healthy) for a single-blinded reader study. The
selected slices, for all levels of subsampling, with and without
post-processing by the U-Net model, were presented to three readers. Image
quality and diagnostic confidence were ranked using pre-defined scales.
Subjective nodule segmentation was evaluated utilizing sensitivity (Se) and
Dice Similarity Coefficient (DSC) with 95% confidence intervals (CI).
</p>
<p>Results: The 64-projection sparse-view images resulted in Se = 0.89 and DSC =
0.81 [0.75,0.86] while their counterparts, post-processed with the U-Net, had
improved metrics (Se = 0.94, DSC = 0.85 [0.82,0.87]). Fewer views lead to
insufficient quality for diagnostic purposes. For increased views, no
substantial discrepancies were noted between the sparse-view and post-processed
images.
</p>
<p>Conclusion: Projection views can be reduced from 2048 to 64 while maintaining
image quality and the confidence of the radiologists on a satisfactory level.
</p>

<h3>Title: OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes. (arXiv:2307.15588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.15588">http://arxiv.org/abs/2307.15588</a></li>
<li>Code URL: <a href="https://github.com/feibryantkit/oafuser">https://github.com/feibryantkit/oafuser</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.15588] OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes](http://arxiv.org/abs/2307.15588) #segmentation</code></li>
<li>Summary: <p>Light field cameras can provide rich angular and spatial information to
enhance image semantic segmentation for scene understanding in the field of
autonomous driving. However, the extensive angular information of light field
cameras contains a large amount of redundant data, which is overwhelming for
the limited hardware resource of intelligent vehicles. Besides, inappropriate
compression leads to information corruption and data loss. To excavate
representative information, we propose an Omni-Aperture Fusion model (OAFuser),
which leverages dense context from the central view and discovers the angular
information from sub-aperture images to generate a semantically-consistent
result. To avoid feature loss during network propagation and simultaneously
streamline the redundant information from the light field camera, we present a
simple yet very effective Sub-Aperture Fusion Module (SAFM) to embed
sub-aperture images into angular features without any additional memory cost.
Furthermore, to address the mismatched spatial information across viewpoints,
we present Center Angular Rectification Module (CARM) realized feature
resorting and prevent feature occlusion caused by asymmetric information. Our
proposed OAFuser achieves state-of-the-art performance on the UrbanLF-Real and
-Syn datasets and sets a new record of 84.93% in mIoU on the UrbanLF-Real
Extended dataset, with a gain of +4.53%. The source code of OAFuser will be
made publicly available at https://github.com/FeiBryantkit/OAFuser.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
