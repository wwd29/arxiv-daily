<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Leveraging Semantic Relationships to Prioritise Indicators of Compromise in Additive Manufacturing Systems. (arXiv:2305.04102v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04102">http://arxiv.org/abs/2305.04102</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04102] Leveraging Semantic Relationships to Prioritise Indicators of Compromise in Additive Manufacturing Systems](http://arxiv.org/abs/2305.04102) #security</code></li>
<li>Summary: <p>Additive manufacturing (AM) offers numerous benefits, such as manufacturing
complex and customised designs quickly and cost-effectively, reducing material
waste, and enabling on-demand production. However, several security challenges
are associated with AM, making it increasingly attractive to attackers ranging
from individual hackers to organised criminal gangs and nation-state actors.
This paper addresses the cyber risk in AM to attackers by proposing a novel
semantic-based threat prioritisation system for identifying, extracting and
ranking indicators of compromise (IOC). The system leverages the heterogeneous
information networks (HINs) that automatically extract high-level IOCs from
multi-source threat text and identifies semantic relations among the IOCs. It
models IOCs with a HIN comprising different meta-paths and meta-graphs to
depict semantic relations among diverse IOCs. We introduce a domain-specific
recogniser that identifies IOCs in three domains: organisation-specific,
regional source-specific, and regional target-specific. A threat assessment
uses similarity measures based on meta-paths and meta-graphs to assess semantic
relations among IOCs. It prioritises IOCs by measuring their severity based on
the frequency of attacks, IOC lifetime, and exploited vulnerabilities in each
domain.
</p></li>
</ul>

<h3>Title: On the usefulness of linear types for correct nonce use enforcement during compile time. (arXiv:2305.04138v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04138">http://arxiv.org/abs/2305.04138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04138] On the usefulness of linear types for correct nonce use enforcement during compile time](http://arxiv.org/abs/2305.04138) #security</code></li>
<li>Summary: <p>Cryptographic algorithms and protocols often need unique random numbers as
parameters (e.g. nonces). Failure to satisfy this requirement lead to
vulnerable implementation and can result in security breach. We show how linear
types and static type checking can be used to enforce the correct generation of
a new unique random number for each function invocation.
</p></li>
</ul>

<h3>Title: Bypassing antivirus detection: old-school malware, new tricks. (arXiv:2305.04149v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04149">http://arxiv.org/abs/2305.04149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04149] Bypassing antivirus detection: old-school malware, new tricks](http://arxiv.org/abs/2305.04149) #security</code></li>
<li>Summary: <p>Being on a mushrooming spree since at least 2013, malware can take a large
toll on any system. In a perpetual cat-and-mouse chase with defenders, malware
writers constantly conjure new methods to hide their code so as to evade
detection by security products. In this context, focusing on the MS Windows
platform, this work contributes a comprehensive empirical evaluation regarding
the detection capacity of popular, off-the-shelf antivirus and endpoint
detection and response engines when facing legacy malware obfuscated via more
or less uncommon but publicly known methods. Our experiments exploit a blend of
seven traditional AV evasion techniques in 16 executables built in C++, Go, and
Rust. Furthermore, we conduct an incipient study regarding the ability of the
ChatGPT chatbot in assisting threat actors to produce ready-to-use malware. The
derived results in terms of detection rate are highly unexpected: approximately
half of the 12 tested AV engines were able to detect less than half of the
malware variants, four AVs exactly half of the variants, while only two of the
rest detected all but one of the variants.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling Augmentation Framework. (arXiv:2305.03980v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03980">http://arxiv.org/abs/2305.03980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03980] Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling Augmentation Framework](http://arxiv.org/abs/2305.03980) #privacy</code></li>
<li>Summary: <p>Denoising diffusion models have shown remarkable potential in various
generation tasks. The open-source large-scale text-to-image model, Stable
Diffusion, becomes prevalent as it can generate realistic artistic or facial
images with personalization through fine-tuning on a limited number of new
samples. However, this has raised privacy concerns as adversaries can acquire
facial images online and fine-tune text-to-image models for malicious editing,
leading to baseless scandals, defamation, and disruption to victims' lives.
Prior research efforts have focused on deriving adversarial loss from
conventional training processes for facial privacy protection through
adversarial perturbations. However, existing algorithms face two issues: 1)
they neglect the image-text fusion module, which is the vital module of
text-to-image diffusion models, and 2) their defensive performance is unstable
against different attacker prompts. In this paper, we propose the Adversarial
Decoupling Augmentation Framework (ADAF), addressing these issues by targeting
the image-text fusion module to enhance the defensive performance of facial
privacy protection algorithms. ADAF introduces multi-level text-related
augmentations for defense stability against various attacker prompts.
Concretely, considering the vision, text, and common unit space, we propose
Vision-Adversarial Loss, Prompt-Robust Augmentation, and Attention-Decoupling
Loss. Extensive experiments on CelebA-HQ and VGGFace2 demonstrate ADAF's
promising performance, surpassing existing algorithms.
</p></li>
</ul>

<h3>Title: Differentially-private Continual Releases against Dynamic Databases. (arXiv:2305.03783v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03783">http://arxiv.org/abs/2305.03783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03783] Differentially-private Continual Releases against Dynamic Databases](http://arxiv.org/abs/2305.03783) #privacy</code></li>
<li>Summary: <p>Prior research primarily examined differentially-private continual releases
against data streams, where entries were immutable after insertion. However,
most data is dynamic and housed in databases. Addressing this literature gap,
this article presents a methodology for achieving differential privacy for
continual releases in dynamic databases, where entries can be inserted,
modified, and deleted. A dynamic database is represented as a changelog,
allowing the application of differential privacy techniques for data streams to
dynamic databases. To ensure differential privacy in continual releases, this
article demonstrates the necessity of constraints on mutations in dynamic
databases and proposes two common constraints. Additionally, it explores the
differential privacy of two fundamental types of continual releases: Disjoint
Continual Releases (DCR) and Sliding-window Continual Releases (SWCR). The
article also highlights how DCR and SWCR can benefit from a hierarchical
algorithm for better privacy budget utilization. Furthermore, it reveals that
the changelog representation can be extended to dynamic entries, achieving
local differential privacy for continual releases. Lastly, the article
introduces a novel approach to implement continual release of randomized
responses.
</p></li>
</ul>

<h3>Title: An Overview of AI and Blockchain Integration for Privacy-Preserving. (arXiv:2305.03928v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03928">http://arxiv.org/abs/2305.03928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03928] An Overview of AI and Blockchain Integration for Privacy-Preserving](http://arxiv.org/abs/2305.03928) #privacy</code></li>
<li>Summary: <p>With the widespread attention and application of artificial intelligence (AI)
and blockchain technologies, privacy protection techniques arising from their
integration are of notable significance. In addition to protecting privacy of
individuals, these techniques also guarantee security and dependability of
data. This paper initially presents an overview of AI and blockchain,
summarizing their combination along with derived privacy protection
technologies. It then explores specific application scenarios in data
encryption, de-identification, multi-tier distributed ledgers, and k-anonymity
methods. Moreover, the paper evaluates five critical aspects of
AI-blockchain-integration privacy protection systems, including authorization
management, access control, data protection, network security, and scalability.
Furthermore, it analyzes the deficiencies and their actual cause, offering
corresponding suggestions. This research also classifies and summarizes privacy
protection techniques based on AI-blockchain application scenarios and
technical schemes. In conclusion, this paper outlines the future directions of
privacy protection technologies emerging from AI and blockchain integration,
including enhancing efficiency and security to achieve a more comprehensive
privacy protection of privacy.
</p></li>
</ul>

<h3>Title: Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information. (arXiv:2305.04146v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04146">http://arxiv.org/abs/2305.04146</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04146] Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information](http://arxiv.org/abs/2305.04146) #privacy</code></li>
<li>Summary: <p>Privacy-preserving instance encoding aims to encode raw data as feature
vectors without revealing their privacy-sensitive information. When designed
properly, these encodings can be used for downstream ML applications such as
training and inference with limited privacy risk. However, the vast majority of
existing instance encoding schemes are based on heuristics and their
privacy-preserving properties are only validated empirically against a limited
set of attacks. In this paper, we propose a theoretically-principled measure
for the privacy of instance encoding based on Fisher information. We show that
our privacy measure is intuitive, easily applicable, and can be used to bound
the invertibility of encodings both theoretically and empirically.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04095">http://arxiv.org/abs/2305.04095</a></li>
<li>Code URL: <a href="https://github.com/rand2ai/fedkl">https://github.com/rand2ai/fedkl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04095] Gradient Leakage Defense with Key-Lock Module for Federated Learning](http://arxiv.org/abs/2305.04095) #defense</code></li>
<li>Summary: <p>Federated Learning (FL) is a widely adopted privacy-preserving machine
learning approach where private data remains local, enabling secure
computations and the exchange of local model gradients between local clients
and third-party parameter servers. However, recent findings reveal that privacy
may be compromised and sensitive information potentially recovered from shared
gradients. In this study, we offer detailed analysis and a novel perspective on
understanding the gradient leakage problem. These theoretical works lead to a
new gradient leakage defense technique that secures arbitrary model
architectures using a private key-lock module. Only the locked gradient is
transmitted to the parameter server for global model aggregation. Our proposed
learning method is resistant to gradient leakage attacks, and the key-lock
module is designed and trained to ensure that, without the private information
of the key-lock module: a) reconstructing private training data from the shared
gradient is infeasible; and b) the global model's inference performance is
significantly compromised. We discuss the theoretical underpinnings of why
gradients can leak private information and provide theoretical proof of our
method's effectiveness. We conducted extensive empirical evaluations with a
total of forty-four models on several popular benchmarks, demonstrating the
robustness of our proposed approach in both maintaining model performance and
defending against gradient leakage attacks.
</p></li>
</ul>

<h3>Title: Reactive Perturbation Defocusing for Textual Adversarial Defense. (arXiv:2305.04067v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04067">http://arxiv.org/abs/2305.04067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04067] Reactive Perturbation Defocusing for Textual Adversarial Defense](http://arxiv.org/abs/2305.04067) #defense</code></li>
<li>Summary: <p>Recent studies have shown that large pre-trained language models are
vulnerable to adversarial attacks. Existing methods attempt to reconstruct the
adversarial examples. However, these methods usually have limited performance
in defense against adversarial examples, while also negatively impacting the
performance on natural examples. To overcome this problem, we propose a method
called Reactive Perturbation Defocusing (RPD). RPD uses an adversarial detector
to identify adversarial examples and reduce false defenses on natural examples.
Instead of reconstructing the adversaries, RPD injects safe perturbations into
adversarial examples to distract the objective models from the malicious
perturbations. Our experiments on three datasets, two objective models, and
various adversarial attacks show that our proposed framework successfully
repairs up to approximately 97% of correctly identified adversarial examples
with only about a 2% performance decrease on natural examples. We also provide
a demo of adversarial detection and repair based on our work.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps. (arXiv:2305.03963v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03963">http://arxiv.org/abs/2305.03963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03963] Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps](http://arxiv.org/abs/2305.03963) #attack</code></li>
<li>Summary: <p>The increasing popularity of deep learning (DL) models and the advantages of
computing, including low latency and bandwidth savings on smartphones, have led
to the emergence of intelligent mobile applications, also known as DL apps, in
recent years. However, this technological development has also given rise to
several security concerns, including adversarial examples, model stealing, and
data poisoning issues. Existing works on attacks and countermeasures for
on-device DL models have primarily focused on the models themselves. However,
scant attention has been paid to the impact of data processing disturbance on
the model inference. This knowledge disparity highlights the need for
additional research to fully comprehend and address security issues related to
data processing for on-device models. In this paper, we introduce a data
processing-based attacks against real-world DL apps. In particular, our attack
could influence the performance and latency of the model without affecting the
operation of a DL app. To demonstrate the effectiveness of our attack, we carry
out an empirical study on 517 real-world DL apps collected from Google Play.
Among 320 apps utilizing MLkit, we find that 81.56\% of them can be
successfully attacked.
</p></li>
</ul>

<p>The results emphasize the importance of DL app developers being aware of and
taking actions to secure on-device models from the perspective of data
processing.
</p>

<h3>Title: Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning. (arXiv:2305.03888v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03888">http://arxiv.org/abs/2305.03888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03888] Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning](http://arxiv.org/abs/2305.03888) #attack</code></li>
<li>Summary: <p>In recent years, on-device deep learning has gained attention as a means of
developing affordable deep learning applications for mobile devices. However,
on-device models are constrained by limited energy and computation resources.
In the mean time, a poisoning attack known as sponge poisoning has been
developed.This attack involves feeding the model with poisoned examples to
increase the energy consumption during inference. As previous work is focusing
on server hardware accelerators, in this work, we extend the sponge poisoning
attack to an on-device scenario to evaluate the vulnerability of mobile device
processors. We present an on-device sponge poisoning attack pipeline to
simulate the streaming and consistent inference scenario to bridge the
knowledge gap in the on-device setting. Our exclusive experimental analysis
with processors and on-device networks shows that sponge poisoning attacks can
effectively pollute the modern processor with its built-in accelerator. We
analyze the impact of different factors in the sponge poisoning algorithm and
highlight the need for improved defense mechanisms to prevent such attacks on
on-device deep learning applications.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Persistent Homology Meets Object Unity: Object Recognition in Clutter. (arXiv:2305.03815v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03815">http://arxiv.org/abs/2305.03815</a></li>
<li>Code URL: <a href="https://github.com/smartslab/thor">https://github.com/smartslab/thor</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03815] Persistent Homology Meets Object Unity: Object Recognition in Clutter](http://arxiv.org/abs/2305.03815) #robust</code></li>
<li>Summary: <p>Recognition of occluded objects in unseen and unstructured indoor
environments is a challenging problem for mobile robots. To address this
challenge, we propose a new descriptor, TOPS, for point clouds generated from
depth images and an accompanying recognition framework, THOR, inspired by human
reasoning. The descriptor employs a novel slicing-based approach to compute
topological features from filtrations of simplicial complexes using persistent
homology, and facilitates reasoning-based recognition using object unity. Apart
from a benchmark dataset, we report performance on a new dataset, the UW Indoor
Scenes (UW-IS) Occluded dataset, curated using commodity hardware to reflect
real-world scenarios with different environmental conditions and degrees of
object occlusion. THOR outperforms state-of-the-art methods on both the
datasets and achieves substantially higher recognition accuracy for all the
scenarios of the UW-IS Occluded dataset. Therefore, THOR, is a promising step
toward robust recognition in low-cost robots, meant for everyday use in indoor
settings.
</p></li>
</ul>

<h3>Title: Adaptive loose optimization for robust question answering. (arXiv:2305.03971v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03971">http://arxiv.org/abs/2305.03971</a></li>
<li>Code URL: <a href="https://github.com/reml-group/alo">https://github.com/reml-group/alo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03971] Adaptive loose optimization for robust question answering](http://arxiv.org/abs/2305.03971) #robust</code></li>
<li>Summary: <p>Question answering methods are well-known for leveraging data bias, such as
the language prior in visual question answering and the position bias in
machine reading comprehension (extractive question answering). Current
debiasing methods often come at the cost of significant in-distribution
performance to achieve favorable out-of-distribution generalizability, while
non-debiasing methods sacrifice a considerable amount of out-of-distribution
performance in order to obtain high in-distribution performance. Therefore, it
is challenging for them to deal with the complicated changing real-world
situations. In this paper, we propose a simple yet effective novel loss
function with adaptive loose optimization, which seeks to make the best of both
worlds for question answering. Our main technical contribution is to reduce the
loss adaptively according to the ratio between the previous and current
optimization state on mini-batch training data. This loose optimization can be
used to prevent non-debiasing methods from overlearning data bias while
enabling debiasing methods to maintain slight bias learning. Experiments on the
visual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,
GQA-OOD, and the extractive question answering dataset SQuAD demonstrate that
our approach enables QA methods to obtain state-of-the-art in- and
out-of-distribution performance in most cases. The source code has been
released publicly in \url{https://github.com/reml-group/ALO}.
</p></li>
</ul>

<h3>Title: Weighted Point Cloud Normal Estimation. (arXiv:2305.04007v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04007">http://arxiv.org/abs/2305.04007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04007] Weighted Point Cloud Normal Estimation](http://arxiv.org/abs/2305.04007) #robust</code></li>
<li>Summary: <p>Existing normal estimation methods for point clouds are often less robust to
severe noise and complex geometric structures. Also, they usually ignore the
contributions of different neighbouring points during normal estimation, which
leads to less accurate results. In this paper, we introduce a weighted normal
estimation method for 3D point cloud data. We innovate in two key points: 1) we
develop a novel weighted normal regression technique that predicts point-wise
weights from local point patches and use them for robust, feature-preserving
normal regression; 2) we propose to conduct contrastive learning between point
patches and the corresponding ground-truth normals of the patches' central
points as a pre-training process to facilitate normal regression. Comprehensive
experiments demonstrate that our method can robustly handle noisy and complex
point clouds, achieving state-of-the-art performance on both synthetic and
real-world datasets.
</p></li>
</ul>

<h3>Title: Robust Image Ordinal Regression with Controllable Image Generation. (arXiv:2305.04213v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04213">http://arxiv.org/abs/2305.04213</a></li>
<li>Code URL: <a href="https://github.com/ch3ngy1/controllable-image-generation">https://github.com/ch3ngy1/controllable-image-generation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04213] Robust Image Ordinal Regression with Controllable Image Generation](http://arxiv.org/abs/2305.04213) #robust</code></li>
<li>Summary: <p>Image ordinal regression has been mainly studied along the line of exploiting
the order of categories. However, the issues of class imbalance and category
overlap that are very common in ordinal regression were largely overlooked. As
a result, the performance on minority categories is often unsatisfactory. In
this paper, we propose a novel framework called CIG based on controllable image
generation to directly tackle these two issues. Our main idea is to generate
extra training samples with specific labels near category boundaries, and the
sample generation is biased toward the less-represented categories. To achieve
controllable image generation, we seek to separate structural and categorical
information of images based on structural similarity, categorical similarity,
and reconstruction constraints. We evaluate the effectiveness of our new CIG
approach in three different image ordinal regression scenarios. The results
demonstrate that CIG can be flexibly integrated with off-the-shelf image
encoders or ordinal regression models to achieve improvement, and further, the
improvement is more significant for minority categories.
</p></li>
</ul>

<h3>Title: Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization. (arXiv:2305.03937v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03937">http://arxiv.org/abs/2305.03937</a></li>
<li>Code URL: <a href="https://github.com/arazd/residualprompts">https://github.com/arazd/residualprompts</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03937] Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization](http://arxiv.org/abs/2305.03937) #robust</code></li>
<li>Summary: <p>Prompt tuning is one of the successful approaches for parameter-efficient
tuning of pre-trained language models. Despite being arguably the most
parameter-efficient (tuned soft prompts constitute <0.1% of total parameters),
it typically performs worse than other efficient tuning methods and is quite
sensitive to hyper-parameters. In this work, we introduce Residual Prompt
Tuning - a simple and efficient method that significantly improves the
performance and stability of prompt tuning. We propose to reparameterize soft
prompt embeddings using a shallow network with a residual connection. Our
experiments show that Residual Prompt Tuning significantly outperforms prompt
tuning on SuperGLUE benchmark. Notably, our method reaches +7 points
improvement over prompt tuning with T5-Base and allows to reduce the prompt
length by 10x without hurting performance. In addition, we show that our
approach is robust to the choice of learning rate and prompt initialization,
and is effective in few-shot settings.
</p></li>
</ul>

<h3>Title: SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04076">http://arxiv.org/abs/2305.04076</a></li>
<li>Code URL: <a href="https://github.com/pkunlp-icler/santa">https://github.com/pkunlp-icler/santa</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04076] SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition](http://arxiv.org/abs/2305.04076) #robust</code></li>
<li>Summary: <p>Distantly-Supervised Named Entity Recognition effectively alleviates the
burden of time-consuming and expensive annotation in the supervised setting.
But the context-free matching process and the limited coverage of knowledge
bases introduce inaccurate and incomplete annotation noise respectively.
Previous studies either considered only incomplete annotation noise or
indiscriminately handle two types of noise with the same strategy. In this
paper, we argue that the different causes of two types of noise bring up the
requirement of different strategies in model architecture. Therefore, we
propose the SANTA to handle these two types of noise separately with (1)
Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity
problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate
decision boundary shifting problem caused by incomplete annotation and a
noise-tolerant loss to improve the robustness. Benefiting from our separate
tailored strategies, we confirm in the experiment that the two types of noise
are well mitigated. SANTA also achieves a new state-of-the-art on five public
datasets.
</p></li>
</ul>

<h3>Title: Automated Spatio-Temporal Graph Contrastive Learning. (arXiv:2305.03920v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03920">http://arxiv.org/abs/2305.03920</a></li>
<li>Code URL: <a href="https://github.com/hkuds/autost">https://github.com/hkuds/autost</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03920] Automated Spatio-Temporal Graph Contrastive Learning](http://arxiv.org/abs/2305.03920) #robust</code></li>
<li>Summary: <p>Among various region embedding methods, graph-based region relation learning
models stand out, owing to their strong structure representation ability for
encoding spatial correlations with graph neural networks. Despite their
effectiveness, several key challenges have not been well addressed in existing
methods: i) Data noise and missing are ubiquitous in many spatio-temporal
scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g.,
mobility traces) usually exhibits distribution heterogeneity across space and
time. In such cases, current methods are vulnerable to the quality of the
generated region graphs, which may lead to suboptimal performance. In this
paper, we tackle the above challenges by exploring the Automated
Spatio-Temporal graph contrastive learning paradigm (AutoST) over the
heterogeneous region graph generated from multi-view data sources. Our \model\
framework is built upon a heterogeneous graph neural architecture to capture
the multi-view region dependencies with respect to POI semantics, mobility flow
patterns and geographical positions. To improve the robustness of our GNN
encoder against data noise and distribution issues, we design an automated
spatio-temporal augmentation scheme with a parameterized contrastive view
generator. AutoST can adapt to the spatio-temporal heterogeneous graph with
multi-view semantics well preserved. Extensive experiments for three downstream
spatio-temporal mining tasks on several real-world datasets demonstrate the
significant performance gain achieved by our \model\ over a variety of
baselines. The code is publicly available at https://github.com/HKUDS/AutoST.
</p></li>
</ul>

<h3>Title: PiML Toolbox for Interpretable Machine Learning Model Development and Validation. (arXiv:2305.04214v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04214">http://arxiv.org/abs/2305.04214</a></li>
<li>Code URL: <a href="https://github.com/selfexplainml/piml-toolbox">https://github.com/selfexplainml/piml-toolbox</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04214] PiML Toolbox for Interpretable Machine Learning Model Development and Validation](http://arxiv.org/abs/2305.04214) #robust</code></li>
<li>Summary: <p>PiML (read $\pi$-ML, /<code>pai.</code>em.`el/) is an integrated and open-access Python
toolbox for interpretable machine learning model development and model
diagnostics. It is designed with machine learning workflows in both low-code
and high-code modes, including data pipeline, model training, model
interpretation and explanation, and model diagnostics and comparison. The
toolbox supports a growing list of interpretable models (e.g. GAM, GAMI-Net,
XGB2) with inherent local and/or global interpretability. It also supports
model-agnostic explainability tools (e.g. PFI, PDP, LIME, SHAP) and a powerful
suite of model-agnostic diagnostics (e.g. weakness, uncertainty, robustness,
fairness). Integration of PiML models and tests to existing MLOps platforms for
quality assurance are enabled by flexible high-code APIs. Furthermore, PiML
toolbox comes with a comprehensive user guide and hands-on examples, including
the applications for model development and validation in banking. The project
is available at https://github.com/SelfExplainML/PiML-Toolbox.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Context-Aware Chart Element Detection. (arXiv:2305.04151v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04151">http://arxiv.org/abs/2305.04151</a></li>
<li>Code URL: <a href="https://github.com/pengyu965/chartdete">https://github.com/pengyu965/chartdete</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04151] Context-Aware Chart Element Detection](http://arxiv.org/abs/2305.04151) #extraction</code></li>
<li>Summary: <p>As a prerequisite of chart data extraction, the accurate detection of chart
basic elements is essential and mandatory. In contrast to object detection in
the general image domain, chart element detection relies heavily on context
information as charts are highly structured data visualization formats. To
address this, we propose a novel method CACHED, which stands for Context-Aware
Chart Element Detection, by integrating a local-global context fusion module
consisting of visual context enhancement and positional context encoding with
the Cascade R-CNN framework. To improve the generalization of our method for
broader applicability, we refine the existing chart element categorization and
standardized 18 classes for chart basic elements, excluding plot elements. Our
CACHED method, with the updated category of chart elements, achieves
state-of-the-art performance in our experiments, underscoring the importance of
context in chart element detection. Extending our method to the bar plot
detection task, we obtain the best result on the PMC test dataset.
</p></li>
</ul>

<h3>Title: Tuning Traditional Language Processing Approaches for Pashto Text Classification. (arXiv:2305.03737v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03737">http://arxiv.org/abs/2305.03737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03737] Tuning Traditional Language Processing Approaches for Pashto Text Classification](http://arxiv.org/abs/2305.03737) #extraction</code></li>
<li>Summary: <p>Today text classification becomes critical task for concerned individuals for
numerous purposes. Hence, several researches have been conducted to develop
automatic text classification for national and international languages.
However, the need for an automatic text categorization system for local
languages is felt. The main aim of this study is to establish a Pashto
automatic text classification system. In order to pursue this work, we built a
Pashto corpus which is a collection of Pashto documents due to the
unavailability of public datasets of Pashto text documents. Besides, this study
compares several models containing both statistical and neural network machine
learning techniques including Multilayer Perceptron (MLP), Support Vector
Machine (SVM), K Nearest Neighbor (KNN), decision tree, gaussian na\"ive Bayes,
multinomial na\"ive Bayes, random forest, and logistic regression to discover
the most effective approach. Moreover, this investigation evaluates two
different feature extraction methods including unigram, and Time Frequency
Inverse Document Frequency (IFIDF). Subsequently, this research obtained
average testing accuracy rate 94% using MLP classification algorithm and TFIDF
feature extraction method in this context.
</p></li>
</ul>

<h3>Title: Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data. (arXiv:2305.03827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03827">http://arxiv.org/abs/2305.03827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03827] Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data](http://arxiv.org/abs/2305.03827) #extraction</code></li>
<li>Summary: <p>Jointly extracting entity pairs and their relations is challenging when
working on distantly-supervised data with ambiguous or noisy labels. To
mitigate such impact, we propose uncertainty-aware bootstrap learning, which is
motivated by the intuition that the higher uncertainty of an instance, the more
likely the model confidence is inconsistent with the ground truths.
Specifically, we first explore instance-level data uncertainty to create an
initial high-confident examples. Such subset serves as filtering noisy
instances and facilitating the model to converge fast at the early stage.
During bootstrap learning, we propose self-ensembling as a regularizer to
alleviate inter-model uncertainty produced by noisy labels. We further define
probability variance of joint tagging probabilities to estimate inner-model
parametric uncertainty, which is used to select and build up new reliable
training instances for the next iteration. Experimental results on two large
datasets reveal that our approach outperforms existing strong baselines and
related methods.
</p></li>
</ul>

<h3>Title: Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03960">http://arxiv.org/abs/2305.03960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03960] Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text](http://arxiv.org/abs/2305.03960) #extraction</code></li>
<li>Summary: <p>Automated generation of business process models from natural language text is
an emerging methodology for avoiding the manual creation of formal business
process models. For this purpose, process entities like actors, activities,
objects etc., and relations among them are extracted from textual process
descriptions. A high-quality annotated corpus of textual process descriptions
(PET) has been published accompanied with a basic process extraction approach.
In its current state, however, PET lacks information about whether two mentions
refer to the same or different process entities, which corresponds to the
crucial decision of whether to create one or two modeling elements in the
target model. Consequently, it is ambiguous whether, for instance, two mentions
of data processing mean processing of different, or the same data. In this
paper, we extend the PET dataset by clustering mentions of process entities and
by proposing a new baseline technique for process extraction equipped with an
additional entity resolution component. In a second step, we replace the
rule-based relation extraction component with a machine learning-based
alternative, enabling rapid adaption to different datasets and domains. In
addition, we evaluate a deep learning-approach built for solving entity and
relation extraction as well as entity resolution in a holistic manner. Finally,
our extensive evaluation of the original PET baseline against our own
implementation shows that a pure machine learning-based process extraction
technique is competitive, while avoiding the massive overhead arising from
feature engineering and rule definition needed to adapt to other datasets,
different entity and relation types, or new domains.
</p></li>
</ul>

<h3>Title: Actively Discovering New Slots for Task-oriented Conversation. (arXiv:2305.04049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04049">http://arxiv.org/abs/2305.04049</a></li>
<li>Code URL: <a href="https://github.com/newslotdetection/newslotdetection">https://github.com/newslotdetection/newslotdetection</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04049] Actively Discovering New Slots for Task-oriented Conversation](http://arxiv.org/abs/2305.04049) #extraction</code></li>
<li>Summary: <p>Existing task-oriented conversational search systems heavily rely on domain
ontologies with pre-defined slots and candidate value sets. In practical
applications, these prerequisites are hard to meet, due to the emerging new
user requirements and ever-changing scenarios. To mitigate these issues for
better interaction performance, there are efforts working towards detecting
out-of-vocabulary values or discovering new slots under unsupervised or
semi-supervised learning paradigm. However, overemphasizing on the conversation
data patterns alone induces these methods to yield noisy and arbitrary slot
results. To facilitate the pragmatic utility, real-world systems tend to
provide a stringent amount of human labelling quota, which offers an
authoritative way to obtain accurate and meaningful slot assignments.
Nonetheless, it also brings forward the high requirement of utilizing such
quota efficiently. Hence, we formulate a general new slot discovery task in an
information extraction fashion and incorporate it into an active learning
framework to realize human-in-the-loop learning. Specifically, we leverage
existing language tools to extract value candidates where the corresponding
labels are further leveraged as weak supervision signals. Based on these, we
propose a bi-criteria selection scheme which incorporates two major strategies,
namely, uncertainty-based sampling and diversity-based sampling to efficiently
identify terms of interest. We conduct extensive experiments on several public
datasets and compare with a bunch of competitive baselines to demonstrate the
effectiveness of our method. We have made the code and data used in this paper
publicly available.
</p></li>
</ul>

<h3>Title: Shall We Trust All Relational Tuples by Open Information Extraction? A Study on Speculation Detection. (arXiv:2305.04181v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04181">http://arxiv.org/abs/2305.04181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04181] Shall We Trust All Relational Tuples by Open Information Extraction? A Study on Speculation Detection](http://arxiv.org/abs/2305.04181) #extraction</code></li>
<li>Summary: <p>Open Information Extraction (OIE) aims to extract factual relational tuples
from open-domain sentences. Downstream tasks use the extracted OIE tuples as
facts, without examining the certainty of these facts. However,
uncertainty/speculation is a common linguistic phenomenon. Existing studies on
speculation detection are defined at sentence level, but even if a sentence is
determined to be speculative, not all tuples extracted from it may be
speculative. In this paper, we propose to study speculations in OIE and aim to
determine whether an extracted tuple is speculative. We formally define the
research problem of tuple-level speculation detection and conduct a detailed
data analysis on the LSOIE dataset which contains labels for speculative
tuples. Lastly, we propose a baseline model OIE-Spec for this new research
task.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Exploring One-shot Semi-supervised Federated Learning with A Pre-trained Diffusion Model. (arXiv:2305.04063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04063">http://arxiv.org/abs/2305.04063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04063] Exploring One-shot Semi-supervised Federated Learning with A Pre-trained Diffusion Model](http://arxiv.org/abs/2305.04063) #federate</code></li>
<li>Summary: <p>Federated learning is a privacy-preserving collaborative learning approach.
Recently, some studies have proposed the semi-supervised federated learning
setting to handle the commonly seen real-world scenarios with labeled data on
the server and unlabeled data on the clients. However, existing methods still
face challenges such as high communication costs, training pressure on the
client devices, and distribution differences among the server and the clients.
In this paper, we introduce the powerful pre-trained diffusion models into
federated learning and propose FedDISC, a Federated Diffusion Inspired
Semi-supervised Co-training method, to address these challenges. Specifically,
we first extract prototypes from the labeled data on the server and send them
to the clients. The clients then use these prototypes to predict pseudo-labels
of the local data, and compute the cluster centroids and domain-specific
features to represent their personalized distributions. After adding noise, the
clients send these features and their corresponding pseudo-labels back to the
server, which uses a pre-trained diffusion model to conditionally generate
pseudo-samples complying with the client distributions and train an aggregated
model on them. Our method does not require local training and only involves
forward inference on the clients. Our extensive experiments on DomainNet,
Openimage, and NICO++ demonstrate that the proposed FedDISC method effectively
addresses the one-shot semi-supervised problem on Non-IID clients and
outperforms the compared SOTA methods. We also demonstrate through
visualization that it is of neglectable possibility for FedDISC to leak
privacy-sensitive information of the clients.
</p></li>
</ul>

<h3>Title: Decentralised Semi-supervised Onboard Learning for Scene Classification in Low-Earth Orbit. (arXiv:2305.04059v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04059">http://arxiv.org/abs/2305.04059</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04059] Decentralised Semi-supervised Onboard Learning for Scene Classification in Low-Earth Orbit](http://arxiv.org/abs/2305.04059) #federate</code></li>
<li>Summary: <p>Onboard machine learning on the latest satellite hardware offers the
potential for significant savings in communication and operational costs. We
showcase the training of a machine learning model on a satellite constellation
for scene classification using semi-supervised learning while accounting for
operational constraints such as temperature and limited power budgets based on
satellite processor benchmarks of the neural network. We evaluate mission
scenarios employing both decentralised and federated learning approaches. All
scenarios achieve convergence to high accuracy (around 91% on EuroSAT RGB
dataset) within a one-day mission timeframe.
</p></li>
</ul>

<h3>Title: Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation. (arXiv:2305.04066v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04066">http://arxiv.org/abs/2305.04066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04066] Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation](http://arxiv.org/abs/2305.04066) #federate</code></li>
<li>Summary: <p>Over-the-air Computation (AirComp) has been demonstrated as an effective
transmission scheme to boost the efficiency of federated edge learning (FEEL).
However, existing FEEL systems with AirComp scheme often employ traditional
synchronous aggregation mechanisms for local model aggregation in each global
round, which suffer from the stragglers issues. In this paper, we propose a
semi-asynchronous aggregation FEEL mechanism with AirComp scheme (PAOTA) to
improve the training efficiency of the FEEL system in the case of significant
heterogeneity in data and devices. Taking the staleness and divergence of model
updates from edge devices into consideration, we minimize the convergence upper
bound of the FEEL global model by adjusting the uplink transmit power of edge
devices at each aggregation period. The simulation results demonstrate that our
proposed algorithm achieves convergence performance close to that of the ideal
Local SGD. Furthermore, with the same target accuracy, the training time
required for PAOTA is less than that of the ideal Local SGD and the synchronous
FEEL algorithm via AirComp.
</p></li>
</ul>

<h3>Title: Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04152">http://arxiv.org/abs/2305.04152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04152] Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics](http://arxiv.org/abs/2305.04152) #federate</code></li>
<li>Summary: <p>The recent development of scalable Bayesian inference methods has renewed
interest in the adoption of Bayesian learning as an alternative to conventional
frequentist learning that offers improved model calibration via uncertainty
quantification. Recently, federated averaging Langevin dynamics (FALD) was
introduced as a variant of federated averaging that can efficiently implement
distributed Bayesian learning in the presence of noiseless communications. In
this paper, we propose wireless FALD (WFALD), a novel protocol that realizes
FALD in wireless systems by integrating over-the-air computation and
channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless
Bayesian learning, WFALD enables (\emph{i}) multiple local updates between
communication rounds; and (\emph{ii}) stochastic gradients computed by
mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein
distance between the samples produced by WFALD and the targeted global
posterior distribution. Analysis and experiments show that, when the
signal-to-noise ratio is sufficiently large, channel noise can be fully
repurposed for Monte Carlo sampling, thus entailing no loss in performance.
</p></li>
</ul>

<h3>Title: MrTF: Model Refinery for Transductive Federated Learning. (arXiv:2305.04201v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04201">http://arxiv.org/abs/2305.04201</a></li>
<li>Code URL: <a href="https://github.com/lxcnju/mrtf">https://github.com/lxcnju/mrtf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04201] MrTF: Model Refinery for Transductive Federated Learning](http://arxiv.org/abs/2305.04201) #federate</code></li>
<li>Summary: <p>We consider a real-world scenario in which a newly-established pilot project
needs to make inferences for newly-collected data with the help of other
parties under privacy protection policies. Current federated learning (FL)
paradigms are devoted to solving the data heterogeneity problem without
considering the to-be-inferred data. We propose a novel learning paradigm named
transductive federated learning (TFL) to simultaneously consider the structural
information of the to-be-inferred data. On the one hand, the server could use
the pre-available test samples to refine the aggregated models for robust model
fusion, which tackles the data heterogeneity problem in FL. On the other hand,
the refinery process incorporates test samples into training and could generate
better predictions in a transductive manner. We propose several techniques
including stabilized teachers, rectified distillation, and clustered label
refinery to facilitate the model refinery process. Abundant experimental
studies verify the superiorities of the proposed \underline{M}odel
\underline{r}efinery framework for \underline{T}ransductive
\underline{F}ederated learning (MrTF). The source code is available at
\url{https://github.com/lxcnju/MrTF}.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: PhysBench: A Benchmark Framework for Remote Physiological Sensing with New Dataset and Baseline. (arXiv:2305.04161v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04161">http://arxiv.org/abs/2305.04161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04161] PhysBench: A Benchmark Framework for Remote Physiological Sensing with New Dataset and Baseline](http://arxiv.org/abs/2305.04161) #fair</code></li>
<li>Summary: <p>In recent years, due to the widespread use of internet videos, physiological
remote sensing has gained more and more attention in the fields of affective
computing and telemedicine. Recovering physiological signals from facial videos
is a challenging task that involves a series of preprocessing, image
algorithms, and post-processing to finally restore waveforms. We propose a
complete and efficient end-to-end training and testing framework that provides
fair comparisons for different algorithms through unified preprocessing and
post-processing. In addition, we introduce a highly synchronized lossless
format dataset along with a lightweight algorithm. The dataset contains over 32
hours (3.53M frames) of video from 58 subjects; by training on our collected
dataset both our proposed algorithm as well as existing ones can achieve
improvements.
</p></li>
</ul>

<h3>Title: Rethinking Class Imbalance in Machine Learning. (arXiv:2305.03900v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03900">http://arxiv.org/abs/2305.03900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03900] Rethinking Class Imbalance in Machine Learning](http://arxiv.org/abs/2305.03900) #fair</code></li>
<li>Summary: <p>Imbalance learning is a subfield of machine learning that focuses on learning
tasks in the presence of class imbalance. Nearly all existing studies refer to
class imbalance as a proportion imbalance, where the proportion of training
samples in each class is not balanced. The ignorance of the proportion
imbalance will result in unfairness between/among classes and poor
generalization capability. Previous literature has presented numerous methods
for either theoretical/empirical analysis or new methods for imbalance
learning. This study presents a new taxonomy of class imbalance in machine
learning with a broader scope. Four other types of imbalance, namely, variance,
distance, neighborhood, and quality imbalances between/among classes, which may
exist in machine learning tasks, are summarized. Two different levels of
imbalance including global and local are also presented. Theoretical analysis
is used to illustrate the significant impact of the new imbalance types on
learning fairness. Moreover, our taxonomy and theoretical conclusions are used
to analyze the shortcomings of several classical methods. As an example, we
propose a new logit perturbation-based imbalance learning loss when proportion,
variance, and distance imbalances exist simultaneously. Several classical
losses become the special case of our proposed method. Meta learning is
utilized to infer the hyper-parameters related to the three types of imbalance.
Experimental results on several benchmark corpora validate the effectiveness of
the proposed method.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive Sensing. (arXiv:2305.03899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03899">http://arxiv.org/abs/2305.03899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03899] NL-CS Net: Deep Learning with Non-Local Prior for Image Compressive Sensing](http://arxiv.org/abs/2305.03899) #interpretability</code></li>
<li>Summary: <p>Deep learning has been applied to compressive sensing (CS) of images
successfully in recent years. However, existing network-based methods are often
trained as the black box, in which the lack of prior knowledge is often the
bottleneck for further performance improvement. To overcome this drawback, this
paper proposes a novel CS method using non-local prior which combines the
interpretability of the traditional optimization methods with the speed of
network-based methods, called NL-CS Net. We unroll each phase from iteration of
the augmented Lagrangian method solving non-local and sparse regularized
optimization problem by a network. NL-CS Net is composed of the up-sampling
module and the recovery module. In the up-sampling module, we use learnable
up-sampling matrix instead of a predefined one. In the recovery module,
patch-wise non-local network is employed to capture long-range feature
correspondences. Important parameters involved (e.g. sampling matrix, nonlinear
transforms, shrinkage thresholds, step size, $etc.$) are learned end-to-end,
rather than hand-crafted. Furthermore, to facilitate practical implementation,
orthogonal and binary constraints on the sampling matrix are simultaneously
adopted. Extensive experiments on natural images and magnetic resonance imaging
(MRI) demonstrate that the proposed method outperforms the state-of-the-art
methods while maintaining great interpretability and speed.
</p></li>
</ul>

<h3>Title: Feature Chirality in Deep Learning Models. (arXiv:2305.03966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03966">http://arxiv.org/abs/2305.03966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03966] Feature Chirality in Deep Learning Models](http://arxiv.org/abs/2305.03966) #interpretability</code></li>
<li>Summary: <p>As deep learning applications extensively increase by leaps and bounds, their
interpretability has become increasingly prominent. As a universal property,
chirality exists widely in nature, and applying it to the explanatory research
of deep learning may be helpful to some extent. Inspired by a recent study that
used CNN (convolutional neural network), which applied visual chirality, to
distinguish whether an image is flipped or not. In this paper, we study feature
chirality innovatively, which shows how the statistics of deep learning models'
feature data are changed by training. We rethink the feature-level chirality
property, propose the feature chirality, and give the measure. Our analysis of
feature chirality on AlexNet, VGG, and ResNet reveals similar but surprising
results, including the prevalence of feature chirality in these models, the
initialization methods of the models do not affect feature chirality. Our work
shows that feature chirality implies model evaluation, interpretability of the
model, and model parameters optimization.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03859">http://arxiv.org/abs/2305.03859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03859] Open problems in causal structure learning: A case study of COVID-19 in the UK](http://arxiv.org/abs/2305.03859) #explainability</code></li>
<li>Summary: <p>Causal machine learning (ML) algorithms recover graphical structures that
tell us something about cause-and-effect relationships. The causal
representation provided by these algorithms enables transparency and
explainability, which is necessary in critical real-world problems. Yet, causal
ML has had limited impact in practice compared to associational ML. This paper
investigates the challenges of causal ML with application to COVID-19 UK
pandemic data. We collate data from various public sources and investigate what
the various structure learning algorithms learn from these data. We explore the
impact of different data formats on algorithms spanning different classes of
learning, and assess the results produced by each algorithm, and groups of
algorithms, in terms of graphical structure, model dimensionality, sensitivity
analysis, confounding variables, predictive and interventional inference. We
use these results to highlight open problems in causal structure learning and
directions for future research. To facilitate future work, we make all graphs,
models and data sets publicly available online.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: Evading Watermark based Detection of AI-Generated Content. (arXiv:2305.03807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03807">http://arxiv.org/abs/2305.03807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03807] Evading Watermark based Detection of AI-Generated Content](http://arxiv.org/abs/2305.03807) #watermark</code></li>
<li>Summary: <p>A generative AI model -- such as DALL-E, Stable Diffusion, and ChatGPT -- can
generate extremely realistic-looking content, posing growing challenges to the
authenticity of information. To address the challenges, watermark has been
leveraged to detect AI-generated content. Specifically, a watermark is embedded
into an AI-generated content before it is released. A content is detected as
AI-generated if a similar watermark can be decoded from it. In this work, we
perform a systematic study on the robustness of such watermark-based
AI-generated content detection. We focus on AI-generated images. Our work shows
that an attacker can post-process an AI-generated watermarked image via adding
a small, human-imperceptible perturbation to it, such that the post-processed
AI-generated image evades detection while maintaining its visual quality. We
demonstrate the effectiveness of our attack both theoretically and empirically.
Moreover, to evade detection, our adversarial post-processing method adds much
smaller perturbations to the AI-generated images and thus better maintain their
visual quality than existing popular image post-processing methods such as JPEG
compression, Gaussian blur, and Brightness/Contrast. Our work demonstrates the
insufficiency of existing watermark-based detection of AI-generated content,
highlighting the urgent needs of new detection methods.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: DocDiff: Document Enhancement via Residual Diffusion Models. (arXiv:2305.03892v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03892">http://arxiv.org/abs/2305.03892</a></li>
<li>Code URL: <a href="https://github.com/Royalvice/DocDiff">https://github.com/Royalvice/DocDiff</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03892] DocDiff: Document Enhancement via Residual Diffusion Models](http://arxiv.org/abs/2305.03892) #diffusion</code></li>
<li>Summary: <p>Removing degradation from document images not only improves their visual
quality and readability, but also enhances the performance of numerous
automated document analysis and recognition tasks. However, existing
regression-based methods optimized for pixel-level distortion reduction tend to
suffer from significant loss of high-frequency information, leading to
distorted and blurred text edges. To compensate for this major deficiency, we
propose DocDiff, the first diffusion-based framework specifically designed for
diverse challenging document enhancement problems, including document
deblurring, denoising, and removal of watermarks and seals. DocDiff consists of
two modules: the Coarse Predictor (CP), which is responsible for recovering the
primary low-frequency content, and the High-Frequency Residual Refinement (HRR)
module, which adopts the diffusion models to predict the residual
(high-frequency information, including text edges), between the ground-truth
and the CP-predicted image. DocDiff is a compact and computationally efficient
model that benefits from a well-designed network architecture, an optimized
training loss objective, and a deterministic sampling process with short time
steps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art
(SOTA) performance on multiple benchmark datasets, and can significantly
enhance the readability and recognizability of degraded document images.
Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-play
and ready-to-use, with only 4.17M parameters. It greatly sharpens the text
edges generated by SOTA deblurring methods without additional joint training.
Available codes: https://github.com/Royalvice/DocDiff
</p></li>
</ul>

<h3>Title: AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion. (arXiv:2305.04001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04001">http://arxiv.org/abs/2305.04001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04001] AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion](http://arxiv.org/abs/2305.04001) #diffusion</code></li>
<li>Summary: <p>Recent advances in diffusion models have showcased promising results in the
text-to-video (T2V) synthesis task. However, as these T2V models solely employ
text as the guidance, they tend to struggle in modeling detailed temporal
dynamics. In this paper, we introduce a novel T2V framework that additionally
employ audio signals to control the temporal dynamics, empowering an
off-the-shelf T2I diffusion to generate audio-aligned videos. We propose
audio-based regional editing and signal smoothing to strike a good balance
between the two contradicting desiderata of video synthesis, i.e., temporal
flexibility and coherence. We empirically demonstrate the effectiveness of our
method through experiments, and further present practical applications for
contents creation.
</p></li>
</ul>

<h3>Title: Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning. (arXiv:2305.04175v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04175">http://arxiv.org/abs/2305.04175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04175] Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning](http://arxiv.org/abs/2305.04175) #diffusion</code></li>
<li>Summary: <p>With the help of conditioning mechanisms, the state-of-the-art diffusion
models have achieved tremendous success in guided image generation,
particularly in text-to-image synthesis. To gain a better understanding of the
training process and potential risks of text-to-image synthesis, we perform a
systematic investigation of backdoor attack on text-to-image diffusion models
and propose BadT2I, a general multimodal backdoor attack framework that tampers
with image synthesis in diverse semantic levels. Specifically, we perform
backdoor attacks on three levels of the vision semantics: Pixel-Backdoor,
Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our
methods efficiently inject backdoors into a large-scale text-to-image diffusion
model while preserving its utility with benign inputs. We conduct empirical
experiments on Stable Diffusion, the widely-used text-to-image diffusion model,
demonstrating that the large-scale diffusion model can be easily backdoored
within a few fine-tuning steps. We conduct additional experiments to explore
the impact of different types of textual triggers. Besides, we discuss the
backdoor persistence during further training, the findings of which provide
insights for the development of backdoor defense methods.
</p></li>
</ul>

<h3>Title: Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation. (arXiv:2305.04044v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04044">http://arxiv.org/abs/2305.04044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04044] Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation](http://arxiv.org/abs/2305.04044) #diffusion</code></li>
<li>Summary: <p>Recently, continuous diffusion models (CDM) have been introduced into
non-autoregressive (NAR) text-to-text generation. However, the discrete nature
of text increases the difficulty of CDM to generate coherent and fluent texts,
and also causes the incompatibility problem between CDM and advanced NLP
techniques, especially the popular pre-trained language models~(PLMs). To solve
it, we propose Diffusion-NAT, which introduces discrete diffusion models~(DDM)
into NAR text-to-text generation and integrates BART to improve the
performance. By revising the decoding process of BART and the typical settings
of DDM, we unify the inference process of BART and the denoising process of DDM
into the same NAR masked tokens recovering task. In this way, DDM can rely on
BART to perform denoising, which can benefit from both the rich pre-learned
knowledge of BART and the iterative refining paradigm of DDM. Besides, we also
propose the iterative self-prompting strategy to further improve the generation
quality. Experimental results on 7 datasets show that our approach can
outperform competitive NAR methods, and even surpass autoregressive methods.
Our code and data will be publicly released.
</p></li>
</ul>

<h3>Title: Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems. (arXiv:2305.03774v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03774">http://arxiv.org/abs/2305.03774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03774] Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems](http://arxiv.org/abs/2305.03774) #diffusion</code></li>
<li>Summary: <p>The global push for new energy solutions, such as Geothermal, and Carbon
Capture and Sequestration initiatives has thrust new demands upon the current
state-of the-art subsurface fluid simulators. The requirement to be able to
simulate a large order of reservoir states simultaneously in a short period of
time has opened the door of opportunity for the application of machine learning
techniques for surrogate modelling. We propose a novel physics-informed and
boundary conditions-aware Localized Learning method which extends the
Embed-to-Control (E2C) and Embed-to-Control and Observed (E2CO) models to learn
local representations of global state variables in an Advection-Diffusion
Reaction system. We show that our model trained on reservoir simulation data is
able to predict future states of the system, given a set of controls, to a
great deal of accuracy with only a fraction of the available information, while
also reducing training times significantly compared to the original E2C and
E2CO models.
</p></li>
</ul>

<h3>Title: Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model. (arXiv:2305.03901v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03901">http://arxiv.org/abs/2305.03901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03901] Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model](http://arxiv.org/abs/2305.03901) #diffusion</code></li>
<li>Summary: <p>MRI and PET are crucial diagnostic tools for brain diseases, as they provide
complementary information on brain structure and function. However, PET
scanning is costly and involves radioactive exposure, resulting in a lack of
PET. Moreover, simultaneous PET and MRI at ultra-high-field are currently
hardly infeasible. Ultra-high-field imaging has unquestionably proven valuable
in both clinical and academic settings, especially in the field of cognitive
neuroimaging. These motivate us to propose a method for synthetic PET from
high-filed MRI and ultra-high-field MRI. From a statistical perspective, the
joint probability distribution (JPD) is the most direct and fundamental means
of portraying the correlation between PET and MRI. This paper proposes a novel
joint diffusion attention model which has the joint probability distribution
and attention strategy, named JDAM. JDAM has a diffusion process and a sampling
process. The diffusion process involves the gradual diffusion of PET to
Gaussian noise by adding Gaussian noise, while MRI remains fixed. JPD of MRI
and noise-added PET was learned in the diffusion process. The sampling process
is a predictor-corrector. PET images were generated from MRI by JPD of MRI and
noise-added PET. The predictor is a reverse diffusion process and the corrector
is Langevin dynamics. Experimental results on the public Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset demonstrate that the proposed method
outperforms state-of-the-art CycleGAN for high-field MRI (3T MRI). Finally,
synthetic PET images from the ultra-high-field (5T MRI and 7T MRI) be
attempted, providing a possibility for ultra-high-field PET-MRI imaging.
</p></li>
</ul>

<h3>Title: Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03935">http://arxiv.org/abs/2305.03935</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03935] Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs](http://arxiv.org/abs/2305.03935) #diffusion</code></li>
<li>Summary: <p>Diffusion models have exhibited excellent performance in various domains. The
probability flow ordinary differential equation (ODE) of diffusion models
(i.e., diffusion ODEs) is a particular case of continuous normalizing flows
(CNFs), which enables deterministic inference and exact likelihood evaluation.
However, the likelihood estimation results by diffusion ODEs are still far from
those of the state-of-the-art likelihood-based generative models. In this work,
we propose several improved techniques for maximum likelihood estimation for
diffusion ODEs, including both training and evaluation perspectives. For
training, we propose velocity parameterization and explore variance reduction
techniques for faster convergence. We also derive an error-bounded high-order
flow matching objective for finetuning, which improves the ODE likelihood and
smooths its trajectory. For evaluation, we propose a novel training-free
truncated-normal dequantization to fill the training-evaluation gap commonly
existing in diffusion ODEs. Building upon these techniques, we achieve
state-of-the-art likelihood estimation results on image datasets (2.56 on
CIFAR-10, 3.43 on ImageNet-32) without variational dequantization or data
augmentation.
</p></li>
</ul>

<h3>Title: Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04111">http://arxiv.org/abs/2305.04111</a></li>
<li>Code URL: <a href="https://github.com/tufts-ml/graph-generation-edge">https://github.com/tufts-ml/graph-generation-edge</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04111] Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling](http://arxiv.org/abs/2305.04111) #diffusion</code></li>
<li>Summary: <p>Diffusion-based generative graph models have been proven effective in
generating high-quality small graphs. However, they need to be more scalable
for generating large graphs containing thousands of nodes desiring graph
statistics. In this work, we propose EDGE, a new diffusion-based generative
graph model that addresses generative tasks with large graphs. To improve
computation efficiency, we encourage graph sparsity by using a discrete
diffusion process that randomly removes edges at each time step and finally
obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at
each denoising step. It makes much fewer edge predictions than previous
diffusion-based models. Moreover, EDGE admits explicitly modeling the node
degrees of the graphs, further improving the model performance. The empirical
study shows that EDGE is much more efficient than competing methods and can
generate large graphs with thousands of nodes. It also outperforms baseline
models in generation quality: graphs generated by our approach have more
similar graph statistics to those of the training graphs.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: DBAT: Dynamic Backward Attention Transformer for Material Segmentation with Cross-Resolution Patches. (arXiv:2305.03919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03919">http://arxiv.org/abs/2305.03919</a></li>
<li>Code URL: <a href="https://github.com/heng-yuwen/dynamic-backward-attention-transformer">https://github.com/heng-yuwen/dynamic-backward-attention-transformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03919] DBAT: Dynamic Backward Attention Transformer for Material Segmentation with Cross-Resolution Patches](http://arxiv.org/abs/2305.03919) #transformer</code></li>
<li>Summary: <p>The objective of dense material segmentation is to identify the material
categories for every image pixel. Recent studies adopt image patches to extract
material features. Although the trained networks can improve the segmentation
performance, their methods choose a fixed patch resolution which fails to take
into account the variation in pixel area covered by each material. In this
paper, we propose the Dynamic Backward Attention Transformer (DBAT) to
aggregate cross-resolution features. The DBAT takes cropped image patches as
input and gradually increases the patch resolution by merging adjacent patches
at each transformer stage, instead of fixing the patch resolution during
training. We explicitly gather the intermediate features extracted from
cross-resolution patches and merge them dynamically with predicted attention
masks. Experiments show that our DBAT achieves an accuracy of 86.85%, which is
the best performance among state-of-the-art real-time models. Like other
successful deep learning solutions with complex architectures, the DBAT also
suffers from lack of interpretability. To address this problem, this paper
examines the properties that the DBAT makes use of. By analysing the
cross-resolution features and the attention weights, this paper interprets how
the DBAT learns from image patches. We further align features to semantic
labels, performing network dissection, to infer that the proposed model can
extract material-related features better than other methods. We show that the
DBAT model is more robust to network initialisation, and yields fewer variable
predictions compared to other models. The project code is available at
https://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer.
</p></li>
</ul>

<h3>Title: Transformer-Based Hierarchical Clustering for Brain Network Analysis. (arXiv:2305.04142v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04142">http://arxiv.org/abs/2305.04142</a></li>
<li>Code URL: <a href="https://github.com/ddvd233/thc">https://github.com/ddvd233/thc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04142] Transformer-Based Hierarchical Clustering for Brain Network Analysis](http://arxiv.org/abs/2305.04142) #transformer</code></li>
<li>Summary: <p>Brain networks, graphical models such as those constructed from MRI, have
been widely used in pathological prediction and analysis of brain functions.
Within the complex brain system, differences in neuronal connection strengths
parcellate the brain into various functional modules (network communities),
which are critical for brain analysis. However, identifying such communities
within the brain has been a nontrivial issue due to the complexity of neuronal
interactions. In this work, we propose a novel interpretable transformer-based
model for joint hierarchical cluster identification and brain network
classification. Extensive experimental results on real-world brain network
datasets show that with the help of hierarchical clustering, the model achieves
increased accuracy and reduced runtime complexity while providing plausible
insight into the functional organization of brain regions. The implementation
is available at https://github.com/DDVD233/THC.
</p></li>
</ul>

<h3>Title: UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese. (arXiv:2305.04166v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04166">http://arxiv.org/abs/2305.04166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04166] UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese](http://arxiv.org/abs/2305.04166) #transformer</code></li>
<li>Summary: <p>Image Captioning is one of the vision-language tasks that still interest the
research community worldwide in the 2020s. MS-COCO Caption benchmark is
commonly used to evaluate the performance of advanced captioning models,
although it was published in 2015. Recent captioning models trained on the
MS-COCO Caption dataset only have good performance in language patterns of
English; they do not have such good performance in contexts captured in Vietnam
or fluently caption images using Vietnamese. To contribute to the low-resources
research community as in Vietnam, we introduce a novel image captioning dataset
in Vietnamese, the Open-domain Vietnamese Image Captioning dataset
(UIT-OpenViIC). The introduced dataset includes complex scenes captured in
Vietnam and manually annotated by Vietnamese under strict rules and
supervision. In this paper, we present in more detail the dataset creation
process. From preliminary analysis, we show that our dataset is challenging to
recent state-of-the-art (SOTA) Transformer-based baselines, which performed
well on the MS COCO dataset. Then, the modest results prove that UIT-OpenViIC
has room to grow, which can be one of the standard benchmarks in Vietnamese for
the research community to evaluate their captioning models. Furthermore, we
present a CAMO approach that effectively enhances the image representation
ability by a multi-level encoder output fusion mechanism, which helps improve
the quality of generated captions compared to previous captioning models.
</p></li>
</ul>

<h3>Title: Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04195">http://arxiv.org/abs/2305.04195</a></li>
<li>Code URL: <a href="https://github.com/eanson023/rehamot">https://github.com/eanson023/rehamot</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04195] Cross-Modal Retrieval for Motion and Text via MildTriple Loss](http://arxiv.org/abs/2305.04195) #transformer</code></li>
<li>Summary: <p>Cross-modal retrieval has become a prominent research topic in computer
vision and natural language processing with advances made in image-text and
video-text retrieval technologies. However, cross-modal retrieval between human
motion sequences and text has not garnered sufficient attention despite the
extensive application value it holds, such as aiding virtual reality
applications in better understanding users' actions and language. This task
presents several challenges, including joint modeling of the two modalities,
demanding the understanding of person-centered information from text, and
learning behavior features from 3D human motion sequences. Previous work on
motion data modeling mainly relied on autoregressive feature extractors that
may forget previous information, while we propose an innovative model that
includes simple yet powerful transformer-based motion and text encoders, which
can learn representations from the two different modalities and capture
long-term dependencies. Furthermore, the overlap of the same atomic actions of
different human motions can cause semantic conflicts, leading us to explore a
new triplet loss function, MildTriple Loss. it leverages the similarity between
samples in intra-modal space to guide soft-hard negative sample mining in the
joint embedding space to train the triplet loss and reduce the violation caused
by false negative samples. We evaluated our model and method on the latest
HumanML3D and KIT Motion-Language datasets, achieving a 62.9\% recall for
motion retrieval and a 71.5\% recall for text retrieval (based on R@10) on the
HumanML3D dataset. Our code is available at
https://github.com/eanson023/rehamot.
</p></li>
</ul>

<h3>Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation. (arXiv:2305.03796v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03796">http://arxiv.org/abs/2305.03796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03796] Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation](http://arxiv.org/abs/2305.03796) #transformer</code></li>
<li>Summary: <p>Unlike recurrent models, conventional wisdom has it that Transformers cannot
perfectly model regular languages. Inspired by the notion of working memory, we
propose a new Transformer variant named RegularGPT. With its novel combination
of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT
constructs working memory along the depth dimension, thereby enabling efficient
and successful modeling of regular languages such as PARITY. We further test
RegularGPT on the task of natural language length extrapolation and
surprisingly find that it rediscovers the local windowed attention effect
deemed necessary in prior work for length extrapolation.
</p></li>
</ul>

<h3>Title: Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces. (arXiv:2305.03819v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03819">http://arxiv.org/abs/2305.03819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03819] Adapting Transformer Language Models for Predictive Typing in Brain-Computer Interfaces](http://arxiv.org/abs/2305.03819) #transformer</code></li>
<li>Summary: <p>Brain-computer interfaces (BCI) are an important mode of alternative and
augmentative communication for many people. Unlike keyboards, many BCI systems
do not display even the 26 letters of English at one time, let alone all the
symbols in more complex systems. Using language models to make character-level
predictions, therefore, can greatly speed up BCI typing (Ghosh and Kristensson,
2017). While most existing BCI systems employ character n-gram models or no LM
at all, this paper adapts several wordpiece-level Transformer LMs to make
character predictions and evaluates them on typing tasks. GPT-2 fares best on
clean text, but different LMs react differently to noisy histories. We further
analyze the effect of character positions in a word and context lengths.
</p></li>
</ul>

<h3>Title: An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information. (arXiv:2305.03977v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03977">http://arxiv.org/abs/2305.03977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03977] An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information](http://arxiv.org/abs/2305.03977) #transformer</code></li>
<li>Summary: <p>Non-autoregressive models have been widely studied in the Complete
Information Scenario (CIS), in which the models have complete input information
to obtain corresponding output. However, their explorations in the Incomplete
Information Scenario (IIS) are extremely limited. Our analyses reveal that the
IIS's incomplete input information will augment the inherent limitations of
existing non-autoregressive models trained under Maximum Likelihood Estimation.
In this paper, we propose for the IIS an Adversarial Non-autoregressive
Transformer (ANT) which has two novel features: 1) Position Aware
Self-Modulation to provide more reasonable hidden representations, and 2)
Dependency Feed Forward Network to strengthen its capacity in dependency
modeling. We compare ANT with other mainstream models in the IIS and
demonstrate that ANT can achieve comparable performance with much fewer
decoding iterations. Furthermore, we show its great potential in various
applications like latent interpolation and semi-supervised learning.
</p></li>
</ul>

<h3>Title: Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks. (arXiv:2305.04100v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04100">http://arxiv.org/abs/2305.04100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04100] Rhetorical Role Labeling of Legal Documents using Transformers and Graph Neural Networks](http://arxiv.org/abs/2305.04100) #transformer</code></li>
<li>Summary: <p>A legal document is usually long and dense requiring human effort to parse
it. It also contains significant amounts of jargon which make deriving insights
from it using existing models a poor approach. This paper presents the
approaches undertaken to perform the task of rhetorical role labelling on
Indian Court Judgements as part of SemEval Task 6: understanding legal texts,
shared subtask A. We experiment with graph based approaches like Graph
Convolutional Networks and Label Propagation Algorithm, and transformer-based
approaches including variants of BERT to improve accuracy scores on text
classification of complex legal documents.
</p></li>
</ul>

<h3>Title: MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents. (arXiv:2305.04177v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04177">http://arxiv.org/abs/2305.04177</a></li>
<li>Code URL: <a href="https://github.com/arazd/miread">https://github.com/arazd/miread</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04177] MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents](http://arxiv.org/abs/2305.04177) #transformer</code></li>
<li>Summary: <p>Learning semantically meaningful representations from scientific documents
can facilitate academic literature search and improve performance of
recommendation systems. Pre-trained language models have been shown to learn
rich textual representations, yet they cannot provide powerful document-level
representations for scientific articles. We propose MIReAD, a simple method
that learns high-quality representations of scientific papers by fine-tuning
transformer model to predict the target journal class based on the abstract. We
train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000
journal classes. We show that MIReAD produces representations that can be used
for similar papers retrieval, topic categorization and literature search. Our
proposed approach outperforms six existing models for representation learning
on scientific documents across four evaluation standards.
</p></li>
</ul>

<h3>Title: OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese. (arXiv:2305.04183v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04183">http://arxiv.org/abs/2305.04183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04183] OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese](http://arxiv.org/abs/2305.04183) #transformer</code></li>
<li>Summary: <p>In recent years, visual question answering (VQA) has attracted attention from
the research community because of its highly potential applications (such as
virtual assistance on intelligent cars, assistant devices for blind people, or
information retrieval from document images using natural language as queries)
and challenge. The VQA task requires methods that have the ability to fuse the
information from questions and images to produce appropriate answers. Neural
visual question answering models have achieved tremendous growth on large-scale
datasets which are mostly for resource-rich languages such as English. However,
available datasets narrow the VQA task as the answers selection task or answer
classification task. We argue that this form of VQA is far from human ability
and eliminates the challenge of the answering aspect in the VQA task by just
selecting answers rather than generating them. In this paper, we introduce the
OpenViVQA (Open-domain Vietnamese Visual Question Answering) dataset, the first
large-scale dataset for VQA with open-ended answers in Vietnamese, consists of
11,000+ images associated with 37,000+ question-answer pairs (QAs). Moreover,
we proposed FST, QuMLAG, and MLPAG which fuse information from images and
answers, then use these fused features to construct answers as humans
iteratively. Our proposed methods achieve results that are competitive with
SOTA models such as SAAA, MCAN, LORA, and M4C. The dataset is available to
encourage the research community to develop more generalized algorithms
including transformers for low-resource languages such as Vietnamese.
</p></li>
</ul>

<h3>Title: Spatiotemporal Transformer for Stock Movement Prediction. (arXiv:2305.03835v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03835">http://arxiv.org/abs/2305.03835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03835] Spatiotemporal Transformer for Stock Movement Prediction](http://arxiv.org/abs/2305.03835) #transformer</code></li>
<li>Summary: <p>Financial markets are an intriguing place that offer investors the potential
to gain large profits if timed correctly. Unfortunately, the dynamic,
non-linear nature of financial markets makes it extremely hard to predict
future price movements. Within the US stock exchange, there are a countless
number of factors that play a role in the price of a company's stock, including
but not limited to financial statements, social and news sentiment, overall
market sentiment, political happenings and trading psychology. Correlating
these factors is virtually impossible for a human. Therefore, we propose STST,
a novel approach using a Spatiotemporal Transformer-LSTM model for stock
movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent
against the ACL18 and KDD17 datasets, respectively. In addition, our model was
used in simulation to determine its real-life applicability. It obtained a
minimum of 10.41% higher profit than the S&amp;P500 stock index, with a minimum
annualized return of 31.24%.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Multi-object Video Generation from Single Frame Layouts. (arXiv:2305.03983v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03983">http://arxiv.org/abs/2305.03983</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03983] Multi-object Video Generation from Single Frame Layouts](http://arxiv.org/abs/2305.03983) #generative</code></li>
<li>Summary: <p>In this paper, we study video synthesis with emphasis on simplifying the
generation conditions. Most existing video synthesis models or datasets are
designed to address complex motions of a single object, lacking the ability of
comprehensively understanding the spatio-temporal relationships among multiple
objects. Besides, current methods are usually conditioned on intricate
annotations (e.g. video segmentations) to generate new videos, being
fundamentally less practical. These motivate us to generate multi-object videos
conditioning exclusively on object layouts from a single frame. To solve above
challenges and inspired by recent research on image generation from layouts, we
have proposed a novel video generative framework capable of synthesizing global
scenes with local objects, via implicit neural representations and layout
motion self-inference. Our framework is a non-trivial adaptation from image
generation methods, and is new to this field. In addition, our model has been
evaluated on two widely-used video recognition benchmarks, demonstrating
effectiveness compared to the baseline model.
</p></li>
</ul>

<h3>Title: LEO: Generative Latent Image Animator for Human Video Synthesis. (arXiv:2305.03989v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03989">http://arxiv.org/abs/2305.03989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03989] LEO: Generative Latent Image Animator for Human Video Synthesis](http://arxiv.org/abs/2305.03989) #generative</code></li>
<li>Summary: <p>Spatio-temporal coherency is a major challenge in synthesizing high quality
videos, particularly in synthesizing human videos that contain rich global and
local deformations. To resolve this challenge, previous approaches have
resorted to different features in the generation process aimed at representing
appearance and motion. However, in the absence of strict mechanisms to
guarantee such disentanglement, a separation of motion from appearance has
remained challenging, resulting in spatial distortions and temporal jittering
that break the spatio-temporal coherency. Motivated by this, we here propose
LEO, a novel framework for human video synthesis, placing emphasis on
spatio-temporal coherency. Our key idea is to represent motion as a sequence of
flow maps in the generation process, which inherently isolate motion from
appearance. We implement this idea via a flow-based image animator and a Latent
Motion Diffusion Model (LMDM). The former bridges a space of motion codes with
the space of flow maps, and synthesizes video frames in a warp-and-inpaint
manner. LMDM learns to capture motion prior in the training data by
synthesizing sequences of motion codes. Extensive quantitative and qualitative
analysis suggests that LEO significantly improves coherent synthesis of human
videos over previous methods on the datasets TaichiHD, FaceForensics and
CelebV-HQ. In addition, the effective disentanglement of appearance and motion
in LEO allows for two additional tasks, namely infinite-length human video
synthesis, as well as content-preserving video editing.
</p></li>
</ul>

<h3>Title: A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar Based on Weighted Loss Semi-supervised GAN. (arXiv:2305.04021v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04021">http://arxiv.org/abs/2305.04021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04021] A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar Based on Weighted Loss Semi-supervised GAN](http://arxiv.org/abs/2305.04021) #generative</code></li>
<li>Summary: <p>Deep convolutional neural network has made great achievements in sea-land
clutter classification for over-the-horizon-radar (OTHR). The premise is that a
large number of labeled training samples must be provided for a sea-land
clutter classifier. In practical engineering applications, it is relatively
easy to obtain label-free sea-land clutter samples. However, the labeling
process is extremely cumbersome and requires expertise in the field of OTHR. To
solve this problem, we propose an improved generative adversarial network,
namely weighted loss semi-supervised generative adversarial network (WL-SSGAN).
Specifically, we propose a joint feature matching loss by weighting the middle
layer features of the discriminator of semi-supervised generative adversarial
network. Furthermore, we propose the weighted loss of WL-SSGAN by linearly
weighting standard adversarial loss and joint feature matching loss. The
semi-supervised classification performance of WL-SSGAN is evaluated on a
sea-land clutter dataset. The experimental results show that WL-SSGAN can
improve the performance of the fully supervised classifier with only a small
number of labeled samples by utilizing a large number of unlabeled sea-land
clutter samples. Further, the proposed weighted loss is superior to both the
adversarial loss and the feature matching loss. Additionally, we compare
WL-SSGAN with conventional semi-supervised classification methods and
demonstrate that WL-SSGAN achieves the highest classification accuracy.
</p></li>
</ul>

<h3>Title: Learning Stochastic Dynamical System via Flow Map Operator. (arXiv:2305.03874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03874">http://arxiv.org/abs/2305.03874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03874] Learning Stochastic Dynamical System via Flow Map Operator](http://arxiv.org/abs/2305.03874) #generative</code></li>
<li>Summary: <p>We present a numerical framework for learning unknown stochastic dynamical
systems using measurement data. Termed stochastic flow map learning (sFML), the
new framework is an extension of flow map learning (FML) that was developed for
learning deterministic dynamical systems. For learning stochastic systems, we
define a stochastic flow map that is a superposition of two sub-flow maps: a
deterministic sub-map and a stochastic sub-map. The stochastic training data
are used to construct the deterministic sub-map first, followed by the
stochastic sub-map. The deterministic sub-map takes the form of residual
network (ResNet), similar to the work of FML for deterministic systems. For the
stochastic sub-map, we employ a generative model, particularly generative
adversarial networks (GANs) in this paper. The final constructed stochastic
flow map then defines a stochastic evolution model that is a weak
approximation, in term of distribution, of the unknown stochastic system. A
comprehensive set of numerical examples are presented to demonstrate the
flexibility and effectiveness of the proposed sFML method for various types of
stochastic systems.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04160">http://arxiv.org/abs/2305.04160</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04160] X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages](http://arxiv.org/abs/2305.04160) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated remarkable language abilities.
GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities
beyond previous visual language models. We attribute this to the use of more
advanced LLMs compared with previous multimodal models. Unfortunately, the
model architecture and training strategies of GPT-4 are unknown. To endow LLMs
with multimodal capabilities, we propose X-LLM, which converts Multi-modalities
(images, speech, videos) into foreign languages using X2L interfaces and inputs
them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple
frozen single-modal encoders and a frozen LLM using X2L interfaces, where <code>X''
denotes multi-modalities such as image, speech, and videos, and</code>L'' denotes
languages. X-LLM's training consists of three stages: (1) Converting Multimodal
Information: The first stage trains each X2L interface to align with its
respective single-modal encoder separately to convert multimodal information
into languages. (2) Aligning X2L representations with the LLM: single-modal
encoders are aligned with the LLM through X2L interfaces independently. (3)
Integrating multiple modalities: all single-modal encoders are aligned with the
LLM through X2L interfaces to integrate multimodal capabilities into the LLM.
Our experiments show that X-LLM demonstrates impressive multimodel chat
abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen
images/instructions, and yields a 84.5\% relative score compared with GPT-4 on
a synthetic multimodal instruction-following dataset. And we also conduct
quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote
the era of LLM-based speech recognition.
</p></li>
</ul>

<h3>Title: Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios. (arXiv:2305.03788v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03788">http://arxiv.org/abs/2305.03788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03788] Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios](http://arxiv.org/abs/2305.03788) #large language model</code></li>
<li>Summary: <p>In recent years, major advancements in natural language processing (NLP) have
been driven by the emergence of large language models (LLMs), which have
significantly revolutionized research and development within the field.
Building upon this progress, our study delves into the effects of various
pre-training methodologies on Turkish clinical language models' performance in
a multi-label classification task involving radiology reports, with a focus on
addressing the challenges posed by limited language resources. Additionally, we
evaluated the simultaneous pretraining approach by utilizing limited clinical
task data for the first time. We developed four models, including
TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and
TurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model
(BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a
substantial general-domain corpus, demonstrate the best overall performance.
Although the task-adaptive pre-training approach has the potential to capture
domain-specific patterns, it is constrained by the limited task-specific corpus
and may be susceptible to overfitting. Furthermore, our results underscore the
significance of domain-specific vocabulary during pre-training for enhancing
model performance. Ultimately, we observe that the combination of
general-domain knowledge and task-specific fine-tuning is essential for
achieving optimal performance across a range of categories. This study offers
valuable insights for developing effective Turkish clinical language models and
can guide future research on pre-training techniques for other low-resource
languages within the clinical domain.
</p></li>
</ul>

<h3>Title: Large Language Models in Sport Science &amp; Medicine: Opportunities, Risks and Considerations. (arXiv:2305.03851v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03851">http://arxiv.org/abs/2305.03851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03851] Large Language Models in Sport Science &amp; Medicine: Opportunities, Risks and Considerations](http://arxiv.org/abs/2305.03851) #large language model</code></li>
<li>Summary: <p>This paper explores the potential opportunities, risks, and challenges
associated with the use of large language models (LLMs) in sports science and
medicine. LLMs are large neural networks with transformer style architectures
trained on vast amounts of textual data, and typically refined with human
feedback. LLMs can perform a large range of natural language processing tasks.
In sports science and medicine, LLMs have the potential to support and augment
the knowledge of sports medicine practitioners, make recommendations for
personalised training programs, and potentially distribute high-quality
information to practitioners in developing countries. However, there are also
potential risks associated with the use and development of LLMs, including
biases in the dataset used to create the model, the risk of exposing
confidential data, the risk of generating harmful output, and the need to align
these models with human preferences through feedback. Further research is
needed to fully understand the potential applications of LLMs in sports science
and medicine and to ensure that their use is ethical and beneficial to
athletes, clients, patients, practitioners, and the general public.
</p></li>
</ul>

<h3>Title: Refining the Responses of LLMs by Themselves. (arXiv:2305.04039v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04039">http://arxiv.org/abs/2305.04039</a></li>
<li>Code URL: <a href="https://github.com/henryyantq/optimallm">https://github.com/henryyantq/optimallm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04039] Refining the Responses of LLMs by Themselves](http://arxiv.org/abs/2305.04039) #large language model</code></li>
<li>Summary: <p>In this paper, we propose a simple yet efficient approach based on prompt
engineering that leverages the large language model itself to optimize its
answers without relying on auxiliary models. We introduce an iterative
self-evaluating optimization mechanism, with the potential for improved output
quality as iterations progress, removing the need for manual intervention. The
experiment's findings indicate that utilizing our response refinement framework
on the GPT-3.5 model yields results that are on par with, or even surpass,
those generated by the cutting-edge GPT-4 model. Detailed implementation
strategies and illustrative examples are provided to demonstrate the
superiority of our proposed solution.
</p></li>
</ul>

<h3>Title: Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04091">http://arxiv.org/abs/2305.04091</a></li>
<li>Code URL: <a href="https://github.com/agi-edgerunners/plan-and-solve-prompting">https://github.com/agi-edgerunners/plan-and-solve-prompting</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04091] Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](http://arxiv.org/abs/2305.04091) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have recently been shown to deliver impressive
performance in various NLP tasks. To tackle multi-step reasoning tasks,
few-shot chain-of-thought (CoT) prompting includes a few manually crafted
step-by-step reasoning demonstrations which enable LLMs to explicitly generate
reasoning steps and improve their reasoning task accuracy. To eliminate the
manual effort, Zero-shot-CoT concatenates the target problem statement with
"Let's think step by step" as an input prompt to LLMs. Despite the success of
Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,
missing-step errors, and semantic misunderstanding errors. To address the
missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of
two components: first, devising a plan to divide the entire task into smaller
subtasks, and then carrying out the subtasks according to the plan. To address
the calculation errors and improve the quality of generated reasoning steps, we
extend PS prompting with more detailed instructions and derive PS+ prompting.
We evaluate our proposed prompting strategy on ten datasets across three
reasoning problems. The experimental results over GPT-3 show that our proposed
zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets
by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought
Prompting, and has comparable performance with 8-shot CoT prompting on the math
reasoning problem. The code can be found at
https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.
</p></li>
</ul>

<h3>Title: Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04118">http://arxiv.org/abs/2305.04118</a></li>
<li>Code URL: <a href="https://github.com/zwhe99/MAPS-mt">https://github.com/zwhe99/MAPS-mt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04118] Exploring Human-Like Translation Strategy with Large Language Models](http://arxiv.org/abs/2305.04118) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated impressive capabilities in
general scenarios, exhibiting a level of aptitude that approaches, in some
aspects even surpasses, human-level intelligence. Among their numerous skills,
the translation abilities of LLMs have received considerable attention. In
contrast to traditional machine translation that focuses solely on
source-target mapping, LLM-based translation can potentially mimic the human
translation process that takes many preparatory steps to ensure high-quality
translation. This work aims to explore this possibility by proposing the MAPS
framework, which stands for Multi-Aspect Prompting and Selection. Specifically,
we enable LLMs to first analyze the given source text and extract three aspects
of translation-related knowledge: keywords, topics and relevant demonstrations
to guide the translation process. To filter out the noisy and unhelpful
knowledge, we employ a selection mechanism based on quality estimation.
Experiments suggest that MAPS brings significant and consistent improvements
over text-davinci-003 and Alpaca on eight translation directions from the
latest WMT22 test sets. Our further analysis shows that the extracted knowledge
is critical in resolving up to 59% of hallucination mistakes in translation.
Code is available at https://github.com/zwhe99/MAPS-mt.
</p></li>
</ul>

<h3>Title: Controllable Mixed-Initiative Dialogue Generation through Prompting. (arXiv:2305.04147v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.04147">http://arxiv.org/abs/2305.04147</a></li>
<li>Code URL: <a href="https://github.com/maxlchen/controllable-mixed-initiative-dialogue-generation">https://github.com/maxlchen/controllable-mixed-initiative-dialogue-generation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.04147] Controllable Mixed-Initiative Dialogue Generation through Prompting](http://arxiv.org/abs/2305.04147) #large language model</code></li>
<li>Summary: <p>Mixed-initiative dialogue tasks involve repeated exchanges of information and
conversational control. Conversational agents gain control by generating
responses that follow particular dialogue intents or strategies, prescribed by
a policy planner. The standard approach has been fine-tuning pre-trained
language models to perform generation conditioned on these intents. However,
these supervised generation models are limited by the cost and quality of data
annotation. We instead prompt large language models as a drop-in replacement to
fine-tuning on conditional generation. We formalize prompt construction for
controllable mixed-initiative dialogue. Our findings show improvements over
fine-tuning and ground truth responses according to human evaluation and
automatic metrics for two tasks: PersuasionForGood and Emotional Support
Conversations.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting. (arXiv:2305.03902v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03902">http://arxiv.org/abs/2305.03902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03902] Prompt What You Need: Enhancing Segmentation in Rainy Scenes with Anchor-based Prompting](http://arxiv.org/abs/2305.03902) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation in rainy scenes is a challenging task due to the
complex environment, class distribution imbalance, and limited annotated data.
To address these challenges, we propose a novel framework that utilizes
semi-supervised learning and pre-trained segmentation foundation model to
achieve superior performance. Specifically, our framework leverages the
semi-supervised model as the basis for generating raw semantic segmentation
results, while also serving as a guiding force to prompt pre-trained foundation
model to compensate for knowledge gaps with entropy-based anchors. In addition,
to minimize the impact of irrelevant segmentation masks generated by the
pre-trained foundation model, we also propose a mask filtering and fusion
mechanism that optimizes raw semantic segmentation results based on the
principle of minimum risk. The proposed framework achieves superior
segmentation performance on the Rainy WCity dataset and is awarded the first
prize in the sub-track of STRAIN in ICME 2023 Grand Challenges.
</p></li>
</ul>

<h3>Title: Annotation-efficient learning for OCT segmentation. (arXiv:2305.03936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03936">http://arxiv.org/abs/2305.03936</a></li>
<li>Code URL: <a href="https://github.com/sjtu-intelligent-optics-lab/annotation-efficient-learning-for-oct-segmentation">https://github.com/sjtu-intelligent-optics-lab/annotation-efficient-learning-for-oct-segmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03936] Annotation-efficient learning for OCT segmentation](http://arxiv.org/abs/2305.03936) #segmentation</code></li>
<li>Summary: <p>Deep learning has been successfully applied to OCT segmentation. However, for
data from different manufacturers and imaging protocols, and for different
regions of interest (ROIs), it requires laborious and time-consuming data
annotation and training, which is undesirable in many scenarios, such as
surgical navigation and multi-center clinical trials. Here we propose an
annotation-efficient learning method for OCT segmentation that could
significantly reduce annotation costs. Leveraging self-supervised generative
learning, we train a Transformer-based model to learn the OCT imagery. Then we
connect the trained Transformer-based encoder to a CNN-based decoder, to learn
the dense pixel-wise prediction in OCT segmentation. These training phases use
open-access data and thus incur no annotation costs, and the pre-trained model
can be adapted to different data and ROIs without re-training. Based on the
greedy approximation for the k-center problem, we also introduce an algorithm
for the selective annotation of the target data. We verified our method on
publicly-available and private OCT datasets. Compared to the widely-used U-Net
model with 100% training data, our method only requires ~10% of the data for
achieving the same segmentation accuracy, and it speeds the training up to ~3.5
times. Furthermore, our proposed method outperforms other potential strategies
that could improve annotation efficiency. We think this emphasis on learning
efficiency may help improve the intelligence and application penetration of
OCT-based technologies. Our code and pre-trained model are publicly available
at
https://github.com/SJTU-Intelligent-Optics-Lab/Annotation-efficient-learning-for-OCT-segmentation.
</p></li>
</ul>

<h3>Title: Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation. (arXiv:2305.03944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.03944">http://arxiv.org/abs/2305.03944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.03944] Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation](http://arxiv.org/abs/2305.03944) #segmentation</code></li>
<li>Summary: <p>Existing knowledge distillation works for semantic segmentation mainly focus
on transferring high-level contextual knowledge from teacher to student.
However, low-level texture knowledge is also of vital importance for
characterizing the local structural pattern and global statistical property,
such as boundary, smoothness, regularity and color contrast, which may not be
well addressed by high-level deep features. In this paper, we are intended to
take full advantage of both structural and statistical texture knowledge and
propose a novel Structural and Statistical Texture Knowledge Distillation
(SSTKD) framework for semantic segmentation. Specifically, for structural
texture knowledge, we introduce a Contourlet Decomposition Module (CDM) that
decomposes low-level features with iterative Laplacian pyramid and directional
filter bank to mine the structural texture knowledge. For statistical
knowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)
to adaptively extract and enhance statistical texture knowledge through
heuristics iterative quantization and denoised operation. Finally, each
knowledge learning is supervised by an individual loss function, forcing the
student network to mimic the teacher better from a broader perspective.
Experiments show that the proposed method achieves state-of-the-art performance
on Cityscapes, Pascal VOC 2012 and ADE20K datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
