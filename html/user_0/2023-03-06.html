<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Interruptions detection in video conferences. (arXiv:2303.02052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.02052">http://arxiv.org/abs/2303.02052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.02052] Interruptions detection in video conferences](http://arxiv.org/abs/2303.02052) #security</code></li>
<li>Summary: <p>In recent years, video conferencing (VC) popularity has skyrocketed for a
wide range of activities. As a result, the number of VC users surged sharply.
The sharp increase in VC usage has been accompanied by various newly emerging
privacy and security challenges. VC meetings became a target for various
security attacks, such as Zoombombing. Other VC-related challenges also
emerged. For example, during COVID lockdowns, educators had to teach in online
environments struggling with keeping students engaged for extended periods. In
parallel, the amount of available VC videos has grown exponentially. Thus,
users and companies are limited in finding abnormal segments in VC meetings
within the converging volumes of data. Such abnormal events that affect most
meeting participants may be indicators of interesting points in time, including
security attacks or other changes in meeting climate, like someone joining a
meeting or sharing a dramatic content. Here, we present a novel algorithm for
detecting abnormal events in VC data. We curated VC publicly available
recordings, including meetings with interruptions. We analyzed the videos using
our algorithm, extracting time windows where abnormal occurrences were
detected. Our algorithm is a pipeline that combines multiple methods in several
steps to detect users' faces in each video frame, track face locations during
the meeting and generate vector representations of a facial expression for each
face in each frame. Vector representations are used to monitor changes in
facial expressions throughout the meeting for each participant. The overall
change in meeting climate is quantified using those parameters across all
participants, and translating them into event anomaly detection. This is the
first open pipeline for automatically detecting anomaly events in VC meetings.
Our model detects abnormal events with 92.3% precision over the collected
dataset.
</p></li>
</ul>

<h3>Title: INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification. (arXiv:2303.01510v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01510">http://arxiv.org/abs/2303.01510</a></li>
<li>Code URL: <a href="https://github.com/catrin-baze/ino-of-factify">https://github.com/catrin-baze/ino-of-factify</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01510] INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification](http://arxiv.org/abs/2303.01510) #security</code></li>
<li>Summary: <p>This paper describes our approach to the multi-modal fact verification
(FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of
social media, fake news can spread rapidly and negatively impact social
security. Automatic claim verification becomes more and more crucial to combat
fake news. In fact verification involving multiple modal data, there should be
a structural coherence between claim and document. Therefore, we proposed a
structure coherence-based multi-modal fact verification scheme to classify fake
news. Our structure coherence includes the following four aspects: sentence
length, vocabulary similarity, semantic similarity, and image similarity.
Specifically, CLIP and Sentence BERT are combined to extract text features, and
ResNet50 is used to extract image features. In addition, we also extract the
length of the text as well as the lexical similarity. Then the features were
concatenated and passed through the random forest classifier. Finally, our
weighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2.
</p></li>
</ul>

<h3>Title: A tool assisted methodology to harden programs against multi-faults injections. (arXiv:2303.01885v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01885">http://arxiv.org/abs/2303.01885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01885] A tool assisted methodology to harden programs against multi-faults injections](http://arxiv.org/abs/2303.01885) #security</code></li>
<li>Summary: <p>Fault attacks consist in changing the program behavior by injecting faults at
run-time in order to break some expected security properties. Applications are
hardened against fault attack adding countermeasures. According to the state of
the art, applications must now be protected against multi-fault injection. As a
consequence developing applications which are robust becomes a very challenging
task, in particular because countermeasures can be also the target of attacks.
The aim of this paper is to propose an assisted methodology for developers
allowing to harden an application against multi-fault attacks, addressing
several aspects: how to identify which parts of the code should be protected
and how to choose the most appropriate countermeasures, making the application
more robust and avoiding useless runtime checks.
</p></li>
</ul>

<h3>Title: Exploiting Input Sanitization for Regex Denial of Service. (arXiv:2303.01996v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01996">http://arxiv.org/abs/2303.01996</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01996] Exploiting Input Sanitization for Regex Denial of Service](http://arxiv.org/abs/2303.01996) #security</code></li>
<li>Summary: <p>Web services use server-side input sanitization to guard against harmful
input. Some web services publish their sanitization logic to make their client
interface more usable, e.g., allowing clients to debug invalid requests
locally. However, this usability practice poses a security risk. Specifically,
services may share the regexes they use to sanitize input strings -- and
regex-based denial of service (ReDoS) is an emerging threat. Although prominent
service outages caused by ReDoS have spurred interest in this topic, we know
little about the degree to which live web services are vulnerable to ReDoS.
</p></li>
</ul>

<p>In this paper, we conduct the first black-box study measuring the extent of
ReDoS vulnerabilities in live web services. We apply the Consistent
Sanitization Assumption: that client-side sanitization logic, including
regexes, is consistent with the sanitization logic on the server-side. We
identify a service's regex-based input sanitization in its HTML forms or its
API, find vulnerable regexes among these regexes, craft ReDoS probes, and
pinpoint vulnerabilities. We analyzed the HTML forms of 1,000 services and the
APIs of 475 services. Of these, 355 services publish regexes; 17 services
publish unsafe regexes; and 6 services are vulnerable to ReDoS through their
APIs (6 domains; 15 subdomains). Both Microsoft and Amazon Web Services patched
their web services as a result of our disclosure. Since these vulnerabilities
were from API specifications, not HTML forms, we proposed a ReDoS defense for a
popular API validation library, and our patch has been merged. To summarize: in
client-visible sanitization logic, some web services advertise ReDoS
vulnerabilities in plain sight. Our results motivate short-term patches and
long-term fundamental solutions.
</p>

<h2>privacy</h2>
<h3>Title: Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation. (arXiv:2303.01687v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01687">http://arxiv.org/abs/2303.01687</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01687] Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation](http://arxiv.org/abs/2303.01687) #privacy</code></li>
<li>Summary: <p>Maximum mean discrepancy (MMD) is a particularly useful distance metric for
differentially private data generation: when used with finite-dimensional
features it allows us to summarize and privatize the data distribution once,
which we can repeatedly use during generator training without further privacy
loss. An important question in this framework is, then, what features are
useful to distinguish between real and synthetic data distributions, and
whether those enable us to generate quality synthetic data. This work considers
the using the features of $\textit{neural tangent kernels (NTKs)}$, more
precisely $\textit{empirical}$ NTKs (e-NTKs). We find that, perhaps
surprisingly, the expressiveness of the untrained e-NTK features is comparable
to that of the features taken from pre-trained perceptual features using public
data. As a result, our method improves the privacy-accuracy trade-off compared
to other state-of-the-art methods, without relying on any public data, as
demonstrated on several tabular and image benchmark datasets.
</p></li>
</ul>

<h3>Title: Exploring Machine Learning Privacy/Utility trade-off from a hyperparameters Lens. (arXiv:2303.01819v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01819">http://arxiv.org/abs/2303.01819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01819] Exploring Machine Learning Privacy/Utility trade-off from a hyperparameters Lens](http://arxiv.org/abs/2303.01819) #privacy</code></li>
<li>Summary: <p>Machine Learning (ML) architectures have been applied to several applications
that involve sensitive data, where a guarantee of users' data privacy is
required. Differentially Private Stochastic Gradient Descent (DPSGD) is the
state-of-the-art method to train privacy-preserving models. However, DPSGD
comes at a considerable accuracy loss leading to sub-optimal privacy/utility
trade-offs. Towards investigating new ground for better privacy-utility
trade-off, this work questions; (i) if models' hyperparameters have any
inherent impact on ML models' privacy-preserving properties, and (ii) if
models' hyperparameters have any impact on the privacy/utility trade-off of
differentially private models. We propose a comprehensive design space
exploration of different hyperparameters such as the choice of activation
functions, the learning rate and the use of batch normalization. Interestingly,
we found that utility can be improved by using Bounded RELU as activation
functions with the same privacy-preserving characteristics. With a drop-in
replacement of the activation function, we achieve new state-of-the-art
accuracy on MNIST (96.02\%), FashionMnist (84.76\%), and CIFAR-10 (44.42\%)
without any modification of the learning procedure fundamentals of DPSGD.
</p></li>
</ul>

<h3>Title: GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01621">http://arxiv.org/abs/2303.01621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01621] GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces](http://arxiv.org/abs/2303.01621) #privacy</code></li>
<li>Summary: <p>In this paper we focus on the problem of generating high-quality, private
synthetic glucose traces, a task generalizable to many other time series
sources. Existing methods for time series data synthesis, such as those using
Generative Adversarial Networks (GANs), are not able to capture the innate
characteristics of glucose data and, in terms of privacy, either do not include
any formal privacy guarantees or, in order to uphold a strong formal privacy
guarantee, severely degrade the utility of the synthetic data. Therefore, in
this paper we present GlucoSynth, a novel privacy-preserving GAN framework to
generate synthetic glucose traces. The core intuition in our approach is to
conserve relationships amongst motifs (glucose events) within the traces, in
addition to typical temporal dynamics. Moreover, we integrate differential
privacy into the framework to provide strong formal privacy guarantees.
Finally, we provide a comprehensive evaluation on the real-world utility of the
data using 1.2 million glucose traces
</p></li>
</ul>

<h3>Title: Usability of Privacy Controls in Top Health Websites. (arXiv:2303.01838v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01838">http://arxiv.org/abs/2303.01838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01838] Usability of Privacy Controls in Top Health Websites](http://arxiv.org/abs/2303.01838) #privacy</code></li>
<li>Summary: <p>With the increasing awareness and concerns around privacy, many service
providers offer their users various privacy controls. Through these controls,
users gain greater authority over the collection, utilisation, and
dissemination of their personal information by the services. However, these
controls may be buried deep within menus or settings, making them difficult for
a user to access. Additionally, the terminology used to describe privacy
controls can sometimes be confusing or technical, further complicating the
user's ability to understand and use them effectively. This is especially true
for health websites, as users often share sensitive information about their
health and well-being. While many privacy controls have been proposed to
protect user data on these sites, existing research focuses on individual
controls (e.g., privacy policies or cookie opt-outs) rather than providing a
comprehensive overview of the privacy landscape. In addition, many studies
concentrate on the technical aspects of privacy controls without considering
the usability of these features from a user's perspective. This paper aims to
fill the gaps in the existing work by analysing four privacy controls, namely
privacy nudge, privacy notice, privacy policy, and privacy setting, and
evaluating their usability on the top 100 most visited health websites. First,
we define usability attributes for each privacy control in three website visit
scenarios; the guest, registering, and log-in visits. These attributes include
awareness, efficiency, comprehension, functionality, and choice. Then, we
design a survey template based on these attributes and scenarios and collect
data about privacy controls. Next, we analyse the availability and usability of
each privacy control on health websites. Finally, we provide suggestions for
improving the design of these privacy controls based on the data analysis
results.
</p></li>
</ul>

<h3>Title: Summary Statistic Privacy in Data Sharing. (arXiv:2303.02014v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.02014">http://arxiv.org/abs/2303.02014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.02014] Summary Statistic Privacy in Data Sharing](http://arxiv.org/abs/2303.02014) #privacy</code></li>
<li>Summary: <p>Data sharing between different parties has become increasingly common across
industry and academia. An important class of privacy concerns that arises in
data sharing scenarios regards the underlying distribution of data. For
example, the total traffic volume of data from a networking company can reveal
the scale of its business, which may be considered a trade secret.
Unfortunately, existing privacy frameworks (e.g., differential privacy,
anonymization) do not adequately address such concerns. In this paper, we
propose summary statistic privacy, a framework for analyzing and protecting
these summary statistic privacy concerns. We propose a class of quantization
mechanisms that can be tailored to various data distributions and statistical
secrets, and analyze their privacy-distortion trade-offs under our framework.
We prove corresponding lower bounds on the privacy-utility tradeoff, which
match the tradeoffs of the quantization mechanism under certain regimes, up to
small constant factors. Finally, we demonstrate that the proposed quantization
mechanisms achieve better privacy-distortion tradeoffs than alternative privacy
mechanisms on real-world datasets.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning. (arXiv:2303.01742v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01742">http://arxiv.org/abs/2303.01742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01742] NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning](http://arxiv.org/abs/2303.01742) #defense</code></li>
<li>Summary: <p>At present, backdoor attacks attract attention as they do great harm to deep
learning models. The adversary poisons the training data making the model being
injected with a backdoor after being trained unconsciously by victims using the
poisoned dataset. In the field of text, however, existing works do not provide
sufficient defense against backdoor attacks. In this paper, we propose a
Noise-augmented Contrastive Learning (NCL) framework to defend against textual
backdoor attacks when training models with untrustworthy data. With the aim of
mitigating the mapping between triggers and the target label, we add
appropriate noise perturbing possible backdoor triggers, augment the training
dataset, and then pull homology samples in the feature space utilizing
contrastive learning objective. Experiments demonstrate the effectiveness of
our method in defending three types of textual backdoor attacks, outperforming
the prior works.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: AdvART: Adversarial Art for Camouflaged Object Detection Attacks. (arXiv:2303.01734v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01734">http://arxiv.org/abs/2303.01734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01734] AdvART: Adversarial Art for Camouflaged Object Detection Attacks](http://arxiv.org/abs/2303.01734) #attack</code></li>
<li>Summary: <p>A majority of existing physical attacks in the real world result in
conspicuous and eye-catching patterns for generated patches, which made them
identifiable/detectable by humans. To overcome this limitation, recent work has
proposed several approaches that aim at generating naturalistic patches using
generative adversarial networks (GANs), which may not catch human's attention.
However, these approaches are computationally intensive and do not always
converge to natural looking patterns. In this paper, we propose a novel
lightweight framework that systematically generates naturalistic adversarial
patches without using GANs. To illustrate the proposed approach, we generate
adversarial art (AdvART), which are patches generated to look like artistic
paintings while maintaining high attack efficiency. In fact, we redefine the
optimization problem by introducing a new similarity objective. Specifically,
we leverage similarity metrics to construct a similarity loss that is added to
the optimized objective function. This component guides the patch to follow a
predefined artistic patterns while maximizing the victim model's loss function.
Our patch achieves high success rates with $12.53\%$ mean average precision
(mAP) on YOLOv4tiny for INRIA dataset.
</p></li>
</ul>

<h3>Title: Modeling and Exploration of Gain Competition Attacks in Optical Network-on-Chip Architectures. (arXiv:2303.01550v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01550">http://arxiv.org/abs/2303.01550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01550] Modeling and Exploration of Gain Competition Attacks in Optical Network-on-Chip Architectures](http://arxiv.org/abs/2303.01550) #attack</code></li>
<li>Summary: <p>Network-on-Chip (NoC) enables energy-efficient communication between numerous
components in System-on-Chip architectures. The optical NoC is widely
considered a key technology to overcome the bandwidth and energy limitations of
traditional electrical on-chip interconnects. While optical NoC can offer high
performance, they come with inherent security vulnerabilities due to the nature
of optical interconnects.
</p></li>
</ul>

<p>In this paper, we investigate the gain competition attack in optical NoCs,
which can be initiated by an attacker injecting a high-power signal to the
optical waveguide, robbing the legitimate signals of amplification. To the best
of our knowledge, our proposed approach is the first attempt to investigate
gain competition attacks as a security threat in optical NoCs. We model the
attack and analyze its effects on optical NoC performance. We also propose
potential attack detection techniques and countermeasures to mitigate the
attack. Our experimental evaluation using different NoC topologies and diverse
traffic patterns demonstrates the effectiveness of our modeling and exploration
of gain competition attacks in optical NoC architectures.
</p>

<h3>Title: Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias. (arXiv:2303.01504v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01504">http://arxiv.org/abs/2303.01504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01504] Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias](http://arxiv.org/abs/2303.01504) #attack</code></li>
<li>Summary: <p>With the swift advancement of deep learning, state-of-the-art algorithms have
been utilized in various social situations. Nonetheless, some algorithms have
been discovered to exhibit biases and provide unequal results. The current
debiasing methods face challenges such as poor utilization of data or intricate
training requirements. In this work, we found that the backdoor attack can
construct an artificial bias similar to the model bias derived in standard
training. Considering the strong adjustability of backdoor triggers, we are
motivated to mitigate the model bias by carefully designing reverse artificial
bias created from backdoor attack. Based on this, we propose a backdoor
debiasing framework based on knowledge distillation, which effectively reduces
the model bias from original data and minimizes security risks from the
backdoor attack. The proposed solution is validated on both image and
structured datasets, showing promising results. This work advances the
understanding of backdoor attacks and highlights its potential for beneficial
applications. The code for the study can be found at
\url{https://anonymous.4open.science/r/DwB-BC07/}.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators. (arXiv:2303.01538v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01538">http://arxiv.org/abs/2303.01538</a></li>
<li>Code URL: <a href="https://github.com/lenbrocki/feature-perturbation-augmentation">https://github.com/lenbrocki/feature-perturbation-augmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01538] Feature Perturbation Augmentation for Reliable Evaluation of Importance Estimators](http://arxiv.org/abs/2303.01538) #robust</code></li>
<li>Summary: <p>Post-hoc explanation methods attempt to make the inner workings of deep
neural networks more interpretable. However, since a ground truth is in general
lacking, local post-hoc interpretability methods, which assign importance
scores to input features, are challenging to evaluate. One of the most popular
evaluation frameworks is to perturb features deemed important by an
interpretability method and to measure the change in prediction accuracy.
Intuitively, a large decrease in prediction accuracy would indicate that the
explanation has correctly quantified the importance of features with respect to
the prediction outcome (e.g., logits). However, the change in the prediction
outcome may stem from perturbation artifacts, since perturbed samples in the
test dataset are out of distribution (OOD) compared to the training dataset and
can therefore potentially disturb the model in an unexpected manner. To
overcome this challenge, we propose feature perturbation augmentation (FPA)
which creates and adds perturbed images during the model training. Through
extensive computational experiments, we demonstrate that FPA makes deep neural
networks (DNNs) more robust against perturbations. Furthermore, training DNNs
with FPA demonstrate that the sign of importance scores may explain the model
more meaningfully than has previously been assumed. Overall, FPA is an
intuitive data augmentation technique that improves the evaluation of post-hoc
interpretability methods.
</p></li>
</ul>

<h3>Title: Counterfactual Edits for Generative Evaluation. (arXiv:2303.01555v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01555">http://arxiv.org/abs/2303.01555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01555] Counterfactual Edits for Generative Evaluation](http://arxiv.org/abs/2303.01555) #robust</code></li>
<li>Summary: <p>Evaluation of generative models has been an underrepresented field despite
the surge of generative architectures. Most recent models are evaluated upon
rather obsolete metrics which suffer from robustness issues, while being unable
to assess more aspects of visual quality, such as compositionality and logic of
synthesis. At the same time, the explainability of generative models remains a
limited, though important, research direction with several current attempts
requiring access to the inner functionalities of generative models. Contrary to
prior literature, we view generative models as a black box, and we propose a
framework for the evaluation and explanation of synthesized results based on
concepts instead of pixels. Our framework exploits knowledge-based
counterfactual edits that underline which objects or attributes should be
inserted, removed, or replaced from generated images to approach their ground
truth conditioning. Moreover, global explanations produced by accumulating
local edits can also reveal what concepts a model cannot generate in total. The
application of our framework on various models designed for the challenging
tasks of Story Visualization and Scene Synthesis verifies the power of our
approach in the model-agnostic setting.
</p></li>
</ul>

<h3>Title: Improving GAN Training via Feature Space Shrinkage. (arXiv:2303.01559v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01559">http://arxiv.org/abs/2303.01559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01559] Improving GAN Training via Feature Space Shrinkage](http://arxiv.org/abs/2303.01559) #robust</code></li>
<li>Summary: <p>Due to the outstanding capability for data generation, Generative Adversarial
Networks (GANs) have attracted considerable attention in unsupervised learning.
However, training GANs is difficult, since the training distribution is dynamic
for the discriminator, leading to unstable image representation. In this paper,
we address the problem of training GANs from a novel perspective, \emph{i.e.,}
robust image classification. Motivated by studies on robust image
representation, we propose a simple yet effective module, namely AdaptiveMix,
for GANs, which shrinks the regions of training data in the image
representation space of the discriminator. Considering it is intractable to
directly bound feature space, we propose to construct hard samples and narrow
down the feature distance between hard and easy samples. The hard samples are
constructed by mixing a pair of training images. We evaluate the effectiveness
of our AdaptiveMix with widely-used and state-of-the-art GAN architectures. The
evaluation results demonstrate that our AdaptiveMix can facilitate the training
of GANs and effectively improve the image quality of generated samples. We also
show that our AdaptiveMix can be further applied to image classification and
Out-Of-Distribution (OOD) detection tasks, by equipping it with
state-of-the-art methods. Extensive experiments on seven publicly available
datasets show that our method effectively boosts the performance of baselines.
The code is publicly available at
https://github.com/WentianZhang-ML/AdaptiveMix.
</p></li>
</ul>

<h3>Title: Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View. (arXiv:2303.01686v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01686">http://arxiv.org/abs/2303.01686</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01686] Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View](http://arxiv.org/abs/2303.01686) #robust</code></li>
<li>Summary: <p>Multi-view 3D object detection (MV3D-Det) in Bird-Eye-View (BEV) has drawn
extensive attention due to its low cost and high efficiency. Although new
algorithms for camera-only 3D object detection have been continuously proposed,
most of them may risk drastic performance degradation when the domain of input
images differs from that of training. In this paper, we first analyze the
causes of the domain gap for the MV3D-Det task. Based on the covariate shift
assumption, we find that the gap mainly attributes to the feature distribution
of BEV, which is determined by the quality of both depth estimation and 2D
image's feature representation. To acquire a robust depth prediction, we
propose to decouple the depth estimation from the intrinsic parameters of the
camera (i.e. the focal length) through converting the prediction of metric
depth to that of scale-invariant depth and perform dynamic perspective
augmentation to increase the diversity of the extrinsic parameters (i.e. the
camera poses) by utilizing homography. Moreover, we modify the focal length
values to create multiple pseudo-domains and construct an adversarial training
loss to encourage the feature representation to be more domain-agnostic.
Without bells and whistles, our approach, namely DG-BEV, successfully
alleviates the performance drop on the unseen target domain without impairing
the accuracy of the source domain. Extensive experiments on various public
datasets, including Waymo, nuScenes, and Lyft, demonstrate the generalization
and effectiveness of our approach. To the best of our knowledge, this is the
first systematic study to explore a domain generalization method for MV3D-Det.
</p></li>
</ul>

<h3>Title: A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation. (arXiv:2303.01743v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01743">http://arxiv.org/abs/2303.01743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01743] A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation](http://arxiv.org/abs/2303.01743) #robust</code></li>
<li>Summary: <p>Estimating the 3DoF rotation from a single RGB image is an important yet
challenging problem. Probabilistic rotation regression has raised more and more
attention with the benefit of expressing uncertainty information along with the
prediction. Though modeling noise using Gaussian-resembling Bingham
distribution and matrix Fisher distribution is natural, they are shown to be
sensitive to outliers for the nature of quadratic punishment to deviations. In
this paper, we draw inspiration from multivariate Laplace distribution and
propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace
distribution is robust to the disturbance of outliers and enforces much
gradient to the low-error region, resulting in a better convergence. Our
extensive experiments show that our proposed distribution achieves
state-of-the-art performance for rotation regression tasks over both
probabilistic and non-probabilistic baselines. Our project page is at
https://pku-epic.github.io/RotationLaplace.
</p></li>
</ul>

<h3>Title: Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models. (arXiv:2303.01870v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01870">http://arxiv.org/abs/2303.01870</a></li>
<li>Code URL: <a href="https://github.com/nmndeep/revisiting-at">https://github.com/nmndeep/revisiting-at</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01870] Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models](http://arxiv.org/abs/2303.01870) #robust</code></li>
<li>Summary: <p>While adversarial training has been extensively studied for ResNet
architectures and low resolution datasets like CIFAR, much less is known for
ImageNet. Given the recent debate about whether transformers are more robust
than convnets, we revisit adversarial training on ImageNet comparing ViTs and
ConvNeXts. Extensive experiments show that minor changes in architecture, most
notably replacing PatchStem with ConvStem, and training scheme have a
significant impact on the achieved robustness. These changes not only increase
robustness in the seen $\ell_\infty$-threat model, but even more so improve
generalization to unseen $\ell_1/\ell_2$-robustness. Our modified ConvNeXt,
ConvNeXt + ConvStem, yields the most robust models across different ranges of
model parameters and FLOPs.
</p></li>
</ul>

<h3>Title: Robust Detection Outcome: A Metric for Pathology Detection in Medical Images. (arXiv:2303.01920v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01920">http://arxiv.org/abs/2303.01920</a></li>
<li>Code URL: <a href="https://github.com/felime/rodeo">https://github.com/felime/rodeo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01920] Robust Detection Outcome: A Metric for Pathology Detection in Medical Images](http://arxiv.org/abs/2303.01920) #robust</code></li>
<li>Summary: <p>Detection of pathologies is a fundamental task in medical imaging and the
evaluation of algorithms that can perform this task automatically is crucial.
However, current object detection metrics for natural images do not reflect the
specific clinical requirements in pathology detection sufficiently. To tackle
this problem, we propose Robust Detection Outcome (RoDeO); a novel metric for
evaluating algorithms for pathology detection in medical images, especially in
chest X-rays. RoDeO evaluates different errors directly and individually, and
reflects clinical needs better than current metrics. Extensive evaluation on
the ChestX-ray8 dataset shows the superiority of our metrics compared to
existing ones. We released the code at https://github.com/FeliMe/RoDeO and
published RoDeO as pip package (rodeometric).
</p></li>
</ul>

<h3>Title: PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees. (arXiv:2303.01959v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01959">http://arxiv.org/abs/2303.01959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01959] PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees](http://arxiv.org/abs/2303.01959) #robust</code></li>
<li>Summary: <p>Point cloud classification is an essential component in many
security-critical applications such as autonomous driving and augmented
reality. However, point cloud classifiers are vulnerable to adversarially
perturbed point clouds. Existing certified defenses against adversarial point
clouds suffer from a key limitation: their certified robustness guarantees are
probabilistic, i.e., they produce an incorrect certified robustness guarantee
with some probability. In this work, we propose a general framework, namely
PointCert, that can transform an arbitrary point cloud classifier to be
certifiably robust against adversarial point clouds with deterministic
guarantees. PointCert certifiably predicts the same label for a point cloud
when the number of arbitrarily added, deleted, and/or modified points is less
than a threshold. Moreover, we propose multiple methods to optimize the
certified robustness guarantees of PointCert in three application scenarios. We
systematically evaluate PointCert on ModelNet and ScanObjectNN benchmark
datasets. Our results show that PointCert substantially outperforms
state-of-the-art certified defenses even though their robustness guarantees are
probabilistic.
</p></li>
</ul>

<h3>Title: Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective. (arXiv:2303.02095v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.02095">http://arxiv.org/abs/2303.02095</a></li>
<li>Code URL: <a href="https://github.com/transmuteai/data-efficient-transformers">https://github.com/transmuteai/data-efficient-transformers</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.02095] Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective](http://arxiv.org/abs/2303.02095) #robust</code></li>
<li>Summary: <p>Coreset selection is among the most effective ways to reduce the training
time of CNNs, however, only limited is known on how the resultant models will
behave under variations of the coreset size, and choice of datasets and models.
Moreover, given the recent paradigm shift towards transformer-based models, it
is still an open question how coreset selection would impact their performance.
There are several similar intriguing questions that need to be answered for a
wide acceptance of coreset selection methods, and this paper attempts to answer
some of these. We present a systematic benchmarking setup and perform a
rigorous comparison of different coreset selection methods on CNNs and
transformers. Our investigation reveals that under certain circumstances,
random selection of subsets is more robust and stable when compared with the
SOTA selection methods. We demonstrate that the conventional concept of uniform
subset sampling across the various classes of the data is not the appropriate
choice. Rather samples should be adaptively chosen based on the complexity of
the data distribution for each class. Transformers are generally pretrained on
large datasets, and we show that for certain target datasets, it helps to keep
their performance stable at even very small coreset sizes. We further show that
when no pretraining is done or when the pretrained transformer models are used
with non-natural images (e.g. medical data), CNNs tend to generalize better
than transformers at even very small coreset sizes. Lastly, we demonstrate that
in the absence of the right pretraining, CNNs are better at learning the
semantic coherence between spatially distant objects within an image, and these
tend to outperform transformers at almost all choices of the coreset size.
</p></li>
</ul>

<h3>Title: Convex Bounds on the Softmax Function with Applications to Robustness Verification. (arXiv:2303.01713v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01713">http://arxiv.org/abs/2303.01713</a></li>
<li>Code URL: <a href="https://github.com/neuralnetworkverification/bounding-softmax">https://github.com/neuralnetworkverification/bounding-softmax</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01713] Convex Bounds on the Softmax Function with Applications to Robustness Verification](http://arxiv.org/abs/2303.01713) #robust</code></li>
<li>Summary: <p>The softmax function is a ubiquitous component at the output of neural
networks and increasingly in intermediate layers as well. This paper provides
convex lower bounds and concave upper bounds on the softmax function, which are
compatible with convex optimization formulations for characterizing neural
networks and other ML models. We derive bounds using both a natural
exponential-reciprocal decomposition of the softmax as well as an alternative
decomposition in terms of the log-sum-exp function. The new bounds are provably
and/or numerically tighter than linear bounds obtained in previous work on
robustness verification of transformers. As illustrations of the utility of the
bounds, we apply them to verification of transformers as well as of the
robustness of predictive uncertainty estimates of deep ensembles.
</p></li>
</ul>

<h3>Title: Robust One-Class Classification with Signed Distance Function using 1-Lipschitz Neural Networks. (arXiv:2303.01978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01978">http://arxiv.org/abs/2303.01978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01978] Robust One-Class Classification with Signed Distance Function using 1-Lipschitz Neural Networks](http://arxiv.org/abs/2303.01978) #robust</code></li>
<li>Summary: <p>We propose a new method, dubbed One Class Signed Distance Function (OCSDF),
to perform One Class Classification (OCC) by provably learning the Signed
Distance Function (SDF) to the boundary of the support of any distribution. The
distance to the support can be interpreted as a normality score, and its
approximation using 1-Lipschitz neural networks provides robustness bounds
against l2 adversarial attacks, an under-explored weakness of deep
learning-based OCC algorithms. As a result, OCSDF comes with a new metric,
certified AUROC, that can be computed at the same cost as any classical AUROC.
We show that OCSDF is competitive against concurrent methods on tabular and
image data while being way more robust to adversarial attacks, illustrating its
theoretical properties. Finally, as exploratory research perspectives, we
theoretically and empirically show how OCSDF connects OCC with image generation
and implicit neural surface parametrization. Our code is available at
https://github.com/Algue-Rythme/OneClassMetricLearning
</p></li>
</ul>

<h3>Title: Physics-Informed Deep Learning For Traffic State Estimation: A Survey and the Outlook. (arXiv:2303.02063v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.02063">http://arxiv.org/abs/2303.02063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.02063] Physics-Informed Deep Learning For Traffic State Estimation: A Survey and the Outlook](http://arxiv.org/abs/2303.02063) #robust</code></li>
<li>Summary: <p>For its robust predictive power (compared to pure physics-based models) and
sample-efficient training (compared to pure deep learning models),
physics-informed deep learning (PIDL), a paradigm hybridizing physics-based
models and deep neural networks (DNN), has been booming in science and
engineering fields. One key challenge of applying PIDL to various domains and
problems lies in the design of a computational graph that integrates physics
and DNNs. In other words, how physics are encoded into DNNs and how the physics
and data components are represented. In this paper, we provide a variety of
architecture designs of PIDL computational graphs and how these structures are
customized to traffic state estimation (TSE), a central problem in
transportation engineering. When observation data, problem type, and goal vary,
we demonstrate potential architectures of PIDL computational graphs and compare
these variants using the same real-world dataset.
</p></li>
</ul>

<h3>Title: Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!. (arXiv:2303.02141v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.02141">http://arxiv.org/abs/2303.02141</a></li>
<li>Code URL: <a href="https://github.com/vita-group/smc-bench">https://github.com/vita-group/smc-bench</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.02141] Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!](http://arxiv.org/abs/2303.02141) #robust</code></li>
<li>Summary: <p>Sparse Neural Networks (SNNs) have received voluminous attention
predominantly due to growing computational and memory footprints of
consistently exploding parameter count in large-scale models. Similar to their
dense counterparts, recent SNNs generalize just as well and are equipped with
numerous favorable benefits (e.g., low complexity, high scalability, and
robustness), sometimes even better than the original dense networks. As
research effort is focused on developing increasingly sophisticated sparse
algorithms, it is startling that a comprehensive benchmark to evaluate the
effectiveness of these algorithms has been highly overlooked. In absence of a
carefully crafted evaluation benchmark, most if not all, sparse algorithms are
evaluated against fairly simple and naive tasks (eg. CIFAR, ImageNet, GLUE,
etc.), which can potentially camouflage many advantages as well unexpected
predicaments of SNNs. In pursuit of a more general evaluation and unveiling the
true potential of sparse algorithms, we introduce "Sparsity May Cry" Benchmark
(SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10
datasets, that accounts for capturing a wide range of domain-specific and
sophisticated knowledge. Our systemic evaluation of the most representative
sparse algorithms reveals an important obscured observation: the
state-of-the-art magnitude- and/or gradient-based sparse algorithms seemingly
fail to perform on SMC-Bench when applied out-of-the-box, sometimes at
significantly trivial sparsity as low as 5%. By incorporating these
well-thought and diverse tasks, SMC-Bench is designed to favor and encourage
the development of more scalable and generalizable sparse algorithms.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedML Parrot: A Scalable Federated Learning System via Heterogeneity-aware Scheduling on Sequential and Hierarchical Training. (arXiv:2303.01778v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01778">http://arxiv.org/abs/2303.01778</a></li>
<li>Code URL: <a href="https://github.com/FedML-AI/FedML">https://github.com/FedML-AI/FedML</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01778] FedML Parrot: A Scalable Federated Learning System via Heterogeneity-aware Scheduling on Sequential and Hierarchical Training](http://arxiv.org/abs/2303.01778) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) enables collaborations among clients for train
machine learning models while protecting their data privacy. Existing FL
simulation platforms that are designed from the perspectives of traditional
distributed training, suffer from laborious code migration between simulation
and production, low efficiency, low GPU utility, low scalability with high
hardware requirements and difficulty of simulating stateful clients. In this
work, we firstly demystify the challenges and bottlenecks of simulating FL, and
design a new FL system named as FedML \texttt{Parrot}. It improves the training
efficiency, remarkably relaxes the requirements on the hardware, and supports
efficient large-scale FL experiments with stateful clients by: (1) sequential
training clients on devices; (2) decomposing original aggregation into local
and global aggregation on devices and server respectively; (3) scheduling tasks
to mitigate straggler problems and enhance computing utility; (4) distributed
client state manager to support various FL algorithms. Besides, built upon our
generic APIs and communication interfaces, users can seamlessly transform the
simulation into the real-world deployment without modifying codes. We evaluate
\texttt{Parrot} through extensive experiments for training diverse models on
various FL datasets to demonstrate that \texttt{Parrot} can achieve simulating
over 1000 clients (stateful or stateless) with flexible GPU devices setting ($4
\sim 32$) and high GPU utility, 1.2 $\sim$ 4 times faster than FedScale, and 10
$\sim$ 100 times memory saving than FedML. And we verify that \texttt{Parrot}
works well with homogeneous and heterogeneous devices in three different
clusters. Two FL algorithms with stateful clients and four algorithms with
stateless clients are simulated to verify the wide adaptability of
\texttt{Parrot} to different algorithms.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Automatic Increase Market Systems (AIMS): Towards a deterministic theory for cryptocurrencies. (arXiv:2303.01735v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01735">http://arxiv.org/abs/2303.01735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01735] Automatic Increase Market Systems (AIMS): Towards a deterministic theory for cryptocurrencies](http://arxiv.org/abs/2303.01735) #fair</code></li>
<li>Summary: <p>The popularity of cryptocurrencies has grown significantly in recent years,
and they have become an important asset for internet trading. One of the main
drawbacks of cryptocurrencies is the high volatility and fluctuation in value.
The value of cryptocurrencies can change rapidly and dramatically, making them
a risky investment. Cryptocurrencies are largely unregulated, which can
exacerbate their volatility. The high volatility of cryptocurrencies has also
led to a speculative bubble, with many investors buying and selling
cryptocurrencies based on short-term price fluctuations rather than their
underlying values. Therefore, how to reduce the fluctuation risk introduced by
exchanges, transform uncertain prices to deterministic value, and promote the
benefits of decentralized finance are critical for the future development of
cryptos and Web 3.0.
</p></li>
</ul>

<p>To address the issues, this paper proposes a novel theory as Automatic
Increase Market Systems (AIMS) for cryptos, which could potentially be designed
to automatically adjust the value of a cryptocurrency helping to stabilize the
price and increase its value over time in a deterministic manner. We build a
crypto, WISH (https://wishbank.wtf), based on AIMS in order to demonstrate how
the automatic increase market system would work in practice, and how it would
influence the supply of the cryptocurrency in response to market demand and
finally make itself to be a stable medium of exchange, ensuring that the AIMS
is fair and transparent.
</p>

<h3>Title: Understanding and Unifying Fourteen Attribution Methods with Taylor Interactions. (arXiv:2303.01506v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01506">http://arxiv.org/abs/2303.01506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01506] Understanding and Unifying Fourteen Attribution Methods with Taylor Interactions](http://arxiv.org/abs/2303.01506) #fair</code></li>
<li>Summary: <p>Various attribution methods have been developed to explain deep neural
networks (DNNs) by inferring the attribution/importance/contribution score of
each input variable to the final output. However, existing attribution methods
are often built upon different heuristics. There remains a lack of a unified
theoretical understanding of why these methods are effective and how they are
related. To this end, for the first time, we formulate core mechanisms of
fourteen attribution methods, which were designed on different heuristics, into
the same mathematical system, i.e., the system of Taylor interactions.
Specifically, we prove that attribution scores estimated by fourteen
attribution methods can all be reformulated as the weighted sum of two types of
effects, i.e., independent effects of each individual input variable and
interaction effects between input variables. The essential difference among the
fourteen attribution methods mainly lies in the weights of allocating different
effects. Based on the above findings, we propose three principles for a fair
allocation of effects to evaluate the faithfulness of the fourteen attribution
methods.
</p></li>
</ul>

<h3>Title: Enhancing Fairness in AI-based Travel Demand Forecasting Models. (arXiv:2303.01692v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01692">http://arxiv.org/abs/2303.01692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01692] Enhancing Fairness in AI-based Travel Demand Forecasting Models](http://arxiv.org/abs/2303.01692) #fair</code></li>
<li>Summary: <p>Artificial Intelligence (AI) and machine learning have been increasingly
adopted for forecasting real-time travel demand. These AI-based travel demand
forecasting models, though generate highly-accurate predictions, may produce
prediction biases and thus raise fairness issues. Using such models for
decision-making, we may develop transportation policies that could exacerbate
social inequalities. However, limited studies have been focused on addressing
the fairness issues of AI-based travel demand forecasting models. Therefore, in
this study, we propose a novel methodology to develop fairness-aware travel
demand forecasting models, which are highly accurate and fair. Specifically, we
add a fairness regularization term, i.e., the correlation between prediction
accuracy and the protected attribute such as race or income, into the loss
function of the travel demand forecasting model. We include an interactive
weight coefficient to both accuracy loss term and fairness loss term. The
travel demand forecasting models can thus simultaneously account for prediction
accuracy and fairness. An empirical analysis is conducted using real-world
ridesourcing-trip data in Chicago. Results show that our proposed methodology
effectively addresses the accuracy-fairness trade-off. It can significantly
enhance fairness for multiple protected attributes (i.e., race, education, age
and income) by only sacrificing a small accuracy drop. This study provides
transportation professionals a new type of decision-support tool to achieve
fair and accurate travel demand forecasting.
</p></li>
</ul>

<h3>Title: Model Explanation Disparities as a Fairness Diagnostic. (arXiv:2303.01704v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01704">http://arxiv.org/abs/2303.01704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01704] Model Explanation Disparities as a Fairness Diagnostic](http://arxiv.org/abs/2303.01704) #fair</code></li>
<li>Summary: <p>In recent years, there has been a flurry of research focusing on the fairness
of machine learning models, and in particular on quantifying and eliminating
bias against subgroups. One prominent line of work generalizes the notion of
subgroups beyond simple discrete classes by introducing the notion of a "rich
subgroup," and seeks to train models that are calibrated or equalize error
rates with respect to these richer subgroup classes. Largely orthogonally,
there has been growing recognition of the importance of understanding how
subgroups of the dataset are being treated relative to the rest of the dataset.
It can easily be shown that certain training features may be significantly more
important (or less important) on a discrete subgroup compared to the whole
dataset with this difference being called Feature Importance Disparity (FID).
However, there are an exponentially large number of rich subgroups defined by a
structured class of functions over protected features (such as race, gender,
age, etc.) and there are many ways that feature importance can be defined. In
this paper, we develop two approaches to efficiently search the rich subgroup
space and find feature/subgroup pairs with large FID that fit within a
specified subgroup size. The first approach considers feature importance
metrics which are separable and models a two-player, zero-sum game to reduce
the computation of subgroups with high FID of constrained size to a
cost-sensitive classification problem. The second approach considers
non-separable importance metrics and uses heuristic optimization techniques to
converge on the subgroups. Both of these approaches were tested on 4 different
datasets with multiple importance notions and found feature/subgroup pairs that
had high FID, often by orders of magnitude, and yield interesting discussions
about the reliability and fairness of the datasets.
</p></li>
</ul>

<h3>Title: FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values. (arXiv:2303.01928v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01928">http://arxiv.org/abs/2303.01928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01928] FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values](http://arxiv.org/abs/2303.01928) #fair</code></li>
<li>Summary: <p>In this paper, we propose FairShap, a novel and interpretable pre-processing
(re-weighting) method for fair algorithmic decision-making through data
valuation. FairShap is based on the Shapley Value, a well-known mathematical
framework from game theory to achieve a fair allocation of resources. Our
approach is easily interpretable, as it measures the contribution of each
training data point to a predefined fairness metric. We empirically validate
FairShap on several state-of-the-art datasets of different nature, with
different training scenarios and models. The proposed approach outperforms
other methods, yielding significantly fairer models with similar levels of
accuracy. In addition, we illustrate FairShap's interpretability by means of
histograms and latent space visualizations. We believe this work represents a
promising direction in interpretable, model-agnostic approaches to algorithmic
fairness.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: BayeSeg: Bayesian Modeling for Medical Image Segmentation with Interpretable Generalizability. (arXiv:2303.01710v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01710">http://arxiv.org/abs/2303.01710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01710] BayeSeg: Bayesian Modeling for Medical Image Segmentation with Interpretable Generalizability](http://arxiv.org/abs/2303.01710) #interpretability</code></li>
<li>Summary: <p>Due to the cross-domain distribution shift aroused from diverse medical
imaging systems, many deep learning segmentation methods fail to perform well
on unseen data, which limits their real-world applicability. Recent works have
shown the benefits of extracting domain-invariant representations on domain
generalization. However, the interpretability of domain-invariant features
remains a great challenge. To address this problem, we propose an interpretable
Bayesian framework (BayeSeg) through Bayesian modeling of image and label
statistics to enhance model generalizability for medical image segmentation.
Specifically, we first decompose an image into a spatial-correlated variable
and a spatial-variant variable, assigning hierarchical Bayesian priors to
explicitly force them to model the domain-stable shape and domain-specific
appearance information respectively. Then, we model the segmentation as a
locally smooth variable only related to the shape. Finally, we develop a
variational Bayesian framework to infer the posterior distributions of these
explainable variables. The framework is implemented with neural networks, and
thus is referred to as deep Bayesian segmentation. Quantitative and qualitative
experimental results on prostate segmentation and cardiac segmentation tasks
have shown the effectiveness of our proposed method. Moreover, we investigated
the interpretability of BayeSeg by explaining the posteriors and analyzed
certain factors that affect the generalization ability through further ablation
studies. Our code will be released via https://zmiclab.github.io/projects.html,
once the manuscript is accepted for publication.
</p></li>
</ul>

<h3>Title: PPCR: Learning Pyramid Pixel Context Recalibration Module for Medical Image Classification. (arXiv:2303.01917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01917">http://arxiv.org/abs/2303.01917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01917] PPCR: Learning Pyramid Pixel Context Recalibration Module for Medical Image Classification](http://arxiv.org/abs/2303.01917) #interpretability</code></li>
<li>Summary: <p>Spatial attention mechanism has been widely incorporated into deep
convolutional neural networks (CNNs) via long-range dependency capturing,
significantly lifting the performance in computer vision, but it may perform
poorly in medical imaging. Unfortunately, existing efforts are often unaware
that long-range dependency capturing has limitations in highlighting subtle
lesion regions, neglecting to exploit the potential of multi-scale pixel
context information to improve the representational capability of CNNs. In this
paper, we propose a practical yet lightweight architectural unit, Pyramid Pixel
Context Recalibration (PPCR) module, which exploits multi-scale pixel context
information to recalibrate pixel position in a pixel-independent manner
adaptively. PPCR first designs a cross-channel pyramid pooling to aggregate
multi-scale pixel context information, then eliminates the inconsistency among
them by the well-designed pixel normalization, and finally estimates per pixel
attention weight via a pixel context integration. PPCR can be flexibly plugged
into modern CNNs with negligible overhead. Extensive experiments on five
medical image datasets and CIFAR benchmarks empirically demonstrate the
superiority and generalization of PPCR over state-of-the-art attention methods.
The in-depth analyses explain the inherent behavior of PPCR in the
decision-making process, improving the interpretability of CNNs.
</p></li>
</ul>

<h3>Title: Artificial Intelligence for Dementia Research Methods Optimization. (arXiv:2303.01949v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01949">http://arxiv.org/abs/2303.01949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01949] Artificial Intelligence for Dementia Research Methods Optimization](http://arxiv.org/abs/2303.01949) #interpretability</code></li>
<li>Summary: <p>Introduction: Machine learning (ML) has been extremely successful in
identifying key features from high-dimensional datasets and executing
complicated tasks with human expert levels of accuracy or greater. Methods: We
summarize and critically evaluate current applications of ML in dementia
research and highlight directions for future research. Results: We present an
overview of ML algorithms most frequently used in dementia research and
highlight future opportunities for the use of ML in clinical practice,
experimental medicine, and clinical trials. We discuss issues of
reproducibility, replicability and interpretability and how these impact the
clinical applicability of dementia research. Finally, we give examples of how
state-of-the-art methods, such as transfer learning, multi-task learning, and
reinforcement learning, may be applied to overcome these issues and aid the
translation of research to clinical practice in the future. Discussion:
ML-based models hold great promise to advance our understanding of the
underlying causes and pathological mechanisms of dementia.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Generative Diffusions in Augmented Spaces: A Complete Recipe. (arXiv:2303.01748v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01748">http://arxiv.org/abs/2303.01748</a></li>
<li>Code URL: <a href="https://github.com/mandt-lab/PSLD">https://github.com/mandt-lab/PSLD</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01748] Generative Diffusions in Augmented Spaces: A Complete Recipe](http://arxiv.org/abs/2303.01748) #diffusion</code></li>
<li>Summary: <p>Score-based Generative Models (SGMs) have achieved state-of-the-art synthesis
results on diverse tasks. However, the current design space of the forward
diffusion process is largely unexplored and often relies on physical intuition
or simplifying assumptions. Leveraging results from the design of scalable
Bayesian posterior samplers, we present a complete recipe for constructing
forward processes in SGMs, all of which are guaranteed to converge to the
target distribution of interest. We show that several existing SGMs can be cast
as specific instantiations of this parameterization. Furthermore, building on
this recipe, we construct a novel SGM: Phase Space Langevin Diffusion (PSLD),
which performs score-based modeling in a space augmented with auxiliary
variables akin to a physical phase space. We show that PSLD outperforms
competing baselines in terms of sample quality and the speed-vs-quality
tradeoff across different samplers on various standard image synthesis
benchmarks. Moreover, we show that PSLD achieves sample quality comparable to
state-of-the-art SGMs (FID: 2.10 on unconditional CIFAR-10 generation),
providing an attractive alternative as an SGM backbone for further development.
We will publish our code and model checkpoints for reproducibility at
https://github.com/mandt-lab/PSLD.
</p></li>
</ul>

<h3>Title: Word-As-Image for Semantic Typography. (arXiv:2303.01818v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01818">http://arxiv.org/abs/2303.01818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01818] Word-As-Image for Semantic Typography](http://arxiv.org/abs/2303.01818) #diffusion</code></li>
<li>Summary: <p>A word-as-image is a semantic typography technique where a word illustration
presents a visualization of the meaning of the word, while also preserving its
readability. We present a method to create word-as-image illustrations
automatically. This task is highly challenging as it requires semantic
understanding of the word and a creative idea of where and how to depict these
semantics in a visually pleasing and legible manner. We rely on the remarkable
ability of recent large pretrained language-vision models to distill textual
concepts visually. We target simple, concise, black-and-white designs that
convey the semantics clearly. We deliberately do not change the color or
texture of the letters and do not use embellishments. Our method optimizes the
outline of each letter to convey the desired concept, guided by a pretrained
Stable Diffusion model. We incorporate additional loss terms to ensure the
legibility of the text and the preservation of the style of the font. We show
high quality and engaging results on numerous examples and compare to
alternative techniques.
</p></li>
</ul>

<h3>Title: Unleashing Text-to-Image Diffusion Models for Visual Perception. (arXiv:2303.02153v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.02153">http://arxiv.org/abs/2303.02153</a></li>
<li>Code URL: <a href="https://github.com/wl-zhao/VPD">https://github.com/wl-zhao/VPD</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.02153] Unleashing Text-to-Image Diffusion Models for Visual Perception](http://arxiv.org/abs/2303.02153) #diffusion</code></li>
<li>Summary: <p>Diffusion models (DMs) have become the new trend of generative models and
have demonstrated a powerful ability of conditional synthesis. Among those,
text-to-image diffusion models pre-trained on large-scale image-text pairs are
highly controllable by customizable prompts. Unlike the unconditional
generative models that focus on low-level attributes and details, text-to-image
diffusion models contain more high-level knowledge thanks to the
vision-language pre-training. In this paper, we propose VPD (Visual Perception
with a pre-trained Diffusion model), a new framework that exploits the semantic
information of a pre-trained text-to-image diffusion model in visual perception
tasks. Instead of using the pre-trained denoising autoencoder in a
diffusion-based pipeline, we simply use it as a backbone and aim to study how
to take full advantage of the learned knowledge. Specifically, we prompt the
denoising decoder with proper textual inputs and refine the text features with
an adapter, leading to a better alignment to the pre-trained stage and making
the visual contents interact with the text prompts. We also propose to utilize
the cross-attention maps between the visual features and the text features to
provide explicit guidance. Compared with other pre-training methods, we show
that vision-language pre-trained diffusion models can be faster adapted to
downstream visual perception tasks using the proposed VPD. Extensive
experiments on semantic segmentation, referring image segmentation and depth
estimation demonstrates the effectiveness of our method. Notably, VPD attains
0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCO-val referring
image segmentation, establishing new records on these two benchmarks. Code is
available at https://github.com/wl-zhao/VPD
</p></li>
</ul>

<h3>Title: Graph-based Extreme Feature Selection for Multi-class Classification Tasks. (arXiv:2303.01792v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01792">http://arxiv.org/abs/2303.01792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01792] Graph-based Extreme Feature Selection for Multi-class Classification Tasks](http://arxiv.org/abs/2303.01792) #diffusion</code></li>
<li>Summary: <p>When processing high-dimensional datasets, a common pre-processing step is
feature selection. Filter-based feature selection algorithms are not tailored
to a specific classification method, but rather rank the relevance of each
feature with respect to the target and the task. This work focuses on a
graph-based, filter feature selection method that is suited for multi-class
classifications tasks. We aim to drastically reduce the number of selected
features, in order to create a sketch of the original data that codes valuable
information for the classification task. The proposed graph-based algorithm is
constructed by combing the Jeffries-Matusita distance with a non-linear
dimension reduction method, diffusion maps. Feature elimination is performed
based on the distribution of the features in the low-dimensional space. Then, a
very small number of feature that have complementary separation strengths, are
selected. Moreover, the low-dimensional embedding allows to visualize the
feature space. Experimental results are provided for public datasets and
compared with known filter-based feature selection techniques.
</p></li>
</ul>

<h3>Title: Multi-Agent Adversarial Training Using Diffusion Learning. (arXiv:2303.01936v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.01936">http://arxiv.org/abs/2303.01936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.01936] Multi-Agent Adversarial Training Using Diffusion Learning](http://arxiv.org/abs/2303.01936) #diffusion</code></li>
<li>Summary: <p>This work focuses on adversarial learning over graphs. We propose a general
adversarial training framework for multi-agent systems using diffusion
learning. We analyze the convergence properties of the proposed scheme for
convex optimization problems, and illustrate its enhanced robustness to
adversarial attacks.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
